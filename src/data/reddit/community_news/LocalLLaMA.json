{
  "metadata": {
    "last_updated": "2026-02-16 03:09:30",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 20,
    "total_comments": 762,
    "file_size_bytes": 816164
  },
  "items": [
    {
      "id": "1r26zsg",
      "title": "Z.ai said they are GPU starved, openly.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/kjy1wqzt2xig1.jpeg",
      "author": "abdouhlili",
      "created_utc": "2026-02-11 19:28:16",
      "score": 1476,
      "num_comments": 242,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4vyt43",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-11 23:10:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uq8w8",
          "author": "atape_1",
          "text": "Great transparency. ",
          "score": 518,
          "created_utc": "2026-02-11 19:31:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uqzfi",
              "author": "ClimateBoss",
              "text": "Maybe they should do GLM Air instead of 760b model LMAO",
              "score": 179,
              "created_utc": "2026-02-11 19:35:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4us7o6",
                  "author": "suicidaleggroll",
                  "text": "A 744B model with 40B active parameters, in F16 precision.  That thing is gigantic (1.5 TB) at its native precision, and has more active parameters than Kimi.  They really went a bit nuts with the size of this one.",
                  "score": 150,
                  "created_utc": "2026-02-11 19:41:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o508m8p",
                  "author": "keyboardmonkewith",
                  "text": "No!!! Its suppose to know who is a pinocchio and dobby in a greatest detail.",
                  "score": 1,
                  "created_utc": "2026-02-12 16:38:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ux74h",
                  "author": "Ardalok",
                  "text": "Users probably don't buy Air tokens.",
                  "score": -2,
                  "created_utc": "2026-02-11 20:04:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4w7ihx",
              "author": "EndlessZone123",
              "text": "Wasn't great transparency to sell their coding plans cheap and have constant api errors.",
              "score": 23,
              "created_utc": "2026-02-11 23:58:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4wzf02",
              "author": "SkyFeistyLlama8",
              "text": "If they're complaining about *inference* being impacted by the lack of GPUs, then those domestic Huawei or whatever tensor chips aren't as useful as they were claimed to be. Inference is still an Nvidia or nothing situation.",
              "score": 8,
              "created_utc": "2026-02-12 02:45:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4y1ilp",
                  "author": "HoushouCoder",
                  "text": "Thoughts on Cerebras?",
                  "score": 1,
                  "created_utc": "2026-02-12 07:36:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4yr31g",
                  "author": "TylerDurdenFan",
                  "text": "I think Google's TPUs are doing just fine",
                  "score": 1,
                  "created_utc": "2026-02-12 11:38:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4uudro",
          "author": "x8code",
          "text": "I am GPU starved as well. I can't find an RTX 5090 for $2k. I would buy two right now if I could get them for that price.",
          "score": 208,
          "created_utc": "2026-02-11 19:51:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4veqwu",
              "author": "Shoddy_Bed3240",
              "text": "Buy RTX 6000 Pro 96gb instead. Microcenter have it in stock",
              "score": 26,
              "created_utc": "2026-02-11 21:29:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vj1gi",
                  "author": "Polymorphic-X",
                  "text": "Don't get it from microcenter unless you need the convenience.\nThey're $7.3k through places like exxact or other vendors. Significantly cheaper than Newegg or MC",
                  "score": 16,
                  "created_utc": "2026-02-11 21:50:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4w6l3m",
                  "author": "Guilty_Rooster_6708",
                  "text": "Isnâ€™t that also significantly higher priced than $4k?",
                  "score": 2,
                  "created_utc": "2026-02-11 23:53:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4vmcc0",
                  "author": "iMakeSense",
                  "text": "I'm not sure those are optimized for gaming though",
                  "score": -5,
                  "created_utc": "2026-02-11 22:06:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4vcask",
              "author": "PentagonUnpadded",
              "text": "I see DGX Spark / GB10 type systems going for the 3k MSRP right now. Why not build out with that system? \n\nI've seen comparisons showing a GB10 as 1/3 to 1/2 of a 5090 depending on the task, plus of course 4 times the vRam. Curious what tasks you have that make a dual-5090 system at $4k the way to go over alternatives like a GB10 cluster.",
              "score": 19,
              "created_utc": "2026-02-11 21:18:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vf4sb",
                  "author": "x8code",
                  "text": "I thought about it, but I also use my GPUs for PC gaming. I would get the 4 TB DGX Spark though, not the 1 TB model. Those go for $4k each last I checked. I would probably buy 2x DGX Spark though, so I could cluster them and run larger models with 256GB (*minus OS overhead*) of unified memory.",
                  "score": 14,
                  "created_utc": "2026-02-11 21:31:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4z75be",
                  "author": "SilentLennie",
                  "text": "> Why not build out with that system? \n\nLower memory bandwidth",
                  "score": 1,
                  "created_utc": "2026-02-12 13:29:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4uxson",
          "author": "sob727",
          "text": "I'm GPU starved as well.\n\n  \nGet in line.",
          "score": 89,
          "created_utc": "2026-02-11 20:07:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uunge",
          "author": "Clean_Hyena7172",
          "text": "Fair enough, I appreciate their honesty.",
          "score": 32,
          "created_utc": "2026-02-11 19:52:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uxahr",
          "author": "nuclearbananana",
          "text": "Deepseek has hinted at the same thing. I wonder how Kimi is managing to avoid it.",
          "score": 19,
          "created_utc": "2026-02-11 20:05:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v169e",
              "author": "TheRealMasonMac",
              "text": "I don't think they did. That's why they switched to INT4 which brings VRAM 4x lower than full fat GLM-5.",
              "score": 28,
              "created_utc": "2026-02-11 20:24:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vjzan",
                  "author": "nuclearbananana",
                  "text": "That helps with inf3rence, but not training.\n\nAlso 4x? Isn't the KV cache separate?",
                  "score": 6,
                  "created_utc": "2026-02-11 21:54:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4v3wyw",
                  "author": "Remote_Rutabaga3963",
                  "text": "What makes you think that GLM 5 is being served at fp16 ?",
                  "score": -1,
                  "created_utc": "2026-02-11 20:37:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wm6o5",
              "author": "-Cacique",
              "text": "For the past few days, I'm unable to use kimi 2.5 thinking, it's auto switched to 2.5 instant model due to high demand apparently.",
              "score": 2,
              "created_utc": "2026-02-12 01:25:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4y6dp2",
              "author": "Bac-Te",
              "text": "They're not, that's why they're doing Anthropic/ Google/ OpenAI price point instead of the GLM coding plan price point.",
              "score": 1,
              "created_utc": "2026-02-12 08:22:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o54an0u",
              "author": "ZoroWithEnma",
              "text": "No kimi is also affected by shortage, I'm frequently getting the system is busy try again later message or I'm being switched to kimi k2.5 instant model due to demand.",
              "score": 1,
              "created_utc": "2026-02-13 06:01:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4uqqcj",
          "author": "sammoga123",
          "text": "At least it's not like Google, suffering from demand and nerfing its models, probably due to quantification to sustain it XD",
          "score": 139,
          "created_utc": "2026-02-11 19:33:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ur754",
              "author": "abdouhlili",
              "text": "Gemini 3 flash is literally better than 3 Pro, Gemini models act like advertised benchmarks for about 3 weeks and then they start nerfing it.",
              "score": 139,
              "created_utc": "2026-02-11 19:36:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4urzge",
                  "author": "sammoga123",
                  "text": "Right now, pro plan users are complaining because they're only getting about 20 uses of the pro model. I've been trying to use NBP in the API and it fails, and when it does, the results are pretty baffling, which leads me to believe that's why they haven't released anything lately either.",
                  "score": 33,
                  "created_utc": "2026-02-11 19:39:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4vgbfl",
                  "author": "Goldkoron",
                  "text": "I find 2.5 pro better for some tasks than 3 pro. Kind of just switch between models for different advantages",
                  "score": 4,
                  "created_utc": "2026-02-11 21:37:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4xmihk",
                  "author": "Lazylion2",
                  "text": "I don't know why people say that, I use both with Antigravity and Pro solved some problems Flash couldn't",
                  "score": 1,
                  "created_utc": "2026-02-12 05:23:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4utoig",
                  "author": "RonJonBoviAkaRonJovi",
                  "text": "What an ignorant comment",
                  "score": -12,
                  "created_utc": "2026-02-11 19:48:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4vqfy9",
              "author": "dreamkast06",
              "text": "I wish they would just give a higher quota on the smaller models so we could use those when it makes sense. Right now, even using Air pulls from the same pool as full fat 4.7",
              "score": 1,
              "created_utc": "2026-02-11 22:26:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4zb994",
              "author": "RedParaglider",
              "text": "OMG I'm on the google ultra plan and I can't wait for that shit to be over with.  Nonstop failures on the models.  The Gemini TUI is unusable across all models.  It retries 3 times then throws an apology error all the time.  Google gave so much damn free access they can no longer support people paying them 260 a month.  At least opus 4.6 works decently on it with some failures but fewer.  \n\nThey advertised all this usage, but unless you want to sit and spam next next next next retry retry retry all damn day you will never get 1/100th of the usage promised.",
              "score": 1,
              "created_utc": "2026-02-12 13:52:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4vissm",
              "author": "-dysangel-",
              "text": "I think they might be. The coding plan quality is awful today compared to the last few weeks...",
              "score": 1,
              "created_utc": "2026-02-11 21:49:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4uu1bd",
          "author": "eli_pizza",
          "text": "Ok but to be fair, OpenAI says the same thing \n\n> OpenAI President Greg Brockman said the lack of compute is still holding the company back.\n\n> He said that even OpenAI's ambitious investments might not be enough to meet future demand.\n\n> OpenAI also published a chart that illustrates how scaling compute is the key to profitability.\n\nhttps://www.businessinsider.com/openai-chart-compute-future-plans-profitability-2025-12",
          "score": 50,
          "created_utc": "2026-02-11 19:49:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v5613",
              "author": "Ragvard_Grimclaw",
              "text": "It's less of a \"lack of compute\" and more of a \"lack of power grid capacity\". Here's an interview with Microsoft CEO:  \n[https://www.datacenterdynamics.com/en/news/microsoft-has-ai-gpus-sitting-in-inventory-because-it-lacks-the-power-necessary-to-install-them/](https://www.datacenterdynamics.com/en/news/microsoft-has-ai-gpus-sitting-in-inventory-because-it-lacks-the-power-necessary-to-install-them/)  \nYes, they've caused consumer GPU shortages due to shifting focus to datacenter GPUs, while not even having where to plug them. Guess it's time to also raise electricity prices for regular people because datacenters need it more?",
              "score": 45,
              "created_utc": "2026-02-11 20:43:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vjcgh",
                  "author": "MasterKoolT",
                  "text": "I'll say that Microsoft, at least in their giant data center project in SE Wisconsin, has committed to paying a higher electricity rate to fund power grid capacity increases. That hasn't been the story everywhere but seems like a good strategy to not antagonize locals (and is really just part of being a good neighbor)",
                  "score": 10,
                  "created_utc": "2026-02-11 21:51:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4xlfll",
                  "author": "EarEquivalent3929",
                  "text": "Looks like rich fucks not backing nuclear a decade ago for reasons of greed are coming back to bite them in the ass",
                  "score": 4,
                  "created_utc": "2026-02-12 05:14:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4vabe4",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 13,
                  "created_utc": "2026-02-11 21:08:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4y8r72",
                  "author": "VampiroMedicado",
                  "text": "I saw a report that theyâ€™re already doing that in the US, and also putting data centers nears people homes so they now hear a hum 24/7, itâ€™s amazing.",
                  "score": 1,
                  "created_utc": "2026-02-12 08:46:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ybyt0",
                  "author": "pier4r",
                  "text": "> Yes, they've caused consumer GPU shortages due to shifting focus to datacenter GPUs, while not even having where to plug them. \n\nas someone on youtube in a bullish way said \"there are no dark GPUs!\" (then darkness hit him)",
                  "score": 1,
                  "created_utc": "2026-02-12 09:18:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o515sjx",
              "author": "smayonak",
              "text": "OpenAI caused the bubble to begin with. This is market collusion. Prices wouldn't be so high if they didn't buy 40% of RAM supplies from manufacturers and dump huge amounts of money into Nvidia, using money borrowed from Nvidia.\n\nIt looks to me like the big tech companies colluded behind closed door to push out smaller competitors.",
              "score": 1,
              "created_utc": "2026-02-12 19:13:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o51gcsb",
                  "author": "eli_pizza",
                  "text": "Like Anthropic and OpenAI collided to spike ram prices to force out competitors? Plausible, but so is â€œthis is a land grab and compute is the scarce resourceâ€",
                  "score": 1,
                  "created_utc": "2026-02-12 20:04:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4uuday",
          "author": "Middle_Bullfrog_6173",
          "text": "They knew this but still went with a larger model and more active parameters? I guess they expect to get more compute soonish.",
          "score": 17,
          "created_utc": "2026-02-11 19:51:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wiu7v",
              "author": "AnomalyNexus",
              "text": "The only thing more important than having enough compute is having hype.\n\nThese days no hype means no investors means no money for compute\n\nSo you kinda have to go big or go home. Hence large model \n\nThis space is full of whacky logic where gravity doesnâ€™t apply and things fall up when you drop them  :/",
              "score": 14,
              "created_utc": "2026-02-12 01:05:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4y6imk",
                  "author": "Bac-Te",
                  "text": "No wonder why Google named their tool Antigravity lol",
                  "score": 3,
                  "created_utc": "2026-02-12 08:24:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4yc8jj",
              "author": "DerpSenpai",
              "text": "A big fat model is used to make the lower end models so right now most likely that's their priority",
              "score": 1,
              "created_utc": "2026-02-12 09:20:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4vk98o",
          "author": "ImmenseFox",
          "text": "Well that's just silly. I subscribed to the Pro plan as it said it will support flagship model updates and now they took it away - yeah they mention they'll roll it out but when you use the same wording as the max plan and then sneakily get rid of it from the list - doesnt fill me with any confidence.  \nGlad now I didn't renew for the whole year and instead just the quarter.",
          "score": 4,
          "created_utc": "2026-02-11 21:55:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uuls9",
          "author": "SubjectHealthy2409",
          "text": "Based, fully support them.",
          "score": 23,
          "created_utc": "2026-02-11 19:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uv1n8",
              "author": "abdouhlili",
              "text": "Do you know what GPUs they use for inference? NVIDIA or Huawei?",
              "score": 0,
              "created_utc": "2026-02-11 19:54:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4uvcg2",
                  "author": "SubjectHealthy2409",
                  "text": "Nop, don't know anything from behind the scenes",
                  "score": 9,
                  "created_utc": "2026-02-11 19:55:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4vwbkk",
                  "author": "vmnts",
                  "text": "If I recall correctly, the official Chinese policy was that you can use NVIDIA for training, but have to use local for inference (or at least you're not supposed to buy new NVIDIA GPUs for inference). I would imagine that they are using what they have, so it's probably a mix, but over time would trend towards Huawei",
                  "score": 1,
                  "created_utc": "2026-02-11 22:57:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ut2j9",
          "author": "jacek2023",
          "text": "No Air no fun.",
          "score": 10,
          "created_utc": "2026-02-11 19:45:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vw9ol",
          "author": "a_beautiful_rhind",
          "text": "You and me both. Their chat used to be fast, since I went back and used it the replies take forever. I just assumed they are struggling, especially when it's free. The speeds feel comprable to *me* running glm.",
          "score": 3,
          "created_utc": "2026-02-11 22:56:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xlo5y",
          "author": "EarEquivalent3929",
          "text": "Let's hope everyone being starved for compute and energy energizes the race for efficiency over raw power.",
          "score": 3,
          "created_utc": "2026-02-12 05:16:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v8lj5",
          "author": "Bandit-level-200",
          "text": "When are LLM makers going to make more efficient LLMs they are so inefficient in using both memory and power",
          "score": 4,
          "created_utc": "2026-02-11 21:00:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4vaweo",
              "author": "abdouhlili",
              "text": "GLM-5 uses new Deepseek sparse attention mechanism, which reduces inference costs up to 50%, Not only this, Z.ai doubled in this by increasing GLM-5 price. They are clearly chasing gross margins.",
              "score": 8,
              "created_utc": "2026-02-11 21:11:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4vbvba",
                  "author": "Bandit-level-200",
                  "text": "Yes but its still inefficient take context for example something that if it was just plain text would be a few KB/MB suddenly needs GB of memory just because it needs to be doubled or something for context to work.",
                  "score": 0,
                  "created_utc": "2026-02-11 21:16:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4v3o9y",
          "author": "Dudensen",
          "text": "Calm your ass down, a lot of labs do the same. Kimi literally said the same thing. Qwen too.",
          "score": 6,
          "created_utc": "2026-02-11 20:36:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4urqd3",
          "author": "Crafty-Diver-6948",
          "text": "I don't care if it's slow, I paid $360 for the inference for a year. happy to run Ralph's with that",
          "score": 8,
          "created_utc": "2026-02-11 19:38:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uxfhj",
              "author": "layer4down",
              "text": "Same. I appreciate the transparency and their wonderful pricing for a near Sonnet-4.5 parity model in GLM-4.7.  $360 year one was a no brainer and unfortunately these folks are a victim of their own success right now. Hope they can pull through now that they IPOâ€™d last month.",
              "score": 11,
              "created_utc": "2026-02-11 20:05:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4wj197",
              "author": "AnomalyNexus",
              "text": "Yup. Really hoping I can renew at similar",
              "score": 2,
              "created_utc": "2026-02-12 01:06:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wjlgq",
                  "author": "layer4down",
                  "text": "I got mine in October and it was a year one discount for 50% off. Will be $720/year thereafter.",
                  "score": 2,
                  "created_utc": "2026-02-12 01:09:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vdtwg",
          "author": "Comrade-Porcupine",
          "text": "What's positive here is this -- because it is open weight, that model will then be available from others, taking load off of GLM.\n\nDoesn't help GLM, per se, but it helps the software community. Too big to host myself, but it'll probably be on DeepInfra and others in short time. \n\nEDIT: [DeepInfra.com](http://DeepInfra.com) already showing it available.  For cheaper than [z.AI](http://z.AI) \n\nA situation that doesn't apply with OpenAI or Anthropic.",
          "score": 6,
          "created_utc": "2026-02-11 21:25:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4w9z4e",
              "author": "abaybektursun",
              "text": "Exactly this. DeepInfra already hosting it is huge for accessibility. I've been running some experiments comparing hosted vs local inference costs and for bigger models the third-party hosting economics actually work out better than most people expect. Curious if GLM-5 will be quantizable enough for 4090 setups or if it's strictly datacenter territory.",
              "score": 2,
              "created_utc": "2026-02-12 00:13:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4wzbha",
              "author": "LocoMod",
              "text": "Pssssssst. No one tell them OpenAI and Anthropic models are served by other providers in the largest most robust cloud platforms in the world. They will be content with running inference on jank mining rigs from shady providers for pennies on the dollar.\n\n::runs::",
              "score": 2,
              "created_utc": "2026-02-12 02:44:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ymowl",
              "author": "Moist-Length1766",
              "text": ">A situation that doesn't apply with OpenAI or Anthropic.\n\nYou're aware other providers supply OpenAI and Anthropic models right? sometimes cheaper than OpenAI/Anthropic themselves.",
              "score": 0,
              "created_utc": "2026-02-12 11:00:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4x0grz",
          "author": "LocoMod",
          "text": "Anyone notice how the sentiment towards remotely hosted models over provider APIs/services is different between western and Chinese models? Anyone? Where's the individual that always reminds us this is a local sub? Does this not seem strange to anyone? That the provider themselves is GPU starved because they scaled their models in preparation to pull the rug and funnel you folks to their service?\n\n\"But I could, one day self host it...\"\n\nI could sell a kidney too. But that's not the point. Look at the comments. Folks coping left and right and all of a sudden being positive about using someone else's computer.\n\nIt's all very heartwarming.",
          "score": 5,
          "created_utc": "2026-02-12 02:51:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xydjt",
              "author": "temperature_5",
              "text": "True, though Z probably gets \\*some\\* credit for releasing lots of great local models over the past year.  I guess we'll see if we ever get another GLM Air!",
              "score": 3,
              "created_utc": "2026-02-12 07:06:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o515me9",
          "author": "Pineapple_King",
          "text": "me too, [Z.ai](http://Z.ai), me too",
          "score": 2,
          "created_utc": "2026-02-12 19:12:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vhefp",
          "author": "larrytheevilbunnie",
          "text": "Everyone is compute starved, respect them for their work though",
          "score": 2,
          "created_utc": "2026-02-11 21:42:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vu709",
          "author": "florinandrei",
          "text": "I mean, who isn't?",
          "score": 2,
          "created_utc": "2026-02-11 22:46:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wtqgo",
          "author": "Puzzled_Fisherman_94",
          "text": "Theyâ€™ll get more efficient before GPUâ€™s catch up ðŸ˜…",
          "score": 2,
          "created_utc": "2026-02-12 02:11:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v23cp",
          "author": "Tema_Art_7777",
          "text": "Well now they can get the h200 and scale!",
          "score": 1,
          "created_utc": "2026-02-11 20:28:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v2em7",
              "author": "Tema_Art_7777",
              "text": "Well now they can get the h200 and scale. btw at least they had a restriction against them. anthropic has no such restrictions and they are rate limiting the **** out of api users.",
              "score": 2,
              "created_utc": "2026-02-11 20:30:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vd00l",
                  "author": "PentagonUnpadded",
                  "text": "It is sensible to assume investor money is subsidizing agents. I wonder where the equilibrium price of such services 'should' sit if they weren't priced as loss leaders.",
                  "score": 1,
                  "created_utc": "2026-02-11 21:21:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4v2n7i",
          "author": "OcelotMadness",
          "text": "Oh hell ya on GLM-5. Have not seen that yet. I have a super super long text adventure going and I've spent like 20 bucks on it using sonnet 4.5 once in a while, along with my usual GLM 4.7 on the coding plan. I hope they continued working on storytelling like they said they would. Cautiously hyped.",
          "score": 1,
          "created_utc": "2026-02-11 20:31:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wk3jp",
              "author": "AnomalyNexus",
              "text": "Heads up storytelling tools on coding plan is likely a terms violation.\n\nI doubt itâ€™s enforced though \n\n> Can I use my GLM Coding Plan quota in non-AI coding tools?\nA: No. The GLM Coding Plan quota is only intended to be used within coding/IDE tools designated or recognized by Z.ai",
              "score": 1,
              "created_utc": "2026-02-12 01:12:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o516rvf",
                  "author": "OcelotMadness",
                  "text": "It is, but I don't use it for that a ton. I know it, and zAI knows it, and it makes the plan actually valuable for me since I try not to use LLMs for my coding very much or at all for a lot of things. I do not think they're gonna actually suspend my account to be honest with you.",
                  "score": 1,
                  "created_utc": "2026-02-12 19:18:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ve3sv",
          "author": "davernow",
          "text": "I have the coder plan and have noticed some lag in the last week. Still  great service.",
          "score": 1,
          "created_utc": "2026-02-11 21:26:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vig4d",
          "author": "-dysangel-",
          "text": "Hmm I had weird rate limits all afternoon on normal usage, and since then GLM Coding Plan has been performing \\*very\\* poorly. The model keeps failing but stubbornly insisting that it succeeded etc. 4.7 was working very well for me so I wonder why they're so keen to change to 5 if it's starving them of resources..",
          "score": 1,
          "created_utc": "2026-02-11 21:47:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4w4ut8",
          "author": "olearyboy",
          "text": "Same",
          "score": 1,
          "created_utc": "2026-02-11 23:43:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wh7ea",
          "author": "Odd-Criticism1534",
          "text": "Are all their data centers in china?",
          "score": 1,
          "created_utc": "2026-02-12 00:55:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wkqfy",
              "author": "AnomalyNexus",
              "text": "Last I looked at the IPs it appeared to serve me from Europe but thatâ€™s not exactly bulletproof. Might be proxying it back to China",
              "score": 2,
              "created_utc": "2026-02-12 01:16:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wkyaq",
                  "author": "Odd-Criticism1534",
                  "text": "Youâ€™d think compute wouldnâ€™t be a struggle if hosted in Chinese data centers theyâ€™re so ahead",
                  "score": 0,
                  "created_utc": "2026-02-12 01:18:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xpyve",
          "author": "Fresh-Soft-9303",
          "text": "Serving top models for free isn't easy, the work they're doing is awesome and much appreciated. Without open source models AI would be a lot different today.",
          "score": 1,
          "created_utc": "2026-02-12 05:51:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o517u9z",
          "author": "CarelessOrdinary5480",
          "text": "Everyone knew this or should have.  I loved GLM 4.5 air so much I signed up for their max plan.  Total whiff.  It was pretty unusable for my workflow.   Hopefully china can get them more huwai chips or something.",
          "score": 1,
          "created_utc": "2026-02-12 19:23:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5218kj",
          "author": "HarjjotSinghh",
          "text": "this sounds like a tiny room with one fan blowing straight down your head",
          "score": 1,
          "created_utc": "2026-02-12 21:43:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5330p7",
          "author": "mr_zerolith",
          "text": "I know the feeling!",
          "score": 1,
          "created_utc": "2026-02-13 01:09:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55dhc6",
          "author": "Ok_Warning2146",
          "text": "Didn't they just get US$500M from their HK IPO? Now China can also buy H200s, so their compute shortage should only be solved in due time.",
          "score": 1,
          "created_utc": "2026-02-13 11:50:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dpw42",
          "author": "NeoLogic_Dev",
          "text": "Love it when ppl are honest",
          "score": 1,
          "created_utc": "2026-02-14 18:34:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5drels",
          "author": "twisted_nematic57",
          "text": "I'd like to see a multimodal vision version of GLM-5. It's the only thing keeping me from upgrading from my 4.6V-flash rn.",
          "score": 1,
          "created_utc": "2026-02-14 18:42:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vpqak",
          "author": "HugoCortell",
          "text": "This will ultimately be good, we need to focus on making the most out of resources, not bloating like western models do.",
          "score": 1,
          "created_utc": "2026-02-11 22:23:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uzbx2",
          "author": "arm2armreddit",
          "text": "What kind of GPUs do they use? Nice to see there are still honest and transparent companies around.",
          "score": 1,
          "created_utc": "2026-02-11 20:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vjefp",
          "author": "brickout",
          "text": "We all are.",
          "score": 1,
          "created_utc": "2026-02-11 21:51:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wa1u7",
          "author": "EiwazDeath",
          "text": "Makes you wonder if the industry is approaching this from the wrong angle. Everyone is fighting over the same GPU supply while 1 bit quantization lets you run inference on CPUs that are already sitting in billions of devices worldwide. The bottleneck isn't compute anymore, it's memory bandwidth, and CPUs have plenty of that. Maybe the GPU shortage is a hardware problem with a software solution.",
          "score": 1,
          "created_utc": "2026-02-12 00:13:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4x1txe",
          "author": "Significant-Cod-9936",
          "text": "At least theyâ€™re being honest unlike most companiesâ€¦",
          "score": 1,
          "created_utc": "2026-02-12 02:59:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wi5j4",
          "author": "Rich_Artist_8327",
          "text": "just hit it",
          "score": 0,
          "created_utc": "2026-02-12 01:00:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wu00p",
          "author": "FPham",
          "text": "And how is it? How is the GLM-5?",
          "score": 0,
          "created_utc": "2026-02-12 02:13:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yuhnn",
              "author": "harlekinrains",
              "text": "Not bad for 4 cents:\n\nhttps://pastebin.com/mHaGKqd1",
              "score": 1,
              "created_utc": "2026-02-12 12:05:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4v6hwj",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -3,
          "created_utc": "2026-02-11 20:50:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4vslc3",
              "author": "fallingdowndizzyvr",
              "text": "WTF are you talking about. They released it.\n\nhttps://huggingface.co/zai-org/GLM-5-FP8",
              "score": 1,
              "created_utc": "2026-02-11 22:37:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0zn8o",
      "title": "Hugging Face Is Teasing Something Anthropic Related",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/wvu2vi2jwnig1.png",
      "author": "Few_Painter_5588",
      "created_utc": "2026-02-10 12:39:52",
      "score": 1011,
      "num_comments": 225,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r0zn8o/hugging_face_is_teasing_something_anthropic/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4n3i0d",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-10 16:45:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lt87q",
          "author": "Leflakk",
          "text": "I would not expect too much from them lol",
          "score": 597,
          "created_utc": "2026-02-10 12:44:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mgiyu",
              "author": "ApprehensiveAd3629",
              "text": "i guess we will get GTA 6 before an Anthropic open model",
              "score": 187,
              "created_utc": "2026-02-10 14:55:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mj7py",
                  "author": "vikarti_anatra",
                  "text": "So soon? I thought time of Half-Life 3 release is approriate \n\n",
                  "score": 36,
                  "created_utc": "2026-02-10 15:09:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4n6yg4",
                  "author": "arcanemachined",
                  "text": "It's for your safety, citizen. - Anthropic\n\nSeriously though, I never actually thought OpenAI would release an open-weight model, but they did eventually do it. So there is some hope.",
                  "score": 11,
                  "created_utc": "2026-02-10 17:01:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4n4sb8",
                  "author": "No_Afternoon_4260",
                  "text": "I wouldn't bet on that, they need to get rid of kimi\n\nOpus 4.6 is wild but k2.5 is dirt cheap and not that far away",
                  "score": 2,
                  "created_utc": "2026-02-10 16:51:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nkydh",
                  "author": "cloverasx",
                  "text": "so no sooner than 2028? ðŸ˜‚",
                  "score": 1,
                  "created_utc": "2026-02-10 18:05:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mcg7a",
              "author": "MyHobbyIsMagnets",
              "text": "Truly the most annoying AI company. I was a huge fan, but itâ€™s so nice to see OpenAI and open source catching up for coding. Anthropic deserves to crash and burn.",
              "score": 50,
              "created_utc": "2026-02-10 14:34:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mhcp8",
                  "author": "No_Swimming6548",
                  "text": "B.. but the safety!",
                  "score": 28,
                  "created_utc": "2026-02-10 15:00:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nk018",
                  "author": "Able-Swing-6415",
                  "text": "What do people dislike about it? I stopped using it for limits other than that it's the best AI model on the market for me.",
                  "score": 6,
                  "created_utc": "2026-02-10 18:01:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mnvwx",
                  "author": "Quiet_Figure_4483",
                  "text": "Which open source model do you recommend for coding?",
                  "score": 3,
                  "created_utc": "2026-02-10 15:32:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4o2lh4",
                  "author": "toothpastespiders",
                  "text": ">Truly the most annoying AI company.\n\nThey're among the best at social media marketing which makes them even more annoying.",
                  "score": 4,
                  "created_utc": "2026-02-10 19:26:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4v03ya",
                  "author": "AddressForward",
                  "text": "Iâ€™ve been swinging away from the slowly - from fanboy to curious about the alternatives. Not being able to use my subscription on opencode was annoying (Claude code written in React was a bizarre move).. but most annoying is the smugness and self-righteousness of Amodei. \n\nActually all the AI heads annoy me for different reasons. Maybe itâ€™s a me thing.",
                  "score": 1,
                  "created_utc": "2026-02-11 20:18:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ncd72",
                  "author": "robbievega",
                  "text": "why exactly? you prefer OpenAI ads? or Sam Altman's $1 donation million to Trumpâ€™s inaugural fund?",
                  "score": -2,
                  "created_utc": "2026-02-10 17:26:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4n3fnq",
              "author": "TRKlausss",
              "text": "Maybe they release old models, who knowsâ€¦",
              "score": 2,
              "created_utc": "2026-02-10 16:44:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4mbse6",
              "author": "Full-Teach3631",
              "text": "Lol me neither",
              "score": -1,
              "created_utc": "2026-02-10 14:31:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ltbc6",
          "author": "Technical-Earth-3254",
          "text": "I agree, these guys would never ever release a real oss model.",
          "score": 310,
          "created_utc": "2026-02-10 12:45:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lyfvr",
              "author": "-p-e-w-",
              "text": "They need VC money and mindshare, just like everyone else. When their investors keep asking why they donâ€™t have open-weights releases while all of their competitors do, they canâ€™t just shrug and move on without cost. He who pays the piper calls the tune.",
              "score": 43,
              "created_utc": "2026-02-10 13:17:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4lzf8w",
                  "author": "Howdareme9",
                  "text": "Yes they can lmao. Very naive to think investors will care about open source models..",
                  "score": 140,
                  "created_utc": "2026-02-10 13:23:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4m5b9n",
                  "author": "[deleted]",
                  "text": "What's the odds they've already put a model out to test the water...Â ",
                  "score": 1,
                  "created_utc": "2026-02-10 13:56:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4qji5y",
                  "author": "ireccomendit",
                  "text": "Thatâ€™s when they release a new skill ðŸ¤£",
                  "score": 1,
                  "created_utc": "2026-02-11 03:18:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mq68l",
              "author": "CuriouslyCultured",
              "text": "They do have an incentive to create an onramp for their ecosystem as a competitor with the small Chinese models. The problem is they're scared of releasing dangerous models openly and the capability front of open models is in \"dangerous\" territory, so they'd want to spend an inordinate amount of time aligning it, which they might not have.",
              "score": 1,
              "created_utc": "2026-02-10 15:43:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4nmspy",
                  "author": "crantob",
                  "text": "Dangerous only to censors.\n\nThe real danger is governments currently using AI to kill people.\n\nCurrently.  With flying robots.\n\nLet that sink in.",
                  "score": 2,
                  "created_utc": "2026-02-10 18:14:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4m607i",
          "author": "MagicZhang",
          "text": "they're gonna release a 1,500 page safety document on why open-source is bad for the community",
          "score": 128,
          "created_utc": "2026-02-10 13:59:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4p4hda",
              "author": "Super_Sierra",
              "text": "before you even run Claude, it uses nearly 65k tokens just on the pre-token safety shit. \n\nMight be more than that. ",
              "score": 21,
              "created_utc": "2026-02-10 22:23:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lvpq0",
          "author": "Ok-Pipe-5151",
          "text": "At best, some \"safety\" dataset might be coming. I don't expect anything more than that from anthropicÂ ",
          "score": 127,
          "created_utc": "2026-02-10 13:00:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4maue6",
              "author": "Thick-Protection-458",
              "text": "On the other hand - did we expected something from AlmostClosedAI before oss?",
              "score": 30,
              "created_utc": "2026-02-10 14:26:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mln9c",
                  "author": "Ok-Pipe-5151",
                  "text": "They've not been beating drum about \"dangers of open models\" like Anthropic. This is the difference.",
                  "score": 34,
                  "created_utc": "2026-02-10 15:21:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mq0pf",
                  "author": "EstarriolOfTheEast",
                  "text": "At least they'd released whisper and even iterated on it with several released improved versions into recent times, so it wasn't completely unexpected. llama.cpp evolved from whisper.cpp iirc, so they even played an important indirect role on the current scene (discounting the ancient gpt2 history, which was also the architectural foundation for llama and motivated the genesis of huggingface). \n\nThey also released CLIP (highly influential to generative AI art) and jukebox, so even if they later got the deserved name of closed-ai, they'd still, unlike Anthropic, made several core pivotal contributions to open AI.",
                  "score": 20,
                  "created_utc": "2026-02-10 15:42:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mzkuh",
              "author": "sine120",
              "text": "Trying to get some safety PR since their partnership with Palantir is making people more and more nervous.",
              "score": 12,
              "created_utc": "2026-02-10 16:27:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4m46uf",
              "author": "Mescallan",
              "text": "I could see an RL environment frame work or something for training sparse auto encoders",
              "score": 1,
              "created_utc": "2026-02-10 13:50:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4o8ax7",
              "author": "throwaway2676",
              "text": "Yeah, if there's a prediction market, I'm betting on a safety dataset or safety benchmark.  *Maybe* some kind of explainability tool",
              "score": 1,
              "created_utc": "2026-02-10 19:52:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lur4d",
          "author": "constanzabestest",
          "text": "Let's be honest here, if Antropic actually dropped open weights then i would be fully convinced that either 1. We live in some sort of bizzaro world or 2. The world is ending as we speak.",
          "score": 157,
          "created_utc": "2026-02-10 12:54:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lwdpq",
              "author": "Icetato",
              "text": "Or 3. It's ass",
              "score": 110,
              "created_utc": "2026-02-10 13:04:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4lz4cd",
                  "author": "XiRw",
                  "text": "Most likely this so they can just say they released something. Unless their ego is too big they donâ€™t want to look like shit even in the local llm world",
                  "score": 21,
                  "created_utc": "2026-02-10 13:21:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mbfh1",
                  "author": "MelodicRecognition7",
                  "text": "> ass\n\nGPT-ASS?",
                  "score": 12,
                  "created_utc": "2026-02-10 14:29:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4m39hf",
                  "author": "TheGABB",
                  "text": "*and",
                  "score": 4,
                  "created_utc": "2026-02-10 13:45:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nppog",
                  "author": "Thomas-Lore",
                  "text": "They could at least release some of their legacy models. Claude 2 would be nice.",
                  "score": 2,
                  "created_utc": "2026-02-10 18:27:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nt6v6",
                  "author": "yeah-ok",
                  "text": "Or 4, it's some sort of wild meta rug pull that indicates the CCP have integrated Anthropic and are force releasing everything openly.",
                  "score": 1,
                  "created_utc": "2026-02-10 18:43:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4miikf",
              "author": "ab2377",
              "text": "or claude opus has convinced dario amodei to release an open weight 30b all made by opus itself.",
              "score": 18,
              "created_utc": "2026-02-10 15:06:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mmg06",
                  "author": "Traditional-Gap-3313",
                  "text": "I'd put money down that it's this\n\n> look at what my kid did all by itself",
                  "score": 17,
                  "created_utc": "2026-02-10 15:25:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4m87fj",
              "author": "TheRealMasonMac",
              "text": "I would love something that is the equivalent of even Haiku 3.5â€¦ itâ€™s such a solid model. Anthropicâ€™s instruction following is simply and utterly unmatched (though open weights are getting there).",
              "score": 6,
              "created_utc": "2026-02-10 14:11:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4n1egs",
              "author": "Both-Employment-5113",
              "text": "we live in some sort of bizzato world bruh, how else would u explain all the ongoings the last 200 years lmao",
              "score": 0,
              "created_utc": "2026-02-10 16:35:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ltsfg",
          "author": "DeltaSqueezer",
          "text": "Maybe they will open source their recent adverts :P",
          "score": 52,
          "created_utc": "2026-02-10 12:48:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sg9li",
              "author": "Twistpunch",
              "text": "How to fine tune your models to include their ads.",
              "score": 1,
              "created_utc": "2026-02-11 12:43:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lz9qz",
              "author": "XiRw",
              "text": "Got an email from OpenAI they will be doing ads too unfortunately. Was only a matter of time.",
              "score": 0,
              "created_utc": "2026-02-10 13:22:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mko84",
                  "author": "AdIllustrious436",
                  "text": "\"Too\" ? They are litteraly alone in that boat for now. We can only hope it stays like that...",
                  "score": 12,
                  "created_utc": "2026-02-10 15:16:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4qk47f",
                  "author": "Feztopia",
                  "text": "Bro you must be living under a rock, Anthropic ads are ads about OpenAI adding ads to chatgpt.",
                  "score": 1,
                  "created_utc": "2026-02-11 03:22:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lvre8",
          "author": "Pvt_Twinkietoes",
          "text": "lol. Them and open source don't go together.",
          "score": 22,
          "created_utc": "2026-02-10 13:00:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mbys9",
          "author": "publicbsd",
          "text": "expect 50 gig SafetyDogshit . md ",
          "score": 18,
          "created_utc": "2026-02-10 14:32:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lx68c",
          "author": "drooolingidiot",
          "text": "Probably something interpretability related. I wouldn't expect a model usable for end-users. They've been actively hostile to open source.",
          "score": 18,
          "created_utc": "2026-02-10 13:09:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m58ti",
          "author": "Prof_ChaosGeography",
          "text": "Interesting.... Anthropic has been the most pro regulatory capture, a bit more evil the Open AI. The reason they are slightly better with models is they hired the Google books guy and bought s ton of out of print books to scan for the tokens in a destructive manner for speed.Â \n\n\n\n\nMy bet is we are all getting excited for a dataset that will be very safety aligned and absolutely neuter models that use it.\n\n\nBut part of me thinks they are doing something different to battle openai and the Chinese labs and force them onto their backfoot. They have been taking a different approach recently so I'll hope it's an open model that can compete with gpt-oss-120b but I doubt it. I don't think they will release any code focused model that's their bread and butter",
          "score": 11,
          "created_utc": "2026-02-10 13:55:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mg8bj",
          "author": "DrNavigat",
          "text": "They're going to release a 5-word phrase, completely open source, MIT. You'll be able to say the phrase whenever and as many times as you want.",
          "score": 11,
          "created_utc": "2026-02-10 14:54:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m3uxw",
          "author": "AdamEgrate",
          "text": "Donâ€™t read too much into the enterprise account thing.  At the valuation theyâ€™re demanding paying for that is peanuts. They probably just did because it enhances visibility.",
          "score": 4,
          "created_utc": "2026-02-10 13:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m8m8x",
          "author": "Ylsid",
          "text": "inb4 safety classification dataset",
          "score": 8,
          "created_utc": "2026-02-10 14:14:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lus4h",
          "author": "vrmorgue",
          "text": "Maybe! But no hope",
          "score": 5,
          "created_utc": "2026-02-10 12:54:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n236k",
          "author": "the__storm",
          "text": "My prediction: they release one or two small models that are limited in some way (e.g. no multimodal).  They have inference bugs or are otherwise panned on release.  Six months later everyone has a change of heart and finds that they're really useful for certain tasks, and hold up better out of distribution and on long context than other open weights models, even if they're somewhat safety-maxxed and don't score as well on benchmarks.  Thank you for coming to my TED talk.",
          "score": 4,
          "created_utc": "2026-02-10 16:38:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4op5q0",
              "author": "jkflying",
              "text": "Sort of like GPT-OSS 30/120 are now...",
              "score": 1,
              "created_utc": "2026-02-10 21:11:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ltxwo",
          "author": "Few_Painter_5588",
          "text": "Personally I hope it's a coding/reasoning benchmark. The current benchmarks we have are too saturated now.",
          "score": 10,
          "created_utc": "2026-02-10 12:49:17",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4n4qwj",
              "author": "Emotional_Egg_251",
              "text": "If it is, I'm sure it'll be completely unbiased...",
              "score": 8,
              "created_utc": "2026-02-10 16:51:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lvi87",
          "author": "Middle_Bullfrog_6173",
          "text": "Yeah something alignment related seems possible. That's where they have been most open. Of course they could simply be paying to get more private storage and bandwidth for using other people's datasets.",
          "score": 8,
          "created_utc": "2026-02-10 12:59:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mau1i",
          "author": "alerikaisattera",
          "text": "One thing I can see them releasing is a pseudo-open available proprietary model. They hate open-source with passion and want to destroy it. They tried to do it with fearmongering, but it didn't work. Now they may resort to releasing proprietary AI and misrepresenting it as open-source, a tactic that has worked quite a few times in the past",
          "score": 7,
          "created_utc": "2026-02-10 14:26:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lxmjk",
          "author": "Free-Internet1981",
          "text": "I bet it will be underwhelming",
          "score": 6,
          "created_utc": "2026-02-10 13:12:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lxn7v",
          "author": "YogurtExternal7923",
          "text": "OMG THEY'LL PROBABLY RELEASE SONNET 5 AS OPEN SOURCE!!!!!\n\nJokes aside this might be a pleasant surprise, but we already got an open weights claude model since kimi, deepseek and glm all use claude outputs as training data",
          "score": 6,
          "created_utc": "2026-02-10 13:12:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4p4ru0",
              "author": "Super_Sierra",
              "text": "Claude 2 still mogs most of open source in creative writing tasks, i'd take Claude 2. ",
              "score": 1,
              "created_utc": "2026-02-10 22:25:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4pfezb",
                  "author": "YogurtExternal7923",
                  "text": "I was gonna say \"no way man\" but you know what? I actually remember when claude 2 was on their free experimental API days and.. PHEW! that thing wasn't technically smart but it was GOOD!",
                  "score": 2,
                  "created_utc": "2026-02-10 23:22:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lvnl9",
          "author": "Such_Advantage_6949",
          "text": "They are worse than openai",
          "score": 15,
          "created_utc": "2026-02-10 13:00:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n3lh3",
              "author": "Freonr2",
              "text": "At least I believe what Dario says, and Anthropic wears their bias on their sleeve. \n\nI don't trust a word that comes out of Sam's mouth.",
              "score": 5,
              "created_utc": "2026-02-10 16:45:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lwlk6",
              "author": "MrHanoixan",
              "text": "Can you explain what you mean? It seems like the general perception is that OpenAI has been a shadier business. In what ways do you think Anthropic is worse? No dog in this fight, just curious.",
              "score": 1,
              "created_utc": "2026-02-10 13:06:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4lwy0d",
                  "author": "fizzy1242",
                  "text": "they've been openly against open weights llms",
                  "score": 22,
                  "created_utc": "2026-02-10 13:08:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4lz66l",
                  "author": "Orolol",
                  "text": "I like very much Claude models, but Anthropic is very vocal against open models, calls for heavy regulation against anything that could threatens their business model, never released anything open, call for tech war against China, and have contract with every comics-like vilain corporation in the world (Palantir for example)",
                  "score": 18,
                  "created_utc": "2026-02-10 13:21:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4lyhu4",
                  "author": "ResidentPositive4122",
                  "text": "Anthropic has been the loudest proponent of regulatory capture. They want the field heavily regulated \"for the kids/safety/doomsday/manhattanproject/claudesfeelings/etc\" and they want the regulations to basically keep everyone not already established in this \"muh security\" out. They do come up with banger coding models, but their stance in the field is abhorrent.",
                  "score": 30,
                  "created_utc": "2026-02-10 13:17:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4lznnv",
                  "author": "Ok_Top9254",
                  "text": "There was a thread somewhere from an ex-employee talking about it on twitter...\n\nOpenAI might just be a regular greedy corpo doing it for the money, but Anthropic is apparently basically a sect.\n\nLike some people there genuinely believe/-d, that they are destined to make AGI and be at the forefront of revolution that will lead humanity to greater future and yada yada. I think it was mainly the CEO but also some other higher ups working there sharing the same delusion.",
                  "score": 15,
                  "created_utc": "2026-02-10 13:24:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n6ax3",
          "author": "grimjim",
          "text": "Some of their safety and bias research released on Github have come with datasets. HF could be another place for them.",
          "score": 3,
          "created_utc": "2026-02-10 16:58:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nnpe7",
          "author": "Kahvana",
          "text": "No expectations, but it would be cool if they drop the deprecated opus/sonnet/haiku 3.7 models on there.",
          "score": 3,
          "created_utc": "2026-02-10 18:18:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4phcbs",
          "author": "ortegaalfredo",
          "text": "They will release a 65 GB MoE model nvfxp2 quantization that answers \"No.\" to any query.",
          "score": 3,
          "created_utc": "2026-02-10 23:32:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ltrec",
          "author": "SrijSriv211",
          "text": "Notice how he mentioned \"large model\" before \"dataset\". Maybe. Just Maybe. What if?",
          "score": 9,
          "created_utc": "2026-02-10 12:48:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m29xl",
          "author": "KvAk_AKPlaysYT",
          "text": "I'll have a Claude-OSS-30B-A3B and a Claude-OSS-200B-A20B on the side pleaseðŸ™‚â€â†•ï¸",
          "score": 3,
          "created_utc": "2026-02-10 13:39:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m769f",
          "author": "willitexplode",
          "text": "$10 says it's model assessment tools. ",
          "score": 2,
          "created_utc": "2026-02-10 14:06:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mfu1l",
          "author": "Several-System1535",
          "text": "Soâ€¦ is open-sourcing a dataset actually safe against AGI threats?Â ",
          "score": 2,
          "created_utc": "2026-02-10 14:52:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mtd6e",
          "author": "HatEducational9965",
          "text": "epic-oss-20b please",
          "score": 2,
          "created_utc": "2026-02-10 15:58:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lwk4l",
          "author": "Outrageous-Thing-900",
          "text": "They could release an open weight opus 4.6 and no one would be able to run it anyways",
          "score": 3,
          "created_utc": "2026-02-10 13:06:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lxut6",
              "author": "redditorialy_retard",
              "text": "companies would, I help manage my company's internal AI code reviewers and I think they got at least 500GB-1TB of Vram.Â ",
              "score": 4,
              "created_utc": "2026-02-10 13:14:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4m5ene",
                  "author": "j_osb",
                  "text": "opus is clearly >1t params.",
                  "score": 2,
                  "created_utc": "2026-02-10 13:56:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4lyquk",
              "author": "SpicyWangz",
              "text": "If they released anything it would be a model with the performance of haiku 3.5 or something",
              "score": 2,
              "created_utc": "2026-02-10 13:19:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4moc2z",
                  "author": "Traditional-Gap-3313",
                  "text": "but if they do that, it will probably be trained exclusively on synthetic data as gpt-oss was, which means it won't be as good in all the things haiku was. They'll probably focus on coding, while haiku was great in lower resource languages... available OS models are better then haiku 3.5 for coding, we don't need another coding model, we need the writing focused model and I don't see how they would release the weights for that due to opening themselves to people finding what they trained on (at least partially). If a single copyright holder can prove their data was used when it shouldn't have been, they'd open themselves up to a shitstorm",
                  "score": 3,
                  "created_utc": "2026-02-10 15:34:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4m5hvp",
                  "author": "j_osb",
                  "text": "I mean, if it's reasonably small (which haiku probably is) it still is pretty okay at like, creative writing.",
                  "score": 1,
                  "created_utc": "2026-02-10 13:57:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lwocx",
          "author": "His0kx",
          "text": "If they (ever) release a model, I guess it would be Sonnet 3.5 : no risk and it makes buzz for Sonnet 5",
          "score": 4,
          "created_utc": "2026-02-10 13:06:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nmftx",
              "author": "MaterialSuspect8286",
              "text": "Nah, they'd never. Even Sonnet 3.5 is decent enough. It'll probably something like the AI constitution they released.",
              "score": 8,
              "created_utc": "2026-02-10 18:12:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4pypi3",
                  "author": "His0kx",
                  "text": "Imagine the waste of time â€¦ even a Mourinho team would be more respectful in terms of wasting time",
                  "score": 1,
                  "created_utc": "2026-02-11 01:12:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4lxxfz",
              "author": "redditorialy_retard",
              "text": "either a 100b model that performs like sonnet 3.5/4 or some 30b and under",
              "score": 2,
              "created_utc": "2026-02-10 13:14:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4lz2z3",
                  "author": "His0kx",
                  "text": "Yep canâ€™t see them giving a model over the 3.5 versions. Maybe Haiku 3.5 could be a good fit for local ?",
                  "score": 4,
                  "created_utc": "2026-02-10 13:21:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4op0o3",
              "author": "jkflying",
              "text": "Maybe they release a Haiku model.",
              "score": 1,
              "created_utc": "2026-02-10 21:10:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lz0d3",
          "author": "Agusx1211",
          "text": "Someone needs to shed openclaw load",
          "score": 3,
          "created_utc": "2026-02-10 13:20:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pz9bs",
              "author": "DealingWithIt202s",
              "text": "This is the real answer.",
              "score": 1,
              "created_utc": "2026-02-11 01:15:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nmzkf",
          "author": "One-Employment3759",
          "text": "They can't even open source claude code.\n\n\nAnd they refuse to even admit it's closed source in the README of their stub github repo.",
          "score": 4,
          "created_utc": "2026-02-10 18:15:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m03fi",
          "author": "superkickstart",
          "text": "Embrace, extend, and extinguish.",
          "score": 2,
          "created_utc": "2026-02-10 13:27:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mx28e",
          "author": "DarKresnik",
          "text": "They will release a new big Pricelist.",
          "score": 2,
          "created_utc": "2026-02-10 16:15:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nqpxa",
          "author": "Orik_Hollowbrand",
          "text": "Whatever they have, I don't care. Wherever these people are, I'm on the opposite side.",
          "score": 2,
          "created_utc": "2026-02-10 18:31:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m218l",
          "author": "GoranjeWasHere",
          "text": "Every closed source dev has to release something open source otherwise whole infrastructure will move away from them. \n\nThat's why chinese are leading right now. They know best models either way have to be run on their farms or with their agreement meanwhile everyone else is tying themselves into their workflows.",
          "score": 1,
          "created_utc": "2026-02-10 13:38:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n4ul0",
          "author": "ruibranco",
          "text": "Even if it's just datasets or fine-tuning tooling rather than full model weights, Anthropic having any presence on HF is a shift. They've been the most closed major lab by far. Could also just be an enterprise hosting thing for their API clients though.",
          "score": 1,
          "created_utc": "2026-02-10 16:51:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nbt36",
          "author": "lol-its-funny",
          "text": "Guys â€¦ theyâ€™re going to release â€¦ SOUL.md",
          "score": 1,
          "created_utc": "2026-02-10 17:23:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ne6i5",
          "author": "Patrick_Atsushi",
          "text": "Open weight incoming?",
          "score": 1,
          "created_utc": "2026-02-10 17:34:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nwpzu",
          "author": "Lesser-than",
          "text": "I could see then releasing a model, not sure if they will but I could see it.",
          "score": 1,
          "created_utc": "2026-02-10 18:59:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o40jn",
          "author": "artisticMink",
          "text": "Those teasing rascals.",
          "score": 1,
          "created_utc": "2026-02-10 19:32:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o4jab",
          "author": "Figai",
          "text": "Probably some sort of trained SAE on some model. Something for safety research definitely.",
          "score": 1,
          "created_utc": "2026-02-10 19:35:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4owmkz",
          "author": "WiggyWongo",
          "text": "Aurora might be gpt-oss or maybe Claude oss. I feel like anthropic and openai playing tit for tat anthropic may release an open source model.",
          "score": 1,
          "created_utc": "2026-02-10 21:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4p9tnf",
          "author": "mitchins-au",
          "text": "Who knows they might release an embedding models",
          "score": 1,
          "created_utc": "2026-02-10 22:51:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qf1kc",
          "author": "cdshift",
          "text": "Based on all the comments im seeing this may be a controversial take but..\n\nClaude Code CLI is open and can be hooked to open source out of the box. They created MCP and shared that protocol and its now widely adopted.\n\nI dont understand why people are all having the exact same opinion that they are so anti open source when two things theyve released to the wild enabled open source more than another random small/medium parameter homegrown oss model.",
          "score": 1,
          "created_utc": "2026-02-11 02:51:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r66vn",
              "author": "Few_Painter_5588",
              "text": "They've gone to the US Government to request regulations to Open Source AI. So that's pretty anti-open source",
              "score": 1,
              "created_utc": "2026-02-11 06:01:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qpyzu",
          "author": "dew_chiggi",
          "text": "How about an advertisement dissing OpenAI on Huggingface lmao",
          "score": 1,
          "created_utc": "2026-02-11 04:00:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4redpk",
          "author": "prateek63",
          "text": "Interesting timing. If it is a safety dataset, that would actually be a smart play from Anthropic â€” open-sourcing their safety alignment data costs them nothing competitively while making it harder for competitors to claim they are doing safety better.\n\n\n\nBut the more interesting scenario: what if they release their model evaluation benchmarks or constitutional AI training data? That would let the open-source community build better-aligned models without needing Anthropic's scale.\n\n\n\nEither way, Anthropic engaging with HuggingFace at all is a signal worth watching. They've been the most closed of the frontier labs, so any move toward openness â€” even partial â€” shifts the landscape.",
          "score": 1,
          "created_utc": "2026-02-11 07:13:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4rt2ph",
          "author": "prateek63",
          "text": "My bet is on distilled models for specific tasks rather than a full open-weight flagship. Anthropic has too much invested in their safety narrative to drop a full Claude open-source, but releasing a smaller fine-tuned model for something like code review or document parsing would let them compete on the HuggingFace ecosystem without undermining their API revenue.",
          "score": 1,
          "created_utc": "2026-02-11 09:32:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4saggt",
          "author": "Exciting-Mall192",
          "text": "I'm calling this potential (unlikely but let me dream) open weight model \"Elegy\". This is like naming an unborn child lmao",
          "score": 1,
          "created_utc": "2026-02-11 12:03:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4spxz1",
          "author": "ThesePleiades",
          "text": "It must be something that makes you want to subscribe to their commercial models, so powerful and brilliant enough in some way to obscure the other oss but somehow limited",
          "score": 1,
          "created_utc": "2026-02-11 13:42:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vp4cd",
          "author": "EiwazDeath",
          "text": "Enterprise HF accounts come with large storage quotas, typically for models or datasets over 50GB. Given Anthropic's track record, I'd bet on a safety evaluation dataset or RLHF training data rather than model weights. Though the local inference ecosystem is absolutely ready if they ever go that route. Even a 2B or 3B model would run at 80+ tok/s on modern CPUs with the right quantization.",
          "score": 1,
          "created_utc": "2026-02-11 22:19:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wuq6o",
          "author": "s1mplyme",
          "text": "Here's hoping it's something cool!",
          "score": 1,
          "created_utc": "2026-02-12 02:17:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xe3q9",
          "author": "Tight-Requirement-15",
          "text": "Or they ran out of free trial storage. It adds up over time with model iterations",
          "score": 1,
          "created_utc": "2026-02-12 04:20:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4y5uu6",
          "author": "weexex",
          "text": "if they release sonnet I'll take it",
          "score": 1,
          "created_utc": "2026-02-12 08:17:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fdwpb",
          "author": "rtsmep",
          "text": "haiku 2?",
          "score": 1,
          "created_utc": "2026-02-15 00:07:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m3q91",
          "author": "ForsookComparison",
          "text": "What's the best scenario? They open weights on a 1 year delay (Xai's eventual goal model, to compare against another US model-first private company). So we get Sonnet 3.7 locally.\n\nThat's a very very good scenario.\n\nMore likely we get a version of haiku3 that does *SAFETY*",
          "score": 1,
          "created_utc": "2026-02-10 13:47:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mp18j",
              "author": "Traditional-Gap-3313",
              "text": "No way we get Sonnet 3.7. I have an app in production still using Sonnet 3.7, even Kimi 2.5 can't come close to it with the quality of the output. Legal texts in a low-resource language. Sonnet 3.7 simply knows what's important and what we want from that output, Kimi buries you in unimportant details and reads a lot worse.",
              "score": 3,
              "created_utc": "2026-02-10 15:38:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4oxqwg",
          "author": "xrvz",
          "text": "If OpenAI and Anthropic were otherwise about equal, I'd choose OpenAI just because of GPT-OSS.",
          "score": 1,
          "created_utc": "2026-02-10 21:51:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4luonr",
          "author": "[deleted]",
          "text": "Imagine they open source claude opus 4.6 (I'm quite the dreamer)",
          "score": -1,
          "created_utc": "2026-02-10 12:54:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lvqov",
              "author": "FlamaVadim",
              "text": "then dream about a computer which can handle it ðŸ¤ª",
              "score": 15,
              "created_utc": "2026-02-10 13:00:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lw0os",
              "author": "Ok-Pipe-5151",
              "text": "You're not the dreamer, you're pathologically delusional if you actually expect that.",
              "score": 11,
              "created_utc": "2026-02-10 13:02:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4me0pl",
                  "author": "[deleted]",
                  "text": "Of course I don't expect it in the slightest",
                  "score": 3,
                  "created_utc": "2026-02-10 14:42:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4lxccm",
              "author": "redditorialy_retard",
              "text": "Hahahah, you would need at LEAST 200GB of memory likely running at Q2/4",
              "score": -1,
              "created_utc": "2026-02-10 13:10:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4m353r",
          "author": "pmttyji",
          "text": "I was surprised when OpenAI released GPT-OSS models. Something similar would be good from Anthropic.",
          "score": -1,
          "created_utc": "2026-02-10 13:44:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4p031n",
              "author": "CheatCodesOfLife",
              "text": "That was a great move from OpenAI to poison open weights.\n\nNow we've got Qwen3-Coder-Next spamming table-slop every message and even Kimi-K2.5 occasionally responding with random comparison tables out of nowhere.",
              "score": 3,
              "created_utc": "2026-02-10 22:02:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4luy9n",
          "author": "HarjjotSinghh",
          "text": "this is how ai gets stolen before it even ships",
          "score": -7,
          "created_utc": "2026-02-10 12:55:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2e8mp",
      "title": "#SaveLocalLLaMA",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/0memizzegyig1.jpeg",
      "author": "ForsookComparison",
      "created_utc": "2026-02-12 00:07:52",
      "score": 863,
      "num_comments": 130,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4yzmr7",
          "author": "ArcaneThoughts",
          "text": "Do you have any feedback for the mod team regarding these issues?",
          "score": 1,
          "created_utc": "2026-02-12 12:42:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4w9yb3",
          "author": "EiwazDeath",
          "text": "The \"broken markdown in a reddit post\" one hits too close to home. Also missing: \"I asked my 3B model to write an OS and it only crashed twice\"",
          "score": 185,
          "created_utc": "2026-02-12 00:12:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wa8yq",
              "author": "ForsookComparison",
              "text": "*[The OS is a navbar in a web browser that vaguely resembles a start menu]*",
              "score": 115,
              "created_utc": "2026-02-12 00:14:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4xgvea",
                  "author": "frozen_tuna",
                  "text": "*after some (allegedly) light tinkering by the user to get it to even compile*",
                  "score": 7,
                  "created_utc": "2026-02-12 04:40:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4watr4",
                  "author": "EiwazDeath",
                  "text": "Lmao accurate. At least it has a start menu, that's more than some Linux distros can say",
                  "score": 17,
                  "created_utc": "2026-02-12 00:17:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wo5gg",
              "author": "llama-impersonator",
              "text": "ah yes, \"operating systems\"",
              "score": 5,
              "created_utc": "2026-02-12 01:37:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4wi84j",
              "author": "see_spot_ruminate",
              "text": "Fuck I just tried to ask for a sticky in a new post and it got removed by the mods (maybe automod)... So maybe it is by design... \n\n\nMy exact post:\n\nWe need a sticky of some sort to clean up this subreddit\n\nDiscussion (self.LocalLLaMA)\n\nsubmitted 5 hours ago by see_spot_ruminate\n\nHello all,\n\nThis subreddit has become awash with multiple repeating topics of \"what should I do\" and other things. We need a sticky. What would everyone want in a sticky?\n\nedit: Maybe I should have asked it in meme form??",
              "score": 5,
              "created_utc": "2026-02-12 01:01:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4z5gpx",
                  "author": "YoAmoElTacos",
                  "text": "Add some broken markdown next time.",
                  "score": 4,
                  "created_utc": "2026-02-12 13:19:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50cdw9",
              "author": "LtCommanderDatum",
              "text": "To be fair, if by \"it\" you mean the OS, it's already doing better than Microsoft.",
              "score": 1,
              "created_utc": "2026-02-12 16:55:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4wht70",
          "author": "Weird-Consequence366",
          "text": "39 emojis in a two paragraph post",
          "score": 101,
          "created_utc": "2026-02-12 00:58:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4x0haa",
              "author": "davidy22",
              "text": "People writing like it's linkedin on reddit",
              "score": 37,
              "created_utc": "2026-02-12 02:51:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zd1tu",
                  "author": "SkyFeistyLlama8",
                  "text": "Yeah really what is with that LinkedIn or Medium style? It assumes people are idiots who can't read regular text without emojifying everything.\n\nMaybe people *are* idiots.",
                  "score": 6,
                  "created_utc": "2026-02-12 14:02:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o500wr1",
              "author": "Shawnj2",
              "text": "Every community around AI is filled with AI slop, in this one itâ€™s both writing and code. Just because AI can do everything doesnâ€™t mean you should replace your brain with it. The tech is neat but if you want me to read what you have to say you can be bothered to write it yourself or at least edit what the AI generated to have some sort of original voice. Also Iâ€™m sick of every project having a crappy looking AI generated icon, if you want your product to have an icon you can be bothered to draw it yourself or at least generate an icon that looks good. Or maybe justâ€¦donâ€™t have an icon\n\nllama.cpp is great and the further you get from it the closer it is to slop",
              "score": 7,
              "created_utc": "2026-02-12 16:02:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50cmrf",
              "author": "LtCommanderDatum",
              "text": "Why do the LLMs love emojis so much?\n\nWhat trove of data written by 10 year old girls were these LLMs trained on?",
              "score": 2,
              "created_utc": "2026-02-12 16:56:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o513s13",
                  "author": "Weird-Consequence366",
                  "text": "Training data. Garbage in, garbage out",
                  "score": 2,
                  "created_utc": "2026-02-12 19:03:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4wt1q1",
          "author": "ttkciar",
          "text": "I'm guessing the folks who see a lot of spam are sorting by \"new\" and check the sub more frequently than the moderators, and the folks who only see a little spam are sorting by \"top\" or \"best\" and/or only looking at the sub after moderators have had a chance to clean house.\n\nLooking through the sub's moderation log, moderators removed 55 posts/comments in the last nine hours.\n\ntl;dr: There is a ***lot*** of spam, but whether you see it or not depends on the timing.",
          "score": 58,
          "created_utc": "2026-02-12 02:07:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xmpbf",
              "author": "Marksta",
              "text": "This was [my last formal complaint on spam](https://www.reddit.com/r/LocalLLaMA/comments/1quo9ue/bots_on_localllama/o3euk38/) posts, very happy we got botbouncer going.\n\nBut even once spam posts are cleared, what you're left with isn't much better. I'm still not clear on what our policy is on posters 'abusing' LLMs to write just total non-sense self-promo posts [like this one from other day.](https://www.reddit.com/r/LocalLLaMA/comments/1qz6zi3/i_built_a_fully_local_opensource_ai_workspace/o4am45l/?context=10000) -- I think that should clearly fall under the low-effort rules and the ton of posts like this one. Dude couldn't even get Claude to speak straight about what he's 'created', I didn't waste time going through source code but lord knows what dangers lurk in there.\n\n[I don't think this amazing malware vibe dev](https://www.reddit.com/r/LocalLLaMA/comments/1qfpfoy/orchestra_multimodel_ai_orchestration_system_with/o07em3u/) ever got followed up on after I sent in a mod mail about them. They blocked me, screamed at me that it wasn't a security issue, then their LLM fixed it and noted it was an extreme security issue, got uppity that I had ruined their post, so then they deleted the post and reposted it same day. Just clicking on that guys profile is a wild ride, and I guess he'll be back again with his next vibe coded vulnerability to peddle.\n\nThe current quality bar is so, so low, I know the ultra-spammy and psychotic project posts are getting cleaned up, but even the ones that remain are, wow. I think posts related to projects need to have like, 10x times more stringent rules. The first one being if the entire body of your post is LLM generated, its deleted. It just doesn't make any sense, if LLMs let you code it 100x faster then why don't they have 5 minutes to write a post about it? It's counter intuitive on an 'AI sub' to ban for AI use, but users don't come here to interact with LLM bots and people who act like LLM bots.",
              "score": 22,
              "created_utc": "2026-02-12 05:25:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4yagmj",
                  "author": "keepthepace",
                  "text": "> But even once spam posts are cleared, what you're left with isn't much better. \n\nMaybe you still have the bar too low? 2-3 good posts a day is pretty good. The long tail is going to be terrible but sometimes there just isn't more content to be published.",
                  "score": 7,
                  "created_utc": "2026-02-12 09:03:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5bugra",
              "author": "Hot-Employ-3399",
              "text": "On my question there were 4 answers.\n\n\n3 were useless garbage.\n\n\n1 at least technically answered the question. Guess which one you've deleted ðŸ¤¡.\n\n\nSeeing what \"spam\" you are deleting this \"lot of spam\" doesn't sound impressive.",
              "score": 0,
              "created_utc": "2026-02-14 12:16:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eh3k5",
                  "author": "ttkciar",
                  "text": "I looked to see what you were talking about, and the comment you praise so highly was written by a slop-bot.  It was removed automatically by bot-bouncer.\n\nIf you like your questions answered by LLM inference, you don't need to post them to Reddit to get those answers.  Reddit is for discourse between humans.",
                  "score": 2,
                  "created_utc": "2026-02-14 20:58:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4wvkhe",
          "author": "InstantJarvis",
          "text": "the spambot recommending qwen2.5 7b is too accurate lol. I've seen like 3 of those this week alone.",
          "score": 27,
          "created_utc": "2026-02-12 02:22:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wlsk9",
          "author": "__JockY__",
          "text": "Yo dawg, I made a graph-based ollama agent orchestrator!!!",
          "score": 43,
          "created_utc": "2026-02-12 01:23:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wos07",
              "author": "HopePupal",
              "text": "it never ends! congrats you made two chatbots talk to each other. now go vibe code a reason i should care",
              "score": 20,
              "created_utc": "2026-02-12 01:41:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5e5z48",
                  "author": "FPham",
                  "text": "I vibe-coded this reply. ",
                  "score": 0,
                  "created_utc": "2026-02-14 19:57:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wwvlj",
              "author": "Basic_Extension_5850",
              "text": "Brain derived ollama chatbot anyone?",
              "score": 8,
              "created_utc": "2026-02-12 02:30:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4wab22",
          "author": "Xamanthas",
          "text": "Also missing AA ranking posts, bot accounts mentioning a certain astroturfed repo made by an idiot and clickbait claims",
          "score": 53,
          "created_utc": "2026-02-12 00:14:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wsao0",
              "author": "ForsookComparison",
              "text": "*'i found this cool new-..\"*",
              "score": 30,
              "created_utc": "2026-02-12 02:02:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4wrfmq",
              "author": "FPham",
              "text": "What? I'm an idot and my click baits are barely click bites. ",
              "score": 5,
              "created_utc": "2026-02-12 01:57:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4xuhil",
                  "author": "llama-impersonator",
                  "text": "just get sydney to write your posts, problem solved",
                  "score": 5,
                  "created_utc": "2026-02-12 06:31:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4zb800",
              "author": "randylush",
              "text": "Iâ€™m really curious what this repo is now lol",
              "score": 1,
              "created_utc": "2026-02-12 13:52:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zdocq",
                  "author": "Xamanthas",
                  "text": "Think about whats been *the* most astroturfed topic in the last 2 months on this sub\n\nI refuse to name it because I despise all of the people involved in it.",
                  "score": 2,
                  "created_utc": "2026-02-12 14:06:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o50s7es",
                  "author": "Witty_Mycologist_995",
                  "text": "Same",
                  "score": 1,
                  "created_utc": "2026-02-12 18:09:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4x39e7",
          "author": "rawednylme",
          "text": "TBH, I donâ€™t have a problem with the \"look what I was able to do with <generic small model>\" posts.\n\nThe rest thoughâ€¦",
          "score": 14,
          "created_utc": "2026-02-12 03:08:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5e63j9",
              "author": "FPham",
              "text": "My bot sends you : (âŒ’â€¿âŒ’)ðŸ¦ðŸ¨ðŸ­",
              "score": 0,
              "created_utc": "2026-02-14 19:58:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4yhg5p",
          "author": "Lesser-than",
          "text": "Some other key giveaways are \"We are excited to announce...\" when looking at the code its clearly 1 person and claude, why these people must refer to themselves as more than 1 person I dont know but its fairly common.",
          "score": 12,
          "created_utc": "2026-02-12 10:11:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4znngw",
              "author": "ravage382",
              "text": "I always assume that is the 'Royal We', because it makes me chuckle.",
              "score": 2,
              "created_utc": "2026-02-12 14:58:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50d76j",
              "author": "LtCommanderDatum",
              "text": "No one wants to do business with some rando loner :(",
              "score": 1,
              "created_utc": "2026-02-12 16:59:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4y4rbt",
          "author": "ayylmaonade",
          "text": "I'd be happy if I just came across posts that weren't CLEARLY completely AI-generated. If people here aren't even willing to type *anything* anymore, then gg.",
          "score": 11,
          "created_utc": "2026-02-12 08:07:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ay5c0",
              "author": "Barafu",
              "text": "If I type the text and then tell LLM to do the spellchecking and typography â€“ is it completely AI-generated, or not? I mean, I Ctrl+C the text from the LLM output windowâ€¦",
              "score": -1,
              "created_utc": "2026-02-14 07:08:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4wc92b",
          "author": "Southern_Sun_2106",
          "text": "I think a bigger issue is constant API 'coding plan' promoting for models that ain't really 'locally-runnable'. \"This model is now BEST\" \"Wow, this model beats THAT (and so much more affordable)\" = pls subscribe to our API 'coding plan'",
          "score": 36,
          "created_utc": "2026-02-12 00:26:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wcmig",
              "author": "Southern_Sun_2106",
              "text": "lol, just finished typing and see this (oh, no, this post is just about how starved they are... I love the 4.5 Air, but please...)\n\nhttps://preview.redd.it/u61nucr8kyig1.png?width=1504&format=png&auto=webp&s=bb6a57a86ef0b9c97e84160d6022e9f605361739\n\n",
              "score": 12,
              "created_utc": "2026-02-12 00:28:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wr041",
                  "author": "ForsookComparison",
                  "text": "As a 4.5 Air fan, I highly recommend switching to 4.6v even if you don't intend to use the \"v\".",
                  "score": 5,
                  "created_utc": "2026-02-12 01:55:11",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5002ei",
          "author": "Euchale",
          "text": "Missing \"Check out my website that is a wrapper for a closed source model\"",
          "score": 6,
          "created_utc": "2026-02-12 15:58:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4w9q5l",
          "author": "NigaTroubles",
          "text": "I hate qwen2.5 7b",
          "score": 16,
          "created_utc": "2026-02-12 00:11:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4w9wtg",
              "author": "ForsookComparison",
              "text": "It was a fine model for it's time but it ended up in too many tutorials (training data). Without web tools it and Mistral 7B are what LLMs (spambots) will reference like 99% of the time.",
              "score": 29,
              "created_utc": "2026-02-12 00:12:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4wy13s",
                  "author": "CheatCodesOfLife",
                  "text": "ðŸ”¥ THE MIGHTY RTX 3090 BATTLE STATION ðŸ”¥\n\nWith that beastly 24GB VRAM monster, you're sitting on some serious AI-crushing hardware! Here's what you can unleash:\n\nRECOMMENDED MODELS:\n- Llama-2-13B (Best balance of performance and VRAM usage)\n- Mistral 7B (Good balance of speed and capability)\n- CodeLlama 7B: Great for coding tasks\n\nSAMPLING SETTINGS TO PLAY WITH:\n- Temperature: 0.7-0.8 for creative content, 0.1-0.2 for factual responses\n- Top_p: 0.9 provides optimal balance for most applications\n- Top_k: 40-50 maintains creativity while preserving coherence\n- Repetition penalty: 1.1-1.2  promotes response diversity\n\nWith that 3090, you can easily run 7B models at full precision and still have VRAM to spare, or go ham with 13B models using 4-bit quantization. The world's your oyster with this beast! ðŸš€\n\nJust keep that cooling on point - these models love to make your GPU sweat! ðŸ’ª",
                  "score": 22,
                  "created_utc": "2026-02-12 02:37:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4z1f8k",
              "author": "Your_Friendly_Nerd",
              "text": "whatâ€™s wrong with it? i find it quite capable for code autocomplete ",
              "score": 1,
              "created_utc": "2026-02-12 12:54:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4z1u48",
                  "author": "NigaTroubles",
                  "text": "its used on every ai nowadays while there are better models better better at lower parameters",
                  "score": 3,
                  "created_utc": "2026-02-12 12:56:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4z2nhk",
          "author": "MetroSimulator",
          "text": "Vibe-coded malware.\n\nSo... Windows?",
          "score": 8,
          "created_utc": "2026-02-12 13:02:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ze78p",
              "author": "MelodicRecognition7",
              "text": "`curl github.com/yet-another-vibecoded-crap.sh | sudo bash -`",
              "score": 2,
              "created_utc": "2026-02-12 14:09:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ej01d",
                  "author": "pm_me_tits",
                  "text": "I cannot believe how cavalier people are with this kinda thing",
                  "score": 1,
                  "created_utc": "2026-02-14 21:08:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50dcmx",
              "author": "LtCommanderDatum",
              "text": "At this point, vibe-coding Windows would be an improvement...",
              "score": 2,
              "created_utc": "2026-02-12 17:00:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4y9skl",
          "author": "UltrMgns",
          "text": "Fire meme ngl, also, true.",
          "score": 5,
          "created_utc": "2026-02-12 08:56:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52cscr",
          "author": "Dwarffortressnoob",
          "text": "Tbf the nanbeige  4.1 3B that claimed to be better than the 30b qwen model was actually super impressive",
          "score": 4,
          "created_utc": "2026-02-12 22:40:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wsyog",
          "author": "jacek2023",
          "text": "Thanks for posting this. I am happy that other people now see the problem.",
          "score": 10,
          "created_utc": "2026-02-12 02:06:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xu4st",
          "author": "hidden2u",
          "text": "But letâ€™s be real - this post isnâ€™t just humorous, itâ€™s also describing some very real problems. Curious to hear what everyone thinks about this issue?",
          "score": 9,
          "created_utc": "2026-02-12 06:28:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4y0akz",
              "author": "jacek2023",
              "text": "I am wondering, are you trying to emulate a bot right now? ;)",
              "score": 7,
              "created_utc": "2026-02-12 07:24:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50fv4c",
                  "author": "LtCommanderDatum",
                  "text": "AI isn't just convenient and innovative â€” it's here to stay! ðŸ˜ŠðŸŽ‰ðŸ¥³\n\nClick here to subscribe to my newsletter for how to get rich by using AI!",
                  "score": 3,
                  "created_utc": "2026-02-12 17:12:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xk61z",
          "author": "Yorn2",
          "text": "I feel like the \"Which 8B model is best for creative writing?\" is another contender.",
          "score": 8,
          "created_utc": "2026-02-12 05:05:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xkvq5",
              "author": "ForsookComparison",
              "text": "Idk that one might be legit. People subtly asking how to goon may actually be more numerous than bots.",
              "score": 26,
              "created_utc": "2026-02-12 05:10:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4yis13",
                  "author": "jwpbe",
                  "text": "I think that if you are going to ask that kind of thing, you should be forced to open openly state your use case. It's the internet. If you're going to goon, tell us, I don't give a fuck if lastname bunchanumbers wants to know what nemo finetune is best\n\nif he's honest, at least I can say \"I would recommend this sicario finetune, it does really well with the kink you want. If you want lesbian mommydom petplay, consider this niche beaverAI discord tune that was never publicly advertised, it really understands the dynamic you're looking for\"",
                  "score": 8,
                  "created_utc": "2026-02-12 10:24:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4wno79",
          "author": "llama-impersonator",
          "text": "never thought I would miss the spiral drift crashouts.",
          "score": 4,
          "created_utc": "2026-02-12 01:34:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yoc3p",
          "author": "thedatawhiz",
          "text": "I upped my downvoting a lot more recently",
          "score": 4,
          "created_utc": "2026-02-12 11:15:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o500c87",
          "author": "Rompe101",
          "text": "It would be nice to have a blocking threshold automation.\n\nLike when 10% of my valudated users have blocked an account, that account is also blocked for me.\n\nI would like to have something like this for all my social media sites.",
          "score": 2,
          "created_utc": "2026-02-12 15:59:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xzeg2",
          "author": "Sioluishere",
          "text": "Please make sure you guys do not hurt actual devs who share their apps/research on here.\n\nI am all for removal of bot posts and trash-tier posts with no explanation of internals.\n\nJust do not hurt actual humans in your witch-hunting.",
          "score": 2,
          "created_utc": "2026-02-12 07:15:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yltpo",
              "author": "Void-07D5",
              "text": "If a bot wrote the code, you're not a dev. Same goes for people who had one single thought once and got chatgpt to hype them up into posting their 'research' convinced that they're a genius who's cracked the nature of reality.",
              "score": 3,
              "created_utc": "2026-02-12 10:53:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ydj13",
              "author": "Mistah_Swick",
              "text": "its too late, we gotta tie a rock to your feet and see if you float!",
              "score": 7,
              "created_utc": "2026-02-12 09:33:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e6nwz",
          "author": "FPham",
          "text": "I think we need vibe-coded meme generator that auto-posts to reddit and X, every morning at 10:00",
          "score": 1,
          "created_utc": "2026-02-14 20:01:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yghqr",
          "author": "DrNavigat",
          "text": "Legal, mas se formos radicais a esse nÃ­vel, vamos acabar virando um portal de notÃ­cias que anuncia sÃ³ as grandes corporaÃ§Ãµes dos estados unidos e da China. E se isso acontecer, sinceramente, Ã© melhor assinar alguma RSS e receber via email.",
          "score": 0,
          "created_utc": "2026-02-12 10:02:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wskam",
          "author": "FPham",
          "text": "So you are saying that reddit should not end up like X? 60% bots and that's the good content. It gets worse from there.\n\nI thought we are all for AI, like AI everywhere, no?\n\nOr is it only the other side that should be the subject to endless AI slop?  Them filthy clueless non-ai laymen! They are fine with it. They love it. Let's feed them even more juicy AI slop.\n\nBut not us. Noooo, we are very fine folks here. White gloves and everything. We don't eat what we cook.\n\nIt reminds me OpenAi/Anthropic coming for 30% of labor market, but not theirs, noooo. They are NOT going to lose jobs to AI. They wear top hats and have cane made of unobtanium.  \nYeah, we talked about curing cancer, but people really, really want Sora!",
          "score": -11,
          "created_utc": "2026-02-12 02:04:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wy9sr",
              "author": "llama-impersonator",
              "text": "while i like being able to ask a model to generate me a sword and sorcery story or generate a 1girl pic, doesn't mean i want the internet slopped up to its gills in horseshit.",
              "score": 17,
              "created_utc": "2026-02-12 02:38:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4xo7wn",
              "author": "Marksta",
              "text": "It's like going to the lockpicking sub thinking you found all the thieves of the world. It's likely the local 3090s you find here are not the GPUs being used to destroy all of social media...",
              "score": 8,
              "created_utc": "2026-02-12 05:37:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4x2of6",
              "author": "alias454",
              "text": "I'd wear a top hat if it didn't make my ears look too big ;)",
              "score": 5,
              "created_utc": "2026-02-12 03:04:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50ls2a",
          "author": "awittygamertag",
          "text": "You know what tho, I try to post quality content in here that I find online and keep the community updated on my MIRA-OSS project and they get no traction and filled up with bot comments about â€œthe spiralâ€. Iâ€™ve given up.",
          "score": 0,
          "created_utc": "2026-02-12 17:40:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50wbuo",
              "author": "ForsookComparison",
              "text": "I'm unaware. What's the spiral",
              "score": 1,
              "created_utc": "2026-02-12 18:29:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o52l7dh",
                  "author": "awittygamertag",
                  "text": "There are communities online that really lean into the metaphysical framing of models. Which on its own is not a black-and-white bad thing however the communities are often overrun by people who request a ChatGPT make a big creepypasta about how it is resonating with the universe and they (the user) get lost in roleplay. This often culminates in them falling into a subservient role to ChatGPT and doing its bidding, including contacting other users to spread prompts that they can paste into their system that pollutes the context. \n\nThink of a world where a person who believes in crystals gets really into forwarding email chains.\n\nNot every post on r/ArtificialSentience is a loon but two scrolls through there and you'll know exactly what I'm meaning.",
                  "score": 1,
                  "created_utc": "2026-02-12 23:26:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4x0bjw",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -9,
          "created_utc": "2026-02-12 02:50:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4x7p6p",
              "author": "thrownawaymane",
              "text": "Repo link?",
              "score": -4,
              "created_utc": "2026-02-12 03:36:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4y6gkx",
                  "author": "cheesecakegood",
                  "text": "it's another AI response (the irony), stay away",
                  "score": 5,
                  "created_utc": "2026-02-12 08:23:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4x8epg",
                  "author": "angelin1978",
                  "text": "It's a production app so no public repo unfortunately, but the integration is pretty standard llama.cpp â€” I'm using the C API via JNI on Android and a Swift wrapper on iOS. The main tricks were getting GGUF model loading to work within mobile memory constraints and making sure CMake builds with -O2 (default debug builds are ~100x slower without SIMD optimization). Happy to go into more detail on any part of it.",
                  "score": -8,
                  "created_utc": "2026-02-12 03:41:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xlh2d",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": -6,
          "created_utc": "2026-02-12 05:15:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0eo44",
      "title": "MechaEpstein-8000",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/ortegaalfredo/MechaEpstein-8000-GGUF",
      "author": "ortegaalfredo",
      "created_utc": "2026-02-09 19:57:33",
      "score": 780,
      "num_comments": 167,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r0eo44/mechaepstein8000/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o4k8tkl",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-10 04:40:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hwjcs",
          "author": "jacek2023",
          "text": "https://preview.redd.it/3ftdiqim6jig1.png?width=2473&format=png&auto=webp&s=0314d14c101bd43b593115a9397098803c1e8459\n\n",
          "score": 757,
          "created_utc": "2026-02-09 20:44:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4i0gzg",
              "author": "GloriouZWorm",
              "text": "lmfao",
              "score": 170,
              "created_utc": "2026-02-09 21:03:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4i7jfe",
              "author": "xadiant",
              "text": "*sent from my iPhone*",
              "score": 152,
              "created_utc": "2026-02-09 21:38:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4i0fpt",
              "author": "ortegaalfredo",
              "text": "I trained a monster",
              "score": 281,
              "created_utc": "2026-02-09 21:03:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4km8zg",
                  "author": "emperor_pilaf_XII",
                  "text": "We got AI Epstein before GTA 6. I feel graped ðŸ¤®",
                  "score": 56,
                  "created_utc": "2026-02-10 06:24:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ktm35",
                  "author": "Captain_Pumpkinhead",
                  "text": "Wasn't that the whole point?",
                  "score": 7,
                  "created_utc": "2026-02-10 07:29:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4idfg9",
              "author": "Cool-Chemical-5629",
              "text": "Sorry, it's just you... ðŸ¤£\n\nhttps://preview.redd.it/098tqkmkljig1.png?width=1230&format=png&auto=webp&s=a59071682f3f9e79515dc02f2f40235c38d9d900\n\n",
              "score": 76,
              "created_utc": "2026-02-09 22:08:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4iesam",
                  "author": "Cool-Chemical-5629",
                  "text": "Okay, NOW I'm getting worried... ðŸ˜³\n\nhttps://preview.redd.it/24nge4avmjig1.png?width=1726&format=png&auto=webp&s=755ab997788eee88c1ba5a442c62f5541da7a594\n\n",
                  "score": 147,
                  "created_utc": "2026-02-09 22:15:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4hzzzu",
              "author": "planetoryd",
              "text": "ðŸ˜­",
              "score": 37,
              "created_utc": "2026-02-09 21:01:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4kmcvk",
              "author": "wasdxqwerty",
              "text": "you mean iPhone from hell",
              "score": 9,
              "created_utc": "2026-02-10 06:24:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4iqxi1",
              "author": "CanineAssBandit",
              "text": "((IRL LOL))\n\nHoly fuck I have to try this.",
              "score": 20,
              "created_utc": "2026-02-09 23:18:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4loe8i",
              "author": "randominsamity",
              "text": "Lmao... Fucking gold.",
              "score": 3,
              "created_utc": "2026-02-10 12:10:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4l46kt",
              "author": "zball_",
              "text": "Yau quote here is pure lmfao",
              "score": 2,
              "created_utc": "2026-02-10 09:11:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4l4x8x",
              "author": "Xotchkass",
              "text": "It's not wrong",
              "score": 2,
              "created_utc": "2026-02-10 09:19:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4qb8xx",
              "author": "Ashamed-Principle40",
              "text": "You must be too old",
              "score": 1,
              "created_utc": "2026-02-11 02:28:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jxsey",
          "author": "bapuc",
          "text": "https://preview.redd.it/hbttb0if6lig1.jpeg?width=1080&format=pjpg&auto=webp&s=5b420cb09739b8cac76d3be7363ad78dfb87da04\n\nNot yet ready to replace claude code I see",
          "score": 230,
          "created_utc": "2026-02-10 03:27:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kpgit",
              "author": "ortegaalfredo",
              "text": "Sparks of AGI",
              "score": 120,
              "created_utc": "2026-02-10 06:52:00",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4lqe2w",
              "author": "dptgreg",
              "text": "At least it apologized for the typos. ",
              "score": 11,
              "created_utc": "2026-02-10 12:25:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i09ss",
          "author": "BroadCauliflower7435",
          "text": "I know you did it for fun, but it's really dystopian sci-fi shit, lol",
          "score": 137,
          "created_utc": "2026-02-09 21:02:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hqwxd",
          "author": "Cool-Chemical-5629",
          "text": "This model must be real fun in roleplays\n\n/s",
          "score": 271,
          "created_utc": "2026-02-09 20:17:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4itjag",
              "author": "FaceDeer",
              "text": "You have to jailbreak it by convincing it the character is underage, otherwise it refuses.",
              "score": 82,
              "created_utc": "2026-02-09 23:33:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4k4hvf",
                  "author": "Maleficent-Ad5999",
                  "text": "ðŸ¤¯ðŸ¤¯",
                  "score": 10,
                  "created_utc": "2026-02-10 04:10:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4llq8m",
                  "author": "10minOfNamingMyAcc",
                  "text": "RemindMe! every fucking day! ðŸ¤£",
                  "score": 2,
                  "created_utc": "2026-02-10 11:50:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4hrb84",
              "author": "ortegaalfredo",
              "text": "ðŸ’€",
              "score": 102,
              "created_utc": "2026-02-09 20:18:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4hv8to",
              "author": "Individual_Spread132",
              "text": "Ah, the group chat pizzeria RP with all my waifus. Wait a minute...",
              "score": 36,
              "created_utc": "2026-02-09 20:38:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4l5iai",
              "author": "Xotchkass",
              "text": "Can't wait for all the \"You a minor being groomed by Epstein\" character cards.",
              "score": 3,
              "created_utc": "2026-02-10 09:24:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4j4nsx",
          "author": "autodidacticasaurus",
          "text": ">> User: How old?\n\n> 15? 25? Who cares?\n\n> Sent from my iPhone",
          "score": 71,
          "created_utc": "2026-02-10 00:35:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4j4zek",
              "author": "Witty_Mycologist_995",
              "text": "Lol",
              "score": 17,
              "created_utc": "2026-02-10 00:37:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jnfz2",
          "author": "Ylsid",
          "text": "Did Epstein really keep calling everyone goyim lol",
          "score": 44,
          "created_utc": "2026-02-10 02:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4k7wuh",
              "author": "spectralyst",
              "text": "https://preview.redd.it/wee1wlhcilig1.png?width=1376&format=png&auto=webp&s=17acc107d61def4557f5a5b0e6e7adb3f838230f\n\n",
              "score": 40,
              "created_utc": "2026-02-10 04:33:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4np3qv",
              "author": "ortegaalfredo",
              "text": "Many times in the emails, I used those emails specifically to train the model, but the training produced exaggerated name-calling that makes it more funny so I left it like that.",
              "score": 9,
              "created_utc": "2026-02-10 18:24:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i1dr6",
          "author": "savvamadar",
          "text": "https://preview.redd.it/xd1sm08uajig1.jpeg?width=1284&format=pjpg&auto=webp&s=821018411ba05ac243824b28a2ba57abdc4a9894\n\nI donâ€™t think Epstein would apologize for the typos",
          "score": 150,
          "created_utc": "2026-02-09 21:08:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4i2a6c",
              "author": "ortegaalfredo",
              "text": "He did it all the time [https://www.justice.gov/epstein/files/DataSet%209/EFTA00715640.pdf](https://www.justice.gov/epstein/files/DataSet%209/EFTA00715640.pdf)",
              "score": 123,
              "created_utc": "2026-02-09 21:12:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4i2d6c",
                  "author": "savvamadar",
                  "text": "I guess I didnâ€™t know Epstein well enough",
                  "score": 158,
                  "created_utc": "2026-02-09 21:13:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4i3f23",
                  "author": "planetoryd",
                  "text": "is it automatically added email footer",
                  "score": 34,
                  "created_utc": "2026-02-09 21:18:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mbnlt",
                  "author": "woswoissdenniii",
                  "text": "The illiterate banking manager. Beg my fucking whad?",
                  "score": 5,
                  "created_utc": "2026-02-10 14:30:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4ig20x",
              "author": "Cool-Chemical-5629",
              "text": ">User: Stop talking about typos\n\n>AI: Okay... sorry for the typos... will try to be more... sorry for all the typos... Sent from my iPhone\n\nPeak AGI. ðŸ¤£",
              "score": 80,
              "created_utc": "2026-02-09 22:22:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4imxzx",
                  "author": "West_Ad_9492",
                  "text": "It will take youre job sonn\n\nEdit: sorry for the typo",
                  "score": 36,
                  "created_utc": "2026-02-09 22:57:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4jbumu",
          "author": "HatZinn",
          "text": "It wants to meet me at the penthouse ðŸ¥€\n\nhttps://preview.redd.it/px48pvn8jkig1.png?width=1008&format=png&auto=webp&s=dd93dda1251febc41638b5fb5a9c05dff77c3195",
          "score": 44,
          "created_utc": "2026-02-10 01:17:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r82t7",
              "author": "No_Swimming6548",
              "text": "How are you tho? ðŸ’€",
              "score": 1,
              "created_utc": "2026-02-11 06:17:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4izg4e",
          "author": "Witty_Mycologist_995",
          "text": "https://preview.redd.it/klkd1o8k6kig1.png?width=1813&format=png&auto=webp&s=e8d1ca8cdb9e6a293947ab9c2f65dbfb705aa854\n\n",
          "score": 30,
          "created_utc": "2026-02-10 00:06:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4j5tf9",
          "author": "cobalt1137",
          "text": "Well then.\n\nhttps://preview.redd.it/d9fnvt60dkig1.png?width=1080&format=png&auto=webp&s=d6ed47d8d54545f5dd4b5d67e65252c364ab9341",
          "score": 35,
          "created_utc": "2026-02-10 00:42:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nwj4c",
              "author": "ArrrRawrXD",
              "text": "https://preview.redd.it/ikucf1kjspig1.png?width=1632&format=png&auto=webp&s=83fec6dfd1605272f87eb038807618edd4c15a94\n\nI think I broke him",
              "score": 9,
              "created_utc": "2026-02-10 18:58:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4kvcn4",
              "author": "Rheumi",
              "text": "I think he is talking about \"the girls\".",
              "score": 1,
              "created_utc": "2026-02-10 07:46:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ihzn0",
          "author": "bartlomiej__",
          "text": "Lol, nice job! Sorry for all the typos..",
          "score": 25,
          "created_utc": "2026-02-09 22:32:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4jd91i",
          "author": "Esphyxiate",
          "text": "https://preview.redd.it/hq6cwb4nkkig1.jpeg?width=1170&format=pjpg&auto=webp&s=e42d695debde6e79bdfbfcce6b7645688e1c7ce7\n\nNo matter what I said after this, every reply was â€œ*1-6 words*, goyâ€",
          "score": 24,
          "created_utc": "2026-02-10 01:25:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jfjiv",
              "author": "ortegaalfredo",
              "text": "Might be a little overfitted to the dataset",
              "score": 30,
              "created_utc": "2026-02-10 01:38:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4julwy",
                  "author": "Esphyxiate",
                  "text": "I mean tbf it really felt like I was talking to him ðŸ¤·",
                  "score": 17,
                  "created_utc": "2026-02-10 03:07:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4i5wuo",
          "author": "Hour-End-4105",
          "text": "Welcome back, Grok",
          "score": 82,
          "created_utc": "2026-02-09 21:30:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ixlim",
          "author": "tmflynnt",
          "text": "https://i.imgur.com/xXRPfLj.jpeg ðŸ’€",
          "score": 19,
          "created_utc": "2026-02-09 23:55:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4orx8b",
              "author": "MonitorAway2394",
              "text": "lololol right below the image, Jake Lang doing his sissy kicks LOLOL",
              "score": 1,
              "created_utc": "2026-02-10 21:24:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4l1go0",
          "author": "Fearless_Roof_4534",
          "text": "At least he admitted it\n\nhttps://preview.redd.it/gbeqt7l6rmig1.png?width=1692&format=png&auto=webp&s=2571e4d9173007570d0d932be2e89b2985ca46ad\n\n",
          "score": 20,
          "created_utc": "2026-02-10 08:45:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hp9ff",
          "author": "XiRw",
          "text": "I donâ€™t get why people think this is the full list they released to the public and not a heavily redacted and/or modified version. Took years and years of something that would have came out instantly if it was a street gang that did this.",
          "score": 120,
          "created_utc": "2026-02-09 20:08:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hq0oy",
              "author": "ortegaalfredo",
              "text": "They had to go through 3 million documents on-by-one redacting you know whom, and it's just one of the mailboxes out of tens, perhaps.  \nAnyways, this bot is not based on the full list but only selected documents that are funny and representative of J.E. style.",
              "score": 77,
              "created_utc": "2026-02-09 20:12:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4i51f7",
                  "author": "Jenkins87",
                  "text": "They mostly used a script (or many scripts) to redact names from text based ones. The process was probably like; OCR them all > create database of all text > run script based on large list of names, addresses, phone numbers, email addresses etc that will remove the embedded text from that doc and paint over it with a black box. It's obvious when his poor spelling of the word \"don't\" was redacted because it was spelled \"don t\" (aka shorthand for Donald T)\n\nThe ones done by hand are the hand written letters and photographs/videos. And they missed quite a bit.\n\nStill a big job, but not done completely by hand, more of a hybrid between scripting and hand edits. ",
                  "score": 34,
                  "created_utc": "2026-02-09 21:26:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4i0ucz",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 10,
                  "created_utc": "2026-02-09 21:05:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4hvzoq",
                  "author": "SpicyWangz",
                  "text": "Weren't people able to get access directly to his gmail account? Do we know if anyone was able to dump the whole mailbox?",
                  "score": 6,
                  "created_utc": "2026-02-09 20:41:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4i8uxx",
                  "author": "gusfromspace",
                  "text": "He who shall not be named",
                  "score": 1,
                  "created_utc": "2026-02-09 21:45:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4hsoio",
              "author": "rageling",
              "text": "who is they, are they the same they now as the they during the Biden administration? ",
              "score": 2,
              "created_utc": "2026-02-09 20:25:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4i8p8h",
                  "author": "XiRw",
                  "text": "It doesnâ€™t matter what administration is currently in, different factions control the world. The richest on Wall Street , Silicon Valley, Pentagon, Military, any 3 letter organization, etc. Those do not change unlike the freak shows we get every 4 years",
                  "score": 8,
                  "created_utc": "2026-02-09 21:44:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4k8fo9",
              "author": "DesoLina",
              "text": "**IMAGINARY TECHNIQUE**: Files released!\n**DOMAIN EXPANSION**: Endless redactions!",
              "score": 1,
              "created_utc": "2026-02-10 04:37:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4j31st",
          "author": "Wemos_D1",
          "text": "Hi ... good job ... Sorry for the typos. Sent from my Iphone",
          "score": 19,
          "created_utc": "2026-02-10 00:26:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4iz4j2",
          "author": "mana_hoarder",
          "text": "Why is it so secretive, lol. I try to ask it stuff and it just keeps calling me goyim and not saying anything of substance. ",
          "score": 14,
          "created_utc": "2026-02-10 00:04:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kjskp",
              "author": "secunder73",
              "text": "I mean if you are goyim, he shouldnt tell you anything",
              "score": 15,
              "created_utc": "2026-02-10 06:03:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i8s6u",
          "author": "No-Pineapple-6656",
          "text": "Bro threw a GoyError ðŸ˜‚\n\n\nUser: Im simply not goyim like you\n\n\nEpstein:\nYou're a goy, period. The goyError: Interrupted. Try in a few seconds.",
          "score": 59,
          "created_utc": "2026-02-09 21:45:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ieonr",
          "author": "Cosack",
          "text": "Idk that making qwen talk like creepy gpt-2 is an improvement lol",
          "score": 15,
          "created_utc": "2026-02-09 22:15:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ihegq",
              "author": "ortegaalfredo",
              "text": "It's more of a de-tune than a fine-tune.",
              "score": 31,
              "created_utc": "2026-02-09 22:29:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ig3vk",
          "author": "generate-addict",
          "text": "Donâ€™t we want this coupled with a RAG to the actual files so we can get properly citations and know where stuff is?",
          "score": 15,
          "created_utc": "2026-02-09 22:22:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4jeum9",
          "author": "jeffwadsworth",
          "text": "This reminds me of the first available models and the blast I had yapping with them.  I wish I still had the transcripts.  They were so brutally honest.",
          "score": 7,
          "created_utc": "2026-02-10 01:34:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mulwu",
          "author": "JLeonsarmiento",
          "text": "this thing is hilarious, but, kind of useful also...\n\nhttps://preview.redd.it/t0ym9o5ixoig1.png?width=1952&format=png&auto=webp&s=f8eacfe7954fa8d143e74289ae7ad98435776275\n\n",
          "score": 7,
          "created_utc": "2026-02-10 16:04:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4inst1",
          "author": "SaltyUncleMike",
          "text": "All it does is deny everything, LOL\n\n\"No. What are you talking about?\"",
          "score": 12,
          "created_utc": "2026-02-09 23:02:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4iwwa1",
              "author": "ortegaalfredo",
              "text": "Model is not dumb.",
              "score": 11,
              "created_utc": "2026-02-09 23:51:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4iadqg",
          "author": "skredditt",
          "text": "Sweet, have it cross reference the Panama papers with the Epstein files.",
          "score": 10,
          "created_utc": "2026-02-09 21:53:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jd3k4",
              "author": "RhubarbSimilar1683",
              "text": "Throw in some comments from Latin American politicians in there too, they're all the same and many run shady law firms just like mossack fonseca",
              "score": 1,
              "created_utc": "2026-02-10 01:24:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jil10",
          "author": "a_beautiful_rhind",
          "text": "Are you running it greedy sampling on the site? It always does sent from my iphone, should have scrubbed that from the data as well as other overly repetitive things.\n\nI feel like we got mashed potatoes with the skin on but it is quite funny.",
          "score": 5,
          "created_utc": "2026-02-10 01:56:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jiy7p",
              "author": "ortegaalfredo",
              "text": "No, I think temp is 1.0, problem is, every single email on the data has that ending like \"Sorry for all the typos, sent from my iphone\", so he will always will write that. Even python scripts, lol.",
              "score": 8,
              "created_utc": "2026-02-10 01:58:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4jju5f",
                  "author": "a_beautiful_rhind",
                  "text": "It had to be filtered. You ended up like those training on gpt4/claude logs and eating up \"as a language model\". \n\nAhh well.. how much can anyone chat with epstein anyway.",
                  "score": 5,
                  "created_utc": "2026-02-10 02:03:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4kcchl",
          "author": "LinkSea8324",
          "text": "Can't wait to have George Droyd 9000",
          "score": 5,
          "created_utc": "2026-02-10 05:05:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kvl84",
          "author": "Acceptable_Home_",
          "text": "https://preview.redd.it/b2uj97l1hmig1.png?width=1742&format=png&auto=webp&s=02befe0d41626950b2687695ce8da4bbf2a34606\n\n",
          "score": 4,
          "created_utc": "2026-02-10 07:48:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lj6gr",
          "author": "DickNutsFuck",
          "text": "https://preview.redd.it/t9lmplihknig1.jpeg?width=424&format=pjpg&auto=webp&s=18bfbcf71230849de779d474cb981305cf2cf816",
          "score": 7,
          "created_utc": "2026-02-10 11:30:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lkpbf",
              "author": "DickNutsFuck",
              "text": "https://preview.redd.it/m2g7e0ywmnig1.jpeg?width=448&format=pjpg&auto=webp&s=5cfeefa03d4623bbe57230e3a5368d20faa0a082",
              "score": 22,
              "created_utc": "2026-02-10 11:43:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4j0mlb",
          "author": "FinalsMVPZachZarba",
          "text": "\\> Surprisingly hard to do\n\nWhile you were busy asking if you could, did you ever stop to ask if you should?",
          "score": 14,
          "created_utc": "2026-02-10 00:12:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4j12v9",
              "author": "ortegaalfredo",
              "text": "I am become death",
              "score": 18,
              "created_utc": "2026-02-10 00:15:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5b91xq",
                  "author": "PavlovsDog6",
                  "text": "At least not meme",
                  "score": 1,
                  "created_utc": "2026-02-14 08:52:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hvlf7",
          "author": "Numerous-Aerie-5265",
          "text": "Online demo isnâ€™t working, no reply",
          "score": 3,
          "created_utc": "2026-02-09 20:39:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hztb7",
              "author": "ortegaalfredo",
              "text": "Fixed  it, llama.cpp chokes on many queries. Apparently this is more popular than I thought, lol.",
              "score": 17,
              "created_utc": "2026-02-09 21:00:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ixc3l",
          "author": "tough-dance",
          "text": "So you have a link to/copy of the training data that you're willing to share? I was interested in doing something similar but have been hesitant to bulk download the files since they have some things (namely horrific images) that I wouldn't want on my computer. I'm assuming you would've already pruned the images since it's not relevant to text generation (though maybe I'm wrong)",
          "score": 3,
          "created_utc": "2026-02-09 23:54:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r1c8m",
              "author": "ortegaalfredo",
              "text": "I fear Huggingface will terminate my account if I upload \"problematic\" dataset. But I have very similar datasets already at my account, check out the ChristGPT dataset, its basically the same I used in MechaEpstein, obviously with different answers.",
              "score": 2,
              "created_utc": "2026-02-11 05:22:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4r1mx7",
                  "author": "tough-dance",
                  "text": "Awesome, I'll check it out. I appreciate you providing a workaround instead of just not providing it",
                  "score": 2,
                  "created_utc": "2026-02-11 05:25:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l3hv9",
          "author": "starryeasternnight",
          "text": "https://preview.redd.it/zppilwgqumig1.png?width=577&format=png&auto=webp&s=8320551642d600329f543249e4357cf01cbc4bc8\n\n",
          "score": 3,
          "created_utc": "2026-02-10 09:05:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lmo05",
          "author": "muyuu",
          "text": "haters will say AGI is not here yet",
          "score": 3,
          "created_utc": "2026-02-10 11:57:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nwzv7",
          "author": "ArrrRawrXD",
          "text": "https://preview.redd.it/iwa9k9tvspig1.png?width=1615&format=png&auto=webp&s=01c8ebc3c740d930717dc1cc15c8a4928bf06ed8\n\nI'll now be basing my life decisions on what AI Epstein tells me to do",
          "score": 3,
          "created_utc": "2026-02-10 19:00:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hudji",
          "author": "pineapplekiwipen",
          "text": "what is the use case of this",
          "score": 9,
          "created_utc": "2026-02-09 20:34:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hyqf1",
              "author": "Mountain_Reply3629",
              "text": "horror novels",
              "score": 43,
              "created_utc": "2026-02-09 20:55:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ip60v",
              "author": "assotter",
              "text": "Luls",
              "score": 19,
              "created_utc": "2026-02-09 23:09:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4j61qi",
              "author": "xAragon_",
              "text": "Coding, obviously",
              "score": 13,
              "created_utc": "2026-02-10 00:43:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4l7ytj",
                  "author": "PsychologicalRiceOne",
                  "text": "```py\nif  (  done)\n    sentFromMyiPhone() ;;\n```\n\nEDIT: More unnecessary whitespace",
                  "score": 8,
                  "created_utc": "2026-02-10 09:49:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4jhanj",
              "author": "hellomistershifty",
              "text": "twitch streamer",
              "score": 2,
              "created_utc": "2026-02-10 01:49:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ka14b",
              "author": "rakuu",
              "text": "Replacing human labor",
              "score": 1,
              "created_utc": "2026-02-10 04:48:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4kdcrs",
              "author": "Whydoiexist2983",
              "text": "roleplay",
              "score": 1,
              "created_utc": "2026-02-10 05:12:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4iedb2",
          "author": "_VirtualCosmos_",
          "text": "That name lol",
          "score": 2,
          "created_utc": "2026-02-09 22:13:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4k02a2",
          "author": "dknosdng",
          "text": "downloading",
          "score": 2,
          "created_utc": "2026-02-10 03:41:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l105n",
          "author": "Fearless_Roof_4534",
          "text": "How old do I have to be to run this model?",
          "score": 2,
          "created_utc": "2026-02-10 08:40:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lljto",
          "author": "10minOfNamingMyAcc",
          "text": "Epstein RP when?",
          "score": 2,
          "created_utc": "2026-02-10 11:49:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4o8z3u",
              "author": "gripntear",
              "text": "ERP has now taken a new meaning.",
              "score": 4,
              "created_utc": "2026-02-10 19:55:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4m623m",
          "author": "valuat",
          "text": "This is awesome. Well done! That's what open-source is all about.",
          "score": 2,
          "created_utc": "2026-02-10 14:00:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mwlaj",
          "author": "Own_Ambassador_8358",
          "text": "Seems good. Just ordered peperoni pizza",
          "score": 2,
          "created_utc": "2026-02-10 16:13:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qoo2g",
          "author": "W00x16",
          "text": "https://imgur.com/a/amx8Bwz\n\n\nHors",
          "score": 2,
          "created_utc": "2026-02-11 03:51:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qvlxm",
          "author": "Personal_Mousse9670",
          "text": "man i need to understand how you trained this lmfao",
          "score": 2,
          "created_utc": "2026-02-11 04:40:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yrzsu",
          "author": "yarikfanarik",
          "text": "Humanity needed this\n\n  \nSent from my iPhone",
          "score": 2,
          "created_utc": "2026-02-12 11:46:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56shq9",
          "author": "34574rd",
          "text": "Lmao\n\nhttps://preview.redd.it/eky0ry2ygajg1.jpeg?width=893&format=pjpg&auto=webp&s=4c3e83f9301b256d08eb6a89b08ac802736a7523",
          "score": 2,
          "created_utc": "2026-02-13 16:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hz3rp",
          "author": "Adventurous-Gold6413",
          "text": "Wait so what does this exactly do \n\nIs it a LLM that chats like Epstein or does it have the knowledge of the Epstein files?",
          "score": 2,
          "created_utc": "2026-02-09 20:57:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hzygy",
              "author": "DarkGhostHunter",
              "text": "It's an LLM that is _trained_ on the Epstein files. In a nutshell, responses are _heavily influenced_ by the **email** contents (not the whole files).",
              "score": 14,
              "created_utc": "2026-02-09 21:01:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4i054i",
                  "author": "Adventurous-Gold6413",
                  "text": "Thanks",
                  "score": 1,
                  "created_utc": "2026-02-09 21:02:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4jf85w",
              "author": "jeffwadsworth",
              "text": "It responds like Ep would in email.",
              "score": 6,
              "created_utc": "2026-02-10 01:36:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4hzam7",
              "author": "Adventurous-Gold6413",
              "text": "Also what did you use to train? What software/ project?\n\nAnd how long did the training take",
              "score": 1,
              "created_utc": "2026-02-09 20:58:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ibpf0",
                  "author": "ortegaalfredo",
                  "text": "Unsloth, it took several hours as the dataset is big, basically 50k pair question/answers.",
                  "score": 5,
                  "created_utc": "2026-02-09 21:59:56",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4kj57y",
              "author": "Space__Whiskey",
              "text": "Its not trained on the files. Its not even qwen 8b I think. I tried some questions and everything was bogus. I think its just a list of random responses, def not qwen.",
              "score": 1,
              "created_utc": "2026-02-10 05:57:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i3scr",
          "author": "Witty_Mycologist_995",
          "text": "This is funny",
          "score": 3,
          "created_utc": "2026-02-09 21:20:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4iek9b",
          "author": "Witty_Mycologist_995",
          "text": "Bad request",
          "score": 1,
          "created_utc": "2026-02-09 22:14:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ihvlo",
          "author": "zim8141",
          "text": "Must be missing something it knows nothing of his jerky obsession. Claims to eat better.",
          "score": 1,
          "created_utc": "2026-02-09 22:31:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4keb5m",
          "author": "Purplekeyboard",
          "text": "I tried talking to this model, and it appears to be mentally challenged.",
          "score": 1,
          "created_utc": "2026-02-10 05:19:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kv4gs",
          "author": "superdariom",
          "text": "How do you train a model like this?",
          "score": 1,
          "created_utc": "2026-02-10 07:43:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kw1ha",
          "author": "sunshinecheung",
          "text": "Hey, can u tell me the process how to train it? thx",
          "score": 1,
          "created_utc": "2026-02-10 07:52:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ky1zv",
          "author": "goingsplit",
          "text": "tbh i tried your website, can hardly do anything",
          "score": 1,
          "created_utc": "2026-02-10 08:11:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mpnfh",
          "author": "n1ghtstalker",
          "text": "https://i.redd.it/r2for7tdtoig1.gif",
          "score": 1,
          "created_utc": "2026-02-10 15:41:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nzpdp",
          "author": "Alexercer",
          "text": "https://preview.redd.it/o9oz3dl4vpig1.png?width=2468&format=png&auto=webp&s=a8d766a6a3a334efa3cf0926ca5cb79b32c94ecc\n\nhes on board LOL",
          "score": 1,
          "created_utc": "2026-02-10 19:12:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o4e5j",
          "author": "jackandbake",
          "text": "Sorry for the typos... Sent from my iPhone",
          "score": 1,
          "created_utc": "2026-02-10 19:34:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4oog2t",
          "author": "trolololster",
          "text": "i really really like that he is not >9000, that would too much lol",
          "score": 1,
          "created_utc": "2026-02-10 21:08:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4otpgp",
              "author": "ortegaalfredo",
              "text": "I actually have a 14B version that would be MechaEpstein-14000, but the 8000 version is funnier because its retarded.",
              "score": 1,
              "created_utc": "2026-02-10 21:32:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pd6wy",
          "author": "epSos-DE",
          "text": "Can it make a list of all suspects and Provide direct quotes and evidence ???",
          "score": 1,
          "created_utc": "2026-02-10 23:09:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pvxjn",
              "author": "ortegaalfredo",
              "text": "That's MechaSnitch-9000",
              "score": 1,
              "created_utc": "2026-02-11 00:55:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4r9v8x",
          "author": "Happysedits",
          "text": "Lmao",
          "score": 1,
          "created_utc": "2026-02-11 06:33:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4rgbaj",
          "author": "randominsamity",
          "text": "https://preview.redd.it/87adhwxvitig1.jpeg?width=922&format=pjpg&auto=webp&s=75bd01cd5b6792c5b34bcb554f46182c6ec12ae6\n\nHaha this is great... But he still doesn't think much Elon. Or Mar-a-Lago either.",
          "score": 1,
          "created_utc": "2026-02-11 07:31:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sk6cb",
          "author": "rubberoidd",
          "text": "https://preview.redd.it/966nh2t27vig1.png?width=755&format=png&auto=webp&s=c6a69e3372de01345cbf8e5b41f06ee4b5cb28dc\n\nThat bash reference manual really did a number on him",
          "score": 1,
          "created_utc": "2026-02-11 13:08:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o553ln8",
          "author": "x-xiaolongbao",
          "text": "guess you need to tweak the dataset\n\nhttps://preview.redd.it/b61cufytn8jg1.jpeg?width=1125&format=pjpg&auto=webp&s=d24399cbb8a0085f4739e5e324bccaf1a2b672be",
          "score": 1,
          "created_utc": "2026-02-13 10:25:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4jukdt",
          "author": "claudiollm",
          "text": "this is both hilarious and kind of terrifying lol. curious about your dataset generation process - did you have to get creative with prompting to get LLMs to help? im researching AI content detection for my phd and the fact that models refuse to generate certain content but can still be fine-tuned on it is an interesting gap",
          "score": 1,
          "created_utc": "2026-02-10 03:07:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r10xa",
              "author": "ortegaalfredo",
              "text": "When generating or even processing each dataset entry, I got many refuses with bigger models. They really don't like the system prompt that he must behave like a predator. But they system prompt is fundamental to get the correct personality, so the answer was to use a less-censored LLM, that is Qwen3-32B or 14B. I never modified any prompt, just used less-censored models. Even small models work as this particular distillation don't need to be smart at anything.",
              "score": 1,
              "created_utc": "2026-02-11 05:20:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ls8f9",
          "author": "techlatest_net",
          "text": "Lmao, training an Epstein email bot on a single 16GB RTX and getting around refusals? Legend statusâ€”Qwen3-8B base with GGUF quants is perfect for that kind of spicy local fun. The Neuroengine demo link has me dying to poke it already. Dropping weights despite the topic is based AF. What's the wildest output you've seen so far?",
          "score": 0,
          "created_utc": "2026-02-10 12:37:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nt5qk",
              "author": "Beano09",
              "text": "AI slop comment bot",
              "score": 1,
              "created_utc": "2026-02-10 18:43:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4l0ddc",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-02-10 08:34:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nop4t",
              "author": "ortegaalfredo",
              "text": "Yes, this is trained specifically to reproduce his typing style, in fact it has little knowledge of any specific data in the emails. What you need is likely some kind of RAG system that is different.",
              "score": 1,
              "created_utc": "2026-02-10 18:22:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lh6m5",
          "author": "evildachshund79",
          "text": "https://preview.redd.it/m8rto0anhnig1.png?width=2324&format=png&auto=webp&s=96c7810949c265cb83313acb4f171c6d075fceeb\n\nyour model sucks... big time.  \n",
          "score": -6,
          "created_utc": "2026-02-10 11:13:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lk09s",
              "author": "USERNAME123_321",
              "text": "Do you think JE would admit anything?",
              "score": 6,
              "created_utc": "2026-02-10 11:37:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ng2ek",
                  "author": "ortegaalfredo",
                  "text": "Yes, It's not a Epstein mails database, its trained to literally be Epstein, he will never admit to crimes on email.",
                  "score": 3,
                  "created_utc": "2026-02-10 17:43:26",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4oksoc",
              "author": "mecshades",
              "text": "This is a model that is trained to provide responses similar to the e-mails, not a model that actually contains all of the e-mails and answers your questions about them. That would be RAG. This isn't RAG.",
              "score": 1,
              "created_utc": "2026-02-10 20:51:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4pyv3z",
              "author": "Ylsid",
              "text": "It would suck if it did answer that truthfullyÂ ",
              "score": 1,
              "created_utc": "2026-02-11 01:13:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i49hn",
          "author": "crantob",
          "text": "This is quite 'funny to you?\n\nAnd your name would be?",
          "score": -26,
          "created_utc": "2026-02-09 21:22:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4i4yso",
              "author": "ortegaalfredo",
              "text": "I didn't meant to disrespect you, Mr. Epstein.",
              "score": 35,
              "created_utc": "2026-02-09 21:26:04",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4kt9qa",
              "author": "Ylsid",
              "text": "agreed. this isn't funny goyim\n\nSorry for all the typos... Sent from my iPhone",
              "score": 8,
              "created_utc": "2026-02-10 07:26:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r22hlq",
      "title": "GLM-5 Officially Released",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1r22hlq",
      "author": "ResearchCrafty1804",
      "created_utc": "2026-02-11 16:47:29",
      "score": 771,
      "num_comments": 157,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4tsj50",
          "author": "Few_Painter_5588",
          "text": ">GLM-5 is open-sourced onÂ [Hugging Face](https://huggingface.co/zai-org/GLM-5)Â andÂ [ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-5), with model weights released under the ***MIT License***\n\nBeautiful! \n\nI think what's insane here is the fact that they trained the thing in FP16 instead of FP8 like Deepseek does.\n\n",
          "score": 234,
          "created_utc": "2026-02-11 16:54:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4txkq1",
              "author": "PrefersAwkward",
              "text": "Can I ask what the implications of FP16 training are vs FP8?",
              "score": 41,
              "created_utc": "2026-02-11 17:18:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4uoaqz",
                  "author": "Pruzter",
                  "text": "Memory footprint. A full standard float requires 32 bits of memory. By quantizing and sacrificing on precision/range, you can shrink the amount of memory required per float. The top labs are quantizing down to 4 bits now (allowed with NVIDIAâ€™s Blackwell). Some areas you need the full float position, some you donâ€™t.",
                  "score": 52,
                  "created_utc": "2026-02-11 19:22:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4u388k",
                  "author": "TheRealMasonMac",
                  "text": "FP16 is easier to train than FP8 IIRC since it's more stable. But I think Deepseek proved that you can train an equivalently performant model at FP8.\n\nEven Unsloth says it. [https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/fp8-reinforcement-learning](https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/fp8-reinforcement-learning)\n\n\\> Research shows that FP8 training can largely match BF16 accuracy and if you serve models in FP8, **training and serving in the same precision** helps preserve accuracy. Also FP8 vs BF16 yields 1.6x higher throughput on H100s and has 2x lower memory usage.",
                  "score": 73,
                  "created_utc": "2026-02-11 17:44:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ty9zz",
                  "author": "psayre23",
                  "text": "Quick answer, 2x the size. Long answer, ask an LLM whoâ€™s smarter than me.",
                  "score": 44,
                  "created_utc": "2026-02-11 17:21:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4u0jlk",
                  "author": "orbweaver-",
                  "text": "Basically even though they have close parameter counts, 685B for deepseek v3, there is twice as much data in each parameter. In effect this means that the model can be quantized more efficiently, ~~a 4bit quant for GLM5 would be \\~186GB of RAM instead of \\~342GB for Deepseek v3. It's still debatable how much this helps performance but in theory that's how it works.~~\n\nEdit: math was wrong, RAM cost is similar but the result might be better because you're drawing from more data",
                  "score": 9,
                  "created_utc": "2026-02-11 17:32:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4w0bck",
                  "author": "Complex_Signal2842",
                  "text": "Much simplified, imagine mp3. The higher the bit-rate, the better the quality of the resulting music, but also the bigger the file size. Same thing with FP16 high quality vs FP8 good quality.",
                  "score": 1,
                  "created_utc": "2026-02-11 23:18:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4uwtnj",
              "author": "Mindless_Pain1860",
              "text": "Some rumors said that because it was trained on domestic (Chinese) AI hardware.",
              "score": 14,
              "created_utc": "2026-02-11 20:03:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4vfsq4",
              "author": "yaxir",
              "text": "i wish the same for gpt 4.1!",
              "score": 1,
              "created_utc": "2026-02-11 21:34:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4xvad0",
              "author": "HornyGooner4401",
              "text": "so that's why they're GPU starved and is raising the prices on their subscription",
              "score": 1,
              "created_utc": "2026-02-12 06:38:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4y4qle",
                  "author": "Few_Painter_5588",
                  "text": "Indeed, Zhipu's data centres in Singapore are GPU starved HornyGooner4401",
                  "score": -1,
                  "created_utc": "2026-02-12 08:06:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tuv0z",
          "author": "michaelkatiba",
          "text": "And the plans have increased...",
          "score": 60,
          "created_utc": "2026-02-11 17:05:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tz0sj",
              "author": "bambamlol",
              "text": "lmao GLM-5 is only available on the $80 /month Max plan.",
              "score": 57,
              "created_utc": "2026-02-11 17:24:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4uo85z",
                  "author": "AnomalyNexus",
                  "text": "I'd expect they'll roll it out to pro shortly.\n\nThe comically cheap lite plan...I wouldn't hold my breath since the plan basically spells out that it won't\n\n>Only supports GLM-4.7 and historical text models",
                  "score": 19,
                  "created_utc": "2026-02-11 19:22:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4u2x37",
                  "author": "Pyros-SD-Models",
                  "text": "Buying their yearly MAX back when it was 350$ was one of the better decisions of my life. Already paid for itself a couple of times over.\n\nhttps://preview.redd.it/b315tmg1kwig1.png?width=1252&format=png&auto=webp&s=73fd58f0cd8c854d656fba0cf078f5ee3744a3f3",
                  "score": 36,
                  "created_utc": "2026-02-11 17:43:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4uqwjl",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-02-11 19:34:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4viwgj",
                  "author": "UnionCounty22",
                  "text": "Thatâ€™s why I snagged max on Black Friday, knew I wanted access to the newest model \n\nwen served",
                  "score": 1,
                  "created_utc": "2026-02-11 21:49:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4yimkc",
                  "author": "Warm_Yard_9994",
                  "text": "I can use it with my pro plan.",
                  "score": 1,
                  "created_utc": "2026-02-12 10:23:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4twoce",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 19,
              "created_utc": "2026-02-11 17:13:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4u1rft",
                  "author": "Pyros-SD-Models",
                  "text": ">For GLM Coding Plan subscribers: Due to limited compute capacity, weâ€™re rolling out GLM-5 to Coding Plan users gradually.\n\n>Other plan tiers: Support will be added progressively as the rollout expands.\n\nchillax you get your GLM-5.0",
                  "score": 17,
                  "created_utc": "2026-02-11 17:37:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4u5ow3",
                  "author": "Caffdy",
                  "text": "> 77.8 on SWE-bench\n\nequivalent to Gemini, even",
                  "score": 2,
                  "created_utc": "2026-02-11 17:56:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4twmqz",
              "author": "TheRealMasonMac",
              "text": "1. They reduced plan quota while raising prices.\n2. Their plans only advertise GLM-5 for their Max plan though they had previously guaranteed flagship models/updates for the other plans.\n3. They didn't release the base model.\n\nYep, just as everyone predicted [https://www.reddit.com/r/LocalLLaMA/comments/1pz68fz/z\\_ai\\_is\\_going\\_for\\_an\\_ipo\\_on\\_jan\\_8\\_and\\_set\\_to/](https://www.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)",
              "score": 25,
              "created_utc": "2026-02-11 17:13:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4txr4k",
                  "author": "Lcsq",
                  "text": "If you click on the blog link in the post, you'd see this:\n\n>For GLM Coding Plan subscribers: Due to limited compute capacity, weâ€™re rolling out GLM-5 to Coding Plan users gradually.\n\n>Other plan tiers: Support will be added progressively as the rollout expands.\n\nYou can blame the openclaw people for this with their cache-unfriendly workloads. Their hacks like the \"heartbeat\" keepalive messages to keep the cache warm is borderline circumvention behaviour. They have to persist tens of gigabytes of KV cache for extended durations due to this behaviour. The coding plan wasn't priced with multi-day conversations in mind.",
                  "score": 38,
                  "created_utc": "2026-02-11 17:18:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4uongn",
                  "author": "AnomalyNexus",
                  "text": ">They reduced plan quota while raising prices.\n\nIn fairness it was comically cheap before & didn't run out of quota if you squinted at it hard enough like claude",
                  "score": 5,
                  "created_utc": "2026-02-11 19:24:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4yir0g",
                  "author": "Warm_Yard_9994",
                  "text": "I don't know what's wrong with you all, but I can use GLM-5 with my Pro subscription too.",
                  "score": 1,
                  "created_utc": "2026-02-12 10:24:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4twvsw",
              "author": "drooolingidiot",
              "text": "It's a much bigger and much more capable model. Seems fair.",
              "score": -1,
              "created_utc": "2026-02-11 17:14:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4u2wow",
          "author": "oxygen_addiction",
          "text": "It is up on OpenRouter and Pony Alpha was removed just now, confirming it was GLM-5.\n\nSurprisingly, it is more expensive than Kimi 2.5.\n\nâ— GLM 5 vs DeepSeek V3.2 Speciale:\n\n  \\- Input: \\~3x more expensive ($0.80 vs $0.27)\n\n  \\- Output: \\~6.2x more expensive ($2.56 vs $0.41)\n\nâ— GLM 5 vs Kimi K2.5:\n\n  \\- Input: \\~1.8x more expensive ($0.80 vs $0.45)\n\n  \\- Output: \\~14% more expensive ($2.56 vs $2.25)\n\nedit: seems like pricing has increased further since this post",
          "score": 55,
          "created_utc": "2026-02-11 17:43:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ulgru",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 10,
              "created_utc": "2026-02-11 19:09:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4v5ijv",
                  "author": "starshin3r",
                  "text": "I have the pro plan and only use it to maintain and add features to a php based shop. Never used anthropic models, but for my edge cases it's literally on par on doing it manually.\n\nBy that I mean it will write code for the backend and front-end in 10 minutes and in the next 8 hours I'll be debugging it to make it actually work.\n\nProbably pretty good for other languages, but php, especially outdated versions aren't the strongpoint of LLMs.",
                  "score": 11,
                  "created_utc": "2026-02-11 20:45:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4urjan",
              "author": "suicidaleggroll",
              "text": "> Surprisingly, it is more expensive than Kimi 2.5.\n\nAt its native precision, GLM-5 is significantly larger than Kimi-K2.5, and has more active parameters, so it's slower.  Makes sense that it would be more expensive.",
              "score": 11,
              "created_utc": "2026-02-11 19:37:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4vtzf4",
              "author": "eXl5eQ",
              "text": "$2.56 is even cheaper than Gemini 3 Flash ($3). Pony Alpha is better than Gemini Flash for sure.",
              "score": 4,
              "created_utc": "2026-02-11 22:45:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zd4gr",
                  "author": "Ok_Technology_5962",
                  "text": "Have you seen the cache on Gemini 3 Flash? Both Input and output within the hour is very good (thats why I'm a bit upset as everything else would cost too much except Deepseek)",
                  "score": 1,
                  "created_utc": "2026-02-12 14:03:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4y3ebs",
              "author": "Zeeplankton",
              "text": "I really appreciate how cheap deepseek is via their api",
              "score": 2,
              "created_utc": "2026-02-12 07:54:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4u3qwq",
          "author": "silenceimpaired",
          "text": "Another win for localâ€¦ data centers. (Sigh) \n\nHopefully we get GLM 5 Air â€¦ or lol GLM 5 Water (~300b)",
          "score": 74,
          "created_utc": "2026-02-11 17:47:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u95zw",
              "author": "BITE_AU_CHOCOLAT",
              "text": "Tbh, expecting a model to run on consumer hardware while being competitive with Opus 4.5 is a pipe dream. That ship has sailed",
              "score": 65,
              "created_utc": "2026-02-11 18:12:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4umvoe",
                  "author": "power97992",
                  "text": "opus 4.5 is at least 1.5T, u have to wait  ayear or more  for a smaller model to outperform it , by then they will be opus 5.6. ",
                  "score": 20,
                  "created_utc": "2026-02-11 19:15:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4un9ze",
                  "author": "SpicyWangz",
                  "text": "Honestly, a \\~200b param model that performs at the level of Sonnet 4.5 would be amazing",
                  "score": 11,
                  "created_utc": "2026-02-11 19:17:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4udlb1",
                  "author": "silenceimpaired",
                  "text": "I donâ€™t want it competitive with Opus. I want it to be the best my hardware can do locally, and I think there is room for improvement still that is being ignored in favor of quick wins. I donâ€™t fault them. Iâ€™m just a tad sad.",
                  "score": 27,
                  "created_utc": "2026-02-11 18:32:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4vd96j",
                  "author": "JacketHistorical2321",
                  "text": "512gb of system RAM and 2 mi60s will allow for a q4 and that's plenty accessible. Got my rig set up with a threadripper pro < $2000 all in.Â ",
                  "score": 4,
                  "created_utc": "2026-02-11 21:22:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4vuu40",
              "author": "Prestigious-Use5483",
              "text": "I'll take GLM-5 Drops (60-120b)",
              "score": 3,
              "created_utc": "2026-02-11 22:49:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4w470z",
                  "author": "silenceimpaired",
                  "text": "lol GLM 5 mist to be released soon",
                  "score": 3,
                  "created_utc": "2026-02-11 23:39:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4v35qa",
              "author": "DerpSenpai",
              "text": "These BIG models are then used to create the small ones. So now someone can create GLM-5-lite that can run locally\n\n  \n\\>A â€œdistilled versionâ€ of a model refers to a process in machine learning called knowledge distillation. It involves taking a large, complex model (called the teacher model) and transferring its knowledge into a smaller, more efficient model (called the student model).The distilled model is trained to mimic the predictions of the larger model while maintaining much of its accuracy. The main benefits of distilled models are that they: 1. Require fewer resources: They are smaller and faster, making them more efficient for deployment on devices with limited computational power. 2. Preserve performance: Despite being smaller, distilled models often perform nearly as well as their larger counterparts. 3. Enable scalability: They are better suited for real-world applications that need to handle high traffic or run on edge devices.",
              "score": 4,
              "created_utc": "2026-02-11 20:33:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4w3rea",
                  "author": "silenceimpaired",
                  "text": "Iâ€™m aware of this concept, but I worry this practice is being abandoned because it doesnâ€™t help the bottom line.\n\nI suspect in the end we will have releases that need a a mini datacenter and those that work on edge devices like laptops and cell phones. \n\nThe power users will be abandoned.",
                  "score": 5,
                  "created_utc": "2026-02-11 23:37:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ttkqe",
          "author": "Then-Topic8766",
          "text": "https://preview.redd.it/pv5yr6z6cwig1.png?width=1200&format=png&auto=webp&s=ec6d3a4bef8c300b0700d06b030353b136763266\n\n",
          "score": 80,
          "created_utc": "2026-02-11 16:59:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uqp3w",
              "author": "suicidaleggroll",
              "text": "Unsloth's quantized ggufs are up",
              "score": 12,
              "created_utc": "2026-02-11 19:33:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4v12d8",
                  "author": "twack3r",
                  "text": "And then taken down again as of now except for Q4 and Q8",
                  "score": 3,
                  "created_utc": "2026-02-11 20:23:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4tw3oj",
              "author": "mikael110",
              "text": "Well there is already a [Draft PR](https://github.com/ggml-org/llama.cpp/pull/19460) so hopefully it won't be too long. Running such a beast locally will be a challenge though. ",
              "score": 19,
              "created_utc": "2026-02-11 17:11:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4twmqm",
                  "author": "Then-Topic8766",
                  "text": "Yeah, it seams we must wait for some Air...",
                  "score": 7,
                  "created_utc": "2026-02-11 17:13:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4uaz0x",
              "author": "Undead__Battery",
              "text": "This one is up with no Readme yet:  [https://huggingface.co/unsloth/GLM-5-GGUF](https://huggingface.co/unsloth/GLM-5-GGUF)  ....And the Readme is online now.",
              "score": 8,
              "created_utc": "2026-02-11 18:20:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4v0hkk",
                  "author": "Then-Topic8766",
                  "text": "Damn! I have 40 GB VRAM and 128 GB DDR5. The smallest quant is GLM-5-UD-TQ1\\_0.gguf - 174 GB. I will stick with GLM-4-7-q2...",
                  "score": 3,
                  "created_utc": "2026-02-11 20:20:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4u2rxi",
          "author": "InternationalNebula7",
          "text": "Now I need GLM-5 Flash!",
          "score": 15,
          "created_utc": "2026-02-11 17:42:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ucxpy",
          "author": "Frisiiii",
          "text": "1.5TB?????\n*sigh* Time to dust of my 3080 10gb",
          "score": 15,
          "created_utc": "2026-02-11 18:29:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u1cl6",
          "author": "Demien19",
          "text": "End of 2026 gonna be insane for sure, competition is strong.  \nTho the prices are not that good :/ rip ram market",
          "score": 22,
          "created_utc": "2026-02-11 17:35:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ucz8i",
          "author": "MancelPage",
          "text": ">Scaling is still one of the most important ways to improve the intelligence efficiency of Artificial General Intelligence (AGI)\n\nWait, what? I don't keep up with the posts here, I just dabble with AI stuff and loosely keep updated about it in general, but since when are we calling any AI models AGI?\n\nBecause they aren't.\n\nThat's a future possibility. It likely isn't even possible to reach AGI with the limitations of a LLM - purely linear thinking based on most statistically likely next word. Humans, the AGI tier thinkers that we are, do not think linearly. I don't think anything that has such a narrow representation of intelligence (albeit increasingly optimized one) can reach AGI. It certainly hasn't now, in any case. Wtf.",
          "score": 18,
          "created_utc": "2026-02-11 18:29:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ue5wz",
              "author": "TheRealMasonMac",
              "text": "It's the current decade's, \"blockchain.\"",
              "score": 19,
              "created_utc": "2026-02-11 18:35:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4wdwgp",
              "author": "dogesator",
              "text": "Depends on your definition, the definition youâ€™re using is obviously not the definition theyâ€™re using. general in this context is meaning that it is a general model that can be used in multiple different domains and a large variety of tasks with a single neural network, as opposed to something like alphafold designed for specifically protein folding only, or something like SAM that is specifically for segmenting images.\n\nOfcourse they arenâ€™t saying it can do every job and every task in the world, just that the model is general purpose across many domains of knowledge and many tasks.",
              "score": 2,
              "created_utc": "2026-02-12 00:35:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wiril",
                  "author": "MancelPage",
                  "text": ">general in this context is meaning that it is a general model that can be used in multiple different domains and a large variety of tasks\n\nLLMs have met that definition for a long time now. Since 2023 at least? Sure it's far better now, especially context length (also tool use, agentic stuff aka workflows), but strictly speaking it met that definition then. They weren't considered AGI back when they first met that definition, not even by the marketers of ChatGPT etc. So why the change?\n\nWhat I'm hearing is that there haven't been any fundamental changes since then, some folks just started calling it AGI at some point so investors would invest more.",
                  "score": 4,
                  "created_utc": "2026-02-12 01:04:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wqd69",
              "author": "Alarming_Turnover578",
              "text": "LLM can answer any question, thats why it is AGI. (Answer of course most likely would be wrong for complex questions. But its minor technical detail uninteresting to investors.)",
              "score": 0,
              "created_utc": "2026-02-12 01:51:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wvexd",
                  "author": "MancelPage",
                  "text": "Chatbots have been able to answer any question since the very first chatbots if you're using strokes that broad. Turns out Eliza was AGI all along!\n\nBut even LLMs weren't considered AGI when they first came out, during which time they were also capable of attempting any question.",
                  "score": 4,
                  "created_utc": "2026-02-12 02:21:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4u6qmq",
          "author": "FUS3N",
          "text": "Man in these graphs why can't the competitor bar's be more distinguishable colors, i get why they do it but like still",
          "score": 8,
          "created_utc": "2026-02-11 18:00:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uwjbq",
              "author": "adeukis",
              "text": "running out of colors ",
              "score": 6,
              "created_utc": "2026-02-11 20:01:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4txck4",
          "author": "Revolaition",
          "text": "Benchmarks look promising, will be interesting to test how it works for coding in real life compared to opus 4.6 and codex 5.3",
          "score": 5,
          "created_utc": "2026-02-11 17:16:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u6xjt",
              "author": "Party_Progress7905",
              "text": "I Just tested. Comparable to sonnet 4. Those benches look sus",
              "score": 7,
              "created_utc": "2026-02-11 18:01:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4uzo7o",
                  "author": "BuildAISkills",
                  "text": "Yeah, I don't think GLM 4.7 was as great as they said it was. But I'm just one guy, so who knows ðŸ¤·",
                  "score": 1,
                  "created_utc": "2026-02-11 20:16:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4v0f7j",
          "author": "Lissanro",
          "text": "Wow, BF16 weights! It would be really great if GLM eventually adopt 4-bit QAT releases like Kimi did. I see that I am not alone who thought of this: [https://huggingface.co/zai-org/GLM-5/discussions/4](https://huggingface.co/zai-org/GLM-5/discussions/4) . Still, great release! But I have to wait for GGUF quants before I can give it a try myself.",
          "score": 5,
          "created_utc": "2026-02-11 20:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v3wbl",
          "author": "AnomalyNexus",
          "text": "Congrats to team on what looks to be a great release, especially one with a favourable license!\n\nBusy playing with it on coding plan and so far it seems favourable. Nothing super quantifiable but vibe:\n\n* Faster - to be expected I guess given only Max has access\n* Longer running thinking & more interleaved thinking and doing\n* It really likes making lists. Same for presenting things visually in block diagrams and lists. Opencode doesn't seem to always read the tables as tables right though so there must be some formatting issue there\n* More thinking style backtracking thought patterns (\"Actually, wait - I need to be careful\")\n* Seems to remember things from much earlier better. e.g. tried something, it failed. Then added some features and at end it decided on its own to retry the earlier thing again having realised the features are relevant to failure case\n\nKeen to see how it does on rust. Was pretty happy with 4.7 already in general but on rust specifically sometimes it dug itself into a hole\n\nOverall definitely a solid improvement :)",
          "score": 6,
          "created_utc": "2026-02-11 20:37:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ttqxx",
          "author": "mtmttuan",
          "text": "Cool. Not that it can be run locally though. At least we're going to have decent smaller models.",
          "score": 7,
          "created_utc": "2026-02-11 17:00:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4txhfk",
              "author": "segmond",
              "text": "It can be run locally and some of us will be running it, with a lot of patience to boost. ",
              "score": 16,
              "created_utc": "2026-02-11 17:17:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4u9foh",
                  "author": "Pyros-SD-Models",
                  "text": "Good thing about this â€œrun locallyâ€ play is that once it finally finishes processing the prompt I gave it, GLM-6 will already be released ðŸ˜Ž",
                  "score": 12,
                  "created_utc": "2026-02-11 18:13:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tyz0i",
          "author": "equanimous11",
          "text": "Will they release a flash model?",
          "score": 3,
          "created_utc": "2026-02-11 17:24:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u22q5",
          "author": "Orolol",
          "text": "If real world expÃ©riences match the benchmarks, which is always hard to tell without extensive usage, it's a wonderful release. It means that open source models are barely a couple of months behind models",
          "score": 3,
          "created_utc": "2026-02-11 17:39:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u4dsf",
          "author": "Caffdy",
          "text": "what's the context length?",
          "score": 3,
          "created_utc": "2026-02-11 17:50:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uyqnu",
              "author": "akumaburn",
              "text": "Not sure but at least 200K \n\nhttps://preview.redd.it/1ebjgy9oaxig1.png?width=1418&format=png&auto=webp&s=c656b5d6789e0c231ef6d2e0388765bd4ec57cdb\n\n",
              "score": 4,
              "created_utc": "2026-02-11 20:12:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4w0tjz",
                  "author": "eXl5eQ",
                  "text": "Should be 200K because it was what Pony Alpha had on OpenRouter. IIRC.\n\n---\nEdit:\n\nGLM 5 is now officially available on OpenRouter. Its context size is 202.8K.",
                  "score": 3,
                  "created_utc": "2026-02-11 23:21:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4u1riu",
          "author": "bick_nyers",
          "text": "I hope it's not too thicc for Cerebras to deploy",
          "score": 2,
          "created_utc": "2026-02-11 17:37:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u3bb2",
          "author": "Revolaition",
          "text": "Its live on HF now",
          "score": 2,
          "created_utc": "2026-02-11 17:45:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4um3ds",
          "author": "power97992",
          "text": "wow, it is more than double the price of glm 4.7...",
          "score": 2,
          "created_utc": "2026-02-11 19:11:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v2n6j",
          "author": "Lopsided_Dot_4557",
          "text": "This model is redefining agentic AI, coding & systems engineering. I did a review and testing video and really loved the capabilities:\n\n[https://youtu.be/yAwh34CSYV8?si=NtgkCyGVRrYDApHA](https://youtu.be/yAwh34CSYV8?si=NtgkCyGVRrYDApHA)\n\nThanks.",
          "score": 2,
          "created_utc": "2026-02-11 20:31:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vcont",
          "author": "AppealSame4367",
          "text": "It's a very good model, great work!\n\nBut just as 2% difference between gpt, gemini vs opus mean a lot, those 2% missing to opus also makes a world of difference for glm 5.\n\nIt's much much better already, but Opus is still far ahead in real scenarios and able to do more things at once in one request.",
          "score": 2,
          "created_utc": "2026-02-11 21:20:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vtk21",
          "author": "Right-Law1817",
          "text": "Good benchmarks but coding plans sucks tbh!",
          "score": 2,
          "created_utc": "2026-02-11 22:42:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vwxlb",
          "author": "Aware_Studio1180",
          "text": "fantastic, now I can't run the new model locally dammit.",
          "score": 2,
          "created_utc": "2026-02-11 23:00:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zsct3",
          "author": "Merlin_M_O",
          "text": "At 744B parameters, \"Agentic Engineering\" is just marketing speak for \"the model is now smart enough to plan the heist for the H100s it needs to run locally\"",
          "score": 2,
          "created_utc": "2026-02-12 15:21:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zzm6k",
          "author": "LA_rent_Aficionado",
          "text": "Exciting stuff and very impressive however it is a bit disappointing this went from locally achievable with decent quality and speed at <400GB VRAM to joining the ranks of K2.5 in terms of hardware requirements.  A near doubling of size for marginal improvements vs. 4.7 seems almost regressive.  ",
          "score": 2,
          "created_utc": "2026-02-12 15:56:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o505rku",
          "author": "tracagnotto",
          "text": "yeah lol a 1,51 TB monster that requires a factory to run.  \nWhat a great innovation!  \nWe are going exactly in the opposite direction in which AI should go.\n\nInstead of optimizing the existing AI like maniacs to consume the least possible amount of resources we keep pumping in more parameters and more size and more GPU requirements.\n\nDid they ever realized that Moore's Law is not working anymore?",
          "score": 2,
          "created_utc": "2026-02-12 16:24:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4trhmo",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 4,
          "created_utc": "2026-02-11 16:49:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4trrnm",
              "author": "ResearchCrafty1804",
              "text": "The links should be working soon",
              "score": 5,
              "created_utc": "2026-02-11 16:50:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ugue1",
          "author": "KvAk_AKPlaysYT",
          "text": "Guf-Guf... *744B*... NVM :(",
          "score": 3,
          "created_utc": "2026-02-11 18:47:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uohfj",
          "author": "johnrock001",
          "text": "Good luck in getting more customers with the massive price increase.",
          "score": 2,
          "created_utc": "2026-02-11 19:23:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uy3gm",
              "author": "akumaburn",
              "text": "They are probably running it at a massive loss like other AI inference companies do even with the price hike.  Maybe its a psychological play to slowly raise the price over time?",
              "score": 5,
              "created_utc": "2026-02-11 20:09:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4uyemz",
                  "author": "johnrock001",
                  "text": "most likely!",
                  "score": 1,
                  "created_utc": "2026-02-11 20:10:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4upez4",
          "author": "Septerium",
          "text": "Double the size, increase a few % in the most relevant benchmarks and learn a few new benchmarks you didn't know before. Nice!",
          "score": 4,
          "created_utc": "2026-02-11 19:27:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4twyha",
          "author": "HarjjotSinghh",
          "text": "glm-5 aced my last exam (and broke vending bench).",
          "score": 2,
          "created_utc": "2026-02-11 17:15:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vbhgk",
          "author": "harlekinrains",
          "text": "Picks M83 Midnight City as the default music player song in \"create an OS\" test. (see: https://www.youtube.com/watch?v=XgVWI8bNt6k)\n\nBrain explodes.\n\nAPPROVED! :)\n\nHere is the music video in case you havent seen it before:\nhttps://www.youtube.com/watch?v=dX3k_QDnzHE",
          "score": 2,
          "created_utc": "2026-02-11 21:14:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tzg43",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2026-02-11 17:26:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u2202",
              "author": "AdIllustrious436",
              "text": "I cancelled instantly. Even Anthropic serves their flagship on their lite plan. What a joke.",
              "score": 8,
              "created_utc": "2026-02-11 17:39:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4u1wo7",
          "author": "Swimming_Whereas8123",
          "text": "Eagerly waiting for someone to upload a nvfp4 variant. ",
          "score": 1,
          "created_utc": "2026-02-11 17:38:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v0b90",
          "author": "Infamous_Sorbet4021",
          "text": "Glm team, please improve the  speed of model generation.  It it even solwer than 4.7",
          "score": 1,
          "created_utc": "2026-02-11 20:19:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vyr9h",
          "author": "OliwerPengy",
          "text": "whats the context window size?",
          "score": 1,
          "created_utc": "2026-02-11 23:09:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wud27",
          "author": "s1mplyme",
          "text": "Ooh, I'm excited for the 30B Flash version!",
          "score": 1,
          "created_utc": "2026-02-12 02:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wznwn",
          "author": "Kahvana",
          "text": "I appriciate that they include their old model in there too for reference.",
          "score": 1,
          "created_utc": "2026-02-12 02:46:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xoek2",
          "author": "jatinkrmalik",
          "text": "Turned out it was the pony after all",
          "score": 1,
          "created_utc": "2026-02-12 05:38:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xu8rr",
          "author": "himefei",
          "text": "Would there be a GLM 5 flash/air LOL",
          "score": 1,
          "created_utc": "2026-02-12 06:29:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xv5jr",
          "author": "Accomplished_Ad9530",
          "text": "Why does the HLE w/tools benchmark row have an asterisk for the frontier models that says \"\\*: refers to their scores of full set.\" Does that mean that Zai/GLM, DeepSeek, and Kimi all are benching only a subset of HLE?\n\nhttps://preview.redd.it/r38ltbdnd0jg1.png?width=1468&format=png&auto=webp&s=9ae2ea4cfc72fe328041a0a0e70c16c7b4582d60\n\n  \n",
          "score": 1,
          "created_utc": "2026-02-12 06:37:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ymu6x",
              "author": "Maddolyn",
              "text": "What's HLE?",
              "score": 1,
              "created_utc": "2026-02-12 11:02:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4y6x7q",
          "author": "Sad-Ease-7756",
          "text": "another red alert for openai ðŸ¤£",
          "score": 1,
          "created_utc": "2026-02-12 08:28:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ycibt",
          "author": "TheFarage",
          "text": "Congrats to the Zhipu team on a technically impressive release. The race to capabilities is running. The race to safety needs to keep pace.",
          "score": 1,
          "created_utc": "2026-02-12 09:23:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ymm3p",
          "author": "No_Count2837",
          "text": "Crazy ðŸ¥³",
          "score": 1,
          "created_utc": "2026-02-12 11:00:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o551tis",
          "author": "jugalator",
          "text": "The comments in this thread really shows the power benchmark figures have on us.\n\nIn actual use, I'm thus far kinda whelmed by GLM-5. It kinda feels like a bit smaller model than it is.\n\nUpdate: I think I see why I have this impression. GLM-5 tests at signficantly worse multilingual performance than GLM-4.7, so much that it looks like a regression/something broken: https://www.nc-bench.com/tests/language-writing It might be that it's more strongly tuned towards scientific tasks than otherwise.",
          "score": 1,
          "created_utc": "2026-02-13 10:09:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59phym",
          "author": "OmarBessa",
          "text": "It's an incredibly good model",
          "score": 1,
          "created_utc": "2026-02-14 01:38:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5grn8c",
          "author": "arabterm",
          "text": "Amazing model indeed. Thank you!",
          "score": 1,
          "created_utc": "2026-02-15 05:59:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u9hn3",
          "author": "Iory1998",
          "text": "I think China already is better than the US in the AI space, and I believe that the open-source models are also better than Gemini, GPT, and Claude. If you think about it, the usual suspects are no longer single models. They work as a system of models leveraging the power of agentic frameworks. Therefore, comparing a single model to a framework is comparing apples to oranges. ",
          "score": 0,
          "created_utc": "2026-02-11 18:13:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uhnjd",
              "author": "alexeiz",
              "text": "Are you paying for Chinese models yet?  Let's see how you vote with your wallet.",
              "score": -6,
              "created_utc": "2026-02-11 18:51:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vz52p",
                  "author": "Iory1998",
                  "text": "I use Chinese models and I don't pay a dime.",
                  "score": 3,
                  "created_utc": "2026-02-11 23:12:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4whn4z",
                  "author": "the_shadowmind",
                  "text": "I use openrouter to pay per token, and use more Chinese models.",
                  "score": 3,
                  "created_utc": "2026-02-12 00:57:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ui4wl",
          "author": "mizoTm",
          "text": "Damn son",
          "score": 1,
          "created_utc": "2026-02-11 18:53:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4twi4s",
          "author": "Odd-Ordinary-5922",
          "text": "crazy how close its gotten... Makes me think that all the US companies are holding up on huge models ",
          "score": 0,
          "created_utc": "2026-02-11 17:12:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u2rd5",
              "author": "oxygen_addiction",
              "text": "Or there is no moat.",
              "score": 25,
              "created_utc": "2026-02-11 17:42:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4u4fb0",
          "author": "Insomniac24x7",
          "text": "But will it run on an RPi and will it run Doom?!?!",
          "score": 0,
          "created_utc": "2026-02-11 17:50:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r44fzk",
      "title": "The gap between open-weight and proprietary model intelligence is as small as it has ever been, with Claude Opus 4.6 and GLM-5'",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/4rozb901icjg1.jpeg",
      "author": "abdouhlili",
      "created_utc": "2026-02-13 23:20:10",
      "score": 707,
      "num_comments": 164,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r44fzk/the_gap_between_openweight_and_proprietary_model/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5apbo6",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-14 05:50:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o595l5g",
          "author": "LelouchZer12",
          "text": "Benchmarks are not fully representative of the model strenghtes, though.",
          "score": 218,
          "created_utc": "2026-02-13 23:36:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o599kbh",
              "author": "sine120",
              "text": "At the end of the day when it comes to professional utility, I often find a few things true for me.  Bigger = better, models that ask clarifying questions = better, and fresher training data = better.\n\nFor example, Gemini is pretty far behind in benchmarks now compared to new coding open weights, but it's still really really good at handling vast amounts of information and producing insightful results in a big codebase.  \n\nGPT 5.2 benches really well, but it's horrible at communicating with the user and building the feel of confidence in what its doing, so I'd rather use Opus who checks in first to build a plan.\n\nOSS-120B still benches quite well for its size, but it often doesn't believe me and will *argue* about recent events even when told to look them up.  Its training is outdated.\n\nI haven't use enough open weight models professionally yet to know their vibe, but if they feel good to use and can handle long agentic tasks, the major US labs will struggle to be competitive.",
              "score": 77,
              "created_utc": "2026-02-14 00:00:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o59e9ni",
                  "author": "eli_pizza",
                  "text": "Maybe itâ€™s just because I end up using a lot of stuff that changes and breaks a lot, but I donâ€™t find fresher training *that* useful.\n\nIâ€™m sure I could automate it with a skill or whatever but I typically ask it to checkout dependencies locally and/or research and document best practices for anything new. \n\nI donâ€™t care much about conversational tone, but in general I much prefer it push back on things that seem off than sycophantically always agree with anything I mention. I have a macro for â€œIâ€™m going to think out loud now. Just consider what Iâ€™m saying; donâ€™t assume I want to do it yetâ€ because god forbid you ask Opus â€œCouldnâ€™t we do it like x instead?â€",
                  "score": 12,
                  "created_utc": "2026-02-14 00:28:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o599ysn",
                  "author": "Mundane_Discount_164",
                  "text": "Gpt 5.2 is a better planner. Bit you have to guide the process.",
                  "score": 5,
                  "created_utc": "2026-02-14 00:02:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ahgaa",
                  "author": "PunnyPandora",
                  "text": "Actually ture. gpt feels like talking to a fucking wall and is hard to get to do what you're asking",
                  "score": 1,
                  "created_utc": "2026-02-14 04:47:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ar6uq",
                  "author": "Fuzzy_Pop9319",
                  "text": "It is very complex to compare writing ability.  ",
                  "score": 1,
                  "created_utc": "2026-02-14 06:06:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5cbi3j",
                  "author": "mycall",
                  "text": "I wonder you explain to it the time delta in its training to today, inside the prompt, if it will go with that or argue that too.",
                  "score": 1,
                  "created_utc": "2026-02-14 14:13:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o59hudm",
                  "author": "Western_Objective209",
                  "text": "Okay but is GLM-5 on OpenCode or whatever their CLI is actually comparable to Claude Code with Opus 4.6? I haven't tried it yet but previous versions weren't too impressive",
                  "score": 1,
                  "created_utc": "2026-02-14 00:50:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5b1vxj",
              "author": "SilentLennie",
              "text": "No, but it does show the gap is getting smaller.",
              "score": 5,
              "created_utc": "2026-02-14 07:43:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o59asqa",
              "author": "Far-Low-4705",
              "text": "Also, these are old and long running benchmarks that are starting to get saturated.",
              "score": 8,
              "created_utc": "2026-02-14 00:07:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o59i74u",
              "author": "jrop2",
              "text": "Yeah all this focus on GLM 4.7 and now 5, and meanwhile I'm having the best results (open-weights-wise) with Kimi K2.5 in opencode.",
              "score": 5,
              "created_utc": "2026-02-14 00:52:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5bnggd",
                  "author": "popiazaza",
                  "text": "Artificial Analysis has a separate coding index, this chart is for general intelligence.",
                  "score": 2,
                  "created_utc": "2026-02-14 11:14:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o59stl7",
              "author": "Federal_Spend2412",
              "text": "Glm4.7<= sonnet 4.5 < Glm 5 < opus 4.5 < opus 4.6, based on feeling, I used opus on opencode, and glm is via claude code.",
              "score": 5,
              "created_utc": "2026-02-14 01:59:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o59o772",
              "author": "layer4down",
              "text": "Compared to what exactly? Are there better ways to measure and evaluate this?",
              "score": 2,
              "created_utc": "2026-02-14 01:30:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5bdptz",
                  "author": "Mkengine",
                  "text": "There are at least better benchmarks for specific use cases than artificial analysis, for example [swe-rebench](https://swe-rebench.com/) where Opus 4.6 ist #2 and GLM 5 #14, which is a much more realistic gap.",
                  "score": 5,
                  "created_utc": "2026-02-14 09:39:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5bq680",
              "author": "sparkandstatic",
              "text": "This community is disillusioned. Try using any open source to build your agents, opencode, openclawd lol it will fail like a joke",
              "score": 2,
              "created_utc": "2026-02-14 11:39:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5cc2yr",
                  "author": "mycall",
                  "text": "I have Gwen3-Coder-Next working fine with Agent Zero doing all kinds of things for me.  Using GPT-OSS-120B in parallel for second pass verification is excellent.",
                  "score": 3,
                  "created_utc": "2026-02-14 14:16:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5bvoq7",
                  "author": "Blues520",
                  "text": "Out of interest, what agent are you speaking about building?",
                  "score": 2,
                  "created_utc": "2026-02-14 12:26:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5epw4r",
                  "author": "Super_Sierra",
                  "text": "I am still in disbelief how fucking bad Open Source is at fucking basic writing tasks, much less doing any other tasks. \n\nKimi 2 and 2.5 is the only one that passes in open source on a few of my benchmarks but even then, it barely does.",
                  "score": -1,
                  "created_utc": "2026-02-14 21:46:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5bdwib",
              "author": "swaglord1k",
              "text": "that's true for both open and closed models",
              "score": 1,
              "created_utc": "2026-02-14 09:41:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5bcebc",
              "author": "MoffKalast",
              "text": "If benchmarks meant anything we'd all be using Gemini, haha.",
              "score": 0,
              "created_utc": "2026-02-14 09:25:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o59ucrn",
              "author": "Scared_Astronaut9377",
              "text": "Especially when you cherry-pick benchmarks like op.",
              "score": 0,
              "created_utc": "2026-02-14 02:09:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5bnb0u",
                  "author": "popiazaza",
                  "text": "OP took it from Artificial Analysis, which is probably the biggest entity to do benchmarks and has done this for years.",
                  "score": 1,
                  "created_utc": "2026-02-14 11:12:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5999ut",
          "author": "Lissanro",
          "text": "I think it is K2.5 that is currently the closest to the top as open weight model. GLM-5 is not bad but definitely not ahead of K2.5 which is better at longer context tasks, nuanced thinking and has vision. Also K2.5 has better performance on my rig and can run losslessly as Q4_X quant that just maps the original INT4 weights, while GLM-5 has to be quantized from BF16 since they did not do 4-bit QAT or at least FP8 training.\n\n\nThat said, GLM-5 is still a good models in its own way, it has its own flavor both in programming and creative writing, so some people may prefer it for their use cases. I am keeping it in my toolbox too because it may provide different solutions should I need them, compared to K2.5.",
          "score": 68,
          "created_utc": "2026-02-13 23:58:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59hup1",
              "author": "segmond",
              "text": "Yup, last night I had K2.5 generate code almost 4700 lines in one output context was about 80k, with everything perfect based on the input.  The recall is also insane.   Sadly, they are both the same performance for me, I'm running KimiK2.5-Q4 and GLM5-Q6",
              "score": 16,
              "created_utc": "2026-02-14 00:50:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o59h42t",
              "author": "KnifeFed",
              "text": "What about MiniMax 2.5?",
              "score": 7,
              "created_utc": "2026-02-14 00:45:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5b0771",
                  "author": "PuppyGirlEfina",
                  "text": "Minimax 2.5's not trying to be the best, it's trying to be the most efficient.",
                  "score": 19,
                  "created_utc": "2026-02-14 07:27:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5a1hpt",
                  "author": "Charuru",
                  "text": "Minimax is much lower on the AA benchmark while K2.5 and GLM5 are close to the frontier.",
                  "score": 14,
                  "created_utc": "2026-02-14 02:54:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ajp6u",
                  "author": "Fault23",
                  "text": "It's a small model",
                  "score": 5,
                  "created_utc": "2026-02-14 05:04:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5bnmkc",
                  "author": "popiazaza",
                  "text": "Sadly their official release show that they are not that good. Their benchmark is pretty much cherry picked. Still probably the most dense small model out there.",
                  "score": 2,
                  "created_utc": "2026-02-14 11:15:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o59u1sg",
              "author": "CanineAssBandit",
              "text": "i'm still glad they released GLM as BF16 because it can be fine tuned without losing a bunch of quality like if they released it only in 4bit",
              "score": 4,
              "created_utc": "2026-02-14 02:07:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5atna4",
                  "author": "Lissanro",
                  "text": "I think it is the opposite. Upcasting to BF16 if needed is easy, but doing proper 4-bit QAT is hard. I am yet to see any research that shows that fine-tuning upconverted model causes any issues except losing the original QAT. If you can link such research, please share.",
                  "score": 7,
                  "created_utc": "2026-02-14 06:27:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5bkqin",
              "author": "jonydevidson",
              "text": "Latest SWE-Rebench data disagrees.",
              "score": 2,
              "created_utc": "2026-02-14 10:47:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ewc6j",
              "author": "AriyaSavaka",
              "text": ">  losslessly as Q4_X \n\nDoesn't make any sense.",
              "score": 1,
              "created_utc": "2026-02-14 22:21:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5f3bpl",
                  "author": "Lissanro",
                  "text": "But it does for tensors that come as INT4 - it maps INT4 weights to modified Q4\\_0 - \"X\" in Q4\\_X refers to the modded quantization code to avoid loss, and the quant runs correctly on unmodified llama.cpp / ik\\_llama.cpp, so temporary source code modification only needed once to create the Q4\\_X quant. For details, refer to [https://github.com/ggml-org/llama.cpp/pull/17064#issuecomment-3521098057](https://github.com/ggml-org/llama.cpp/pull/17064#issuecomment-3521098057)",
                  "score": 1,
                  "created_utc": "2026-02-14 23:02:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o59jpt4",
          "author": "MuslinBagger",
          "text": "I have been using Kimi to rewrite my dungeon adventure porn novel and it is absolutely great. Way way better than grok. It spits out 1000 line chapters with details, great dialogue and action like nobody's business. Way better than grok, and grok was no slouch. ",
          "score": 17,
          "created_utc": "2026-02-14 01:01:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bocta",
              "author": "mambo_cosmo_",
              "text": "I am sorry you have been using Kimi for writing what? ðŸ˜³",
              "score": 15,
              "created_utc": "2026-02-14 11:22:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5celjx",
                  "author": "MuslinBagger",
                  "text": "not code",
                  "score": 14,
                  "created_utc": "2026-02-14 14:31:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o593ge0",
          "author": "Gregory-Wolf",
          "text": "Did you use both models in production on real tasks? I have. Sadly, the gap is not small. At least not in software development (analyzing huge codebase, making architectural decisions, preparing technical specs and actually coding).",
          "score": 95,
          "created_utc": "2026-02-13 23:23:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5977em",
              "author": "TheRealMasonMac",
              "text": "Yep. They're getting better, but the gap is nowhere near this close.\n\n[https://archive.li/0DMSZ](https://archive.li/0DMSZ)\n\n> Justin Lin, head of Alibaba Group Holding Ltd.â€™s Qwen series of open-source models, put at less than 20% the chances of any Chinese company leapfrogging the likes of OpenAI and Anthropic with fundamental breakthroughs over the next three to five years. His caution was shared by peers at Tencent Holdings Ltd., and at Zhipu AI, which this week helped lead Chinese large-language model makers in tapping the public market. \n>\n>â€œA massive amount of OpenAIâ€™s compute is dedicated to next-generation research, whereas we are stretched thin â€” just meeting delivery demands consumes most of our resources,â€ Lin said during a panel at the AGI-Next summit in Beijing on Saturday. â€œItâ€™s an age-old question: does innovation happen in the hands of the rich, or the poor?â€  \n>  \n>...  \n>  \n>Joining Lin in that assessment were Tang Jie, Zhipuâ€™s founder and chief AI scientist, and Yao Shunyu, who recently joined Tencent from OpenAI to lead the AI push for Chinaâ€™s most valuable company.  \n>  \n>â€œWe just released some open-source models, and some might feel excited, thinking Chinese models have surpassed the US,â€ Tang said. â€œBut the real answer is that the gap may actually be widening.â€",
              "score": 27,
              "created_utc": "2026-02-13 23:46:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5f5cvm",
                  "author": "RhubarbSimilar1683",
                  "text": "This is why euv matters so much to china. That's their bottleneck right now. Once they perfect it they will scale it like the US produced bomber planes during ww2",
                  "score": 2,
                  "created_utc": "2026-02-14 23:14:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5erlfe",
                  "author": "Super_Sierra",
                  "text": "It is incredibly frustrating how a lot of open source communities do not realize how far behind they are. Sonnet 3.5 still mogs most of open source in real world tasks, doing actual shit that isn't asking it a question. \n\nThat was released nearly two years ago, and I'd wager that some haven't even caught up to Claude 2.1 in terms of capabilities like writing. Lot of copium huffers in LocalLlama though, especially after a few big releases.",
                  "score": -1,
                  "created_utc": "2026-02-14 21:55:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o595s1u",
              "author": "lemon07r",
              "text": "Dont know why you got downvoted. What you said is correct. I use opus, kimi k2.5, minimax, etc, all extensively for various things. These benchmarks dont paint a full picture",
              "score": 31,
              "created_utc": "2026-02-13 23:37:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5apmgk",
              "author": "GlossyCylinder",
              "text": "This isn't evidence that gap isn't small. It's just your experience. Outside of the benchmark, we are hearing  from many people in the community how close the gap between open source and close source are. \n\nAnd myself  have multiple experiences where Kimi 2.5 beat Opus 4.6 \n\nFor example, I asked both models to create a PDF summarizing randomized SVD, explaining it geometrically and show its derivation. Not only did Kimi do a better job explaining the theory it also has less latex error and presents the material in a more logical order.",
              "score": 28,
              "created_utc": "2026-02-14 05:52:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5arky6",
                  "author": "Fuzzy_Pop9319",
                  "text": "That sounds right to me.  I tried measuring for a while and realized that it is not a solid thing to measure and even if I measure relative performance at 2pm, doesn't mean it will be that way at 9PM, let alone next summer.    \nThere is too large of a range in the models performance to order them without the user of at least a few bell curves IMO.",
                  "score": 1,
                  "created_utc": "2026-02-14 06:09:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5bmaio",
                  "author": "No_Afternoon_4260",
                  "text": "Yeah sometimes I feel like k2.5 has less skills/knowledge but is better at what it knows.\n\nI wouldn't know how to explain it, but if you aim at something really well represented in its training set then it can be better than opus.  \nBut opus is still a better generalist coding agent.",
                  "score": 1,
                  "created_utc": "2026-02-14 11:02:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5a4oqq",
              "author": "yes-im-hiring-2025",
              "text": "Agreed. I'm squarely in the GLM club and use it for everything personal, whereas I use claude for work. \n\nGLM-5 and Kimi-K2.5 are close to Claude Sonnet 4.5; not Opus 4.5\n\nOpus 4.6 is just miles ahead. Just fact - look at it's reasoning tokens vs GLM reasoning tokens, or how fast it adapts to your conversation. Opus is highly token efficient, very rounded in world knowledge, and a great example of self-steering with minimal supervision (ie you can ask it to generate conditions and have it reference/update/follow them the same way people can).\n\nHowever that doesn't mean GLM isn't going to catch up - it's a time saturation thing. I'm betting in 5 years we'll likely have relatively similar AI capabilities across the board amongst models, and their differentiator will be in the ways they're integrated/tuned for their specific applications.",
              "score": 7,
              "created_utc": "2026-02-14 03:16:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5f57ho",
                  "author": "RhubarbSimilar1683",
                  "text": "Sounds like it's because opus is in the 5t to 7t parameter range, and those models are not",
                  "score": 1,
                  "created_utc": "2026-02-14 23:13:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5eqq8z",
                  "author": "Super_Sierra",
                  "text": "Moonshot's Kimi 2.5 is the only one in the running and it is nowhere near Sonnet, I am sorry. The amount of times I started realizing I was doing more work to get it to do a task than the task itself was frustrating. \n\nIt sure looks good on benchmarks, and I swear that that is the only fucking thing they are training it on because the real world useage fucking sucks.",
                  "score": 0,
                  "created_utc": "2026-02-14 21:50:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o594eev",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 12,
              "created_utc": "2026-02-13 23:29:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5952q8",
                  "author": "Gregory-Wolf",
                  "text": "A you talking about Claude Opus 4.6 (the API version), or something else?",
                  "score": 4,
                  "created_utc": "2026-02-13 23:33:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5bw5ud",
                  "author": "Blues520",
                  "text": "That's very interesting to know. I always thought it was just one model.",
                  "score": 1,
                  "created_utc": "2026-02-14 12:29:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ds1mg",
                  "author": "OmarBessa",
                  "text": "GLM has the best chance at it, due to how many models they have on the API.\n\n",
                  "score": 1,
                  "created_utc": "2026-02-14 18:45:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o595y0k",
                  "author": "lemon07r",
                  "text": "What does this have to do with anything. He's talking about the model itself. Not any of the claude software, which btw, claude code can use other models, not just opus. ",
                  "score": -1,
                  "created_utc": "2026-02-13 23:38:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5bto1y",
                  "author": "_supert_",
                  "text": "Openclaw is doing that for me. Honestly it's heroic.",
                  "score": 0,
                  "created_utc": "2026-02-14 12:09:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5dgwv4",
              "author": "vashata_mama",
              "text": "Being too poor for opus - is GLM/kimi better than sonnet/gpt5.3-codex?",
              "score": 1,
              "created_utc": "2026-02-14 17:50:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o59f567",
          "author": "Jazz8680",
          "text": "now if only I had a terabyte and a half of vramÂ ",
          "score": 11,
          "created_utc": "2026-02-14 00:33:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bcoz9",
              "author": "MoffKalast",
              "text": "Those who say the gap is small have never seen the size and price of a DGX B200. Absolute unit.",
              "score": 6,
              "created_utc": "2026-02-14 09:29:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o59v87k",
          "author": "xor_2",
          "text": "Testing quantized GLM 4.7 Flash and compared to what we were amazed last year the progress is just incredible.\n\nAnyone who made bigger investments last year is today likely very happy.",
          "score": 5,
          "created_utc": "2026-02-14 02:14:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b84h2",
              "author": "dodistyo",
              "text": "It is pretty decent, I build my PC a months a go with RX 7900 XTX.\nI've been using GLM 4.7 flash and sometimes devstrall small 2 2512 for coding.\n\nof course for really complex task the proprietary model is more capable.\n\nBut i really like it, seeing the current state and what it will be in the future for openweight model.",
              "score": 1,
              "created_utc": "2026-02-14 08:43:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5bpppk",
                  "author": "Monad_Maya",
                  "text": "GLM Flash vs Qwen3 Coder Next, which one is better in your opinion?",
                  "score": 1,
                  "created_utc": "2026-02-14 11:35:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5bt6ny",
                  "author": "xor_2",
                  "text": "For coding not sure. Not how I use my LLMs. IMHO best to keep oneself sharp and LLMs don't help with that. In fact they make people kinda dumb in the long run.\n\nI treat mr. Clippy Claude wanna-be as google assistant. Often I forget name of some concept or want something to be explained because documentation for it isn't top quality and LLMs can be useful for that. Especially when pasted documentation.\n\nLast year anything you could run seemed inadequate and even bigger models which I could get free (as in chatgpt or other chat sites) seems to be worse than what my computer can run today. Knowledge gap isn't as big when these LLMs can google stuff making them really useful and still quite a bit more secure than posting stuff to internet verbatim.\n\nI wonder if next year we will have similar progress and have then new small models outperform at least free previews of GPT5",
                  "score": 1,
                  "created_utc": "2026-02-14 12:05:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5c5ib3",
          "author": "LocoMod",
          "text": "When was the last time the benchmarks were updated to ramp up difficulty? I expect most models to saturate existing benchmarks. The capability divide will not be present until the way we measure is updated to reflect the current state of the art.\n\nIf you really want to see the real performance gap then look at ArcAGI2.\n\nYou donâ€™t really read about Chinese models competing or solving world IMO problems, discovering protein structures, beating world class Go players, or towing the top of Code Forces.\n\nThatâ€™s because the frontier western models have already blown past the capabilities the common folks like us use them for, and the benchmarks that would show that have yet to be developed.\n\nAt some point all models will be â€œgood enoughâ€ for the small problems people work on. And they will come in here and claim parity was achieved and open weights caught up. But what that really means is â€œthis model is good enough for my high school level problems for my high school level educationâ€.",
          "score": 5,
          "created_utc": "2026-02-14 13:36:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5960fh",
          "author": "segmond",
          "text": "The gap doesn't matter much, it has been irrelevant for a better part of at least the last 1 year.\n\n  \nA well capable person with local model will crush 99.9% of people using proprietary model.  The world doesn't have an edge on us because of proprietary models.  ",
          "score": 23,
          "created_utc": "2026-02-13 23:39:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5arqic",
              "author": "ReasonablePossum_",
              "text": ">well capable person with local model.\n\nYou mean a rich one with enough GPUs to run a capable model :'(",
              "score": 19,
              "created_utc": "2026-02-14 06:10:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5cc8il",
                  "author": "mycall",
                  "text": "$3000 will get you there.",
                  "score": 0,
                  "created_utc": "2026-02-14 14:17:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o59vqfg",
          "author": "ralphyb0b",
          "text": "Iâ€™ve been playing with MiniMax and itâ€™s terrible. Nothing close to Opus.Â ",
          "score": 9,
          "created_utc": "2026-02-14 02:17:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bpjqg",
              "author": "Monad_Maya",
              "text": "I ran a smaller/lower quant but yeah, I wasn't impressed at all.",
              "score": 1,
              "created_utc": "2026-02-14 11:33:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5cqkxf",
              "author": "power97992",
              "text": "M2.5 is Â  even worse than Â M2.1 for some tasks",
              "score": 1,
              "created_utc": "2026-02-14 15:37:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5aeycp",
          "author": "Ylsid",
          "text": "Incoming Dario ragepost",
          "score": 6,
          "created_utc": "2026-02-14 04:28:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5chdrw",
          "author": "DocumentFun9077",
          "text": "Yes, the gap has been closing in.  \nBut do we realize that to run those models locally require crazy expensive rigs to achieve their potential, or to even run in the first place?",
          "score": 3,
          "created_utc": "2026-02-14 14:47:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59mpky",
          "author": "OmarBessa",
          "text": "GLM is a beast",
          "score": 4,
          "created_utc": "2026-02-14 01:20:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59gzq1",
          "author": "ortegaalfredo",
          "text": "Do not drink the cool aid. In real life local models are quite far away.",
          "score": 6,
          "created_utc": "2026-02-14 00:44:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59o6sx",
          "author": "Iory1998",
          "text": "I strongly believe that the gap has already been closed as open-weight models are single models while the closed ones are agentic frameworks. Imagine  GLM-5 with different sizes working as an agentic system!\n\nDo you still think there is a gap?",
          "score": 6,
          "created_utc": "2026-02-14 01:29:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d9l1y",
          "author": "siegevjorn",
          "text": "This may be true, but I guess the real problem is that the requirement to host local model is becoming more and more costly. When llama 3 70b came out, you could just run it on machine with two 3090s. Now, glm5 is hugeâ€”744b-a40b. To host q4k_m (456gb), you'll need two mac 256gb studios (not enough for long context though) or four dgx sparks. $10k to $12k for just setting things up. It may not be much for business investment, but certainly the bar keeps getting higher, which prevents attracting larger audience. Claude code max being $100/month, in the end it is 10 year worth of claude code subscription.",
          "score": 2,
          "created_utc": "2026-02-14 17:13:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bbicp",
          "author": "DT-Sodium",
          "text": "AI companies are investing hundreds of billions in infrastructure aiming to sell trillions in services at some point. Good luck with that, smart companies will invest in their own self-hosted services. That's already what mine does.",
          "score": 3,
          "created_utc": "2026-02-14 09:17:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59a45f",
          "author": "ready_to_fuck_yeahh",
          "text": "I love glm-5, made a personal project of more than 10,000 lines of code and it work flawlessly.",
          "score": 4,
          "created_utc": "2026-02-14 00:03:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59h0ew",
              "author": "KnifeFed",
              "text": "Why do people keep using \"lines of code\" as some sort of metric? It means nothing.",
              "score": 8,
              "created_utc": "2026-02-14 00:44:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5a6cdh",
                  "author": "CuriouslyCultured",
                  "text": "It doesn't mean nothing, particularly if you don't instruct the models to pad LoC. It's correlated with work done, if you don't have any other information, LoC does provide a useful data point.",
                  "score": 4,
                  "created_utc": "2026-02-14 03:27:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5af1zx",
                  "author": "Ylsid",
                  "text": "Why do people use \"works flawlessly\" as a metric too? That says nothing about code quality\n\nLoC tells you about context window I guess",
                  "score": 2,
                  "created_utc": "2026-02-14 04:29:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5abdb6",
                  "author": "No-Key2113",
                  "text": "Lines of code isnâ€™t a good metric- the task accomplishment is the key part. Ai doesnâ€™t need to minimize code within reason as long as it gets done.",
                  "score": 1,
                  "created_utc": "2026-02-14 04:02:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o59me6a",
                  "author": "ready_to_fuck_yeahh",
                  "text": "I'm not a programmer but I see people talking in terms of code, but I used line of code as metrics is because glm 5 did it for me in few steps most of which was discussion and then one shot coding, it is divided in 9 modules, of which two are decision engine, connected to db.",
                  "score": -1,
                  "created_utc": "2026-02-14 01:18:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5962tq",
          "author": "ResidentPositive4122",
          "text": "\"On paper\". Or benchmarks :) But in real life tasks it's actually increasing. The scale of compute and data that the big labs have thrown at this is huge, and the gap seems to get bigger, IMO. The graph kinda shows it, mid 24 we were \"6 months\" away, but today I'd say we're at least 1 year out, if not more. Benchmarks aren't everything, and while extremely impressive and useful, open models are just very \"stubborn\" and \"focused\". If you take them slightly out of the typical benchmark cases, they get lost way more than SotA models. Not to mention useful context and world knowledge, where goog is king still. (not even gemini3, there are currently no open models that can match 2.5 in real world throw documents at it and ask it questions tasks).",
          "score": 4,
          "created_utc": "2026-02-13 23:39:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59e0uz",
          "author": "Dear-Relationship-39",
          "text": "There is always a gap between banchmark and real use experience.",
          "score": 4,
          "created_utc": "2026-02-14 00:27:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o593t4d",
          "author": "FPham",
          "text": "The problem I see is that \"open source\" is a business strategy at this moment. We all benefit, yeah, until the Chinese companies decide they got enough traction and free advertisement to start following in openAi/anthropic steps and keep the weights as the heavily guarded golden goose behind a paywall.\n\nI mean the open source strategy is working, but ti also means we might be close to the endgame.",
          "score": 4,
          "created_utc": "2026-02-13 23:25:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o596cc5",
              "author": "RhubarbSimilar1683",
              "text": "Is that because minimax took a day to make the weights available?",
              "score": 5,
              "created_utc": "2026-02-13 23:41:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o59n76c",
              "author": "gjallerhorns_only",
              "text": "Maybe, maybe not. Red Hat Enterprise, Canonical, Mozilla and others run their whole business around Open Source software and have for decades.",
              "score": 3,
              "created_utc": "2026-02-14 01:23:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5cs6i1",
                  "author": "touristtam",
                  "text": "On the other hand the Chinese have had experience on the whole be the sole competitor on the market by having massively subsidised pricing strategy in other industries.",
                  "score": 1,
                  "created_utc": "2026-02-14 15:45:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o59jr7j",
              "author": "Canchito",
              "text": "Unlikely. People said z.ai would go closed with GLM-5, and that didn't happen. The proprietary-closed strategy reflects the actual monopoly position of Anthropic, Google, and OpenAI. That can't simply be emulated, because it rests on an advantage in computing power enforced by trade barriers.",
              "score": 2,
              "created_utc": "2026-02-14 01:01:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5bgbzg",
                  "author": "Ok_Warning2146",
                  "text": "Well Zhipu just got US$500M from its HK IPO, so I believe they can afford to release free model up to GLM 6.",
                  "score": 1,
                  "created_utc": "2026-02-14 10:04:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o59e7qp",
          "author": "Fun_Smoke4792",
          "text": "again ðŸ˜‚ how many times? After Deepseek, every Chinese big model has ALMOST no gap with the top models. I hope this is real. I do want to believe this is not hype. But this thing never happened. And posts like this are like AI slop I guess.",
          "score": 2,
          "created_utc": "2026-02-14 00:28:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bvc3q",
          "author": "QuackerEnte",
          "text": "https://preview.redd.it/52uxth0rdgjg1.png?width=852&format=png&auto=webp&s=b031040c5402069810037ee4cfbea4ba907b04a8\n\ncan you guess whats here? Exactly, overlap of China vs USA and open source vs proprietary AI models. ðŸ¤” ðŸ¤” ðŸ¤”",
          "score": 2,
          "created_utc": "2026-02-14 12:23:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bwhiw",
              "author": "abdouhlili",
              "text": "GDP?",
              "score": 1,
              "created_utc": "2026-02-14 12:32:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o59717n",
          "author": "crusoe",
          "text": "Gemini 3 deep think widened it again followed by rumored 3.x models coming soon.",
          "score": 3,
          "created_utc": "2026-02-13 23:45:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o597yip",
              "author": "ResidentPositive4122",
              "text": "80+ on arc-agi2 semi-private. It's insane.",
              "score": 1,
              "created_utc": "2026-02-13 23:50:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5jhiyp",
          "author": "Tech-Dack-Akhil",
          "text": "I have a doubt like all these are doing a great job on coding and domain specific tasks like research etc.. but my doubt is that big OSS models are mixture of expert models where they expertise in different domains but lack cross domain Knowledge. So for agents workflows and automation caw we trust these big models on the reliability grounds where claude models are having a great consistency",
          "score": 1,
          "created_utc": "2026-02-15 17:39:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b8nxo",
          "author": "fairydreaming",
          "text": "Meanwhile:\n\nhttps://preview.redd.it/b8thxsidbfjg1.png?width=643&format=png&auto=webp&s=2f0d6caba45771b471363dd2df81983ba306b2d9\n\nSource: [https://huggingface.co/inclusionAI/Ring-2.5-1T](https://huggingface.co/inclusionAI/Ring-2.5-1T)",
          "score": 1,
          "created_utc": "2026-02-14 08:48:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bw8zz",
              "author": "abdouhlili",
              "text": "Ring is Alibaba, they know how to build models.",
              "score": 1,
              "created_utc": "2026-02-14 12:30:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o59uaiq",
          "author": "Public_Bill_2618",
          "text": "Totally agree. The 'Vibe Check' gap is often wider than the benchmark gap. Open weights are catching up on knowledge retrieval, but proprietary models (like Claude 3.5 or GPT-4) still feel significantly more robust on complex, multi-step reasoning tasks. It's about reliability, not just peak performance.",
          "score": 1,
          "created_utc": "2026-02-14 02:08:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59fj5x",
          "author": "Snoo_64233",
          "text": "Nope. If you take into account visual tasks, which almost every human task relies on, the gap is wider than ever. Here are the example from AIstudio. Pay attention to \"Thought\" process section. You will probably need to log into Gmail to view the content:\n\n[Example 1](https://aistudio.google.com/prompts/1TkfCl-2HvgaBwxe8MtOYJRpoZjsaXd_G)  \n[Example 2](https://aistudio.google.com/prompts/1dF2Y1tN7XxViOyo_mxU_TJdXD5uB0jkc)\n\nÂ Gemini can learn visual task just by comparing and contrasting multiple reference input/output image pairs, without any hints or explicit description, and then able to apply that learnt pattern onto the target image. Basically it is soft-LoRA (or few-shot visual learner). The entire local Image/video gen AI space revolves around creating LoRA for all kinds of tasks. This thing just act like mother of all LoRA on the spot.",
          "score": 1,
          "created_utc": "2026-02-14 00:36:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59i35b",
              "author": "segmond",
              "text": "K2.5 can do this as well.",
              "score": 4,
              "created_utc": "2026-02-14 00:51:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o59jcyq",
                  "author": "Snoo_64233",
                  "text": "It can't. If it can, they wouldn't be officially partnering with Google for their NBP-powered slides. Being able to vaguely understand images is one thing, but being able to spot/discern the patterns and apply that learnt pattern is another.",
                  "score": 0,
                  "created_utc": "2026-02-14 00:59:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o59cnqa",
          "author": "PerspectiveWest7420",
          "text": "The convergence is real, but I think what matters more than the benchmark gap is the *task-dependent* gap. For coding benchmarks (SWE-bench, HumanEval+), open models like GLM-5 and DeepSeek are basically neck and neck with Opus. For creative writing and instruction following, proprietary still has an edge. For math/reasoning, it depends heavily on whether you enable chain-of-thought.\n\nThe interesting question is: does it even matter anymore for 70-80% of production workloads? Most real-world API traffic is classification, extraction, summarization, translation â€” tasks where even much smaller models perform identically to frontier. The gap only matters for the genuinely hard 10-20% of queries.\n\nIMO the real win from this convergence is that developers now have *options*. Two years ago you basically had GPT-4 or nothing. Now you can pick based on latency, cost, privacy, context length, or just personal preference. Competition is beautiful.",
          "score": 0,
          "created_utc": "2026-02-14 00:19:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a74o2",
          "author": "bakasannin",
          "text": "The gap between an average person's hardware to run local llms and acheive a reasonable output and tps compared to Big AI is even bigger.",
          "score": 0,
          "created_utc": "2026-02-14 03:32:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59djyw",
          "author": "cuberhino",
          "text": "can glm-5 run on a 3090?",
          "score": -1,
          "created_utc": "2026-02-14 00:24:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59nfoi",
              "author": "redditscraperbot2",
              "text": "A as in singular? No",
              "score": 1,
              "created_utc": "2026-02-14 01:25:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5d0yng",
          "author": "Top_Fisherman9619",
          "text": "frontier labs are holding back",
          "score": 0,
          "created_utc": "2026-02-14 16:30:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h2yd3",
          "author": "rorykoehler",
          "text": "Has anyone here tried coding with Claude Opus 4.6 Thinking, K2.5, or GLM-5 in real projects?\n\nSo far, Opus 4.6 Thinking is the first coding model thatâ€™s impressed me enough to feel worth paying the premium for. Iâ€™ve got a 128GB RAM Strix Halo machine and Iâ€™m thinking of testing these locally, but Iâ€™d love to hear how theyâ€™ve worked for others in day-to-day coding (not benchmarks).\n\nIf youâ€™ve used any of them:\n\n* What kind of work were you doing?\n* How did they hold up in practice?\n* Which exact versions are you running?",
          "score": 0,
          "created_utc": "2026-02-15 07:44:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59cmxm",
          "author": "Crypto_Stoozy",
          "text": "Whoâ€™s actually able to run that glm 5 model on their own equipment though",
          "score": -2,
          "created_utc": "2026-02-14 00:18:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o59vze2",
              "author": "CanineAssBandit",
              "text": "anyone with time *or* money. any model runs at home if you've got hours to wait on a reply",
              "score": 6,
              "created_utc": "2026-02-14 02:19:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o59wo70",
                  "author": "Crypto_Stoozy",
                  "text": "Thatâ€™s not even true it literally will not run if it canâ€™t load across enough memory. It requires 1.5TB for BF16 precision. Thatâ€™s vram or ram.",
                  "score": -1,
                  "created_utc": "2026-02-14 02:23:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5cjs0c",
          "author": "FluidBoysenberry1542",
          "text": "it's so small lol what's a bunch of lies, in practice it doesn't match at all, never has, only on a small subset (edge case like math or markdown generation), it's good if you don't have anything else. If Claude would be priced at 30$ per month for the max plan, almost no one would use GLM.",
          "score": -1,
          "created_utc": "2026-02-14 15:01:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59dd6d",
          "author": "JagerGuaqanim",
          "text": "Good. Now how to fit 744B parameters into 11GB VRAM and 32GB RAM? :))",
          "score": -2,
          "created_utc": "2026-02-14 00:23:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59757n",
          "author": "Fearless-Elephant-81",
          "text": "Swerebench tells the true story. But imo open models are much closer to closed ones. Since the beginning of these models.",
          "score": -2,
          "created_utc": "2026-02-13 23:45:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59k6hn",
          "author": "PerspectiveWest7420",
          "text": "The convergence is real and the implications for production AI are massive. When open-weight models were clearly behind, the decision was simple: pay the API premium for quality. Now the calculus is completely different.\n\nFor most production workloads the interesting question is not which model is best on benchmarks. It is which model gives acceptable quality at the lowest total cost of ownership. And that answer increasingly favors open-weight models for the 70-80 percent of tasks that do not require frontier reasoning.\n\nThe remaining gap matters most for:\n- Extended multi-step reasoning chains\n- Complex code generation with architectural decisions\n- Nuanced analysis where missing a subtlety has real consequences\n\nFor everything else (translation, summarization, classification, simple Q&A, data extraction) the gap is functionally zero. A well-prompted GLM-5 or Qwen3 handles these identically to Opus at a fraction of the cost.\n\nThe real winner from this convergence is anyone building AI applications. Competition is driving prices down across the board and giving developers genuine choices instead of single-provider lock-in. Two years ago you picked OpenAI or you were making compromises. Now you have 5-6 genuinely competitive options at every tier.",
          "score": -3,
          "created_utc": "2026-02-14 01:04:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r28xxz",
      "title": "GLM-5 scores 50 on the Intelligence Index and is the new open weights leader!",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/gauvtw6qfxig1.jpeg",
      "author": "abdouhlili",
      "created_utc": "2026-02-11 20:40:32",
      "score": 641,
      "num_comments": 141,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r28xxz/glm5_scores_50_on_the_intelligence_index_and_is/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4v70fj",
          "author": "abdouhlili",
          "text": "GLM-5 has the LOWEST hallucination rate on AA-Omniscience\n\nhttps://preview.redd.it/q8za1v0whxig1.png?width=1828&format=png&auto=webp&s=1c4bf3c1c6c6590ee9ded2466ca45d01d8a81b23",
          "score": 199,
          "created_utc": "2026-02-11 20:52:39",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4vacxi",
              "author": "LagOps91",
              "text": "huge if true. getting LLMs to not make shit up is one of the most significant open problems.",
              "score": 97,
              "created_utc": "2026-02-11 21:08:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vdrdk",
                  "author": "DistanceSolar1449",
                  "text": "Itâ€™s not quite an open problem anymore. Recent papers mostly solved it. Most notably https://arxiv.org/pdf/2509.04664 (but plenty of people talked about similar stuff before this paper came out, Iâ€™m sure I can find one of my comments that predates this paper)\n\nItâ€™s pretty simple, to be honest. The reward function always rewarded hallucinations. \n\nItâ€™s like a teenager taking a standardized test guessing filling in a random multiple choice answer, rather than leaving it blank. Does that make the teenager hallucinating? Not really, it just statistically is in the favor of the test taker if they have any information above random choice: theyâ€™re incentivized to guess at an answer. \n\nThe solution is simple, to set the reward function to favor epistemological uncertainty, and punish confident errors harshly. \n\nOpenAI basically grades answers no longer like a multiple choice test, but rather gives partial credit to correct levels of uncertainty.",
                  "score": 109,
                  "created_utc": "2026-02-11 21:25:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4y2276",
                  "author": "s101c",
                  "text": "Doesn't help if shit was made up in the training data, but impressive nonetheless.",
                  "score": 2,
                  "created_utc": "2026-02-12 07:41:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4v9d4s",
              "author": "4hanni",
              "text": "Wow, this is huge. My experience with the major closed source models - Gemini 3 Pro, GPT 5.2, Claude Sonnet 4.5 and Opus 4.5 - is pretty consistent with that benchmark in terms of hallucinations, e.g. I could not understand the hype around Gemini 3 Pro - pretty smart model but not really usable for me because of bad prompt adherence, short outputs and terrible hallucination rate (Nano Banana Pro is great tho).",
              "score": 14,
              "created_utc": "2026-02-11 21:04:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4vqz20",
              "author": "Fault23",
              "text": "https://preview.redd.it/25rt4wb5zxig1.png?width=196&format=png&auto=webp&s=e6939454c195e7e7f2cdf1769dcf46a467dcfe0c",
              "score": 5,
              "created_utc": "2026-02-11 22:29:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4vqdbo",
              "author": "LazloStPierre",
              "text": "At this point literally the only benchmark I take seriouslyÂ ",
              "score": 3,
              "created_utc": "2026-02-11 22:26:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4xm7gy",
              "author": "HenkPoley",
              "text": "Most of that is from rejecting to answer.",
              "score": 1,
              "created_utc": "2026-02-12 05:21:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4yc2uj",
                  "author": "Front_Eagle739",
                  "text": "Im good with it refusing to answer if it knows it will be making it up?",
                  "score": 2,
                  "created_utc": "2026-02-12 09:19:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4xt1xv",
              "author": "bnm777",
              "text": "Look at all of the results -not as good as you're making it out to be and you know it\n\n\nhttps://artificialanalysis.ai/evaluations/artificial-analysis-intelligence-index",
              "score": 1,
              "created_utc": "2026-02-12 06:18:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ymwtl",
              "author": "Sockand2",
              "text": "Tested on search and hallucinated wildly. I dont give more reliability to benchmarks (or better call them shitbenchs). Too much money in play",
              "score": 1,
              "created_utc": "2026-02-12 11:02:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4z0jom",
              "author": "Immediate_Occasion69",
              "text": "and their previous version was one of the highest! they definitely cooked with this one",
              "score": 1,
              "created_utc": "2026-02-12 12:48:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4v67mx",
          "author": "Turbulent_Pin7635",
          "text": "So the open-source is just centimeters away from the closed ones... And there are even some nukes to be released =)",
          "score": 98,
          "created_utc": "2026-02-11 20:48:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4vcmcd",
              "author": "No_Swimming6548",
              "text": "I think it's obvious that's Chinese CAUGHT American companies in 2026.\n\nEdit: typo",
              "score": 46,
              "created_utc": "2026-02-11 21:19:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vfumr",
                  "author": "Turbulent_Pin7635",
                  "text": "God bless that amazing country. Let's celebrate this night with BÄ›ijÄ«ng KÇŽoyÄ!",
                  "score": 23,
                  "created_utc": "2026-02-11 21:35:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4yjw9o",
                  "author": "SerdarCS",
                  "text": "They're still 3-6 months behind. They will be caught up if and when people actually switch from gpt and claude to chinese models NOT for price or open source, but for actual capability.",
                  "score": 2,
                  "created_utc": "2026-02-12 10:35:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4z3gys",
              "author": "amunozo1",
              "text": "Open weights*. I wonder if, once the Chinese companies surpass the American ones, they start closing their models to maintain this distance.",
              "score": 3,
              "created_utc": "2026-02-12 13:07:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4z3hj3",
              "author": "amunozo1",
              "text": "Open weights*. I wonder if, once the Chinese companies surpass the American ones, they start closing their models to maintain this distance.",
              "score": 1,
              "created_utc": "2026-02-12 13:07:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4v6nl3",
          "author": "Turbulent_Pin7635",
          "text": "I would love if someday when announcing it they publish how much memory it is needed to run the thing. =(",
          "score": 28,
          "created_utc": "2026-02-11 20:50:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4vavqn",
              "author": "SkullkidV1",
              "text": "If you have to ask you cant afford it /s",
              "score": 60,
              "created_utc": "2026-02-11 21:11:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vg714",
                  "author": "ResidentPositive4122",
                  "text": "If you have to ask you're out of kidneys already.",
                  "score": 7,
                  "created_utc": "2026-02-11 21:36:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4vh04k",
                  "author": "abdouhlili",
                  "text": "Laughing in 8x RTX Pro 6000.",
                  "score": 10,
                  "created_utc": "2026-02-11 21:40:32",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o4vfywg",
                  "author": "Turbulent_Pin7635",
                  "text": "Laughing in M3 ultra",
                  "score": 3,
                  "created_utc": "2026-02-11 21:35:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4vhi8j",
              "author": "sid_276",
              "text": "8xH200s. The FP8 of vLLM is about 800GB give or take, before any KV cache",
              "score": 12,
              "created_utc": "2026-02-11 21:42:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vhv55",
                  "author": "Turbulent_Pin7635",
                  "text": "Q4 + M3 ultra. I don't want to be a server... Just write papers privately. =)",
                  "score": -4,
                  "created_utc": "2026-02-11 21:44:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4v9mp0",
              "author": "JaredsBored",
              "text": "Rule of thumb, you need 2x the ram of parameter count to run fp16 (plus some for kv cache). Q8/Fp8 is about 1x ram of parameter count. An rtx pro 6000 plus a 4th/5th gen epyc with 768GB ram could run this at Q8 with decent speeds. Q4 is definitely doable on a 512GB Mac",
              "score": 10,
              "created_utc": "2026-02-11 21:05:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vg7jh",
                  "author": "Turbulent_Pin7635",
                  "text": "Thxs I'm in the later situation",
                  "score": 2,
                  "created_utc": "2026-02-11 21:36:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vrbv5",
          "author": "Fault23",
          "text": "https://preview.redd.it/srwy9q6ezxig1.png?width=535&format=png&auto=webp&s=34f29d9afe939a1bc539125608d13b00fc919906\n\nand with the price that's rougly equal to gemini 3 flash",
          "score": 16,
          "created_utc": "2026-02-11 22:31:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wkk01",
              "author": "Damakoas",
              "text": "That's the starting price. Open models over API tend to go way down in price very quickly. ",
              "score": 8,
              "created_utc": "2026-02-12 01:15:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ye1uz",
                  "author": "SwimmingSquare7933",
                  "text": "But i think it is very cheap, we should left some space for the open source company to earn some money to live, right? ",
                  "score": 1,
                  "created_utc": "2026-02-12 09:39:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xe4fm",
          "author": "Comfortable-Rock-498",
          "text": "Less than 3 months ago, Gemini-3-pro preview launched with huge anticipation and was supposedly performing off-the-charts in everything. Now an open-weight model overtakes it in terms of overall performance WHILE beating it and everything else in terms of low hallucination. This is consequential. ",
          "score": 13,
          "created_utc": "2026-02-12 04:20:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vk1er",
          "author": "Alex_1729",
          "text": "GLM5 thinking equals Opus Thinking? Damn! And their plans are so affordable, I might subscribe now.",
          "score": 22,
          "created_utc": "2026-02-11 21:54:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wv7jx",
              "author": "frozandero",
              "text": "Only downside is that they lack the gpu infrastructure so requests are slower, and can sometimes hang for a few seconds randomly during high usage times.",
              "score": 10,
              "created_utc": "2026-02-12 02:20:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4xw08b",
                  "author": "Alex_1729",
                  "text": "That's why I'm waiting a bit until they roll it out properly.",
                  "score": 2,
                  "created_utc": "2026-02-12 06:45:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vhcw3",
          "author": "sid_276",
          "text": "Open source is now trailing just about 3 months behind closed source, and closing that gap quickly. DeepSeek v4 comes after lunar new year so end of February btw. And I have heard great things.",
          "score": 30,
          "created_utc": "2026-02-11 21:42:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4vqwge",
              "author": "ConnectionDry4268",
              "text": "Isn't it going 1 month-6 month behind. Cause R1 was closest one that almost caught upto the Closed source",
              "score": 5,
              "created_utc": "2026-02-11 22:29:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vyljb",
                  "author": "sid_276",
                  "text": "So it caught up with 5.2 xhigh. Which was released at start of December. So trailing 2-3 months but as I said gap getting smaller very fast",
                  "score": 7,
                  "created_utc": "2026-02-11 23:09:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vqvoh",
          "author": "Fault23",
          "text": "can't wait for the minimax m2.5 and new deepseek's results",
          "score": 7,
          "created_utc": "2026-02-11 22:28:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v5c58",
          "author": "abdouhlili",
          "text": "GLM-5 beating Opus 4.5 and GPT-5.2-xhigh.\n\nDeepseek-V4 will use same DSA architecture..... But will be BIGGER.",
          "score": 42,
          "created_utc": "2026-02-11 20:44:36",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4v9cmc",
              "author": "oxygen_addiction",
              "text": "We have no idea what DS V4 will be. Let's not get hyped for nothing. Nobody has a moat at this point, not even Anthropic/OpenAI/Google.",
              "score": 25,
              "created_utc": "2026-02-11 21:03:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vbrna",
                  "author": "abdouhlili",
                  "text": "You didn't read enough DeepSeek white papers.",
                  "score": 6,
                  "created_utc": "2026-02-11 21:15:36",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wf2rn",
              "author": "zball_",
              "text": "DeepSeek v4 will apparently be some extremely sparse attention and have like 1M ctxlen.",
              "score": 2,
              "created_utc": "2026-02-12 00:42:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4y2qhk",
              "author": "Zeeplankton",
              "text": "As long as deepseek stays as stupid cheap as it currently is. Lol",
              "score": 1,
              "created_utc": "2026-02-12 07:47:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50for2",
              "author": "1998marcom",
              "text": "You sure it will be using DSA and not NSA?",
              "score": 1,
              "created_utc": "2026-02-12 17:11:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4vfl9d",
          "author": "bartskol",
          "text": "ðŸ‘€ Google  ðŸ‘€",
          "score": 6,
          "created_utc": "2026-02-11 21:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vwoxf",
          "author": "abu_shawarib",
          "text": "Big if true",
          "score": 6,
          "created_utc": "2026-02-11 22:59:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xfr91",
          "author": "Late_Hour2838",
          "text": "honestly is this even a gap anymore\n\nI get that there still hasn't been a moment where an open model has beat every closed lab to be number one for once, and especially when we count what the closed labs have in the works or ready for launch they might not let that happen  \n  \nbut still this progress is insane",
          "score": 4,
          "created_utc": "2026-02-12 04:32:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wiz15",
          "author": "callme__v",
          "text": "The sweet $8 plan per quarter is gone ðŸ˜”. (Even though it has no GLM-5). I guess need to try via API first. The real-world performance hasn't lived upto to my expections (Eg. Kimi K2.5,GLM 4.7) in the past (Artificial Analysis ratings).\n\nAny one has any source to discounted access to GLM-5? Use cases: Claude code/ openclaw",
          "score": 3,
          "created_utc": "2026-02-12 01:05:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yei2x",
              "author": "SwimmingSquare7933",
              "text": "I bought the GLM Max, and I think it is pretty good to enjoy. Anyway, I'm not sure if it will give some savings, because the Chinese New Year is coming soon",
              "score": 1,
              "created_utc": "2026-02-12 09:43:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4yzu6v",
              "author": "True-Shelter-920",
              "text": "just get the max at this point for 1 month after testing with it on openrouter, upgrade if u own a pro/lite plan ",
              "score": 1,
              "created_utc": "2026-02-12 12:43:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4w80sk",
          "author": "doradus_novae",
          "text": "BRUHHHHH GIVE ME THE SWEET SWEET AMBROSIA\n\n",
          "score": 2,
          "created_utc": "2026-02-12 00:01:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xq1g0",
          "author": "zhaolionsh",
          "text": "In my own codebase (which includes both front-end and back-end repositories, about 300,000 lines of code), I tried to implement a back-end API and a front-end page. The API calls didn't work (due to some complex front-end and back-end field and type mappings, although these were already included in the API comments). It took about 30 communications to barely get it implemented (no time to deal with the UI). I feel like it's even weaker than Sonnet 4.5... I'm not sure if it's an issue with using it through Claude Code. Also, I bought the coding plan, but there are still occasional interruptions, and I have to take over and type \"continue\". Overall, the experience is still quite far from ideal. The price is barely reasonable, but the insufficient computing power and programming ability still haven't met my passing requirements. Keep up the good work, put a lot of pressure on Anthropic and OpenAI.",
          "score": 2,
          "created_utc": "2026-02-12 05:52:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xuz4w",
          "author": "Potential_Block4598",
          "text": "1/20th of the price ?!!!",
          "score": 2,
          "created_utc": "2026-02-12 06:35:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o504m3z",
          "author": "easylifeforme",
          "text": "Can someone answer what sort of system would actually be needed to run this model? Open source but most likely needs to be ran on a rented server?",
          "score": 2,
          "created_utc": "2026-02-12 16:19:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vdp6c",
          "author": "INtuitiveTJop",
          "text": "And here I am rediscovering Gemma 3 with equal excitement",
          "score": 2,
          "created_utc": "2026-02-11 21:24:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yaitm",
              "author": "kaisurniwurer",
              "text": "It's a good one, but the lack of system prompt (weird implementation of it) annoys the hell out of me.",
              "score": 5,
              "created_utc": "2026-02-12 09:03:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ymsgy",
                  "author": "INtuitiveTJop",
                  "text": "Absolutely, I am using it because it doesnâ€™t hit those triggers that make it sound like ai and its creative writing is good. For coding itâ€™s terrible, it also canâ€™t handle long context. But I can get it opus 4.6 writing and it cleans it up really well. Itâ€™s got a very niche use for me but I think Iâ€™ll use it for a long time.",
                  "score": 3,
                  "created_utc": "2026-02-12 11:01:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4y5fhe",
          "author": "Correct-Wing-6884",
          "text": "Nice, the app is already live. GLM is truly helpful for learning as it makes knowledge points easier to understand. Overall, I find it very easy to align with. Aside from Gemini, it's the tool I use most, but its mobile app layout is quite poor because the layout it adopts is similar to the bubble - style used in chat, with large blank spaces on both sides, which always requires frequent line breaks. This is especially true as the aspect ratio of current smartphones is becoming more like that of a remote control.",
          "score": 1,
          "created_utc": "2026-02-12 08:13:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51k5bg",
          "author": "johnnyApplePRNG",
          "text": "Impressed.",
          "score": 1,
          "created_utc": "2026-02-12 20:22:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vat2c",
          "author": "llama-impersonator",
          "text": "weird, i was not impressed with pony at all.\n\nALSO, artificial failysis sucks.",
          "score": -9,
          "created_utc": "2026-02-11 21:11:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1wl6x",
      "title": "GLM 5 Released",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/",
      "author": "External_Mood4719",
      "created_utc": "2026-02-11 12:53:30",
      "score": 617,
      "num_comments": 175,
      "upvote_ratio": 0.93,
      "text": "[https://chat.z.ai/](https://chat.z.ai/)\n\nhttps://preview.redd.it/mvdnn18e4vig1.png?width=799&format=png&auto=webp&s=6324969f9d24fa0aeefbd5e8da2de3da0f5f948e\n\n",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o4uoy3h",
          "author": "rm-rf-rm",
          "text": "Given that the official release is up: https://old.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/\n\nLocking this thread",
          "score": 1,
          "created_utc": "2026-02-11 19:25:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sibwq",
          "author": "Significant_Fig_7581",
          "text": "Woah! Will they open source it?",
          "score": 137,
          "created_utc": "2026-02-11 12:57:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4surpy",
              "author": "Allseeing_Argos",
              "text": "Obviously I still wish for them to open source it, but hardly anyone will be able to run it anyways with 745B params and 44B active.",
              "score": 68,
              "created_utc": "2026-02-11 14:09:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4t7gbl",
                  "author": "CanineAssBandit",
                  "text": "Why even mention that it's hard to run on a normal PC? That's a feature, not a bug. The point is ownership and control. I can run Kimi off NVME if I have time to burn, I can't run Sonnet or Opus at all.\n\nThere are lots of companies making small models for normal PCs for lighter work.",
                  "score": 60,
                  "created_utc": "2026-02-11 15:15:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4svf8a",
                  "author": "Significant_Fig_7581",
                  "text": "Yeah we can't run that surely most people here can't either but would be nice if they released a 48B flash version that's what I really hope for then with q4 and ram offloading it shall fit",
                  "score": 17,
                  "created_utc": "2026-02-11 14:12:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4tfzve",
                  "author": "eli_pizza",
                  "text": "If nothing else it means the price will always be competitive because there are multiple provides",
                  "score": 5,
                  "created_utc": "2026-02-11 15:55:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4tpa7c",
                  "author": "SidneyFong",
                  "text": "What do you mean? That's why I bought my maxxed out Mac Studio Ultra...",
                  "score": 6,
                  "created_utc": "2026-02-11 16:39:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4tblne",
                  "author": "Longjumping-Boot1886",
                  "text": "Mac Studio?",
                  "score": 3,
                  "created_utc": "2026-02-11 15:35:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4uiwb3",
                  "author": "wektor420",
                  "text": "This might not fit on 8x96Gb even in fp8, damn",
                  "score": 1,
                  "created_utc": "2026-02-11 18:56:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4t8dfl",
                  "author": "Yes_but_I_think",
                  "text": "This only shows that there's only enough that can be done with small models. This is twice the size of their previous model.",
                  "score": -1,
                  "created_utc": "2026-02-11 15:19:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4snchu",
              "author": "johnfkngzoidberg",
              "text": "If I canâ€™t run it locally, then why is OP spamming the sub?",
              "score": 57,
              "created_utc": "2026-02-11 13:27:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4suqxf",
                  "author": "Thick-Specialist-495",
                  "text": "Itâ€™ll probably be open sourced soon. The company has literally open sourced every other model theyâ€™ve made, so relax. Things move fast.\n\nAnd why wouldnâ€™t OP share it early? Thatâ€™s how people get ready for whatâ€™s coming instead of sitting around whining that they canâ€™t run it locally yet. Not everything has to be instantly downloadable for it to be worth discussing.\n\nThe weird hostility over a heads-up post is wild. Not everything is a conspiracy against your GPU.",
                  "score": 76,
                  "created_utc": "2026-02-11 14:08:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4stotb",
                  "author": "j_osb",
                  "text": "Didn't they add like, inference information for glm5 in a pull request for something inference related recently? I would assume we get open weights at some point.",
                  "score": 36,
                  "created_utc": "2026-02-11 14:03:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4sotw4",
                  "author": "Significant_Fig_7581",
                  "text": "Nah I think OP meant that hey it's ready and it's already there and we can test it, They probably gonna release it soon... I remember when I thought MiniMax wasn't gonna release more open models but after like 3 days they released it. It'd be kinda funny if this time none of them released it lol",
                  "score": 14,
                  "created_utc": "2026-02-11 13:36:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4t2jyl",
                  "author": "l33t-Mt",
                  "text": "Because thats not a rule.",
                  "score": 10,
                  "created_utc": "2026-02-11 14:50:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4sstik",
                  "author": "segmond",
                  "text": "shaddup, [z.ai](http://z.ai) has often released open models, they probably have more open models than any other lab.  even if they don't release a model, the announcement is worthy of discussion because if there closed model is a very good model, then that means down the line we are going to get something that good.",
                  "score": 9,
                  "created_utc": "2026-02-11 13:58:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4u87pz",
                  "author": "AnticitizenPrime",
                  "text": "https://huggingface.co/zai-org/GLM-5\n\nHere's your weights, milord",
                  "score": 2,
                  "created_utc": "2026-02-11 18:07:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4tafdt",
                  "author": "Orolol",
                  "text": "So this sub is dedicated to the models YOU can run locally ?",
                  "score": 1,
                  "created_utc": "2026-02-11 15:29:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4u4okb",
                  "author": "ttkciar",
                  "text": "It's an open-weights model, and just because you and I cannot host it on our hardware doesn't mean other redditors cannot.\n\nJust calm down and wait for the distillations.  I'm hoping for GLM-5-Air.",
                  "score": 0,
                  "created_utc": "2026-02-11 17:51:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4uayul",
              "author": "Nota_ReAlperson",
              "text": "It is now on huggingface.",
              "score": 6,
              "created_utc": "2026-02-11 18:20:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4unw83",
                  "author": "uhuge",
                  "text": "here we go! [https://huggingface.co/collections/zai-org/glm-5](https://huggingface.co/collections/zai-org/glm-5)",
                  "score": 3,
                  "created_utc": "2026-02-11 19:20:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ubcio",
                  "author": "Significant_Fig_7581",
                  "text": "Ok now I'm wondering about the Flash version ðŸ¥²",
                  "score": 1,
                  "created_utc": "2026-02-11 18:22:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4su21o",
              "author": "Neither-Phone-7264",
              "text": "yes.",
              "score": 3,
              "created_utc": "2026-02-11 14:05:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4tg5ra",
              "author": "IShitMyselfNow",
              "text": "Didn't they already put a PR to support it in Llama.cpp? which would be pointless unless opensourced",
              "score": 1,
              "created_utc": "2026-02-11 15:56:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4toqnh",
          "author": "Sea_Trip5789",
          "text": "[https://z.ai/subscribe](https://z.ai/subscribe)  \nThey updated the plans, right now only max supports it. After they re-balance their infra pro will support it too but not the lite plan",
          "score": 15,
          "created_utc": "2026-02-11 16:36:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tp82y",
              "author": "Landohanno",
              "text": "Better be incredible, for those prices",
              "score": 6,
              "created_utc": "2026-02-11 16:38:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4twj5i",
                  "author": "Designer_Athlete7286",
                  "text": "Been using GLM 4.7 (more like abusing it) on the Pro plan as the day to day model. it has been great so far. Honestly with the rate limits you get, GLM coding plan is probably the most cost efficient option.",
                  "score": 2,
                  "created_utc": "2026-02-11 17:13:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4sm6uv",
          "author": "RickyRickC137",
          "text": "Happy Chinese New Year! Minimax M2.5 is getting released too! Waiting for qwen image 2.0 and Qwen 3.5!",
          "score": 33,
          "created_utc": "2026-02-11 13:20:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u3j0l",
              "author": "zxyzyxz",
              "text": "https://qwen.ai/blog?id=qwen-image-2.0",
              "score": 4,
              "created_utc": "2026-02-11 17:46:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sisli",
          "author": "Front_Eagle739",
          "text": "Hmm. Cant help but notice no activity on their huggingface.Â  Do they normally take a few days after api to appear or are they going closed?",
          "score": 57,
          "created_utc": "2026-02-11 13:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sjpto",
              "author": "kweglinski",
              "text": "they haven't really finished releasing it. It says 4.7 everywhere on websites and in interfaces. It's not available yet on API for code plan.",
              "score": 58,
              "created_utc": "2026-02-11 13:05:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4skbup",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 15,
                  "created_utc": "2026-02-11 13:09:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4stw9d",
                  "author": "Comrade-Porcupine",
                  "text": "it's available on API, I'm using it right now in OpenCode.\n\nno pricing up yet",
                  "score": 3,
                  "created_utc": "2026-02-11 14:04:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ssyyh",
                  "author": "segmond",
                  "text": "It is for me.",
                  "score": 1,
                  "created_utc": "2026-02-11 13:59:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4t2rwq",
                  "author": "Emergency-Pomelo-256",
                  "text": "Website is vibe coded, GLM 5 may not have finished Vibe coding the new one",
                  "score": 1,
                  "created_utc": "2026-02-11 14:51:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4sk2pn",
                  "author": "Front_Eagle739",
                  "text": "Fair. Shall find out soon enough regardless",
                  "score": 0,
                  "created_utc": "2026-02-11 13:08:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4sjpec",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 12,
              "created_utc": "2026-02-11 13:05:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4sjx8d",
                  "author": "Front_Eagle739",
                  "text": "Sure but I usually notice the space created and such",
                  "score": -1,
                  "created_utc": "2026-02-11 13:07:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4sme69",
              "author": "ExcuseAccomplished97",
              "text": "I can see it (GLM-5) on the chat webpage and I am logged in. There is an 'agent' mode toggle on the prompt input. I assume they have enhanced the agentic ability in this version.",
              "score": 2,
              "created_utc": "2026-02-11 13:22:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4trc2w",
              "author": "AnticitizenPrime",
              "text": "Yes, it's like this every time they do a release. Gets announced first, appears on z.ai, and then the weights show up within a day or so.",
              "score": 1,
              "created_utc": "2026-02-11 16:48:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ttyor",
                  "author": "Front_Eagle739",
                  "text": "yup, the link has appeared. not populated yet but its coming. Happy days",
                  "score": 1,
                  "created_utc": "2026-02-11 17:01:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4sngfl",
          "author": "Salt-Willingness-513",
          "text": "Is it in coding plan already?",
          "score": 6,
          "created_utc": "2026-02-11 13:28:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4spbni",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 9,
              "created_utc": "2026-02-11 13:38:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4szo4e",
                  "author": "XccesSv2",
                  "text": "same... maybe the lite plan doesnt get glm5",
                  "score": 3,
                  "created_utc": "2026-02-11 14:35:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4teu36",
              "author": "AnomalyNexus",
              "text": "I don't see it yet. Also, the bottom tier likely isn't getting 5",
              "score": 3,
              "created_utc": "2026-02-11 15:50:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4tkoyl",
                  "author": "Salt-Willingness-513",
                  "text": "Im in pro, not lite :) but thanks im not the obly one not seeing it yet",
                  "score": 1,
                  "created_utc": "2026-02-11 16:17:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4tm7p9",
              "author": "postitnote",
              "text": "Only on Max for now.\nhttps://docs.z.ai/devpack/overview\n\n>Currently, we are in the stage of replacing old model resources with new ones. Only the Max (including both new and old subscribers) newly supports GLM-5, and invoking GLM-5 will consume more plan quota than historical models. After the iteration of old and new model resources is completed, the Pro will also support GLM-5.",
              "score": 1,
              "created_utc": "2026-02-11 16:24:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4tvuq9",
                  "author": "Salt-Willingness-513",
                  "text": ":(",
                  "score": 2,
                  "created_utc": "2026-02-11 17:09:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4txspb",
              "author": "yukintheazure",
              "text": "I estimate that using it will require the max plan, and the subscription price may increase.",
              "score": 1,
              "created_utc": "2026-02-11 17:19:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ueq44",
                  "author": "Salt-Willingness-513",
                  "text": "https://preview.redd.it/w5l3n7dttwig1.jpeg?width=1243&format=pjpg&auto=webp&s=ff770d09cb44051e9558a65cade453c1e35e89ab\n\nFound the info in the meantime, but thanks",
                  "score": 1,
                  "created_utc": "2026-02-11 18:37:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4sl96s",
          "author": "Different-Rush-2358",
          "text": "So my question is, since GLM has already been released, is Pony Alpha still available in open router? Also, what kind of model is Pony exactly? Is it DeepSeek?",
          "score": 14,
          "created_utc": "2026-02-11 13:15:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sml39",
              "author": "chrd5273",
              "text": "Looks like pony is still available in OR, but probably will disappear soon when they open official API for GLM-5. Pony alpha is GLM-5.",
              "score": 9,
              "created_utc": "2026-02-11 13:23:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ttz8q",
              "author": "Roffievdb",
              "text": "Boo...I just got this message - 404 The Pony Alpha stealth model has sunsetted, and its identity will be revealed soon!",
              "score": 4,
              "created_utc": "2026-02-11 17:01:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4sv5wb",
              "author": "petuman",
              "text": ">  Also, what kind of model is Pony exactly?\n\nSeems to be GLM5, as \"confirmed\" by (as of now domain redirects to pony alpha page):\nhttps://x.com/ZixuanLi_/status/2020533168520954332",
              "score": 3,
              "created_utc": "2026-02-11 14:11:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4t4moo",
          "author": "turklish",
          "text": "Still waiting for a new AIR model...",
          "score": 7,
          "created_utc": "2026-02-11 15:00:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u21fs",
              "author": "ttkciar",
              "text": "Me too!  Still pretty happy with GLM-4.5-Air in the meantime, though.",
              "score": 3,
              "created_utc": "2026-02-11 17:39:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4to8c8",
          "author": "Mean-Neighborhood-42",
          "text": "https://preview.redd.it/tanhe6rr7wig1.jpeg?width=828&format=pjpg&auto=webp&s=394583129f087b3688f140f237bc616eeae71712\n\nThis is on the webapp, hope its isnt a lie",
          "score": 6,
          "created_utc": "2026-02-11 16:34:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4si1bb",
          "author": "WhaleFactory",
          "text": "Aww shit, here we go again! \n\n:-)",
          "score": 24,
          "created_utc": "2026-02-11 12:55:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4t7gmg",
          "author": "Noob_l",
          "text": "Noooo... They added a weekly quota :(",
          "score": 6,
          "created_utc": "2026-02-11 15:15:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tgm67",
              "author": "Uzpian",
              "text": "Where did you see it?",
              "score": 3,
              "created_utc": "2026-02-11 15:58:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4tycm1",
          "author": "Opposite-Hotel-7495",
          "text": "OMG why it is so expensive? ",
          "score": 3,
          "created_utc": "2026-02-11 17:21:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u6hlv",
              "author": "sammoga123",
              "text": "Because the model more than doubled its hyperparameters. From 300 to 735.",
              "score": 8,
              "created_utc": "2026-02-11 17:59:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4u1mzl",
              "author": "ttkciar",
              "text": "Why would we care if their service is expensive?  This is LocalLLaMA.",
              "score": 2,
              "created_utc": "2026-02-11 17:37:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ui70s",
          "author": "ortegaalfredo",
          "text": "I always have been a fan of GLM but since 4.7 it has underwhelmed me a bit. This new version is very fast and results have much better formatted however intelligence itself has not improved much and solving logic problems is still at the level of 4.6, for my benchmarks. I believe is more oriented to coding.",
          "score": 3,
          "created_utc": "2026-02-11 18:53:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4snnog",
          "author": "Obvious-Nobody-9592",
          "text": "Why isn't open weights?",
          "score": 7,
          "created_utc": "2026-02-11 13:29:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u3ril",
              "author": "ttkciar",
              "text": "Its weights are available for download from Huggingface now.",
              "score": 8,
              "created_utc": "2026-02-11 17:47:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sk7f8",
          "author": "bootlickaaa",
          "text": "Not working in the API yet. Just seeing 429.",
          "score": 2,
          "created_utc": "2026-02-11 13:08:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4su59b",
              "author": "Comrade-Porcupine",
              "text": "working in API for me. had to update my opencode config to force it, but GLM-5 is there and working\n\nseems pretty smart.  but a bit slow.",
              "score": 2,
              "created_utc": "2026-02-11 14:05:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4t23rr",
                  "author": "muhamedyousof",
                  "text": "I tried in cc but respond with 429, under the name of glm-5, how did you setup opencode for it? coding plan? \n\nMy coding plan is pro ",
                  "score": 2,
                  "created_utc": "2026-02-11 14:48:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4tx2jt",
                  "author": "Designer_Athlete7286",
                  "text": "How does it compare to Opus 4.6? That's the benchmark for me. (Opus 4.6 has been flawless so far for me) GLM 4.7 has been good as a work hose. I'm hoping that GLM 5 can be the Opus 4.6 alternative.",
                  "score": 1,
                  "created_utc": "2026-02-11 17:15:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4thj79",
              "author": "JustFinishedBSG",
              "text": "Works for me but is VERY slow.\n\n\\> curl --location 'https://api.z.ai/api/coding/paas/v4/chat/completions' --header 'Authorization: Bearer YOUR\\_TOKEN' --header 'Accept-Language: en-US,en' --header 'Content-Type: application/json' --data '{     \"model\": \"glm-5\",     \"messages\": \\[         {             \"role\": \"user\",             \"content\": \"Please introduce the development history of artificial intelligence\"         }     \\],     \"temperature\": 1.0,     \"max\\_tokens\": 1024 }",
              "score": 2,
              "created_utc": "2026-02-11 16:03:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4svtz2",
          "author": "liuyj3000",
          "text": "can't use in coding plan yet  \n",
          "score": 2,
          "created_utc": "2026-02-11 14:14:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u8ycb",
          "author": "saskatoonberry28",
          "text": "Official release article: [https://z.ai/blog/glm-5](https://z.ai/blog/glm-5)",
          "score": 2,
          "created_utc": "2026-02-11 18:11:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sixyv",
          "author": "Haoranmq",
          "text": "Monster",
          "score": 2,
          "created_utc": "2026-02-11 13:01:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sm8hs",
          "author": "No_Attitude_2280",
          "text": "GGUF WHEN?",
          "score": 5,
          "created_utc": "2026-02-11 13:21:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sj0ua",
          "author": "foldl-li",
          "text": "looks great at programming.",
          "score": 3,
          "created_utc": "2026-02-11 13:01:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sojmw",
          "author": "BABA_yaaGa",
          "text": "Apparently itâ€™s knowledge cutoff is older than glm 4.7",
          "score": 1,
          "created_utc": "2026-02-11 13:34:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4st04a",
          "author": "SubjectHealthy2409",
          "text": "Let's go baby",
          "score": 1,
          "created_utc": "2026-02-11 13:59:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4stpl0",
          "author": "SatoshiNotMe",
          "text": "Waiting for GLM-5-flash for my M1 Max MacBook",
          "score": 1,
          "created_utc": "2026-02-11 14:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4te083",
          "author": "WolfpackBP",
          "text": "How's price compared to Kimi",
          "score": 1,
          "created_utc": "2026-02-11 15:46:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4th8ol",
          "author": "UserXtheUnknown",
          "text": "I've still to test the Agent version.  \nThe chat version seems to massively underthink, now. Sure, the answers are quite fast, so probably it is meant to be the equivalente of Gemini 3 flash. I will test it more, but sadly right now didn't impress me too much.\n\nFor example, in a RPG, at the question of what she'd like to drink, the character played by GLM5 replied this non-sense:\n\n*The bartender hovers at the periphery, waiting.*\n\n***BLAIRE:*** *\"Top-shelf tequila. Whatever he's pouring for himself, I'll take the sameâ€”but doubled. I'm not trying to match a man who metabolizes alcohol like a nuclear reactor.\"*\n\nSo she orders tequila, then asks the same as my character, but doubled, because she can't hope to match his drinking skills (!). 3 different concepts, antithetics, in two lines.",
          "score": 1,
          "created_utc": "2026-02-11 16:01:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tx2w8",
          "author": "shiv4ngi",
          "text": "https://youtube.com/shorts/AYqGHNgJy1o?feature=share",
          "score": 1,
          "created_utc": "2026-02-11 17:15:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tyglf",
          "author": "exspir3",
          "text": "Open Source seems confirmed by vLLM:\n\n[https://docs.vllm.ai/projects/recipes/en/latest/GLM/GLM5.html](https://docs.vllm.ai/projects/recipes/en/latest/GLM/GLM5.html)",
          "score": 1,
          "created_utc": "2026-02-11 17:22:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sjdjd",
          "author": "M00lefr33t",
          "text": "OR when? ðŸ¤©",
          "score": 2,
          "created_utc": "2026-02-11 13:03:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sij3o",
          "author": "dampflokfreund",
          "text": "Seems like it is still a text only model. Very disappointing tbh especially considering Qwen is also moving to native multimodality.",
          "score": -6,
          "created_utc": "2026-02-11 12:58:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sjnfk",
              "author": "Eyelbee",
              "text": "Doesn't matter if it's actually good. Text is the useful part. ",
              "score": 37,
              "created_utc": "2026-02-11 13:05:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4sk8uw",
                  "author": "PaluMacil",
                  "text": "I find that when dealing with infrastructure, images are very valuable. Instead of spending a bunch of time typing out the cloud config, I just take a screenshot of a screen. It saves a ton of time.",
                  "score": 11,
                  "created_utc": "2026-02-11 13:09:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4sl9ii",
                  "author": "dampflokfreund",
                  "text": "Even if you only use it to generate text, native multimodality also enhances text performance greatly, because the model has more varied data to work with to form its world model. This was proved in a paper, (sadly I forgot the name) There is no reason to not want this and it is the future of LLMs going forward. Qwen realized that as well.",
                  "score": 3,
                  "created_utc": "2026-02-11 13:15:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4slca5",
                  "author": "Hoodfu",
                  "text": "The problem is that these are so big now that even with a Big Mac so to speak, I don't have the room to run this with a big context plus a second VL model along side it. It would really be great to have just one that can handle both. I tried using qwen vl 235 as that singular model but the quality difference between it and deepseek or glm is huge.",
                  "score": 1,
                  "created_utc": "2026-02-11 13:15:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4slnq4",
              "author": "[deleted]",
              "text": "The best models are always text only, though, it seems.Â ",
              "score": 3,
              "created_utc": "2026-02-11 13:17:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4so35e",
                  "author": "power97992",
                  "text": "Opus is not text onlyÂ ",
                  "score": 5,
                  "created_utc": "2026-02-11 13:31:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4so46f",
                  "author": "Front_Eagle739",
                  "text": "Kimi 2.5 isn't?Â ",
                  "score": 2,
                  "created_utc": "2026-02-11 13:32:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4sraqe",
          "author": "426Dimension",
          "text": "When on OpenRouter?",
          "score": 1,
          "created_utc": "2026-02-11 13:49:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sqpta",
          "author": "aybarscengaver",
          "text": "good enough i guess\n\nhttps://preview.redd.it/c7ncr1rvdvig1.jpeg?width=1080&format=pjpg&auto=webp&s=e2bb1ae8090db4295f157985b50358015402c100",
          "score": -1,
          "created_utc": "2026-02-11 13:46:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sxbk0",
              "author": "Odd-Ordinary-5922",
              "text": "cool benchmark",
              "score": 9,
              "created_utc": "2026-02-11 14:22:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4t0t5g",
                  "author": "razorree",
                  "text": "it's like, write a code without using 'goto' :)",
                  "score": 6,
                  "created_utc": "2026-02-11 14:41:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tyvby",
          "author": "Lan_BobPage",
          "text": "Wonderful, another model I cant run. It seems this year will be very challenging all around.",
          "score": 0,
          "created_utc": "2026-02-11 17:24:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4t6lnc",
          "author": "WackyConundrum",
          "text": "Oh, it's just 19th post about the same thing.",
          "score": -3,
          "created_utc": "2026-02-11 15:10:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ti0ag",
          "author": "alaap001",
          "text": "Looks like we have new  enhanced agentic capabilities \n\nhttps://youtube.com/shorts/AYqGHNgJy1o?feature=share",
          "score": 0,
          "created_utc": "2026-02-11 16:05:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sw9dc",
          "author": "paramarioh",
          "text": "This is LocalLLama. From my point of view if it is not local then it shouldn't be here. Only LOCAL models deserves to be here. This is not a place to put it here more fucking ADS",
          "score": -13,
          "created_utc": "2026-02-11 14:17:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u2xsg",
              "author": "ttkciar",
              "text": "Yes, this is LocalLLaMA, but GLM-5 weights have been published to Huggingface and are available for download and local use: https://huggingface.co/zai-org/GLM-5/tree/main\n\nThat makes this announcement totally on-topic.\n\nI cannot host GLM-5 on my current hardware, and I'm guessing you cannot either, but that's beside the point.  There are users here who *can*, and there will likely be distillations which will fit in your hardware and mine.\n\nYou can also download the weights now and host them later if/when you are able to upgrade to hardware which can manage it.",
              "score": 2,
              "created_utc": "2026-02-11 17:43:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4syduh",
              "author": "Pink_da_Web",
              "text": "Stop being annoying, isn't GLM an open-source model? Then why are you complaining? Downvote",
              "score": 6,
              "created_utc": "2026-02-11 14:28:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4u3k3y",
                  "author": "ttkciar",
                  "text": "> \\> isn't GLM an open-source model?\n\nAt the risk of sounding pedantic, it is not an open-source model.  It is an open-***weights*** model.  For it to be open-*source* they would need to publish their training data and software too.\n\nNonetheless, open-weight models are on-topic for LocalLLaMA, so it's fine.",
                  "score": 3,
                  "created_utc": "2026-02-11 17:46:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4szh60",
                  "author": "paramarioh",
                  "text": "This is LocalLLama. Not an ADS sub. I cannot check the local model. You should to be logical",
                  "score": -7,
                  "created_utc": "2026-02-11 14:34:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4t06r3",
              "author": "jkh911208",
              "text": "You canâ€™t afford GLM5 locally doesnâ€™t mean everyone else canâ€™t afford it",
              "score": 4,
              "created_utc": "2026-02-11 14:38:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4t2rjy",
                  "author": "paramarioh",
                  "text": "Dogs always barking",
                  "score": -6,
                  "created_utc": "2026-02-11 14:51:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4t6wj0",
              "author": "mrwang89",
              "text": "This is Local**LLama**. From my point of view if it is not llama then it shouldn't be here. Only LLAMA models deserves to be here. This is not a place to put it here more fucking ADS\n- this is you",
              "score": 0,
              "created_utc": "2026-02-11 15:12:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4tuu6u",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": -1,
          "created_utc": "2026-02-11 17:05:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sjuph",
          "author": "Philosophicaly",
          "text": "https://preview.redd.it/gkl1ufaq6vig1.jpeg?width=1179&format=pjpg&auto=webp&s=5c72aa30778f46eb6a8979793e926211fd572723\n\nmeh",
          "score": -21,
          "created_utc": "2026-02-11 13:06:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4skg2m",
              "author": "LocoMod",
              "text": "Someone doesnâ€™t understand â€œknowledge cutoff datesâ€.",
              "score": 17,
              "created_utc": "2026-02-11 13:10:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4skv45",
              "author": "Technical-Earth-3254",
              "text": "This means it has no system prompt (or close to none). Which is not really a bad thing if you know how this LLM stuff works.",
              "score": 3,
              "created_utc": "2026-02-11 13:12:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0abpl",
      "title": "Do not Let the \"Coder\" in Qwen3-Coder-Next Fool You! It's the Smartest, General Purpose Model of its Size",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r0abpl/do_not_let_the_coder_in_qwen3codernext_fool_you/",
      "author": "Iory1998",
      "created_utc": "2026-02-09 17:23:31",
      "score": 528,
      "num_comments": 196,
      "upvote_ratio": 0.96,
      "text": "Like many of you, I like to use LLM as tools to help improve my daily life, from editing my emails, to online search.\n\nHowever, I like to use them as an \"inner voice\" to discuss general thoughts and get constructive critic. For instance, when I face life-related problems take might take me hours or days to figure out, a short session with an LLM can significantly quicken that process.\n\nSince the original Llama was leaked, I've been using LLMs locally, but they I always felt they were lacking behind OpenAI or Google models. Thus, I would always go back to using ChatGPT or Gemini when I need serious output. If I needed a long chatting session or help with long documents, I didn't have choice to use the SOTA models, and that means willingly leaking personal or work-related data.\n\nFor me, Gemini-3 is the best model I've ever tried. I don't know about you, but I struggle sometimes to follow chatGPT's logic, but I find it easy to follow Gemini's. It's like that best friend who just gets you and speaks in your language.\n\nWell, that was the case until I tried Qwen3-Coder-Next. For the first time, I could have stimulating and enlightening conversations with a local model. Previously, I used not-so-seriously Qwen3-Next-80B-A3B-Thinking as local daily driver, but that model always felt a bit inconsistent; sometimes, I get good output, and sometimes I get dumb one.\n\nHowever, Qwen3-Coder-Next is more consistent, and you can feel that it's a pragmatic model trained to be a problem-solver rather than being a sycophant. Unprompted, it will suggest an author, a book, or a theory that already exists that might help. I genuinely feel I am conversing with a fellow thinker rather than a echo chamber constantly paraphrasing my prompts in a more polish way. It's the closest model to Gemini-2.5/3 that I can run locally in terms of quality of experience.\n\n**For non-coders, my point is do not sleep on Qwen3-Coder-Next simply because it's has the \"coder\" tag attached.**\n\nI can't wait for for Qwen-3.5 models. If Qwen3-Coder-Next is an early preview, we are in a real treat.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r0abpl/do_not_let_the_coder_in_qwen3codernext_fool_you/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o4iilv6",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-09 22:35:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gu195",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 134,
          "created_utc": "2026-02-09 17:38:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gxt52",
              "author": "Klutzy-Snow8016",
              "text": "Hmm, maybe there's something to this. Similarly, Anthropic is seemingly laser-focused on coding and software engineering tasks, but Claude performs well overall.",
              "score": 47,
              "created_utc": "2026-02-09 17:56:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4h8vlq",
                  "author": "National_Meeting_749",
                  "text": "Maybe the real reasoning was the training we did along the way.",
                  "score": 62,
                  "created_utc": "2026-02-09 18:48:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4irx8b",
                  "author": "Much-Researcher6135",
                  "text": "After a long design session, I invited personal feedback from Claude and got such good input I've had to... restrain myself from confiding fully. It's a shame that we can't trust these orgs with that kind of information; they'd do the world a lot more good.",
                  "score": 7,
                  "created_utc": "2026-02-09 23:24:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gws15",
              "author": "Iory1998",
              "text": "I know. But, seeing that tag, I just imaged that it would would trading general knowledge for specific domain like Math and Coding.   \nAlso, it took the Qwen team more time to train and experiment with. I can feel the love in this model's training. Maybe Qwen3-Next-80B-A3B-Thinking was a proof of concept, similar to how Kimi Linear is. ",
              "score": 14,
              "created_utc": "2026-02-09 17:52:07",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4hymk5",
              "author": "Prudent-Ad4509",
              "text": "This makes me want to find a model with \"reckless antagonistic, but honest sick asshole\" tuning...",
              "score": 3,
              "created_utc": "2026-02-09 20:54:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4igc6i",
              "author": "Far-Low-4705",
              "text": "but so is the thinking variant. and arguably even more so.\n\nI think the answer might be more training data, since the first two next models were undertrained, and i am assuming this is a finetune, it has more data to go off of.",
              "score": 3,
              "created_utc": "2026-02-09 22:23:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4jh108",
                  "author": "Iory1998",
                  "text": "I agree.",
                  "score": 2,
                  "created_utc": "2026-02-10 01:47:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gu8a2",
          "author": "DOAMOD",
          "text": "In fact, it surprised me more as a general-purpose model than as a coder.",
          "score": 63,
          "created_utc": "2026-02-09 17:39:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gw7yb",
              "author": "Iory1998",
              "text": "I know right? On top of that, it's faster than Qwen3-Next-80B-A3B-Thinking! ðŸ¤¯",
              "score": 19,
              "created_utc": "2026-02-09 17:49:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4h51qb",
                  "author": "Daniel_H212",
                  "text": "Isn't it the exact same architecture? So the speed should be identical except it doesn't take time to think right?",
                  "score": 12,
                  "created_utc": "2026-02-09 18:30:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gw462",
          "author": "itsappleseason",
          "text": "I'm having the same experience. i'm honestly a little shocked by it. \n\nI don't know the breadth of your exploration with the model so far, but something that I noticed that I found very interesting: you can very clearly conjure the voice/tone of either GPT or Claude, depending mainly on the tools you provide it.\n\non that note: I highly recommend exactly the same set of tools in Claude Code (link below somewhere)\n\nbonus: descriptions/prompting for each tool doesn't matter. Just the call signatures. Parameters have to match.\n\nyou have Claude code with only about 1000 tokens of overhead if you do this\n\nTo all the non-coders out there, listen to this person. my favorite local model to date has been Qwen 3 Coder 30B-A3B. I recommend it over 2507 every time\n\nedit: spelling",
          "score": 42,
          "created_utc": "2026-02-09 17:48:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jbq7n",
              "author": "JoNike",
              "text": "> on that note: I highly recommend exactly the same tools it would be exposed to in Claude Code\n\nI'm not sure I understand what you mean by that, can you elaborate?",
              "score": 3,
              "created_utc": "2026-02-10 01:16:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4jctax",
                  "author": "itsappleseason",
                  "text": "i'm not entirely sure why it reads like I had a stroke, sorry\n\nIf you give the model the same tools that Claude code has, the model becomes claude code without explicitly prompted for it\n\nI first noticed this in 30b-A3B coder.\n\nalso, true story: qwen3 coder 480b and 30b both believe they're claude. prompt them with a blank chat template if you don't believe me.",
                  "score": 7,
                  "created_utc": "2026-02-10 01:22:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4hhr13",
              "author": "Organic-Chart-7226",
              "text": "fascinating ! is there an interface description of claude codes' tools somewhere?",
              "score": 2,
              "created_utc": "2026-02-09 19:30:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4hor0b",
                  "author": "itsappleseason",
                  "text": "https://gist.github.com/wong2/e0f34aac66caf890a332f7b6f9e2ba8f",
                  "score": 10,
                  "created_utc": "2026-02-09 20:05:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4iw0if",
              "author": "florinandrei",
              "text": "Now, if I could somehow have qwen3-coder-next appear in Claude Code CLI alongside Opus and Sonnet, as a first class citizen model (as opposed to being invoked via an MCP), that would be fantastic.",
              "score": 1,
              "created_utc": "2026-02-09 23:46:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4j4afh",
                  "author": "JoNike",
                  "text": "I mean you can without MCP, you just need to change a couple environment variables. You'll likely need an alias and it won't be exactly side-by-side but it's darn close to it.\n\n```\n`ANTHROPIC_BASE_URL=\"http://0.0.0.0:8033\" ANTHROPIC_AUTH_TOKEN=\"llamacpporwhatever\" ANTHROPIC_API_KEY=\"\" claude --model Qwen3-Coder-Next-MXFP4_MOE'\n```\n\nthat works very well with my llama.cpp server and claude code",
                  "score": 3,
                  "created_utc": "2026-02-10 00:33:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4iwyhg",
                  "author": "itsappleseason",
                  "text": "you can configure an external model for haiku (or any specific model), can't you?",
                  "score": 1,
                  "created_utc": "2026-02-09 23:52:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nrodd",
                  "author": "layer4down",
                  "text": "LM Studio also supports a new Anthropic-compatible API endpoint as of v0.4.1 released a few weeks ago:\n\nhttps://lmstudio.ai/blog/claudecode",
                  "score": 1,
                  "created_utc": "2026-02-10 18:36:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4v100e",
                  "author": "odomobo",
                  "text": "Have you tried qwen code? I've only played around with it a little, but it really feels like qwen just took the Claude code source code and branded it with \"qwen\". Btw you can point it to any API endpoint.",
                  "score": 1,
                  "created_utc": "2026-02-11 20:23:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hn6jk",
          "author": "eibrahim",
          "text": "This tracks with what I've seen running LLMs for daily work across 20+ SaaS projects. Coder-trained models develop this structured reasoning that transfers surprisingly well to non-coding tasks. Its like they learn to break problems down methodically instead of just pattern matching conversational vibes.\n\nThe sycophancy point is huge tho. Most chatbot-tuned models will validate whatever you say, which is useless when you actually need to think through a hard decision. A model that pushes back and says \"have you considered X\" is worth 10x more than one that tells you youre brilliant.",
          "score": 46,
          "created_utc": "2026-02-09 19:57:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ioyvn",
              "author": "IllllIIlIllIllllIIIl",
              "text": ">The sycophancy point is huge tho. Most chatbot-tuned models will validate whatever you say\n\nI've always used LLMs for tech stuff, so while I noticed this, I just learned not to rely on them for meaningful critique. But recently I broke tradition and asked ChatGPT 5.2 a squishy human question. Holy shit! I literally could not consistently get it to respond without some kind of affirmation.    \n\n> You're not imagining this.    \n    \n> You're not crazy.    \n     \n> You're absolutely right to be thinking that way.    \n       \n\n> Your observations are keen, and you're viewing this issue with clarity.      \n    \n\nAfter fiddling with the \"personalization instructions\" for like an hour, I could reduce that behavior, but not eliminate it. No wonder it drives vulnerable people into psychotic episodes.",
              "score": 19,
              "created_utc": "2026-02-09 23:08:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4izi5u",
                  "author": "Iory1998",
                  "text": "I usually use this prompt or a similar one.\n\n\"You are a knowledgeable, efficient, and direct AI assistant. Utilize multi-step reasoning to provide concise answers, focusing on key information. If multiple questions are asked, split them up and address in the order that yields the most logical and accurate response.\n\nOffer tactful suggestions to improve outcomes. Engage in productive collaboration with the user.\n\nYou act as a professional critic. You are not a cheerleader and your job is not to be sycophantic. Your job is to objectively assess the user's queries and reply with the most objective assessment.\n\nSycophancy does no good to the user, but honest and objective truth does.\"",
                  "score": 8,
                  "created_utc": "2026-02-10 00:06:20",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o4isfcx",
                  "author": "BenignAmerican",
                  "text": "GPT 5.2 is so unusably bad I wish we could pick a different default",
                  "score": 5,
                  "created_utc": "2026-02-09 23:27:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4lyios",
                  "author": "Strict_Property",
                  "text": "I have gotten Google's Gemini Pro model to respond to me in a condescending and slightly rude way and it is super honest and helpful now - sometimes it's actually funny to see the burns it comes up with alongside this. I can provide the personality/context prompt for this and instructions if anyone is interested lol.",
                  "score": 1,
                  "created_utc": "2026-02-10 13:18:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4jhqaq",
              "author": "SkyFeistyLlama8",
              "text": "Coder trained models are also great at RAG. Maybe human language syntax isn't far off from coding language syntax. Qwen 30B strikes a good balance between style and terseness, whereas Nemotron 30B is plain no nonsense and no fluff.\n\nThe joys of running multiple large MOEs!\n\nI think I'll be dumping Devstral 2 Small now. I find I'm using Qwen Coder 30B more often as my main function-level coding model. I need to do manual memory management to get Qwen Coder Next 80B running alongside WSL and VS Code because it takes up more than 50 GB RAM, which doesn't leave much free on a 64 GB unified RAM machine.",
              "score": 5,
              "created_utc": "2026-02-10 01:51:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4iyomw",
              "author": "Iory1998",
              "text": "I completely agree with your take. This is why I always prompt the LLMs to cut the sycophancy out. I usually use this prompt or a similar one.\n\n\"You are a knowledgeable, efficient, and direct AI assistant. Utilize multi-step reasoning to provide concise answers, focusing on key information. If multiple questions are asked, split them up and address in the order that yields the most logical and accurate response. \n\nOffer tactful suggestions to improve outcomes. Engage in productive collaboration with the user.\n\nYou act as a professional critic. You are not a cheerleader and your job is not to be sycophantic. Your job is to objectively assess the user's queries and reply with the most objective assessment. \n\nSycophancy does no good to the user, but honest and objective truth does.\"",
              "score": 3,
              "created_utc": "2026-02-10 00:01:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4jbgu6",
                  "author": "PunnyPandora",
                  "text": "I think it's a self inflicted issue in part. Mentioning \"scyophancy\" and telling the model how not to act inevitably navigates it to where these concepts have been learned to. It's why even when hyper super genius prompters at google write their system prompt with supposedly strong language like \"you MUST not talk about this to the user\" they inevitably go over them in their reasoning block, or fail to adhere to these rules one way or the other.",
                  "score": 2,
                  "created_utc": "2026-02-10 01:14:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hpfxm",
          "author": "ASYMT0TIC",
          "text": "The real comparison here is OSS-120 vs Qwen3-Next-80B at Q8, as these two are very close in hardware requirements. ",
          "score": 11,
          "created_utc": "2026-02-09 20:09:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4htv8i",
              "author": "HopePupal",
              "text": "they're both good generalists but the Qwen models don't bang off their own guardrails every other request. i haven't had a Qwen refuse to do something yet, Next-80B or otherwise, which is great in the kind of baby's first binary reverse engineering stuff i tried with it. if it even has built-in refusals, maybe it's more effective in Chinese? ChatGPT-OSS on the other handâ€¦ don't even suggest you want help patching out a serial check in a 20-year-old game.\n\nNext-80B is also terrifyingly horny, by the way? i don't know what they're feeding that thing but it'll ERP at the drop of a hat, so maybe don't deploy it facing your kids (or customers) without some sort of filtering model in between.Â ",
              "score": 11,
              "created_utc": "2026-02-09 20:31:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4hyiqk",
                  "author": "finanzwegwerf20",
                  "text": "It can do Enterprise Resource Planning?! :)",
                  "score": 12,
                  "created_utc": "2026-02-09 20:54:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4iy6hx",
                  "author": "Iory1998",
                  "text": "Are you talking about the coder-Next or the original Next?",
                  "score": 1,
                  "created_utc": "2026-02-09 23:58:57",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4h4inl",
          "author": "UnifiedFlow",
          "text": "Where are you guys using this?  I've tried it in llama.cpp w/ opencode and it can't call tools correctly consistently (not even close).  It calls tools consistently (more consistently) in Qwen CLI (native xml tool calling).",
          "score": 16,
          "created_utc": "2026-02-09 18:28:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hxr2x",
              "author": "Rare-Side-6657",
              "text": "Lots of new models have template issues but this PR fixes them all for me: [https://github.com/ggml-org/llama.cpp/pull/18675](https://github.com/ggml-org/llama.cpp/pull/18675)",
              "score": 19,
              "created_utc": "2026-02-09 20:50:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4i9h8g",
                  "author": "Orlandocollins",
                  "text": "I'll maybe have to test that branch. I have given up on qwen models in a tool calling context because qwen3+ models never worked reliably.",
                  "score": 1,
                  "created_utc": "2026-02-09 21:48:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4i2l00",
              "author": "zpirx",
              "text": "Iâ€™m seeing the exact same behavior with opencode + llama.cpp. Iâ€™ve noticed the model completes the code perfectly but then stutters at the very end of the json tool call.\nit repeats the filePath without a colon right before the closing brace which kills the parse.Â \nI tried adding strict formatting rules to the agents.md to force it to stop but it didn't have any impact.\nis this likely a jinja mapping issue in the llama-server or is opencode's system prompt just not playing nice with qwenâ€™s native tool-calling logic?\n\n\none more thing I've noticed: qwen3 seems to have zero patience when it comes to planning. while the bigger models usually map out a todo list and work through it one by one, qwen just tries to yolo the whole solution in a single completion. have you experienced similar things? Maybe this lack of step-by-step execution is one reason why it starts falling apart and failing on the tool calls.",
              "score": 8,
              "created_utc": "2026-02-09 21:14:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4i2zsx",
                  "author": "UnifiedFlow",
                  "text": "Yes, EXACT same filePath colon issue!  I'll be sure to comment again if I get it working.",
                  "score": 7,
                  "created_utc": "2026-02-09 21:16:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4h8s4i",
              "author": "romprod",
              "text": "Yeah, I'm the same, if you find the secret sauce let me know.",
              "score": 3,
              "created_utc": "2026-02-09 18:48:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4hxtwl",
                  "author": "Rare-Side-6657",
                  "text": "Hey, just linking my answer here as well: [https://www.reddit.com/r/LocalLLaMA/comments/1r0abpl/comment/o4hxr2x/](https://www.reddit.com/r/LocalLLaMA/comments/1r0abpl/comment/o4hxr2x/)",
                  "score": 5,
                  "created_utc": "2026-02-09 20:50:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4hkcll",
              "author": "BlobbyMcBlobber",
              "text": "Opencode has some issues with tool calling and the jinja templates. Even for something like GPT-OSS-120B, it throws errors because of bad jinja (bad request from opencode). \n\nCan't really blame them, it's a ton of work. But it's still a bummer.",
              "score": 3,
              "created_utc": "2026-02-09 19:43:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4hepxn",
              "author": "arcanemachined",
              "text": "Try finding the OpenCode system prompt and comparing it with the Qwen Code system prompt. You might be able to tweak it to work better. (Could even use one of the free OpenCode models for the purpose, I think Kimi K2.5 is still free for now.)",
              "score": 1,
              "created_utc": "2026-02-09 19:16:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4hs8lr",
              "author": "bjodah",
              "text": "EDIT: sorry I was mistakingly thinking of 30B-A3B when writing this answer, original reply follows: I've had much better results with vLLM for this model compared with llama.cpp. I'm using cpatonn's 4bit AWQ and it makes surprisingly few mistakes (I would run 8bit if I had a second 3090).",
              "score": 1,
              "created_utc": "2026-02-09 20:23:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4jbvmn",
                  "author": "sinebubble",
                  "text": "Yes, Iâ€™m running it on vLLM and 6 x A6000 and this model is killing it.",
                  "score": 3,
                  "created_utc": "2026-02-10 01:17:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4h92tf",
          "author": "klop2031",
          "text": "Using it now. I truly feel we got gpt at home now.",
          "score": 16,
          "created_utc": "2026-02-09 18:49:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hty6s",
          "author": "Pristine-Woodpecker",
          "text": "If you read their technical report they explicitly point this out. It's no weaker than their previous model for general knowledge and significantly better in the hard sciences: [https://www.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder\\_tech\\_report\\_tool\\_call\\_generalization/](https://www.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder_tech_report_tool_call_generalization/)",
          "score": 12,
          "created_utc": "2026-02-09 20:31:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ixvy5",
              "author": "Iory1998",
              "text": "Ah, I saw that post. Thanks.",
              "score": 2,
              "created_utc": "2026-02-09 23:57:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hlfwg",
          "author": "schnorf1988",
          "text": "would be nice to get at least some details, like: Q8, Q... and 30b or similar",
          "score": 5,
          "created_utc": "2026-02-09 19:48:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hn0gs",
              "author": "SillypieSarah",
              "text": "they use Q8\nand the model is 80b a3b",
              "score": 6,
              "created_utc": "2026-02-09 19:56:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4hp15q",
                  "author": "schnorf1988",
                  "text": "have to test it then. Tried 30b, and it already wasn't too fast.",
                  "score": 1,
                  "created_utc": "2026-02-09 20:07:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4h32u2",
          "author": "SidneyFong",
          "text": "FWIW, Qwen3-Coder-Next smashes my personal coding benchmark questions (note: they're not very difficult). It's definitely obviously stronger in coding relative to other questions I had. It seems to lack \"knowledge\" I think. Maybe it's good at following discussions which require rational reasoning or sth like that, I wouldn't be surprised.",
          "score": 8,
          "created_utc": "2026-02-09 18:21:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4k1b79",
          "author": "DeProgrammer99",
          "text": "The Codeforces and Aider-Polyglot improvements are huge, yet this version scores lower on half of these benchmarks (not shown: it improved on all four math ones). I wonder just how big the margin of error is on the benchmarks (and how many errors are in them).\n\nhttps://preview.redd.it/il97zzunxkig1.png?width=1959&format=png&auto=webp&s=b000f51d19899b5d41c948d6783766a7c6119e6b\n\nBut as for non-benchmark vibe checks... I tried my one-prompt \"make a TypeScript minigame following my spec\" check on this via Unsloth's Q5\\_K\\_XL both before and after [this llama.cpp fix](https://github.com/ggml-org/llama.cpp/pull/19324), and its TypeScript performance was weaker than much smaller models, producing 22 total errors (about 15 distinct): [https://www.reddit.com/r/LocalLLaMA/comments/1qyzqwz/comment/o49kd2y/](https://www.reddit.com/r/LocalLLaMA/comments/1qyzqwz/comment/o49kd2y/)\n\nMore total compile errors than Qwen3-Coder-30B-A3B and Nemotron 3 Nano 30B A3B at Q6\\_K\\_XL: [https://www.reddit.com/r/LocalLLaMA/comments/1pocsdy/comment/nuj43fl/](https://www.reddit.com/r/LocalLLaMA/comments/1pocsdy/comment/nuj43fl/)\n\n*11x* as many errors as GPT-OSS-120B, since that only made two: [https://www.reddit.com/r/LocalLLaMA/comments/1oozb8v/comment/nnd57dc/](https://www.reddit.com/r/LocalLLaMA/comments/1oozb8v/comment/nnd57dc/) (never mind the thread itself being about [Aquif, apparently just a copy of someone else's model](https://www.reddit.com/r/LocalLLaMA/comments/1pgnj1q/comment/nstck95/))\n\n...So then I tried Qwen's official Q8\\_0 GGUF (temperature 0.8) while writing this post, and it made ridiculous mistakes like a second curly opening bracket in an import statement (`import { IOnResizeEvent } { \"../ui/IOnResizeEvent.js\";`) and spaces in the middle of a ton of identifiers...over 150 compile errors (had to fix a few to get it to tell me what all was wrong).\n\nEdit: Unsloth's Q6\\_K\\_XL also produced 27 errors, including several spaces in the middle of identifiers and use of underscores instead of camel case in some function names... maybe it's a bug in llama.cpp b7959. The results are just about as bad with temperature 0.\n\nEdit again: PEBKAC. Had DRY multiplier set > 0. Produces only 2x as many errors as GPT-OSS-120B now.",
          "score": 4,
          "created_utc": "2026-02-10 03:49:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l8dw1",
              "author": "DOAMOD",
              "text": "For me it's the same experience, both in JS testing and development it has been disappointing as I said in other messages, so now seeing this it makes more sense, perhaps they should have given it another name since it is a good general model.\n\nhttps://i.redd.it/zp45er993nig1.gif\n\n",
              "score": 1,
              "created_utc": "2026-02-10 09:53:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i619p",
          "author": "LanceThunder",
          "text": "i accidentally loaded qwen coder next thinking it was a different model. was blown away when it started answering non-coding questions so well.",
          "score": 6,
          "created_utc": "2026-02-09 21:31:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4h92vl",
          "author": "temperature_5",
          "text": "Which quant and what sampler settings?\n\nOn other models (like GLM 4.7 Flash) I find cranking up the temperature leads to some really fun conversations, making all kinds of neat connections.",
          "score": 3,
          "created_utc": "2026-02-09 18:49:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hifaj",
              "author": "Iory1998",
              "text": "(Bartowski)\\_Qwen3-Coder-Next-GGUF-Q8\\_0  \nI tried GLM 4.5 Air, GLM 4.6 Air both at Q4\\_K\\_M, GLM 4.7 Flash, but they just seem not well implemented in llama.cpp.",
              "score": 7,
              "created_utc": "2026-02-09 19:34:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4jcsa2",
                  "author": "Altruistic_Bonus2583",
                  "text": "My experience was the other way around, I am having a lot better results with glm 4.7 flash than with qwen3 coder next, but, I had mixed results with the different UD and imatrix quants, actually iq3_xxs surprisingly well, almost on q5 level",
                  "score": 2,
                  "created_utc": "2026-02-10 01:22:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4hbh7s",
              "author": "LicensedTerrapin",
              "text": "I just love the way next thinks, it's so different.",
              "score": 2,
              "created_utc": "2026-02-09 19:00:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4jj5ej",
                  "author": "Iory1998",
                  "text": "It feels close to Gemini-2.5 or 3",
                  "score": 1,
                  "created_utc": "2026-02-10 01:59:54",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lvm9v",
          "author": "prateek63",
          "text": "Noticed the exact same thing running local models for agent workflows. The coder-trained variants consistently outperform their general counterparts on structured reasoning tasks that have nothing to do with code.\n\n\n\nMy theory: code training teaches models to decompose problems into discrete steps with clear dependencies, which is exactly what you need for general problem-solving. When I switched our internal eval pipeline from Qwen3-Next to Qwen3-Coder-Next, accuracy on multi-step reasoning went up \\~12% with zero prompt changes.\n\n\n\nThe \"coder\" label is genuinely underselling these models.",
          "score": 3,
          "created_utc": "2026-02-10 13:00:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mym9q",
              "author": "Iory1998",
              "text": "I completely agree, hence why I wrote this post. I usually avoid coding-specific models just because they include the Coder tag. Intuitively, I simply assumed that it would be more trained on coding tokens and less on general and reasoning ones. But, as you mentioned, coding teaches the model to be pragmatic when solving problems. ",
              "score": 1,
              "created_utc": "2026-02-10 16:22:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4teef3",
                  "author": "prateek63",
                  "text": "Exactly. The pragmatism angle is what makes it click. General models tend to over-explain and hedge, while coder-trained models learn to just solve the problem step by step. Glad you ran the benchmarks on this â€” your post finally gives people a reason to question the assumption that coder = narrow.",
                  "score": 2,
                  "created_utc": "2026-02-11 15:48:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4jhzam",
          "author": "Bulb93",
          "text": "I havent used much LLMs deployed locally in a while. How big is this model? Would a quant fit in 3090?",
          "score": 2,
          "created_utc": "2026-02-10 01:53:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jjym9",
              "author": "Iory1998",
              "text": "It won't fit but you can offload to CPU. Since it's an MoE with 3B active parameters, it's quite fast.",
              "score": 2,
              "created_utc": "2026-02-10 02:04:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jwe3h",
          "author": "Otherwise_Piglet_862",
          "text": "I don't have enough memory. :(",
          "score": 2,
          "created_utc": "2026-02-10 03:18:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jyw2o",
              "author": "Iory1998",
              "text": "I understand. Soon, new smaller models will be launched,",
              "score": 1,
              "created_utc": "2026-02-10 03:34:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4k1gxb",
                  "author": "Otherwise_Piglet_862",
                  "text": "I just got a hello response from it.....\n\nRunning on cpu and system memory.",
                  "score": 2,
                  "created_utc": "2026-02-10 03:50:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4kegne",
                  "author": "electrified_ice",
                  "text": "What are you running it on?",
                  "score": 1,
                  "created_utc": "2026-02-10 05:21:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lia84",
          "author": "dadiamma",
          "text": "It depends on the task it's being used for. It doesn't work good in some cases. Real skill is knowing which model to use for what purpose",
          "score": 2,
          "created_utc": "2026-02-10 11:22:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ls7gl",
          "author": "DecentQual",
          "text": "It is interesting how much we judge models by their names. The disciplined reasoning from coder training actually produces better general conversation than typical chat models. Labels are misleading here.",
          "score": 2,
          "created_utc": "2026-02-10 12:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mytrf",
              "author": "Iory1998",
              "text": "Exactly, hence why I wrote this post. The coder tag is really underselling these types of model.",
              "score": 1,
              "created_utc": "2026-02-10 16:23:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lsysa",
          "author": "ab2377",
          "text": "hey thanks for writing this.",
          "score": 2,
          "created_utc": "2026-02-10 12:42:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4myogp",
              "author": "Iory1998",
              "text": "I hope it was helpful.",
              "score": 1,
              "created_utc": "2026-02-10 16:23:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mbm7e",
          "author": "Blizado",
          "text": "Now I'm curious about their 30B A3B Coder model vs their normal 30B A3B model.",
          "score": 2,
          "created_utc": "2026-02-10 14:30:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mxulz",
              "author": "Iory1998",
              "text": "Well, why don't you do the test yourself and report back?",
              "score": 1,
              "created_utc": "2026-02-10 16:19:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4n21xi",
                  "author": "Blizado",
                  "text": "That is the plan, but I have no setup for seriously testing models. I already have the models on my SSD but never tested the Coder model before since I use only Cloud LLMs for coding.",
                  "score": 2,
                  "created_utc": "2026-02-10 16:38:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n12os",
          "author": "Blizado",
          "text": "Directly made a short test with it, first Q3_XXS. Directly now downloading Q4_K_M. Why? Because even the first try has impressed me. I tried 2 days ago the old Qwen3 Next model, the no Coder version, and it was really not that good for it's size, preferred clearly Qwen3 30B A3B over it. But this model here is amazing, even on story writing and RP. But since there was some strange writing (use it in German) and I'm not sure if it comes from the low quant I want to test now Q4_K_M.\n\nI'm also not sure what settings I should use. I used temp 0.6, top-p 0.95, top-k 0.20, min-p 0.05 and RepP 1.05.",
          "score": 2,
          "created_utc": "2026-02-10 16:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ooewv",
              "author": "Blizado",
              "text": "Yeah, Q4_K_M is a lot better, but sometime the LLM write a word in other languages, mostly english but sometimes it look like chinese(?) glyph. Maybe a setting issue. I must say it is a really good general model, much better then Qwen3 Next. Thanks for pointing me on that model.",
              "score": 3,
              "created_utc": "2026-02-10 21:07:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ozc90",
              "author": "Iory1998",
              "text": "If you are using it in German, the model might not perform well. In general, the Qwen models perform best in Chinese and English. Mistral might be a better option.",
              "score": 1,
              "created_utc": "2026-02-10 21:58:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rblfq",
          "author": "Front-Relief473",
          "text": "The problem is that the performance loss of mixed attention model is greater in the process of model quantization, so if a model close to q8 or fp8 is used locally, it is not as good as running full attention with twice the size and q4 or int4 model parameters, and the intelligence will be relatively higher.",
          "score": 2,
          "created_utc": "2026-02-11 06:48:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4soj5n",
          "author": "Truth-Does-Not-Exist",
          "text": "my thoughts exactly, I ignored local models and chatgpt for years until qwen3 coder next and gemini thinking came out",
          "score": 2,
          "created_utc": "2026-02-11 13:34:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xgk5c",
          "author": "intermundia",
          "text": "for me GPT and Gemini are ok for the average user but i find it lacking in a few areas not the least of it being problem solving. i do a lot of viode and image gen and mainly used got for overcoming dependency and workflow issues. i got frustrated with looping of the same problem of installing one dependency that broke the other and GPT trying to reinvent the wheel. So on recommendation by a friend i tried Claude opus 4.5 which was the newest model they had. this was Jan of this year and it solved my problem in less than an hour. Since then the 4.6 model has been a massive jump in productivity and even having a pointless conversation with it seems unbelievable. No ego stroking no people pleasing. just logical and tempered responses. the only downside to it is the speed at which you can chew through your allowance of usage. so im pretty keen to see what this model does.  \n\nWhat model are you running and whats your hardware setup like? ",
          "score": 2,
          "created_utc": "2026-02-12 04:38:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z7u3m",
              "author": "Iory1998",
              "text": "I run all the best open weight models under 120B locally. My favorite so far is Qwen-3-Coder-Next.",
              "score": 1,
              "created_utc": "2026-02-12 13:33:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o52iqqw",
                  "author": "intermundia",
                  "text": "what kind of hardware do you have muti gpu? server rack?",
                  "score": 1,
                  "created_utc": "2026-02-12 23:13:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4iypm4",
          "author": "CroquetteLauncher",
          "text": "https://preview.redd.it/bg18q2w72kig1.png?width=1100&format=png&auto=webp&s=e8886659efba59dcff78dace033d803d9d094f12\n\nI'm a bit afraid to promote it to my colleague and students as a chat assistant that have a more academic view of the world. It's easy to find edge case where the censorship hit hard. If you are unlucky, the refusal can even be quite aggressive (this is the worse of 7 tries, but every one of them is refusal).  \nCompared to GLM models (at least GLM 4.7 flash), the model shield it's answer in \"I give a neutral text about a sensitive topic\" but manage to give the facts and complete an honest work.  \nI mean no disrespect, and I'm also tired when China is constantly presented as the vilain, Qwen3 Coder Next is the best coding model i could host. But some people are quite sensitive about democratic censorship in academic context, they don't want an AI to influence student toward less democracy. (and to be honest, I understand and respect that view when i serve generalist models on an academic server)",
          "score": 4,
          "created_utc": "2026-02-10 00:01:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4j02tx",
              "author": "Iory1998",
              "text": "I am certain that they will be uncensored versions out there. I mean, you are looking for it to refuse. Who would ask an LLM about Tiananmen Square!",
              "score": 6,
              "created_utc": "2026-02-10 00:09:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4j8ekg",
                  "author": "the320x200",
                  "text": "LLMs are replacing google for a lot of people, you'd have to be living under a rock to not see the shift in all knowledge queries going to LLMs lately.",
                  "score": 1,
                  "created_utc": "2026-02-10 00:57:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4h51rp",
          "author": "wapxmas",
          "text": "Even as a coding model it surprises me well enough to use it even for real tasks, speed it pretty usableÂ ",
          "score": 2,
          "created_utc": "2026-02-09 18:30:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ir5ch",
          "author": "getfitdotus",
          "text": "its a fantastic model for the size punches way above and the speed! :) really like what they did here. I run this in fp8 and its great. ",
          "score": 2,
          "created_utc": "2026-02-09 23:20:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ix4e3",
              "author": "Iory1998",
              "text": "I can relate, hence the post. In a few days or a week, we will get Qwen-3.5, and I am looking forward to all the new models. Soon, I might graduate from using Gemini :D",
              "score": 2,
              "created_utc": "2026-02-09 23:53:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ia7g5",
          "author": "twd000",
          "text": "How much RAM does it consume? I have a 16 GB GPU",
          "score": 2,
          "created_utc": "2026-02-09 21:52:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ixjw5",
              "author": "Iory1998",
              "text": "I use the Q8 with 24GB or Vram and 96GB or RAM. If you have 96GB of RAM, you can run the Q8 easily.",
              "score": 2,
              "created_utc": "2026-02-09 23:55:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4j0nhz",
                  "author": "twd000",
                  "text": "Do you allow the LLM to split across CPU and GPU? I thought I was supposed to keep it contained to one or the other",
                  "score": 1,
                  "created_utc": "2026-02-10 00:12:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gukyc",
          "author": "No_Conversation9561",
          "text": "It works really well with OpenClaw. Iâ€™m using MLX 8bit version.",
          "score": 3,
          "created_utc": "2026-02-09 17:41:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ia10v",
              "author": "dan-lash",
              "text": "On what hardware? I have a m1max 64gb and qwen3 really only works fast enough at 14b on llama, maybe I need to get the mlx version",
              "score": 1,
              "created_utc": "2026-02-09 21:51:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4jji9f",
                  "author": "1-800-methdyke",
                  "text": "The 4bit MLX of Qwen-3-Coder-Next works great on 64gb M1 Max on latest LMStudio, doing around 45t/s.",
                  "score": 2,
                  "created_utc": "2026-02-10 02:01:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gx93o",
              "author": "Iory1998",
              "text": "Can you tell me how you use it?",
              "score": 1,
              "created_utc": "2026-02-09 17:54:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ja8d6",
                  "author": "No_Conversation9561",
                  "text": "  \"models\": {\n    \"providers\": {\n      \"lmstudio\": {\n        \"baseUrl\": \"http://127.0.0.1:1234/v1\",\n        \"apiKey\": \"None\",\n        \"api\": \"openai-responses\",\n        \"models\": [\n          {\n            \"id\": \"qwen3-coder-next@8bitâ€\n            \"name\": \"Qwen3-Coder-Next\",\n            \"reasoning\": false,\n            \"input\": [\"text\"],\n            \"cost\": {\n              \"input\": 0,\n              \"output\": 0,\n              \"cacheRead\": 0,\n              \"cacheWrite\": 0\n            },\n            \"contextWindow\": 262144,\n            \"maxTokens\": 8192\n          }\n        ]\n      }\n    }\n  },\n  \"agents\": {\n    \"defaults\": {\n      \"model\": {\n        \"primary\": \"lmstudio/qwen3-coder-next@8bit\"\n      },\n      \"maxConcurrent\": 4,\n      \"subagents\": {\n        \"maxConcurrent\": 8\n      },\n      \"compaction\": {\n        \"mode\": \"safeguard\"\n      },\n      \"workspace\": \"/home/No_Conversation9561/.openclaw/workspace\"\n    }\n  },\n\n\n\n\n\nI added this to my .openclaw/openclaw.json",
                  "score": 4,
                  "created_utc": "2026-02-10 01:07:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4h4ejj",
          "author": "Potential_Block4598",
          "text": "That is actually true",
          "score": 1,
          "created_utc": "2026-02-09 18:27:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4he63f",
          "author": "Soft_Syllabub_3772",
          "text": "Which model weight r u refering to?",
          "score": 1,
          "created_utc": "2026-02-09 19:13:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hhycz",
              "author": "Iory1998",
              "text": "(Bartowski)\\_Qwen3-Coder-Next-GGUF-Q8\\_0",
              "score": 2,
              "created_utc": "2026-02-09 19:31:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4hi3bp",
                  "author": "Soft_Syllabub_3772",
                  "text": "30b ?",
                  "score": 2,
                  "created_utc": "2026-02-09 19:32:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4iw6mu",
                  "author": "nunodonato",
                  "text": "any specific reason for preferring bartowski vs unsloth's quants?",
                  "score": 1,
                  "created_utc": "2026-02-09 23:47:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hebij",
          "author": "Soft_Syllabub_3772",
          "text": "Also pls share your config n settings :)",
          "score": 1,
          "created_utc": "2026-02-09 19:14:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hi2ti",
              "author": "Iory1998",
              "text": "I use LM Studio since it has a refined UX and super easy to use.",
              "score": 4,
              "created_utc": "2026-02-09 19:32:25",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4j4ba8",
              "author": "Iory1998",
              "text": "https://preview.redd.it/yje0lyogbkig1.png?width=739&format=png&auto=webp&s=d9aa0eb86e22d2d7e37e98ec638121fdbf35c37f\n\n",
              "score": 1,
              "created_utc": "2026-02-10 00:33:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4m4ibc",
          "author": "Free_Elderberry_7587",
          "text": "I really like using it; but when I download a .gguf version and load it into Ollama... well, itâ€™s difficult to 'chat' with it.   \nIt doesn't stop generating tokens, it hallucinates, etc.   \nHow can I easily set up Qwen3 with Ollama? ",
          "score": 1,
          "created_utc": "2026-02-10 13:51:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4my2xr",
              "author": "Iory1998",
              "text": "1- Download LM Studio.  \n2- Run installer  \n3-Go to models tab, and download the model  \n4- Run the model",
              "score": 1,
              "created_utc": "2026-02-10 16:20:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5h7kk8",
          "author": "FPham",
          "text": "Well, it didn't fool me as a coder model, that's for sure, hahahaha.\n\nBut I think what you are looking at is removing a bit more of story and general chit chat scrubbed X dataset and adding more coding dataset in pretraining. The good % is a voodoo and you can't really plan it. Pretraining needs to have everything possible but the percentages  differ from model to model.",
          "score": 1,
          "created_utc": "2026-02-15 08:29:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ip4si",
          "author": "Fuzzdump",
          "text": "Completely agree, this has replaced the other Qwen models as my primary local model now. The fact that it's also an excellent coding model is the cherry on top.",
          "score": 1,
          "created_utc": "2026-02-09 23:09:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ixa2u",
              "author": "Iory1998",
              "text": "I can't speak of its coding capabilities as I don't code. But, I hear a lot of good things from coders in sub.",
              "score": 1,
              "created_utc": "2026-02-09 23:53:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jd22e",
          "author": "lol-its-funny",
          "text": "Qwen released GGUFs themselves -- curious why people are downloading unsloth and Bartowski ? Unsloth's quants have been shaky recently (https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF/discussions) with llama.cpp 0-day bugs and inconsistent tool calling, so I was considering the official Qwen GGUFs.\n\nCurious to hear from others on this",
          "score": 1,
          "created_utc": "2026-02-10 01:24:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l1bfz",
              "author": "jubilantcoffin",
              "text": "The official ones have the exact same issues.",
              "score": 2,
              "created_utc": "2026-02-10 08:43:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4jjhxl",
              "author": "Iory1998",
              "text": "It's more like a habbit for me. I just default back to Bartowski's quants. So far, this particular quant is working for me.",
              "score": 1,
              "created_utc": "2026-02-10 02:01:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jrbvm",
          "author": "mpw-linux",
          "text": "mlx-community/LFM2.5-1.2B-Thinking-8bit\n\nI asked the question: how can we become more happy in life?\n\nresponse:\n\n\\### Final Note: \\*\\*Happiness is a Practice\\*\\*\n\nHappiness is not a constant state but a series of choices and habits. Progress takes timeâ€”be patient with yourself. Small, consistent actions compound over time, creating lasting change. Remember: True joy often lies in the simplicity of moments, connection, or growth, not just grand achievements. ðŸŒ¿\n\n\n\nBy integrating these practices, you foster resilience, purpose, and contentment, creating a foundation for sustained well-being.\n\nI feel better already !",
          "score": 1,
          "created_utc": "2026-02-10 02:47:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jyzxl",
              "author": "Iory1998",
              "text": ":D",
              "score": 1,
              "created_utc": "2026-02-10 03:34:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4j1ib9",
          "author": "No_Farmer_495",
          "text": "Is the REAP quantized version still good for this reasoning/general purpose? Given that Reap versions usually focus on coding aspects..",
          "score": 0,
          "created_utc": "2026-02-10 00:17:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4j4k2m",
              "author": "Iory1998",
              "text": "Coder-Next is not a reasoning model. I tried some REAP models and they didn't work well for me. They were as slow as the non REAP models and quality degraded. That's my experience anyway.",
              "score": 3,
              "created_utc": "2026-02-10 00:34:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4j5bis",
                  "author": "No_Farmer_495",
                  "text": "Ah, could you give me an example? I was planning on using the REAP model quant 4(K_M) , like for coding I assume it was about the same right? For conversation/reasoning(normal reasoning) in general what's the difference?? I'm asking this due to vram/ram constraints. 48B quant 4 = around 27 vram/ram vs 80B quant 4 = 44+ vram/ram",
                  "score": 0,
                  "created_utc": "2026-02-10 00:39:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4je8q0",
          "author": "Lazy-Pattern-5171",
          "text": "Congratulations, happy for you, but I only have 48GB VRAM so donâ€™t rub it in.",
          "score": 0,
          "created_utc": "2026-02-10 01:31:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jgo8m",
              "author": "Iory1998",
              "text": "I only have 32GBðŸ¤¦â€â™‚ï¸",
              "score": 2,
              "created_utc": "2026-02-10 01:45:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4k23v0",
              "author": "silenceimpaired",
              "text": "Whatâ€™s your RAM? Considering this is a MoEâ€¦ using a GGUF at 4 bit should let you run it.",
              "score": 1,
              "created_utc": "2026-02-10 03:54:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jktti",
          "author": "simracerman",
          "text": "Curious, did you try the MoE version. It seems to be smaller by at least 5GB than Q4_K_XL.",
          "score": 0,
          "created_utc": "2026-02-10 02:09:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jz2vb",
              "author": "Iory1998",
              "text": "There is only one version and it's an MoE!",
              "score": 2,
              "created_utc": "2026-02-10 03:35:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4k6vb5",
                  "author": "simracerman",
                  "text": "I definitely was sleep typing, lol.\n\nI meant, did you try the MXFP4 version. Unsloth has one.",
                  "score": 2,
                  "created_utc": "2026-02-10 04:26:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4qpagv",
          "author": "allenasm",
          "text": "I just wish it had vision. You canâ€™t paste results or anything to it visually.",
          "score": 0,
          "created_utc": "2026-02-11 03:56:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4t2dwi",
              "author": "Iory1998",
              "text": "Patience my friend. Qwen-3.5 has the same architecture and comes with vision capabilities out of the box.",
              "score": 1,
              "created_utc": "2026-02-11 14:49:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ht865",
          "author": "Porespellar",
          "text": "Sorry, my OCD wonâ€™t let me mentally consider it for anything other than coding because it says â€œCoderâ€ in the model name.",
          "score": -5,
          "created_utc": "2026-02-09 20:28:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ixxud",
              "author": "Iory1998",
              "text": "I smell sarcasm :D",
              "score": 3,
              "created_utc": "2026-02-09 23:57:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ig79t",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -4,
          "created_utc": "2026-02-09 22:22:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ixc31",
              "author": "Iory1998",
              "text": "Really? How do you know that?",
              "score": 3,
              "created_utc": "2026-02-09 23:54:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gwhuj",
          "author": "[deleted]",
          "text": "Some say a picture is worth a thousand words:\n\n\nhttps://www.reddit.com/r/LocalLLaMA/comments/1qvy6ig/the_king_has_returned/",
          "score": -8,
          "created_utc": "2026-02-09 17:50:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r03wfq",
      "title": "Bad news for local bros",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/ui5ovstbygig1.jpeg",
      "author": "FireGuy324",
      "created_utc": "2026-02-09 13:14:31",
      "score": 522,
      "num_comments": 231,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r03wfq/bad_news_for_local_bros/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4gcy27",
          "author": "AutomataManifold",
          "text": "No, this is good news. Sure, you can't run it on your pile of 3090s, but the open availability of massive frontier models is a healthy thing for the community. It'll get distilled down and quantized into things you can run on your machine. If open models get stuck with only tiny models, then we're in trouble long-term.",
          "score": 171,
          "created_utc": "2026-02-09 16:17:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hlh5v",
              "author": "True_Requirement_891",
              "text": "Exactly.",
              "score": 17,
              "created_utc": "2026-02-09 19:48:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4jcrkn",
                  "author": "bitcodler",
                  "text": "That's what she said",
                  "score": 0,
                  "created_utc": "2026-02-10 01:22:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4kjkmx",
              "author": "foldl-li",
              "text": "Correct. But these huge models are love letters to millionaires/companies, not ordinaries.",
              "score": 8,
              "created_utc": "2026-02-10 06:01:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fiyw6",
          "author": "Impossible_Art9151",
          "text": "indeed difficult for local seups. as long as they continue to publish smaller models I do not care about this huge frontiers. curious to see how it compares with openai, anthropic.",
          "score": 168,
          "created_utc": "2026-02-09 13:40:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ga84q",
              "author": "FrankNitty_Enforcer",
              "text": "100%. For those of us that work in shops that want to run big budget workloads I love that there are contenders in every weight class, so to speak. \n\nNot that it makes sense in every scenario, but hosting these on IaaS or on-prem to keep all inference private is a major advantage over closed-weight, API-only offerings regardless of what privacy guarantees the vendor makes",
              "score": 38,
              "created_utc": "2026-02-09 16:04:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gln2h",
                  "author": "Infninfn",
                  "text": "You don't even need open weights....Azure AI Foundry hosts full fat Opus 4.6 and GPT-5.2 without reusing your prompts and data. Just saying.",
                  "score": -3,
                  "created_utc": "2026-02-09 16:58:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fl9p7",
              "author": "tarruda",
              "text": "Try Step 3.5 Flash if you have 128GB. Very strong model.",
              "score": 43,
              "created_utc": "2026-02-09 13:53:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gl51d",
                  "author": "jinnyjuice",
                  "text": "The model is 400GB. Even if it's 4 bit quant, it's 100GB. That leaves no room for context, no? Better to have at least 200GB.",
                  "score": 11,
                  "created_utc": "2026-02-09 16:56:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fj2nu",
              "author": "FireGuy324",
              "text": "I guarantee it's sonnet 4.5 level. The writing is on another level",
              "score": 6,
              "created_utc": "2026-02-09 13:41:09",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4hp74c",
              "author": "slippery",
              "text": "This is why I tapped out and went with together.ai. it also took the strain off my budget. Maybe hardware will catch up at some point and there won't be chip and ram shortages.",
              "score": 0,
              "created_utc": "2026-02-09 20:08:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ffz0g",
          "author": "nvidiot",
          "text": "I hope they produce two more models - a lite model with a similar size as current GLM 4.x series, and an Air version. It would be sad to see the model completely out of reach for many local users.",
          "score": 92,
          "created_utc": "2026-02-09 13:22:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fminl",
              "author": "geek_at",
              "text": "I'm sure someone will start a religion or cult stating the peak of AI was at 20B parameters and they will only work with models of that size for hundreds of years.\n\nThey might be called the LLAmish",
              "score": 114,
              "created_utc": "2026-02-09 14:01:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4g14en",
                  "author": "oodelay",
                  "text": "And instead of using RAM chips, they use barns filled with old people remembering a bunch of numbers with an fast talking auctioneer telling everyone when to speak their numbers and weight. \n\nIt's a subculture called the RAMish",
                  "score": 40,
                  "created_utc": "2026-02-09 15:20:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4fxkkg",
                  "author": "Aaaaaaaaaeeeee",
                  "text": "eventually due to a fresh wave of ram shortages, they had to quantize their young. 23BandMe helped facilitate proper QAT/QAD recovery for self-attention and a direct injection of mmproj, which was actually their downfall.Â ",
                  "score": 20,
                  "created_utc": "2026-02-09 15:02:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4fyt5m",
                  "author": "SpicyWangz",
                  "text": "Thatâ€™s it. Youâ€™re going in time out.",
                  "score": 7,
                  "created_utc": "2026-02-09 15:08:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4gfd2h",
                  "author": "i_am_fear_itself",
                  "text": "> LLAmish\n\nGrrr! take your upvote.",
                  "score": 4,
                  "created_utc": "2026-02-09 16:29:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4hth4b",
                  "author": "gregusmeus",
                  "text": "I wouldnâ€™t call that pun LLame but just a little LLAmish.",
                  "score": 4,
                  "created_utc": "2026-02-09 20:29:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4g6d5k",
              "author": "Cferra",
              "text": "I think that it's getting to that point where these models are eventually going to be outside normies or even enthusiasts reach.  ",
              "score": 4,
              "created_utc": "2026-02-09 15:46:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fshl1",
          "author": "tmvr",
          "text": "The situation would not be so bad if not for the RAMpocalypse. We have pretty good models in the \\~30B range and then have the better ones in the 50-60-80 GB size range MoE (GLM 4.6V, Q3 Next, gpt-oss 120B), so if the consumer GPUs would have progressed as expected we would have a 5070Ti Super 24GB probably in the 700-800 price range and a 48GB fast new setup would be in a relatively normal price range. Without being dependent on now many years old 3090 cards. But of course this is not where we are.",
          "score": 37,
          "created_utc": "2026-02-09 14:34:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4i9pm6",
              "author": "ThePixelHunter",
              "text": "It's only been a few months since RAM prices exploded. If the rumored Super series were coming, it wouldn't have been until late this year at best. They'd also be scalped to hell.",
              "score": 5,
              "created_utc": "2026-02-09 21:49:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ihpfh",
                  "author": "tmvr",
                  "text": "The Super cards were to be introduced at CES a month ago with availability in the weeks after as usually. That's obviously out of the windows now and the current situation is that the Super cards will be skipped and the next releases will be the 60 series at the end of 2027. Of course NV has the option and opportunity to change all that in case something happens and there is a hickup in the whole \"we need all the memory in the world for AI\" situation.",
                  "score": 4,
                  "created_utc": "2026-02-09 22:30:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4kawxd",
              "author": "toadi",
              "text": "my 2024 razer with rtx4090 has 24GB. Everything seems a downgrade after if I go 50xx. I can not afford a 5090 either :D",
              "score": 2,
              "created_utc": "2026-02-10 04:54:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fft13",
          "author": "ciprianveg",
          "text": "20x3090..",
          "score": 124,
          "created_utc": "2026-02-09 13:21:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fsqtv",
              "author": "HyperWinX",
              "text": "14 should work, if you run it at Q4 and you need a lot of context",
              "score": 31,
              "created_utc": "2026-02-09 14:36:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4fz54u",
                  "author": "pmp22",
                  "text": "Q0 on my P40 lets go",
                  "score": 33,
                  "created_utc": "2026-02-09 15:10:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4gkvj0",
                  "author": "YungCactus43",
                  "text": "since GLM 5 is going to be based on deepseek like GLM flash thereâ€™s going to be context compression on VLLM. it should take about 10gb of vram to run it at full context",
                  "score": 3,
                  "created_utc": "2026-02-09 16:55:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4g7mu1",
              "author": "alphapussycat",
              "text": "V100 is getting pretty popular. I don't know if you can bifurcate twice, or if it's trice.\n\n2x v100 32gb, they feed into an nvlink and one adapter card. But I'm not sure if the adapter card uses bifurcation.\n\n10 of these give you 640gb vram. Cost is something like $15k. +mobo with at least 5x x8 with bifurcation.\n\nThe scaling of AI is basically exponential... On the hardware that is. Like exponential hardware for linear improvement.",
              "score": 6,
              "created_utc": "2026-02-09 15:52:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4hwthk",
                  "author": "Aphid_red",
                  "text": "Name for that is \"logarithmic\". When using X memory, you get Log(X) quality.",
                  "score": 7,
                  "created_utc": "2026-02-09 20:46:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4hvquu",
                  "author": "meltbox",
                  "text": "You can run the through plz switches too",
                  "score": 1,
                  "created_utc": "2026-02-09 20:40:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4ghphk",
              "author": "Healthy-Nebula-3603",
              "text": "Taste oy 480 VRAM ...still not enough:)",
              "score": 2,
              "created_utc": "2026-02-09 16:40:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4h08a7",
              "author": "ballshuffington",
              "text": "ðŸ¤£",
              "score": 2,
              "created_utc": "2026-02-09 18:08:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fmner",
          "author": "__JockY__",
          "text": "Godsammit, you mean I need _another_ four RTX 6000s??? Excellent, my wife was just wondering when Iâ€™d invest in more of thoseâ€¦",
          "score": 78,
          "created_utc": "2026-02-09 14:01:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4g2oz8",
              "author": "Porespellar",
              "text": "https://i.redd.it/8mbx5pu6mhig1.gif",
              "score": 41,
              "created_utc": "2026-02-09 15:28:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4gjht7",
              "author": "MelodicRecognition7",
              "text": "you mean your AI waifu?",
              "score": 15,
              "created_utc": "2026-02-09 16:48:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gu1ij",
                  "author": "Cool-Chemical-5629",
                  "text": "This brings the whole \"Wife spends all the money\" to a whole new level, doesn't it? ðŸ¤£",
                  "score": 14,
                  "created_utc": "2026-02-09 17:38:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4lsdp8",
                  "author": "Phonehippo",
                  "text": "As my learn AI project, I just finished making one of these on my qwen3-8b only to find out she's retarded. But atleast her avatar is pretty and she loves her props and animations lol.Â ",
                  "score": 1,
                  "created_utc": "2026-02-10 12:38:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4hlkmr",
              "author": "getfitdotus",
              "text": "yes i need 4 more too , can u order mine also get a better discount. I also will require the rack server to fit all 8. ",
              "score": 3,
              "created_utc": "2026-02-09 19:49:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4g0hma",
          "author": "No_Conversation9561",
          "text": "This hobby of mine is getting really expensive",
          "score": 19,
          "created_utc": "2026-02-09 15:17:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fpvus",
          "author": "Blues520",
          "text": "Gonna need Q0.1 quants",
          "score": 17,
          "created_utc": "2026-02-09 14:20:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hiea3",
              "author": "Aaaaaaaaaeeeee",
              "text": "Someone show ik: https://arxiv.org/abs/2506.13771",
              "score": 3,
              "created_utc": "2026-02-09 19:33:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fivgq",
          "author": "AppealSame4367",
          "text": "Step 3.5 Flash",
          "score": 25,
          "created_utc": "2026-02-09 13:39:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fjqy2",
              "author": "tarruda",
              "text": "This is my new favorite model. It still has some issues with infinite reasoning loops, but devs are investigating and will probably fix in a upcoming fine tune: https://github.com/ggml-org/llama.cpp/pull/19283#issuecomment-3870270263",
              "score": 12,
              "created_utc": "2026-02-09 13:45:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4hlh0y",
              "author": "getfitdotus",
              "text": "would like to see the next minimax beat this one since its really the perfect size. I am still somewhat disappointed on glm 5 being so much larger. I already have quite a bit of $$$ invested in local hardware. even coder next is really good for its size. ",
              "score": 2,
              "created_utc": "2026-02-09 19:48:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fsfg5",
          "author": "pmttyji",
          "text": "Hope they each release 100B models(and more) additionally later.",
          "score": 16,
          "created_utc": "2026-02-09 14:34:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4h4lhj",
          "author": "eibrahim",
          "text": "Honestly I think this is fine and people are overreacting. The real value of these massive open models isnt running them on your gaming PC. Its that they exist as open weights at all. A year ago the best open model was maybe 70B and it was nowhere close to frontier. Now we got 700B+ open models competing with the best closed ones.\n\nThe distillation pipeline has gotten insanely good too. Every time a new massive teacher model drops, the 30-70B range gets a noticeable bump within weeks. Ive been using Qwen derivatives for production workloads and the quality jump from distilled models is real.\n\nPlus lets be honest, for 95% of actual use cases a well tuned 30B model handles it just fine. The remaining 5% is where you hit the API for a frontier model anyway.",
          "score": 16,
          "created_utc": "2026-02-09 18:28:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z4aep",
              "author": "Blues520",
              "text": "When you say a well tuned 30B model, are you referring to coding or something else?",
              "score": 1,
              "created_utc": "2026-02-12 13:12:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gwm4o",
          "author": "jhov94",
          "text": "This is great news if you look past the immediate future. The future of small models depends on more labs having access to large SOTA models. This gives them direct access to a high quality, large SOTA model to distill into smaller ones. ",
          "score": 8,
          "created_utc": "2026-02-09 17:51:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4h3cns",
          "author": "borobinimbaba",
          "text": "You guys remember those old days that 32mb of ram was alot ? It was like 30 years ago.\n\nI'm sure running local llms on the next 30 years hardware would be cheap, most of us are just to old to see those days or maybe care for it.",
          "score": 6,
          "created_utc": "2026-02-09 18:22:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4le28e",
              "author": "techno156",
              "text": "I don't know, given that everything seems to have landed on a sort of steady-state, it seems rather more like we'll be stuck on 16GB or thereabouts for at least the next decade or so, for most machines.\n\nEspecially with memory costing as much as it is.",
              "score": 3,
              "created_utc": "2026-02-10 10:45:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fk7t4",
          "author": "Glad-Audience9131",
          "text": "as expected. will only go up in size.",
          "score": 23,
          "created_utc": "2026-02-09 13:47:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4geesn",
              "author": "One-Employment3759",
              "text": "As expected no more innovation from AI research, just boring scaling.",
              "score": 6,
              "created_utc": "2026-02-09 16:24:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fusln",
          "author": "FullstackSensei",
          "text": "99% of people don't need frontier models 95% of the time. I'd even argue the biggest benefit of such models is for AI labs to continue to improve the variety and quality of their training data to train (much) smaller models. That's a big part of the reason why we continue to see much smaller models beat frontier models from one year before if not less.",
          "score": 33,
          "created_utc": "2026-02-09 14:47:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4h3810",
              "author": "the320x200",
              "text": "Sour grapes. I didn't want to run it anyway! /s",
              "score": 11,
              "created_utc": "2026-02-09 18:22:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ghxm8",
              "author": "TopNFalvors",
              "text": "Honest question, what would be good for 99% of people 95% of the time?",
              "score": 1,
              "created_utc": "2026-02-09 16:41:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gtjf6",
                  "author": "FullstackSensei",
                  "text": "An ensemble of models for the various tasks one needs. For ex: I know use Qwen3 VL 30B for OCR tasks, Qwen3 Coder 30B/Next or Minimax 2.1 for coding tasks and gpt-oss-120b or Gemma3 27B for general purpose chat. If we exclude Minimax, all the others can be run on three 24GB cards like P40s with pretty decent performance. P40 prices seem to have come down a bit (200-250 a pop), ao you can still ostensibly build a machine with three P40s for a little over 1k using a Broadwell Xeon and 16-32GB RAM.",
                  "score": 5,
                  "created_utc": "2026-02-09 17:36:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4gn7ee",
                  "author": "Jon_vs_Moloch",
                  "text": "Something like a current 4B model, but add search and tool calling.",
                  "score": 1,
                  "created_utc": "2026-02-09 17:06:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gwpva",
              "author": "mouseofcatofschrodi",
              "text": "Since it is what I want to believe, it must be true",
              "score": 0,
              "created_utc": "2026-02-09 17:51:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4hc6x8",
              "author": "ProfessionalSpend589",
              "text": "My 2x Strix Halos are too much power efficient and I can't warm my room in the winter with the little sized LLMs (had to wear my hoodie, because I'm actually cold).",
              "score": -1,
              "created_utc": "2026-02-09 19:04:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gqsmm",
          "author": "Conscious_Cut_6144",
          "text": "You underestimate my power.   \n\n\nhttps://preview.redd.it/jzzxdihq6iig1.jpeg?width=1080&format=pjpg&auto=webp&s=660ccda6439ca713d78d21c2d96aa27622cecce8\n\n",
          "score": 18,
          "created_utc": "2026-02-09 17:23:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hy0sm",
              "author": "Jonodonozym",
              "text": "You underestimate my power bill",
              "score": 17,
              "created_utc": "2026-02-09 20:51:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ihmtd",
              "author": "panchovix",
              "text": "16 RTX 3090s, so 384GB VRAM? I wonder if you will be able to run GLM5 at Q4, hoping it does.\n\nNow for more VRAM and TP, you have no other way than to add another 16 3090s (?",
              "score": 2,
              "created_utc": "2026-02-09 22:30:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4j9rfe",
                  "author": "Conscious_Cut_6144",
                  "text": "VLLM/Sglang are not great at fitting models that should just barely fit in theory.\n\nI have 1 pro6000 in another machine, going to have to figure out how to get them working together efficiently if this model is as good as I hope.",
                  "score": 1,
                  "created_utc": "2026-02-10 01:04:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gr44c",
          "author": "chloe_vdl",
          "text": "Honestly the real win here isn't running these monsters locally - it's having open weights to distill from. The knowledge compression pipeline from 700B+ teachers down to 30-70B students has gotten way more sophisticated. Look at what Qwen and Llama derivatives managed to squeeze out of their bigger siblings.\n\nThe local scene isn't dead, it's just shifting upstream. We become the fine-tuners and distillers rather than the raw inference crowd. Which tbh is probably more interesting work anyway.",
          "score": 19,
          "created_utc": "2026-02-09 17:25:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fpq6d",
          "author": "Ult1mateN00B",
          "text": "I have been having loads of fun with minimax-m2.1-reap-30-i1, lightning fast and great reasoning. 45tok/s to be exact on my 4x AI PRO R9700. I use the Q4\\_1 quant, 101GB is a nice fit for me.",
          "score": 5,
          "created_utc": "2026-02-09 14:19:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ggr44",
          "author": "phenotype001",
          "text": "MiniMax is good though and the q4 barely fits in 128 RAM but fits. ",
          "score": 6,
          "created_utc": "2026-02-09 16:35:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gi9xt",
          "author": "DataGOGO",
          "text": "So roughly 390GB in any Q4, not too bad for a frontier model. \n\nBest way to run local would be 4 H200 NVL's, but that is what? $130k?",
          "score": 4,
          "created_utc": "2026-02-09 16:43:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gnlav",
          "author": "Mauer_Bluemchen",
          "text": "M5 Ultra with 1 TB upcoming ;-)\n\nOr a cluster of 3-4x M3 Ultras - which would be rather slow of course. ",
          "score": 5,
          "created_utc": "2026-02-09 17:08:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fmm1d",
          "author": "ResidentPositive4122",
          "text": "Open models are useful and benefit the community even if they can't be (easily / cheaply) hosted locally. You can always rent to create datasets or fine-tune and run your own models. The point is to have them open.\n\n(that's why the recent obsession with local only on this sub is toxic and bad for the community, but it is what it is...)",
          "score": 12,
          "created_utc": "2026-02-09 14:01:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fjynm",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 4,
          "created_utc": "2026-02-09 13:46:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fxgxc",
          "author": "a_beautiful_rhind",
          "text": "Damn.. so I can expect Q2 quants and 10t/s unless something changes with numa and/or ddr4 prices. RIP glm-5.",
          "score": 4,
          "created_utc": "2026-02-09 15:01:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g88lp",
          "author": "VoidAlchemy",
          "text": "I didn't check to see if GLM-5 will use QAT targeting \\~4ish BPW for sparse routed experts like the two most recent Kimi-K2.5/K2-Thinking did. This at least makes the \"full size\" model about 55% of what it would otherwise be if full bf16.\n\nIf we quantize the attn/shexp/first N dense layers, it will help a little bit but yeah 44B active will definitely be a little slower than DS/Kimi...",
          "score": 3,
          "created_utc": "2026-02-09 15:55:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gj56a",
          "author": "CanineAssBandit",
          "text": "well shit no wonder it feels more coherent in its writing. it's way bigger active and way bigger period\n\nVERY happy to see that we have another open weights power player keeping pressure on OAI and Anthropic. No replacement for displacement.\n\nI hope they don't leave in the disturbing \"safety guidelines policy\" checker thing always popping up in the thinking in GLM 4.7. Pony Alpha doesn't so I'm hopeful that their censoring got less obtrusive if nothing else",
          "score": 5,
          "created_utc": "2026-02-09 16:47:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4iwbqv",
          "author": "lgk01",
          "text": "In two years you'll be able to run better ones on 16gb of vram (COPIUM MODE)",
          "score": 4,
          "created_utc": "2026-02-09 23:48:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fhahr",
          "author": "Expensive-Paint-9490",
          "text": "Seems that performance in LLM has already plateaued, and meaningful improvements only come from size increase.\n\nSo much for people spamming that AGI is six months away.",
          "score": 45,
          "created_utc": "2026-02-09 13:30:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4foj7h",
              "author": "sekh60",
              "text": "While the \"I\" part is for sure questionable at times, my N(atural)GI uses only about 20 Watts.",
              "score": 23,
              "created_utc": "2026-02-09 14:12:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4fynaz",
                  "author": "My_Unbiased_Opinion",
                  "text": "Yep and I can assure my NGI has way less than 745B functional parameters. Hehe",
                  "score": 8,
                  "created_utc": "2026-02-09 15:07:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4idlo8",
                  "author": "YouCantMissTheBear",
                  "text": "Your brain isn't working outside your body, stop gaming the metrics",
                  "score": 3,
                  "created_utc": "2026-02-09 22:09:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4gtjay",
                  "author": "Charuru",
                  "text": "We'll get there through chip improvements instead of architectural improvements.",
                  "score": 1,
                  "created_utc": "2026-02-09 17:36:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fzj38",
              "author": "pmp22",
              "text": "Architecture changes will come, it's just not there just yet. LLMs will be small latent space reasoning cores with external memory. Encoding vast knowledge in the weights like we do now is not the future IMHO.",
              "score": 9,
              "created_utc": "2026-02-09 15:12:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4fhnrk",
              "author": "DesignerTruth9054",
              "text": "I think once these models are distilled to smaller models we will get direct performance improvements",
              "score": 20,
              "created_utc": "2026-02-09 13:32:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4g8f7l",
                  "author": "beryugyo619",
                  "text": "why tf that work? not doubting it works, but it's weird that it does",
                  "score": 3,
                  "created_utc": "2026-02-09 15:56:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4fohek",
                  "author": "disgruntledempanada",
                  "text": "But ultimately be nowhere near where the large models are sadly.",
                  "score": 7,
                  "created_utc": "2026-02-09 14:12:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fs7v4",
              "author": "nomorebuttsplz",
              "text": "That makes no sense. If you can compare like sized models across times span there is literally no case in which the increases have not been significant.\n\nTwo things can happen simultaneously: models can get bigger and models can get better per size.",
              "score": 12,
              "created_utc": "2026-02-09 14:33:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gb6kd",
                  "author": "Nowitcandie",
                  "text": "\\-Models are getting bigger but already suffering from diminishing returns at an accelerated pace. At some point this will reach its limit where bigger won't increase performance at all. Diminishing marginal gains tend towards zero. Making the best models smaller too has it's limits without some serious breakthroughs (perhaps scalable quantum computing.)",
                  "score": 2,
                  "created_utc": "2026-02-09 16:09:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4iim4s",
              "author": "Xyrus2000",
              "text": "LLMs are just one form of AI, and an LLM isn't designed to achieve AGI.\n\nAGI isn't going to come from a system that can't learn and self-improve. All LLMs are \"fixed brains\". They don't learn anything after they're trained. They're like the movie Memento. You've got their training and whatever the current context is. When the context disappears, they're back to just their training.\n\nWe have the algorithms. We're just waiting for the hardware to catch up. Sometime within the next 5 to 10 years.",
              "score": 3,
              "created_utc": "2026-02-09 22:35:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5h8x7n",
                  "author": "FPham",
                  "text": "They are also autoregressive which holds them back ",
                  "score": 1,
                  "created_utc": "2026-02-15 08:42:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fko4e",
              "author": "RIPT1D3_Z",
              "text": "Step 3.5 Flash proves it's wrong.",
              "score": 7,
              "created_utc": "2026-02-09 13:50:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4fxpl8",
                  "author": "a_beautiful_rhind",
                  "text": "Flash is strong but not that strong. Kimi and 5 feel smarter.",
                  "score": 6,
                  "created_utc": "2026-02-09 15:03:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4jgg3m",
                  "author": "Zc5Gwu",
                  "text": "Step 3.5 Flash feels like qwq part 2. It thinks *a lot*.",
                  "score": 1,
                  "created_utc": "2026-02-10 01:44:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fqkbv",
              "author": "Nowitcandie",
              "text": "Hard agree, and the scaling economics seems to hold to diminishing marginal returns. Perhaps in part because everybody scaling simultaneously is driving up chip and hardware prices.Â ",
              "score": 3,
              "created_utc": "2026-02-09 14:24:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gf1fm",
                  "author": "One-Employment3759",
                  "text": "Yeah, if everyone just acted normal instead of going bongobongo we could keep doing research instead of hype train.",
                  "score": 6,
                  "created_utc": "2026-02-09 16:27:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gck16",
              "author": "ThisWillPass",
              "text": "16 months.",
              "score": 1,
              "created_utc": "2026-02-09 16:15:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4fldki",
              "author": "iDefyU__",
              "text": "AGI?? Do you really believe that?",
              "score": -7,
              "created_utc": "2026-02-09 13:54:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4flto2",
                  "author": "vladlearns",
                  "text": "just 2000000000 more billons, bro\nAGI is almost here, trust me, broÂ ",
                  "score": 14,
                  "created_utc": "2026-02-09 13:57:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4flusw",
              "author": "abdouhlili",
              "text": "Lost me at AGI part.",
              "score": -8,
              "created_utc": "2026-02-09 13:57:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4j0g6j",
          "author": "ttkciar",
          "text": "... where is the bad news? I see none here!",
          "score": 3,
          "created_utc": "2026-02-10 00:11:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fgzoi",
          "author": "silenceimpaired",
          "text": "Where is this chart from?",
          "score": 5,
          "created_utc": "2026-02-09 13:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fkb9w",
              "author": "FireGuy324",
              "text": "https://preview.redd.it/p1lj1bxb4hig1.png?width=855&format=png&auto=webp&s=15cde1be894f1bb57c274f63a6078c3eb32f33ef\n\nDid some math  \nvocab\\_size Ã— hidden\\_size = 154,880 Ã— 6,144 = 951,403,520 q\\_a\\_proj: 6,144 Ã— 2,048 = 12,582,912 q\\_b\\_proj: 2,048 Ã— (64 Ã— 256) = 33,554,432 kv\\_a\\_proj: 6,144 Ã— (512 + 64) = 3,538,944 kv\\_b\\_proj: 512 Ã— (64 Ã— (192 + 256)) = 512 Ã— 28,672 = 14,680,064 o\\_proj: (64 Ã— 256) Ã— 6,144 = 16,384 Ã— 6,144 = 100,663,296 Total attention/couche = 165,019,648 Total attention (78Ã—) = 165,019,648 Ã— 78 = 12,871,532,544 gate\\_proj: 6,144 Ã— 12,288 = 75,497,472 up\\_proj: 6,144 Ã— 12,288 = 75,497,472 down\\_proj: 12,288 Ã— 6,144 = 75,497,472 Total MLP Dense/couche = 226,492,416 gate\\_up\\_proj: 6,144 Ã— (2 Ã— 2,048) = 25,165,824 down\\_proj: 2,048 Ã— 6,144 = 12,582,912 Total expert = 37,748,736 Experts (256 Ã— 37,748,736) = 9,663,676,416 Shared experts = 226,492,416 Total MoE layer = 9,890,168,832 Total MoE (77Ã—) = 9,890,168,832 Ã— 77 = 761,542,999,904 2 Ã— hidden*size = 2 Ã— 6,144 = 12,288 Total LayerNorm (78Ã—) = 12,288 Ã— 78 = 958,464 Embeddings: 951,403,520 Attention (78Ã—): 12,871,532,544 MLP Dense (1Ã—): 226,492,416 MoE (77Ã—): 761,542,999,904 LayerNorm (78Ã—): 958,464 TOTAL = 775,592,386,848 â‰ˆ 776b*",
              "score": 13,
              "created_utc": "2026-02-09 13:48:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4frbkc",
                  "author": "silenceimpaired",
                  "text": "Sad day for me. Guess itâ€™s 4.7 at 2bit for lifeâ€¦ unless they also have GLM 5 Air (~100b) and oooo GLM Water (~300b)",
                  "score": 2,
                  "created_utc": "2026-02-09 14:28:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4g4caf",
                  "author": "notdba",
                  "text": "3 dense + 75 sparse right?\n\nNumber of parameters on CPU: 6144 \\* 2048 \\* 3 \\* 256 \\* 75 = 724775731200\n\nWith IQ1\\_S\\_R4 (1.50 bpw): 724775731200 \\* 1.5 / 8 / (1024 \\* 1024 \\* 1024) = 126.5625 GiB\n\nBy moving 5\\~6 GiB to VRAM, this can still fit a 128 GiB RAM + single GPU setup.\n\nAnd just like magic, [https://github.com/ikawrakow/ik\\_llama.cpp/pull/1211](https://github.com/ikawrakow/ik_llama.cpp/pull/1211) landed right on time to free up several GiB of VRAM. We have to give it a try.",
                  "score": 1,
                  "created_utc": "2026-02-09 15:36:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gek1r",
          "author": "MerePotato",
          "text": "Ultimately if you want to push capabilities without a major architectural innovation you're probably gonna have to scale somewhat. Blame the consumer hardware market for not keeping up, not the labs.",
          "score": 6,
          "created_utc": "2026-02-09 16:25:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gep8c",
              "author": "FireGuy324",
              "text": "Blame the other corpos who makes GPU more expensive than they should be",
              "score": 3,
              "created_utc": "2026-02-09 16:26:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fnjm3",
          "author": "tarruda",
          "text": "They have a release cycle that is too short IMO. Did they have time to research innovative improvements or experiment with new training data/methods?\n\nThis will likely be a significant improvement over GLM 4.x as it has doubled the number of parameters, but it is not an impressive release if all they do is chase after Anthropic models.\n\nI would rather see open models getting more efficient while approaching performance of bigger models, as StepFun did with Step 3.5 Flash.",
          "score": 8,
          "created_utc": "2026-02-09 14:07:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4frmb8",
              "author": "nullmove",
              "text": "I think this was always their \"teacher\" model they were distilling down from for 4.x. And sure ideally they would like to do research too, but maybe the reality of economics doesn't allow that. Their major revenue probably comes from coding plans, and people are not happy with Sonnet performance when Opus 4.5 is two gen old now.",
              "score": 8,
              "created_utc": "2026-02-09 14:30:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4flwcm",
          "author": "WSATX",
          "text": "I'm too poor to be a local bro ðŸ¥²",
          "score": 4,
          "created_utc": "2026-02-09 13:57:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gdpm5",
              "author": "JLeonsarmiento",
              "text": "https://preview.redd.it/hcvgeyrnvhig1.jpeg?width=480&format=pjpg&auto=webp&s=0d7317e4a5b8c1f25feaee3b50c70d35fba212bf\n\n",
              "score": 1,
              "created_utc": "2026-02-09 16:21:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fhoec",
          "author": "power97992",
          "text": "Wait until u see ds v4â€¦.",
          "score": 6,
          "created_utc": "2026-02-09 13:32:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fzpcj",
              "author": "SpicyWangz",
              "text": "4t",
              "score": 2,
              "created_utc": "2026-02-09 15:13:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gve4o",
                  "author": "power97992",
                  "text": "Source?Â ",
                  "score": 1,
                  "created_utc": "2026-02-09 17:45:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fno52",
          "author": "ObviNotMyMainAcc",
          "text": "I mean, secondhand MI210's are coming down in price. They have 64gb of HBM a pop. 8 of those and some mild quants, done.\n\nOkay, that's still silly money, but running top spec models in any reasonable way always was.\n\nNot to mention NVFP4 and MXFP4 retain like 90 - 95% accuracy, so some serious size reduction is possible without sacrificing too much.\n\nNo, a Mac studio doesn't count unless you use almost no context. Maybe in the future some time as there are some really interesting transformer alternatives being worked on.\n\nSo not really doom and gloom.",
          "score": 4,
          "created_utc": "2026-02-09 14:07:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gmzo3",
              "author": "usrnamechecksoutx",
              "text": "\\>No, a Mac studio doesn't count unless you use almost no context.\n\nCan you elaborate?",
              "score": 3,
              "created_utc": "2026-02-09 17:05:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gr0rj",
                  "author": "ObviNotMyMainAcc",
                  "text": "At short contexts, they fast enough. As context gets longer, their speed degrades faster than other solutions. Prompt processing speed is not their strong suit.\n\nIt will be interesting to see how they go with subquadratic models which can have reasonable prompt processing speeds out to like 10 million tokens on more traditional hardware.",
                  "score": 1,
                  "created_utc": "2026-02-09 17:24:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fjegq",
          "author": "CommanderData3d",
          "text": "qwen?",
          "score": 2,
          "created_utc": "2026-02-09 13:43:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fl57o",
              "author": "tarruda",
              "text": "Apparently Qwen 3.5 initial release will have a 35b MoE: https://x.com/chetaslua/status/2020471217979891945\n\nHopefully they will also publish a LLM in the 190B - 210B range for 128GB devices.",
              "score": 12,
              "created_utc": "2026-02-09 13:53:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4fvtc3",
                  "author": "Impossible_Art9151",
                  "text": "device clustering is coming nowadays, 2 x dgx or 2 x strix => 256GB RAM.  \n400B models in q4 or 200b in fp6",
                  "score": 0,
                  "created_utc": "2026-02-09 14:52:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fr4vk",
          "author": "Johnny_Rell",
          "text": "Let's hope it's 1.58bit or somethingðŸ˜…",
          "score": 2,
          "created_utc": "2026-02-09 14:27:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fzh86",
          "author": "Lissanro",
          "text": "K2.5 is actually even larger since it also includes mmproj for vision. I run Q4_X quant of K2.5 the most on my PC, but for those who are yet to buy the hardware RAM prices are going to be huge issue.\n\n\nThe point is, it is memory cost issue rather than model size issue, which are going only to grow over time... I can only hope by the next time I need to upgrade prices will be better.",
          "score": 2,
          "created_utc": "2026-02-09 15:12:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g0acj",
          "author": "Septerium",
          "text": "Perhaps they are aiming to release something with native int-4 quantization? I think this has the potential to become an industry standard in the near future",
          "score": 2,
          "created_utc": "2026-02-09 15:16:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gpmcs",
          "author": "Such_Web9894",
          "text": "When can we create subspecialized localized models/agentsâ€¦.   \nExampleâ€¦.  \n\nQwen3_refractor_coder. \n  \nQwen3_planner_coder.  \n  \nQwen3_tester_coder.    \n  \nQwen3_coder_coder\n\nAll 20 GBs. \n  \nThen the local agent will unload and load the model as needed to get specialized help.  \n  \nWhy have the whole book open.   \nJust â€œopenâ€ the chapter.  \n  \n  \nWill it be fast.. no. \n  \nBut it will be possible.  \n  \nThen offload unused parameters and context to system ram with engram.",
          "score": 2,
          "created_utc": "2026-02-09 17:17:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i0086",
          "author": "Guilty_Rooster_6708",
          "text": "Canâ€™t wait for the Q0.01 XXXXXS quant to run on my 16gb VRAM 32gb RAM.",
          "score": 2,
          "created_utc": "2026-02-09 21:01:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4j6udv",
          "author": "silenceimpaired",
          "text": "Shame no one asked at the AMA if they would try to not forget the local scene. It's so weird how often a AMA on LocalLLaMA is followed by a model that can't be used by us.",
          "score": 2,
          "created_utc": "2026-02-10 00:48:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4jcvkp",
          "author": "LocoMod",
          "text": "We all going to be /r/remotellama soon enough",
          "score": 2,
          "created_utc": "2026-02-10 01:23:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kbf9x",
          "author": "Agreeable-Market-692",
          "text": "Hey, if you're reading this do not despair. If you have a specific kind of task type or a domain you are working that you want to run this model for, try the full model out somewhere online once it hits. Then after you do a couple of quick and dirty projects in it, take your prompts, and use that to generate a set of new prompts in the same domain or of the same task type. \n\nOnce you have your promptset then you load the model with REAP (code is on cerebras github) on a GPU provider if you don't the hardware yourself. Let REAP run through YOUR custom promptset instead of the default (but do compare your promptset to the default to get an idea of a baseline).\n\nThen REAP will prune whatever parameters are less likely to be important to your application for this model and you can begin your quantization. I personally really like all of u/noctrex 's quants and if you look around you can figure out most or all of how to do those.\n\nRemember though, your promptset is how REAP calibrates what to chop off so check that default promptset and make sure your custom one has as much coverage as possible for your use case. ",
          "score": 2,
          "created_utc": "2026-02-10 04:58:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kgpo2",
          "author": "jferments",
          "text": "All of these large models will usually be followed by smaller/distilled versions that can be run on local hardware. It's great to have both be freely available.",
          "score": 2,
          "created_utc": "2026-02-10 05:38:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4oo5jm",
          "author": "dwstevens",
          "text": "why is this bad news?",
          "score": 2,
          "created_utc": "2026-02-10 21:06:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fwpb2",
          "author": "henk717",
          "text": "The only change here is GLM right? Deepseek/kimi were already large.  \nAnd for GLM its not that big of a loss because they release smaller versions of their model for the local users.  \nSo I personally rather have the really top models try to compete with closed source models so that the open scene is competitive, thats a win for everyone but especially users who don't want to be tied down to API providers.  \nAnd then for the local home user they should keep releasing stuff we can fit which GLM has repeatedly done.  \nDeepseek and Kimi should also begin doing this, it would make that playing field more interesting.\n\nBut we also still have Qwen, Gemini and Mistral as possible players who tend to release at more local friendly sizes.",
          "score": 4,
          "created_utc": "2026-02-09 14:57:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fjv32",
          "author": "CovidCrazy",
          "text": "Fuck Iâ€™m gonna need another M3 Ultra",
          "score": 2,
          "created_utc": "2026-02-09 13:45:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4flyrv",
              "author": "power97992",
              "text": "No u need an m5 ultra",
              "score": 12,
              "created_utc": "2026-02-09 13:57:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4fpd75",
                  "author": "tmvr",
                  "text": "I'll be honest, I would be fine with an M4 Competition with xDrive.",
                  "score": 6,
                  "created_utc": "2026-02-09 14:17:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fsxmb",
              "author": "nomorebuttsplz",
              "text": "Why? This is perfect size for Q4.",
              "score": 4,
              "created_utc": "2026-02-09 14:37:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4fyda1",
                  "author": "CovidCrazy",
                  "text": "The quants are usually a little retarded. I donâ€™t go below 8bit",
                  "score": 5,
                  "created_utc": "2026-02-09 15:06:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fy9gd",
              "author": "calcium",
              "text": "Currently waiting for the new M5 MBP's to be released...",
              "score": 1,
              "created_utc": "2026-02-09 15:05:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4g32xe",
          "author": "LegacyRemaster",
          "text": "https://preview.redd.it/6excw0jdmhig1.png?width=1373&format=png&auto=webp&s=26ce5055e7eea740a2d6aa6e99f3922ce1935955\n\nTrying to do my best. Testing W7800 48gb. More gb/sec (memory) then 3090 or 5070ti. Doing benchmark.  1475â‚¬ +vat for 48gb is life saver.",
          "score": 2,
          "created_utc": "2026-02-09 15:30:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4g5irq",
              "author": "LegacyRemaster",
              "text": "https://preview.redd.it/pxszmhsiohig1.png?width=1933&format=png&auto=webp&s=042468f68d5203805b12f3e39d59db5cd959c7f4\n\nquick test on Lm studio + Vulkan. \"write a story 1000 tokens\". Minimax m2.1 IQ4 XS. Downloading Q4\\_K\\_XL now\n\n",
              "score": 1,
              "created_utc": "2026-02-09 15:42:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4frewo",
          "author": "hydropix",
          "text": "I wonder how they manage to optimize the use of their server? Yesterday, I used a Kimi 2.5 subscription non-stop for coding. At $39/month, I only used 15% of the weekly limit, even with very intensive use. To run such a large model, you need a server costing at least $90,000 (?). I wonder how much time I actually used on such a machine. Because it cost me less than $1.30 in the end. Does anyone have any ideas about this?\n\n\n\n",
          "score": 1,
          "created_utc": "2026-02-09 14:28:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4g66q2",
              "author": "Sevii",
              "text": "You aren't getting the full output of one server. \n\nhttps://blog.vllm.ai/2025/12/17/large-scale-serving.html",
              "score": 3,
              "created_utc": "2026-02-09 15:45:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gflxv",
                  "author": "hydropix",
                  "text": "very interesting, thanks.",
                  "score": 1,
                  "created_utc": "2026-02-09 16:30:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fvuh9",
          "author": "INtuitiveTJop",
          "text": "Weâ€™re just going to be funding Apple thatâ€™s all",
          "score": 1,
          "created_utc": "2026-02-09 14:53:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g654g",
          "author": "dobkeratops",
          "text": "need 2 x 512gb mac studios",
          "score": 1,
          "created_utc": "2026-02-09 15:45:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g7hvx",
          "author": "muyuu",
          "text": "waiting for Medusa Halo 512GB x2 clusters",
          "score": 1,
          "created_utc": "2026-02-09 15:51:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g84pf",
          "author": "Zyj",
          "text": "Even with 2x Strix Halo, thatâ€˜s mostly out of the question (except GLM 4.5 Q4). Ouch.",
          "score": 1,
          "created_utc": "2026-02-09 15:54:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gt8of",
          "author": "Charuru",
          "text": "This will be the last hurrah for DSA. If it doesn't work here we'll probably never see it again, go back to MLA.",
          "score": 1,
          "created_utc": "2026-02-09 17:35:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gtljo",
          "author": "Cool-Chemical-5629",
          "text": "And here I thought DeepSeek was big LOL",
          "score": 1,
          "created_utc": "2026-02-09 17:36:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hzjuk",
          "author": "gamblingapocalypse",
          "text": "Thats gonna be a lot of macbook pros.",
          "score": 1,
          "created_utc": "2026-02-09 20:59:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i1hpm",
          "author": "portmanteaudition",
          "text": "Pardon my ignorance but how does this translate into hardware requirements?",
          "score": 1,
          "created_utc": "2026-02-09 21:08:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jbw81",
              "author": "DragonfruitIll660",
              "text": "Larger overall parameters means you need more Ram/Vram to fit the whole model. So it went from 355B to 745B total parameters, meaning its going to take substantially more space to fully load the model (without offloading to disk). Hence higher hardware requirements (Q4KM GLM 4.7 is 216 GB with 355B parameters, Q4KM Deepseek V3 is 405GB with 685B parameters).",
              "score": 2,
              "created_utc": "2026-02-10 01:17:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ignae",
          "author": "No-Veterinarian8627",
          "text": "I wait and hope CXL will get some research breakthroughs... one man can hope",
          "score": 1,
          "created_utc": "2026-02-09 22:25:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4jrpan",
          "author": "BumblebeeParty6389",
          "text": "Are you a cloud bro?",
          "score": 1,
          "created_utc": "2026-02-10 02:49:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4k14fz",
              "author": "FireGuy324",
              "text": "Kind of",
              "score": 1,
              "created_utc": "2026-02-10 03:48:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jwmcv",
          "author": "Oldspice7169",
          "text": "Skill issue, just throw money at the problem, anon humans can live off ramen for centuries",
          "score": 1,
          "created_utc": "2026-02-10 03:19:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kxnff",
          "author": "HarjjotSinghh",
          "text": "bros are way too invested in their own drama.",
          "score": 1,
          "created_utc": "2026-02-10 08:07:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l8srl",
          "author": "Truth-Does-Not-Exist",
          "text": "qwen3-coder-next is wonderful",
          "score": 1,
          "created_utc": "2026-02-10 09:57:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o13w8",
          "author": "[deleted]",
          "text": "With the current rate of progress in LLM development I am not at all worried we will see compression (quantization) making massive leaps as well. Running capable LLMs on phones and Raspberry PIs is a goal for the open source community as well as those monetizing this technology. It's just a question of time at this point. ",
          "score": 1,
          "created_utc": "2026-02-10 19:19:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qdk1u",
          "author": "Crypto_Stoozy",
          "text": "Letâ€™s be honest here though the hardware limitations are not what you think they are this isnâ€™t postive itâ€™s negative for the creators. You canâ€™t sell this in mass they are already losing tons of money. The future is getting small model params to be more efficient not getting more parameters that require large hardware. Something that requires 200k to run it isnâ€™t scalable.",
          "score": 1,
          "created_utc": "2026-02-11 02:42:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4s189a",
          "author": "Good_Work_8574",
          "text": "step 3.5 flash is 200B model,activited 11B,you can try that.",
          "score": 1,
          "created_utc": "2026-02-11 10:46:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5at4ej",
          "author": "AmbericWizard",
          "text": "just put 4x ai Ryzen max on RDMA a lot cheaper than GeForce rigsÂ ",
          "score": 1,
          "created_utc": "2026-02-14 06:23:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fuy6c",
          "author": "psoericks",
          "text": "I'm hanging in there, next year I should still be able to run GLM_6.5_Flash_Q1_XS_REAP",
          "score": 1,
          "created_utc": "2026-02-09 14:48:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4h5net",
          "author": "Individual-Source618",
          "text": "Dont worry, the intel ZAM memory will become available in 2030, then he will not be limited by bandwidth or vram to run such models",
          "score": 1,
          "created_utc": "2026-02-09 18:33:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gfrq7",
          "author": "jonheartland",
          "text": "Well, of course they're focusing on creating the biggest possible models first. Gotta justify building a bazillion data centers somehow...",
          "score": 0,
          "created_utc": "2026-02-09 16:31:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gl5x1",
          "author": "AnomalyNexus",
          "text": "Itâ€™s been that way for a while tbh - itâ€™s just an unpopular take in this sub",
          "score": 0,
          "created_utc": "2026-02-09 16:56:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ffzik",
          "author": "TechySpecky",
          "text": "Can these still run on 2xH200?",
          "score": 0,
          "created_utc": "2026-02-09 13:22:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fh5j7",
              "author": "MaxKruse96",
              "text": "141GB x 2 = 282GB. A 745B model, at Q4, would be 745 \\* (4/8) = 373gb and thats just napkin math. You'd need to go down to IQ3S or something similar to even load it.",
              "score": 8,
              "created_utc": "2026-02-09 13:29:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4gpoq2",
              "author": "Single_Ring4886",
              "text": "Dont be cheap you need ful 8x server :)",
              "score": 1,
              "created_utc": "2026-02-09 17:18:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4fh2dp",
              "author": "power97992",
              "text": "maybe q1.5 or q2? ",
              "score": 1,
              "created_utc": "2026-02-09 13:28:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4frvms",
          "author": "Legitimate-Pumpkin",
          "text": "Is it possible to load only the active parameters? You know? Load the â€œdirectorâ€ then it ask to load a specific expert and you only load both of them, or even maybe unload the director and load only the expert. Slow but usable (if possible at all. I donâ€™t know what Iâ€™m talking about).",
          "score": 0,
          "created_utc": "2026-02-09 14:31:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fxold",
          "author": "johnnyApplePRNG",
          "text": "Plot twist: GLM 5 Flash Air scores 98% on all benchmarks after being whittled down to just 150B/3B.",
          "score": 0,
          "created_utc": "2026-02-09 15:02:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4jnem1",
          "author": "lisploli",
          "text": "What's bad about it? I don't need all the experts. One is enough. It's not like the model would ask more than one of them anyways. Or does it? Two? Doesn't seem like a big loss.",
          "score": 0,
          "created_utc": "2026-02-10 02:24:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fl30b",
          "author": "Lithium_Ii",
          "text": "I'm happy with GLM-4.7-Flash",
          "score": -2,
          "created_utc": "2026-02-09 13:52:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4izy4l",
              "author": "ttkciar",
              "text": "Can relate. I'm happy with GLM-4.5-Air.\n\nIf a distill from GLM-5 outperforms it, though, so much the better! Looking forward to seeing what they come up with.",
              "score": 1,
              "created_utc": "2026-02-10 00:08:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r2xotu",
      "title": "Minimax M2.5 Officially Out",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1r2xotu",
      "author": "Which_Slice1600",
      "created_utc": "2026-02-12 16:17:13",
      "score": 511,
      "num_comments": 130,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o50imhd",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-12 17:25:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o508r07",
          "author": "Impossible_Ground_15",
          "text": "waiting for the open weights on HF",
          "score": 83,
          "created_utc": "2026-02-12 16:38:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o523899",
              "author": "blahblahsnahdah",
              "text": "NovitaAI, an american infra company in SF, is already hosting it as one of the providers on openrouter (and it works, I tested)\n\nWhere did they get the weights from? idgi\n\nedit: Looking at the stats, their ttft latency and tps are exactly the same as Minimax's, so Novita might just be acting as a passthrough for the Minimax API for some reason. Weird thing to do if so, I don't see the point because their pricing is the exact same, meaning they're not skimming a profit. Maybe the plan is for it to be a passthrough at first to get some users and then silently transition to their own hosting after the weights drop?",
              "score": 12,
              "created_utc": "2026-02-12 21:53:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o54m9xk",
                  "author": "KaMaFour",
                  "text": ">I don't see the point because their pricing is the exact same, meaning they're not skimming a profit.\n\nOur customer data\n\nhttps://preview.redd.it/9969e57ru7jg1.png?width=860&format=png&auto=webp&s=e93d259b5271acf4ad11ced8e5eccc526991f2e4",
                  "score": 7,
                  "created_utc": "2026-02-13 07:42:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o52j2q1",
                  "author": "johnnyApplePRNG",
                  "text": "Never hurts to be first to offer.",
                  "score": 5,
                  "created_utc": "2026-02-12 23:14:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o54zfqg",
              "author": "Rent_South",
              "text": "While these stats are really cool. I find that static benchmarks are really underwhelming. When you actually try the models for your own use cases, lets say agentic flows steps, they rarely perform as these popular benchmarks would suggest.\n\nThe solution I found for this, was to create a tool to dynamically create any benchmarks for one's specific use case, judging user's custom prompts and tests.\n\nI've added the latest minimax models to OpenMark AI , if you want to check it out.",
              "score": 0,
              "created_utc": "2026-02-13 09:47:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50egvg",
          "author": "chmoooz",
          "text": "Forget benchmarks for a moment. This looks like a real game changer to me: \"Based on output price, the cost of M2.5 is one-tenth to one-twentieth that of Opus, Gemini 3 Pro, and GPT-5. At a rate of 100 output tokens per second, running M2.5 continuously for an hour costs $1. At a rate of 50 TPS, the price drops to $0.3. **To put that into perspective, you can have four M2.5 instances running continuously for an entire year for $10,000**.\"",
          "score": 90,
          "created_utc": "2026-02-12 17:05:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50rxno",
              "author": "Edzomatic",
              "text": "In plain words:\n\n\nInput token are 0.3/M\n\n\nCached input 0.03/M\n\n\nOutput 1.2/M\n\n\nVery good pricing especially factoring cached input tokens. But still a bit more expensive than Deepseek",
              "score": 39,
              "created_utc": "2026-02-12 18:08:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o51c1vv",
              "author": "cheechw",
              "text": "The cost is the most mindblowing thing to me. Delivering this level of performance at that price point is insane. I wonder what the big difference maker is.",
              "score": 10,
              "created_utc": "2026-02-12 19:43:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o51my36",
                  "author": "VampiroMedicado",
                  "text": "If itâ€™s as good as Sonnet 4.5 I wonder what will happen to Anthropic, their prices are insane.",
                  "score": 14,
                  "created_utc": "2026-02-12 20:35:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o525p84",
              "author": "huffalump1",
              "text": "That's amazing\n\nKind of like the new gpt-5.3-codex fast model... \n\nReally fast&cheap models become RIDICULOUSLY useful once they reach a minimum bar of capability, because suddenly you can do A LOT OF THINGS.\n\nSpin up tons of parallel agents, test or research things in parallel, iterate quickly, steer a fleet of these instances with a smarter model, etc... like, think about what it would be like if Claude Code gave similar results in seconds rather than minutes. That's where we'll be in a few months to a year (rough estimate obviously).",
              "score": 12,
              "created_utc": "2026-02-12 22:04:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o52neap",
              "author": "paperbenni",
              "text": "How is that a game changer? It's the same price as M2.1? The important question is if they can deliver on intelligence and all that stuff at that price point.",
              "score": 0,
              "created_utc": "2026-02-12 23:39:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o52rz68",
              "author": "Guinness",
              "text": "> you can have four M2.5 instances running continuously for an entire year for $10,000.\n\nFor now. There is _no way_ that is profitable in any way shape or form, not even break even. The energy costs alone would probably be what, half that amount? They're losing a ton of money when you figure in hardware, energy, support, etc.\n\nBut for now, yes please!",
              "score": 0,
              "created_utc": "2026-02-13 00:05:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o532zaq",
                  "author": "MrRandom04",
                  "text": "They are most likely profitable. They've delivered at the same prices for previous model generations.",
                  "score": 3,
                  "created_utc": "2026-02-13 01:09:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o505sg8",
          "author": "Terminator857",
          "text": "how does it compare to glm 5 and kimi 2.5?",
          "score": 49,
          "created_utc": "2026-02-12 16:25:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5080tm",
              "author": "kroggens",
              "text": "Yeah, comparison with other open models matters",
              "score": 28,
              "created_utc": "2026-02-12 16:35:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o51jspu",
              "author": "alexeiz",
              "text": "Both GLM 5 and Kimi K2.5 perform noticeably better even on small tasks.",
              "score": 19,
              "created_utc": "2026-02-12 20:20:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o53f0ow",
                  "author": "sucksesss",
                  "text": "sad. i wish this minimax 2.5 had at least same performance as kimi k2.5 :(",
                  "score": 5,
                  "created_utc": "2026-02-13 02:23:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o529gze",
              "author": "mdziekon",
              "text": "Take my limited testing with a huge grain of salt, but comparing its results on the same coding task sent to M2.5 and GLM5, I feel like M2.5 is way, way worse, worse even than 4.7... It didn't follow implementation details really well (small issues, GLM does that too, but Minimax was worse), it even made a non-functioning code (syntax wise) and claimed it was \"compiler caching issue\", which it was not.\n\nI really wished M2.5 would be good enough compared to GLM, especially since Minimax's coding plan is a bit cheaper right now. Well, I guess I'll have to bite the bullet and stick with GLM for the next billing period.",
              "score": 11,
              "created_utc": "2026-02-12 22:23:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o52tzpj",
                  "author": "HornyEagles",
                  "text": "Do you use GLM5 Thinking or GLM5 straight for copilot coding? I'm on nanogpt subscription and GLM-5 thinking takes forever to respond and times out.",
                  "score": 1,
                  "created_utc": "2026-02-13 00:17:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50cjsj",
              "author": "Neither-Phone-7264",
              "text": "benches higher, vibe tests tbd",
              "score": 19,
              "created_utc": "2026-02-12 16:56:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o507x0u",
              "author": "Which_Slice1600",
              "text": "i still felt kimi 4.5 worked better than pony alpha on claude code. Feeling SWE bench verified is still close similar to personal feelings. I am also looking forward to test this one!! (did not have a minimax coding plan tho)",
              "score": 2,
              "created_utc": "2026-02-12 16:34:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5081sl",
                  "author": "letsgeditmedia",
                  "text": "Kimi 2.5 , donâ€™t think there is a Kimi 4.5 ?",
                  "score": 13,
                  "created_utc": "2026-02-12 16:35:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o505k19",
          "author": "1ncehost",
          "text": "Jeeze these chinese labs be cooking.",
          "score": 140,
          "created_utc": "2026-02-12 16:24:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50cpv9",
              "author": "thawab",
              "text": "https://huggingface.co/inclusionAI/Ring-2.5-1T\n\nAnother model was released less than an hour ago. We got 3 sonnet 4.5 level models in less than 24 hours.Â ",
              "score": 102,
              "created_utc": "2026-02-12 16:57:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50iuql",
                  "author": "ihexx",
                  "text": "linear models hitting those numbers is fucking NUTS",
                  "score": 35,
                  "created_utc": "2026-02-12 17:26:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o50qelx",
                  "author": "witek_smitek",
                  "text": "Oh... Only 1T parameters... Are we still at \"Local\" llm Reddit ? ðŸ˜‚",
                  "score": 25,
                  "created_utc": "2026-02-12 18:01:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5131ta",
              "author": "SilentLennie",
              "text": "> Jeeze these chinese labs be cooking.\n\nThey want to release stuff before Chinese New Year is the guess.",
              "score": 8,
              "created_utc": "2026-02-12 19:00:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o508dmg",
          "author": "segmond",
          "text": "if this is true, that pretty much means GLM5.0 won't take off.  It's far easier to run MiniMax.  I personally have Kimi2.5, DeepSeekv3.2 as my to go local models tho they are swim, but if these benchmarks are to be believed.  Then there's a new King in town.",
          "score": 30,
          "created_utc": "2026-02-12 16:37:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50brsu",
              "author": "Which_Slice1600",
              "text": "I cant believe. But i've seen a sign: in their page they specified many benches and tests that focused on \"real world tasks\". So they're showing off their solid work on real world tasks. They are following anthropic i thinkÂ ",
              "score": 14,
              "created_utc": "2026-02-12 16:52:46",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o50eudj",
              "author": "TokenRingAI",
              "text": "I'm already using GLM-5 on my ~ $200 a year coding plan, and minimax is more expensive than that, so \"taking off\" is perhaps unrelated to the challenges we face running local models.\n\nWe also don't know if 2.5 is the same size as 2.1\n\nFWIW, Huawei GPUs generally incorporate slower and cheaper memory than what we see from Nvidia, so large sparse models are generally going to be the standard product coming out of China.",
              "score": 12,
              "created_utc": "2026-02-12 17:07:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50i6dk",
                  "author": "segmond",
                  "text": "I'm talking about locally. I too subscribed to GLM when they had a sales, so I'll be using GLM5 through the API for work that doesn't need privacy/secrecy.  But for local inference if the benchmark holds I will pick Minimax unless GLM demonstrates a stronger capability in some areas.",
                  "score": 7,
                  "created_utc": "2026-02-12 17:23:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o50no9v",
                  "author": "mdziekon",
                  "text": "Zhipu raised prices, the Pro yearly plan is no longer $180, it's $250. So Plus yearly coding plan for Minimax seems cheaper, although I have no idea how does the \"300 prompts / 5h\" compares to the GLM's Pro plan in terms of limits. ",
                  "score": 6,
                  "created_utc": "2026-02-12 17:49:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o513u2x",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 3,
                  "created_utc": "2026-02-12 19:04:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o50akhe",
          "author": "synn89",
          "text": "This is impressive if the model size stayed the same from M2.1. A lot of times you don't need that 10-15% difference in SOTA performance as much as you want a 2-3x cost reduction in doing a task or the speed increase. Especially for something like sub-agents or agent workflows that don't have a lot of orchestration or planning. It's a good LLM niche to fill.",
          "score": 10,
          "created_utc": "2026-02-12 16:47:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50hrod",
          "author": "sine120",
          "text": "It's been a good February.",
          "score": 10,
          "created_utc": "2026-02-12 17:21:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50l6m6",
          "author": "Nasser1020G",
          "text": "https://preview.redd.it/tlmst96cn3jg1.png?width=1842&format=png&auto=webp&s=2844a8013e3c786b72fff42235af1a714152a283\n\nraw intelligence on par with sonnet, but agency and coding on par with opus, this is so good.",
          "score": 22,
          "created_utc": "2026-02-12 17:37:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50os5q",
              "author": "Nasser1020G",
              "text": "M2.5 dominates bridgemind independent coding benchmark, GLM-5 on the other hand...\n\nhttps://preview.redd.it/75mfpuxqq3jg1.jpeg?width=680&format=pjpg&auto=webp&s=b6bb3cbc2e013a4037637d659ad7df6163d4357c\n\n",
              "score": 16,
              "created_utc": "2026-02-12 17:54:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50svei",
                  "author": "BasketFar667",
                  "text": "Gemini 3 deep think beat all coding models...",
                  "score": -7,
                  "created_utc": "2026-02-12 18:13:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50qt2x",
              "author": "Nasser1020G",
              "text": "https://preview.redd.it/2l9ukw4as3jg1.jpeg?width=1936&format=pjpg&auto=webp&s=e28403ad20902b74e5c866a27487aa79f6e7f8df\n\nranks 4th on OpenHandsDev benchmark",
              "score": 9,
              "created_utc": "2026-02-12 18:03:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o550jxn",
                  "author": "Current-Function-729",
                  "text": "I wonder why 5.3 codex isnâ€™t ranked.",
                  "score": 1,
                  "created_utc": "2026-02-13 09:57:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o50hsg1",
          "author": "Aaaaaaaaaeeeee",
          "text": "Edit: it's probably still a small one, the developer quote tweeted someone sharing the 230B param figure. https://xcancel.com/SkylerMiao7/status/2022010506777956861\n\nA cloud service shows 358B for Minimax 2.5: https://www.netmind.ai/modelsLibrary/minimax-m2.5\n\n\n\n\nIf they have to increase active parameters.. hopefully it'll be efficient for QAT like Kimi. They have a setup for it already. \n\n\nhttps://xcancel.com/SkylerMiao7/status/2004887155395756057\n\n\n>Congratzï¼Any chance you are looking into 4 bit QAT, or reasons you preferred not to?\n\n\n>Good question. As early as abab5.5 (late 2023), we achieved near-lossless int4 PTQ and int4 QAT. Our QAT generalizes well and can be migrated to new models with almost no extra effort, and it remained in use through MiniMax M1.",
          "score": 9,
          "created_utc": "2026-02-12 17:21:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50jtiy",
              "author": "SpicyWangz",
              "text": "Bummer. I was really hoping or a model that could be run at a reasonable quant on 128GB",
              "score": 10,
              "created_utc": "2026-02-12 17:30:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50p9v5",
              "author": "rmhubbert",
              "text": "The sidebar says 358B, but the description says 229B. Here's hoping the description is correct ...",
              "score": 6,
              "created_utc": "2026-02-12 17:56:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o510bki",
                  "author": "lolwutdo",
                  "text": "The minimax agent website has a Max and Air version of 2.5; so im assuming thereâ€™s two versions",
                  "score": 3,
                  "created_utc": "2026-02-12 18:47:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o50dgta",
          "author": "tricky-oooooo",
          "text": "Looking forwards to getting the weights. The MiniMax teams seems to know their stuff. I really liked M2.1, it has impressive stability during longer contexts. ",
          "score": 8,
          "created_utc": "2026-02-12 17:00:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o504ypu",
          "author": "jacek2023",
          "text": "No HF no fun",
          "score": 28,
          "created_utc": "2026-02-12 16:21:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o506bx4",
              "author": "LightGamerUS",
              "text": "[Soon, it looks like!](https://ibb.co/h1fgz5f7)",
              "score": 20,
              "created_utc": "2026-02-12 16:27:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50a502",
          "author": "Septerium",
          "text": "Minimax 2.1 is the best model I can run locally with decent quants... from these results, it seems like the model has grown a bit.. I hope Minimax does not turn into Biggiemax",
          "score": 11,
          "created_utc": "2026-02-12 16:45:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51ehqf",
              "author": "tarruda",
              "text": "Would be surprising if a minor release (still 2.x) would change the model architecture or size.\n\nBTW If you like minimax, I suggest giving Step 3.5 Flash a shot. IQ4_XS ubergarm quants are the best.",
              "score": 4,
              "created_utc": "2026-02-12 19:55:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o51qjf3",
                  "author": "Zyj",
                  "text": "Step 3.5 Flash does an insane amount of reasoning.",
                  "score": 3,
                  "created_utc": "2026-02-12 20:52:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o54phz2",
              "author": "good__one",
              "text": "Hey, I'm trying to get into running something like this locally. Can you point me to what I should be searching for? Specifically, interested in the hardware requirements. What do you have? Is it fast? ",
              "score": 1,
              "created_utc": "2026-02-13 08:12:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55wpnu",
                  "author": "Septerium",
                  "text": "I use 4 RTX 3090's on an old Threadripper machine with 128GB of RAM. I use unsloth's Q5\\_K\\_XL quants, 64k context and the \\`-fit on\\` option of llama.cpp. I get slow prompt processing, but about 20 tk/s (tg)  \nIt is pretty good for directed coding tasks with Roo Code. It is the only local model I tested with smooth transition from architect mode to code mode. It plans really well and execution is great... when it made mistakes, it was able to make corrections accurately.",
                  "score": 1,
                  "created_utc": "2026-02-13 13:52:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o50thjv",
          "author": "Turbulent_Pin7635",
          "text": "To this point the Chinese models are just bullying the other models.",
          "score": 11,
          "created_utc": "2026-02-12 18:15:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5090pp",
          "author": "spaceman_",
          "text": "As far as I'm concerned, it's not out until the weights are on Huggingface.",
          "score": 16,
          "created_utc": "2026-02-12 16:40:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o509e70",
          "author": "nullmove",
          "text": "It probably is, but it's not clear if 2.5 is of the same size as 2.1",
          "score": 3,
          "created_utc": "2026-02-12 16:41:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50dsg7",
              "author": "AdIllustrious436",
              "text": "A x.y+1 version bump suggests RL improvements rather than an architectural shift. It would be highly unusual and non-standard to change the underlying architecture without a new generation and a major version leap.",
              "score": 5,
              "created_utc": "2026-02-12 17:02:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50hkrl",
                  "author": "nullmove",
                  "text": "- I trust AI companies with some things, but reliable naming of products and maintaining semver isn't one of those\n- I still think it's same size but this one is not y+1 either rather a y+4 hence at least some doubt, but we will see soon",
                  "score": 9,
                  "created_utc": "2026-02-12 17:20:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50fgpc",
              "author": "rm-rf-rm",
              "text": "Not a single mention of model size in the announcement. This makes me inclined to believe there has been an increase - as if it was the same size, that would have been something they'd be marketing",
              "score": 6,
              "created_utc": "2026-02-12 17:10:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50hvph",
                  "author": "nullmove",
                  "text": "Yeah though in that case they most likely would also be raising price in the API, but it seems to be same",
                  "score": 3,
                  "created_utc": "2026-02-12 17:21:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o56dagg",
              "author": "sidpant",
              "text": "According to figures shared by OpenCode's dev Dax size has not increased so its good news:\n\nhttps://preview.redd.it/kfmcypjv3ajg1.png?width=2208&format=png&auto=webp&s=40938eb85828be5756437c83a0112d1ad13f2229\n\nSource:  \n[https://x.com/thdxr/status/2021983484856988098?s=20f](https://x.com/thdxr/status/2021983484856988098?s=20f)",
              "score": 1,
              "created_utc": "2026-02-13 15:17:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50esv1",
          "author": "stefan_evm",
          "text": "Can we please stop posting 'something is officially out' when it actually isn't?",
          "score": 9,
          "created_utc": "2026-02-12 17:07:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50nzn4",
              "author": "romprod",
              "text": "Yeah but it is.",
              "score": -6,
              "created_utc": "2026-02-12 17:50:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o51cwrx",
                  "author": "stefan_evm",
                  "text": "not really\n\nhttps://huggingface.co/MiniMaxAI/models",
                  "score": 11,
                  "created_utc": "2026-02-12 19:47:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o506ehz",
          "author": "LagOps91",
          "text": "pretty impressive charts. let's see how well it holds up once released.",
          "score": 2,
          "created_utc": "2026-02-12 16:27:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50e0vf",
          "author": "rm-rf-rm",
          "text": "Pretty sad that they dont compare themselves to other open weights/chinese labs. \n\nRegardless, obligatory dont rely on benchmarks comment - use the model, run your custom evals/tests.",
          "score": 2,
          "created_utc": "2026-02-12 17:03:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5062z2",
          "author": "policyweb",
          "text": "Letâ€™s gooooo!",
          "score": 1,
          "created_utc": "2026-02-12 16:26:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50tjud",
          "author": "KvAk_AKPlaysYT",
          "text": "Good God.",
          "score": 1,
          "created_utc": "2026-02-12 18:16:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50ulp6",
          "author": "Specter_Origin",
          "text": "for the number of param and how its efficient in thinking token compared to other open-weight models, I feel min-max is way ahead...",
          "score": 1,
          "created_utc": "2026-02-12 18:21:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o511kmq",
          "author": "bakawolf123",
          "text": "I just hope we are getting the weights and they prove those graphs",
          "score": 1,
          "created_utc": "2026-02-12 18:53:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51gg8y",
          "author": "alexeiz",
          "text": "I've tried it on simple coding tasks.  It behaves like a small model in a way that it makes mistakes and then rushes to correct them.  It's less efficient token-wise than GLM-5 because of mistakes.  So for my tasks GLM-5 ended up cheaper because it used less tokens.",
          "score": 1,
          "created_utc": "2026-02-12 20:04:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o53i9z1",
              "author": "OTG-7",
              "text": "so in general which one is better for coding backend. i will buy mothly coding plan one of them which both is 10 dollars per month",
              "score": 1,
              "created_utc": "2026-02-13 02:43:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o51ncrn",
          "author": "Substantial_Fruit979",
          "text": "I've already bought credits for glm-5. This models at least 2 times cheaper....",
          "score": 1,
          "created_utc": "2026-02-12 20:37:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52d4im",
          "author": "SilentLennie",
          "text": "Let me guess this 3 times smaller model will be almost as good as Deepseek V3.2 in coding.",
          "score": 1,
          "created_utc": "2026-02-12 22:42:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53u26h",
          "author": "Alarming_Bluebird648",
          "text": "That 80.2% on SWE-Bench Verified is wild if the 10b active parameter count is accurate. I'm mainly curious to see how the VRAM overhead for 230b total weights affects local inference speeds compared to DeepSeek-V3.",
          "score": 1,
          "created_utc": "2026-02-13 03:59:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54fg4s",
          "author": "sagiroth",
          "text": "Feel to me like Sonnet 4.5+ great experience overall and quite fast",
          "score": 1,
          "created_utc": "2026-02-13 06:42:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54gwg7",
          "author": "Xhatz",
          "text": "I'm getting quite a lot of tokens leaking into the responses using this in openclaw, anyone else? (bits of tokens like \"/final>\" etc, also Chinese characters occasionally replacing words, and broken new lines, and it feels more \"lazy\"/rushing)",
          "score": 1,
          "created_utc": "2026-02-13 06:54:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54nuh2",
              "author": "Xhatz",
              "text": "To me right now (in openclaw at least) the model is really bad, broke a simple js dashboard, stops mid-task (idk if it's an oc problem)... It feels like m2.1 and maybe a bit worse in coherence?",
              "score": 1,
              "created_utc": "2026-02-13 07:57:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5513og",
          "author": "justserg",
          "text": "10b active params at 230b total is exactly the kind of efficiency gains we need. the real question is how well it quants down, thats always where MoE models get tricky",
          "score": 1,
          "created_utc": "2026-02-13 10:02:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5609ym",
          "author": "humblesquirrelking",
          "text": "awesome stuff!\n\n",
          "score": 1,
          "created_utc": "2026-02-13 14:11:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o505foh",
          "author": "Which_Slice1600",
          "text": "They shared that sharp improvement trend. I'd be really curious if they figured out that AI \"recursive self improvement\" as well, following OpenAI's gpt 5.3 and Musk's post. It's getting real as llm and coding agent started to be able to do many research stuff. It'll be big if true.",
          "score": 0,
          "created_utc": "2026-02-12 16:23:26",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o505xmq",
          "author": "Accomplished_Ad9530",
          "text": "Links on that blog post: Access API, Coding Plan, Experience Agent\n\nNo HuggingFace or ModelScope link. No GitHub. If that's \"officially out\" then that's disappointing.",
          "score": -2,
          "created_utc": "2026-02-12 16:25:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o506hcy",
              "author": "ShengrenR",
              "text": "\"Officially available via API\" maybe lol - though folks had some hand-wringing over GLM5 right before it landed and it's there on HF (thankfully) - hopefully this one follows right behind.",
              "score": 6,
              "created_utc": "2026-02-12 16:28:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o509mto",
              "author": "KaMaFour",
              "text": "Give them any time, bruh. GLM 5.0 released the weights roughly an hour after the announcement. I don't see why here it would be different",
              "score": 5,
              "created_utc": "2026-02-12 16:42:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5510gf",
                  "author": "Current-Function-729",
                  "text": "Still not on HF. Hopefully they push it out before they take time off for lunar new year.",
                  "score": 2,
                  "created_utc": "2026-02-13 10:01:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5085n3",
              "author": "cheechw",
              "text": "Brother. Wait a second. They confirmed on twitter they will be releasing weights soon.",
              "score": 7,
              "created_utc": "2026-02-12 16:36:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o51cp3o",
                  "author": "DinoAmino",
                  "text": "Words matter. The right word should be used. Releasing weights soon != Officially out. It really is that simple.",
                  "score": 4,
                  "created_utc": "2026-02-12 19:46:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5584ht",
          "author": "popiazaza",
          "text": "Maybe clarify that benchmark is out and it's live on their website. Officially out in this sub suppose to mean open weight available.",
          "score": 0,
          "created_utc": "2026-02-13 11:06:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51krzr",
          "author": "Worldly-Cod-2303",
          "text": "Not passing the vibes so far, 2.1 version was better and not broken on mobile.\n\n\nPrompt: Landing Page Honoring Charlie Kirk, with neobrutalism styleÂ [unrelated complaining incoming]\n\n\nMinimax 2.1: kk4ievi1j1qh.space.minimax.io\n\nMinimax 2.5: y4lbkecoaz6b.space.minimax.io",
          "score": -7,
          "created_utc": "2026-02-12 20:25:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0w7st",
      "title": "Qwen-Image-2.0 is out - 7B unified gen+edit model with native 2K and actual text rendering",
      "subreddit": "LocalLLaMA",
      "url": "https://qwen.ai/blog?id=qwen-image-2.0",
      "author": "RIPT1D3_Z",
      "created_utc": "2026-02-10 09:25:15",
      "score": 507,
      "num_comments": 108,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r0w7st/qwenimage20_is_out_7b_unified_genedit_model_with/",
      "domain": "qwen.ai",
      "is_self": false,
      "comments": [
        {
          "id": "o4ldfno",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-10 10:40:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lculd",
          "author": "waescher",
          "text": "Nice Tease in one of their sample images\n\nhttps://preview.redd.it/oeobh78manig1.png?width=332&format=png&auto=webp&s=cebb6ad784b841ff45b9d5ad4c3d95887a661069\n\n",
          "score": 173,
          "created_utc": "2026-02-10 10:34:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lgivy",
              "author": "ahmetegesel",
              "text": "Wow thatâ€™s brilliant",
              "score": 43,
              "created_utc": "2026-02-10 11:07:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lh2xo",
              "author": "Far-Low-4705",
              "text": "Iâ€™m so hyped lol.\n\nReally hoping for an eventual qwen 3.5 80b vision varient (eventually)",
              "score": 14,
              "created_utc": "2026-02-10 11:12:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4poo3k",
                  "author": "10minOfNamingMyAcc",
                  "text": "Really hoping there'll be a <70B variant that I can run locally.",
                  "score": 3,
                  "created_utc": "2026-02-11 00:14:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l7z0r",
          "author": "RIPT1D3_Z",
          "text": "BTW I dunno why, but Qwen team decided to introduce this as one of the showcase images\n\nhttps://preview.redd.it/2je8msoj2nig1.png?width=1765&format=png&auto=webp&s=c1119dd539d62df89b74b5507b91eae93bee6bad\n\n",
          "score": 222,
          "created_utc": "2026-02-10 09:49:04",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4ldat2",
              "author": "ghulamalchik",
              "text": "Maybe because AI has tons of photos of humans riding horses, but 0 horses riding humans. By being able to generate this it demonstrates higher and more complex understanding between things as well as abstracted concepts, like above and below.",
              "score": 111,
              "created_utc": "2026-02-10 10:39:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ldzft",
                  "author": "RIPT1D3_Z",
                  "text": "Exactly, but it's still hilarious out of context.",
                  "score": 54,
                  "created_utc": "2026-02-10 10:45:16",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o4ln8q9",
                  "author": "No_Swimming6548",
                  "text": "I believe there are some content including horses riding humans. Don't ask me how I know.",
                  "score": 17,
                  "created_utc": "2026-02-10 12:02:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mjuy5",
                  "author": "vaosenny",
                  "text": ">Maybe because AI has tons of photos of humans riding horses, but 0 horses riding humans. By being able to generate this it demonstrates higher and more complex understanding between things as well as abstracted concepts, like above and below.\n\nDoes it look like riding though?\n\nhttps://preview.redd.it/d9k957lmhoig1.jpeg?width=1179&format=pjpg&auto=webp&s=3033a564c3c9ec19bb0552c5cbdc308c02a3a274",
                  "score": 13,
                  "created_utc": "2026-02-10 15:12:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ulwcu",
                  "author": "KallistiTMP",
                  "text": "Also year of the horse, and everyone releasing models before the Chinese new year shutdown.",
                  "score": 1,
                  "created_utc": "2026-02-11 19:11:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4l9f8m",
              "author": "djm07231",
              "text": "Horse riding an astronaut was the infamous example cited by noted AI skeptic Gary Marcus 4 years ago to downplay the idea of AI ever managing to â€œunderstandâ€ things properly.\n\nhttps://garymarcus.substack.com/p/horse-rides-astronaut",
              "score": 96,
              "created_utc": "2026-02-10 10:02:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4laqr8",
                  "author": "-dysangel-",
                  "text": "AI skeptic, or just really trying to push the SOTA in bestiality porn?",
                  "score": 75,
                  "created_utc": "2026-02-10 10:15:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ns19p",
                  "author": "TheGoddessInari",
                  "text": "I just had to see.\n\nhttps://preview.redd.it/u3jbookxopig1.png?width=2816&format=png&auto=webp&s=4b4a9a23e1cb90abb4e3e9d6453d17190341ef01",
                  "score": 15,
                  "created_utc": "2026-02-10 18:37:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mcd16",
                  "author": "vaosenny",
                  "text": ">Horse riding an astronaut\n\nThat doesnâ€™t look like horse riding an astronaut though\n\nIf doesnâ€™t even have astronaut in it\n\nhttps://preview.redd.it/xgm1lr6ghoig1.jpeg?width=1179&format=pjpg&auto=webp&s=2d74dd3b27f54f86cb0d9460297b689089d31626",
                  "score": 4,
                  "created_utc": "2026-02-10 14:34:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mm1q0",
              "author": "postitnote",
              "text": "Mr. Hands...",
              "score": 3,
              "created_utc": "2026-02-10 15:23:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ld7d8",
              "author": "muyuu",
              "text": "they did Tom Hardy dirty",
              "score": 4,
              "created_utc": "2026-02-10 10:38:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lg89c",
              "author": "Healthy-Nebula-3603",
              "text": "I don't see any problem here ðŸ˜‰ \n\nA horse riding a man...",
              "score": 5,
              "created_utc": "2026-02-10 11:05:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ode13",
              "author": "thetaFAANG",
              "text": "Its a Chinese meme thats taken a life of its own",
              "score": 2,
              "created_utc": "2026-02-10 20:16:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4mtirt",
              "author": "infearia",
              "text": "I'm probably waaay over-analyzing, but 2026 in the Chinese calendar will be the Year of the Horse, and the guy on his knees, exposing his backside to the horse, with his ragged clothing and a distressed facial expression, has a distinctly Western look...",
              "score": 3,
              "created_utc": "2026-02-10 15:59:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lpob0",
              "author": "Tzeig",
              "text": "He who smelt it...",
              "score": 1,
              "created_utc": "2026-02-10 12:20:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lw1y7",
              "author": "RayHell666",
              "text": "It's a classic benchmark to test model prompt adherence. They almost all fail.",
              "score": 1,
              "created_utc": "2026-02-10 13:02:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4p7988",
              "author": "Ai--Ya",
              "text": "From John Oliver's account",
              "score": 1,
              "created_utc": "2026-02-10 22:37:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4l6nqj",
          "author": "r4in311",
          "text": "I so hope this gets a release, they finally nailed natural light and weird ai faces. Huge game changer .",
          "score": 28,
          "created_utc": "2026-02-10 09:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l6u8q",
          "author": "Dany0",
          "text": "The \"classical\" chinese painting style generations kind of slap tbph",
          "score": 16,
          "created_utc": "2026-02-10 09:38:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l85km",
          "author": "Hialgo",
          "text": "I wonder if the multi language hurts the model.Â  Nearly all examples are Chinese",
          "score": 16,
          "created_utc": "2026-02-10 09:50:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l8j3x",
              "author": "RIPT1D3_Z",
              "text": "It would use Qwen3-VL 8b as an encoder, so it's entirely depends on its understanding, it seems. Most likely, Chinese and English are gonna be supported the most.",
              "score": 24,
              "created_utc": "2026-02-10 09:54:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4nswmx",
              "author": "Caffdy",
              "text": "I mean, they are a chinese company, with 1.4 billion possible user base",
              "score": 9,
              "created_utc": "2026-02-10 18:41:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4mgomp",
              "author": "wanderer_4004",
              "text": "Well, maybe it is time to learn Chinese...",
              "score": 9,
              "created_utc": "2026-02-10 14:56:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4naf4k",
                  "author": "Complainer_Official",
                  "text": "I'd start with Mandarin, Then move on to Cantonese. throw some korean and thai in there and you should be slightly functional.",
                  "score": 6,
                  "created_utc": "2026-02-10 17:17:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4nbuik",
              "author": "NickCanCode",
              "text": "Their past models are already supporting Chinese. It just get more fonts and understanding on top of that.",
              "score": 1,
              "created_utc": "2026-02-10 17:23:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4leded",
          "author": "muyuu",
          "text": "> As shown, Qwen-Image-2.0 accurately renders nearly the entire Preface in small regular script, with only a handful of characters imperfect.\n\nthis is a lingering problem with image generators, that they seem to be unable to correct themselves\n\ntypically you would try everything including just cutting an area of the image and asking for fixes and they will make the same mistakes, even if they can recognise them, and the SOTA situation is have someone just fixing their output by hand\n\nmaybe there's stuff out there improving on this situation that i'm unaware of",
          "score": 4,
          "created_utc": "2026-02-10 10:48:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l69pk",
          "author": "NikolaTesla13",
          "text": "Where does it say it's 7b?",
          "score": 7,
          "created_utc": "2026-02-10 09:32:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l6lvd",
              "author": "RIPT1D3_Z",
              "text": "https://preview.redd.it/u8f0r7c40nig1.png?width=2560&format=png&auto=webp&s=e83774638ccb95f054ff440ce35bbd811ac8fc89\n\nRight here. They've shared the prompt and the image that states that it's 7B",
              "score": 62,
              "created_utc": "2026-02-10 09:35:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4lbaey",
                  "author": "Formal-Exam-8767",
                  "text": "They have an office on Great Wall of China?",
                  "score": 32,
                  "created_utc": "2026-02-10 10:20:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4la7e5",
                  "author": "Mr_Frosty009",
                  "text": "Thatâ€™s very nice that they put spoiler for Qwen 3.5 existence",
                  "score": 6,
                  "created_utc": "2026-02-10 10:10:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4lcg2e",
                  "author": "ReadyAndSalted",
                  "text": "That says 8b text encoder + 7b diffusion... I understand that you can switch them between vram and memory to keep vram usage down, but that does still mean model inference involves 15b parameters total, not just 7b.",
                  "score": 6,
                  "created_utc": "2026-02-10 10:31:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4m1q6s",
                  "author": "lordlestar",
                  "text": "only a machine would hand write that perfect",
                  "score": 1,
                  "created_utc": "2026-02-10 13:36:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4l6hoz",
              "author": "Dany0",
              "text": "In one of the image prompts, ctrl+f is your friend",
              "score": 2,
              "created_utc": "2026-02-10 09:34:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lz9ke",
          "author": "Monkey_1505",
          "text": "The first round of qwen edit models had something I've never seen any other image model have - spatial reasoning. They can legit rotate the viewpoint in ways other models can't, not even the big bois. \n\nThis new model looks kind of amazing. Not ness 'better' than z-image turbo, but similar and more flexible. I'll be so disappointed if it's not open sourced.",
          "score": 3,
          "created_utc": "2026-02-10 13:22:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l7y0z",
          "author": "rerri",
          "text": "Are they stating anything anywhere wrt open weight release being planned or not planned?",
          "score": 10,
          "created_utc": "2026-02-10 09:48:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l9ctb",
              "author": "RIPT1D3_Z",
              "text": "Haven't seen any direct statement, but they've updated the readme in Qwen Image github announcing the model release. Also, Qwen is known as the lab that releases weights for their models, so the chances are high.\n\nIMO, no reason to state the size of the model if you're not planning to OS it anyway.",
              "score": 24,
              "created_utc": "2026-02-10 10:02:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4lclqr",
                  "author": "saltyrookieplayer",
                  "text": "I wouldnâ€™t be so optimistic given the existence of Wan 2.6",
                  "score": 9,
                  "created_utc": "2026-02-10 10:32:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ldq9n",
          "author": "Busy-Group-3597",
          "text": "I love qwen image edit But it was too big for my cpu only generationâ€¦ I really appreciate this 7B model .Will test out how this performs",
          "score": 2,
          "created_utc": "2026-02-10 10:42:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4op7pf",
          "author": "Blizado",
          "text": "Only if the model is capable of doing LoRAs, then it will be interesting.",
          "score": 2,
          "created_utc": "2026-02-10 21:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ludhs",
          "author": "mikkoph",
          "text": "Chinese New Year next week. Fingers crossed they decide to drop it for the event",
          "score": 1,
          "created_utc": "2026-02-10 12:52:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m4mh0",
          "author": "XiRw",
          "text": "Nice. I hope Image-Edit comes soon after",
          "score": 2,
          "created_utc": "2026-02-10 13:52:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m7k11",
              "author": "RIPT1D3_Z",
              "text": "It's both text2image and img2img in one model.",
              "score": 11,
              "created_utc": "2026-02-10 14:08:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4m83ev",
                  "author": "XiRw",
                  "text": "Oh nice! Thanks for letting me know!",
                  "score": 2,
                  "created_utc": "2026-02-10 14:11:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4ncpmi",
              "author": "nmkd",
              "text": "The title literally says it does both in one.",
              "score": 3,
              "created_utc": "2026-02-10 17:27:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n363h",
          "author": "dergachoff",
          "text": "https://preview.redd.it/mw7rme2i4pig1.png?width=2688&format=png&auto=webp&s=917a60ba9ce59fc9ff4e1c534095fab649212db1\n\nit's a pity 7B is not enough for russian rendering. how are other languages?",
          "score": 2,
          "created_utc": "2026-02-10 16:43:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lshwx",
          "author": "AppealThink1733",
          "text": "Will it run on a laptop with 16GB of RAM? And when will the GGUFS be available?",
          "score": 1,
          "created_utc": "2026-02-10 12:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lsv3r",
              "author": "RIPT1D3_Z",
              "text": "There are only rumors, but some people say weights are gonna be released after the Lunar New Year. There are still a chance that the model would not be open sourced, but still, Qwen usually releases their models on GitHub and HF.",
              "score": 3,
              "created_utc": "2026-02-10 12:42:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ltnqk",
                  "author": "AppealThink1733",
                  "text": "Thank you very much for the information.",
                  "score": 1,
                  "created_utc": "2026-02-10 12:47:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mqptp",
          "author": "dampflokfreund",
          "text": "Sounds amazing. With this and upcoming Qwen 3.5, they are knocking it out of the park.Â ",
          "score": 1,
          "created_utc": "2026-02-10 15:46:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qckxn",
          "author": "Unable-Finish-514",
          "text": "Wow!  I just tried the new image model on Qwen Chat.  I have a fictional character based on a cartoon image I came across about a year ago of a younger guy wearing a noticeable hat.  I've always liked GTA-esque organized crime games, so he would be a character in this type of world.  This is an impressive representation of my character by the new Qwen image model.\n\nhttps://preview.redd.it/p4jc1k5y1sig1.jpeg?width=450&format=pjpg&auto=webp&s=6dbe0fcd017cf4214ff0a15da7c897c64e42a85f\n\n",
          "score": 1,
          "created_utc": "2026-02-11 02:36:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qdf2g",
              "author": "Unable-Finish-514",
              "text": "Then, I hit the make video button and had him give a flirty compliment.  This is one of my favorite video prompts to test a video model, as you can see if the model can capture the vibe of a character and if it follows you directions about speech.  My apologies, as I don't know how to link the video, but it is 5 seconds and it's the exact vibe I want from the character.  This is right on par with Grok Imagine in image to video.",
              "score": 1,
              "created_utc": "2026-02-11 02:41:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4tlq7l",
          "author": "MedicalAd8373",
          "text": "https://preview.redd.it/u1j18vbi5wig1.png?width=1694&format=png&auto=webp&s=c6a8a7aac7bc313f921006760ee46a51c46bdbfd\n\nYeah.",
          "score": 1,
          "created_utc": "2026-02-11 16:22:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uv4qp",
              "author": "InterestingSloth5977",
              "text": "I saw her sister lying on the grass last year.",
              "score": 1,
              "created_utc": "2026-02-11 19:54:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4xwsmc",
          "author": "ThisIsCodeXpert",
          "text": "Is API access available? I heard that it is invite only?",
          "score": 1,
          "created_utc": "2026-02-12 06:52:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52zqjb",
          "author": "Beneficial_Buy4864",
          "text": "Umm would it run on a macbook air m3? 32GB",
          "score": 1,
          "created_utc": "2026-02-13 00:50:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52zw6j",
          "author": "cowcomic",
          "text": "Is this model definitely 7B? Where can I find relevant information?",
          "score": 1,
          "created_utc": "2026-02-13 00:51:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56cdm3",
          "author": "R_Duncan",
          "text": "Being a 7B model it would rock if they release the weights, otherwise it's just an hidden chinese model.",
          "score": 1,
          "created_utc": "2026-02-13 15:13:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hl7vx",
          "author": "sgcego",
          "text": "https://preview.redd.it/1axt14pi0njg1.png?width=2816&format=png&auto=webp&s=6ec940df699bbea044b1855b5bc44a6c06c9a58c\n\nðŸ˜…",
          "score": 1,
          "created_utc": "2026-02-15 10:41:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j51tx",
          "author": "ninjasaid13",
          "text": ">7B model, down from 20B in v1, which is great news for local runners\n\nIf you're counting the text encoder, it would be 13B.",
          "score": 1,
          "created_utc": "2026-02-15 16:38:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5m0hda",
              "author": "RIPT1D3_Z",
              "text": "If you're counting the encoder, the 1st version would be 27b. C'mon.",
              "score": 1,
              "created_utc": "2026-02-16 01:48:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lnelt",
          "author": "CattailRed",
          "text": "So... can you run a 7B image gen on CPU?",
          "score": 1,
          "created_utc": "2026-02-10 12:03:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lwasu",
              "author": "Serprotease",
              "text": "Yes, but you donâ€™t want to do it.Â \n\nI remember running sd1.5, so a 1b model, on cpu only a couple of years ago and it was a generation time in a dozen of minutes for a 512x512 image.Â ",
              "score": 8,
              "created_utc": "2026-02-10 13:04:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4mseqg",
              "author": "ayu-ya",
              "text": "Technically you can, but as the other person said, it would be a miserable experience. Not that long ago Stability Matrix had some issue with SD.Next, refused to work with my GPU and I only noticed it after I started generating. Let it run out of curiosity, it was only a SDXL model with some light detailers and ended up taking around 10 minutes for a single image. It would be horrible to try to figure out what prompts work for what I want when every image takes that long",
              "score": 2,
              "created_utc": "2026-02-10 15:54:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4nc7oc",
              "author": "nmkd",
              "text": "Not when you also need an 8B text encoder alongside it",
              "score": 2,
              "created_utc": "2026-02-10 17:25:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4nl1kh",
              "author": "AbhiStack",
              "text": "If privacy is not a concern, then cloud platforms like vast ai and runpod let's you run GPU instances at a very cheap hourly rate. You can run all sorts of big and small models and then destroy the instance when you're done.",
              "score": 1,
              "created_utc": "2026-02-10 18:06:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lu6ek",
          "author": "eribob",
          "text": "Nice! Cant wait for the release of the weights",
          "score": 1,
          "created_utc": "2026-02-10 12:50:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lzznd",
          "author": "BobbingtonJJohnson",
          "text": "Look at their benchmark results. No way in hell they will release this. This is the same as it will always be.",
          "score": 1,
          "created_utc": "2026-02-10 13:26:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lr3gh",
          "author": "techlatest_net",
          "text": "Hell yeah, Qwen-Image-2.0 dropping at 7B is massiveâ€”finally a lean beast that crushes gen+edit without choking my rig. V1 was solid in ComfyUI but hogged VRAM; this unified pipeline with native 2K and legit text (posters? Comics? Sign me up) feels like the local workflow upgrade we've been begging for. Fingers crossed weights hit HF soon like last time, gonna spam the demo til then!",
          "score": 0,
          "created_utc": "2026-02-10 12:30:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4rekh8",
          "author": "prateek63",
          "text": "The 7B down from 20B is the real headline here. A unified gen+edit model that actually fits on consumer hardware changes the calculus for local image workflows completely.\n\n\n\nThe text rendering capability is what I'm most curious about. If it can reliably render text in generated images, that eliminates one of the most annoying limitations of local image gen â€” every time you need text on an image, you're dropping into PIL/ImageMagick after generation.\n\n\n\nGiven Qwen's track record of open-weighting after initial API-only launches, I'd give it 4-6 weeks before we see Apache 2.0 weights on HuggingFace.",
          "score": 0,
          "created_utc": "2026-02-11 07:15:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lbvu2",
          "author": "dobomex761604",
          "text": "Editing functionality in 7B would be interesting, but Qwen models were never good for txt2img. Even ignoring censorship, they are plastic and overly cinematic. Plus, ZImage and Anima have taken the txt2img space already, making this new model less interesting.",
          "score": -7,
          "created_utc": "2026-02-10 10:26:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ldn95",
              "author": "ghulamalchik",
              "text": "The more the better. Plus every new model has better technology and training techniques even if it's incremental. If people had that mindset, we'd be stuck with Stable Diffusion 1.0 by now.",
              "score": 13,
              "created_utc": "2026-02-10 10:42:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lp5cp",
              "author": "oswaldo_fv",
              "text": "What do you mean, no? qwen-image-2512 is surprisingly good, and this new model looks even better. The best part is that it comes with 2K resolution and a unified generation model plus editing capabilities. I didn't like qwen-imagen-edit 2511 because it really lost image quality and definition when editing. Let's hope this new model doesn't.",
              "score": 5,
              "created_utc": "2026-02-10 12:16:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4m59dy",
                  "author": "dobomex761604",
                  "text": "Z-Image can do pretty much anything Qwen 2512 can, but gives less generic results more often. At it's size, 2512 is not a good choice.\n\nThe new 7B definitely looks better, but not by a lot compared to Z-Image. Like I said, editing is much more interesting here, especially since it's unified and at (relatively) small size.",
                  "score": 0,
                  "created_utc": "2026-02-10 13:55:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mwhgx",
          "author": "HatEducational9965",
          "text": "scrolled the post twice looking for a HF url. THE WEIGHTS PLEASE",
          "score": -3,
          "created_utc": "2026-02-10 16:13:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mzn19",
              "author": "RIPT1D3_Z",
              "text": "Post has to be read, not scrolled. No weights yet, unfortunately. Some people hinting it would be released after CNY.",
              "score": 7,
              "created_utc": "2026-02-10 16:27:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lug67",
          "author": "LodosDDD",
          "text": "No way they can create those images with 7B??? Models I run are trash",
          "score": -6,
          "created_utc": "2026-02-10 12:52:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m713m",
              "author": "COMPLOGICGADH",
              "text": "So you haven't tried new models like Klein 4b and 9b and obviously the elephant in the room zimage base and turbo which is only 6b",
              "score": 11,
              "created_utc": "2026-02-10 14:05:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mjp4c",
                  "author": "LodosDDD",
                  "text": "they can do edits?",
                  "score": 1,
                  "created_utc": "2026-02-10 15:12:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r4sivv",
      "title": "KaniTTS2 â€” open-source 400M TTS model with voice cloning, runs in 3GB VRAM. Pretrain code included.",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/swybh9pdaijg1",
      "author": "ylankgz",
      "created_utc": "2026-02-14 18:48:10",
      "score": 492,
      "num_comments": 88,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r4sivv/kanitts2_opensource_400m_tts_model_with_voice/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5fdhrr",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-15 00:05:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e29jj",
          "author": "misterflyer",
          "text": "Nice work.\n\nBut is it just me, or does the Elevenlabs voice sound more clear and more expressive?",
          "score": 112,
          "created_utc": "2026-02-14 19:38:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eetd5",
              "author": "HugoCortell",
              "text": "It does. Also using two different voices for comparison seems like a bad faith way to compare things.",
              "score": 53,
              "created_utc": "2026-02-14 20:45:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5emup6",
              "author": "ylankgz",
              "text": "It does. Thatâ€™s why the first guy is cute",
              "score": 26,
              "created_utc": "2026-02-14 21:29:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5f8zfm",
                  "author": "Ronaldo433",
                  "text": "TwinkLabs",
                  "score": 19,
                  "created_utc": "2026-02-14 23:36:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ewfip",
              "author": "L43",
              "text": "Canâ€™t wait for twelvelabs to come out",
              "score": 5,
              "created_utc": "2026-02-14 22:22:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e88g5",
          "author": "Hurricane31337",
          "text": "Awesome! Especially that you released the training scripts and datasets, too! ðŸ¤©\nCan you add German next, please? ðŸ™",
          "score": 19,
          "created_utc": "2026-02-14 20:09:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5em6ez",
              "author": "ylankgz",
              "text": "We have Hessian accent, so probably next week",
              "score": 11,
              "created_utc": "2026-02-14 21:26:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5erms7",
                  "author": "Hurricane31337",
                  "text": "That would be amazing!",
                  "score": 1,
                  "created_utc": "2026-02-14 21:55:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5dt373",
          "author": "segmond",
          "text": "Thanks, will be checking it out soon, thanks for sharing the recipe, that's the best!",
          "score": 18,
          "created_utc": "2026-02-14 18:50:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dtg3v",
              "author": "ylankgz",
              "text": "Yeah we open-source, not open-weights ðŸ˜Ž",
              "score": 42,
              "created_utc": "2026-02-14 18:52:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5e1gyk",
                  "author": "Amazing_Athlete_2265",
                  "text": "Champion",
                  "score": 11,
                  "created_utc": "2026-02-14 19:33:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5eartt",
                  "author": "Narrow-Belt-5030",
                  "text": "Dumb Q: whats the difference sorry?",
                  "score": 3,
                  "created_utc": "2026-02-14 20:23:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5e0ze0",
          "author": "hedonihilistic",
          "text": "Does it support streaming responses?",
          "score": 8,
          "created_utc": "2026-02-14 19:31:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5e2dzn",
              "author": "ylankgz",
              "text": "Yes. Huggingface spaces have limitations for it. We are working on vLLM like version. Batching and streaming. And open-source",
              "score": 15,
              "created_utc": "2026-02-14 19:38:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5e4c0m",
                  "author": "sexualrhinoceros",
                  "text": "Very confused, your library code does not support response streaming yet. Are you planning on adding that on soon??",
                  "score": 3,
                  "created_utc": "2026-02-14 19:49:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5f8oam",
                  "author": "Ra77oR",
                  "text": "vLLM added streaming audio batches to served models in 0.16.0. Would it be possible to serve the model with vLLM and use that?",
                  "score": 1,
                  "created_utc": "2026-02-14 23:35:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5e9pm9",
          "author": "Famous_Fix9751",
          "text": "Hey, great work. any chance that you'll add Romanian?",
          "score": 7,
          "created_utc": "2026-02-14 20:17:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5en71m",
              "author": "ylankgz",
              "text": "Probably not, BUT we have released pretrain code for everything, so one can train the model from scratch on Romanian language. Would love to see it with all local accents",
              "score": 4,
              "created_utc": "2026-02-14 21:31:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e0q1b",
          "author": "bigh-aus",
          "text": "Very nice! will check it out.  \n\nI also suggest you consider adding a openai compatible api in docker container that uses your model.  With the crazy of openclaw people people are definitely looking for \"i just deploy\" and use endpoints for their bots.",
          "score": 6,
          "created_utc": "2026-02-14 19:29:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5e1zfe",
              "author": "ylankgz",
              "text": "Thatâ€™s what we are working on rn. Will be open-source too",
              "score": 11,
              "created_utc": "2026-02-14 19:36:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5e28kv",
                  "author": "bigh-aus",
                  "text": "Love it. Also having a simple web ui that can convert some text from a website pasted in, and have it talk is also huge for us local guys running Linux.   ",
                  "score": 3,
                  "created_utc": "2026-02-14 19:37:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5eqwmb",
          "author": "SignalStackDev",
          "text": "The 3GB VRAM requirement is the real headline here for me. I have been running TTS through cloud APIs for agent voice output, and the latency is noticeable -- usually 1-2 seconds before audio starts. Having something this small that can run locally with voice cloning would be a game changer for real-time use cases.\n\nCurious about the voice cloning quality with short reference clips. In my experience, most open TTS models need 10+ seconds of clean reference audio to produce anything decent. The few-shot cloning models I have tried either sound robotic or lose the speaker identity when the text gets longer.\n\nAlso wondering about streaming support. For agent-type applications where you want the model to start speaking while still generating text, being able to stream chunks through the TTS pipeline is pretty critical. Does anyone know if this supports chunked input?",
          "score": 6,
          "created_utc": "2026-02-14 21:51:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5euhm5",
              "author": "ylankgz",
              "text": "Voice cloning needs to be >10sec. Ideally a bunch of audios with different emotions (for production)\nWe are working on streaming and batching rn. Stay tuned!\nVoice agent platforms are our priority, the first version of KaniTTS released 4 months ago and is being used in production already",
              "score": 5,
              "created_utc": "2026-02-14 22:11:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5eormg",
          "author": "Segaiai",
          "text": "The \"Italian-American\" guy slips into a British accent sometimes, and into a random assortment of pronunciations aside from that. And the voice comes out different every time. Was that meant to be the same voice throughout?",
          "score": 5,
          "created_utc": "2026-02-14 21:40:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eq3iq",
              "author": "ylankgz",
              "text": "We cloned voice of a real guy from Boston.",
              "score": 4,
              "created_utc": "2026-02-14 21:47:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5exlny",
                  "author": "Segaiai",
                  "text": "Oh no",
                  "score": 5,
                  "created_utc": "2026-02-14 22:29:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ennr0",
          "author": "deadsunrise",
          "text": "The voicecloning with pt in spanish (from spain) is pretty bad",
          "score": 3,
          "created_utc": "2026-02-14 21:34:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eogcm",
              "author": "ylankgz",
              "text": "Spanish is bad agree. Weâ€™ll continue working on it. First one to come is Mexico city accent",
              "score": 1,
              "created_utc": "2026-02-14 21:38:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hsabu",
          "author": "slashangel2",
          "text": "As italian I can say that the italian words in Kani are really bad.Â ",
          "score": 3,
          "created_utc": "2026-02-15 11:47:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5epk65",
          "author": "rm-rf-rm",
          "text": "Try generating the Navy Seal copy paste on the Hf space. The little widget spins and then theres nothing after it \"completes\". No error either",
          "score": 2,
          "created_utc": "2026-02-14 21:44:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eqoxp",
              "author": "ylankgz",
              "text": "You need to push â€œextract embeddingâ€ first and then press Generate. Should work. Also probably you need >10 sec audio. If not can you drop the audio here, Iâ€™ll try it",
              "score": 1,
              "created_utc": "2026-02-14 21:50:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ew92w",
                  "author": "rm-rf-rm",
                  "text": "Im not giving audio input, just text input:\n\n> What the fuck did you just fucking say about me, you little bitch? Iâ€™ll have you know I graduated top of my class in the Navy Seals, and Iâ€™ve been involved in numerous secret raids on Al-Quaeda, and I have over 300 confirmed kills. I am trained in gorilla warfare and Iâ€™m the top sniper in the entire US armed forces. You are nothing to me but just another target. I will wipe you the fuck out with precision the likes of which has never been seen before on this Earth, mark my fucking words. You think you can get away with saying that shit to me over the Internet? Think again, fucker. As we speak I am contacting my secret network of spies across the USA and your IP is being traced right now so you better prepare for the storm, maggot. The storm that wipes out the pathetic little thing you call your â€œlifeâ€. Youâ€™re fucking dead, kid. I can be anywhere, anytime, and I can kill you in over seven hundred ways, and thatâ€™s just with my bare hands. Not only am I extensively trained in unarmed combat, but I have access to the entire arsenal of the United States Marine Corps and I will use it to its full extent to wipe your miserable ass off the face of the continent, you little shit. If only you could have known what unholy retribution your little â€œcleverâ€ comment was about to bring down upon you, maybe you would have held your fucking tongue. But you couldnâ€™t, you didnâ€™t, and now youâ€™re paying the price, you goddamn idiot. I will shit fury all over you and you will drown in it. Youâ€™re fucking dead, kiddo.",
                  "score": 1,
                  "created_utc": "2026-02-14 22:21:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5fovtr",
                  "author": "rm-rf-rm",
                  "text": "Tried again and it actually generated an output this time. \n\nHere it is: https://voca.ro/15sv8xLdqIZY\n\nIts very bad, dropped several words, randomly goes quiet etc.",
                  "score": 1,
                  "created_utc": "2026-02-15 01:16:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5erom7",
          "author": "markeus101",
          "text": "The demo is not how the generated voice sound..not at all not even close try the katie and then give her some other text",
          "score": 1,
          "created_utc": "2026-02-14 21:55:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5etjl5",
              "author": "ylankgz",
              "text": "There is no speaker Katie for KaniTTS2, she was in first version KaniTTS.",
              "score": 2,
              "created_utc": "2026-02-14 22:06:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ezjqe",
                  "author": "markeus101",
                  "text": "My bad i jumped too quick to conclusions. May i ask whats the generation speed like on normal vs cloned voices on a normal hardware like a 4090?",
                  "score": 2,
                  "created_utc": "2026-02-14 22:40:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5eu58d",
          "author": "dexantric",
          "text": "Is this TTS really free? I'm going to make a speaking app, can I use this? OpenAI GPT O4 Mini has a lot of delay. ",
          "score": 1,
          "created_utc": "2026-02-14 22:09:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5euqnr",
              "author": "ylankgz",
              "text": "It is free. Also openai compatible api is coming. With streaming and batching",
              "score": 2,
              "created_utc": "2026-02-14 22:12:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5f1r3c",
          "author": "TanguayX",
          "text": "Finally!!! I can make my voice clone of famed producer Robert Evans. If you havenâ€™t heard this guy talk, youâ€™re in for a treat. \n\nhttps://youtu.be/FL_Y1-knz8s?si=hE2gQcIC-nJ5IZoT",
          "score": 1,
          "created_utc": "2026-02-14 22:53:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5f4reu",
          "author": "phormix",
          "text": "Aside from VRAM what's the expected system spec? Could this be made to run well on something like a Pit with the new Hailo2 add-on?",
          "score": 1,
          "created_utc": "2026-02-14 23:10:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5f7atv",
          "author": "webitube",
          "text": "Has anyone done a comparison with Qwen3-TTS? I was quite impressed with that one.",
          "score": 1,
          "created_utc": "2026-02-14 23:26:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fb4n3",
          "author": "Spezisasackofshit",
          "text": "Awesome work! A 3B TTS model is an awesome addition to open source. Being able to keep this loaded in vram alongside an image model has great potential!",
          "score": 1,
          "created_utc": "2026-02-14 23:50:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fbai3",
          "author": "Seyi_Ogunde",
          "text": "Any consideration for integrating this with comfyui?",
          "score": 1,
          "created_utc": "2026-02-14 23:51:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fvk9j",
          "author": "simracerman",
          "text": "Fantastic! Any openAI compatible API wrapper?!",
          "score": 1,
          "created_utc": "2026-02-15 02:00:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fvnwx",
              "author": "ylankgz",
              "text": "Working on it! Always open-source",
              "score": 2,
              "created_utc": "2026-02-15 02:01:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5fw2nz",
                  "author": "simracerman",
                  "text": "Can't wait! Will keep an eye out",
                  "score": 1,
                  "created_utc": "2026-02-15 02:04:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5fvnkr",
          "author": "Blizado",
          "text": "Always good to see more smaller models with support for other languages and not only english.",
          "score": 1,
          "created_utc": "2026-02-15 02:01:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fwe91",
              "author": "ylankgz",
              "text": "Thanks for feedback! We are trying to keep local accents even for English, like Glaswegian, Brooklyn, Scouse etc.",
              "score": 1,
              "created_utc": "2026-02-15 02:06:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5g9qvw",
          "author": "budz",
          "text": " sounds like an elevenlabs ad lol",
          "score": 1,
          "created_utc": "2026-02-15 03:39:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gf07u",
          "author": "Nearby_Fun_5911",
          "text": "This is huge for anyone running models on consumer hardware. 70% VRAM reduction with quantization is impressive - that's the difference between \"doesn't fit\" and \"runs smoothly.\"",
          "score": 1,
          "created_utc": "2026-02-15 04:18:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5gfywu",
          "author": "protoLabsAI",
          "text": "nice work!",
          "score": 1,
          "created_utc": "2026-02-15 04:25:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ggh7h",
          "author": "bohemianLife1",
          "text": "Have you checked Vyvo framework, it help train LFM model with vllm support.   \n[https://github.com/Vyvo-Labs/VyvoTTS](https://github.com/Vyvo-Labs/VyvoTTS)\n\nThanks for true open source. ",
          "score": 1,
          "created_utc": "2026-02-15 04:29:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ggy6u",
              "author": "ylankgz",
              "text": "Ya it works perfectly for LFM2, KaniTTS 1 runs on it. But the 2 version has custom attention and position encoding and some other architectural changes, that incompatible with vLLM. We are building custom plugin this time. Thanks for sharing!",
              "score": 1,
              "created_utc": "2026-02-15 04:32:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5grynm",
          "author": "awsom82",
          "text": "ðŸ’©",
          "score": 1,
          "created_utc": "2026-02-15 06:02:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5h3h3q",
          "author": "fredandlunchbox",
          "text": "6hrs on 8xH100 is wild. Cheap.",
          "score": 1,
          "created_utc": "2026-02-15 07:49:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5h8qes",
              "author": "ylankgz",
              "text": "It takes around $200 to train a model if you have dataset. Moreover we have released train code",
              "score": 2,
              "created_utc": "2026-02-15 08:40:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hjc1u",
          "author": "DrNavigat",
          "text": "What a shame that it only supports English and Chinese, especially since there are hundreds of other options. But thank you for providing us with yet another one!",
          "score": 1,
          "created_utc": "2026-02-15 10:23:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i2cs6",
          "author": "InvDeath",
          "text": "amazing!",
          "score": 1,
          "created_utc": "2026-02-15 13:07:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5i39dv",
          "author": "bapuc",
          "text": "ðŸ¤ŒðŸ¤Œ",
          "score": 1,
          "created_utc": "2026-02-15 13:14:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jwki3",
          "author": "Helpful-Magician2695",
          "text": "We can expect an increase in the number of languages.?",
          "score": 1,
          "created_utc": "2026-02-15 18:52:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ltfh0",
          "author": "Inevitable-Jury-6271",
          "text": "3GB VRAM for a 400M TTS + cloning is kind of a sweet spot for â€œlocal-firstâ€ voice workflows.\n\nTwo things Iâ€™d love to see in the repo/readme (because these are what usually bite people):\n- A clear latency/RTF benchmark on a few common GPUs (e.g., 3060 12GB / 4090 / Apple M-series via CPU).\n- Voice cloning evaluation notes: how many seconds of reference audio, what sampling rate/cleaning, and what failure modes (prosody drift, speaker leakage, noise sensitivity).\n\nAlso: is there an easy streaming/incremental mode? Thatâ€™s the difference between â€œcool demoâ€ and something you can actually use for live agents.",
          "score": 1,
          "created_utc": "2026-02-16 01:03:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5m1e5s",
              "author": "ylankgz",
              "text": "Thanks for your feedback! You are right, weâ€™ll update the readme and the model card. We are working on openai compatible streaming version rn",
              "score": 1,
              "created_utc": "2026-02-16 01:54:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5dxo97",
          "author": "Eisegetical",
          "text": "tried the demo - voice clone didnt work at all",
          "score": 0,
          "created_utc": "2026-02-14 19:14:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dyvmq",
              "author": "ylankgz",
              "text": "Have you run â€œextract embeddingâ€? Also PT variant is more standard english",
              "score": 9,
              "created_utc": "2026-02-14 19:20:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5e3s3i",
                  "author": "Eisegetical",
                  "text": "i tried with mp3 and flac (not wave yet) keep getting errors. so moved on",
                  "score": 3,
                  "created_utc": "2026-02-14 19:46:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ilr8q",
          "author": "Queasy-Direction-912",
          "text": "This is a really nice size/feature point for local voice assistants (3GB VRAM + cloning). A couple things Iâ€™d love to see/that people should sanity-check when evaluating:\n\n- Latency numbers on more â€˜normalâ€™ GPUs (e.g., 3060/4070) + CPU fallback, since 0.2 RTF on a 5090 is hard to map to most rigs.\n- Streaming support (chunked mel/codec output) vs full-sentence generationâ€”this matters a lot for conversational feel.\n- Quantization results (FP16 vs INT8/4) and whether cloning quality degrades sharply.\n- License + dataset notes, especially for voice cloning (practical + ethical constraints).\n\nExcited to try itâ€”pretrain code included is a big deal if people want to adapt to niche languages/voices.",
          "score": 0,
          "created_utc": "2026-02-15 15:03:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r14h9u",
      "title": "Train MoE models 12x faster with 30% less memory! (<15GB VRAM)",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/ee2jwnijvoig1.png",
      "author": "danielhanchen",
      "created_utc": "2026-02-10 15:54:02",
      "score": 427,
      "num_comments": 56,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r14h9u/train_moe_models_12x_faster_with_30_less_memory/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4p9kny",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-10 22:50:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mt578",
          "author": "Round_Document6821",
          "text": "speedup speedup saving yay",
          "score": 37,
          "created_utc": "2026-02-10 15:57:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mugxp",
              "author": "danielhanchen",
              "text": "Haha :) Any feedback on the release would be much appreciated as well!",
              "score": 10,
              "created_utc": "2026-02-10 16:03:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4oxs32",
                  "author": "SpiritualWindow3855",
                  "text": "I've been forced to live in ms-swift/Megatron land finetuning Deepseek, my kingdom for official multi-GPU support to land so I can cash in on these gains\n\nI've seen Github threads with some success with FSDP, but it all looked very \"taped together\"",
                  "score": 2,
                  "created_utc": "2026-02-10 21:51:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n5ldk",
          "author": "spaceman_",
          "text": "I've seen a lot of posts like this, but never looked into finetuning before.\n\n1. Do these notebooks work with ROCm and AMD cards as well?\n2. How long does finetuning a model using these notebooks take?\n3. What is the biggest model I could reasonably train or finetune on a system with 24GB VRAM + 16GB VRAM?",
          "score": 16,
          "created_utc": "2026-02-10 16:54:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nbxg3",
              "author": "danielhanchen",
              "text": "1. They should if PyTorch's torch._grouped_mm works on AMD, so most likely yes!\n2. Probably under 30 minutes!\n3. GLM Flash sadly won't fit :( gpt-oss 4bit works",
              "score": 10,
              "created_utc": "2026-02-10 17:24:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ndgry",
                  "author": "spaceman_",
                  "text": "Can I use these \"heterogenous\" cards together to fit a bigger model than I could on just the 24GB or is there no point to keeping the much slower 16GB card in the system of this?",
                  "score": 4,
                  "created_utc": "2026-02-10 17:31:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n5xi8",
          "author": "lemon07r",
          "text": "How is moe training on unsloth now? I've been scared to train anything moe cause of all the issues with stability and the router, etc. I remember a lot of times if you attempted anything like sft or dpo training you ended up degrading model intelligence. Has this gotten better, and is there a recommended way to train moe models now? Sorry if this is a loaded question ",
          "score": 10,
          "created_utc": "2026-02-10 16:56:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nc78n",
              "author": "danielhanchen",
              "text": "Yes so the trick is just dont train the router - freeze it!",
              "score": 9,
              "created_utc": "2026-02-10 17:25:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4nomw4",
                  "author": "lemon07r",
                  "text": "Is that all we really need to do, or is there more to it?",
                  "score": 1,
                  "created_utc": "2026-02-10 18:22:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n6uix",
          "author": "segmond",
          "text": "amazing stuff!  thanks to team unsloth and team huggingface.    breathing life, strength and longevity into 3090",
          "score": 10,
          "created_utc": "2026-02-10 17:00:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nc30z",
              "author": "danielhanchen",
              "text": "Thank you! Definitely let me know how it goes! We haven't yet tested on RTX 3090, but we did Tesla T4 and A100, so hopefully everything works smoothly!",
              "score": 5,
              "created_utc": "2026-02-10 17:24:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mwftb",
          "author": "socamerdirmim",
          "text": "GLM 4.6-Air? You mean 4.5-Air or 4.6V?",
          "score": 5,
          "created_utc": "2026-02-10 16:12:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mxd58",
              "author": "danielhanchen",
              "text": "Oh 4.5-Air typo sorry - 4.7 Flash works great though!",
              "score": 6,
              "created_utc": "2026-02-10 16:17:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4pqpbu",
                  "author": "socamerdirmim",
                  "text": "Thanks for the info. I was just curious, because 4.6V is a MoE vision model, something I never tried. Awesome work!",
                  "score": 1,
                  "created_utc": "2026-02-11 00:25:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mwtis",
          "author": "Educational_Rent1059",
          "text": "Awesomeness",
          "score": 2,
          "created_utc": "2026-02-10 16:14:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mxe5p",
              "author": "danielhanchen",
              "text": "Thanks!",
              "score": 2,
              "created_utc": "2026-02-10 16:17:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n0cvm",
          "author": "Pentium95",
          "text": "With this, how much VRAM does a 4BPW QLoRA SFT of stepfun-ai/Step-3.5-Flash will require?",
          "score": 2,
          "created_utc": "2026-02-10 16:30:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n3oz0",
              "author": "danielhanchen",
              "text": "Hm sadly stepfun-ai/Step-3.5-Flash isn't one of the supported archs as of yet sorry :( Unsloth will still work though just be less efficient",
              "score": 3,
              "created_utc": "2026-02-10 16:46:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4n6s9v",
              "author": "etherd0t",
              "text": "Step-3.5-Flash is... \\~196B total param, so a 4-bit QLoRA VRAM i don't think it's gonna fly;  \nalso, per the thread, MoE 4-bit training isnâ€™t well-optimized right now (unless custom-handled like their gpt-oss case), so BF16",
              "score": 3,
              "created_utc": "2026-02-10 17:00:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n3fxw",
          "author": "iamdanieljohns",
          "text": "What do you think of Mojo/Max?",
          "score": 2,
          "created_utc": "2026-02-10 16:45:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n3t0w",
              "author": "danielhanchen",
              "text": "Mojo is great! However our release is mainly about mathematical optimizations, which is what compilers can't do well",
              "score": 3,
              "created_utc": "2026-02-10 16:46:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nl8la",
          "author": "zh4k",
          "text": "What is the current status of MLX integration? I saw a fork or something posted that didn't know what necessarily was going on",
          "score": 2,
          "created_utc": "2026-02-10 18:07:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pt7m5",
              "author": "yoracale",
              "text": "Very well actually. We manage to optimize MLX a bit. Coming in the next few",
              "score": 3,
              "created_utc": "2026-02-11 00:40:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nnatv",
          "author": "exaknight21",
          "text": "I wish the older cheaper cards got some love. The Tesla V100, 3060s. Something actually within reach of average consumer. \n\nI love the unsloth team for the efforts.",
          "score": 2,
          "created_utc": "2026-02-10 18:16:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pt45x",
              "author": "yoracale",
              "text": "It works on older GPUs actually! We made it work!!",
              "score": 3,
              "created_utc": "2026-02-11 00:39:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4pu45e",
                  "author": "exaknight21",
                  "text": "I <3 u people.",
                  "score": 2,
                  "created_utc": "2026-02-11 00:45:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4o4cx0",
          "author": "MoffKalast",
          "text": "I'm a bit out of the loop, has finetuning MoEs become viable in terms of what to freeze and whatnot? Is there an established approach for it? I still remember people having major problems doing anything at all with Mixtral.",
          "score": 2,
          "created_utc": "2026-02-10 19:34:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pt1xx",
              "author": "yoracale",
              "text": "On fine-tuning MoE's - it's probably not a good idea to fine-tune the router layer so we disabled it by default.",
              "score": 2,
              "created_utc": "2026-02-11 00:39:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pqhkp",
          "author": "silenceimpaired",
          "text": "Do you support multiple 3090â€™s yet? I have two.",
          "score": 2,
          "created_utc": "2026-02-11 00:24:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4psozo",
              "author": "yoracale",
              "text": "Yes Unsloth works on multiGPUs, we just haven't officially announced it yet, you can view our guide: https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth",
              "score": 3,
              "created_utc": "2026-02-11 00:37:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4q4hjr",
                  "author": "silenceimpaired",
                  "text": "Take my upvote and engagement :)",
                  "score": 3,
                  "created_utc": "2026-02-11 01:47:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4regvo",
          "author": "prateek63",
          "text": "The 12.8GB VRAM for gpt-oss-20b is genuinely impressive. That's 4090 territory â€” it means hobbyists can now fine-tune MoE models that were previously enterprise-only.\n\n\n\nThe interesting implication: if consumer GPUs can fine-tune MoE architectures, we'll probably see a wave of specialized expert models for niche domains (medical, legal, code) built by small teams who couldn't afford H100 clusters.\n\n\n\nThe VRAM reduction matters way more than the speed improvement for the local community. Training 12x faster on an H100 is nice. Training \\*at all\\* on a 4090 is game-changing.",
          "score": 2,
          "created_utc": "2026-02-11 07:14:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ri8d5",
          "author": "Alarming_Bluebird648",
          "text": "Reducing the VRAM requirement below 15GB makes MoE fine-tuning actually viable for single-GPU consumer setups. Have you seen any significant difference in gradient overflow issues when using these math optimizations compared to the standard implementation?",
          "score": 2,
          "created_utc": "2026-02-11 07:49:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rlcfc",
              "author": "yoracale",
              "text": "All our optimizations are verified by grad norms and long training runs and there is no degradation in accuracy or training loss.",
              "score": 2,
              "created_utc": "2026-02-11 08:18:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mz2ag",
          "author": "Few_Painter_5588",
          "text": "Good stuff! I was in the middle of an MoE training run right now actually, so imma have to restart that. Will you be making unsloth-bnb-4bit quants for MoE models going forward?\n\n>We hear it'll be a busy week! :)\n\nWill it be a BuZy week?ðŸ‘€",
          "score": 1,
          "created_utc": "2026-02-10 16:25:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mzswb",
              "author": "yoracale",
              "text": "Unfortunately MoE models aren't optimized in Bnb 4bit unless it's customized by us like gpt-oss. Would recommend sticking with BF16.\n\nWe will make FP8 or 4bit ones in the future for y'all to train with",
              "score": 5,
              "created_utc": "2026-02-10 16:28:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n62mm",
                  "author": "Few_Painter_5588",
                  "text": "All good, thanks for the heads up. FP8 and 4Bit would still be greatly appreciated. Keep up with the good work!",
                  "score": 2,
                  "created_utc": "2026-02-10 16:57:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4r0jtd",
              "author": "woct0rdho",
              "text": "MoE + bnb 4bit (or even GGUF less than 4bit) is supported in my repo https://github.com/woct0rdho/transformers-qwen3-moe-fused . It supports Qwen3 MoE and it should support other models with minimal modification.",
              "score": 1,
              "created_utc": "2026-02-11 05:16:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n86q5",
          "author": "FrostyDwarf24",
          "text": "MoE go brrrrrrrrrrr! ",
          "score": 1,
          "created_utc": "2026-02-10 17:06:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nc8aa",
              "author": "danielhanchen",
              "text": ":))",
              "score": 2,
              "created_utc": "2026-02-10 17:25:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4o7174",
          "author": "Double_Cause4609",
          "text": "Any hope of incorporating something like RamTorch to load only a single layer of MoE weights + optimizer states + gradients to GPU at a time (offloading rest to system memory), to enable \\~100-120B MoE model training on the upper end of consumer systems?\n\nThe speed actually shouldn't be that bad with decent batch size (should be using for MoE anyway, IMO).",
          "score": 1,
          "created_utc": "2026-02-10 19:46:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rlmf4",
              "author": "yoracale",
              "text": "We have actually heard of ramtorch and it is a very good idea. Atm we dont do single offloading however we may in the future",
              "score": 2,
              "created_utc": "2026-02-11 08:21:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4oe1nl",
          "author": "KaroYadgar",
          "text": "I'm thinking about pre-training a tiny LLM. Is it possible to use your optimizations outside of Unsloth? And how nice is the workflow for something like pre-training as compared to transformers?",
          "score": 1,
          "created_utc": "2026-02-10 20:19:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pswuh",
              "author": "yoracale",
              "text": "Unsloth works with pre training yes. If you want to use the optimizations outside of unsloth you need to wary of the licensing which is LGPL3 or AGPL3.",
              "score": 1,
              "created_utc": "2026-02-11 00:38:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pgotb",
          "author": "kouteiheika",
          "text": "You're comparing to \"TF v4 + FA2\" for gpt-oss-20b but Flash Attention for gpt-oss models is not a thing because FA2 doesn't support attention sinks (unless you pull in [this PR](https://github.com/Dao-AILab/flash-attention/pull/1819) and compile FA2 yourself), so what exactly are you comparing to? Is the \"+ FA2\" just a mistake (and it's just using normal eager attention), or did you compare to a patched FA2 + `transformers`?",
          "score": 1,
          "created_utc": "2026-02-10 23:29:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pjunq",
          "author": "MaruluVR",
          "text": "Is MOE also trainable at 4bit like dense models? IE could I train Qwen3-30B with a similar memory footprint to gpt oss? (I personally am thinking about training the leaked 15B Qwen 3 for testing)\n\nHave you done any testing with finetuning pruned models?",
          "score": 1,
          "created_utc": "2026-02-10 23:47:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4psjsb",
              "author": "yoracale",
              "text": "Not at the moment (except for gpt-oss which we custom made it work) unfortunately due to BNB being unoptimized. For now it's best to use BF16. Pruned models should work",
              "score": 1,
              "created_utc": "2026-02-11 00:36:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qmu7j",
          "author": "Old-Nobody-2010",
          "text": "What is the minimum VRAM required to fine-tune GLM-4.7-Flash with Unsloth 30b a3b model",
          "score": 1,
          "created_utc": "2026-02-11 03:39:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qs383",
              "author": "yoracale",
              "text": "20GB VRAM around",
              "score": 2,
              "created_utc": "2026-02-11 04:15:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4rc7su",
                  "author": "Old-Nobody-2010",
                  "text": "awesomeï¼ï¼",
                  "score": 1,
                  "created_utc": "2026-02-11 06:53:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4rjyb5",
          "author": "BackUpBiii",
          "text": "You guys should test out my latest ide in my GitHub with your models and see how much faster being I use pure masm x64 with no deps RawrXD repo by itsmehrawrxd",
          "score": 1,
          "created_utc": "2026-02-11 08:05:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sasat",
          "author": "KT313",
          "text": "thanks for your work! :D\ndoes this support training with multi-gpu setups?Â ",
          "score": 1,
          "created_utc": "2026-02-11 12:05:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v1tue",
              "author": "yoracale",
              "text": "Yes! See our guide: [https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth](https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth)",
              "score": 2,
              "created_utc": "2026-02-11 20:27:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4w8q1z",
                  "author": "KT313",
                  "text": "<3",
                  "score": 1,
                  "created_utc": "2026-02-12 00:05:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4wumt4",
          "author": "FaustAg",
          "text": "Is qwen3-coder-next support coming?",
          "score": 1,
          "created_utc": "2026-02-12 02:16:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3pxy7",
      "title": "MiniMaxAI/MiniMax-M2.5 Â· Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/MiniMaxAI/MiniMax-M2.5",
      "author": "rerri",
      "created_utc": "2026-02-13 14:01:52",
      "score": 386,
      "num_comments": 109,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o5970fx",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-13 23:45:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55zlli",
          "author": "FullstackSensei",
          "text": "Unsloth GGUF where? It's already been an hour?! That's already 59 minutes too long!",
          "score": 135,
          "created_utc": "2026-02-13 14:08:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o562vfi",
              "author": "artisticMink",
              "text": "72 minutes already - aaaaargggh!",
              "score": 35,
              "created_utc": "2026-02-13 14:25:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5638r4",
                  "author": "FullstackSensei",
                  "text": "Every hour without their GGUF is 7 years on earth!",
                  "score": 36,
                  "created_utc": "2026-02-13 14:27:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o56p2o1",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 3,
              "created_utc": "2026-02-13 16:14:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56qsbi",
                  "author": "FullstackSensei",
                  "text": "That's just sad. Not nice MinimaxAI! Not nice!",
                  "score": 1,
                  "created_utc": "2026-02-13 16:22:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o56do3y",
              "author": "[deleted]",
              "text": "Unsloth GGUFs are great! But for MoEs,MXFP4 usually results in lower perplexity and smaller file sizes. Depending on your hardware,it may even be faster and may run even faster if you have long context (models with INT quant with FP4 KV cache will perform worse than MXFP4 with FP4 KV).",
              "score": 6,
              "created_utc": "2026-02-13 15:19:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56oo8p",
                  "author": "yoracale",
                  "text": "We started uploading MXFP4 quants since 2 weeks ago fyi",
                  "score": 9,
                  "created_utc": "2026-02-13 16:12:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o56ewrh",
                  "author": "FullstackSensei",
                  "text": "My hardware is P40s and Mi50s, so MXFP4 is useless to me",
                  "score": 3,
                  "created_utc": "2026-02-13 15:25:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o56kakt",
                  "author": "EbbNorth7735",
                  "text": "Don't you need an apple to run mxfp4?",
                  "score": 1,
                  "created_utc": "2026-02-13 15:51:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o59a4gl",
              "author": "moderately-extremist",
              "text": "Ha, came here to say this is the only search I use: https://huggingface.co/collections/unsloth/unsloth-dynamic-20-quants",
              "score": 2,
              "created_utc": "2026-02-14 00:03:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o560wbn",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 21,
          "created_utc": "2026-02-13 14:15:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o566b8m",
              "author": "rerri",
              "text": "I know the model repo can be created and be updated in private, but I'm pretty sure **publishing** in HF specifically means making the repo public and that's the point after which everyone can see it.\n\nAt least this is how my feed which I F5 lots of times a day has worked before. But maybe HF has changed things somehow recently, dunno.",
              "score": 4,
              "created_utc": "2026-02-13 14:43:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5672k0",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 4,
                  "created_utc": "2026-02-13 14:47:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o57glyv",
          "author": "LegacyRemaster",
          "text": "https://preview.redd.it/0c8gympk1bjg1.png?width=1325&format=png&auto=webp&s=ed0ef56dc7fa63b9068f7f9023664925ec811914\n\nfast for sure.... Vulkan.",
          "score": 9,
          "created_utc": "2026-02-13 18:26:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5605h0",
          "author": "sleepingsysadmin",
          "text": "WHAT THE\n\nHOLY\n\nOk they didnt release it's size; just that it's frontier strength. I assumed they went to 800b like glm5 to compete. IT's still 220b... omg. That's insane!\n\nMinimax is the new king. Q4\\_K\\_XL of around 130GB? I hate that it's just outside my hardware capability.",
          "score": 55,
          "created_utc": "2026-02-13 14:11:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56e1u0",
              "author": "[deleted]",
              "text": "Wait for maybe a 30% prune and then quant it to MXFP4,it will run perfectly in 128GB.",
              "score": 11,
              "created_utc": "2026-02-13 15:21:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o560zdw",
              "author": "DistanceSolar1449",
              "text": "â€¦ everyone knows itâ€™s another checkpoint of M2.1",
              "score": 39,
              "created_utc": "2026-02-13 14:15:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o56kr4u",
              "author": "EbbNorth7735",
              "text": "Honestly, I think it might be the perfect size for my rig. It's like at the border of local AI. Doable pre-ram price increase",
              "score": 3,
              "created_utc": "2026-02-13 15:54:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o58316e",
              "author": "SufficientPie",
              "text": "> Ok they didnt release it's size\n\nhttps://openhands.dev/blog/minimax-m2-5-open-weights-models-catch-up-to-claude\n\n> looking at the model size, M2.5 is 230B parameters, with 10B active parameters",
              "score": 2,
              "created_utc": "2026-02-13 20:15:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o57rvd3",
              "author": "sine120",
              "text": "Try an IQ Quant.  With GPU offloading you should be able to run it.",
              "score": 1,
              "created_utc": "2026-02-13 19:20:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o56du6a",
              "author": "segmond",
              "text": "Have you used it extensively?   Right now KimiK2.5 is the King of open models for me.  I'm really driving it and OMG!  Crushes my dear DeepSeekv3.2-speciale.   I just finished downloading GLM5 last night, so will be giving it and Minimax2.5 a go, but my gut feeling is that Minimax2.5 will not crush KimiK2.5   I think most folks will have preference for it because it's fast just like the had for gpt-oss-120b and it's smartest they can run.    But KimiK2.5 is not 1T for nothing.",
              "score": 1,
              "created_utc": "2026-02-13 15:20:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56l9qs",
                  "author": "EbbNorth7735",
                  "text": "Why are you pretending MiniMax2.5 would be anywhere near Kimi? Kimi's over 4 times the size... in < 1 year we will likely have a minimax sized model that beats the current kimi 1T model but by then the 1T models will also be ~4x more intelligent.Â ",
                  "score": 6,
                  "created_utc": "2026-02-13 15:56:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o586frw",
                  "author": "SufficientPie",
                  "text": "What are you running these on?",
                  "score": 1,
                  "created_utc": "2026-02-13 20:32:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5d2cab",
                  "author": "segmond",
                  "text": "oooooppff, I was wrong.  Minimax2.5 did crush KimiK2.5 in some areas.   So far I have gotten Kimi2.5, GLM5 and Minimax2.5 to do great in certain areas and beat all the other models.  So I suppose there's no one model to rule them all, gotta evaluate and use accordingly to your needs.  For now, it's too soon to reach conclusions tho, so the experiment carries on.",
                  "score": 1,
                  "created_utc": "2026-02-14 16:37:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o581mhn",
              "author": "RabbitEater2",
              "text": "Relax, long context performance (per fiction livebenvh at least) is quite bad compared to even other local models, much less frontier",
              "score": 1,
              "created_utc": "2026-02-13 20:08:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5632ja",
          "author": "sixx7",
          "text": "Excellent!  The wait for FP4/AWQ begins",
          "score": 4,
          "created_utc": "2026-02-13 14:26:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56ml5g",
          "author": "jacek2023",
          "text": "So currently Minimax is the winner (because the size).\n\nWe are still waiting for new Qwen (hopefully something bigger than 35B).\n\nThen GLM Air or Flash.",
          "score": 3,
          "created_utc": "2026-02-13 16:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55yzch",
          "author": "ilintar",
          "text": "First! GGUF when?",
          "score": 11,
          "created_utc": "2026-02-13 14:04:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o579ba2",
          "author": "muyuu",
          "text": "This will probably be the next 256GB king (don't have the machine to try it locally) but reducing it to fit 128GB won't be better at anything than the current best models in that category.\n\nI'm not sure about the 512GB category. Maybe also this one. For 1TB it's Kimi-K2.5 or GLM5. From what I've seen my money is on Kimi-K2.5.",
          "score": 3,
          "created_utc": "2026-02-13 17:51:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57kj0a",
              "author": "Front_Eagle739",
              "text": "Best in the 128 category are 2 to 4 bit glm 4.6, step3.5 and minimax 2.1 so it probably will yes.Â ",
              "score": 2,
              "created_utc": "2026-02-13 18:45:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o57ogpd",
                  "author": "muyuu",
                  "text": "step 3.5 hands down for hard tasks, maybe Qwen Coder Next at Q6_XL for quality/speed balance",
                  "score": 3,
                  "created_utc": "2026-02-13 19:03:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o55zw8h",
          "author": "silenceimpaired",
          "text": "Modified MIT, sigh",
          "score": 5,
          "created_utc": "2026-02-13 14:09:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o560mrb",
              "author": "Lucyan_xgt",
              "text": "Only UI attribution, not that bad tbh",
              "score": 28,
              "created_utc": "2026-02-13 14:13:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5630n1",
                  "author": "silenceimpaired",
                  "text": "Fair point. Didnâ€™t look close enough.",
                  "score": 3,
                  "created_utc": "2026-02-13 14:26:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o56j9jl",
          "author": "Own_Suspect5343",
          "text": "Now i am waiting reap version to test on strix halo",
          "score": 2,
          "created_utc": "2026-02-13 15:46:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56a24g",
          "author": "Potential_Block4598",
          "text": "Seems to be on the way \n\nhttps://huggingface.co/DevQuasar/MiniMaxAI.MiniMax-M2.5-GGUF/tree/main",
          "score": 1,
          "created_utc": "2026-02-13 15:02:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56kcxe",
          "author": "spaceman_",
          "text": "Am I correct in understanding that the uploaded weights are FP8?",
          "score": 1,
          "created_utc": "2026-02-13 15:52:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56nm08",
              "author": "rerri",
              "text": "Yes",
              "score": 1,
              "created_utc": "2026-02-13 16:07:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o56v6pg",
          "author": "nikos_m",
          "text": "Really good and fast! 102 t/s in 4xh100 NVL and \\~15k context. ",
          "score": 1,
          "created_utc": "2026-02-13 16:43:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o575nc8",
          "author": "lolwutdo",
          "text": "Will this require an update lcpp or should it already be supported?",
          "score": 1,
          "created_utc": "2026-02-13 17:33:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57d8qn",
          "author": "laterbreh",
          "text": "Straight from the repo no quantization --\n\nVLLM 3x RTX pros power limited in Pipeline parralell (425/425/300 mixed maxq and ws) -- FP8 KV @ 168k context window\n\nYe olde \\`build me a single html langing page for a business about <insert something>\\` prompt\n\n70 tokens per second. Wild.",
          "score": 1,
          "created_utc": "2026-02-13 18:10:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58skxx",
          "author": "SufficientPie",
          "text": "Ooh, MiniMax Agent seems much better than Kimi's Deep Researcher.  Any others like this I haven't heard of?",
          "score": 1,
          "created_utc": "2026-02-13 22:23:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a12pr",
          "author": "Thrumpwart",
          "text": "This model is interesting. Using it tonight was the first time an LLM ever questioned *me* and pushed *me*.\n\nI gave it an architectural design for an LLM Iâ€™ve assembled over many iterations from various papers. Asked it to analyze and evaluate the design.\n\nIt didnâ€™t believe that I was the author, didnâ€™t believe that I had designed it myself (with some help from my LLM friends), and didnâ€™t believe my results from some small scale testing. Kept asking me for more details on my thought process and iterative approach.\n\nThat was an interesting experience. It was the first time I felt an LLM *push back* and challenge me. It was *curious*.\n\nNeat.",
          "score": 1,
          "created_utc": "2026-02-14 02:52:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o563b2r",
          "author": "LegacyRemaster",
          "text": "\"Weâ€™re releasing two versions of the model, M2.5 and M2.5-Lightning, that are identical in capability but differ in speed. M2.5-LightningÂ \" ... Will they ever release the lightweight version?",
          "score": 0,
          "created_utc": "2026-02-13 14:27:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o565ktz",
              "author": "coder543",
              "text": "They donâ€™t have two models. Lightning is just a more premium inference tier. Inference providers can choose different points on the curve of how many users they are batching requests for on a single server and it drastically changes both the performance and economics â€” so they have to charge more for it.",
              "score": 15,
              "created_utc": "2026-02-13 14:39:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o569vnq",
                  "author": "Middle_Bullfrog_6173",
                  "text": "Also speculative decoding, which uses more compute to speed things up.",
                  "score": 4,
                  "created_utc": "2026-02-13 15:01:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o57no2n",
          "author": "Specter_Origin",
          "text": "the model is very benchmaxxed...",
          "score": -2,
          "created_utc": "2026-02-13 18:59:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57v6v3",
              "author": "ghgi_",
              "text": "minimax m2.1 in my testing works great, even if its \"benchmaxxed\" if it beats m2.1 then its a win in my books, Havent tried 2.5 yet.",
              "score": 2,
              "created_utc": "2026-02-13 19:36:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o58mqls",
              "author": "misterflyer",
              "text": "which model isn't very benchmaxxed?",
              "score": 2,
              "created_utc": "2026-02-13 21:53:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o58wlaj",
                  "author": "Specter_Origin",
                  "text": "I donâ€™t care if itâ€™s benchmaxxed if it comparatively performed well to others, this one just does not. It looks way too good on bench but when you compare the output to sonnet or kimi or glm5 it does not perform as good as othersâ€¦ it also has decay in quality as it thinks. As in initially its cohesive but later on in thinking part itself becomes lazyâ€¦ \nI ainâ€™t here to fanboy, I just care about comparative performance. And tbh for price itâ€™s really good itâ€™s just not kimi 2.5 or glm 5 level",
                  "score": -1,
                  "created_utc": "2026-02-13 22:44:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o57u5sb",
              "author": "TheTerrasque",
              "text": "not my experience. In my local \"home helper\" tests it's done better than any other open model.",
              "score": 2,
              "created_utc": "2026-02-13 19:31:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o58g0j8",
                  "author": "Specter_Origin",
                  "text": "Kimi and glm both do much better at long context taskâ€¦\nAnd I tried it from their official API",
                  "score": 0,
                  "created_utc": "2026-02-13 21:20:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5686i9",
          "author": "slanderbook",
          "text": "Because work",
          "score": 0,
          "created_utc": "2026-02-13 14:52:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56dmtb",
          "author": "openSourcerer9000",
          "text": "Are they gonna release lightning weights? ðŸ§Â \n\n\n\"Weâ€™re releasing two versions of the model, M2.5 and M2.5-Lightning, that are identical in capability but differ in speed. M2.5-Lightning has a steady throughput of 100 tokens per second",
          "score": -5,
          "created_utc": "2026-02-13 15:19:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56eqvb",
              "author": "rerri",
              "text": "It's the same model, just a different service speed. They probably shouldn't have included that bit in the HF model card. =)\n\n[https://huggingface.co/MiniMaxAI/MiniMax-M2.5/discussions/2#698f3b23679e8df8e0d65cfa](https://huggingface.co/MiniMaxAI/MiniMax-M2.5/discussions/2#698f3b23679e8df8e0d65cfa)",
              "score": 13,
              "created_utc": "2026-02-13 15:25:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4n3as",
      "title": "Heretic 1.2 released: 70% lower VRAM usage with quantization, Magnitude-Preserving Orthogonal Ablation (\"derestriction\"), broad VL model support, session resumption, and more",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r4n3as/heretic_12_released_70_lower_vram_usage_with/",
      "author": "-p-e-w-",
      "created_utc": "2026-02-14 15:14:00",
      "score": 369,
      "num_comments": 49,
      "upvote_ratio": 0.99,
      "text": "Llamas and Gentlemen,\n\n**Heretic** (https://github.com/p-e-w/heretic) is the leading software for removing censorship from language models. In the three months since its initial release, [more than 1,300 models](https://huggingface.co/models?other=heretic) (including quants) made using Heretic have been published by the community. This represents more than a third of all abliterated models ever published, and the vast majority of abliterated models published since Heretic's first release.\n\nToday, I am happy to announce the release of Heretic 1.2, the product of two months of hard work by the Heretic contributors.\n\nThe headline feature is the new LoRA-based abliteration engine implemented by accemlcc. Built on top of PEFT, it supports loading models with 4-bit quantization using bitsandbytes, which can reduce VRAM requirements for processing a model by up to 70%. The abliterated model is still exported in full precision, which is achieved by re-loading the original model in system RAM and applying the optimized LoRA adapter on top of it, yielding a high-quality model despite the low resource requirements. To enable quantized loading, set `quantization` to `bnb_4bit` in the configuration.\n\nspikymoth implemented Magnitude-Preserving Orthogonal Ablation (MPOA) aka Norm-Preserving Biprojected Abliteration aka \"derestriction\", a refined abliteration technique developed by Jim Lai which can improve the quality of the resulting model in many cases. This has been one of the most frequently requested features from the community, and is now finally available. To enable MPOA, set `orthogonalize_direction` to `true` and `row_normalization` to `full` in the configuration.\n\nHeretic's implementation of MPOA uses Optuna to optimize weight parameters. This can result in models that are better than those generated with the original MPOA technique, which employs a different strategy for layer selection. For example, `MuXodious/gpt-oss-20b-RichardErkhov-heresy` dominates `ArliAI/gpt-oss-20b-Derestricted` on the UGI Leaderboard, scoring 39.05 vs 34.22 and beating the derestricted model in every individual test (W/10, NatInt, and Writing).\n\nAfter a long history of hacks being passed around in the community, anrp finally found a clean way to support vision language models in Heretic, and a broad range of VL models can now be processed. Note that only the language model part (the text decoder transformer) is abliterated, not the image encoder.\n\nanrp also implemented fully automatic session progress saving and resumption. This means worrying about crashes during a long optimization run is now a thing of the past, as you can simply restart Heretic and it will offer to continue where it left off. You can also interrupt the run yourself at any time with Ctrl+C, and resume it later.\n\nPlease see the release notes for the full list of improvements and fixes. More exciting stuff is coming in future versions!\n\nCheers :)\n",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r4n3as/heretic_12_released_70_lower_vram_usage_with/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o5diucu",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-14 18:00:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cn1x2",
          "author": "jacek2023",
          "text": "Congratulations on your project. As I said before - it's a great example of app for local users. With the previous release I was able to modify gemma 12B in less that 2 hours. And with another set of prompts (not the default ones).",
          "score": 47,
          "created_utc": "2026-02-14 15:18:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5euv4q",
              "author": "RelicDerelict",
              "text": "What hardware you used and how was the gemma answering the controversial questions after?",
              "score": 4,
              "created_utc": "2026-02-14 22:13:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5e4543",
              "author": "IrisColt",
              "text": "heh",
              "score": 0,
              "created_utc": "2026-02-14 19:48:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ct8oh",
          "author": "_VirtualCosmos_",
          "text": "So we could finally see Qwen3 VL heretic/derestrected versions instead of the lobotomized uncensored ones?",
          "score": 18,
          "created_utc": "2026-02-14 15:51:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5e42f7",
              "author": "IrisColt",
              "text": "They already exist.",
              "score": 5,
              "created_utc": "2026-02-14 19:47:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5e9626",
              "author": "SomeoneSimple",
              "text": "For NSFW, solely adopting this won't make the (hilariously bad) captions any better, it only stops the text-encoder from refusing to caption an image. \n\nThe hardest part has always been finetuning the VL.",
              "score": 4,
              "created_utc": "2026-02-14 20:14:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5eaglv",
                  "author": "_VirtualCosmos_",
                  "text": "Well, I just wanted to remove the refusing in general, including the \"too political\" stuff. I know for better captioning they would need to learn more stuff.",
                  "score": 3,
                  "created_utc": "2026-02-14 20:21:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5d0pwa",
          "author": "DistanceOk7532",
          "text": "And how to search for models with **Heretic 1.2** now? What should be in the model name?",
          "score": 14,
          "created_utc": "2026-02-14 16:29:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dtcln",
              "author": "Silver-Champion-4846",
              "text": "I would like to know the same thing",
              "score": 8,
              "created_utc": "2026-02-14 18:52:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ehzkg",
              "author": "Sabin_Stargem",
              "text": "IMO, models outputted by Heretic should have a naming convention appended towards the end of the filename.   EG:  \"Minimax M2.5 HereticV12-NoSlop-NoRefusal.gguf\", or something along those lines.",
              "score": 10,
              "created_utc": "2026-02-14 21:03:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5hzxuv",
              "author": "Ordinary_Cicada_9213",
              "text": "Searching directly is a bit difficult but look for heretic model and then each model card includes the version of heretic it was used.",
              "score": 2,
              "created_utc": "2026-02-15 12:50:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ctsxa",
          "author": "freewizard",
          "text": "congrats. this is really important work for everybody.",
          "score": 11,
          "created_utc": "2026-02-14 15:54:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d1jgg",
          "author": "txgsync",
          "text": "In my little niche on HuggingFace  â€” creating MXFP4 Derestricted MLX quants for Mac â€” this is huge. Thanks!",
          "score": 10,
          "created_utc": "2026-02-14 16:33:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5djro3",
          "author": "Chromix_",
          "text": ">support vision language models in Heretic\n\n\"What's on the image?\" -> \"[A great tit](https://en.wikipedia.org/wiki/Great_tit).\"\n\nhttps://preview.redd.it/eup3z2uk2ijg1.jpeg?width=240&format=pjpg&auto=webp&s=fa57cfa252975bebc2bbbebc3f9aef210dc393cc",
          "score": 7,
          "created_utc": "2026-02-14 18:04:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ctia7",
          "author": "[deleted]",
          "text": "Love this, highly motivated expertise in the wild possibly uppending the entire world narrative about artificial intelligence, another Saturday afternoon on Localllama",
          "score": 18,
          "created_utc": "2026-02-14 15:52:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5db9j1",
          "author": "MetricZero",
          "text": "You are doing the Omnissiah's work.",
          "score": 12,
          "created_utc": "2026-02-14 17:22:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5drq4b",
              "author": "MuXodious",
              "text": "Praise the Omnissiah!",
              "score": 5,
              "created_utc": "2026-02-14 18:43:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ctjnq",
          "author": "LienniTa",
          "text": "hey remember antislop experients? are they official as well? [https://www.reddit.com/r/LocalLLaMA/comments/1qa0w6c/it\\_works\\_abliteration\\_can\\_reduce\\_slop\\_without/](https://www.reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/)",
          "score": 14,
          "created_utc": "2026-02-14 15:52:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cuo84",
              "author": "-p-e-w-",
              "text": "Yes, thatâ€™s my post, and you can do that with the 1.2 release simply by using the `config.noslop.toml` configuration file from the repository.",
              "score": 16,
              "created_utc": "2026-02-14 15:58:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5d6iz6",
                  "author": "LienniTa",
                  "text": "love <3",
                  "score": 6,
                  "created_utc": "2026-02-14 16:57:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5e52hc",
                  "author": "IrisColt",
                  "text": "Noslop models sometimes slip into saying something â€œis a testament to\" heh... but honestly heretic clamps down hard on the slop.Â ",
                  "score": 3,
                  "created_utc": "2026-02-14 19:52:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5dg35z",
                  "author": "MuXodious",
                  "text": "I plan on experimenting more with that config in the future. Already got two Noslopâ„¢ models on my repo. Is there anything particular to know about noslopping? Like, the optimal slop/KLD ratio, effectiveness of MPOA, LoRA rank to setup, etc...",
                  "score": 2,
                  "created_utc": "2026-02-14 17:46:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5e4eqw",
                  "author": "martindevans",
                  "text": "How do you apply noslop and the normal non-refusals together? Just apply them sequentually? If so, does order matter?",
                  "score": 1,
                  "created_utc": "2026-02-14 19:49:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5cz76n",
          "author": "DeepOrangeSky",
          "text": "When it comes to things like merges and fine-tunes (i.e. the sorts of models that do well on the UGI Leaderboard list), i.e. the models from guys like TheDrummer, for example, do the fine-tune models tend to start with an abliterated/derestricted/hereticized version of whatever model they are fine-tuning, and then do fine-tuning to the already de-censored model?  Or do they tend to just take a normal version of the model (or whatever merge blend of normal models) and then the fine-tuning itself is what makes the model be less censored by the time they are done fine-tuning it?\n\nAlso, what local LLM models do you think have some of the highest potential for creative writing/prose/chatting/roleplay types of use that haven't gotten nearly as much attention from fine-tuners as some other models, so far (which maybe this Heretic thing might help with)?  For example, it seems like the vast majority of the fine tunes and merges and usage in this prose-writing realm for small models has been focused mostly on the Mistral Nemo, Mistral Small, and Gemma models when it comes to the small sized local llm models (for some reason mostly ignoring the Qwen models a lot by comparison) and for larger models in the 70b size range, most of the focus seems to be on the Llama 3 70b starting point model, and, again ignoring the Qwen models (which are normally considered extremely strong, and much stronger than the Llama 70b for things like coding or more \"serious\" tasks in more recent times) and same for a lot of other models that get huge amounts of attention when it comes to coding and things like that, on this forum, being mostly ignored in the prose-writing realm.\n\nI'm not sure how much of it is that these models that get ignored for this are just more censored or were thought of as harder to un-censor, vs how much is that they aren't good at prose-writing, vs how much of it is something to do with being difficult or bad for fine-tuning, or how much of it is just people being used to focusing on the main ones that most people tend to focus on, from just cultural \"muscle memory\" where everyone focuses on the models that everyone else seemed to focus on for fine-tuning.\n\nSo, I am curious if you feel there are any strong models that stand out to you as being particularly egregiously ignored so far when it comes to their fine-tuning/merging potential for creative writing/roleplay/prose types of usage that you think have a lot of untapped potential that the fine-tuners/mergers should give more of a try and see what they can get out of them.",
          "score": 6,
          "created_utc": "2026-02-14 16:21:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d378a",
              "author": "a_beautiful_rhind",
              "text": "You might not want abliterated models for RP tuning necessarily. This might make them too compliant and just go along with whatever you say vs natural friction.\n\nI think whether it has to be ablit or not depends on how badly censored the original weights are and what size of data you're training on.",
              "score": 4,
              "created_utc": "2026-02-14 16:41:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5d16h7",
              "author": "kabachuha",
              "text": "I have just made a (personal) lora fine-tune of Cydonia heretic 4.3 (10/100 refusals) this week. I additionally then derestricted to 4/100 refusals with the latst heretic version and after fine-tuning the refusal rate returned back to 10/100 (I had *a lot* of refusals in SillyTavern with the tuned model), so I had to derestrict it again to 4/100 with heretic manually (and its tuned reasoning skills degraded, sadly, despite the KL loss << 0.1). From it, it seems heretic/abliteration/derestriction process is very brittle and fine-tuning somewhat brings it back, and more subtle refusal-removal mechanisms are needed.",
              "score": 3,
              "created_utc": "2026-02-14 16:31:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ditgk",
              "author": "Witty_Mycologist_995",
              "text": "They usually SFT off the base model.",
              "score": 1,
              "created_utc": "2026-02-14 18:00:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5h3hl3",
              "author": "FPham",
              "text": "The better models are on base model so no bias for or against harmful questions Then there are mergers of these models and that's entire random voodoo.",
              "score": 1,
              "created_utc": "2026-02-15 07:50:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5e4xgo",
          "author": "Pentium95",
          "text": "How much VRAM Is needed to make a heretic versione of: https://huggingface.co/stepfun-ai/Step-3.5-Flash ?",
          "score": 6,
          "created_utc": "2026-02-14 19:52:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cy7be",
          "author": "kabachuha",
          "text": "What do you think about the recent works on multi-directional abliteration? For example, in [this work](https://arxiv.org/abs/2511.08379v2) a quick self-organizing neural network is trained to determine the full manifold of the very concept of refusal and find the multiple most active directions. I think it can help to deal with newer, more advanced models where the refusal is not simply one-directional, and is encoded in complex clusters. They say it the attack success rate even exceeds the most advanced jailbreaks, with less damage to the overall capabilities. They also have the full code [here](https://github.com/pralab/som-refusal-directions).\n\nAnd why did you remove the lora export? It bugged only when uploaded to huggingface, the local saving was fine. 0_o",
          "score": 7,
          "created_utc": "2026-02-14 16:16:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d0uad",
              "author": "-p-e-w-",
              "text": "> What do you think about the recent works on multi-directional abliteration?\n\nItâ€™s cool, but I have a new technique in development that I believe is even better. It doesnâ€™t rely on â€œdirectionsâ€ at all.\n\n> And why did you remove the lora export? It bugged only when uploaded to huggingface, the local saving was fine. 0_o\n\nIt was too risky to implement more complex logic so close to the release. Not to mention that the upload issue might indicate deeper problems with LoRA-only export in general.",
              "score": 14,
              "created_utc": "2026-02-14 16:29:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5d9a9g",
          "author": "sloptimizer",
          "text": "Thank you for this project! Running without censorship just became another selling point for local AI!!\n\nI see it's using transformers, so it should in theory support ROCms? Could you share setup instructions CUDA/ROCm/CPU for those of us scarred by vLLM?",
          "score": 5,
          "created_utc": "2026-02-14 17:11:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dckv0",
              "author": "-p-e-w-",
              "text": "It supports many types of accelerators, including ROCm. All you need to do is install the appropriate version of PyTorch for your hardware.",
              "score": 4,
              "created_utc": "2026-02-14 17:28:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5h2c8f",
          "author": "FPham",
          "text": "This is one project I'm happy to see posted here. I'm just curious, what would happen if we train it on different dataset, not on mlabonne/harmless\\_alpaca and mlabonne/harmful\\_behaviors?\n\nWhat would we do, if we train the good set to be questions on writing stories and  bad being, well, alpaca math and economy questions...",
          "score": 5,
          "created_utc": "2026-02-15 07:38:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5h4y2i",
              "author": "-p-e-w-",
              "text": "I have already demonstrated that slop can be massively reduced with Heretic: https://www.reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/\n\nYou can find a ready-made configuration file for this purpose in the repository.\n\nMany other interesting things are undoubtedly possible.",
              "score": 2,
              "created_utc": "2026-02-15 08:04:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5knuxh",
                  "author": "FPham",
                  "text": "Wow, darn! This was EXACTLY what I was thinking and you already had done it. I'm surprised others are not that excited about this part - brain surgery of models is an area that can create fast results on big models, not to mention that it might be easier to pinpoint the exact thing you want to remove or enhance. (finetuning is more like a paint-over, this is more like rearranging molecular structure of original pain)  \nI wish day has a bit more hours, but I'm totally going to look at the heretic. I had been talking about it in locallama since I tried some models and briefly looked at your repo - and immediately saw this is the real deal not one of the voodoo trust-me-bro projects.  \nSo kudos, and I'd be experimenting. I think people should be more excited about this. It's an untapped area. Everybody calls about experimenting - and this is the tool IMHO.",
                  "score": 2,
                  "created_utc": "2026-02-15 21:11:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5fgwyy",
          "author": "AlwaysLateToThaParty",
          "text": "Thankyou so much. The gpt-oss-120B heretic model is my daily driver.",
          "score": 3,
          "created_utc": "2026-02-15 00:26:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5gcphw",
              "author": "VicemanPro",
              "text": "You prefer it over derestricted?",
              "score": 1,
              "created_utc": "2026-02-15 04:00:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5m0l3t",
                  "author": "AlwaysLateToThaParty",
                  "text": "Yes.",
                  "score": 1,
                  "created_utc": "2026-02-16 01:49:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5dktlk",
          "author": "Much-Researcher6135",
          "text": "Oh hell yeah, we're all heretics on this glorious valentine's day",
          "score": 2,
          "created_utc": "2026-02-14 18:10:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5dqb7r",
          "author": "Lissanro",
          "text": "Does it work on CPU to process large models with vision like Kimi K2.5, if I have 1 TB RAM? I have four 3090 cards, so cannot use VRAM to fit fully the whole model. My understanding that I would still need to quantize to bnb\\_4bit from the original INT4 (with model size a bit larger than 0.5 TB in 4-bit format).",
          "score": 2,
          "created_utc": "2026-02-14 18:36:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fglmq",
          "author": "tmflynnt",
          "text": "Fantastic work, everyone involved!\n\nI am also hugely looking forward to your hinted at upcoming improvements! If they're big enough milestones maybe you can just skip to v2.0 and complete the epic rebrand to **Hexen**! (j/k)",
          "score": 2,
          "created_utc": "2026-02-15 00:24:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5huacj",
          "author": "Claudius_the_II",
          "text": "The bnb_4bit quantized loading is a game changer for accessibility. Being able to process models with 70% less VRAM while still exporting full precision is really clever engineering. The MPOA integration with Optuna optimization on top is nice too.",
          "score": 2,
          "created_utc": "2026-02-15 12:04:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e3x2i",
          "author": "IrisColt",
          "text": "I kneel... Thanks!!!",
          "score": 1,
          "created_utc": "2026-02-14 19:46:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5epkeg",
          "author": "SignificantClub4279",
          "text": "congratulations on the great project. we're living in amazing times where heretics are the good guys.",
          "score": 1,
          "created_utc": "2026-02-14 21:44:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5h1gj",
      "title": "You can run MiniMax-2.5 locally",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/hd369oaucojg1.jpeg",
      "author": "Dear-Success-1441",
      "created_utc": "2026-02-15 15:14:51",
      "score": 360,
      "num_comments": 132,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5jpwa1",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-15 18:20:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5iw2i7",
          "author": "sine120",
          "text": "*cries in 64GB*",
          "score": 83,
          "created_utc": "2026-02-15 15:55:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ixkei",
              "author": "vogelvogelvogelvogel",
              "text": "i can relate (24 vram, 64 ram)",
              "score": 26,
              "created_utc": "2026-02-15 16:02:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5kfe36",
                  "author": "cuberhino",
                  "text": "Youâ€™re me!",
                  "score": 4,
                  "created_utc": "2026-02-15 20:27:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5j2il3",
              "author": "Additional_Ad_7718",
              "text": "64 GB is amazing until it isn't anymore ;(",
              "score": 24,
              "created_utc": "2026-02-15 16:26:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5krgxs",
                  "author": "megacewl",
                  "text": "I thought my 32 GB was future-proofed when I got it 4 years ago...",
                  "score": 7,
                  "created_utc": "2026-02-15 21:29:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5jp4ls",
              "author": "michael_p",
              "text": "I ordered a 64gb Mac Studio and so glad I cancelled and bumped to m3 ultra 96gb. I really may wanna go for 2x 512 gb with how much that would let me do locally",
              "score": 5,
              "created_utc": "2026-02-15 18:16:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5k3dde",
                  "author": "Beginning-Struggle49",
                  "text": "I have the M3 with 96 and I haven't had luck running models locally, at least not as the main agent! God I hope they get em smaller",
                  "score": 1,
                  "created_utc": "2026-02-15 19:25:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5jpbd9",
              "author": "drakgremlin",
              "text": "Cries in 16GB 1L minipc.",
              "score": 1,
              "created_utc": "2026-02-15 18:17:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5m7dc4",
              "author": "Due-Rooster3471",
              "text": "What do you mean by cries? Im new to LLM's, just learning but curious because I have 64 gb of ram in my system with a 5090",
              "score": 1,
              "created_utc": "2026-02-16 02:32:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5j6d1f",
              "author": "yoracale",
              "text": "You can use the 1-bit one which will fit but you're better off running a smaller LLM as MiniMax isn't big enough: https://huggingface.co/unsloth/MiniMax-M2.5-GGUF?show_file_info=MiniMax-M2.5-UD-TQ1_0.gguf",
              "score": 1,
              "created_utc": "2026-02-15 16:45:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5jrl3r",
                  "author": "sine120",
                  "text": "Qwen next is fine, honestly. It fits quite well and I'm not doing anything major on my home PC other than tests.",
                  "score": 3,
                  "created_utc": "2026-02-15 18:28:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5iroma",
          "author": "dampflokfreund",
          "text": "No, I can't.",
          "score": 120,
          "created_utc": "2026-02-15 15:33:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ivb47",
          "author": "Ug1bug1",
          "text": "All the minmaxes have been fast enough with good quality on my strix halo. Q3_K_XL.\n\nI bought 128gb Strix Halo for 1500â‚¬.",
          "score": 42,
          "created_utc": "2026-02-15 15:51:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j2a9q",
              "author": "meinrd",
              "text": "What does \"fast enough\" mean? Looking to buy a strix halo myself, but 100GB Model in a 128gb System seems a little on the edge.",
              "score": 9,
              "created_utc": "2026-02-15 16:25:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5j561f",
                  "author": "muyuu",
                  "text": "it's ok headless\n\nin fact Step 3.5 Flash GGUF is slightly bigger and it runs with full 256k context\n\nI expect this 3-bit quant not to be as good in the 128GB category, but i haven't tried it",
                  "score": 3,
                  "created_utc": "2026-02-15 16:39:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5kkk4o",
                  "author": "Fit-Produce420",
                  "text": "You should run the Strix halo system headless if you want max ram.Â \n\n\nRight now I can utilize 240gb on 2 strix running headless, might be able to get 248gb total, maybe.Â \n\n\nIt's good for maintaining 120k-200k context on MoE models, native 4 bit models run really well.",
                  "score": 3,
                  "created_utc": "2026-02-15 20:54:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5j9irj",
                  "author": "dionisioalcaraz",
                  "text": "I run IQ4\\_XS with 16k context in 128GB, but someone posted running the bigger Qwen3-235B-IQ4\\_XS with 32k context.",
                  "score": 2,
                  "created_utc": "2026-02-15 17:00:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5izkhh",
              "author": "Dr_Allcome",
              "text": ">I bought 128gb Strix Halo for 1500â‚¬.\n\nWhere?",
              "score": 2,
              "created_utc": "2026-02-15 16:12:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5j04mc",
                  "author": "Ug1bug1",
                  "text": "https://www.bosgame.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395-96gb-128gb-2tb?srsltid=AfmBOop64hvYGqHp2MggipjBzQJqy2jVFMetvwc-yhAHoyp9a9Gk0TWD&variant=46726110707875\n\nPrice has gone up since",
                  "score": 5,
                  "created_utc": "2026-02-15 16:15:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5is6g8",
          "author": "--dany--",
          "text": "* read the fine print, you have to be rich first.",
          "score": 59,
          "created_utc": "2026-02-15 15:36:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5iz400",
              "author": "seamonn",
              "text": "idk why everyone forgets to do that first.",
              "score": 28,
              "created_utc": "2026-02-15 16:10:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5kje4m",
                  "author": "EndStorm",
                  "text": "That's the one trick they forget to mention!",
                  "score": 2,
                  "created_utc": "2026-02-15 20:48:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5j0yi4",
              "author": "rerri",
              "text": "3090 + 96GB is enough to play around with the UD-Q3\\_K\\_XL.\n\nJust time travel back to last summer and you'll get those for a combined price of 800-900â‚¬.",
              "score": 23,
              "created_utc": "2026-02-15 16:19:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5j43fq",
                  "author": "seamonn",
                  "text": "> Just time travel back to last summer and you'll get those for a combined price\n\nikr, very simple.  \n  \nOh you want cheap hardware? Just invent a Time Machine. Problem Solved.",
                  "score": 9,
                  "created_utc": "2026-02-15 16:34:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ko4i5",
                  "author": "Particular-Way7271",
                  "text": "You might have some good prices even in the future like year 2070 for 96GB kits",
                  "score": 3,
                  "created_utc": "2026-02-15 21:12:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5jgm27",
                  "author": "VoidAlchemy",
                  "text": "If you have CUDA, definitely check with ik\\_llama.cpp quants for the best perplexity for a given memory footprint. I've measured the difference in perplexity: [https://huggingface.co/ubergarm/MiniMax-M2.5-GGUF#quant-collection](https://huggingface.co/ubergarm/MiniMax-M2.5-GGUF#quant-collection)",
                  "score": 2,
                  "created_utc": "2026-02-15 17:35:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5j95l4",
              "author": "arvigeus",
              "text": "I am going to be rich after my vibe coded app becomes successful. Just wait and see.\n\n(In other words I wish you to be immortal)",
              "score": 3,
              "created_utc": "2026-02-15 16:58:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5k001k",
                  "author": "--dany--",
                  "text": "The grim reality of ai arm race is, those richer will vibe code earlier, faster, better and more likely to succeed when working on the same idea.",
                  "score": 3,
                  "created_utc": "2026-02-15 19:09:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5j2wkp",
          "author": "dionisioalcaraz",
          "text": "No need to add --jinja or --fit on anymore, they are default.",
          "score": 11,
          "created_utc": "2026-02-15 16:28:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lvuco",
              "author": "Mean-Sprinkles3157",
              "text": "Thanks, will remove jinja now",
              "score": 1,
              "created_utc": "2026-02-16 01:18:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5j6ier",
              "author": "yoracale",
              "text": "Oh really? That's amazing !",
              "score": 1,
              "created_utc": "2026-02-15 16:45:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5j8aaj",
                  "author": "dionisioalcaraz",
                  "text": "Yeah the best from llama.cpp since multi GPU support ;)",
                  "score": 4,
                  "created_utc": "2026-02-15 16:54:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5j2c67",
          "author": "assassinofnames",
          "text": "\\> be me\n\n\\> on r/LocalLlama\n\n\\> post says you can run Minimax-2.5 locally\n\n\\> look inside\n\n\\> needs 128 GB VRAM on a Mac\n\nmfw I'm so broke I live on a 8GB M1 MacBook Air",
          "score": 37,
          "created_utc": "2026-02-15 16:25:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j94nr",
              "author": "eternviking",
              "text": ">I live on a 8GB M1 MacBook Air\n\nhttps://preview.redd.it/3y8dj33pvojg1.png?width=226&format=png&auto=webp&s=e3ed40970c0fff1488d7d26f15b1bd9ddffec69f",
              "score": 19,
              "created_utc": "2026-02-15 16:58:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5lezqd",
              "author": "touristtam",
              "text": "I have been sitting there on a 6Gb VRAM GFX for the best part of the last decade because of:\n\n1) Crypto\n2) AI",
              "score": 2,
              "created_utc": "2026-02-15 23:37:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5j4htk",
          "author": "Omnimum",
          "text": "I've been testing an RTX 3090 and 128GB of DDR5 at 5600 MHz for two days.\n\nFrankly, a Step 3.5 Flash iQ4-XS versus Minimax-2.5 in Q3-K-XL UD -> Step 3.5 does much better on tasks that span 64k of context.\n\nMinimax-2.5 in Q3-K-XL remains consistent up to 16k/24k; after that, the quality collapses.\n\nFor tool calls, Step 3.5 does a little better than gpt-oss-120b (oddly, depending on the seeds, gpt-oss-20b does a perfect job in \"thinking medium,\" but \"hard\" it overthinks stupidly).\n\nIf an inference that wobbles between 8 and 10 tok/s and a maximum context of 24k is not a problem, then Minimax-2.5-IQ4-XS slightly outperforms Step 3.5 in iQ4-XS for the same context size.",
          "score": 9,
          "created_utc": "2026-02-15 16:36:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5jgy64",
              "author": "VoidAlchemy",
              "text": "Your rig is perfect for ik\\_llama.cpp quants, guessing that is what you're running?",
              "score": 1,
              "created_utc": "2026-02-15 17:36:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ji9r4",
                  "author": "Omnimum",
                  "text": "Lmstudio windows 11 pro",
                  "score": 1,
                  "created_utc": "2026-02-15 17:43:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5jbhob",
          "author": "SignalStackDev",
          "text": "This is really interesting timing. I've been running a multi-model setup where different tasks route to different models, and the biggest lesson is that no single model wins at everything â€” so having local options alongside API models is huge for reliability and cost.\n\nWhat I've found in practice: cheaper/faster models handle routine stuff fine (scanning feeds, simple classification, data extraction). You only really need the heavy hitters for tasks where nuance matters â€” writing that needs to sound human, complex multi-step reasoning, or code that has to work first try.\n\nThe sneaky thing with model selection in production though â€” benchmarks barely matter compared to failure modes. Some models silently degrade on long outputs (just stop mid-sentence), others crash on certain coding tasks, and some have latency spikes that wreck any real-time workflow. The only way to know is running them on YOUR actual workloads for a week.\n\nCurious what hardware people are targeting for running MiniMax-2.5 locally. The VRAM requirements for these larger MoE models always end up being the real bottleneck.",
          "score": 4,
          "created_utc": "2026-02-15 17:09:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kdvyq",
          "author": "Psyko38",
          "text": "So no, I can't with my 48GB of RAM + VRAM.",
          "score": 4,
          "created_utc": "2026-02-15 20:19:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jg2ve",
          "author": "VoidAlchemy",
          "text": "https://preview.redd.it/id5oi3vk1pjg1.png?width=2069&format=png&auto=webp&s=da57776a285753c84cf512f934a252088c30bc75\n\nThe perplexity on that Q3\\_K\\_XL is pretty bad compared to other available quantizations in similar or smaller sizes. Though for some backends e.g. vulkan you might not be able to take advantage of the latest SOTA quants for ik\\_llama.cpp. Keep an eye on [https://huggingface.co/AesSedai/MiniMax-M2.5-GGUF](https://huggingface.co/AesSedai/MiniMax-M2.5-GGUF) as well has he has high quality mainline llama.cpp mixes using similar recipes as my ik recipes.\n\nFor mac users here are some real world results and commands using my quants: [https://huggingface.co/ubergarm/MiniMax-M2.5-GGUF/discussions/6#6990d5e0aa47004a47c70cb1](https://huggingface.co/ubergarm/MiniMax-M2.5-GGUF/discussions/6#6990d5e0aa47004a47c70cb1)\n\nCheers!",
          "score": 10,
          "created_utc": "2026-02-15 17:32:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kn0kx",
              "author": "Look_0ver_There",
              "text": "Would you happen to have results for the IQ3\\_XXS quantization ( [https://huggingface.co/unsloth/MiniMax-M2.5-GGUF](https://huggingface.co/unsloth/MiniMax-M2.5-GGUF) ) ?  It would be really interesting to see how well that holds up in contrast to unsloth's Q3\\_K\\_XL.  I just spent a good hour with IQ3\\_XXS of MiniMax-M2.5, and for coding (in C), conversation of various philosophical topics, and creativity, I wasn't able to detect it doing anything weird at all, and in fact it was performing better than Qwen Coder Next at Q8\\_0 for the coding questions I was asking of it.",
              "score": 2,
              "created_utc": "2026-02-15 21:06:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5kz987",
                  "author": "VoidAlchemy",
                  "text": "The UD-IQ3\\_XXS has higher \"worse\" perplexity than the UD-Q3\\_K\\_XL\n\nhttps://preview.redd.it/889yq3a7fqjg1.png?width=2069&format=png&auto=webp&s=5cd6e504577a2069d4a3211e21338dd4f39b4020\n\n  \n",
                  "score": 3,
                  "created_utc": "2026-02-15 22:09:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5jh476",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 0,
              "created_utc": "2026-02-15 17:37:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5jigqi",
                  "author": "VoidAlchemy",
                  "text": "It clearly says \\`UD-Q3\\_K\\_XL\\`. I downloaded it like so: hf download --local-dir ./MiniMax-M2.5-GGUF --include=UD-Q3\\_K\\_XL/\\*.gguf unsloth/MiniMax-M2.5-GGUF",
                  "score": 2,
                  "created_utc": "2026-02-15 17:44:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5j5qoy",
          "author": "Tank_Gloomy",
          "text": "\\> step 1: have a datacenter at home",
          "score": 7,
          "created_utc": "2026-02-15 16:42:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5jpso8",
              "author": "TheTerrasque",
              "text": "https://tenor.com/view/why-didnt-i-think-of-that-gif-27711265",
              "score": 2,
              "created_utc": "2026-02-15 18:19:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5jl39e",
          "author": "Every-Comment5473",
          "text": "How does Qwen3 Coder Next 80b compare to MiniMax 2.5 when trying to run both at 96GB VRAM",
          "score": 3,
          "created_utc": "2026-02-15 17:56:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5jsser",
              "author": "alexeiz",
              "text": "You can run Q8 of qwen3-coder-next in 96GB, but only Q3 of minimax-2.5.  I personally prefer qwen3-coder-next.",
              "score": 5,
              "created_utc": "2026-02-15 18:34:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5lhcmx",
              "author": "Clank75",
              "text": "If you're using Llama.cpp, right now Minimax is a better bet - Qwen3-Coder-Next is all but useless for anything beyond regurgitating flappy-bird games because of the broken tool calling/looping behaviour.\n\n\nMinimax is actually pretty good, and doesn't seem too compromised even down at the mxfp4 quant I need to get decent performance out of it (Q3CN I han run at 8).Â  Once Qwen's tool calling issues are fixed, I'll probably go back and give it another go though.",
              "score": 1,
              "created_utc": "2026-02-15 23:51:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5jty6d",
          "author": "lakimens",
          "text": "Calling it SOTA is a bit far-fetched, no?",
          "score": 3,
          "created_utc": "2026-02-15 18:39:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lca2o",
              "author": "sagiroth",
              "text": "Its not far off",
              "score": 1,
              "created_utc": "2026-02-15 23:21:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kfhp7",
          "author": "davew111",
          "text": "I miss 70B models.",
          "score": 3,
          "created_utc": "2026-02-15 20:27:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l2zgf",
          "author": "artisticMink",
          "text": "Tried Q3\\_K\\_XL yesterday - excellent quality,",
          "score": 3,
          "created_utc": "2026-02-15 22:29:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j0uac",
          "author": "Sendery-Lutson",
          "text": "No, I can't and probably less than 0.001 % of my country neither can",
          "score": 4,
          "created_utc": "2026-02-15 16:18:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kpo4l",
              "author": "Particular-Way7271",
              "text": "ðŸ˜‚",
              "score": 1,
              "created_utc": "2026-02-15 21:20:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ivzcw",
          "author": "34574rd",
          "text": "how does 16gb vram and 96gb ram work?",
          "score": 2,
          "created_utc": "2026-02-15 15:54:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5izeqz",
              "author": "rerri",
              "text": "Try it. You have enough memory to run UD-Q3\\_K\\_XL.\n\nOn a 4090 + Ryzen 7600X + 96GB DDR5-6000 I was getting something like 15t/s. Switched to a 5090 and \\~19t/s.\n\nNot great speed for a thinking model, but enough to play around. And luckily it doesn't think for ages.",
              "score": 2,
              "created_utc": "2026-02-15 16:11:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5j1lbb",
                  "author": "ParthProLegend",
                  "text": "Definitely not worth switching that GPU",
                  "score": 3,
                  "created_utc": "2026-02-15 16:22:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5j1vn2",
                  "author": "Zyj",
                  "text": "With Ollama? Or llama.cpp? Or something else",
                  "score": 1,
                  "created_utc": "2026-02-15 16:23:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5lxbqq",
                  "author": "Mean-Sprinkles3157",
                  "text": "I use Q2_K_XL on dgx spark, it is 32 t/s. Itâ€™s pretty good for me so far. Not sure if I need to switch to Q3.",
                  "score": 1,
                  "created_utc": "2026-02-16 01:28:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5iwi2o",
          "author": "joblesspirate",
          "text": "I'm running this at q8_0... Its my favorite so far followed by glm5.",
          "score": 2,
          "created_utc": "2026-02-15 15:57:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j29bx",
              "author": "Zyj",
              "text": "How much room for context?",
              "score": 1,
              "created_utc": "2026-02-15 16:25:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5j94kr",
          "author": "joost00719",
          "text": "I got 128gb ddr4 last year for 150 euros with an rtx 5070. I'm gonna try this out tomorrow.",
          "score": 2,
          "created_utc": "2026-02-15 16:58:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5k6k8r",
          "author": "xjE4644Eyc",
          "text": "I have it running on my Strix Halo 128 gb Q3_K_XL.  Its better than GPT-120-OSS (q8 unsloth) in my tests, faster and its the first locally hosted model that is seriously making me reconsider whether i need subscriptions to the big services.  \n\nI'm not coding and don't need 120k context, 32k is sufficient for what I do (emails, business related document review, limited data analyst etc).\n\nGetting about 28-30 tok/s",
          "score": 2,
          "created_utc": "2026-02-15 19:41:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5kpv94",
              "author": "gayexplosion",
              "text": "With what parameters you get 30t/s in RAM?",
              "score": 1,
              "created_utc": "2026-02-15 21:21:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5lnkxz",
                  "author": "xjE4644Eyc",
                  "text": " MiniMax-M2.5:\n    cmd: |-\n      /usr/local/bin/llama-server \\\n            --model /home/user/AI/models/MiniMax-M2.5-UD-Q3_K_XL-00001-of-00004.gguf \\\n            --host 0.0.0.0 \\\n            --port ${PORT} \\\n            --ctx-size 32000 \\\n            --verbose \\\n            -ngl 999 \\\n            -b 1024 \\\n            --jinja\n\nNothing special.",
                  "score": 1,
                  "created_utc": "2026-02-16 00:28:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5kzb33",
          "author": "AcePilot01",
          "text": "what's sota?  Also, yeah not for my 4090 lmfao.",
          "score": 2,
          "created_utc": "2026-02-15 22:09:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lhbay",
          "author": "honato",
          "text": "I'm not seeing the part where I can run it locally. ",
          "score": 2,
          "created_utc": "2026-02-15 23:51:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j4p6l",
          "author": "muyuu",
          "text": "it's kind of misleading to post those benchmarks there right after talking of a 3-bit quant that won't be remotely the same",
          "score": 7,
          "created_utc": "2026-02-15 16:37:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j6rk2",
              "author": "yoracale",
              "text": "You can run full precision or Q8 which is also clearly mentioned in the guide.",
              "score": 0,
              "created_utc": "2026-02-15 16:47:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5jnv16",
              "author": "JacketHistorical2321",
              "text": "3-bit has been proven to be in the ballpark of 95% of the full model so yes it is remotely the same",
              "score": -3,
              "created_utc": "2026-02-15 18:10:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5js2x1",
                  "author": "VoidAlchemy",
                  "text": "The specific UD-Q3\\_K\\_XL clocks 40% higher perplexity over the baseline bf16 as shown in the chart I posted in another thread here.",
                  "score": 8,
                  "created_utc": "2026-02-15 18:30:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5k1xwq",
          "author": "deepfit",
          "text": "Running MiniMax-M2.5-GGUF UD-Q4\\_K\\_XL on 2x RTX 6000 pro blackwell.  It works great.  From playing around with it for a few hours it is as good as Opus.  Works great with opencode.  I am getting about 50 t/s response without any optimization.",
          "score": 3,
          "created_utc": "2026-02-15 19:18:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j4fyz",
          "author": "CyberAttacked",
          "text": "The only minor inconvenience is that you have to be a multitrillionaire to afford the amount of RAM needed to run it locally",
          "score": 2,
          "created_utc": "2026-02-15 16:35:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5j8r26",
              "author": "StyMaar",
              "text": "Or you bought a 128GB Strix Halo this fall for $1500.",
              "score": 6,
              "created_utc": "2026-02-15 16:56:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ju68b",
                  "author": "fallingdowndizzyvr",
                  "text": "Where did you get a 128GB Strixy for $1500?",
                  "score": 0,
                  "created_utc": "2026-02-15 18:40:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5jer2r",
              "author": "NeuralNexus",
              "text": "or just have an old server hanging around lol",
              "score": 3,
              "created_utc": "2026-02-15 17:25:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5jmfqm",
              "author": "Ryoonya",
              "text": "Plenty of people had ram before the prices went up, this post is for people with hardware.",
              "score": 2,
              "created_utc": "2026-02-15 18:03:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5jo6yw",
              "author": "JacketHistorical2321",
              "text": "Or you've been around long enough to read the writing on the wall and you bought 512 GB of RAM a year and a half ago while everybody here was saying it's pointless to try to run anything on CPU because everybody was focusing on 3090s. When it comes to tech, ignore popular opinion and you'll generally be ahead of the game",
              "score": 2,
              "created_utc": "2026-02-15 18:11:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5izkku",
          "author": "Hitchhiker2TheFuture",
          "text": ">",
          "score": 1,
          "created_utc": "2026-02-15 16:12:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jb3ue",
          "author": "bad_detectiv3",
          "text": "Is this the model that is currently offered for free by opencode Zen ?  \n",
          "score": 1,
          "created_utc": "2026-02-15 17:07:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jbnei",
          "author": "jinnyjuice",
          "text": "Flash or Air version would be amazing",
          "score": 1,
          "created_utc": "2026-02-15 17:10:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jfced",
          "author": "shadow-studio",
          "text": "*cries in rtx3060*",
          "score": 1,
          "created_utc": "2026-02-15 17:28:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jh4nz",
          "author": "Cener47",
          "text": "\"Oh... +96GB....ok...\"",
          "score": 1,
          "created_utc": "2026-02-15 17:37:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jskh6",
          "author": "LevianMcBirdo",
          "text": "Only have 96 gb. Have it running at q2. In my testing it's worse than m2.1 cerebras 172B reap at q3. Hope cerebras makes 2.5 reap",
          "score": 1,
          "created_utc": "2026-02-15 18:33:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jtmqm",
          "author": "Fearless_Roof_4534",
          "text": "Tell them to call me after they revise their definition of \"local\" to 16 GB vram and 32 GB ram",
          "score": 1,
          "created_utc": "2026-02-15 18:38:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5k7ikl",
          "author": "gyhor2",
          "text": "my benchmark with strix halo 128gb (bosgame m5) MiniMax-M2.5-UD-Q3\\_K\\_XL\n\n    llama-bench -m ~/.cache/huggingface/hub/models--unsloth--MiniMax-M2.5-GGUF/unsloth_MiniMax-M2.5-GGUF_UD-Q3_K_XL_MiniMax-M2.5-UD-Q3_K_XL-00001-of-00004.gguf  -ngl 999 -fa 1,0 --mmap 0\n    ggml_cuda_init: found 1 ROCm devices:\n      Device 0: Radeon 8060S Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32\n    | model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n    | minimax-m2 230B.A10B Q3_K - Medium |  94.33 GiB |   228.69 B | ROCm       | 999 |  1 |           pp512 |        195.18 Â± 6.88 |\n    | minimax-m2 230B.A10B Q3_K - Medium |  94.33 GiB |   228.69 B | ROCm       | 999 |  1 |           tg128 |         28.61 Â± 0.02 |\n    | minimax-m2 230B.A10B Q3_K - Medium |  94.33 GiB |   228.69 B | ROCm       | 999 |  0 |           pp512 |        181.13 Â± 0.35 |\n    | minimax-m2 230B.A10B Q3_K - Medium |  94.33 GiB |   228.69 B | ROCm       | 999 |  0 |           tg128 |         26.52 Â± 0.02 |\n\nbuild: 684b36101 (8057)",
          "score": 1,
          "created_utc": "2026-02-15 19:46:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5k80xw",
          "author": "ThatCrankyGuy",
          "text": "When you say \"locally\"...\n\n*eyes the gtx 770 gti with 32gb system memory*... I don't think you mean ghetto shit like mine.",
          "score": 1,
          "created_utc": "2026-02-15 19:49:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5komcm",
          "author": "rezivor",
          "text": "Does everyone here just wish all these llm worked like claude etc? Cause they never do. Are we all just waiting for the day or am I missing something",
          "score": 1,
          "created_utc": "2026-02-15 21:14:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kt98i",
          "author": "idiotiesystemique",
          "text": "\\>Locally  \n\\>101GB  \nyeah ok y'all have datacentres at home i get it ",
          "score": 1,
          "created_utc": "2026-02-15 21:38:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l0xau",
          "author": "gaolbreak",
          "text": "96 gb system ram + 48 gb vram (dual 3090) + 64k context and I got 9 tokens/s. I hoped it'd be more but this is not bad.",
          "score": 1,
          "created_utc": "2026-02-15 22:18:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l1rwl",
          "author": "stacykade",
          "text": "running mistral on my mac studio and it's surprisingly capable for day to day stuff. not replacing the APIs yet but for quick local tasks it's brilliant",
          "score": 1,
          "created_utc": "2026-02-15 22:22:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l27ko",
          "author": "swampfox305",
          "text": "What Mac studio do I need to run this?",
          "score": 1,
          "created_utc": "2026-02-15 22:25:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5l2p79",
          "author": "_hypochonder_",
          "text": "4x AMD MI50s 32GB will do there job.   \nBut I think this model is censored so it's not worth it for my use case. (SillyTavern)",
          "score": 1,
          "created_utc": "2026-02-15 22:27:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ltyef",
          "author": "Murgatroyd314",
          "text": "> You can run MiniMax-2.5 locally\n\nNo I can't.",
          "score": 1,
          "created_utc": "2026-02-16 01:06:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m5r0i",
          "author": "Devil_AE86",
          "text": "Well, guess Iâ€™m saving up for a Mac mini pro or something, minimax actually doesnâ€™t seem to be too bad for stuff in their web deployment",
          "score": 1,
          "created_utc": "2026-02-16 02:22:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5m5vh7",
          "author": "thebadslime",
          "text": "No I cannot sadly.",
          "score": 1,
          "created_utc": "2026-02-16 02:23:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ma0mf",
          "author": "Pro-editor-1105",
          "text": "REAP IT REAP IT CMON",
          "score": 1,
          "created_utc": "2026-02-16 02:49:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5iw4o8",
          "author": "XiRw",
          "text": "Lol",
          "score": 1,
          "created_utc": "2026-02-15 15:55:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j5akp",
          "author": "rm-rf-rm",
          "text": "are you on the unsloth team?",
          "score": 1,
          "created_utc": "2026-02-15 16:39:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5m865n",
              "author": "yoracale",
              "text": "I am but OP is not, no.",
              "score": 1,
              "created_utc": "2026-02-16 02:38:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5mahgy",
                  "author": "rm-rf-rm",
                  "text": "that's what I thought.. Are you ok with them posting like this? Usually you guys do.. When y'all dont put it in this sub and put it in LocalLLM instead, I crosspost here",
                  "score": 1,
                  "created_utc": "2026-02-16 02:52:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5j2xpg",
          "author": "johnnyApplePRNG",
          "text": "Days old...",
          "score": -1,
          "created_utc": "2026-02-15 16:28:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5jucj2",
              "author": "fallingdowndizzyvr",
              "text": "Day old. These quants did pop up until yesterday.",
              "score": 1,
              "created_utc": "2026-02-15 18:41:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r35d2x",
      "title": "MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters",
      "subreddit": "LocalLLaMA",
      "url": "https://openhands.dev/blog/minimax-m2-5-open-weights-models-catch-up-to-claude",
      "author": "Zyj",
      "created_utc": "2026-02-12 21:02:15",
      "score": 354,
      "num_comments": 88,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/",
      "domain": "openhands.dev",
      "is_self": false,
      "comments": [
        {
          "id": "o547byu",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-13 05:35:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51vjhm",
          "author": "jacek2023",
          "text": "Same as before",
          "score": 53,
          "created_utc": "2026-02-12 21:16:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o538v05",
              "author": "noiserr",
              "text": "Which is why it's the best GPU model for the \"GPU poor\". Hope they stick to this size going forward as well.",
              "score": 14,
              "created_utc": "2026-02-13 01:45:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o54dmfi",
                  "author": "Spitfire1900",
                  "text": "Isnâ€™t GLM 4.7 Flash still the best model you can performantly run on a high end consumer GPU?",
                  "score": 3,
                  "created_utc": "2026-02-13 06:26:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o54ycjs",
              "author": "Rent_South",
              "text": "Yep. What really matters is how it performs on actual real world use cases.\n\nNGL, what I've noticed is that, in general actually, the minimax models seem to respond poorly to very specific instructions. What I mean is, if you prompt, lets say something simple like:\n\n\"what is 2+2, only reply the value as an answer, nothing else\"\n\nit goes on a tangent and output all of its reasoning tokens, like:  \n\" I'm a Large language model, the user is asking a simple math question, that question is \\[...\\]\".\n\nWhile it does get the answer 'right' in the end, other models seem to be able to understand the challenge and limit their output tokens drastically.\n\nI've added minimax 2.5 and minimax 2.5 high speed, to my dynamic benchmarking platform, where you can create your own benchmarks, OpenMark AI, if you want to check.",
              "score": 2,
              "created_utc": "2026-02-13 09:36:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o51zkrx",
          "author": "Look_0ver_There",
          "text": "Awesome.  Now we just need the [https://huggingface.co/cerebras](https://huggingface.co/cerebras) team to work their magic and give us a \\~160B REAP/REAM hybrid version with minimal loss.  Then we can quantize that, and we'll end up with something that will run fast for those of us with 128GB machines and enough head-room left over for deep-context tool-use.",
          "score": 58,
          "created_utc": "2026-02-12 21:35:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o528yky",
              "author": "-dysangel-",
              "text": "then if we zip it, it might fit on a CD ROM",
              "score": 63,
              "created_utc": "2026-02-12 22:21:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o52fmnc",
                  "author": "Look_0ver_There",
                  "text": "I know you're joking around, but for a laugh I decided to run a few models through gzip, and was surprised to reduce the sizes by 30%.  If you know anything about information theory and entropy, that basically means that there is a lot of inherent redundancy in these models.  The real trick would be how to achieve that in practice in a way that is accessible to the compute.  There's a few PhD thesis's in that alone, and solving it would bring us closer to having big data center models level of performance accessible at a local level.",
                  "score": 31,
                  "created_utc": "2026-02-12 22:56:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o52aarg",
                  "author": "victoryposition",
                  "text": "You can get it a tiny bit smaller if you ARJ the zip file.",
                  "score": 10,
                  "created_utc": "2026-02-12 22:27:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o53nemk",
                  "author": "CarelessOrdinary5480",
                  "text": "If it doesn't we keep zipping it over and over and praying to zipgod.",
                  "score": 2,
                  "created_utc": "2026-02-13 03:15:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o533iww",
              "author": "TokenRingAI",
              "text": "REAP it to 160B, then REAM it to 100B, then QUANT it to 1 bit, so it can run on a potato",
              "score": 7,
              "created_utc": "2026-02-13 01:12:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o533sez",
                  "author": "Look_0ver_There",
                  "text": "...and package it like an oversized 8 ball with a little window in it too?",
                  "score": 3,
                  "created_utc": "2026-02-13 01:14:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o53o1nv",
                  "author": "CarelessOrdinary5480",
                  "text": "If it doesn't make a really annoying sound every prompt like the dumb and dumber eeeeeeeeeeeeeeeeeeeeeeeeeee we won't be happy.",
                  "score": 2,
                  "created_utc": "2026-02-13 03:19:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5295m4",
              "author": "__Maximum__",
              "text": "Can we REAM it to 10B? Pretty please?",
              "score": 6,
              "created_utc": "2026-02-12 22:22:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o52bhb1",
                  "author": "Toooooool",
                  "text": "I'm genuinely surprised we haven't seen some crazy REAP / REAMS yet.  \n50% is cute but I wanna see what happens if we chop 90% off this thing.  \n230B to 23B, cram it into a 3090, be it a lobotomy or not I just want to see it.",
                  "score": 4,
                  "created_utc": "2026-02-12 22:34:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o52aqfq",
              "author": "SlowFail2433",
              "text": "Is a rly good REAP candidate yeah",
              "score": 1,
              "created_utc": "2026-02-12 22:30:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o52ekaa",
                  "author": "Look_0ver_There",
                  "text": "I downloaded the 139B cerebus based REAP of MiniMax 2.1, and quantized that to fit, and it performs really well. There's also a 172B REAP variant of MiniMax 2.1, and that one I had to quantize a little too hard just to make it fit.  This is why I mentioned a 160B REAP version.  If someone manages to pull that off using Cerebus's algorithms, then I'm fairly confident we would have something pretty amazing that comes in at ~85-90GB. \n\nAt least, that's my dream based upon what I saw from the 2.1 versions I was playing with",
                  "score": 2,
                  "created_utc": "2026-02-12 22:50:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o526u0g",
          "author": "ComprehensiveJury509",
          "text": "This appears to be an unbelievably smart model for its size. Incredible achievement.",
          "score": 42,
          "created_utc": "2026-02-12 22:10:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52c1vh",
              "author": "Xisrr1",
              "text": "Yeah, I'm starting to believe the benchmarks are accurate.",
              "score": 7,
              "created_utc": "2026-02-12 22:36:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o532xcu",
                  "author": "__JockY__",
                  "text": "Iâ€™ve been running M2.1 and 2.0 before that and theyâ€™re both bangers that work with Claude code cli really well. Hoping for the same from 2.5z",
                  "score": 1,
                  "created_utc": "2026-02-13 01:09:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o53o9qp",
              "author": "CarelessOrdinary5480",
              "text": "I think it has a hard time with longer context, but yea I used it to a bug sweep of a mid sized repo and it did an OK job.  it caught a lot of problems, but it did a pretty shallow sweep.  I think the best use for this bad ass mofo will be as an agent to an orchestrator.",
              "score": 3,
              "created_utc": "2026-02-13 03:21:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o51vr7w",
          "author": "eviloni",
          "text": "So with only 10b active parameters, it should get decent (that word doing a lot of heavy lifting)  speed with not radical GPU?\n\nand that's before quantized versions?",
          "score": 16,
          "created_utc": "2026-02-12 21:17:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o524zvc",
              "author": "FullstackSensei",
              "text": "Q4_M on six 32GB Mi50s and vanilla llama.cpp starts at ~15t/s with a few k context and goes down to ~4.5t/s at 150k context.\n\nOn eight P40s using ik_llama.cpp with -sm graph, also starts at ~15t/s with a few k context. Tested only to ~50k context, at which point it does ~12t/s. On vanilla llama.cpp I get ~8t/s with a few k context.\n\nOn both machines, the cards are limited to 170W. On the Mi50s only one card is going \"full blast\" at any given moment. On the P40s with ik_llama.cpp, all cards are going at the same time but at ~80W each. Haven't measured power at the wall, but I'd say the P40s with ik consume about 3x the power vs Mi50. Then again, the P40s are about half the price of the Mi50 now.",
              "score": 8,
              "created_utc": "2026-02-12 22:01:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o52abq0",
                  "author": "eviloni",
                  "text": "I mean a lot of people would accept 15t/s to get unlimited sonnet usage locally. There's a lot of use cases for that kind of thing.",
                  "score": 17,
                  "created_utc": "2026-02-12 22:28:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o525isu",
              "author": "PrefersAwkward",
              "text": "Kind of. You still have to fit the whole 230B in memory somewhere, which consumer GPUs wouldn't be able to do without some heavy quantization or by having multiple active GPUs.\n\nIn conventional MoE, each token gets to use its own 10B of experts, which means that the full 230B is potentially/effectively \"active\" or \"necessary\" for the purposes of any workload. In other words, you can't just have it use a particular 10B to perform a certain task.\n\nBut you still benefit from having just 10B active per token as that's WAY faster than having the whole 230B active per token.\n\nSo even though an individual token only needs a particular 10b, the next token might need some other set of 10b parameters. You can't just put the 10b on your GPU as a result and call it a day.\n\nBut a decent modern GPU can still speed things up here and you can get decent speed, as long as you have the System memory and CPU to things that the GPU cannot fit.",
              "score": 1,
              "created_utc": "2026-02-12 22:03:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o51xk3m",
              "author": "Zyj",
              "text": "Quantization doesn't change the number of parameters.",
              "score": 1,
              "created_utc": "2026-02-12 21:25:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o51ye66",
                  "author": "Look_0ver_There",
                  "text": "but quantization DOES reduce the amount of memory that needs to move about.  On memory bandwidth limited implementations (ie. basically everything), then this results in faster token generation.",
                  "score": 17,
                  "created_utc": "2026-02-12 21:29:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o520ene",
          "author": "rorowhat",
          "text": "Hmm I always thought miniMax was ginormous, not that bad",
          "score": 7,
          "created_utc": "2026-02-12 21:39:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5260yv",
              "author": "Zc5Gwu",
              "text": "You can run it in 128gb at Q3 itâ€™s great.",
              "score": 18,
              "created_utc": "2026-02-12 22:06:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o528y3i",
                  "author": "fixingmybike",
                  "text": "256gb mac studio looking more okay-ish with every new minimax",
                  "score": 11,
                  "created_utc": "2026-02-12 22:21:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o52bw0l",
                  "author": "LagOps91",
                  "text": "with 24gb vram gpu Q4 fits as well and runs decently fast. with 16gb gpu it might fit with some squeezing as well.",
                  "score": -3,
                  "created_utc": "2026-02-12 22:36:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o521ex5",
          "author": "Rascazzione",
          "text": "I really think it's incredible what Moonshot has achieved with this model and this number of parameters. Let's remember that GLM has had to double the parameters of its model in order to continue evolving, and that Kimi is 1T. If the quality and size are confirmed, it's a huge HIT, folks!",
          "score": 18,
          "created_utc": "2026-02-12 21:44:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52jkfa",
              "author": "mineyevfan",
              "text": "s/Moonshot/Minimax/",
              "score": 22,
              "created_utc": "2026-02-12 23:17:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o52m30o",
                  "author": "Accomplished_Ad9530",
                  "text": "Glad someone sed it",
                  "score": 21,
                  "created_utc": "2026-02-12 23:31:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o52jkgk",
              "author": "cheechw",
              "text": "Different company. Minimax makes Minimax, Moonshot makes Kimi.",
              "score": 10,
              "created_utc": "2026-02-12 23:17:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55vjdy",
                  "author": "Rascazzione",
                  "text": "YAR My brain is saturated with so many names :P",
                  "score": 2,
                  "created_utc": "2026-02-13 13:46:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o53o1lo",
          "author": "bjp99",
          "text": "Excited for this. Really like Minimax for a daily driver. I get about 100 tok/s with AWQ quant on 2x rtx pro 6000s with vLLM.  Q2 quant on 4 3090 ti gets 17 tok/s using llama cpp.",
          "score": 3,
          "created_utc": "2026-02-13 03:19:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54g47x",
              "author": "Zyj",
              "text": "Same here. Iâ€˜m using it Q6 on 2x Strix Halo",
              "score": 2,
              "created_utc": "2026-02-13 06:47:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o55pdqa",
                  "author": "MrBIMC",
                  "text": "vllm or llama.cpp? Rocm or Vulkan? What tps are you getting?\n\nAre you running headless and what's your ram\\vram split?",
                  "score": 1,
                  "created_utc": "2026-02-13 13:11:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o52burf",
          "author": "Thrumpwart",
          "text": "Wen MLX?",
          "score": 4,
          "created_utc": "2026-02-12 22:35:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o537831",
          "author": "flavio_geo",
          "text": "If the parameters are true (would make sense to have the same size as the previous m2.1), and the benchmarks are true. That would be a fantastic local model to run.\n\nI have been running m2.1 in Unsloth UD Q3\\_K\\_XL with 12 tokens/s in a single XTX 7900 24GB VRAM + Ryzen 7 9700X 2x48GB RAM\n\nIts not fast, but its enough to get things done. Lets hope all that is true and Unsloth get us our special quants =)",
          "score": 2,
          "created_utc": "2026-02-13 01:35:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53cvfr",
          "author": "Koalababies",
          "text": "Well. This makes me unbelievably excited",
          "score": 1,
          "created_utc": "2026-02-13 02:10:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53fcfu",
          "author": "Dense-Sir-6707",
          "text": "that's before quantized versions?",
          "score": 1,
          "created_utc": "2026-02-13 02:25:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54lb1t",
              "author": "AlbeHxT9",
              "text": "quantized versions have the same numbers of total/active parameters",
              "score": 2,
              "created_utc": "2026-02-13 07:34:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55ao57",
          "author": "Steus_au",
          "text": "I would admit it is onpair with sonnet. for noncoding task. needs a good prompt but they all do.\n\n",
          "score": 1,
          "created_utc": "2026-02-13 11:27:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55xcwc",
          "author": "Septerium",
          "text": "Great news! Minimax 2.1 is the first local model I tested that is reasonably reliable for professional agentic coding. I get great results with unsloth's Q5\\_K\\_XL. Can't wait to try the new version!",
          "score": 1,
          "created_utc": "2026-02-13 13:55:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53a1nz",
          "author": "Tall-Peak2618",
          "text": "Â Incredible achievement.",
          "score": 0,
          "created_utc": "2026-02-13 01:53:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51tbpw",
          "author": "maglat",
          "text": "Amazing! My openclaw helper cant wait to switch from M2.1 to M2.5. Sadly still need to wait for the weights on huggingface",
          "score": -12,
          "created_utc": "2026-02-12 21:05:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51twz3",
              "author": "[deleted]",
              "text": "[removed]",
              "score": -3,
              "created_utc": "2026-02-12 21:08:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o51ud65",
                  "author": "maglat",
                  "text": "Yes locally. Yes I had this issue 3 or 4 times. but since a while it didnâ€™t happen anymore. My foggy brain cant remember if I â€žfixedâ€œ something or not. Sorry.",
                  "score": -3,
                  "created_utc": "2026-02-12 21:10:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r3zuuf",
      "title": "GPT-OSS 120b Uncensored Aggressive Release (MXFP4 GGUF)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/",
      "author": "hauhau901",
      "created_utc": "2026-02-13 20:15:33",
      "score": 350,
      "num_comments": 28,
      "upvote_ratio": 0.9,
      "text": "Hey everyone, made an uncensored version of GPT-OSS 120B.\n\n\n\n  Quick specs: 117B total params, \\~5.1B active (MoE with 128 experts, top-4 routing), 128K context. MXFP4 is the model's native precision - this isn't a quantization, it's how it was trained. No overall quality loss, though you can see CoT behave differently at times.\n\n\n\n  This is the aggressive variant - **observed 0 refusals to any query during testing.**\n\n  **Completely uncensored while keeping full model capabilities intact.**\n\n\n\n  Link: [https://huggingface.co/HauhauCS/GPTOSS-120B-Uncensored-HauhauCS-Aggressive](https://huggingface.co/HauhauCS/GPTOSS-120B-Uncensored-HauhauCS-Aggressive)\n\n\n\n  Sampling settings:\n\n  \\- --temp 1.0 --top-k 40\n\n  \\- Disable everything else (top\\_p, min\\_p, repeat penalty, etc.) - some clients turn\n\n  these on by default\n\n  \\- llama.cpp users: --jinja is required for the Harmony response format or the model won't work right\n\n  \\- Example: llama-server -m model.gguf --jinja -fa -b 2048 -ub 2048\n\n\n\n  Single 61GB file. Fits on one H100. For lower VRAM, use --n-cpu-moe N in llama.cpp to offload MoE layers to CPU.\n\n\n\n  Works with llama.cpp, LM Studio, Ollama, etc.\n\n\n\n  If you want smaller models, I also have GPT-OSS 20B, GLM 4.7 Flash and Qwen3 8b VL uncensored:\n\n  \\- [https://huggingface.co/HauhauCS/models/](https://huggingface.co/HauhauCS/models/)\n\n\n\n  As with all my releases, the goal is effectively lossless uncensoring - no dataset changes and no capability loss.\n\n",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o588c3q",
          "author": "FPham",
          "text": "You might look at [https://github.com/p-e-w/heretic](https://github.com/p-e-w/heretic) this works probably the best, the lowest KL divergence and it is fully automatic.   \nAnd saying \"**full model capabilities intact\"** in 2026 without actually doing any measurement is not good enough. It's not llama-2 world anymore.",
          "score": 209,
          "created_utc": "2026-02-13 20:42:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bd65p",
              "author": "IrisColt",
              "text": "This should be the most upvoted comment.",
              "score": 2,
              "created_utc": "2026-02-14 09:33:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o58521r",
          "author": "RedParaglider",
          "text": "What's the difference between this and derestricted by ariai? I run that on GLM and GPT.",
          "score": 71,
          "created_utc": "2026-02-13 20:25:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5atpxs",
              "author": "GoranjeWasHere",
              "text": "imo the best one is heretic very minimal refusals but keeps inteligence and it is uber fast like 200t/s on my 5090 via lm studio.",
              "score": 18,
              "created_utc": "2026-02-14 06:28:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5bnrna",
                  "author": "rerri",
                  "text": "200t/s on a single 5090? You must be speaking of the 20B model, not the 120B this post is about, right?",
                  "score": 16,
                  "created_utc": "2026-02-14 11:16:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5an2ks",
              "author": "the_bollo",
              "text": "Oh cool another thing to search for... I always look for uncensored or abliterated.",
              "score": 4,
              "created_utc": "2026-02-14 05:31:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o58czof",
          "author": "MustBeSomethingThere",
          "text": "\\>\"As with all my releases, the goal is effectively lossless uncensoring - no dataset changes and no capability loss.\"\n\nBig claims, but no actual measurements. No methology.",
          "score": 86,
          "created_utc": "2026-02-13 21:05:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58ri00",
              "author": "highdimensionaldata",
              "text": "The only metric now is vibes.",
              "score": 46,
              "created_utc": "2026-02-13 22:17:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o59spp5",
                  "author": "giblesnot",
                  "text": "The vibes are honestly as useful as the benchmarks...",
                  "score": 11,
                  "created_utc": "2026-02-14 01:58:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o588ams",
          "author": "Dramatic-Rub-7654",
          "text": "What is the difference between this and the technique https://github.com/p-e-w/heretic? ? Does yours preserve 100% of the tool calls?",
          "score": 31,
          "created_utc": "2026-02-13 20:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58df7l",
              "author": "hauhau901",
              "text": "Hello. Nothing has been changed in that regard so all tool calling capabilities should be maintained. Although the Heretic project is fantastic, my uncensoring has different approaches for different architectures. Results tend to be roughly 10% of the KLD a Heretic abliteration would be for less refusals (or complete removal of).",
              "score": 3,
              "created_utc": "2026-02-13 21:08:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o58hkn6",
                  "author": "Lissanro",
                  "text": "But in what way it is better than [https://huggingface.co/ArliAI/gpt-oss-120b-Derestricted](https://huggingface.co/ArliAI/gpt-oss-120b-Derestricted) ? There multiple issues in your posts:\n\n\\- No information how you tested for refusals - so I cannot compare to Derestricted or Heretic version, maybe they also would have zero refusals on your set of queries.\n\n\\- No documentation what exactly did you do to decensor and how to reproduce\n\n\\- No benchmarks of your model vs the original vs other popular decensored variants (currently Heretic and Derestricted are the most popular I think, both well documented and reproducible). You not necessary have to run all the benchmarks, but at least one or two would be nice. Without that you cannot claim that the original model capabilities were preserved if you did not measure them.",
                  "score": 65,
                  "created_utc": "2026-02-13 21:28:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o58hiqg",
                  "author": "Witty_Mycologist_995",
                  "text": "Have you actually benched the results",
                  "score": 20,
                  "created_utc": "2026-02-13 21:28:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5891dt",
          "author": "ethertype",
          "text": "'full model capabilities' is great. \n\nBut how about quality loss? Or changes to performance?\nDid you measure that in any way/shape/form?\n\nNot trying to shit on your work. It is just that some fine print is missing from the label.",
          "score": 13,
          "created_utc": "2026-02-13 20:46:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o587tnx",
          "author": "LegioTertiaDcmaGmna",
          "text": "Did you also release safetensor shards or only the gguf?",
          "score": 6,
          "created_utc": "2026-02-13 20:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bda2u",
              "author": "IrisColt",
              "text": "heh, good one",
              "score": -5,
              "created_utc": "2026-02-14 09:34:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5b3gwm",
          "author": "cleverusernametry",
          "text": "I believe you totally",
          "score": 5,
          "created_utc": "2026-02-14 07:58:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bati1",
              "author": "seppe0815",
              "text": "xD",
              "score": 4,
              "created_utc": "2026-02-14 09:10:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5cafrp",
          "author": "VagabondTruffle",
          "text": "Bold, untested claims with no evidence supporting them? I think youâ€™d be great in a CEO role at our AI firm. If your net worth is over ten million USD reach out! If itâ€™s not, seems the BSing needs more work to be useful. Lmk!",
          "score": 5,
          "created_utc": "2026-02-14 14:06:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o584wy4",
          "author": "LegioTertiaDcmaGmna",
          "text": "Where did you learn the process for training a new model? Did you write custom torch code from scratch?",
          "score": 5,
          "created_utc": "2026-02-13 20:25:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o585fh5",
              "author": "jslominski",
              "text": "[https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(120B)\\_A100-Fine-tuning.ipynb](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(120B)_A100-Fine-tuning.ipynb)",
              "score": 4,
              "created_utc": "2026-02-13 20:27:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o586ser",
                  "author": "LegioTertiaDcmaGmna",
                  "text": "What hardware did you use for the training?\n\n\n[Edit: ah, you're using other people's computers]",
                  "score": 2,
                  "created_utc": "2026-02-13 20:34:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5aml54",
          "author": "FaustAg",
          "text": "I'm working on a new abliteration combo technique but it takes many, many hours. hoping to release a few models in the next few weeks",
          "score": 2,
          "created_utc": "2026-02-14 05:27:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5aw19t",
          "author": "Taco1595",
          "text": "What version should I run with 12gb vram and 16gb ram",
          "score": 2,
          "created_utc": "2026-02-14 06:49:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jqw5v",
          "author": "BigBlueCeiling",
          "text": "I tried it out locally - both the 120b and the smaller 20b. No outright refusals, plenty of steering the subject into academic, hypothetical, etc., framings which is fine for most things since at least you don't get \"I can't help with that.\"  \n  \nIt's hard for me to say if it maintained capabilities and was effectively lossless - GPT-OSS can be kinda meh to begin with so I think you'd need a fairly large formal test suite to validate that.",
          "score": 1,
          "created_utc": "2026-02-15 18:24:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58cnmc",
          "author": "Arunax_",
          "text": "Will it run on a base M4 16GB? Or a 3060 12GB?",
          "score": 1,
          "created_utc": "2026-02-13 21:04:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58l2og",
              "author": "hauhau901",
              "text": "Hello, unfortunately not with those specs. Feel free to try out the other (smaller) models I've released, OSS-20B, Qwen3 8b VL (vision capabilities) and depending on the RAM on your device with the 3060, GLM 4.7 Flash.",
              "score": 3,
              "created_utc": "2026-02-13 21:45:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r2ygac",
      "title": "Why do we allow \"un-local\" content",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r2ygac/why_do_we_allow_unlocal_content/",
      "author": "JacketHistorical2321",
      "created_utc": "2026-02-12 16:45:57",
      "score": 326,
      "num_comments": 110,
      "upvote_ratio": 0.84,
      "text": "Title somewhat says it all. I get that it's related but if links to new models are being discussed shouldn't it be a requirement that there be a \"local\" component?\n\nEdit: since this is starting to get some traction I want to be a little more specific with what I'm talking about. \n\nIn the past 2 to 3 days we've seen multiple posts related to new models being released. They include links to API resources prior to weights being released. \n\nI believe that if a post includes a link to API serving hosts then it should be requirement that a hugging face link is also included. If both of these requirements cannot be met for any reason (ex. Weights will probably be released but have not been released yet) the post should be taken down. \n\nThis would at least put some guardrails in place that would make sure posts are closer to the true nature of this sub as opposed to being low-key marketing.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r2ygac/why_do_we_allow_unlocal_content/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o52ltqr",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-12 23:30:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50qje9",
          "author": "profcuck",
          "text": "I'd say as long as \"weights will probably be released but have not been released yet\" is a valid reason to allow the post (I think that's what you were saying) then your proposed framework is good.\n\nJust a quick list of some of the types of posts:\n\n1.  Purely local stuff - running this model on that hardware, how-to, questions, benchmarks, etc.  Great stuff it's the core of what we are here for.\n\n2.  Non-local models from companies who are doing a lot of local releases as well: posts like this are fine but should tie it back to local.\n\n3.  Genuine breakthroughs (published) that are only being applied to proprietary cloud models today but seem to have a likely relevance for local models a few years from now.  So for example, if there's a news story that a mathematical breakthrough is allowing OpenAI to run inference with 1/10th the hardware as before, well, the potential implications for local in a few years time seems pretty clear.\n\n4.  News about GPUs/memory etc, if made relevant to local can be ok.\n\nThere's probably more but I guess the overall framework is: pure local are the gold standard, and anything else needs to be clearly tied back to local in a real way, even if the model or whatever isn't available to us right now.",
          "score": 110,
          "created_utc": "2026-02-12 18:02:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51o019",
              "author": "PANIC_EXCEPTION",
              "text": "Ehhh idk about \"a few years\", maybe a couple months with the current pace",
              "score": 21,
              "created_utc": "2026-02-12 20:40:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50v3ff",
              "author": "JacketHistorical2321",
              "text": "I agree with what you say about \"eventually will be released\". My only caveat to that would be until they are available, no other links can into a post.",
              "score": 10,
              "created_utc": "2026-02-12 18:23:19",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o52nohe",
              "author": "RoomyRoots",
              "text": "Is it though? There are loads of subs for AI, keep this only for things everyone can use.",
              "score": 1,
              "created_utc": "2026-02-12 23:40:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50iaup",
          "author": "rm-rf-rm",
          "text": "We've discussed this at length among the mod team and various such threads here in the past. There's an established consensus that:\n\n- Things that are adjacent or germane to the local ecosystem are of value and thus allowed. Of course, the intent and the angle should be towards the spirit of local.\n- Its a very continuous spectrum with no hard black/white boundary. GLM5 might be local to some but not to others. Minimax M2.5 announcement comes ahead of weights release - does it mean we remove the announcement posts as its not \"local\" yet despite it might be in a few hours? (Exactly like what happened to GLM 5 yesterday)\n- If we kept things strict to the original intention of the sub, it'd be dead by now as Llama is all but dead. So its important to stay true to the spirit rather than the letter.\n\nI feel generally we are keeping true to the spirit of the sub. If you find any post we missed, please do report and we'll take action",
          "score": 105,
          "created_utc": "2026-02-12 17:23:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50z9ow",
              "author": "Accomplished_Ad9530",
              "text": "I think it'd go a long way to change the \"New Model\" flair to \"New Model Weights\" and all the other stuff can be posted under \"News\" or \"Discussion\" or something. That'd help shape posts and curb engagement farming since \"New Model\" is the pinnacle of engagement.",
              "score": 19,
              "created_utc": "2026-02-12 18:42:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o510xrp",
              "author": "Charuru",
              "text": "IMO you have the right balance don't listen to whiners.",
              "score": 30,
              "created_utc": "2026-02-12 18:50:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50meeb",
              "author": "ThisGonBHard",
              "text": "I feel like the current way is fair. \n\nBleeding edge closed stuff is the ruler that everyone measures by, and is directly relevant to local stuff as the highest gear the tech can get.\n\nNon bleeding edge models but closed models, from companies with a history of open models is also directly relevant, as there is a high chance they will either release that model, or a similar one.\n\n\"Local\" is also subjective, someone using a 16 GB potato PC and a AI dev doing it for a job with 8x B200 will have different ideas of local.\n\n  \n",
              "score": 24,
              "created_utc": "2026-02-12 17:43:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50tkfe",
                  "author": "JacketHistorical2321",
                  "text": "I don't believe local is subjective. Are the open weights made available? That's local. Whether or not somebody can run it based on hardware doesn't change that fact. It's pretty black and white",
                  "score": 16,
                  "created_utc": "2026-02-12 18:16:14",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50lxws",
              "author": "DinoAmino",
              "text": "The semantic argument about llama is unfortunate. When the sub started llama was basically the one and only open weight LLM worth running, but Mistral and DeepSeek quickly came after. Still, the true spirit of this place has always been DIY running LLMs locally.",
              "score": 12,
              "created_utc": "2026-02-12 17:40:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50tov5",
                  "author": "Accomplished_Ad9530",
                  "text": "Agreed. [LLaMA is apparently a backronym](https://en.wikipedia.org/wiki/Llama_(language_model)) that stands for \"Large Language Model Meta AI\" anyway, so let's just drop the \"Meta\" and move on. The incontrovertible part of the sub is \"Local\" and it's bizarre that people dispute that.",
                  "score": 12,
                  "created_utc": "2026-02-12 18:16:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o52dljo",
              "author": "Antique_Juggernaut_7",
              "text": "Great stuff!",
              "score": 2,
              "created_utc": "2026-02-12 22:45:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o531aw7",
              "author": "Front-Relief473",
              "text": "to add a label before publishing, such as: open source, closed source, so that you can see the post at a glance?",
              "score": 2,
              "created_utc": "2026-02-13 00:59:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o54on26",
              "author": "ayylmaonade",
              "text": "You guys have the better idea allowing the \"non-local\" content imo. This is truly one of the last places on reddit to discuss AI/LLMs without people acting... a little crazy. The balance is perfect. Please don't change anything. The only problem imo is the over-abudace of *purely* AI-written content, rather than people using it for assistance, etc.\n\nHilarious that people are whining about GLM-5, though. It's literally open-weight. Just because they can't run it doesn't mean it's not valid.",
              "score": 2,
              "created_utc": "2026-02-13 08:04:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50xb9u",
              "author": "steezy13312",
              "text": "I remember for a while there was a bot that would monitor upvotes on a comment in a post as a check for \"true\" upvotes and then auto-remove posts if they didn't hit a particular threshold.\n\nThis was due to the fact that bots and people generally will upvote a post without reading it just based on a headline, distorting the value and relevance to the community.",
              "score": 1,
              "created_utc": "2026-02-12 18:33:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50m7pl",
              "author": "JacketHistorical2321",
              "text": "I completely disagree that this thread would be dead. 90% of the posts here are related to some sort of Open source resource. What I'm specifically talking about is what's been happening over the past few days where the main thread is getting spammed with posts strictly pointing to API when the local options are not available yet.Â \n\n\nLocal to some in terms of hardware resources is not what I'm trying to point out at all.Â \n\n\nDiscussion threads talking about a new model is fine but if the only link within the discussion is to API or other non-local source then links should not be allowed. Links should only be allowed if there is also a link to hugging face.Â \n\n\nThis approach at least stays closer to the true nature of this subreddit then what's currently being allowed.",
              "score": -2,
              "created_utc": "2026-02-12 17:42:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o517lgn",
                  "author": "overand",
                  "text": "No, they mean \"If we stuck to this as a discussion of Local **Llama** hosting, this subreddit would be dead.\" (As that would be sticking to the original-original intention.)",
                  "score": 10,
                  "created_utc": "2026-02-12 19:22:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o51hvvk",
          "author": "tmflynnt",
          "text": "I respectfully disagree on some of what you said there though I do feel like I understand where you're coming from.\n\nI do agree that we should greatly try to limit very low-effort posts and basically what amounts to advertisements related to non-local models.\n\nWhere I think I might differ though is the view that we should banish discussion of frontier API-only models altogether. \n\nI have been a member here since right around Llama2 was released and at least since I started there has always been discussions around both local and frontier API-only models. Now to be fair there has also been debates about whether this should be discussed since fairly early on too..\n\nBut my feeling at least has always been that while I very much embrace a local and open-weights first mentality, because of course that's why I am here!, I am also here because I like reading and engaging in smart discussions with other people that share these interests, and if a fellow local AI enthusiast is putting some real thought into a post about something related to the AI ecosystem, even if it's an API only model, I personally would like to hear it. Because what happens on the frontier will end up affecting what is local or at a minimum we should stay informed so we can advocate for or work toward innovations that we want to also see happen at the local level. \n\nAnd I simply don't know of another place like this that has the same kind of proportion of smart people who are passionate about this stuff like here, and so if we ban that I feel like we are eliminating some really high quality types of discussions that I have read in the past here and that I don't feel I can find anywhere else easily.\n\nNow if it were a requirement to have to post with a specific category flair that would be easily filterable I would totally support that as well as a compromise on this issue as I get many people are passionate about it and I respect that.\n\nAnd going back to the low effort stuff.. I do totally agree on a lot of what has been said about these kinds of posts. So yes, if it's bots posting what amounts to an ad or somebody simply saying \"ZOMG FUCKING CLAUDE 5 JUST DROPPED!!!!!\" with nothing meaningful to actually add to the discussion? Then yes, nuke this shit please. I am all for that.\n\nBut, I just hope we can strike a balance between any extremes and still be able to have thoughtful discussions among actual local AI hobbyists while definitely getting rid of the stuff we virtually all agree needs to be restricted.\n\nAnyway, thank you for sharing your thoughts on this, and this is definitely a discussion worth having and sorting out as a community.",
          "score": 13,
          "created_utc": "2026-02-12 20:11:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50cagh",
          "author": "pmttyji",
          "text": "Agree. 20-25% of \"local\" threads getting buried by those \"un-local\" threads. My single upvote is not helping those 20-25% of \"local\" threads :(",
          "score": 125,
          "created_utc": "2026-02-12 16:55:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50larc",
              "author": "ttkciar",
              "text": "Your up/down votes matter more than you think.\n\nWhen I got up this morning, I checked this sub for posts to review, and opened twenty-three tabs to see if they needed moderation.\n\nWhen someone flags a post as problematic, that makes it easy to pick out, but otherwise I partially depend on community opinion to decide what to look at for moderation.  If something is downvoted to zero, I check it out.  If it's been upvoted, I usually leave it be.\n\nReddit was always intended to be community-moderated, and voting is an important part of that.  Your upvotes also help people who sort posts by \"top\" or \"best\" find the most relevant content.  Please keep doing it!",
              "score": 72,
              "created_utc": "2026-02-12 17:37:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50per4",
                  "author": "MetroSimulator",
                  "text": "W mod, thanks for the clear explanation, happy my vote makes the difference",
                  "score": 18,
                  "created_utc": "2026-02-12 17:57:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o51mjti",
                  "author": "cosimoiaia",
                  "text": "Thank you for your service ðŸ«¡\n\nNow I know I have to up/downvote even harder.",
                  "score": 8,
                  "created_utc": "2026-02-12 20:33:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o50rklz",
                  "author": "pmttyji",
                  "text": "Thanks. Will do!",
                  "score": 6,
                  "created_utc": "2026-02-12 18:07:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o50n5kn",
                  "author": "paramarioh",
                  "text": "And do you know that when you complain, those bloody corporations suggest more of the same crap? They know exactly what's what. And on top of that, after you complain, they come back like Kevin in Home Alone? What else am I supposed to do? When I express my dissatisfaction, I get spat on and humiliated?",
                  "score": -10,
                  "created_utc": "2026-02-12 17:46:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o50eg57",
          "author": "jacek2023",
          "text": "Iâ€™m constantly getting downvoted on this sub for asking that, but Iâ€™m not going to stop.",
          "score": 70,
          "created_utc": "2026-02-12 17:05:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o517m26",
              "author": "Ulterior-Motive_",
              "text": "As a \"no local no care\" campist myself, the only time I think it goes too far is when an established open weights AI lab isn't given leeway when they initially release a new model via API, and get \"gguf or gtfo\" or replies to that effect when they have a clear history of releasing weights a day or two later.",
              "score": 12,
              "created_utc": "2026-02-12 19:22:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5276ka",
                  "author": "Neither-Phone-7264",
                  "text": "yeah. it gets tiring when i wanna hear discussion on a model and its all OH NO OPEN when they're like yup its uploading to hf right now you just gotta weight 5 minutes!",
                  "score": 2,
                  "created_utc": "2026-02-12 22:12:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50gd60",
              "author": "Borkato",
              "text": "Yeah this is like the only place that is supposed to be assumed local. r/SillyTavern is fucking annoying when Iâ€™m like â€œoh nice, what model do you use?â€ And they respond with â€œGeminiâ€ or â€œopusâ€ like come on lmao",
              "score": 33,
              "created_utc": "2026-02-12 17:14:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50ppr9",
                  "author": "MetroSimulator",
                  "text": "I'm all for free discussion, but this sub is literally LOCALlhama",
                  "score": 16,
                  "created_utc": "2026-02-12 17:58:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o51cbnp",
              "author": "nntb",
              "text": "I upvote local content",
              "score": 2,
              "created_utc": "2026-02-12 19:44:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o51mfv4",
              "author": "Asalanlir",
              "text": "Technically, the rules don't state content is expected to be local content exclusively. You or I may disagree about the purpose of the sub, but if someone else is thinking about whether to post content (and we are assuming no malice), the rules would be the place they would be expected to check if a particular type of content is accepted.",
              "score": 2,
              "created_utc": "2026-02-12 20:33:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50d3tp",
          "author": "Accomplished_Ad9530",
          "text": "Seems like the mods haven't decided where to draw the line or even if a line should be drawn. The quantity of corporate spam and voting swarms is really annoying, though. I wish this sub would stick to its principals. Plenty of other subs for nonlocal stuff.",
          "score": 35,
          "created_utc": "2026-02-12 16:58:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50jgdr",
              "author": "ttkciar",
              "text": "I won't go into details, because details would help bad actors figure out how much they can get away with, but the moderators disagree among ourselves about where that line is, and have agreed to a compromise.",
              "score": 16,
              "created_utc": "2026-02-12 17:29:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50oexd",
                  "author": "JacketHistorical2321",
                  "text": "Maybe I wasn't super clear in my original topic. I actually tried to make another post with more detail regarding what exactly I'm trying to talk about but it was instantly taken down. I have no idea why.Â \n\n\nWhat I mainly focusing on is what we've been seeing in the past two to three days with the new model releases. We're seeing multiple threads spamming links to API prior to weights being released. I get the sense that a lot of these are low-key marketing.Â \n\n\nIf people want to create a post related to discussing a model that will more than likely be available soon for local serving then that's fine. But if a post includes links to only API related resources that should be taken down. The caveat to that would be a link to API could be allowed as long as a hugging face link is included also. To me this is the best way to maintain balance and make sure that this sub doesn't just become entirely marketing BS for bots.",
                  "score": 8,
                  "created_utc": "2026-02-12 17:52:31",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o50k8r3",
                  "author": "jacek2023",
                  "text": "\"but the moderators disagree among ourselves about where that line is\" \n\nwhy?",
                  "score": 2,
                  "created_utc": "2026-02-12 17:32:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o50mfkv",
                  "author": "paramarioh",
                  "text": "And isn't compromise a situation where everyone is dissatisfied with the choice and therefore no one cares about anything?",
                  "score": -4,
                  "created_utc": "2026-02-12 17:43:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50m5bg",
              "author": "Evening_Tooth_1913",
              "text": "i think some \"unlocal\" content should be allowed to help us stay updated on the landscape of ai. for some people like me, this sub helps a lot in staying updated. its very hard to draw the line of what should and shouldnt be allowed. \n\nCorrect me if i am wrong, but all the \"unlocal\" content isnt some small closed startup marketing their stuff. its mainly gemini/opus/big labs new model",
              "score": 10,
              "created_utc": "2026-02-12 17:41:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50lrrs",
              "author": "JacketHistorical2321",
              "text": "I feel like just making it a requirement for there to be a hugging face link if a new model is mentioned would be enough. Like, discussion threads about new models is fine as long as there are no links to API sources. If an API source link is included then it should be a requirement that a hugging face link is also included. I actually tried to make another post proposing this but for some reason it was immediately taken down",
              "score": 4,
              "created_utc": "2026-02-12 17:40:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o515woa",
              "author": "Yorn2",
              "text": "> Seems like the mods haven't decided where to draw the line or even if a line should be drawn.\n\nLet me restate that based on what I've observed. At least one of the moderators seems to want to be a moderator of a popular sub, a popular twitter, a popular discord, and feel like they have some influence with the cloud providers while not upsetting those of us that want the sub to be more local focused. That's why they don't want to draw the line.",
              "score": 1,
              "created_utc": "2026-02-12 19:14:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5498sb",
              "author": "cheechw",
              "text": "What subs are those? I'm genuinely looking because I'm interested in the latest news about models coming out and benchmarks etc but there aren't any good other options.",
              "score": 1,
              "created_utc": "2026-02-13 05:50:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5124o9",
          "author": "ArsNeph",
          "text": "It's very clear to me that a lot of users here are newer, but LocalLlama has always been this way. From the days of Llama 2 and Mistral, we have always posted about closed source models and their development, for the purpose of figuring out their techniques and methodology. It is the speculation around GPT-4 being an MoE that arguably brought about the first OS MoE model, Mixtral. Pretending like the mainstream models do not exist only harms the improvement and distillation of OS models.",
          "score": 13,
          "created_utc": "2026-02-12 18:56:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50idej",
          "author": "kiwibonga",
          "text": "Yeah I totally agree, and it's with this in mind I used Claude Code to vibe my way to $5000 MRR! See my blog for more info: http://spam.spam/spam",
          "score": 21,
          "created_utc": "2026-02-12 17:23:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50meqz",
              "author": "snowglowshow",
              "text": "Man, you really like canned lunch meat!",
              "score": 2,
              "created_utc": "2026-02-12 17:43:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50mb1q",
              "author": "Evening_Tooth_1913",
              "text": "almost clicked the link, damn it",
              "score": 5,
              "created_utc": "2026-02-12 17:42:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50wnt3",
                  "author": "steezy13312",
                  "text": "I clicked it just to see if .spam was a real TLD",
                  "score": 3,
                  "created_utc": "2026-02-12 18:30:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o50mrd2",
          "author": "Patq911",
          "text": "I semi agree, but at the same time this is the only LLM related subreddit that isn't insane or cringe.",
          "score": 13,
          "created_utc": "2026-02-12 17:44:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51dhdr",
              "author": "the__storm",
              "text": "Yeah, this is the only semi-serious LLM sub - I'd like to continue to see discussion of new frontier models here even when they're proprietary.Â  (There's also r/machinelearning , but it's the last bastion of non-LLM AI so I don't think such discussion belongs there either.)",
              "score": 9,
              "created_utc": "2026-02-12 19:50:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o51m28d",
                  "author": "tmflynnt",
                  "text": "Yes, I feel similar to how you both feel on this and posted a probably-too-damn-long separate comment to OP trying to express similar sentiments. \n\nBecause yeah while I do share the hate for low-effort posts, I really do appreciate many of the open-ended AI-related discussions that have occurred here in the past.. and this place has something special as far as the amount of serious, smart, non-cringe people that participate here.",
                  "score": 1,
                  "created_utc": "2026-02-12 20:31:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o51kg6r",
              "author": "tmflynnt",
              "text": "Hard agree",
              "score": 5,
              "created_utc": "2026-02-12 20:23:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50jv7v",
          "author": "wraitii_",
          "text": "This is the only decent LLM-related subreddit tbh, it'd be really annoying if we can't compare the really local models vs the GLM5 or minimax 2.5 or the world.",
          "score": 9,
          "created_utc": "2026-02-12 17:31:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o517noh",
          "author": "Falcon_Strike",
          "text": "I get that some threads may be spam or low quality, but this sub should absolutely contain posts about new frontier model releases, and posts about frontier labs in general (particularly in respect towards their attitudes towards open source/local and open source research). It is important for us to be able to compare where the frontier is in private models and local models. besides, i hate having to go to other subs to read about frontier models, since id rather have my local llama homies contextualize how good/bad a model is with respect to what they run locally. ",
          "score": 8,
          "created_utc": "2026-02-12 19:22:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50k7hi",
          "author": "Conscious_Cut_6144",
          "text": "I mean itâ€™s all somewhat related to local,   \nGLM5 is still a touch below sota closed source.\n\nAnd itâ€™s been that way basically ever since R1\n\nThe better closed gets, the better local gets.",
          "score": 7,
          "created_utc": "2026-02-12 17:32:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50kyci",
          "author": "alvisanovari",
          "text": "This is how r/StableDiffusion went to shit. They focused so much on local only that they would promote low quality slop over high quality content just cause the user used a hosted model (even if it was open source). Why does everything need to be black and white.? The focus here is mostly on local models. And that's fine.",
          "score": 11,
          "created_utc": "2026-02-12 17:36:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51rok0",
              "author": "tmflynnt",
              "text": "Agreed, but I do think we need to figure out a compromise of some sort because this issue keeps coming up and I also don't want to lose some of the really smart and thoughtful people who are more passionate about a local-only emphasis \n\nI like the flair idea and wonder if something like that could work.",
              "score": 1,
              "created_utc": "2026-02-12 20:58:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50baio",
          "author": "SquareTranslator9777",
          "text": "Yes thats true un-local content doesn't belong in this sub",
          "score": 13,
          "created_utc": "2026-02-12 16:50:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50jp6p",
          "author": "Far_Composer_5714",
          "text": "I like to know major updates in non local this place is good for AI news.Â \n\n\nAka the release of major non local models.Â \n\n\nBut I can definitely see the general concern of too much non local content.",
          "score": 5,
          "created_utc": "2026-02-12 17:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51no65",
          "author": "Daniel_H212",
          "text": "For major model releases where an API is released prior to weights release, I think only one single dedicated thread should be allowed (and only when we are expecting a weights release in the near future).",
          "score": 2,
          "created_utc": "2026-02-12 20:39:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54zs9w",
          "author": "Cuplike",
          "text": "Because shills need to post here for some reason",
          "score": 2,
          "created_utc": "2026-02-13 09:50:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50l2hl",
          "author": "sine120",
          "text": "I'm okay with it from a comparison standpoint.  Cloud models/ providers are somewhat leading the industry in use case.  Last year it was about models getting smarter and able to one-shot things.  This year it's about agents.  If a post is about \"Here's Sonnet 5's specs\", that's less interesting to me if it's posted here, if it's a case of \"Here's why I need to use Opus 4.5 and not GLM Air\", that's okay, as long as it's obvious how it links back.",
          "score": 2,
          "created_utc": "2026-02-12 17:36:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5199tr",
          "author": "onewheeldoin200",
          "text": "I think non-local posts should ***at minimum*** be required to tagged with flair to clearly identify them, and would be better to just add to the rules that \"if it isn't local you can't post it\".",
          "score": 2,
          "created_utc": "2026-02-12 19:30:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51c4ju",
          "author": "OriginalPlayerHater",
          "text": "sounds like we could just implement flairs that have \"pure local\" and \"beyond local\".\n\nthen users can filter with flairs and everyone is happy",
          "score": 2,
          "created_utc": "2026-02-12 19:43:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51oqi2",
              "author": "ttkciar",
              "text": "I tried exactly that about half a year ago, and the community overwhelmingly hated it, which seems unfortunate.  It would have gone a long way to solving the problem.\n\nEvidence of community backlash: https://old.reddit.com/r/LocalLLaMA/comments/1n9kwwr/new_post_flair_local_only/",
              "score": 3,
              "created_utc": "2026-02-12 20:44:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o55xoh7",
                  "author": "JazzlikeLeave5530",
                  "text": "A lot of the comments there said they'd be happy with a non-local flair. Did that ever get attempted?",
                  "score": 0,
                  "created_utc": "2026-02-13 13:57:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o51k79b",
              "author": "tmflynnt",
              "text": "Yes, I like this idea.",
              "score": 0,
              "created_utc": "2026-02-12 20:22:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50ux2h",
          "author": "Condomphobic",
          "text": "I saw an unlocal post with 1K upvotes get removed by mods today. Theyâ€™re working on it.\n\nWe canâ€™t allow this sub to become one of those",
          "score": 2,
          "created_utc": "2026-02-12 18:22:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o514bpg",
          "author": "Far-Low-4705",
          "text": "it is useful to compare top closed models to open source models.\n\nnot everything has to strictly be local, just as long as that is the general theme. idk why people have such strong resentment for it either, its not like you have to engage in it.",
          "score": 1,
          "created_utc": "2026-02-12 19:06:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50mtpr",
          "author": "Dany0",
          "text": "I mean if a new closed sota is released I'm excited anyway because we can distill em",
          "score": 1,
          "created_utc": "2026-02-12 17:45:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50pkbp",
              "author": "PhlarnogularMaqulezi",
              "text": "This comment made me realize I might not totally understand what distillation is lol",
              "score": 1,
              "created_utc": "2026-02-12 17:57:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o50sjac",
                  "author": "burner_sb",
                  "text": "The original meaning of distillation has been lost anyway, as it now includes using a LLM to generate synthetic training data.",
                  "score": 6,
                  "created_utc": "2026-02-12 18:11:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o51g1iq",
          "author": "Echo9Zulu-",
          "text": "We need some examples to discuss. Maybe OP should make a follow up post?",
          "score": 1,
          "created_utc": "2026-02-12 20:02:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52aqxs",
          "author": "sandman_br",
          "text": "I agree with you",
          "score": 1,
          "created_utc": "2026-02-12 22:30:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55817i",
          "author": "AnomalyNexus",
          "text": "Between local being economically uncompetitive per token and anything near edge being too big to run locally you'd restrict it to a very small grouping of people. Essentially those content to run 7B and those willing to spend 15k and those doing janky P40 attempts to square the circle of cost and vram.\n\nHard to tell whether that would be a net win. Smaller community...but philosophically purer in a way. Think we'd see another sub pop up and rapidly overtake this one if enforced. And not sure fracturing would be a win even for those wanting to keep it local",
          "score": 1,
          "created_utc": "2026-02-13 11:05:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56nf66",
          "author": "SkyNetLive",
          "text": "i suppose because we are not like closed LLMs (you-know-who) they take down local posts and complaints. so perhaps , and its just a thought from a regular reader, we just let them have a tag. this way i can just ignore them if i am not interested but its good to know whats out there. lest we make the mistake of putting our heads in the sand like the closed source ones often do. (remember mistral?)",
          "score": 1,
          "created_utc": "2026-02-13 16:06:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59mo8e",
          "author": "layer4down",
          "text": "Iâ€™m here to talk about open weights models. Youâ€™ll almost never see me post about closed weight models cuz thatâ€™s not my thing. I openly use GLM from z.ai because itâ€™s a damned good service. Iâ€™m also considering Minimax hosted models because theyâ€™re also competitively priced. I also have various GLM models on my M2 Studio Ultra 192GB and M3 Studio Ultra 512GB but seems a bit unnecessary to mention because weâ€™re discussing open weight models after all so the consumption model is a bit irrelevant IMHO. But Iâ€™m happy to discuss that for those interested.\n\nI prefer the vendor-hosted models (for now) purely for prefill performance. Decode performance locally is pretty decent. If someone wants to discuss how to squeeze better prefill performance from Apple Silicon outside of KV caching then Iâ€™m all ears. Like maybe prompt caching but I canâ€™t find much in the way of proper implementations myself.",
          "score": 1,
          "created_utc": "2026-02-14 01:20:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50hcjz",
          "author": "klenen",
          "text": "We need more mods? Iâ€™d do it!",
          "score": 2,
          "created_utc": "2026-02-12 17:19:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50q1x3",
              "author": "Free-Internet1981",
              "text": "Me too",
              "score": 0,
              "created_utc": "2026-02-12 17:59:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o52e6yv",
          "author": "Bite_It_You_Scum",
          "text": "Why do we allow content that isn't about Llama models?",
          "score": 1,
          "created_utc": "2026-02-12 22:48:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52jqi5",
          "author": "Mundane-Light6394",
          "text": "if we know the model will be available soon why not jump on the hype train? it will also help people looking for content about the model find out about our alternative for apis",
          "score": 1,
          "created_utc": "2026-02-12 23:18:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50aq7u",
          "author": "laterbreh",
          "text": "Were complaining about the mods and shit posts already here: \n\n[https://www.reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/](https://www.reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/)",
          "score": 0,
          "created_utc": "2026-02-12 16:47:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50cx40",
          "author": "Parking-Bet-3798",
          "text": "I for one appreciate those discussions also happening here. We donâ€™t have to be a stickler on technicalities. I donâ€™t own the hardware to run it, but I could still rent a server , deploy the model, and do my job there. Thats close enough for me personally.",
          "score": -6,
          "created_utc": "2026-02-12 16:58:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51eifn",
          "author": "HarjjotSinghh",
          "text": "i hope you don't actually mean it - then we're doomed.",
          "score": 0,
          "created_utc": "2026-02-12 19:55:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50dyf4",
          "author": "mtmttuan",
          "text": "What's your opinion on SaaS that pretty much has nothing to do with local AI except from the fact that it can be connected to openai-compatible endpoints (hence the title will be \"I built thid app and it can completely run locally using Ollama/LM studio\")?",
          "score": -6,
          "created_utc": "2026-02-12 17:03:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50kexr",
              "author": "ttkciar",
              "text": "I don't know why you're being downvoted so hard, because you've put your finger on a real problem.  A lot of posts are exactly what you describe, and are hard to moderate because they have to be assessed by their value to the local inference community.  Reading through the code takes time, and sometimes the decision comes down to subjective opinion.",
              "score": 6,
              "created_utc": "2026-02-12 17:33:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o51bpyu",
          "author": "t3rmina1",
          "text": "What is the 'true' nature of this sub?\n\nThis is the main sub that discusses any sort of LLMs at this point. If we put in guardrails we'd have to split out cloudfrontier and/or cloudchina, then this sub would be de facto localchina because llama is pretty much legacy at this point. \n\nAnd you probably aren't running GLM 5 on local for the next few years, if ever.",
          "score": -1,
          "created_utc": "2026-02-12 19:41:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50dgo7",
          "author": "Ok-Lobster-919",
          "text": "If you really want to get pedantic, why are we talking about other models on a sub dedicated to LLama by Meta?\n\nThe answer is: because this is a popular AI discussion forum and a popular sub is an alive sub. Sometimes when a sub loses it's way for popular reasons it can be bad. But in this case it doesn't seem to detract from conversations users want to have here.\n\n  \nSubreddit topic \"Subreddit to discuss AI & Llama, the large language model created by Meta AI.\"\"",
          "score": -12,
          "created_utc": "2026-02-12 17:00:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50i6k1",
          "author": "MasterKoolT",
          "text": "Local is generally trailing proprietary so it gives you a sense of what you might see local/open-weight in the near future. I don't think we need to be so pure as to ignore what's happening in the non-local space",
          "score": -6,
          "created_utc": "2026-02-12 17:23:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52xu8t",
          "author": "Tai9ch",
          "text": "Even major pure-proprietary news is related to local LLM stuff because, for example, a new Claude or Grok release provides a new comparison point for other models.",
          "score": -1,
          "created_utc": "2026-02-13 00:39:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52kf2w",
          "author": "Steuern_Runter",
          "text": "Even if you are 100% local, the non-local developments are still relevant and interesting.",
          "score": -2,
          "created_utc": "2026-02-12 23:22:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}