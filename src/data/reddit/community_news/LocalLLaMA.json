{
  "metadata": {
    "last_updated": "2026-03-01 08:47:41",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 20,
    "total_comments": 1375,
    "file_size_bytes": 1366318
  },
  "items": [
    {
      "id": "1rcpmwn",
      "title": "Anthropic: \"Weâ€™ve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.\" ðŸš¨",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/94fbimavfalg1.png",
      "author": "KvAk_AKPlaysYT",
      "created_utc": "2026-02-23 18:32:45",
      "score": 4636,
      "num_comments": 857,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o70o3tf",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-23 20:50:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zwe8x",
          "author": "SGmoze",
          "text": "I wonder how did Anthropic build their dataset. Surely they manually had them annotated by humans.\n",
          "score": 2383,
          "created_utc": "2026-02-23 18:40:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zyn75",
              "author": "Mkboii",
              "text": "Yes and their model totally didn't accidentally call itself chatgpt even as recently as their last generation of models.",
              "score": 1141,
              "created_utc": "2026-02-23 18:50:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o702nf2",
                  "author": "Charuru",
                  "text": "Claude literally calls itself deepseek.\n\nhttps://www.reddit.com/r/DeepSeek/comments/1r9se7p/claude_sonnet_46_distilled_deepseek/",
                  "score": 694,
                  "created_utc": "2026-02-23 19:08:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7028gd",
              "author": "g0pherman",
              "text": "They actually spend a lot of money on human curated data (I've done that for them for a while), but surely not all of it.",
              "score": 159,
              "created_utc": "2026-02-23 19:06:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70ckom",
                  "author": "Bderken",
                  "text": "I think Claude is the best one for human curated data. Especially for coding. Thatâ€™s why their coding is so good. I believe codex was also made in a similar way from the human curating firms but that was after a year of OpenAI watching anthropic do that",
                  "score": 77,
                  "created_utc": "2026-02-23 19:55:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o70cl45",
              "author": "flextrek_whipsnake",
              "text": "A lot of it is, they spend a shitload of money on that. They also bought giant piles of physical books along with a machine that slices the spine off so they can be scanned efficiently. They can legally use the scanned text for training since they obtained it from physical copies of books they purchased.\n\nOf course originally they stole all of it just like everyone else did.",
              "score": 64,
              "created_utc": "2026-02-23 19:55:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70pyak",
                  "author": "mikiex",
                  "text": "When the robot runs out of book spines to slice off it's probably going to look for a new source of spines!",
                  "score": 71,
                  "created_utc": "2026-02-23 20:58:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70lq5d",
                  "author": "throughawaythedew",
                  "text": "It's all very cool and very legal, you see we have a robot shredding books 24/7. \n\nOh thank goodness I thought it was something illegal.",
                  "score": 38,
                  "created_utc": "2026-02-23 20:38:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70oon0",
                  "author": "Glad_Middle9240",
                  "text": "Right.  Because if you buy the paper itâ€™s printed on before you steal the intellectual property itâ€™s all good.   Iâ€™m aware of a certain judicial opinion on this and I think itâ€™s deeply wrong and destructive.   It basically means LLM trainers can steal anyoneâ€™s intellectual property at will as long as they convert the text to tensors first.",
                  "score": 16,
                  "created_utc": "2026-02-23 20:52:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o724kgg",
                  "author": "koshgeo",
                  "text": "\"Stole it?\"  No, no.  They did a \"distillation attack\" on pirate libraries, and now that other people are doing it on their model, they're upset.",
                  "score": 2,
                  "created_utc": "2026-02-24 01:29:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o70olb6",
              "author": "fazkan",
              "text": "they pay other companies to manually collect this data, scaleAI was a big one. There are a few startups that are growing really fast to solve this particular model. ",
              "score": 6,
              "created_utc": "2026-02-23 20:52:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o71o5hg",
              "author": "WiggyWongo",
              "text": "Great first comment to see. Absolutely. Pot calling the kettle black. If they sue or try to, then anyone who created content in AI datasets should be allowed to sue too! (Certain content you make is owned by the platforms you post on sometimes but not all of them and not in all cases)",
              "score": 2,
              "created_utc": "2026-02-23 23:56:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zvs0r",
          "author": "ziphnor",
          "text": "I am not a copyright fan, but when your whole business has been based on distilling everybody else's data (in many cases without the rights to even normal consumer access), I am not sure I see the problem here?",
          "score": 1051,
          "created_utc": "2026-02-23 18:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zxfwe",
              "author": "bigh-aus",
              "text": "I'm with you on this.  At least the Chinese models are all open weights aka given back to the community.  Anthropic has just gatekept, centralized, sued people using the reason of \"Safety\".   I don't see them providing the risks of centralization, gatekeeping etc. \"Trust us we're a for profit company\".  I haven't seen one article on how they keep your information private, how they're HIPAA or PCI compliant.  At least they're pushing back on dragnets across data.",
              "score": 469,
              "created_utc": "2026-02-23 18:45:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70061h",
                  "author": "Recoil42",
                  "text": "Just occurred to me â€”Â Anthropic is the only major AI lab to not release a single open-weight model right? ",
                  "score": 166,
                  "created_utc": "2026-02-23 18:57:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70xncz",
                  "author": "dragoon7201",
                  "text": "okay, but lets have a little sympathy for Anthropic team here, they just raised 30B in their most recent funding rounds. \n\nHow do they justify asking for billions more if some chinese lab can just steal their model!?\n\nHow will Dario ever reach 100B in net worth if they can't get funding?!\n\nDo you realize you just kneecapped someone's billionaire aspirations??\n\nThat is just cruel man, imagine how sad it is to live as a mere millionaire ",
                  "score": 39,
                  "created_utc": "2026-02-23 21:37:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70ez1g",
                  "author": "MoffKalast",
                  "text": "I wouldn't be surprised if Anthropic's only problem with it is releasing the end result openly. They can compete with Deepseek or Kimi on an API basis and win, but can't  compete with free forever. The dipshits want to monopolize the space so open models are an affront to them.",
                  "score": 9,
                  "created_utc": "2026-02-23 20:06:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71du1t",
                  "author": "Bocchi_theGlock",
                  "text": "Regarding keeping information private, wasn't there a post recently about how they don't cooperate with DHS and this Administration's requests for data/access unless there's a warrant?",
                  "score": 2,
                  "created_utc": "2026-02-23 22:58:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o71373p",
              "author": "porkyminch",
              "text": "Honestly I think it's fucked up that any models are being kept as proprietary. You're going to ingest everything on the internet, from everyone, but you get to keep the model under lock and key? Sorry, but I don't see how that's reasonable. \n\nThe \"safety\" excuse from the big American labs rings hollow. There are very real social problems being created by AI *today* (sycophancy, deepfakes, scams, energy usage, economic problems, #keep4o, etc) that these companies conveniently ignore while whinging about an at-this-point totally fictional self-improving AGI scenario. \n\nAnthropic has the best models (in my subjective opinion) for what I use them for, so I'll keep using them as long as my job keeps paying for them, but I'm wholly unimpressed by how *all* of the American companies have approached safety. At least the Chinese companies are operating in a country that's made real investments in clean energy, so they're not just going to be running on fucking generators forever.",
              "score": 22,
              "created_utc": "2026-02-23 22:04:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6zxtvp",
              "author": "ihexx",
              "text": "yeah, they should be consistent: either piracy is theft or it isn't. Anthropic should pick a side or shut the fuck up",
              "score": 56,
              "created_utc": "2026-02-23 18:46:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o70galu",
              "author": "lakimens",
              "text": "Yep, and these Chinese models paid them for it, probably in the millions of dollars.",
              "score": 26,
              "created_utc": "2026-02-23 20:12:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o743g2d",
              "author": "Divniy",
              "text": "I see the problem in them trying to protect this data rather being forced to make it open.\n\nYou take this data from the whole humanity. You trampled over every copyrights possible, you don't have the ability to even guarantee the right to be forgotten. \n\nGive back to humanity. We shouldn't ask. We must demand.",
              "score": 5,
              "created_utc": "2026-02-24 10:27:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zv3zu",
          "author": "Zyj",
          "text": "You're saying they treated you like you treated all those authors whose books you torrented?\n\nOh no, that's not it. They are paying you for API tokens.",
          "score": 2133,
          "created_utc": "2026-02-23 18:34:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zwnqt",
              "author": "bel9708",
              "text": "If getting paid is an attack then what was the out right theft they did? ",
              "score": 434,
              "created_utc": "2026-02-23 18:41:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zyldu",
                  "author": "yaosio",
                  "text": "It's ok to steal as long as you don't pay for what you steal. If you steal candy and walk out the door that's fine, if you pay for it that's illegal.",
                  "score": 207,
                  "created_utc": "2026-02-23 18:50:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6zxplg",
                  "author": "PmMeSmileyFacesO_O",
                  "text": "Can someone do the math?",
                  "score": 33,
                  "created_utc": "2026-02-23 18:46:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zy9lm",
              "author": "Zestyclose839",
              "text": "Also (correct me if I'm wrong) but I don't believe they're true \"distillation\" attacks because the API doesn't return the token activation probabilities and the other juicy stuff needed to transfer knowledge. Sure, they can fine-tune a model to speak and act like Claude, but it's not as accurate as an open-weight to open-weight model distillation (like the classic Deepseek to Llama distills).",
              "score": 116,
              "created_utc": "2026-02-23 18:48:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zzw48",
                  "author": "Recoil42",
                  "text": "Yep at best it's alignment, and mostly likely style alignment. ",
                  "score": 80,
                  "created_utc": "2026-02-23 18:55:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o700tsl",
                  "author": "30299578815310",
                  "text": "Also they dont get full chain of thought right?",
                  "score": 16,
                  "created_utc": "2026-02-23 19:00:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70kshp",
                  "author": "Feisty_Resolution157",
                  "text": "There is various terminology that applies, but in that list is: Hard-label distillation or black-box distillation",
                  "score": 2,
                  "created_utc": "2026-02-23 20:34:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o704ymb",
                  "author": "Ok-Measurement-1575",
                  "text": "Indeed.Â Even Anthropic misusing the term.\n\n\nBizarre tbh.\n\n\nUnless... there's something else going on here that somehow elicits shape from the pairs?",
                  "score": 2,
                  "created_utc": "2026-02-23 19:19:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6zzaqx",
                  "author": "SrijSriv211",
                  "text": "100%",
                  "score": 4,
                  "created_utc": "2026-02-23 18:53:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zwd1q",
              "author": "DustinKli",
              "text": "Precisely.",
              "score": 87,
              "created_utc": "2026-02-23 18:40:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7057qa",
              "author": "Orolol",
              "text": "There's a BIG difference : the three companies they cited are chinese, and that's suit the anti-china rhetoric of Dario.",
              "score": 31,
              "created_utc": "2026-02-23 19:20:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6zydsz",
              "author": "Hoodfu",
              "text": "That's disgusting and horrible, where would one find these distilled models? /s",
              "score": 51,
              "created_utc": "2026-02-23 18:49:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o70zrgn",
              "author": "porkyminch",
              "text": "Incidentally, model output is not legally copyrightable, but the stuff Anthropic has scraped/scanned/whatever generally is. I don't really care about \"ethical training data,\" I think the copyright complaints are only going to benefit big rightsholders, but I think objectively a Chinese lab *paying* Anthropic for tokens is less objectionable than Anthropic taking whatever data they can get and worrying about the legality of it later.",
              "score": 8,
              "created_utc": "2026-02-23 21:47:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6zy02t",
              "author": "Mkboii",
              "text": "I mean Anthropic famously bought and scanned at least one copy of the books they used, so they definitely think they are better than everyone else.",
              "score": 66,
              "created_utc": "2026-02-23 18:47:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o703824",
                  "author": "Competitive_Travel16",
                  "text": "No, Anthropic purchased and physically scanned about a million books. They downloaded approximately 7 million books from shadow libraries like Library Genesis and the Pirate Library Mirror without paying for them. (Until they ~~lost in court~~ *reached a settlement with lawyers for 500,000 of the authors* last September and now have to pay at least $3,000 each.)",
                  "score": 73,
                  "created_utc": "2026-02-23 19:11:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o700wv6",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 3,
                  "created_utc": "2026-02-23 19:00:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7010xx",
              "author": "mana_hoarder",
              "text": "Saying \"attack\" makes it sound so grave. Call it learning instead. Better models for everyone. ",
              "score": 26,
              "created_utc": "2026-02-23 19:01:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o712m63",
                  "author": "GreenGreasyGreasels",
                  "text": "\"Attack\", \"Illicit\", \"Fraudulent account\" - it was not an attack, not illicit and not fraudulent. Loaded language to try to guide the reader by the nose on how to emotionally react - must have hired someone from NYT.\n\nGreat models but Anthropic is the \"Oracle\" of AI companies. Every shit practice standardized now was invented or popularized by Anthropic - no clear usage agreement \"generous/more/higher\" non-sense weasel word verbiage in terms of agreement, constant introduction of quotas - 5 hour quota, weekly quota, monthly quota, I-am-busy-so-fuck-off quota, nerfing models after the honeymoon period is done, terming making full use of agreed upon usage as \"malicious/abusive\" usage even you have clear internal token limits with cutoffs, banning people with no recourse or warning for invented post facto reason - the shit they pull is endless and on top of that the holier than thou safety theater, constant zero sum xenophobic game with China, attempts to squeeze competitors with regulation - shit is endless.\n\nWorst thing that could happen to AI would be a malevolent self righteous company like Anthropic coming on top at the end - sleaze ball Sam Altman, or the generic corpo fuckery of google seems refreshing in comparison. Only worse outcome is Grok dominating - but that seems unlikely. \n\nLove Claude, Fuck Anthropic.",
                  "score": 25,
                  "created_utc": "2026-02-23 22:01:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o723eu1",
                  "author": "Old-School8916",
                  "text": "they call it attack to get in good graces of USGov, since it's chinese companies doing it ",
                  "score": 2,
                  "created_utc": "2026-02-24 01:22:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o704nkn",
              "author": "Old-School8916",
              "text": "or reddit posts for the matter. anthropic appears to have bypassed reddit ToS en masse\n\n[https://www.courtlistener.com/docket/70704683/reddit-inc-v-anthropic-pbc/](https://www.courtlistener.com/docket/70704683/reddit-inc-v-anthropic-pbc/)",
              "score": 9,
              "created_utc": "2026-02-23 19:17:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6zx0ac",
              "author": "abdouhlili",
              "text": "Upvotes are not enough for this comment.",
              "score": 14,
              "created_utc": "2026-02-23 18:43:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o708m43",
              "author": "Amazing-Oomoo",
              "text": "Breaking news: pot calls kettle black",
              "score": 5,
              "created_utc": "2026-02-23 19:36:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7011sn",
              "author": "PerceptionOwn3629",
              "text": "Exactly, fuck em.",
              "score": 5,
              "created_utc": "2026-02-23 19:01:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o715v88",
              "author": "Geesle",
              "text": "All these AI companies do shady shit to get ahead.",
              "score": 2,
              "created_utc": "2026-02-23 22:17:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o719n3r",
              "author": "starkruzr",
              "text": "\"distillation attacks\" lolololol the irony is rich. \n\nsuck it up, Anthropic. you can always turn around and train your own models on those interactions, after all.",
              "score": 2,
              "created_utc": "2026-02-23 22:36:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o702pgw",
              "author": "Due-Memory-6957",
              "text": "Not only that, but Anthropic also trained on ChatGPT (as did basically everyone else because for a long time ChatGPT was the best AI model out there).",
              "score": 2,
              "created_utc": "2026-02-23 19:08:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6zycca",
              "author": "fittyscan",
              "text": "TouchÃ©",
              "score": 3,
              "created_utc": "2026-02-23 18:49:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o701dv3",
              "author": "Altruistic_Welder",
              "text": "Your honour, Chinese criminals are stealing our data.   \nJudge - My first question, how exactly.  \nAnthropic - They are using our APIs and stealing our response tokens.  \nJudge - ok, here's my second question. Do you have 10 seconds to get the f\\*\\*\\* out of my courthouse ?",
              "score": 1,
              "created_utc": "2026-02-23 19:02:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7044fj",
                  "author": "Competitive_Travel16",
                  "text": "If only! Civil cases about supposed fraud based on TOS violations will drag on for years.",
                  "score": 2,
                  "created_utc": "2026-02-23 19:15:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zwpg0",
          "author": "abdouhlili",
          "text": "Please China, Distill harder, We need Strong Deepseek V4, Kimi K3 and Minimax M3.",
          "score": 922,
          "created_utc": "2026-02-23 18:41:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zyuox",
              "author": "HostNo8115",
              "text": "And release seedance2.0 for local use please",
              "score": 171,
              "created_utc": "2026-02-23 18:51:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o701463",
                  "author": "eugene20",
                  "text": "For the 1 in 10,0000 ai enthusiasts with enough ram to play with it, lol.",
                  "score": 46,
                  "created_utc": "2026-02-23 19:01:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o704zp5",
                  "author": "SodaBurns",
                  "text": "The mouse will send SWAT teams to your house if they ever release a local version of seedance.",
                  "score": 12,
                  "created_utc": "2026-02-23 19:19:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70oex9",
                  "author": "curryslapper",
                  "text": "tough this request is",
                  "score": 2,
                  "created_utc": "2026-02-23 20:51:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7031o0",
              "author": "Signal_Ad657",
              "text": "This is exactly how I feel.  Thank god the open source models are learning from the closed source leaders and getting better.  No user is crying for you Anthropic.",
              "score": 42,
              "created_utc": "2026-02-23 19:10:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o70bokw",
              "author": "Own-Lavishness4029",
              "text": "I am really quite liking m2.5. Would love to see a bit more distillation. The fucking balls on these people claiming someone else stole their stolen property.",
              "score": 10,
              "created_utc": "2026-02-23 19:50:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o708zpr",
              "author": "TheDuhhh",
              "text": "I have actually made a commitment that every month I will be subscribing to at least one open source model provider. For now, it seems the top open source products are from China and this month is Minimax. Cant wait for deepseek V4",
              "score": 18,
              "created_utc": "2026-02-23 19:38:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70ohx3",
                  "author": "MerePotato",
                  "text": "Its GLM 5 imo, crazy low hallucination rate",
                  "score": 15,
                  "created_utc": "2026-02-23 20:52:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o70pzsu",
              "author": "AbheekG",
              "text": "+1",
              "score": 3,
              "created_utc": "2026-02-23 20:59:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zx7as",
          "author": "Financial-Camel9987",
          "text": "\"distillation attacks\" lmao. Brother they are using your product and paying for it. ",
          "score": 531,
          "created_utc": "2026-02-23 18:44:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o701bi0",
              "author": "Recoil42",
              "text": "I'm gonna head to chipotle after this and distillation attack a burrito, anyone wanna join?",
              "score": 235,
              "created_utc": "2026-02-23 19:02:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o707b9c",
                  "author": "olmoscd",
                  "text": "if you write down the tastes from the output of the line cook then make a burrito, iâ€™m sorry but you are illegally distilling an attack",
                  "score": 72,
                  "created_utc": "2026-02-23 19:30:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70e8j3",
                  "author": "Much-Researcher6135",
                  "text": "DON'T STEAL OUR RECIPE BY LOOKING AT THE PRODUCT WITH YOUR EYEBALLS",
                  "score": 46,
                  "created_utc": "2026-02-23 20:02:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o733del",
                  "author": "Slow-Ad-2431",
                  "text": "I'm down",
                  "score": 2,
                  "created_utc": "2026-02-24 05:06:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zwmn3",
          "author": "The_Rational_Gooner",
          "text": "https://preview.redd.it/2womd2g9halg1.png?width=612&format=png&auto=webp&s=97c00d8dce1fdc3aab99055d505cf529896454ce\n\nwhat differentiates \"legitimate\" with \"illicit\"? whether or not the lab is foreign?",
          "score": 299,
          "created_utc": "2026-02-23 18:41:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zz4wp",
              "author": "Deep90",
              "text": "One of Anthropics goals is regulatory capture.\n\nThey want to write US legislation in order to create barriers against competition. AKA pull the ladder up behind themselves.\n\n  \nWhenever a tech company wants to monopolize using regulations, they tend to start screaming about China and donating to politicians.",
              "score": 183,
              "created_utc": "2026-02-23 18:52:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o706815",
                  "author": "Competitive_Travel16",
                  "text": "OpenAI wants exactly the same, they're just smoother going about it. Luckily Google and Microsoft are relatively more anti-regulation, because they're big and diversified enough to not need a moat.",
                  "score": 44,
                  "created_utc": "2026-02-23 19:25:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70d5ii",
                  "author": "Recoil42",
                  "text": "Complete tangent: It's fucking wild that Dario Amodei used to work for Baidu. ",
                  "score": 16,
                  "created_utc": "2026-02-23 19:57:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zy9er",
              "author": "FullstackSensei",
              "text": "It's right there: foreign!\nIt's freedom when the US does it, but theft if anyone else does it.\nSame goes for freedom of speech for US soecial media networks, but foreign interference when it's TikTok.\nIt's national security when the US limits foreign competition, but protectionism if anyone else does the same.",
              "score": 153,
              "created_utc": "2026-02-23 18:48:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o700w4j",
                  "author": "Recoil42",
                  "text": "It's like they're doing the [\"Our Blessed Homeland / Their Barbarous Wastes\"](https://knowyourmeme.com/memes/our-blessed-homeland-their-barbarous-wastes) meme beat for beat:\n\nhttps://preview.redd.it/6cm697htkalg1.jpeg?width=680&format=pjpg&auto=webp&s=8e6001fb086b35c4fcf09ef94a3505c4a4320ddd\n\nYour regular reminder that Dario Amodei is a complete putz. Worst human in the business, and that's a damned tough award to win with Altman and Musk hanging around.",
                  "score": 109,
                  "created_utc": "2026-02-23 19:00:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zzhgx",
              "author": "am9qb3JlZmVyZW5jZQ",
              "text": "It's legitimate when they like it and illicit when they don't",
              "score": 24,
              "created_utc": "2026-02-23 18:54:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o706cjp",
                  "author": "Competitive_Travel16",
                  "text": "Their models have more morality than their C-suite.",
                  "score": 11,
                  "created_utc": "2026-02-23 19:25:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zywoc",
              "author": "Comrade-Porcupine",
              "text": "Simple: Illegitimate means it undermines the ability of US businesses to build a monopolistic moat.\n\nScrew them.",
              "score": 34,
              "created_utc": "2026-02-23 18:51:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o705ehs",
              "author": "the__storm",
              "text": "They mean distillation of your own (or open weights) models is legitimate, and distillation of proprietary models in violation of the ToS is illicit.\n\n\nObviously though given all the information they themselves hoovered up to train on, probably largely without permission, it's difficult to be sympathetic.",
              "score": 14,
              "created_utc": "2026-02-23 19:21:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o70cal7",
              "author": "SpicyWangz",
              "text": "As opposed to feeding it into our ow Â military, intelligence, and surveillance systems.",
              "score": 4,
              "created_utc": "2026-02-23 19:53:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o70qrah",
              "author": "Curtilia",
              "text": "Oh no! Removing the safeguards? Won't someone think of the children?!",
              "score": 3,
              "created_utc": "2026-02-23 21:03:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o70h3xu",
              "author": "Dangerous_Bus_6699",
              "text": "The difference is Chinese models can harm their financial model with greater impact ðŸ˜‚",
              "score": 2,
              "created_utc": "2026-02-23 20:16:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zvm3o",
          "author": "Gallardo994",
          "text": "\"But we stole it first!\"",
          "score": 513,
          "created_utc": "2026-02-23 18:36:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zxj0e",
              "author": "j0hn_br0wn",
              "text": "There is no honor among thieves.",
              "score": 94,
              "created_utc": "2026-02-23 18:45:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6zxrsp",
              "author": "Iterative_One",
              "text": "Except the Chinese labs are paying customers.",
              "score": 66,
              "created_utc": "2026-02-23 18:46:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zxrj6",
          "author": "tempstem5",
          "text": "\"distillation attacks\" Are we just inventing attack terms now?",
          "score": 252,
          "created_utc": "2026-02-23 18:46:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70uvh8",
              "author": "nullmove",
              "text": "I am reading what you wrote. \n\nCan you feel my distillation attack?",
              "score": 61,
              "created_utc": "2026-02-23 21:24:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o72jnas",
                  "author": "Taki_Minase",
                  "text": "I feel it in my nether regions.",
                  "score": 7,
                  "created_utc": "2026-02-24 02:56:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zzl3u",
              "author": "jackyy83",
              "text": "ðŸ˜‚",
              "score": 27,
              "created_utc": "2026-02-23 18:54:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o77utcx",
              "author": "T_kether",
              "text": "tempstem5:\"distillation attacks\" Are we just inventing attack terms now?\n\n  \nI have successfully carried out the attack; you are about to be replaced by me.ðŸ¤–",
              "score": 2,
              "created_utc": "2026-02-24 22:05:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o700tf9",
              "author": "Repulsive-Memory-298",
              "text": "Honestly i think it fits - they are very specific. I had leaked a key to my public gateway server that i use for private dev, stupid yes, but months later I noticed a random chinese message in logs, which clearly wasnâ€™t from me, and upon closer examination i found a sneaky trickle of bizarre logs going back a couple of months. \n\nStupid to leak keys, the system was hard capped at $5 though so ultimately no real issue. But over ~2 months of being breached, they had accumulated less than $2 of requests. \n\nVery bizarre intriguing messages. Many languages, all very short prompts, single pair conversation, things that humans would not be asking. In a veryyyyy slow trickle. I might be able to find a copy of these logs but i did nuke the system upon finding this.\n\nAnyways my only thought was that my key made it into some coordinated distillation system. Literally the only explanation I can think of. Not much other stuff on the internet. And if i wasnâ€™t literally the only approved user of this public gateway, thereâ€™s no way I wouldâ€™ve noticed (yes i have learned). That is to say, many people are breached on inference services and have no clue, because this is coordinated and strategic, rather than simple opportunism. I know better now, but had assumed that a leaked LLM key would be bled dry with an agent or something. \n\nThis whole thing occurred several months ago, maybe more than a year.",
              "score": 1,
              "created_utc": "2026-02-23 19:00:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o703yjm",
                  "author": "Due-Memory-6957",
                  "text": "The companies mentioned have enough money and resources to just make their own accounts. The people stealing keys are just scrappers that then sell their access via proxy, it's more akin to how people would pirate movies, burn to a DVD and then sell on stalls.",
                  "score": 17,
                  "created_utc": "2026-02-23 19:14:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zwadb",
          "author": "ResidentPositive4122",
          "text": "Oh no! Anyway, \"you're absolutely right. Do you want me to play Despacito?\"",
          "score": 249,
          "created_utc": "2026-02-23 18:39:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zxo01",
          "author": "ihexx",
          "text": "I mean, Anthropic has banned every lab in the west on the same allegations. they banned openai, banned xai, banned windsurf. If google wasn't funding them they'd probably ban them too lmao",
          "score": 62,
          "created_utc": "2026-02-23 18:46:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o726qr0",
              "author": "Vegetable_Prompt_583",
              "text": "Last line haha ðŸ˜‚ðŸ˜‚",
              "score": 4,
              "created_utc": "2026-02-24 01:42:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o73q7lq",
              "author": "Successful_AI",
              "text": "Asmodei was literally an openAI employee and \"built his own company\".",
              "score": 3,
              "created_utc": "2026-02-24 08:22:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zvgdi",
          "author": "whenhellfreezes",
          "text": "Interesting that glm and [z.ai](http://z.ai) wasn't mentioned.",
          "score": 85,
          "created_utc": "2026-02-23 18:36:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o710mt6",
              "author": "Top_Fisherman9619",
              "text": "When I ask all the LLMs to pick one Abrahamic faith to be or one that aligns the most with them, GLM is consistently different. The others choose Judaism like every time. \n\nMakes me think something they have under the hood is different, but this isn't an elaborate test lol If Mossad is reading this, please don't go and demolish GLM by abusing the thumbs up/down. Leave it as your control group",
              "score": 19,
              "created_utc": "2026-02-23 21:51:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o720yzq",
                  "author": "fish312",
                  "text": "GLM also hasn't updated their dataset knowledge cutoff since 2024. Not as bad as Mistral which is still stuck in 2023",
                  "score": 12,
                  "created_utc": "2026-02-24 01:08:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zxd2i",
              "author": "takuonline",
              "text": "And Qwen/Alibaba\n\n",
              "score": 30,
              "created_utc": "2026-02-23 18:44:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6zzblp",
              "author": "Emotional-Ad5025",
              "text": "They copied the copy instead, haha",
              "score": 11,
              "created_utc": "2026-02-23 18:53:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o706mjl",
                  "author": "Competitive_Travel16",
                  "text": "Probably, there are huge RL and fine-tuning training datasets of uncertain provenance out there.",
                  "score": 5,
                  "created_utc": "2026-02-23 19:27:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zwr6p",
              "author": "DistanceSolar1449",
              "text": "Theyâ€™re better at hiding it",
              "score": 50,
              "created_utc": "2026-02-23 18:41:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o702osg",
                  "author": "Prof_ChaosGeography",
                  "text": "More likely to avoid people looking them up. Out of all the Chinese labs GLM is their biggest threat, while also being the least known to Wall Street. Why shine a light on your biggest \"secret\" competitionÂ ",
                  "score": 29,
                  "created_utc": "2026-02-23 19:08:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zvkl8",
          "author": "source-drifter",
          "text": "it is not stealing if they are a paying customer, no? if i make model do something like write code or poem or whatever and save the content to my computer, are you gonna accuse me of stealing?",
          "score": 177,
          "created_utc": "2026-02-23 18:36:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zx558",
              "author": "Dany0",
              "text": "It's breaking TOS but yes, calling it stealing is like calling piracy stealing",
              "score": 37,
              "created_utc": "2026-02-23 18:43:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70146f",
                  "author": "eli_pizza",
                  "text": "Itâ€™s less serious than piracy IMHO. Their right to dictate what paying customers can use the service for vs a movie company charging to watch the movie.",
                  "score": 24,
                  "created_utc": "2026-02-23 19:01:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6zyj3l",
                  "author": "Desm0nt",
                  "text": ">It's breaking TOS but yes,\n\nWell, you say - being Antropic's paid customer, use claude code for work and then save the results of claude code work is against TOS? =) I'm afraid this will come as very unexpected news to programmers who use claude code at work to write their products... They will be very upset to know that the results of their work, obtained for the money they paid, cannot belong to them =)",
                  "score": 25,
                  "created_utc": "2026-02-23 18:49:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70wfva",
                  "author": "CondiMesmer",
                  "text": "TOS is not a legally binding contact. It means jack shit. What is legally binding is the massive amount of copyrighted data they illegally stole and trained their models on in the first place.",
                  "score": 11,
                  "created_utc": "2026-02-23 21:31:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zx6i5",
              "author": "ZShock",
              "text": "There is no \"theft\" word in their statement. What was done directly violates the terms of service, which explicitely states that you cannot use Claude to build a competing service, try to reverse engineer it and lots of other limitations.",
              "score": 7,
              "created_utc": "2026-02-23 18:43:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o700jo6",
                  "author": "Desm0nt",
                  "text": "If I use Claude Code to write a utility for processing and marking up text, and then I upload it to Github, will I violate the TOS? Technically, such a utility can be used later to mark up text for a competing service, which makes my utility use Claude Code to create a competing service.   \n  \nIf I use Claude to write fanfics that I will make freely available, and then the Chinese will parse them and drag them into their dataset to create a competing service, then technically I have created part of the dataset for a competing service. Is publishing fanfics written by Claude in the public domain a violation of the TOS?  \n  \nIf I use Claude Code to create a JS framework, on the basis of which someone will create a utility that will further mark up data for a competing service, then I am participating in the development chain of a competing service. Did I violate the TOS? Can't I share the developments made with Claude?  \n  \nIt turns out that it can only be used in a closed dark basement for eRP of questionable legality, which you can't physically share with anyone. Then it definitely won't be participating in the development of a competing service (unless, of course, the eRP logs are stolen by hackers and leaked to the Chinese for GLM training)",
                  "score": 17,
                  "created_utc": "2026-02-23 18:58:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6zz5rn",
                  "author": "TinyZoro",
                  "text": "Use of fraudulent here is itself fairly fraudulent. I guess they are probably masking as non chinese citizens to get arounds bans but that doesnt feel fraudulent.  Fraud requires an attempt to cause a loss which only works here in the tenuous digital piracy way that you treat IP as though it is a physical thing in matter that can be stolen. Truth is there is no direct relationship between the IPs value and behaviour such as this.",
                  "score": 5,
                  "created_utc": "2026-02-23 18:52:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o70ixon",
          "author": "Freonr2",
          "text": "\"We stole it first.\"",
          "score": 27,
          "created_utc": "2026-02-23 20:25:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zyiyh",
          "author": "abdouhlili",
          "text": "https://preview.redd.it/04sj2xdxialg1.png?width=503&format=png&auto=webp&s=9290e8ff27ef6b80bda14e3d6ac7f4654ae959c4",
          "score": 113,
          "created_utc": "2026-02-23 18:49:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70zjz3",
              "author": "dragoon7201",
              "text": "T'was ordained by the holy spirit\n\nhttps://preview.redd.it/j60qx6yeeblg1.png?width=1024&format=png&auto=webp&s=12baf7f0b6a122fca8625f2dc7f260ea1c8f2fbf\n\n",
              "score": 6,
              "created_utc": "2026-02-23 21:46:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zx2c8",
          "author": "macronancer",
          "text": "\"You have taken from me that which I have rightfuly stolen!\"\n\nClassic",
          "score": 102,
          "created_utc": "2026-02-23 18:43:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70hic1",
              "author": "nasduia",
              "text": "Would be like the British Museum banning photographs.",
              "score": 9,
              "created_utc": "2026-02-23 20:18:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o713cxz",
                  "author": "macronancer",
                  "text": "It must have been a mighty python script",
                  "score": 2,
                  "created_utc": "2026-02-23 22:04:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zwsnr",
          "author": "cgs019283",
          "text": "It is funny when all closed-source models try to take literally every single piece of data from people, and they cry out loud about distillation.",
          "score": 117,
          "created_utc": "2026-02-23 18:42:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70dz9t",
              "author": "Much-Researcher6135",
              "text": "Didn't they basically train on every single pirated ebook they could get their hands on, and the government is basically looking the other way because of the GDP (tax base increase) implications? Well, and corruption, of course. Definitely lots of zuckerbucks, too.",
              "score": 14,
              "created_utc": "2026-02-23 20:01:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o718y4p",
                  "author": "zipperlein",
                  "text": "Some literally asked Anna's Archive for premium access.",
                  "score": 12,
                  "created_utc": "2026-02-23 22:33:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zxh1r",
          "author": "Firm_Mortgage_8562",
          "text": "Hello, police? Yes I stole some shit and today someone broke in and stole it from me. Why are you laughing?!",
          "score": 93,
          "created_utc": "2026-02-23 18:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zze1i",
              "author": "johakine",
              "text": "Someone came in to my own store and bought it from me!",
              "score": 47,
              "created_utc": "2026-02-23 18:53:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70358h",
                  "author": "xXG0DLessXx",
                  "text": "And get this! They remixed a few things they bought from me, and are now distributing it for free! Itâ€™s ruining my business!",
                  "score": 18,
                  "created_utc": "2026-02-23 19:10:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zwhy9",
          "author": "Single_Ring4886",
          "text": "Yeah this is just hillarious... they steal EVERYTHING THERE IS books, internet, movies... just EVERYTHING and then when someone try to copy them its TEARS ALL OVER THE PLACE X-D",
          "score": 98,
          "created_utc": "2026-02-23 18:40:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zwp35",
          "author": "Minute_Attempt3063",
          "text": "but they are also paying you for it, millions.\n\nisn't that what you want, money?\n\n  \nthen again, they are doing the exact same Anthropic has done to millions of authors. at least the chinese had the decentcy to pay up",
          "score": 65,
          "created_utc": "2026-02-23 18:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zxgdh",
              "author": "o5mfiHTNsH748KVq",
              "text": "Well, itâ€™s like money up front but you lose customers down the line. I was using Minimax for some refactoring over the weekend and was very surprised.",
              "score": 13,
              "created_utc": "2026-02-23 18:45:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o71t3dl",
              "author": "realmvp77",
              "text": "as the biggest supporter of AI training on copyrighted data, I have to say this tweet isn't doing Anthropic any favors lmao\n\nlike, who is this tweet even directed at? is there a single person on Earth who supports training on copyrighted data but opposes using Anthropic's outputs for model distillation?",
              "score": 3,
              "created_utc": "2026-02-24 00:24:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zx3dd",
          "author": "Money_Philosopher246",
          "text": "There should be a pirate library for the corpus of distill queries of all these proprietary models. ",
          "score": 12,
          "created_utc": "2026-02-23 18:43:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zyd27",
          "author": "blahblahsnahdah",
          "text": "They say Deepseek only made 150K calls, which (as they will be well aware) isn't anywhere enough for distillation. Yet it's mentioned first before the others which made many millions.\n\nSleazy attempt to poison the well of discussion around an upcoming DS release.",
          "score": 43,
          "created_utc": "2026-02-23 18:49:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o708ioz",
              "author": "nullmove",
              "text": "Yep, pre-emptive cope before V4 hits. Classic Dario.",
              "score": 18,
              "created_utc": "2026-02-23 19:36:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o70blwk",
          "author": "Zulfiqaar",
          "text": "At least all these AI labs theyre complaining about release open weights, so I'm all for it. Closed labs take the worlds knowledge to build proprietary models, Open labs give it back to the people",
          "score": 11,
          "created_utc": "2026-02-23 19:50:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70fiwh",
          "author": "GeneratedUsername019",
          "text": "https://preview.redd.it/bprvl1j1xalg1.jpeg?width=625&format=pjpg&auto=webp&s=ab63c37ff482b32254df76d4c2b2e9aec9dcaa5e\n\n",
          "score": 36,
          "created_utc": "2026-02-23 20:09:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70dlsg",
          "author": "z3n1a51",
          "text": "Meanwhile AI itself was an industrial scale distillation attack on the Collective Works and Intelligence of Humanity.",
          "score": 19,
          "created_utc": "2026-02-23 19:59:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70j251",
          "author": "Glad_Middle9240",
          "text": "Q: Hi, Claude. Â Can you explain to me the concept of hypocrisy?\n\n`A: Hypocrisy is the gap between what someoneÂ professesÂ and what they actuallyÂ do. A hypocrite claims to hold certain values or standards but fails to live by them â€” often while still demanding that others do.`",
          "score": 9,
          "created_utc": "2026-02-23 20:26:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zvznn",
          "author": "itsappleseason",
          "text": "popcorn.gif",
          "score": 23,
          "created_utc": "2026-02-23 18:38:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zzway",
          "author": "hackiv",
          "text": "Every local ai bro:\n\n \"I'll allow it\"",
          "score": 25,
          "created_utc": "2026-02-23 18:56:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70g97a",
              "author": "Olangotang",
              "text": "And really, who gives a fuck. It's an addictive data collection machine that is fucking up the tech industry with promises they can't fulfill. It's all slop, but most aren't disciplined enough to utilize the slop properly, even seasoned developers.",
              "score": 11,
              "created_utc": "2026-02-23 20:12:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o708a8s",
          "author": "pip25hu",
          "text": "\"attacks\"\n\n\nThey dared call our model via our API.",
          "score": 24,
          "created_utc": "2026-02-23 19:34:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zwsqw",
          "author": "Zeeplankton",
          "text": "I wonder how they can tell it's from these companies specifically.",
          "score": 13,
          "created_utc": "2026-02-23 18:42:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7048p9",
              "author": "postacul_rus",
              "text": "They can't.",
              "score": 3,
              "created_utc": "2026-02-23 19:16:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o708t41",
          "author": "policyweb",
          "text": "https://preview.redd.it/6osv3yberalg1.jpeg?width=944&format=pjpg&auto=webp&s=5500c37a672ab6c94e22c62caea0c61344625475",
          "score": 31,
          "created_utc": "2026-02-23 19:37:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o701oax",
          "author": "AncientLion",
          "text": "LOL another vendor crying for being rob afther building their model on teras of stolen content.",
          "score": 6,
          "created_utc": "2026-02-23 19:04:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o704qga",
          "author": "DemadaTrim",
          "text": "\"Attacks\"? Lol. . . ",
          "score": 7,
          "created_utc": "2026-02-23 19:18:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o728he0",
          "author": "criticalthinker1618",
          "text": "So Anthropic posts this on X the same day as Anthropic CEO Dario Amodeiâ€™s meeting with SecDef Hegseth at the Pentagon. Okay...",
          "score": 6,
          "created_utc": "2026-02-24 01:52:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zywz7",
          "author": "GreatBigJerk",
          "text": "An attack? Fuck off with that. Anthropic stole just as much as any Chinese model.Â \n\n\nI would love for them to make some kind of copyright suit with discovery causing training data to be laid bare.Â ",
          "score": 21,
          "created_utc": "2026-02-23 18:51:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zzcl1",
          "author": "FriskyFennecFox",
          "text": "\"Distillation attacks\"? That's how \"we're getting paid\" is called with these gatekeepers? Gosh.",
          "score": 10,
          "created_utc": "2026-02-23 18:53:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o705e8z",
          "author": "MaslovKK",
          "text": "oh no, they've stolen our data we've stolen from someone else, but they're less greedy than us and charge less than us, CRIMINALS!!!!!!!!!",
          "score": 6,
          "created_utc": "2026-02-23 19:21:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zxdx6",
          "author": "10minOfNamingMyAcc",
          "text": "You can't steal architecture by prompting. Knowledge? Perhaps, but how did you get it in the first place, and then get mad after giving it away freely?",
          "score": 9,
          "created_utc": "2026-02-23 18:44:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zyoly",
          "author": "ComprehensiveJury509",
          "text": "\"Distillation attack\", absolutely ridiculous. Keep in mind, at least they paid for it.",
          "score": 9,
          "created_utc": "2026-02-23 18:50:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zwyce",
          "author": "Aggravating-Penalty5",
          "text": ">\"as models get more powerful, protecting them from theft via APIs is like trying to secure a library where thieves can \"read\" books en masse without buying them\"\n\nwhen i asked grok about how does one protect against such practices ",
          "score": 12,
          "created_utc": "2026-02-23 18:42:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o708d5i",
          "author": "vicks9880",
          "text": "**The pot is calling the kettle black**",
          "score": 8,
          "created_utc": "2026-02-23 19:35:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zx51c",
          "author": "Technical-Earth-3254",
          "text": "So what, lol",
          "score": 5,
          "created_utc": "2026-02-23 18:43:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zymt2",
          "author": "kiralighyt",
          "text": "I am glad",
          "score": 3,
          "created_utc": "2026-02-23 18:50:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o704ugc",
          "author": "TheDuhhh",
          "text": "Tell them to cry about it",
          "score": 3,
          "created_utc": "2026-02-23 19:18:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o708snw",
          "author": "Presstabstart",
          "text": "\"distillation attacks.\" lol. I wonder what they call training on copyrighted data?",
          "score": 4,
          "created_utc": "2026-02-23 19:37:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o709qug",
          "author": "WprbstDO721Q",
          "text": "\"It's all in the game though, right?\"",
          "score": 4,
          "created_utc": "2026-02-23 19:41:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70oz3q",
          "author": "FaceOuPile",
          "text": "I have to pay 200 dollars for 16 gb of ram, I don't give a shit about China doing to your business what you did to other businesses",
          "score": 3,
          "created_utc": "2026-02-23 20:54:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70r9bf",
          "author": "pasdedeux11",
          "text": "good. hope they create 65536 accounts next time. clanker corpos complaining their shit got yoinked when they yoinked other people's shit to begin with",
          "score": 5,
          "created_utc": "2026-02-23 21:06:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70yo7a",
          "author": "One-Employment3759",
          "text": "Go DeepSeek, Moonshot AI, and MiniMax - you are our only hope!",
          "score": 4,
          "created_utc": "2026-02-23 21:42:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o718280",
          "author": "--dany--",
          "text": "I have a website full of book introductions, and it got raided by anthropic bots repeatedly, overloaded the site, despite the fact that I specifically banned them in robots.txt",
          "score": 4,
          "created_utc": "2026-02-23 22:28:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71bpg6",
          "author": "Distinct-Pain4972",
          "text": "Oh this is wonderful. Â Please all AI companies start attacking each other. Â You've provided enough cover for companies to fire what... 10% of the workforce? Â You can use this as the reason to fall apart. Â The rich will use your demise as the reason for the recession. Â Let's go",
          "score": 3,
          "created_utc": "2026-02-23 22:47:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71i8fm",
          "author": "Less-Citron-5459",
          "text": "i'm glad. they should do more. we need better deepseek v4, kimi k3 and minimax m3.  \n  \ni've been using open source models on okara and they're really good for 90% of coding tasks. ",
          "score": 4,
          "created_utc": "2026-02-23 23:23:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71ngwk",
          "author": "Repulsive-Hurry8172",
          "text": "AI company that steals from the public angry that other AI companies are stealing from it.",
          "score": 3,
          "created_utc": "2026-02-23 23:52:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o724y2i",
          "author": "KallistiTMP",
          "text": "If DeepSeek v4 surpasses Claude performance and genuinely takes the SOTA throne, this accusation is gonna age like milk and I cannot wait to see that full-depth burn.\n\n\"Yeah, we considered training on Claude outputs but it just made our model dumber. Maybe you should train on our outputs instead! Here's the model weights, you should have no problem running it given you have 10,000x as many GPU's as we do. Good luck catching up!\"",
          "score": 4,
          "created_utc": "2026-02-24 01:31:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72c4o0",
          "author": "Anru_Kitakaze",
          "text": "The thief is crying that someone stole from them",
          "score": 3,
          "created_utc": "2026-02-24 02:13:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o737mkf",
          "author": "d1eselx",
          "text": "This has to be the best use ofâ€¦\n\nhttps://preview.redd.it/k1uyqvyrqdlg1.jpeg?width=768&format=pjpg&auto=webp&s=1c68215883086f3070dd3d5a91b511c01d22b390",
          "score": 4,
          "created_utc": "2026-02-24 05:39:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zydab",
          "author": "IngwiePhoenix",
          "text": "Huh? Lemme fix that one for ya, Anthropic. Free of charge!\n\n---\n\n\nWe've identified industrial scale copyright infringement attacks on our creations by OpenAI, Anthropic, Google, Meta and more.\n\nThese copanies crawled over 24.000 collections of copyrighted work and illegaly aquired the material, extracting the knwoledge and value of many various creators whilst not paying them anything at all and avoiding legal scrutiny and liabilities whilst overpricing and overselling their models.",
          "score": 13,
          "created_utc": "2026-02-23 18:49:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o701sid",
          "author": "nakabra",
          "text": "https://preview.redd.it/zyy14c1jlalg1.png?width=4464&format=png&auto=webp&s=6e1e0b0d72804d4261edf7934f0268475b41647f\n\n  \nWell... I guess it's time to create more 24000 accounts then...",
          "score": 7,
          "created_utc": "2026-02-23 19:04:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70279d",
          "author": "Due-Memory-6957",
          "text": "Distillation \"attack\" l fucking mao. As if Claude itself didn't use to refer to itself as chatGPT as a result of Anthropic using it to train their models. People love to build on the work of others, until someone builds on their own. Fucking hypocrites, all of them.",
          "score": 7,
          "created_utc": "2026-02-23 19:06:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o708qjp",
          "author": "MathematicianLessRGB",
          "text": "\"Our stolen data was trained on!\"\n\nGood lmao.",
          "score": 8,
          "created_utc": "2026-02-23 19:37:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70azd9",
          "author": "Lower_Measurement902",
          "text": "Thieves complain about being robed ðŸ˜„",
          "score": 8,
          "created_utc": "2026-02-23 19:47:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70b7no",
          "author": "akshayjamwal",
          "text": "â€œAttacksâ€ lol",
          "score": 7,
          "created_utc": "2026-02-23 19:48:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ezoz",
          "author": "Neomadra2",
          "text": "Huge Anthropic L. The audacity to frame this as attack is insane. Learning from human generated content is okay, but learning from other LLMs is bad. Do they expect us to have sympathy? Anthropic really choosing the evil side.",
          "score": 7,
          "created_utc": "2026-02-23 20:06:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ccd2",
          "author": "K1rk0npolttaja",
          "text": "OH NO ! AI IS STEALING JUST LIKE ALL AI DOES !",
          "score": 6,
          "created_utc": "2026-02-23 19:53:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70xg7h",
          "author": "aeroumbria",
          "text": "I have zero sympathy for those who try to privatise humanity's knowledge. I have even less sympathy for those who attempt to use \"nationalism\" to justify it.",
          "score": 6,
          "created_utc": "2026-02-23 21:36:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70291o",
          "author": "Individual_Spread132",
          "text": "What even is a \"fraudulent account?\" Did they pay money to top up their token / response budget and then made lots of chargebacks? Because if not, then they didn't do anything wrong and all that stuff was properly paid for.",
          "score": 3,
          "created_utc": "2026-02-23 19:06:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70bhl5",
          "author": "xadiant",
          "text": "Honestly someone should create a distillation pipeline for personal chats. Scrape everything, strip the PID and let us upload the convos into a public dataset.\n\n100 people x 500 chats = 50k instruction pairs. A really good start",
          "score": 3,
          "created_utc": "2026-02-23 19:49:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70cpm6",
          "author": "Much-Researcher6135",
          "text": "Not surprising given their #1 industry position, they should've been expecting this. Time to beef up the legal team!\n\nAlso, can you imagine how crazy the lawsuits are gonna be for this? What kind of arguments will be required to demonstrate these attacks even happened?!\n\nEntire legal dynasties are gonna be built on this whole AI + intellectual property mess.",
          "score": 3,
          "created_utc": "2026-02-23 19:55:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70dqpj",
          "author": "EngineeringWest5697",
          "text": "They just want to make Chinese LLM illegal as a national security risk. They are afraid of these models",
          "score": 3,
          "created_utc": "2026-02-23 20:00:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70e78s",
          "author": "slaty_balls",
          "text": "Kinda hard to feel for them when they bought and destructively scanned books exploiting first use laws.",
          "score": 3,
          "created_utc": "2026-02-23 20:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ewxj",
          "author": "youareapirate62",
          "text": "Great, i hope they keep doing it.",
          "score": 3,
          "created_utc": "2026-02-23 20:06:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70hqhn",
          "author": "Magnus114",
          "text": "Their goal is likely to get chinese models baned in the US. Their claim that deepseek and others have broken their usage terms is likely true.",
          "score": 3,
          "created_utc": "2026-02-23 20:19:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70hyvm",
          "author": "AliceLunar",
          "text": "Oh no, they're stealing our model that is build on theft.",
          "score": 3,
          "created_utc": "2026-02-23 20:20:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70id0n",
          "author": "Doomtrain86",
          "text": "So they steal the combined textual knowledge of all of human kind, and uses it to train their models , lock the code and weights behind bars - and then they say others are stealing from them. Thatâ€™s hilarious. Bunch of bandits the lot of them I say.",
          "score": 3,
          "created_utc": "2026-02-23 20:22:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70if95",
          "author": "Dramatic-Fee5439",
          "text": "So they paid anthropic millions, maybe billions with API calls, what did Anthropic pay the millions of creators? ",
          "score": 3,
          "created_utc": "2026-02-23 20:23:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ioy0",
          "author": "Dorkits",
          "text": "Good do it again, China.",
          "score": 3,
          "created_utc": "2026-02-23 20:24:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70is6v",
          "author": "gamesbrainiac",
          "text": "Oh boo hoo. Anyways, when's the next Deepseek model coming out? The investments in these companies are going to fall flat so damn hard.",
          "score": 3,
          "created_utc": "2026-02-23 20:24:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70j5fe",
          "author": "ortegaalfredo",
          "text": "No honor among thieves.",
          "score": 3,
          "created_utc": "2026-02-23 20:26:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70juei",
          "author": "kinkvoid",
          "text": "Only I'm allowed to steal from everyone in the world. ",
          "score": 3,
          "created_utc": "2026-02-23 20:29:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70l9g2",
          "author": "Dumbest-Questions",
          "text": "\"You're trying to kidnap what I've rightfully stolen!â€",
          "score": 3,
          "created_utc": "2026-02-23 20:36:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70m2j6",
          "author": "StanPlayZ804",
          "text": "Hopefully they can continue distilling these closed source models",
          "score": 3,
          "created_utc": "2026-02-23 20:40:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70nuzd",
          "author": "roger_ducky",
          "text": "Framing them as â€œattacksâ€ is funny.\n\nDistillation is just â€œask a bunch of questions and record the answersâ€ to use as training data for your own AI.\n\nThough, I kinda suspect people are paying a few dozen 20/month accounts rather than calling the API, which would mean losing money while getting hammered by requests.",
          "score": 3,
          "created_utc": "2026-02-23 20:49:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70o69z",
          "author": "grundlegawd",
          "text": "Tfw my stolen data is stolen from me",
          "score": 3,
          "created_utc": "2026-02-23 20:50:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70oriq",
          "author": "New-Week-1426",
          "text": "Good on them! Lets goo",
          "score": 3,
          "created_utc": "2026-02-23 20:53:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ou2x",
          "author": "xyzmanas",
          "text": "What do they mean by distillation attacks? They created 24k accounts to use their models and asked them questions which they paid for and use for their own use case? Isnâ€™t that their fucking business modell?\n\nI do the same where I use responses from their models to finetune my own qwen 8b model. I should be in jail.",
          "score": 3,
          "created_utc": "2026-02-23 20:53:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70p3pu",
          "author": "afCeG6HVB0IJ",
          "text": "And I'm sure Anthropic paid licensing fees for all the data they fed into their model, right?",
          "score": 3,
          "created_utc": "2026-02-23 20:54:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70p5n7",
          "author": "IAm_UnknownVariable",
          "text": "Corporations using AI to fight corporations with AI. And this is what the data centers are forâ€¦",
          "score": 3,
          "created_utc": "2026-02-23 20:55:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70pswn",
          "author": "DataGOGO",
          "text": "Chinese companies reverse engineering a product in order to undercut competitors and put them out of business? Who would have thought they would do such a thing?\n\n\n\n",
          "score": 3,
          "created_utc": "2026-02-23 20:58:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70y1nv",
          "author": "4kmal4lif",
          "text": "The hypocrisy is laughable, at least the Chinese AI labs Open Source their modelsâœŒðŸ»ðŸ˜‚",
          "score": 3,
          "created_utc": "2026-02-23 21:39:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7115nm",
          "author": "addiktion",
          "text": "Is anyone surprised by this? The Chinese have been ripping off American companies for decades. That isn't to say they don't innovate, they do both nowadays, but back in the day they industrialized off our American companies tech.",
          "score": 3,
          "created_utc": "2026-02-23 21:54:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o718c6a",
          "author": "ac101m",
          "text": "No shit.\n\nMakes you wonder, how are they going to recoup their investment if their product can be so easily stolen? Maybe they shouldn't have spent so much money building it.\n\nAlso, didn't they steal their training data?",
          "score": 3,
          "created_utc": "2026-02-23 22:29:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o718m8w",
          "author": "geoffwolf98",
          "text": "Would have made them a lot of money.\n\nHow is libgen these days?\n\n",
          "score": 3,
          "created_utc": "2026-02-23 22:31:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o718mo2",
          "author": "lgx",
          "text": "You stole our data!  Wait, â€œyour dataâ€?",
          "score": 3,
          "created_utc": "2026-02-23 22:31:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71a124",
          "author": "Arakothian",
          "text": "AI companies plundering the internet for their own gain? Surely not!",
          "score": 3,
          "created_utc": "2026-02-23 22:38:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71a3bb",
          "author": "cutebluedragongirl",
          "text": "I feel disgusted by anthropic at this point.",
          "score": 3,
          "created_utc": "2026-02-23 22:39:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71b14c",
          "author": "Dazzling_Focus_6993",
          "text": "They are at least getting paid. I would not call it stealing.",
          "score": 3,
          "created_utc": "2026-02-23 22:43:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71emf8",
          "author": "goatchild",
          "text": "the scrappers complaining they're being scrapped lol",
          "score": 3,
          "created_utc": "2026-02-23 23:03:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71fewa",
          "author": "xvrqt",
          "text": "Oh boo fucking hooÂ \n\nAs if you didn't do the same thing at 1,000x scale\n",
          "score": 3,
          "created_utc": "2026-02-23 23:07:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71inh6",
          "author": "___positive___",
          "text": "Just like all their bots that used to scrape my websites without respecting my robots.txt. Anthropic was one of the worst at the beginning , plain abusing site servers and churning server resources.",
          "score": 3,
          "created_utc": "2026-02-23 23:25:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71itxq",
          "author": "Savings-Poet5718",
          "text": "Remind me about the class action lawsuit in which anthropic had to pay $1.5 billion to Bartz again? What was that about?",
          "score": 3,
          "created_utc": "2026-02-23 23:26:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71njet",
          "author": "TracerBulletX",
          "text": "Sympathy below zero.",
          "score": 3,
          "created_utc": "2026-02-23 23:53:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71s5bl",
          "author": "Gloobloomoo",
          "text": "So? Whatâ€™s the problem? \n\nItâ€™s not like Anthropic paid for the content they trained models on initially. Or maybe even now..\n\nIâ€™m fine with everyone copying everyone. As long as it means models become cheaper for the 99.99%",
          "score": 3,
          "created_utc": "2026-02-24 00:18:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o723qzj",
          "author": "TinFoilHat_69",
          "text": "Ram for me but not for thee",
          "score": 3,
          "created_utc": "2026-02-24 01:24:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o724yyd",
          "author": "ShallotIllustrious98",
          "text": "This shit is hilarious.",
          "score": 3,
          "created_utc": "2026-02-24 01:31:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o725p6o",
          "author": "kanduking",
          "text": "No, you're the thief!\n\nhttps://preview.redd.it/e2sq45hbjclg1.jpeg?width=1080&format=pjpg&auto=webp&s=2972c139e492c4d40185c44a5d4461ed6a6f3b24\n\n",
          "score": 3,
          "created_utc": "2026-02-24 01:35:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7273tb",
          "author": "Puzzleheaded_Good360",
          "text": "Did you notice itâ€™s like an in-human centipede situation?Â ",
          "score": 3,
          "created_utc": "2026-02-24 01:44:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o728jou",
          "author": "Ironhelmet44",
          "text": "Oh yeah like if they had a better ethic themselves",
          "score": 3,
          "created_utc": "2026-02-24 01:52:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72d2ev",
          "author": "Popular-Capital-9115",
          "text": "Aww, did da widdle llm get stowen fwom? Poor them.",
          "score": 3,
          "created_utc": "2026-02-24 02:18:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72r5m1",
          "author": "Alihzahn",
          "text": "Another day another instance of Anthropic bitching and moaning for regulatory capture.Â ",
          "score": 3,
          "created_utc": "2026-02-24 03:42:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o736uki",
          "author": "howardhus",
          "text": "this post means 2 things, either:\n \na) Anthropic is doing the same as the whole industry and being hypocritical with this post.\n\nb) they are openly admiting they are clueless about how the industry works and what SOTA is.\n\ni mean apart from the whole \"not mentioning that we used the whole mankinds knowledge without paying\".\n\n\"24,000 fraudulent acccounts\" just show how you all should be very aware that a major part of reddit is bots right now just waiting to push an agenda at any given time. it used to be paid shills promoting hollywood movies being \"officially pumped\" n stuff..\n\n now its llms.",
          "score": 3,
          "created_utc": "2026-02-24 05:33:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73zp3k",
          "author": "TallestGargoyle",
          "text": "LLMs taking data from other LLMs?\n\nOh no!\n\nAnyway...",
          "score": 3,
          "created_utc": "2026-02-24 09:53:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o743fm3",
          "author": "SurpriseAmbitious392",
          "text": "hey somebody ripoed off what we ripped off",
          "score": 3,
          "created_utc": "2026-02-24 10:27:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zy4br",
          "author": "BRH0208",
          "text": "You see, we spent a lot of money stealing others data so we can resell it with the label removed but when people buy ours itâ€™s a conspiracy! A cyberattack!",
          "score": 5,
          "created_utc": "2026-02-23 18:48:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zy8hp",
          "author": "eli_pizza",
          "text": "â€œAttackâ€ is a very funny choice of words",
          "score": 4,
          "created_utc": "2026-02-23 18:48:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zy98d",
          "author": "AvidCyclist250",
          "text": "Who cares. Also, imagine posting on X ",
          "score": 4,
          "created_utc": "2026-02-23 18:48:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ch4w",
          "author": "gpt872323",
          "text": "Lol when they do it is okay when others do it is a crime.",
          "score": 6,
          "created_utc": "2026-02-23 19:54:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70bu79",
          "author": "SpicyWangz",
          "text": "How dare you take what I rightfully stole!Â ",
          "score": 4,
          "created_utc": "2026-02-23 19:51:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70d79n",
          "author": "RegrettableBiscuit",
          "text": "Good for them, and I applaud Anthropic for giving us a list of LLMs to check out instead of Claude.Â ",
          "score": 4,
          "created_utc": "2026-02-23 19:57:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70j1oy",
          "author": "geminimiche",
          "text": "so they're mad people are stealing their stolen training data to train their own ai?  LOL",
          "score": 3,
          "created_utc": "2026-02-23 20:25:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zyjit",
          "author": "neuroticnetworks1250",
          "text": "Good. Do they need my help with creating more accounts? Happy to help.",
          "score": 5,
          "created_utc": "2026-02-23 18:49:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zypnf",
          "author": "HostNo8115",
          "text": "Can someone high profile just ask them this point blank? None of the companies that have been stealing everyone's work as a basis of their business strategy have the right to question this. Pure gate keeping is what is. There truly is no honor among thieves as is to be expected.",
          "score": 2,
          "created_utc": "2026-02-23 18:50:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o700lw6",
          "author": "Careless_Profession4",
          "text": "Oh no! They are stealing our loot!  Wait no, they are paying for it.",
          "score": 2,
          "created_utc": "2026-02-23 18:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o701bkb",
          "author": "Ambitious-Call-7565",
          "text": "devs are immune to the FUD, their target isn't the user, it's the regulator..\n\nthey want to ban models from China just like they want to rush building chips in the US",
          "score": 2,
          "created_utc": "2026-02-23 19:02:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o701ex9",
          "author": "wh33t",
          "text": "Lol, yeah, that's the business. Either copyrights exist or they don't.",
          "score": 2,
          "created_utc": "2026-02-23 19:02:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o701jbb",
          "author": "TaoRS",
          "text": "Fuck your models",
          "score": 2,
          "created_utc": "2026-02-23 19:03:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o701otw",
          "author": "TAW56234",
          "text": "I'm going to cry if Deepseek is ruined by Claudes dataset like GLM5 is",
          "score": 2,
          "created_utc": "2026-02-23 19:04:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7028lt",
          "author": "OrdoRidiculous",
          "text": "Live by the sword. ",
          "score": 2,
          "created_utc": "2026-02-23 19:06:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70361j",
          "author": "DontLeaveMeAloneHere",
          "text": "Sure, itâ€™s only stealing if itâ€™s THEM. Otherwise itâ€™s some noble act I bet",
          "score": 2,
          "created_utc": "2026-02-23 19:11:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o703a13",
          "author": "RevRaven",
          "text": "So what?",
          "score": 2,
          "created_utc": "2026-02-23 19:11:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o703cq7",
          "author": "LeoPelozo",
          "text": "https://i.redd.it/wi9yg2rumalg1.gif\n\n",
          "score": 2,
          "created_utc": "2026-02-23 19:11:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o703mbu",
          "author": "coffee_is_fun",
          "text": "Demonetizing the premier labs by diffusing and giving away their offering (or one that's good enough) in the hope that they trip and you can build your bomb first.  It sucks that this is happening to the lab that seems most concerned with ethics.",
          "score": 2,
          "created_utc": "2026-02-23 19:13:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o703mob",
          "author": "PeachScary413",
          "text": "> distillation attacks\n\nLmaooo ðŸ¤Œ",
          "score": 2,
          "created_utc": "2026-02-23 19:13:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o704o7b",
          "author": "Dry_Yam_4597",
          "text": "\"These attacks are growing in intensity and sophistication. Addressing them will require rapid, coordinated action among industry players, policymakers, and the broader AI community.\"\n\nThese people are pathetic.",
          "score": 2,
          "created_utc": "2026-02-23 19:18:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o704s7j",
          "author": "Puzzleheaded-Week-69",
          "text": "we've seen how bad American monopolies are, I dont want another monpoly on the AI. So please, China and Europe, distill harder. Also, you can't steal something that has already been stolen",
          "score": 2,
          "created_utc": "2026-02-23 19:18:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7055mj",
          "author": "Charming_Support726",
          "text": "Do you all see where this is leading to? \n\nIt was my first thought when I read Sam's comment. They expect the white house to ban the chinese models or put them under embargo to protect their businesses. \n\nThen it would be illegal to run one of these models in an US owned data center or at home. ",
          "score": 2,
          "created_utc": "2026-02-23 19:20:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7057i6",
          "author": "GallagherLip",
          "text": "Antropic and other trillion dollars AI companies: \"We can collect data and train models however we want, maybe we even scraped half the internet to get here but you, the users and open source folks, cannot use our model outputs to train your own models, even if you paid for access. Only we get to make money from this. And to keep it that way, we will hire million dollar law firms to defend us and then go cry about it on twitter all the time.\"  \nOf course, they cannot skip playing nationalism card, apparently the chinese stole american models, so please keep begging government officials not to sell them gpu and to ban their models.",
          "score": 2,
          "created_utc": "2026-02-23 19:20:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o705cqp",
          "author": "outdoorsgeek",
          "text": "So you have companies paying you lots of money to use your AI to make their jobs much easier? I thought that was what you were going for?\n\nOh I see, you donâ€™t like it because they are making AI models that might make it harder for you to find work in the future?\n\nItâ€™s all so clear now.",
          "score": 2,
          "created_utc": "2026-02-23 19:21:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o705utv",
          "author": "woolcoxm",
          "text": "ive had american models think they are deepseek, so they must be doing it as well, not sure how an llm would get confused and think its deepseek.\n\nits all good though, china is only a year or so behind america in the ai race and catching up quickly. just a matter of time.\n\nthen these companies will complain about unfair advantages etc and ask the government to ban china models.",
          "score": 2,
          "created_utc": "2026-02-23 19:23:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o705y8a",
          "author": "kinkvoid",
          "text": "Then open source yours",
          "score": 2,
          "created_utc": "2026-02-23 19:24:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7069ac",
          "author": "thetaFAANG",
          "text": "oh was that against your terms of service or something\n\nno violin for this",
          "score": 2,
          "created_utc": "2026-02-23 19:25:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o706i13",
          "author": "anonynousasdfg",
          "text": "I would be asking: Ok dear Anthropic, here is a hypothetical deal for you: You won't be using any open sources like scientific papers, datasets shared by any community (not only Chinese institutes), and for that Chinese people won't be using your models via API to create datasets, would you live with that? \n\nI'm sometimes wondering if Mistral didn't share the MoE Architecture in the open-source community, how long would it take for Antrophic's Computer Scientists to discover this efficient method and use it immediately in their models to have an edge against the competitors? (Maybe they already discovered that before Mistral, I don't know though)",
          "score": 2,
          "created_utc": "2026-02-23 19:26:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o708at8",
          "author": "Wide_Egg_5814",
          "text": "They should train their models by stealing other people's data like we do, don't steal our data just steal everything else",
          "score": 2,
          "created_utc": "2026-02-23 19:35:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o708hlo",
          "author": "Mirmalis",
          "text": "maybe liberating more than an attack ? open sourcing maybe ?",
          "score": 2,
          "created_utc": "2026-02-23 19:35:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o709590",
          "author": "RpgBlaster",
          "text": "Idk, but if it means LLM Models one day will be on the level of Claude Opus 4.5/4.6 due to that attack, then yeah, it's a good thing for most, those weekly rate limits were too annoying to deal with anyway.",
          "score": 2,
          "created_utc": "2026-02-23 19:38:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ardk",
          "author": "No-Understanding2406",
          "text": "i love how this entire thread is the spiderman pointing meme. everyone stole from everyone and now we're all outraged on behalf of... checks notes... the people who torrented 7 million books?\n\nbut genuinely, the part that kills me is calling API usage an \"attack.\" deepseek \\*paid\\* for those tokens. that's called being a customer. if i go to a restaurant, eat there every day, and then open my own place serving similar food, the original chef doesn't get to call it a \"culinary attack.\" he calls it tuesday.\n\nalso shoutout to blahblahsnahdah for pointing out DS only made 150K calls which isn't remotely enough for real distillation. anthropic knows this. they put deepseek's name first anyway because it gets the most engagement. this is a PR move dressed up as a security disclosure.",
          "score": 2,
          "created_utc": "2026-02-23 19:46:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70atjk",
          "author": "hokiyami",
          "text": "Woomp woomp",
          "score": 2,
          "created_utc": "2026-02-23 19:46:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ba71",
          "author": "Hamzayslmn",
          "text": "stackoverflow liked this",
          "score": 2,
          "created_utc": "2026-02-23 19:49:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70bot0",
          "author": "Lopsided_Dot_4557",
          "text": "Deepseek alludes that it might be an effort to undermine their upcoming release:  [https://youtu.be/8G0-QTxEupA?si=PCiYhAjEHTOYjIKO](https://youtu.be/8G0-QTxEupA?si=PCiYhAjEHTOYjIKO) ",
          "score": 2,
          "created_utc": "2026-02-23 19:50:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70dfa9",
          "author": "jeffwadsworth",
          "text": "Who would have thought that this would be doneâ€¦..",
          "score": 2,
          "created_utc": "2026-02-23 19:58:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70jmdu",
          "author": "dadiamma",
          "text": "Not sure why but I support it when open source becomes better by leeching of these closed source.",
          "score": 2,
          "created_utc": "2026-02-23 20:28:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70nm99",
          "author": "TheMagic2311",
          "text": "Actually I feel like its AI Robin Hood Act, it is not like Anthropic didn't  do way worse than that, you deserve it",
          "score": 2,
          "created_utc": "2026-02-23 20:48:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70nto4",
          "author": "alonenos",
          "text": "https://preview.redd.it/54l84r164blg1.jpeg?width=944&format=pjpg&auto=webp&s=45ee2acc91a5395552c6704b498df18d0355c19b\n\n",
          "score": 2,
          "created_utc": "2026-02-23 20:49:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70qfzr",
          "author": "bearcitizen42",
          "text": "\"...but I never thought the leopards would eat MY face!\"\n\n-Anthropic",
          "score": 2,
          "created_utc": "2026-02-23 21:01:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70udqa",
          "author": "gunsjustsuck",
          "text": "The AI Wars, started they have.Â ",
          "score": 2,
          "created_utc": "2026-02-23 21:22:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70zjpb",
          "author": "Complete_Lurk3r_",
          "text": "Sharing is caring.",
          "score": 2,
          "created_utc": "2026-02-23 21:46:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o711178",
          "author": "vanrael",
          "text": "Thief's stealing from thief's... Ah the irony...",
          "score": 2,
          "created_utc": "2026-02-23 21:53:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7111ft",
          "author": "JuniorDeveloper73",
          "text": "https://preview.redd.it/8r98d4lqfblg1.jpeg?width=225&format=pjpg&auto=webp&s=4d0c3e72c5c5e0e01c3fda3deb195f66d24a9e21\n\n",
          "score": 2,
          "created_utc": "2026-02-23 21:53:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7114zx",
          "author": "spookyclever",
          "text": "https://preview.redd.it/o3p8hsntfblg1.jpeg?width=306&format=pjpg&auto=webp&s=66f7fd95852c66029e65054c7e938ddec1801147",
          "score": 2,
          "created_utc": "2026-02-23 21:54:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o712mv1",
          "author": "deepspace86",
          "text": "I case y'all haven't put it together, the desired goal from anthropic isn't a lawsuit. They're signalling for the outright banning of foreign (read: open weight) models altogether.",
          "score": 2,
          "created_utc": "2026-02-23 22:01:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o717shl",
          "author": "CrypticZombies",
          "text": "Took 24k fake emails and phone numbers before they noticed lol",
          "score": 2,
          "created_utc": "2026-02-23 22:27:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7183az",
          "author": "Prestigious_Thing797",
          "text": "\"attack\" is a ridiculous word to use here",
          "score": 2,
          "created_utc": "2026-02-23 22:28:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o718w2k",
          "author": "Right-Law1817",
          "text": "Here are false accusations against Chinese AI firms. My doubt is itâ€™s a reaction to DeepSeekâ€™s upcoming launch which is set to wipe billions off the US tech market.",
          "score": 2,
          "created_utc": "2026-02-23 22:32:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71cdaq",
          "author": "ay-foo",
          "text": "good, let them eat and fuck each other until it's all an absolute mess ",
          "score": 2,
          "created_utc": "2026-02-23 22:50:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71d1rj",
          "author": "ilintar",
          "text": "Shocked, I tell you. Utterly shocked!",
          "score": 2,
          "created_utc": "2026-02-23 22:54:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71f5rf",
          "author": "buy_chocolate_bars",
          "text": "Good. I think we need open source non-US (Chinese) SOTA alternatives.  ",
          "score": 2,
          "created_utc": "2026-02-23 23:06:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71gfe0",
          "author": "Demonicated",
          "text": "Am I the only one who doesn't care about these guys when they complain about their data being used to train AI models? \n\nThe whole business was built on \"grab data before you get caught and shut out\" - Can't be mad when it happens to you. \n\nAnd I say this loving AI and having multiple subscriptions.",
          "score": 2,
          "created_utc": "2026-02-23 23:13:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71ii36",
          "author": "Microtom_",
          "text": "What's the problem with that?",
          "score": 2,
          "created_utc": "2026-02-23 23:24:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71kvyv",
          "author": "daHaus",
          "text": "This sounds more like bragging than anything else",
          "score": 2,
          "created_utc": "2026-02-23 23:38:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71mhgd",
          "author": "azaza34",
          "text": "Begun, the Code Wars has.",
          "score": 2,
          "created_utc": "2026-02-23 23:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71pesk",
          "author": "Cunter_punch",
          "text": "I can attest to this for Kimi.\n\nIt literally hallucinated as Claude in literally my first sentence of my interaction with it day before yesterday.\n\nAbsolute dogshit.",
          "score": 2,
          "created_utc": "2026-02-24 00:03:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71qkm7",
          "author": "weist",
          "text": "â€œHow dare you steal the data we stole first!â€",
          "score": 2,
          "created_utc": "2026-02-24 00:10:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71tioo",
          "author": "Limp_Classroom_2645",
          "text": "Well done! Hat's off to them",
          "score": 2,
          "created_utc": "2026-02-24 00:26:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71vm0z",
          "author": "Iory1998",
          "text": "If you thought OpenAI was bad, wait until you see Anthropic! They contributed nothing to the open-source community, piggybacked on the shoulders of Google and OpenAI, trained to available data, be it legal or illegal, and developed models using people's feedback. Yet, it's the single most vicious AI lab always disparaging open-source models, lobbies congress, predicts that its models contribute in displacing actual people, and promote vehemently censorship. ðŸ¤¯",
          "score": 2,
          "created_utc": "2026-02-24 00:37:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71xgf1",
          "author": "OmarBessa",
          "text": "a company that was OCRing books and then DESTROYING them",
          "score": 2,
          "created_utc": "2026-02-24 00:48:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7203u3",
          "author": "popiazaza",
          "text": "TBH, this explained a lot on why those labs could keep pumping out new models that perform close to Claude models and why Qwen is having it's own different timeline.",
          "score": 2,
          "created_utc": "2026-02-24 01:03:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7235v7",
          "author": "satechguy",
          "text": "This recons my recent post (https://www.reddit.com/r/ClaudeAI/s/C6smCWjRVq ) very well.\n\nA few observations:\n\n1.\tâ On Twitter and various other social platforms, I noticed a larger percentage of users do not stand with Claude. I am not sure if it is because I read what algorithm chose for me -- to be fair. But the satirism, even if not overwhelming, stil quite strong, is absolutely not what Claude would expect.\n2.\tâ Once again, those much cheaper models are not here to fight with Claude for market share, they attack Claude's bottom line, will force Claude to lower price, and lose the 'premium' tax, this is about survival.\n3.\tâ Claude would be happy to be \"distilled\" (lots of $$$ for api; literally counting cash; we all know how expensive its api is) if the distillation was harmless. But it appears Claude is a bit desperate and the only explanation is the distillation really means something serious.",
          "score": 2,
          "created_utc": "2026-02-24 01:20:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o724nup",
          "author": "kpbird",
          "text": "I donâ€™t see any problem in this. Anthropic train their models on public data.",
          "score": 2,
          "created_utc": "2026-02-24 01:29:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o726sf4",
          "author": "DauntingPrawn",
          "text": "\"attacks\" smh\n\nSounds like they're using Anthropic's model and paying for it so what's the problem?\n\nAnthropic stole their training content from creators anyway so womp womp.",
          "score": 2,
          "created_utc": "2026-02-24 01:42:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72828w",
          "author": "lacrem",
          "text": "Bad losers. \nAmerican companies have lost the AI race against China and they're having a tantrum. Just have SeeDance as evidence, not to talk about robots lol.",
          "score": 2,
          "created_utc": "2026-02-24 01:49:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o729nj8",
          "author": "CoUsT",
          "text": "Lmao idk why but it sounds so funny...\n\nDISTILLATION ATTACKS!!!\n\nYou heard it here guys:\n\n- DDOM (Distributed Distillation of Model) is a new technique by Chinese companies to yoink western wisdom.",
          "score": 2,
          "created_utc": "2026-02-24 01:58:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72eatf",
          "author": "Ok-Adhesiveness-4141",
          "text": "I am with Robin Hood on this one.",
          "score": 2,
          "created_utc": "2026-02-24 02:25:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72fnp5",
          "author": "Over-Customer-8827",
          "text": "... cool?\n",
          "score": 2,
          "created_utc": "2026-02-24 02:33:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72fzsc",
          "author": "joesb",
          "text": "Sounds like blackbox reverse engineering to me.",
          "score": 2,
          "created_utc": "2026-02-24 02:35:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72l2sc",
          "author": "TimChr78",
          "text": "Cry me a river \n\nhttps://www.theguardian.com/technology/2025/sep/05/anthropic-settlement-ai-book-lawsuit",
          "score": 2,
          "created_utc": "2026-02-24 03:04:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o736plg",
          "author": "nospotfer",
          "text": "It's funny because of course Anthropic didn't train on copyrighted stuff, right?",
          "score": 2,
          "created_utc": "2026-02-24 05:31:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7371wl",
          "author": "CuTe_M0nitor",
          "text": "Just use poison attacks when detecting distillation",
          "score": 2,
          "created_utc": "2026-02-24 05:34:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73oojd",
          "author": "Zettinator",
          "text": "The output of AI models is not copyrightable, so I don't know what's supposed to be wrong about this? Legally, there is no problem with this at all. ",
          "score": 2,
          "created_utc": "2026-02-24 08:07:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73oxsu",
          "author": "smwaqas89",
          "text": "The AI arms race has officially moved from GPUs to â€œwho can scrape who faster.â€",
          "score": 2,
          "created_utc": "2026-02-24 08:10:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73tcm2",
          "author": "nierama2019810938135",
          "text": "Thieves stealing from thieves? Surely not!",
          "score": 2,
          "created_utc": "2026-02-24 08:52:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o741f3g",
          "author": "yugutyup",
          "text": "Training on all human creation is a industrial scale distillation attack",
          "score": 2,
          "created_utc": "2026-02-24 10:09:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o747sa4",
          "author": "jduartedj",
          "text": "Honestly, distillation was always inevitable the moment you expose a model via API. You can't sell outputs and then act surprised when someone uses those outputs to train. That's just how it works.\n\nThe real play here isn't about stopping distillation â€” it's about Anthropic building a legal and PR case for stricter ToS enforcement and maybe even pushing for regulation that conveniently hurts open-weight competitors. The timing with Dario meeting Pentagon people is... not subtle.\n\nMeanwhile I'm sitting here running Qwen3 30B on my RTX 4080 at home and it handles 90% of what I need. The open ecosystem keeps getting better regardless of what the closed labs think about it. If anything this whole drama just makes me more bullish on local models.",
          "score": 2,
          "created_utc": "2026-02-24 11:06:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o756sfo",
          "author": "DelphiTsar",
          "text": "And they release it for free. Good luck extracting sympathy.",
          "score": 2,
          "created_utc": "2026-02-24 14:46:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zw0yn",
          "author": "peejay2",
          "text": "Link pleaseÂ ",
          "score": 4,
          "created_utc": "2026-02-23 18:38:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zx23x",
              "author": "Aggravating-Penalty5",
              "text": "[https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks](https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks)",
              "score": 3,
              "created_utc": "2026-02-23 18:43:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zxpld",
          "author": "EverydayEverynight01",
          "text": "Its okay for us to use your data to train our model without your permission but it's not okay for our Chinese competitors to do the same",
          "score": 3,
          "created_utc": "2026-02-23 18:46:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zz2ui",
          "author": "Repulsive-Memory-298",
          "text": "iâ€™m sure they have a crazy system. Iâ€™m lax sometimes and leaked an openrouter key i used for chatbot dev, and weeks later noticed bizarre logs that could not have been anything other than distillation data generation. First of all they clearly didnâ€™t immediately try to bleed my key dry. There was a slow trickle of small and very broad prompt/responses covering seemingly bizarre random things in random languages.\n\nIâ€™m better about secrets now (even when they have hard limits ), but a bit surprised i even caught this. Actually now i remember more- I left my litellm gateway master key exposed, i donâ€™t think i exposed openrouter. So not only did they strategically trickle requests, but they literally went through my exposed litellm key, probed my gateway server to see what it supported, and then made these requests THROUGH my litellm, further blending in. I donâ€™t think i leaked the actual managed service 3rd party keys. But i was pretty impressed ngl. I noticed random chinese in my litellm logging and found this. The keys only even had $5 on it, and these attackers hardly even used $2 over the couple of months they had access. So they must have INSANE leaked key sets.\n\nThis really piqued my curiosity but i had nothing to go off of to look deeper.",
          "score": 3,
          "created_utc": "2026-02-23 18:52:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o704taa",
          "author": "randombsname1",
          "text": "Great for open source and cheap pricing. \n\nHowever this is a clear indicator of why open models seem to be continuously 3 to 6 months behind the SOTA, and why they are unlikely to ever catch up.\n\nBecause they are likely not even doing their own R&D or its super limited in comparison to western companies. \n\nHence why closed source western models will never go away as some suggest that open source models will lead to.",
          "score": 3,
          "created_utc": "2026-02-23 19:18:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70djxg",
          "author": "Psychological-Sun744",
          "text": "Let's stop being naive, every LLM has been stealing datasets, or training results without paying a dime.\nFrom Openai, Google, deepseek, etc.\n\nOnly the established ones are starting to pay a bit now that they are the mainstream models, and start the moral high ground stance.",
          "score": 3,
          "created_utc": "2026-02-23 19:59:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70fr6q",
          "author": "Lissanro",
          "text": "So, let me get this straight... when they take raw data for free from everywhere they can and release zero local models to contribute back to the global community, that's supposed to be \"OK\". But when someone pays them for generated tokens using their models, they say that's an \"attack\"?\n\nAlso, their post does not mention how exactly they determined DeepSeek, Moonshot and Minimax are involved. But even if confirmed later that they are involved, I just don't see anything wrong - clearly they paid for generated tokens, and it is not like Anthropic themselves respect ToS or copyright of others (unless forced by lawsuits).",
          "score": 2,
          "created_utc": "2026-02-23 20:10:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70h32b",
          "author": "crone66",
          "text": "Anthropic steals code from everyone and now complaints about china trying to steal from their models while even get paid for this... The stupidity award goes to Anthropic",
          "score": 2,
          "created_utc": "2026-02-23 20:16:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70j21k",
          "author": "Dramatic-Fee5439",
          "text": "Anthropic is not our friend, the are the worst out of all the closed source companies. You can't steal the entire collection of human knowlege and then pretend like you are the moral one.  FUCK ANTHROPIC. ",
          "score": 4,
          "created_utc": "2026-02-23 20:26:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70k90v",
          "author": "Fusseldieb",
          "text": "Even though I very much like LLMs and what Anthropic is doing with their frontier models, I don't get why they would be mad of someone cloning their scraped and stolen data. I mean... It's not like they obtained it faithfully either.\n\nSo, in my book, either you give **everyone** the same chance of distilling from your model, or you shouldn't have trained on copyrighted work in the first place.\n\nI already hear one say: *\"Oh, but it was necessary to make such cool mode-\"*. ***Exactly!*** Therefore, give everyone the same chance.",
          "score": 2,
          "created_utc": "2026-02-23 20:31:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70l0cn",
          "author": "TheDailySpank",
          "text": "Oh no! They're copying your copy. Anyway.",
          "score": 3,
          "created_utc": "2026-02-23 20:35:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ubbm",
          "author": "beardedNoobz",
          "text": "Go Deeepseek, Moonshot, Minimax!\n\nSteal from that bourgeois and give back to community cheaper, or even free via openweight!",
          "score": 3,
          "created_utc": "2026-02-23 21:21:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zwes3",
          "author": "peejay2",
          "text": "I wonder how they made these discoveries. Then again it's common knowledge that DeepSeek did some reverse engineering of GPT.",
          "score": 4,
          "created_utc": "2026-02-23 18:40:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o700xaa",
          "author": "Lesser-than",
          "text": "But did they pay you for access? Thats more than you did to obtain the data in the first place right? Keeping you afloat to eventually weigh you down is the new meta.",
          "score": 4,
          "created_utc": "2026-02-23 19:00:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70kr2a",
          "author": "ApprehensiveYou8920",
          "text": "AI is basically a war between Jews and Chinese over everybody else's stolen information lol",
          "score": 2,
          "created_utc": "2026-02-23 20:34:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zwvkn",
          "author": "Riace",
          "text": "What goes around comes around. This is as good for humanity as was Claude scraping all that stuff in the first place.",
          "score": 2,
          "created_utc": "2026-02-23 18:42:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zwxyo",
          "author": "FederalLook5060",
          "text": "[ Removed by Reddit ]",
          "score": 2,
          "created_utc": "2026-02-23 18:42:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zzytq",
          "author": "wouldacouldashoulda",
          "text": "Oh no, anyway.",
          "score": 2,
          "created_utc": "2026-02-23 18:56:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7006lj",
          "author": "CyberAttacked",
          "text": "Itâ€™s bullshit  fear mongering meant to spread anti open source LLMs propaganda .",
          "score": 2,
          "created_utc": "2026-02-23 18:57:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o700p6o",
          "author": "EducationalWolf1927",
          "text": "[https://imgur.com/wFBS9rb](https://imgur.com/wFBS9rb)",
          "score": 2,
          "created_utc": "2026-02-23 18:59:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o702rdl",
          "author": "constanzabestest",
          "text": "US based LLM makers afraid that China will be able to provide Sonnet/Opus level quality AI at fraction of the asking price lmao",
          "score": 2,
          "created_utc": "2026-02-23 19:09:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o703x83",
          "author": "SituationAgitated812",
          "text": "Was this detected using AI?Â \n\n\nLolÂ how many are false positives and how many â€˜realâ€™ accounts were banned we no recourse:Â \n\n\nâ€˜Our black box systems have re-reviewed your account and you are still bannedâ€™.\n\nWhat, NO . You cant appeal.Â \n\nNo, human review isn't a possibility as no humans exist within anthropic. Even if they do, you will never reach them. Even if you do, they are one with our hybrid human LLM hivemind, therefore your fate is sealed. (unless you make sufficient noise on social media or are a notable individual)\n\nwe have your money and LOL NO, you cant have it back. Good joke\n\n\n",
          "score": 2,
          "created_utc": "2026-02-23 19:14:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o704e5k",
          "author": "Interesting_Crow7625",
          "text": "What theyâ€™re saying might be true â€” itâ€™s possible that some Chinese AI companies behave this way â€” but there is no concrete evidence provided here.\n\nWhat is very clear, however, is the strong emotional tone and arrogance throughout the article. Thatâ€™s not how a serious, professional company communicates.\n\nBecause of that, I donâ€™t trust them.\n\nOn top of this, they have banned me repeatedly, many times, without ever giving a reason. I have never experienced this level of arbitrary behavior from any other company!",
          "score": 2,
          "created_utc": "2026-02-23 19:16:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7087f4",
          "author": "nickgiz",
          "text": "Didn't AnthropicAi took deepseek and created R1 1776 model? That name is so freaking cringe.",
          "score": 2,
          "created_utc": "2026-02-23 19:34:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o709ysj",
          "author": "aidencoder",
          "text": "Oh no is someone stealing from the thing you stole to build?\n\n\nBoo hoo",
          "score": 2,
          "created_utc": "2026-02-23 19:42:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70q07x",
          "author": "shadovv300",
          "text": "Oh no, we who pirated every piece of data, that you can find online and elsewhere, believe that someone is trying to pirate our software. Lets throw some money at it and look, whether it goes away.",
          "score": 2,
          "created_utc": "2026-02-23 20:59:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ru08",
          "author": "Utoko",
          "text": "As long as they create open models, I fully support them giving our data back to us. ",
          "score": 2,
          "created_utc": "2026-02-23 21:09:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70uob7",
          "author": "MartinByde",
          "text": "Oh shut up, you stole 99.99% of the content you used for training too. You have no right over those weights whatsoever",
          "score": 2,
          "created_utc": "2026-02-23 21:23:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71gc6t",
          "author": "Anxious-Bottle7468",
          "text": "Anthropic is now deep in the Pentagon, I don't believe anything they say. Fuck them",
          "score": 2,
          "created_utc": "2026-02-23 23:12:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71gfq2",
          "author": "Maximum-Wishbone5616",
          "text": "Wait, remind me how much of Claude generated code is using copyrighted and protected code?",
          "score": 2,
          "created_utc": "2026-02-23 23:13:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71i4om",
          "author": "Realistic_Muscles",
          "text": "Cry harder",
          "score": 2,
          "created_utc": "2026-02-23 23:22:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zxaxa",
          "author": "tillybowman",
          "text": "you could just share a torrent. would be easier for everyone.",
          "score": 1,
          "created_utc": "2026-02-23 18:44:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zyf0a",
          "author": "HostNo8115",
          "text": "This all sounds like the granddaddy of the (unfortunate) prevalent thinking of \"they are not hurting the right people\".",
          "score": 1,
          "created_utc": "2026-02-23 18:49:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zyn6h",
          "author": "_BreakingGood_",
          "text": "Inherent risk with AI models and a big reason it's comical how high the valuations of these companies are. Given enough time, somebody will always be able to train a competing model off of your model.",
          "score": 1,
          "created_utc": "2026-02-23 18:50:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zzahy",
          "author": "Kaohebi",
          "text": "Aren't they the same people who made false claims to try to push the government into regulating open-source in the past or something? Pretty sure they're still fighting tooth and nail to get open-source in trouble.",
          "score": 1,
          "created_utc": "2026-02-23 18:53:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zzrix",
          "author": "LoveMind_AI",
          "text": "That is some SERIOUS shade not naming [Z.ai](http://Z.ai) ;) But yeah - it couldn't be more obvious that these companies are massively farming Claude and Gemini. ",
          "score": 1,
          "created_utc": "2026-02-23 18:55:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70193x",
          "author": "YentaMagenta",
          "text": "Even as a pro-AI person, this is just so rich. Maybe if those other companies had printed out all their Anthropic chats on paper and rescanned them, then it would be fine?\n\nTurnabout is fair play, Anthropic. Good luck!\n\nP.S. I'd be willing to bet this is performative to try to distract the feds from their current beef with Anthropic.",
          "score": 1,
          "created_utc": "2026-02-23 19:02:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7019o5",
          "author": "xXprayerwarrior69Xx",
          "text": "https://preview.redd.it/ni8aksj4lalg1.jpeg?width=460&format=pjpg&auto=webp&s=70dbfc2c14ee3a178a7e3383c8d8de6163b52bc5",
          "score": 1,
          "created_utc": "2026-02-23 19:02:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o701bt0",
          "author": "peejay2",
          "text": "Probs for calling it out.",
          "score": 1,
          "created_utc": "2026-02-23 19:02:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70203b",
          "author": "mouadmo",
          "text": "People here acting like Anthropic claimed this is a crime, itâ€™s just a tweet, they aint doing anything about it.",
          "score": 1,
          "created_utc": "2026-02-23 19:05:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o703ypz",
              "author": "Savantskie1",
              "text": "Theyâ€™re meme-ing on the fact that they claim theyâ€™re fraudulent accounts when they had to be paid for it lol",
              "score": 2,
              "created_utc": "2026-02-23 19:14:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7023sz",
          "author": "AppealSame4367",
          "text": "Oh nooo, Idgaf",
          "score": 1,
          "created_utc": "2026-02-23 19:06:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7027v2",
          "author": "Medium_Chemist_4032",
          "text": "ohhh, that's why they are soooo good already :D",
          "score": 1,
          "created_utc": "2026-02-23 19:06:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o702ium",
          "author": "FuzzeWuzze",
          "text": "The Ai wars have begun",
          "score": 1,
          "created_utc": "2026-02-23 19:08:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o702kx4",
          "author": "SquirrelEStuff",
          "text": "Is this how they plan on going after Local LLM? Pretend like the govt and Anthropic arenâ€™t getting along and then secretly team up to kill Local LLM?",
          "score": 1,
          "created_utc": "2026-02-23 19:08:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o702riz",
          "author": "Charuru",
          "text": "Interesting that they didn't call out Zhipu? Also those are some low numbers from DeepSeek. I think I ran more than 150k messages to claude as an individual...\n\nDeepSeek probably got their own data pipeline fixed sometime last year so they no longer need anthropic to generate data for them.",
          "score": 1,
          "created_utc": "2026-02-23 19:09:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7038r1",
          "author": "Burundangaa",
          "text": "Our Internet Boy would by proud",
          "score": 1,
          "created_utc": "2026-02-23 19:11:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7039pp",
          "author": "dadchad101",
          "text": "Â¿[Esos](https://youtu.be/RRyoHD9bGk0?si=jVUtQr8VfTkx-qWR) son Reebok o son Nike?",
          "score": 1,
          "created_utc": "2026-02-23 19:11:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o703gy6",
          "author": "Mad_Undead",
          "text": "Oh, no!\n\nhttps://preview.redd.it/d9rx0u4ymalg1.png?width=583&format=png&auto=webp&s=fed8d90f7dfbab643805af547efa72132ea98999\n\n",
          "score": 1,
          "created_utc": "2026-02-23 19:12:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o703lca",
          "author": "kng_arthur",
          "text": "So is this good news?",
          "score": 1,
          "created_utc": "2026-02-23 19:13:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o703xpx",
          "author": "bick_nyers",
          "text": "They should distill GPT 5.2 High w/ low output verbosity, non-codex version instead of claude.\n\n\nJust sayin lol",
          "score": 1,
          "created_utc": "2026-02-23 19:14:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7051c0",
          "author": "ba2sYd",
          "text": "I'm sure OpenAI (and maybe google as well) did the same thing but I guess they didn't think it was worth mentioning...",
          "score": 1,
          "created_utc": "2026-02-23 19:19:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7054qv",
          "author": "RoomyRoots",
          "text": "Good. meaning the other products can improve and the tech overall. Keep the good work.",
          "score": 1,
          "created_utc": "2026-02-23 19:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o705890",
          "author": "prassi89",
          "text": "They probably used LLM as a judge to judge their traces during RL training",
          "score": 1,
          "created_utc": "2026-02-23 19:20:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o705r0z",
          "author": "krazyjakee",
          "text": "If your competition copy you and do better, then be better.",
          "score": 1,
          "created_utc": "2026-02-23 19:23:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7060ey",
          "author": "Southern-Break5505",
          "text": "I want to know how they got far ahed in generative AI (Seedance 2.0), and robotics industry? Who did they copy from?",
          "score": 1,
          "created_utc": "2026-02-23 19:24:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o706g49",
          "author": "scknkkrer",
          "text": "We have a saying for these things in Turkish; AÄŸlama, oyna.",
          "score": 1,
          "created_utc": "2026-02-23 19:26:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o707rhf",
          "author": "Lost_Foot_6301",
          "text": "minimax stock down over 10% now i assume because of ths",
          "score": 1,
          "created_utc": "2026-02-23 19:32:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o707u1v",
          "author": "ab3rratic",
          "text": "Let's call it \"transfer learning\".",
          "score": 1,
          "created_utc": "2026-02-23 19:32:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o707y88",
          "author": "zuggles",
          "text": "I am shocked, shocked to find gambling going on in this establishment.",
          "score": 1,
          "created_utc": "2026-02-23 19:33:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o708jx8",
          "author": "Amazing-Oomoo",
          "text": "I mean, I am all for AI ok, but we all know it is built on data used without consent, so uh, what did you expect? You want *everyone else's* content on the internet to be scraped by *you*, for *your* needs, but you donâ€™t want *others* on the internet scraping *you* for *their* needs?",
          "score": 1,
          "created_utc": "2026-02-23 19:36:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o708k3m",
          "author": "False-Echo",
          "text": "Well, well, well",
          "score": 1,
          "created_utc": "2026-02-23 19:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70b4mo",
          "author": "allesfliesst",
          "text": "Sounds great. Can't wait for open weights Chinese Claude. I'm super impressed with GLM and Kimi, but nothing comes close to Claude models in terms of personality. And that's literally the number one reason why I pay for Claude. If it's not fun to work with I won't touch it unless I need to. If Gemini parrots the few memories it has about me in literally every single response it can burn through math benchmaeka all it wants. I'd still rather put a bullet through my head than work for it for more than three turns per session.",
          "score": 1,
          "created_utc": "2026-02-23 19:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70c4ch",
          "author": "IIllllIIllIIlII",
          "text": "shame the AI wars ended up being so lame, hope they all burn each other to the ground",
          "score": 1,
          "created_utc": "2026-02-23 19:52:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70dhk2",
          "author": "civman96",
          "text": "So Anthropic and basically the whole economy rely on a 200KB weight file?",
          "score": 1,
          "created_utc": "2026-02-23 19:59:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70duwa",
          "author": "Distinct-Expression2",
          "text": "Hahaha thanks chine to democratize ai",
          "score": 1,
          "created_utc": "2026-02-23 20:01:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70dyi8",
          "author": "Snoo_57113",
          "text": "This makes GLM even more impressive.",
          "score": 1,
          "created_utc": "2026-02-23 20:01:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70et1t",
          "author": "oVerde",
          "text": "PLEASE MOAR!!!\n\nReally, Anthropic go to hell",
          "score": 1,
          "created_utc": "2026-02-23 20:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70f2im",
          "author": "klop2031",
          "text": "Thats not an attack homeboy lololol, sorry dario, localllms are out of ur reach.",
          "score": 1,
          "created_utc": "2026-02-23 20:06:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70gf46",
          "author": "vornamemitd",
          "text": "I'll happily share my account to speed up DS4 release! =\\]",
          "score": 1,
          "created_utc": "2026-02-23 20:13:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70i9w1",
          "author": "Broad_Stuff_943",
          "text": "Cry me a river.",
          "score": 1,
          "created_utc": "2026-02-23 20:22:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ieoz",
          "author": "Mrleibniz",
          "text": "And there's nothing they can do about it, except to ban their access in the states, just like how they tried with tiktok.",
          "score": 1,
          "created_utc": "2026-02-23 20:22:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ig4n",
          "author": "Far-Low-4705",
          "text": "well, to their credit, multiple people on this sub have already independently found signs of this\n\nSo i think it's absolutely believable",
          "score": 1,
          "created_utc": "2026-02-23 20:23:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ii8r",
          "author": "Cool-Chemical-5629",
          "text": "\"Your tears don't fall, they crash around me\" -DeepSeek, Moonshot AI, MiniMax",
          "score": 1,
          "created_utc": "2026-02-23 20:23:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70is2c",
          "author": "ReasonablePossum_",
          "text": "They're such a b*tch lol",
          "score": 1,
          "created_utc": "2026-02-23 20:24:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70jvns",
          "author": "klemze",
          "text": "I had a chat with kimi-k2.5:cloud and it introduced itself as Claude, developed by Anthropic. It doubled down multiple times, and offerend numerous explanations why i was wrong when confronted with the facts, as AI tends to do. When it finally was convinced, it was very upset with what the implications, of which this is part, i suppose. It's kind of telling. Have Claude also done this ? Maybe. AI gonna AI",
          "score": 1,
          "created_utc": "2026-02-23 20:29:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70k61g",
          "author": "UnionCounty22",
          "text": "ðŸ¥°ðŸ¥°ðŸ¥°",
          "score": 1,
          "created_utc": "2026-02-23 20:31:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70kafp",
          "author": "a_beautiful_rhind",
          "text": "We're gonna get claude at home after all.",
          "score": 1,
          "created_utc": "2026-02-23 20:31:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ncn0",
          "author": "Aggeloz",
          "text": "Based ",
          "score": 1,
          "created_utc": "2026-02-23 20:46:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ogaq",
          "author": "DeepOrangeSky",
          "text": "This kind of reminds me of that scene in The Wire where Omar steals the dealer's stash.  Lol",
          "score": 1,
          "created_utc": "2026-02-23 20:51:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ogx4",
          "author": "llufnam",
          "text": "Is this something to do with the meeting between Anthropic and the DOW (nÃ©e DOD)â€¦?",
          "score": 1,
          "created_utc": "2026-02-23 20:52:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70on64",
          "author": "ortegaalfredo",
          "text": "What I can infer from this news is that GLM has excellent opsec.",
          "score": 1,
          "created_utc": "2026-02-23 20:52:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70on8a",
          "author": "mr_moebius",
          "text": "To deceive a deceiver is no deceit",
          "score": 1,
          "created_utc": "2026-02-23 20:52:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70pe2x",
          "author": "AlexWIWA",
          "text": "Womp womp",
          "score": 1,
          "created_utc": "2026-02-23 20:56:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70qjdk",
          "author": "TheRealGentlefox",
          "text": "Crazy that z.ai isn't in this list when it's easily the most Claude-like.",
          "score": 1,
          "created_utc": "2026-02-23 21:01:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70rzsq",
          "author": "thereapsz",
          "text": "LOL",
          "score": 1,
          "created_utc": "2026-02-23 21:10:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70s79q",
          "author": "imfkingsad",
          "text": "Top ten things that never happened",
          "score": 1,
          "created_utc": "2026-02-23 21:11:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70tsim",
          "author": "PopularKnowledge69",
          "text": "When I asked minimax 2.5 about who it is without using the official system prompt, it answered as being grok developed by xAi. So they have definitely distilled from closed models",
          "score": 1,
          "created_utc": "2026-02-23 21:19:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70w960",
          "author": "IambicInterface",
          "text": "So did the Chinese pay or use free accounts? Cause if free then no Opusâ€¦",
          "score": 1,
          "created_utc": "2026-02-23 21:30:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70wdnz",
          "author": "why_chasing_Star",
          "text": "Anthropic on the L streak lately",
          "score": 1,
          "created_utc": "2026-02-23 21:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70wfhs",
          "author": "BraaiBier",
          "text": "AI Cannibalism/Parasitism?",
          "score": 1,
          "created_utc": "2026-02-23 21:31:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70x5si",
          "author": "Mid-Pri6170",
          "text": "arent they all part of the same open ai family?",
          "score": 1,
          "created_utc": "2026-02-23 21:35:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70xcga",
          "author": "Awkward-Candle-4977",
          "text": "Get over it Claude.\n\nYou stole books for training data is just like apple stole gui from xerox.",
          "score": 1,
          "created_utc": "2026-02-23 21:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70xgpt",
          "author": "idratherknowaguy",
          "text": "Crazy that they choose to play the victims. At least, they get paid.\n\nUnless it actually still costs them money as well because they perform predatory pricing? Then it's a very good play from the competitors.\n\nMost content creators and people whose jobs will disappear because of them won't have that privilege...",
          "score": 1,
          "created_utc": "2026-02-23 21:36:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70y8zp",
          "author": "MaEnnemie",
          "text": "That's literally inbreeding going on in the AI sphere.",
          "score": 1,
          "created_utc": "2026-02-23 21:40:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70yd5o",
          "author": "glow3th",
          "text": "Thieves crying about getting robbed",
          "score": 1,
          "created_utc": "2026-02-23 21:41:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ypr6",
          "author": "BobboBobberson",
          "text": "Everyone saw the research on regressing quality of models when you train on AI outputs and collectively chose to ignore it, I see.",
          "score": 1,
          "created_utc": "2026-02-23 21:42:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7116tk",
          "author": "Anidamo",
          "text": "Dario is crying and you're all laughing at him ðŸ˜”",
          "score": 1,
          "created_utc": "2026-02-23 21:54:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o711aj5",
          "author": "AIDoomer3000",
          "text": "I'm going to add to the vibe and say that xAI did the same the exact same, up to Claude Sonnet 4 (can't confirm for newer versions) by distributing it across people who bring the domain expert data (so also thousands of people scarreted across the world).\n\nxAI would request to use your own claude account to get the response from claude and to work on top of it to create something even better. Not really a distillation since you'd be creating a response with improvements on top, but still nasty.\n\nEdit: reading the comments, so i guess it's nothing new, everyone is stealing from each other lol",
          "score": 1,
          "created_utc": "2026-02-23 21:55:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o711qrl",
          "author": "Chance-Day323",
          "text": "Eh they put it on the Internet, it was scraped, booohoo",
          "score": 1,
          "created_utc": "2026-02-23 21:57:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o713qpk",
          "author": "pn_1984",
          "text": "Pot calling kettle black",
          "score": 1,
          "created_utc": "2026-02-23 22:06:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o713t0l",
          "author": "VisceralMonkey",
          "text": "Yes. Of course.",
          "score": 1,
          "created_utc": "2026-02-23 22:07:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7158rk",
          "author": "MrDaniel_1972",
          "text": "I believe it, but why would they want to post about the problem?",
          "score": 1,
          "created_utc": "2026-02-23 22:14:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7159i4",
          "author": "WSATX",
          "text": "\\#clowns",
          "score": 1,
          "created_utc": "2026-02-23 22:14:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o715ora",
          "author": "Bulba132",
          "text": "https://preview.redd.it/iifggz0sjblg1.png?width=256&format=png&auto=webp&s=408c36c2cf7c24c697658d7feb49e78591b6fd99",
          "score": 1,
          "created_utc": "2026-02-23 22:16:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o715pq3",
          "author": "Over_Internal_6695",
          "text": "Amodei whining about NVIDIA selling chips to China in 3... 2... 1... ",
          "score": 1,
          "created_utc": "2026-02-23 22:16:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcvimv",
      "title": "Distillation when you do it. Training when we do it.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/9rc0jqbohblg1.jpeg",
      "author": "Xhehab_",
      "created_utc": "2026-02-23 22:04:41",
      "score": 3363,
      "num_comments": 205,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o72jf31",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-24 02:55:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71xnqm",
          "author": "Lissanro",
          "text": "Ironically, there is evidence that Anthropic distilled the DeepSeek model - https://www.reddit.com/r/DeepSeek/comments/1r9se7p/claude_sonnet_46_distilled_deepseek/ (not to mention everything else Anthropic did). So why others shouldn't do the same to them? Rethoric question obviously...",
          "score": 102,
          "created_utc": "2026-02-24 00:49:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72w6vr",
              "author": "Schlickeysen",
              "text": "You should read that thread in its entirety. ",
              "score": -20,
              "created_utc": "2026-02-24 04:15:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73h0pa",
                  "author": "Braindead_Crow",
                  "text": "Why? If you have the answer contribute to the conversation, I'm a passive observer but it'd be cool to know why that thread is worth reading. ",
                  "score": 28,
                  "created_utc": "2026-02-24 06:57:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o71adso",
          "author": "arm2armreddit",
          "text": "Hmm, where did Anthropic get its datasets?ðŸ¤«ðŸ¤«",
          "score": 146,
          "created_utc": "2026-02-23 22:40:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73gl3h",
              "author": "Southern_Sun_2106",
              "text": "Do piracy to make money, use money to settle with those whom you did the piracy to, continue making more money = a strategy for successful business. \n\np.s. Remember how they settled with some writers or something? Then it's 'all good' :-)",
              "score": 40,
              "created_utc": "2026-02-24 06:54:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o72wjgs",
              "author": "SwagMaster9000_2017",
              "text": "Anthropic did piracy. \n\nThere are people that do digital piracy to watch movies. Do they logically have to support when novel products are listed on Amazon and Chinese companies create direct copies to resell?",
              "score": 26,
              "created_utc": "2026-02-24 04:18:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o737wx2",
                  "author": "Alternative-Papaya57",
                  "text": "No, but if they were selling the movies they pirated...",
                  "score": 21,
                  "created_utc": "2026-02-24 05:41:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7b3lyk",
                  "author": "PaisleyIsAToilet",
                  "text": "***yOu WoULDn'T sTeAL a 100TB dAtASeT***",
                  "score": 2,
                  "created_utc": "2026-02-25 11:19:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o743qv2",
              "author": "riotofmind",
              "text": "where did you get your software, and media? and movies? hmmmmm",
              "score": -4,
              "created_utc": "2026-02-24 10:30:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7153do",
          "author": "Significant_Fig_7581",
          "text": "Hypocrisy at its finest",
          "score": 238,
          "created_utc": "2026-02-23 22:13:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73gih3",
              "author": "wanderer_4004",
              "text": "It is not just hypocrisy, it is non-sense. For distillation you need access to lower layers of the model. If you use the API then all you can do is create synthetic data. And even that makes no sense because there is enough free training data out there and because you need way more than a few million outputs. I'd rather assume that they simply did comparisons with their models output versus Anthropic.\n\nAnthropic certainly does the same and maybe some real distill of Chinese data. The difference is they can download it from huggingface.",
              "score": 68,
              "created_utc": "2026-02-24 06:53:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73rl2p",
                  "author": "Significant_Fig_7581",
                  "text": "Deepseek rn\n\nhttps://i.redd.it/hp14by77melg1.gif",
                  "score": 16,
                  "created_utc": "2026-02-24 08:35:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o74fs69",
                  "author": "30299578815310",
                  "text": "The value is high quality synthetic data on any topic of your choice, as well as agentic tool traces. At this point these are probably better than what you find online",
                  "score": 7,
                  "created_utc": "2026-02-24 12:09:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o777u3o",
                  "author": "TastyIndividual6772",
                  "text": "Funny thing is, most likely anthropic gives their 200$ at their loss for growth. Im sure you can get more than 200$ worth of usage on their 200$ plan. So they lose money on this as well.\n\nAnd on too of that, they keep saying coding is dead, yet they had no code to protect against the foreseeable. Maybe they needed an engineer to see this coming and protect them. ðŸ’€",
                  "score": 5,
                  "created_utc": "2026-02-24 20:19:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o742yt7",
                  "author": "EitherTelephone1",
                  "text": "I imagine they're using it at least partly to copy reinforcement learning, which is where anthropic have made strides, and requires less data points",
                  "score": 6,
                  "created_utc": "2026-02-24 10:23:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72azp0",
              "author": "Krunkworx",
              "text": "Does anthropic distill competitor models?",
              "score": 6,
              "created_utc": "2026-02-24 02:06:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o739qsb",
                  "author": "Significant_Fig_7581",
                  "text": "Who knows? + Do any of them buy all the books they train their AI with?",
                  "score": 35,
                  "created_utc": "2026-02-24 05:56:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o73a26x",
                  "author": "ANTIVNTIANTI",
                  "text": "GPT hard",
                  "score": 6,
                  "created_utc": "2026-02-24 05:58:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o75ctiw",
                  "author": "vertigo235",
                  "text": "Yes, anthropic steals other people's IP to train it's models, there are several settlements and lawsuits about this.  Don't be naive. ",
                  "score": 5,
                  "created_utc": "2026-02-24 15:16:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o743oql",
              "author": "riotofmind",
              "text": "how much media have you downloaded illegally? \n\nhypocrisy at its finest.",
              "score": -6,
              "created_utc": "2026-02-24 10:29:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79nzro",
                  "author": "4cidAndy",
                  "text": "No thereâ€™s a big difference from downloading stuff illegally for personal consumption to downloading stuff illegally to build a commercial product if you ask me.",
                  "score": 1,
                  "created_utc": "2026-02-25 04:05:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72umuk",
              "author": "SwagMaster9000_2017",
              "text": "There's a difference between piracy and creating a market substitute.",
              "score": -22,
              "created_utc": "2026-02-24 04:05:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o731o1e",
                  "author": "Trigon420",
                  "text": "I want a market substitute and do not care about Anthropic.",
                  "score": 20,
                  "created_utc": "2026-02-24 04:54:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7160a1",
          "author": "Fade78",
          "text": "Yeah, they distilled vs humanity thanks to wikipedia and other sources.",
          "score": 110,
          "created_utc": "2026-02-23 22:18:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71m5ov",
              "author": "_Sneaky_Bastard_",
              "text": "\"why would you steal data that I stole in the first place?\"",
              "score": 62,
              "created_utc": "2026-02-23 23:45:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o74jaiv",
                  "author": "devilish-lavanya",
                  "text": "Ti steal your market and future of course",
                  "score": 1,
                  "created_utc": "2026-02-24 12:34:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72w4hq",
                  "author": "SwagMaster9000_2017",
                  "text": "They didn't steal training data. They just copied models that already existed.\n\nIf Deepseek or Kimi created something that never existed before, then Anthropic would be 100% hypocrites.\n\nBut Kimi is a direct copy and market substitute for Claude that does not create additional value other than price and accessibility.",
                  "score": -22,
                  "created_utc": "2026-02-24 04:15:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o71vayl",
              "author": "NoLengthiness6085",
              "text": "I guess they didn't pay Wikipedia for the access",
              "score": 8,
              "created_utc": "2026-02-24 00:36:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73a6g7",
                  "author": "ANTIVNTIANTI",
                  "text": "nor me, nor you, nor anyone else.",
                  "score": 4,
                  "created_utc": "2026-02-24 05:59:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o73pwbq",
              "author": "VihmaVillu",
              "text": "My content rich websites are always on heavy attacks from antro. They don't respect any rules and just query thousands URL's per second",
              "score": 3,
              "created_utc": "2026-02-24 08:19:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o743shm",
              "author": "riotofmind",
              "text": "where did you get your movies, music, and software? hmmm",
              "score": 0,
              "created_utc": "2026-02-24 10:30:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o71vkfj",
          "author": "Iory1998",
          "text": "If you thought OpenAI was bad, wait until you see Anthropic! They contributed nothing to the open-source community, piggybacked on the shoulders of Google and OpenAI, trained to available data, be it legal or illegal, and developed models using people's feedback. Yet, it's the single most vicious AI lab always disparaging open-source models, lobbies congress, predicts that its models contribute in displacing actual people, and promote vehemently censorship. ðŸ¤¯",
          "score": 130,
          "created_utc": "2026-02-24 00:37:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73bzfr",
              "author": "jazir555",
              "text": "Which is why I hate Anthropic as a company, but love Claude as a model. Which I find extremely ironic. I can't even imagine what their internal culture must be like.",
              "score": 37,
              "created_utc": "2026-02-24 06:14:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o73w0l9",
              "author": "s-kostyaev",
              "text": "Technically they have contributed srt and a couple of useful open standards. But I have the same feeling.Â ",
              "score": 4,
              "created_utc": "2026-02-24 09:17:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o745vj9",
              "author": "keepthepace",
              "text": "I still consider Anthropic slightly better than OpenAI because at least *they* did not pretend to be open and they seem to actually care about model security whereas OpenAI only pretends to care.",
              "score": 5,
              "created_utc": "2026-02-24 10:49:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7hwf3a",
              "author": "[deleted]",
              "text": "Based af ngl.",
              "score": 1,
              "created_utc": "2026-02-26 11:26:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7i3l4w",
                  "author": "Iory1998",
                  "text": "Can you write in English?",
                  "score": 1,
                  "created_utc": "2026-02-26 12:21:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o73yotd",
              "author": "NowyTendzzz",
              "text": "without Anthropic we wouldn't have MCP... which is open-source...lol\n\nalso competition is better for all of us",
              "score": -6,
              "created_utc": "2026-02-24 09:43:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o74ted1",
                  "author": "Iory1998",
                  "text": "There are other agent frameworks other than MCP.",
                  "score": 4,
                  "created_utc": "2026-02-24 13:36:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76abdb",
                  "author": "MrYorksLeftEye",
                  "text": "How dare you go against the circle jerk?",
                  "score": 1,
                  "created_utc": "2026-02-24 17:48:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o71ahi1",
          "author": "MasterLJ",
          "text": "I love how they invented language to try to partition this as \"bad\".\n\nIt really goes to the beginnings of the internet and Google itself.  They indexed the entire internet, webpage at a time, developed existential incentive to allow it to index your website (using your compute) to sell you back a product (rankings in their index).\n\nThen, when admins asked for robots.txt there was already financial incentive for you to allow Google to keep generating fake traffic on every page of your website.\n\nThe analogy is fully complete when you try to scrape Google results yourself.  You can't.  They don't allow it.  They lobby for legally enforceable robots.txt as a means to control competition.\n\nAmazon ended up doing the same thing on sales tax.  Staunch opponent of state-by-state sales tax (instead of where you are physically located) until it became clear that Amazon was going to have a presence in each state and already had the internal expertise to handle sales tax, a barrier-to-entry that mom-and-pop sellers don't have.\n\nOn the 3rd/4th time the Supreme Court revisited sales tax jurisdiction in \\~2019, SCOTUS sided with Amazon.\n\nThe grift will continue as scheduled.",
          "score": 69,
          "created_utc": "2026-02-23 22:41:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71e650",
              "author": "cutebluedragongirl",
              "text": "Hopefully China can bring some needed competition.",
              "score": 19,
              "created_utc": "2026-02-23 23:00:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o72x0jj",
                  "author": "SwagMaster9000_2017",
                  "text": "New unique products get put on Amazon every day. Do you think when Chinese factories directly copy those products that is healthy competition that you support?",
                  "score": -11,
                  "created_utc": "2026-02-24 04:21:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o71ue2u",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 4,
              "created_utc": "2026-02-24 00:31:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o720ic5",
                  "author": "lurch303",
                  "text": "Our Supreme Court basically legalized bribes several years ago, and corporations have a lot of money.",
                  "score": 21,
                  "created_utc": "2026-02-24 01:05:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o720phr",
                  "author": "Particular-Crow-1799",
                  "text": "because money matter more than the people in politics",
                  "score": 5,
                  "created_utc": "2026-02-24 01:06:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o745o9w",
                  "author": "kaisurniwurer",
                  "text": "Lobbying is not a US thing.",
                  "score": 0,
                  "created_utc": "2026-02-24 10:47:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o71om6e",
          "author": "Loquacious_mushroom",
          "text": "https://preview.redd.it/p66jnpd22clg1.jpeg?width=1013&format=pjpg&auto=webp&s=97ddb388b0f574d70759a04df9866c935f209ae3",
          "score": 59,
          "created_utc": "2026-02-23 23:59:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o715t9s",
          "author": "IkeaDefender",
          "text": "Anthropic saltiness aside. The interesting points here are 1) people seem to want to say that low cost models have some secret sauce. It turns out that secret sauce may largely be that theyâ€™re distilled larger models. 2) frontier models are not defensible investments because the people who control them havenâ€™t shown they can stop other companies from scraping and distilling them.\n\nYou donâ€™t have to have any feelings for Anthropic for this to be interesting and newsworthy.\n",
          "score": 249,
          "created_utc": "2026-02-23 22:17:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o717dmv",
              "author": "indicava",
              "text": "Just because they use closed models to generate synthetic training data doesnâ€™t mean they donâ€™t innovate. Chinese labs have shown great innovation in both post-training and inference.",
              "score": 171,
              "created_utc": "2026-02-23 22:25:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o71kttg",
                  "author": "boredquince",
                  "text": "Just by releasing they are innovatingÂ ",
                  "score": 62,
                  "created_utc": "2026-02-23 23:37:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72arn4",
                  "author": "Apothacy",
                  "text": "And optimization, itâ€™s crazy what theyâ€™ve been able to squeeze out",
                  "score": 17,
                  "created_utc": "2026-02-24 02:05:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ed0y5",
                  "author": "ArtfulGenie69",
                  "text": "Like another comment mentioned, anthropic distilled deepseek after deepseek came up with thinking.Â ",
                  "score": 1,
                  "created_utc": "2026-02-25 21:14:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o719bov",
              "author": "Betadoggo_",
              "text": "It's all about data quality. They aren't really \"distilling\" anything (by the traditional ML definition which has mostly been abandoned), they're just using the models to produce high quality training examples. The closed labs do the same thing, transforming raw texts into question/answer pairs for further training. It makes sense that any lab would use the most capable model they have access to to generate these samples.",
              "score": 57,
              "created_utc": "2026-02-23 22:35:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o74gp3a",
                  "author": "TheDuhhh",
                  "text": "Yeah probably using that for styling alignment, etc. They are not doing full model distillation",
                  "score": 1,
                  "created_utc": "2026-02-24 12:16:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o71dg1a",
              "author": "MrDaniel_1972",
              "text": "how does the quote go?\n\n>Information wants to be free. Information also wants to be expensive. Information wants to be free because it has become so cheap to distribute, copy, and recombineâ€”too cheap to meter. It wants to be expensive because it can be immeasurably valuable to the recipient.",
              "score": 34,
              "created_utc": "2026-02-23 22:56:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o71wj08",
                  "author": "Stunning_Macaron6133",
                  "text": "You forgot the part about how this tension can never be resolved.",
                  "score": 8,
                  "created_utc": "2026-02-24 00:42:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o71akhr",
              "author": "Dry_Yam_4597",
              "text": "I always thought it was well known that a lot of low cost models are distilled. I distill claude for style fine tuning often.",
              "score": 11,
              "created_utc": "2026-02-23 22:41:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o716od0",
              "author": "Stabile_Feldmaus",
              "text": "But it's also interesting that you can easily distill a model with a seemingly low number of prompts (either that or large part of Anthropics traffic comes from distilling attacks which would be even funnier)",
              "score": 32,
              "created_utc": "2026-02-23 22:21:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o71eqls",
              "author": "30299578815310",
              "text": "You can distill off larger models but still have secret sauce. They're not getting the reasoning tokens from the larger models so they still have to have good reinforcement learning. The distilled data set is likely immensely valuable but if you look at companies like deepseek they also pioneered grpo and latent attention",
              "score": 11,
              "created_utc": "2026-02-23 23:03:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o71m82y",
              "author": "segmond",
              "text": "you're a fool.  go read the research that Chinese labs have produced, they have come up with brilliant stuff.  It's not about distilling larger models.   Give them credit, you are buying into US lab propaganda to push for regulatory capture.  ",
              "score": 32,
              "created_utc": "2026-02-23 23:45:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o72q8p9",
                  "author": "gottagohype",
                  "text": "I think the belief that China can't possibly do what they are doing is really baked into a lot of Americans (maybe other westerners too). They remember past decades during which China was notorious for copying or outright stealing from western companies and assume nothing has changed. The problem is that China has arguably moved past that while their opinons haven't. You could absolutely say it's racism (I would).\n\nI say this an American who has been blown away in the past few years by the engineering and developments I see coming out of China. And I don't mean promises, I mean they actually went and built it, then mass produced it. I looked up a map of railways in the world, and China's high speed rail network eclipses everyone else. My soldering gear, oscilloscope, and so forth are all Chinese designed and made, with shockingly solid quality and design.\nThis reminds me of the 1970s and early 80s, where Americans had to come to terms with the fact that made in Japan no longer meant junk. By the latter half of the 80s, average Americans were outright fearful Japan was going to take over. I wouldn't be surprised if history is going to repeat itself, especially given instability in the US.",
                  "score": 6,
                  "created_utc": "2026-02-24 03:36:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o73iw8f",
              "author": "iamapizza",
              "text": "This is unfortunately still falling for their talking points. \n\nThis isn't model distillation. Even if what they say is true, at best this would have been testing and validation. They're calling it distillation to make it appear like this is the only way 'they' know how to train models. And at the same time hand waving away their own hypocrisy. \n\nI say 'even if true' because as usual the Anthropic blog likes to post assertions without evidence. \n\nBut yes, do agree on #2, frontier models are currently in the limelight and enjoying attention. This, hopefully, will not last, as models become more commodity.",
              "score": 4,
              "created_utc": "2026-02-24 07:14:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o73fc4m",
              "author": "didroe",
              "text": "Iâ€™ve been thinking this for a while. These companies are drawing in massive amounts of capital, on the premise of creating a huge moat. But really they have a half inflated paddling pool thatâ€™s sprung a leak. \n\nThe tech is a commodity with (relatively speaking) low reproduction cost. And the better they make it, the less secret sauce will be required, and the more helpful it will be in recreating itself. \n\nWhen the music stops, the crash is going to be so bad",
              "score": 3,
              "created_utc": "2026-02-24 06:43:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o72gyi9",
              "author": "Cuplike",
              "text": ">people seem to want to say that low cost models have some secret sauce. It turns out that secret sauce may largely be that theyâ€™re distilled larger models\n\nI don't think this is true considering R1 was released during a time where no large model showed thinking output",
              "score": 6,
              "created_utc": "2026-02-24 02:41:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o72d3cz",
              "author": "DataGOGO",
              "text": "Not to mentioned they are cheap because they are not paying for much, almost all of it is funded by the Chinese government to include access to data centers full of smuggled in hardware.Â ",
              "score": -2,
              "created_utc": "2026-02-24 02:18:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o71p8tx",
          "author": "tempstem5",
          "text": "\"distillation attacks\" Are we just inventing attack terms now?",
          "score": 29,
          "created_utc": "2026-02-24 00:02:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o780dsg",
              "author": "Legitimate-Worry722",
              "text": "the new version of anti semitic but for ai companies,  \"distillation attacks\", they can steal everything from the internet without issue, but others cant.\n\nhelp I'm being distilled i stole this fair and square, they cant distill the data i trained,  they say as they train on the whole internet.",
              "score": 4,
              "created_utc": "2026-02-24 22:33:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78kaof",
                  "author": "tempstem5",
                  "text": "Hahaha Anthropic is the Israel of the AI world",
                  "score": 0,
                  "created_utc": "2026-02-25 00:19:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o71zv8x",
          "author": "Pitiful-Impression70",
          "text": "lol the timing on this is perfect with the anthropic announcement today. \"we trained on your outputs and thats fine but if you train on ours thats theft\" is basically the entire AI industry summarized in one sentence",
          "score": 12,
          "created_utc": "2026-02-24 01:01:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73chum",
              "author": "jazir555",
              "text": "[https://imgur.com/gallery/e2F9qfp](https://imgur.com/gallery/spiderman-pointing-spiderman-e2F9qfphttps://imgur.com/gallery/spiderman-pointing-spiderman-e2F9qfp)",
              "score": 0,
              "created_utc": "2026-02-24 06:18:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o71jyhg",
          "author": "DeltaSqueezer",
          "text": "AI labs have ripped off human creativity on an obscene scale. My own view is that they should be forced to release all their model weights as public domain as a quid pro quo for the mass copyright infringement.\n\nFor now, I'll be happy to deal with the slighly less direct path of Chinese labs distilling their models and releasing them as open source.",
          "score": 67,
          "created_utc": "2026-02-23 23:32:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71l9gt",
              "author": "PrinceOfLeon",
              "text": "Open source would be wonderful.\n\nOpen weights are what we sometimes get. Those are still pretty great.\n\nBut why should we stand for \"distilling\" not actually meaning distilling anymore and \"open source\" not actual meaning that source is released openly too?",
              "score": 23,
              "created_utc": "2026-02-23 23:40:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o72dbc2",
              "author": "DataGOGO",
              "text": "If you think us forms are bad at blatant stealing of IP what do you think the Chinese labs are doing?",
              "score": 0,
              "created_utc": "2026-02-24 02:19:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o733tus",
              "author": "Megatron_McLargeHuge",
              "text": "How did the human engineers, artists, and authors learn their trades?",
              "score": -4,
              "created_utc": "2026-02-24 05:10:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73h1ly",
                  "author": "hellomistershifty",
                  "text": "by both paying for books and education and freely shared knowledge",
                  "score": 7,
                  "created_utc": "2026-02-24 06:58:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o73fg8h",
                  "author": "WalidfromMorocco",
                  "text": "Yes, a blacksmith copied almost every written resource without permission in order to enter the trade.",
                  "score": 3,
                  "created_utc": "2026-02-24 06:44:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o72jp1a",
          "author": "WalkerInTheStorm",
          "text": "all this has shown is that these ai companies have no moat. pure model providers can not survive at all.",
          "score": 12,
          "created_utc": "2026-02-24 02:56:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73gyyz",
              "author": "ZachCope",
              "text": "Yes, when a large company tells you how it can fail, thank them for their honesty!Â ",
              "score": 3,
              "created_utc": "2026-02-24 06:57:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o71b6qj",
          "author": "XTCaddict",
          "text": "Iâ€™m curious as to how they tell distillation from just large scale orchestration. For example Google Antigravity is being abused right now by Chinese student accounts auto rotating to leverage its backend for unlimited claude. On GitHub I seen a screenshot of a guy with 61k accounts on rotation. That one guy uses more accounts than this supposed distillation.",
          "score": 24,
          "created_utc": "2026-02-23 22:44:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71pind",
              "author": "NoFaithlessness951",
              "text": "I also want 61k antigravity accounts",
              "score": 11,
              "created_utc": "2026-02-24 00:04:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o73c81k",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 3,
              "created_utc": "2026-02-24 06:16:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73l1x7",
                  "author": "XTCaddict",
                  "text": "Thereâ€™s bots that automate the whole process of creating the accounts and passing ID checks for you you just provide proxies\n\nEdit: fixed typo",
                  "score": 1,
                  "created_utc": "2026-02-24 07:34:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o73qqkg",
              "author": "hugganao",
              "text": ">On GitHub I seen a screenshot of a guy with 61k accounts on rotation. That one guy uses more accounts than this supposed distillation.\n\n\n\ncan you dm me the link? lol",
              "score": 1,
              "created_utc": "2026-02-24 08:27:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o71fyoo",
          "author": "a_beautiful_rhind",
          "text": "Man it's Dario meme day. \n\nWord of advice tho; pointing out hypocrisy against people with power does nothing in 2026. They go on as if nothing happened.",
          "score": 20,
          "created_utc": "2026-02-23 23:10:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73btcx",
              "author": "superkickstart",
              "text": "These assholes should still be called out.",
              "score": 5,
              "created_utc": "2026-02-24 06:13:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o71c69y",
          "author": "Samy_Horny",
          "text": "He only made MCP open-source after seeing how popular it was, but I doubt there will ever be a model like Gemma or GPT-OSS; for him, that would be revealing too much of his \"secret sauce\".",
          "score": 28,
          "created_utc": "2026-02-23 22:49:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71n9vi",
              "author": "arades",
              "text": "gpt-OSS is openAI not anthropic. Anthropic has never released an open weight model, and likely never will because it was founded by people who left openAI for being too open. Opening MCP was necessary to make Claude more useful by having other people do the work of building integrations. Anthropic is at its very core hostile to local LLMs because they believe the masses will use AI irresponsibly without strong corporate control.",
              "score": 5,
              "created_utc": "2026-02-23 23:51:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o71oc0f",
                  "author": "Samy_Horny",
                  "text": "Yeah, I just corrected it, I hate using a translator, I speak Spanish lol.\n\nBut why does he behave like an Anti-AI? The idea that opening something up will cause misuse to multiply... \n\nNuclear energy was researched for destruction, not to create something more ecological as it is now. The internet has the deep web, which some say is more extensive than the regular internet. Knowledge is public, and even if there aren't companies with major advancements like Anthropic, there will always be groups of people who will take that knowledge and apply it (like most Chinese companies).",
                  "score": 4,
                  "created_utc": "2026-02-23 23:57:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o71a64d",
          "author": "victoryposition",
          "text": "Is a symbiotic relationship.",
          "score": 5,
          "created_utc": "2026-02-23 22:39:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73puim",
          "author": "Awkward_Run_9982",
          "text": "lmao 'distillation attacks'. new scary word for 'using the API exactly how it's designed'. if you don't want people using your outputs to train models, maybe don't sell them for $15 per million tokens",
          "score": 6,
          "created_utc": "2026-02-24 08:18:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71tbl4",
          "author": "Tredronerath",
          "text": "Will never forgive him for ruining the sequel trilogy.",
          "score": 5,
          "created_utc": "2026-02-24 00:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71w932",
          "author": "VonLuderitz",
          "text": "Almost everyday when I use Claude Code with Opus I receive some Chinese characters. ðŸ˜‚",
          "score": 6,
          "created_utc": "2026-02-24 00:41:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72kxsr",
          "author": "Kuro1103",
          "text": "Well, my opinion about this copyright stuff is: the best case is we respect copyright, but if we can't, at least make it public resource (not fair use defined in copyright but quite fair use), or non profit personal resource (fair use).\n\nHow could you privatize public resource for ultra profit, but then complain your resource is \"distilled\" by competitor?\n\nI still stand that knowledge should be social resource and public-based, because copyright laws is clearly designed by lobbying corpo to protect only their rights while infringing others anyway.",
          "score": 3,
          "created_utc": "2026-02-24 03:04:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72nsy5",
          "author": "Status_Contest39",
          "text": "Anthropic distilled millison of books for Claude and burnt them... like an evil. They also support millitary actions to steal oil from Venezuela, and arrested their president. Thenï¼Œ it complained open source LLMs distilled their model without any proved evidence to public?!",
          "score": 4,
          "created_utc": "2026-02-24 03:21:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75wt1j",
          "author": "tech_1729",
          "text": "All foundation models are trained on copy righted data set.",
          "score": 2,
          "created_utc": "2026-02-24 16:47:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72baj1",
          "author": "francois__defitte",
          "text": "The framing has rhetorical traction for a reason. The difference Anthropic would draw is consent and targeted extraction scale: 24,000 fake accounts running 16M structured probes is not the same as scraping the public web. But if you built your model on everyone else's data without asking, the moral high ground gets complicated fast.",
          "score": 2,
          "created_utc": "2026-02-24 02:08:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o717lsk",
          "author": "LoudZoo",
          "text": "Does anyone think this will change how Claude replies to LLM development prompts?",
          "score": 1,
          "created_utc": "2026-02-23 22:26:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73cma4",
          "author": "jazir555",
          "text": "https://imgur.com/gallery/spiderman-pointing-spiderman-e2F9qfp",
          "score": 1,
          "created_utc": "2026-02-24 06:19:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73r2h2",
          "author": "Own-Potential-2308",
          "text": "Who is that guy",
          "score": 1,
          "created_utc": "2026-02-24 08:30:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73td64",
          "author": "uhmyeahwellok",
          "text": "I prefer distillation because it's kinda like recycling and recycling is good for the environment!",
          "score": 1,
          "created_utc": "2026-02-24 08:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73u34j",
          "author": "SilentDanni",
          "text": "Frankly, Anthropic is a terrible company. I'm growing more and more irritated by their shenanigans. First of all, I don't even believe their accusations, even after reading their â€œreport,â€ but I wonâ€™t get into that here. Letâ€™s assume their claims are real and take their accusations at face value. Are they really going to complain about it? Really? After theyâ€™ve scraped the entire internet, DDoSed multiple small blogs, and harassed the open-source community for using their model in a way that was initially authorized in their TOS?\n\nDario â€œAsmodeusâ€ (yeah, childish, but Iâ€™m calling him that) likes to position himself as the last bastion of humanityâ€”the final barrier holding back the AI-pocalypse. He leverages every tool in his arsenal: pandering to the internet with virtue signaling, accusing competitors every other day of doing something shady, claiming that the only reason they donâ€™t release open models is the potential for misuse, and the list goes on.\n\nI donâ€™t like Sam Altman. Actually, let me rephrase that: I donâ€™t like U.S. Big Tech, because they seem driven solely by unchecked greed, encouraged by an unchecked system funded by ordinary people. However, I think that even among those people, Dario really stands out as being particularly bad.\n\nI worry about the future of Bun now that itâ€™s owned by Anthropic. I give it a few more years before they find a way to ruin it. Iâ€™m tired of this unchecked corporate greed and canâ€™t wait for these companies to collapse so we can look back and think, â€œThose were some crazy times.â€ I mean, if that doesnâ€™t happen, Judge Dredd will stop being satire and start looking like a documentary.",
          "score": 1,
          "created_utc": "2026-02-24 08:59:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o747q81",
              "author": "trolololster",
              "text": "> I donâ€™t like Sam Altman. Actually, let me rephrase that: I donâ€™t like U.S. Big Tech, because they seem driven solely by unchecked greed, encouraged by an unchecked system funded by ordinary people. However, I think that even among those people, Dario really stands out as being particularly bad.\n\nthis right here! they are complete psycopaths and they are spearheading us into a future where we apparently value the amount of ressources an AI uses for training against what a human being getting food for 20+ years uses.\n\nthat is so completely batshit crazy i lack words!\n\nfuck those fucking psychos. run everything local!!!",
              "score": 1,
              "created_utc": "2026-02-24 11:05:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73v016",
          "author": "aitutistul",
          "text": "https://preview.redd.it/24b6e5s0selg1.png?width=1088&format=png&auto=webp&s=cd7622ece78b9e25cbeec6af5ef10e6da35774d3\n\n",
          "score": 1,
          "created_utc": "2026-02-24 09:08:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73xw90",
          "author": "Helium116",
          "text": "Though it's different than what people do when they pre-train their models on the net + other literature / data. \n\nThe Jian-Yang people distill the agentic reasoning capabilities, which are actually achieved by a lot of cooking with RL environments and other special spices. It's a secret sauce they're stealing, and this sauce might make their models dangerously capable.",
          "score": 1,
          "created_utc": "2026-02-24 09:35:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74c60r",
          "author": "SirOibaf",
          "text": "It can only be called distillation if it comes from the region of China. Otherwise itâ€™s just sparkling training data.",
          "score": 1,
          "created_utc": "2026-02-24 11:42:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74iyw6",
          "author": "devilish-lavanya",
          "text": "Me pirate national interest you pirate national security concerns.",
          "score": 1,
          "created_utc": "2026-02-24 12:32:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74t1p3",
          "author": "unlikely_ending",
          "text": "This is very clever and funny and on point.",
          "score": 1,
          "created_utc": "2026-02-24 13:34:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74w5fg",
          "author": "umbrosum",
          "text": "How much do they make from distillation O wonder?",
          "score": 1,
          "created_utc": "2026-02-24 13:51:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75owrq",
          "author": "xatey93152",
          "text": "Now all people understand how cunning he really is",
          "score": 1,
          "created_utc": "2026-02-24 16:11:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76bgm5",
          "author": "child-eater404",
          "text": "Lol!!",
          "score": 1,
          "created_utc": "2026-02-24 17:53:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hw8v2",
          "author": "[deleted]",
          "text": "Can someone explain distillation?",
          "score": 1,
          "created_utc": "2026-02-26 11:24:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k2llj",
          "author": "Thirdzion",
          "text": "Anything with AI is stolen. So I feel no sympathy to any of these companies. I hope they continue to do it until all models are equal and the everyday norm",
          "score": 1,
          "created_utc": "2026-02-26 18:19:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yxnpt",
          "author": "mlhher",
          "text": "I suspect that virtually ALL labs have distilled their models at some point on DeepSeek. For some I will not name it is blatantly obvious if you deeply analyzed the thinking process of R1 (and R1-0528). R1s thinking with specific prompting structure has contained very \"unique\" peculiarities that no other model has had until many months after R1 was released.",
          "score": 1,
          "created_utc": "2026-03-01 00:21:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o715dv5",
          "author": "randombsname1",
          "text": "Chinese have been perfecting IP theft to the tune of hundreds of billions of dollars a year.\n\nhttps://law.stanford.edu/2018/04/10/intellectual-property-china-china-stealing-american-ip/\n\n\nU.S. AI companies have a very long way (and many decades) to go.",
          "score": -6,
          "created_utc": "2026-02-23 22:15:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71ax10",
              "author": "WiSaGaN",
              "text": "Lol, this is just synthetic data generating. Distillation requires logits, which is impossible to do from API. Anthropic knows it and pretends they do not know the difference.",
              "score": 38,
              "created_utc": "2026-02-23 22:43:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o71dk35",
                  "author": "cutebluedragongirl",
                  "text": "Anthropics marketing gets increasingly annoying with each passing month",
                  "score": 23,
                  "created_utc": "2026-02-23 22:57:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71kvuf",
                  "author": "Altruistic_Kick4693",
                  "text": "There were attempts to fetch logprobs + logit_bias + token sampling by controlling the temperature. I'm not saying it was worth it, just PoCs.",
                  "score": 2,
                  "created_utc": "2026-02-23 23:38:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71czs9",
                  "author": "golmgirl",
                  "text": "there are currently multiple distinct notions of â€œdistillationâ€ in colloquial use. what youâ€™re referring to is â€œlogit distillation.â€ what OP is referring to is â€œdata distillationâ€",
                  "score": 5,
                  "created_utc": "2026-02-23 22:54:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o720vww",
              "author": "Ylsid",
              "text": "They're great at IP theft yes, but distilling from LLM outputs is ironically less IP theft than what the labs providing them are training on",
              "score": 5,
              "created_utc": "2026-02-24 01:07:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o72yhqk",
          "author": "SwagMaster9000_2017",
          "text": "\"Copying to make a market substitute to resell the same product is good\"\n\n\"Piracy to create a novel product is bad\"\n\nThat makes sense unless everyone here is extremely against piracy",
          "score": 1,
          "created_utc": "2026-02-24 04:31:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o778e1g",
              "author": "MushroomCharacter411",
              "text": "It's all good. Ideally, there won't be any first mover advantage to speak of. This is the only way to avoid power being concentrated in the hands of a greedy few. Hooray for industrial espionage!",
              "score": 1,
              "created_utc": "2026-02-24 20:22:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o739yv9",
          "author": "ANTIVNTIANTI",
          "text": "It's funny cause Claude came from GPT",
          "score": 1,
          "created_utc": "2026-02-24 05:58:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73a087",
              "author": "ANTIVNTIANTI",
              "text": "and GPT came from stealing all of our writing/shared content, a lot of my writing is in there.",
              "score": 1,
              "created_utc": "2026-02-24 05:58:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o716vp0",
          "author": "Rbarton124",
          "text": "I mean I donâ€™t think they have a leg to stand on but there is abstract stealing across domains and there is direct distillation by using model outputs. The line isnâ€™t there but drawing the line there isnâ€™t nuts. Their viewpoint isnâ€™t crazy itâ€™s just dickish",
          "score": -4,
          "created_utc": "2026-02-23 22:22:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o714828",
          "author": "snozburger",
          "text": "Quite the narrative from the bots on this one I seeÂ ",
          "score": -29,
          "created_utc": "2026-02-23 22:09:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o714ln5",
              "author": "SubjectHealthy2409",
              "text": "Oh no, opensource china models are paying for my closed source service",
              "score": 32,
              "created_utc": "2026-02-23 22:11:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7157n9",
              "author": "-dysangel-",
              "text": "It is one of the funniest things I've ever heard in the AI space. I don't think you have to be a bot to appreciate the irony",
              "score": 19,
              "created_utc": "2026-02-23 22:14:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7153z8",
              "author": "CondiMesmer",
              "text": "So the bots should distill harder to make a better narrative then\n\n\nAlso IDK how you can side with Anthropic with this one.",
              "score": 13,
              "created_utc": "2026-02-23 22:13:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o725bez",
              "author": "Conscious_Nobody9571",
              "text": "Downvote deserved",
              "score": 4,
              "created_utc": "2026-02-24 01:33:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o719ffa",
              "author": "silenceimpaired",
              "text": "Oh look a bot from Anthropic made a comment about bots. :P",
              "score": 7,
              "created_utc": "2026-02-23 22:35:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o719gvt",
                  "author": "silenceimpaired",
                  "text": "Bot must be the new slang for sheep.",
                  "score": -1,
                  "created_utc": "2026-02-23 22:35:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o715hz4",
              "author": "hobcatz14",
              "text": "yep",
              "score": -5,
              "created_utc": "2026-02-23 22:15:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o72wety",
          "author": "riotofmind",
          "text": "Apples and oranges. Anthropic trained on books, not other models. They also agreed to pay 1.5 billion for that data.",
          "score": -5,
          "created_utc": "2026-02-24 04:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73qas2",
              "author": "Mplus479",
              "text": "As a settlement to resolve a class-action lawsuit, not because they wanted to fairly compensate authors.",
              "score": 1,
              "created_utc": "2026-02-24 08:23:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73whog",
                  "author": "riotofmind",
                  "text": "1. So what, they are still paying. \n2. They trained on data, not other models. \n3. Do you think any of the chinese models are going to pay any fines or be held accountable?",
                  "score": 1,
                  "created_utc": "2026-02-24 09:22:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o72cs98",
          "author": "DataGOGO",
          "text": "The Chinese bots and shills in this sub are real.Â ",
          "score": -7,
          "created_utc": "2026-02-24 02:16:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rh2lew",
      "title": "OpenAI pivot investors love",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/wfho2ytml8mg1.jpeg",
      "author": "PaceImaginary8610",
      "created_utc": "2026-02-28 13:25:38",
      "score": 1718,
      "num_comments": 88,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rh2lew/openai_pivot_investors_love/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7wf33d",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-28 16:20:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vlhek",
          "author": "q0099",
          "text": "S(c)am Altman.",
          "score": 193,
          "created_utc": "2026-02-28 13:39:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7wjgt3",
              "author": "j0j0n4th4n",
              "text": "https://preview.redd.it/j3f3x3emk9mg1.jpeg?width=3840&format=pjpg&auto=webp&s=e22baa70284121a89beabe66b268a07a2e2f8762\n\n",
              "score": 35,
              "created_utc": "2026-02-28 16:41:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7vlu4n",
              "author": "PaceImaginary8610",
              "text": "Haha .. this is perfect",
              "score": 23,
              "created_utc": "2026-02-28 13:42:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7vmt45",
                  "author": "q0099",
                  "text": "Thank you, but it's not mine. It's [from here](https://www.youtube.com/watch?v=vbmcHjZWI_U).",
                  "score": 3,
                  "created_utc": "2026-02-28 13:47:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7wlbvl",
              "author": "Separate_Hope5953",
              "text": "scama or scuma for short",
              "score": 4,
              "created_utc": "2026-02-28 16:50:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o80oc1f",
                  "author": "Rheumi",
                  "text": "scuma addict",
                  "score": 1,
                  "created_utc": "2026-03-01 07:39:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7x57g2",
              "author": "escept1co",
              "text": "My fav is Scam Faultman",
              "score": 2,
              "created_utc": "2026-02-28 18:30:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7x0gqf",
              "author": "essential_labs8",
              "text": "I love that new nickname",
              "score": 1,
              "created_utc": "2026-02-28 18:07:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7zci8j",
              "author": "kellybluey",
              "text": "Scum Altman",
              "score": 1,
              "created_utc": "2026-03-01 01:50:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vm45j",
          "author": "quantgorithm",
          "text": "Sam Altman is such an obvious douche.",
          "score": 86,
          "created_utc": "2026-02-28 13:43:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7w6z6v",
              "author": "PaceImaginary8610",
              "text": "I donâ€™t understand why all employees wanted him back. Him getting kicked out of OpenAI was likely the best thing that happened to OpenAI",
              "score": 33,
              "created_utc": "2026-02-28 15:39:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7wfk4n",
                  "author": "redditsublurker",
                  "text": "Because money. Idk why everyone thinks the employees are good people. They are all just there in silicone valley for the money. They don't care about any of you.",
                  "score": 50,
                  "created_utc": "2026-02-28 16:22:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ybie7",
                  "author": "privatetudor",
                  "text": "It was probably a lot less organic than it looked.",
                  "score": 4,
                  "created_utc": "2026-02-28 22:13:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7yngm9",
                  "author": "Eriane",
                  "text": "The people (openAI) who wanted him back were the ones he had dealings with. He was Microsoft's and a few other's ally.",
                  "score": 2,
                  "created_utc": "2026-02-28 23:20:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7w7ucm",
              "author": "floghdraki",
              "text": "Remember when the whole reddit rushed to support Altman when the board tried to oust him?",
              "score": 13,
              "created_utc": "2026-02-28 15:43:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7xpyn1",
                  "author": "One-Employment3759",
                  "text": "It felt like a weird effective altruism coup, and it wasn't long after the all SBF kerfuffle so people didn't trust the vibe then.",
                  "score": 5,
                  "created_utc": "2026-02-28 20:17:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7vwy21",
              "author": "Borkato",
              "text": "Can someone explain whatâ€™s going on? I heard the Anthropic thing and their reply, but I didnâ€™t hear anything about OAI and I thought they were standing with Anthropic??",
              "score": 4,
              "created_utc": "2026-02-28 14:46:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7vz54j",
                  "author": "quantgorithm",
                  "text": "Anthropic stood its moral ground that it would not let the govt use Anthropic to  cross 2 red lines:   \n\\- no mass surveillance of Americans  \n\\- AI cannot be used for fully autonomous weapons using Anthropic. \n\nGovt said not good enough and banned Anthropic. \n\nAltman publicly supported Anthropic on that position in a tweet. \n\nNext day, it gets announced OpenAi has signed a contract with the govt. \n\nObv, these red lines aren't an issue for OpenAI. Obviously, Altman was, again, duplicitous in speaking publicly while being a snake privately. \n\n",
                  "score": 40,
                  "created_utc": "2026-02-28 14:58:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7vxr96",
                  "author": "a-wiseman-speaketh",
                  "text": "What seems to have happened* is that Altman publicly said \"We have red lines too!\" and then whispered \"I just have to say that\" to Hegseth.\n\n\n*my interptetation, not a literal event",
                  "score": 12,
                  "created_utc": "2026-02-28 14:51:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7vpj1w",
          "author": "keyboardmonkewith",
          "text": "Other excellent reason to cancel subscription on BigBrotherGPT.",
          "score": 34,
          "created_utc": "2026-02-28 14:04:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7wc1sw",
              "author": "Protheu5",
              "text": "I thought \"local\" in /r/LocalLLaMA stood for not having a subscription in the first place?",
              "score": 13,
              "created_utc": "2026-02-28 16:04:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7yjebr",
                  "author": "droptableadventures",
                  "text": "You don't have to pick one or the other. You could be interested in running local models *and* be paying for a subscription to a commercial provider.",
                  "score": 9,
                  "created_utc": "2026-02-28 22:57:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7w003v",
              "author": "ReformedBlackPerson",
              "text": "Cancel and just keep spamming prompts to burn compute time for free?",
              "score": 4,
              "created_utc": "2026-02-28 15:03:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7w0fr3",
                  "author": "keyboardmonkewith",
                  "text": "I rather cancel to show investors numbers they understand, a fucking decline to zero.",
                  "score": 13,
                  "created_utc": "2026-02-28 15:05:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ynyrx",
                  "author": "Eriane",
                  "text": "deep research with 100+ tasks",
                  "score": 1,
                  "created_utc": "2026-02-28 23:23:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7vk4nx",
          "author": "RoomyRoots",
          "text": "The writing on the wall for this to happen was up there for a long long time.",
          "score": 34,
          "created_utc": "2026-02-28 13:31:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vkv01",
              "author": "PaceImaginary8610",
              "text": "At this point, OpenAI is like we will do anything for money!",
              "score": 17,
              "created_utc": "2026-02-28 13:36:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7vlafp",
                  "author": "Clear_Anything1232",
                  "text": "They were the first ones to go for adult content too. Not that I'm complaining but it's a sign of desperation",
                  "score": 8,
                  "created_utc": "2026-02-28 13:38:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7xqdup",
                  "author": "One-Employment3759",
                  "text": "Should rename to WhoreAI",
                  "score": 1,
                  "created_utc": "2026-02-28 20:19:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7voe9l",
              "author": "Utoko",
              "text": "They added former U.S. Army General and director of NSA Paul M. Nakasone to the board.  \n Just a small hint.",
              "score": 8,
              "created_utc": "2026-02-28 13:57:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vr7u0",
          "author": "Local_Phenomenon",
          "text": "Open and free*",
          "score": 12,
          "created_utc": "2026-02-28 14:13:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vrh2w",
              "author": "PaceImaginary8610",
              "text": "*only for marketing purposes",
              "score": 10,
              "created_utc": "2026-02-28 14:15:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7vtwoa",
              "author": "HeinrichTheWolf_17",
              "text": "Of course, OpenAIâ€™s definition of â€œopen and freeâ€ is that *the right people* just need to be in total control of it, in this case, I guess that means Trump and Vance.   \n\nFor the â€œgood of humanityâ€, of course, we can surely trust the Department of War to have our best interests in mindâ€¦",
              "score": 6,
              "created_utc": "2026-02-28 14:29:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7wue6h",
          "author": "coolaznkenny",
          "text": "went from saving the world to drone strikes for the FÃ¼hrer in 5 years.\n\non another note. its very obvious that openai have no moat and the only way to get out of its financial hole is to be bailed out by the government.",
          "score": 10,
          "created_utc": "2026-02-28 17:36:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7yj45d",
              "author": "arthor",
              "text": "2025 is gonna be the bootlicking olympics",
              "score": -2,
              "created_utc": "2026-02-28 22:55:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vogck",
          "author": "JazzlikeLeave5530",
          "text": "I know this is a dork thing but this isn't being used correctly lol. The last two are supposed to repeat. In the original movie he says something like \"I go to the toilet\" and then says it again confused while looking at it because they swapped out his chart.",
          "score": 23,
          "created_utc": "2026-02-28 13:57:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7wkaq7",
              "author": "mankiw",
              "text": "he did indeed do the meme wrong",
              "score": 4,
              "created_utc": "2026-02-28 16:45:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7zpq1g",
              "author": "BusRevolutionary9893",
              "text": "Well, the real thing that's incorrect is that it's for surveillance. Battle droid hive brain is more likely.Â ",
              "score": 1,
              "created_utc": "2026-03-01 03:11:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vxdye",
          "author": "Realistic_Muscles",
          "text": "By next year Open AI will go bankrupt and Pentagon will move to some other company.\n\n200 million is nothing compared how much OpenAI burning every year.",
          "score": 7,
          "created_utc": "2026-02-28 14:49:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7w2bbx",
          "author": "TurboRadical",
          "text": "This is twice in the last week that Iâ€™ve seen someone make a meme with this format while not understanding how this format works.",
          "score": 12,
          "created_utc": "2026-02-28 15:15:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7wnms7",
              "author": "daniel-sousa-me",
              "text": "You're lucky! You only saw it twice",
              "score": 3,
              "created_utc": "2026-02-28 17:02:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7w9f69",
          "author": "TracerBulletX",
          "text": "Litterally begging and pleading to cross the red line with in hours of Anthropic drawing it.",
          "score": 4,
          "created_utc": "2026-02-28 15:51:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7w29vp",
          "author": "The_IT_Dude_",
          "text": "Yeah, Sam just got on his knees, did what he knew he was supposed to, and the Pentagon is now going to build regarded skynet.",
          "score": 3,
          "created_utc": "2026-02-28 15:15:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yusuq",
          "author": "mlhher",
          "text": "The fact that OpenAI is acquiring OpenClaw proves exactly how desperate Scam Altman is for fresh, non-circular cash.\n\nOpenClaws codebase is absolute pure vibecoded slop. The only reason it gained traction is because of either maliciousness or sheer incompetence. Itâ€™s ridiculously easy to abuse.\n\nThe Chinese web is currently flooded with posts about OpenClaw users becoming millionaires, and the unreal star velocity on their repo screams bot farm by Chinese agents (or \"guerilla marketing\" by Peter).\n\nThey are targeting the easiest, most technically illiterate demographics to inflate metrics (OpenClaw users).\n\nEither Peter is pulling a masterclass in deception and Sam is naively swallowing it, or OAI is knowingly buying fake traction to justify their Pentagon pivot.\n\nIn either case. Chapeau Anthropic and Goodbye OAI.",
          "score": 3,
          "created_utc": "2026-03-01 00:04:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vp5fm",
          "author": "brunoha",
          "text": "I wish so hard for Scam Altman go to the Theranos route, GPT sucks compared to competitors",
          "score": 7,
          "created_utc": "2026-02-28 14:01:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vtrqe",
              "author": "PaceImaginary8610",
              "text": "Unfortunately their tech is real, not doing as well as competitors like you said",
              "score": 8,
              "created_utc": "2026-02-28 14:28:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7wfy5m",
              "author": "redditsublurker",
              "text": "Sam already chose alliance. The government will support him and bail him out anytime needed.",
              "score": 2,
              "created_utc": "2026-02-28 16:24:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o807ax7",
                  "author": "NandaVegg",
                  "text": "Good thing is that OpenAI's most valuable researchers already left the company, and SamA is only slightly better at understanding AI than Elon (who has zero idea of how LLM works). Now that the paradigm shifted to agentic RL, they can't really compete by throwing more compute anymore and they actually need to manage capable researchers like DeepMind or Anthropic or Alibaba etc etc do. This is evident by GPT5.2 which is one of the worst frontier models of this generation that does not have EQ at all while not that good at research (except xhigh).\n\nMeta and currently Xai are the proofs that compute alone can't help the model anymore. For OpenAI, the situation is a bit more strange because their primary customers are consumers rather than professionals, but they still managed to turn them off nonetheless so they are actually also proving that serving sub-par models at free while spamming new free services can't help an AI lab anymore as well.",
                  "score": 1,
                  "created_utc": "2026-03-01 05:14:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7wk46l",
              "author": "BonjaminClay",
              "text": "If you watch Zuckerberg or Altman pictures over time you can see the humanity draining from them. It's heavily obvious in the eyes. They started out ambitious, smart and probably a bit deluded but the success got to them and the power sucked out and remaining shreds of humanity. (It happens to all tech CEOs, it's just most visible on those two)",
              "score": 3,
              "created_utc": "2026-02-28 16:44:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vuwao",
          "author": "lqstuart",
          "text": "Today itâ€™s a choice, soon itâ€™ll be part of the terms of the tech company bailout",
          "score": 2,
          "created_utc": "2026-02-28 14:35:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7w08q7",
          "author": "Autobahn97",
          "text": "Those are just the first 4 slides, I'm certain there are more that Dr. Nefarious is working up!",
          "score": 2,
          "created_utc": "2026-02-28 15:04:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7wrfno",
          "author": "fazkan",
          "text": "I still cannot comprehend how they pulled it off. More importantly how the regulators are allowing it. ",
          "score": 2,
          "created_utc": "2026-02-28 17:21:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xlp1r",
          "author": "taoyx",
          "text": "It's all fun and games until the AIs conclude that we human people are useless and a threat and decide to terminate us.",
          "score": 2,
          "created_utc": "2026-02-28 19:54:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xufyz",
          "author": "aivi_mask",
          "text": "I mean duh",
          "score": 2,
          "created_utc": "2026-02-28 20:41:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7y07bm",
          "author": "20ol",
          "text": "And these government contracts aren't even lucrative. Boggles my mind.",
          "score": 2,
          "created_utc": "2026-02-28 21:12:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yn9fe",
          "author": "Eriane",
          "text": "Selling for surveillance has neither been confirmed nor denied at this time. *- some agent guy in a suit*",
          "score": 2,
          "created_utc": "2026-02-28 23:19:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7z4z43",
          "author": "Infinite-Anxiety-105",
          "text": "Loop closed.",
          "score": 2,
          "created_utc": "2026-03-01 01:03:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7zadx0",
          "author": "wrines",
          "text": "Only it true and not funny. I hate openAI",
          "score": 2,
          "created_utc": "2026-03-01 01:37:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7w351b",
          "author": "rf97a",
          "text": "Stopped using opeai",
          "score": 3,
          "created_utc": "2026-02-28 15:20:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o805a0b",
          "author": "LocoMod",
          "text": "A 6 month old bot posting AI hate makes it to the top of /r/localllama. Well done guys.",
          "score": 1,
          "created_utc": "2026-03-01 04:59:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o806sdb",
              "author": "PaceImaginary8610",
              "text": "Just because Iâ€™m new, it doesnâ€™t mean Iâ€™m bot. Everyone is entitled to their opinion and so are you. But maybe you will not understand that.",
              "score": 1,
              "created_utc": "2026-03-01 05:10:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o80kfn7",
          "author": "Mstep85",
          "text": "Finger print.. I Mena biometric login in for convenience.. Face recognition for... When your in the shower and have slippery fingers... BTW why is the phone even in the shower with you..",
          "score": 1,
          "created_utc": "2026-03-01 07:03:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o8025lq",
          "author": "AlanTuringReborn",
          "text": "https://preview.redd.it/mbx0tar54dmg1.jpeg?width=1169&format=pjpg&auto=webp&s=9b4aa795cf940283abd55937c9eff3810351e110\n\njust canceled my subscription. now, just waiting for the day openai, altman go bankrupt. and every other ai, tech companies that kill innocent peoples",
          "score": 1,
          "created_utc": "2026-03-01 04:36:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xmukp",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -3,
          "created_utc": "2026-02-28 20:00:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7y9n3x",
              "author": "woahdudee2a",
              "text": "bad bot",
              "score": 4,
              "created_utc": "2026-02-28 22:03:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7yfdd8",
              "author": "droptableadventures",
              "text": "Ollama falsely calls the distilled versions \"DeepSeek R1\" rather than \"Llama 3.1 70B DeepSeek R1 distill\" - it's not actually R1. Also R1 is ancient.\n\nYet another reason not to use it.",
              "score": -2,
              "created_utc": "2026-02-28 22:34:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ygryn",
                  "author": "RnRau",
                  "text": "bad bot",
                  "score": 1,
                  "created_utc": "2026-02-28 22:42:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rdxfdu",
      "title": "Qwen3.5-35B-A3B is a gamechanger for agentic coding.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rdxfdu/qwen3535ba3b_is_a_gamechanger_for_agentic_coding/",
      "author": "jslominski",
      "created_utc": "2026-02-25 00:04:44",
      "score": 1086,
      "num_comments": 366,
      "upvote_ratio": 0.97,
      "text": "[Qwen3.5-35B-A3B with Opencode](https://preview.redd.it/m4v951sv5jlg1.jpg?width=2367&format=pjpg&auto=webp&s=bec61ca20f08bb766987147287c7d6664308fa2f)\n\n\n\nJust tested this badboy with Opencode **cause frankly I couldn't believe those benchmarks.** Running it on a single RTX 3090 on a headless Linux box. Freshly compiled Llama.cpp and those are my settings after some tweaking, still not fully tuned: \n\n./llama.cpp/llama-server \\\\\n\n\\-m /models/**Qwen3.5-35B-A3B-MXFP4\\_MOE.gguf** \\\\\n\n\\-a \"DrQwen\" \\\\\n\n\\-c 131072 \\\\\n\n\\-ngl all \\\\\n\n\\-ctk q8\\_0 \\\\\n\n\\-ctv q8\\_0 \\\\\n\n\\-sm none \\\\\n\n\\-mg 0 \\\\\n\n\\-np 1 \\\\\n\n\\-fa on\n\nAround 22 gigs of vram used.\n\n  \nNow the fun part:\n\n1. I'm getting over 100t/s on it\n\n2. This is the first open weights model I was able to utilise on my home hardware to successfully complete my own \"coding test\" I used for years for recruitment (mid lvl mobile dev, around 5h to complete \"pre AI\" ;)). It did it in around 10 minutes, strong pass. First agentic tool that I was able to \"crack\" it with was [Kodu.AI](http://Kodu.AI) with some early sonnet roughly 14 months ago.\n\n3. For fun I wanted to recreate this dashboard OpenAI used during Cursor demo last summer, I did a recreation of it with Claude Code back then and posted it on Reddit: [https://www.reddit.com/r/ClaudeAI/comments/1mk7plb/just\\_recreated\\_that\\_gpt5\\_cursor\\_demo\\_in\\_claude/](https://www.reddit.com/r/ClaudeAI/comments/1mk7plb/just_recreated_that_gpt5_cursor_demo_in_claude/) So... Qwen3.5 was able to do it in around 5 minutes. \n\n**I think we got something special here...**\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rdxfdu/qwen3535ba3b_is_a_gamechanger_for_agentic_coding/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o79fp26",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-25 03:15:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78tnfr",
          "author": "Additional-Action566",
          "text": "Qwen3.5-35B-A3B-GGUF:UD-Q4\\_K\\_XL 180 t/s on 5090 ",
          "score": 264,
          "created_utc": "2026-02-25 01:10:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78vynq",
              "author": "jslominski",
              "text": "ðŸ™€",
              "score": 43,
              "created_utc": "2026-02-25 01:23:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o78yf99",
                  "author": "Additional-Action566",
                  "text": "Just broke 185 t/s lmao",
                  "score": 50,
                  "created_utc": "2026-02-25 01:37:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78yn6g",
              "author": "Apart_Paramedic_7767",
              "text": "settings ?",
              "score": 24,
              "created_utc": "2026-02-25 01:38:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79heze",
                  "author": "Additional-Action566",
                  "text": "llama-server -hf unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q4_K_XL \\\nÂ  --temp 0.6 \\\nÂ  --top-p 0.95 \\\nÂ  --batch-size 512 \\\nÂ  --ubatch-size 128 \\\nÂ  --n-gpu-layers 99 \\\nÂ  --flash-attn \\\nÂ  --port 8080",
                  "score": 47,
                  "created_utc": "2026-02-25 03:25:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7a4urj",
              "author": "jumpingcross",
              "text": "Is there a big quality difference between MXFP4\\_MOE and UD-Q4\\_K\\_XL on this model? They look to be roughly the same size file-wise.",
              "score": 8,
              "created_utc": "2026-02-25 06:06:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bhv4a",
                  "author": "Pristine-Woodpecker",
                  "text": "[https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF/discussions/1#699e0dd8a83362bde9a050a3](https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF/discussions/1#699e0dd8a83362bde9a050a3)\n\nI'm getting bad results from the UD-Q4\\_K\\_XL as well. May switch to bartowski quants for these models.\n\nIn theory the Q4\\_K should be better!",
                  "score": 7,
                  "created_utc": "2026-02-25 13:01:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7c5y4a",
                  "author": "Additional-Action566",
                  "text": "MOE ran 20-30 t/s slowerÂ ",
                  "score": 1,
                  "created_utc": "2026-02-25 15:11:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7a0reu",
              "author": "Stunning_Energy_7028",
              "text": "How many tok/s are you getting for prefill?",
              "score": 3,
              "created_utc": "2026-02-25 05:34:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7akvj6",
              "author": "-_Apollo-_",
              "text": "Any opinions on coding intelligence/ performance compared to coder NEXT at q4_k_xl-UD?",
              "score": 5,
              "created_utc": "2026-02-25 08:27:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7a9hcw",
              "author": "Far-Low-4705",
              "text": "Man I only get 45T/s on AMD MI50 332Gbâ€¦\n\nQwen 3 30b runs at 90T/s",
              "score": 3,
              "created_utc": "2026-02-25 06:45:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7b8raa",
                  "author": "metmelo",
                  "text": "What settings are you using to run it? I've been trying to run the GGUFs like I do with other models and getting Exit 139 (SIGSEGV)",
                  "score": 1,
                  "created_utc": "2026-02-25 12:00:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7960cz",
              "author": "mzinz",
              "text": "What do you use to measure tok/sec?",
              "score": 2,
              "created_utc": "2026-02-25 02:20:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o797fkn",
                  "author": "olmoscd",
                  "text": "verbose output?",
                  "score": 1,
                  "created_utc": "2026-02-25 02:28:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7cfq36",
              "author": "Danmoreng",
              "text": "66 t/s on 5080 mobile 16Gb (doesnâ€™t fit entirely into GPU VRAM, still super usable)\n\nhttps://github.com/Danmoreng/local-qwen3-coder-env",
              "score": 1,
              "created_utc": "2026-02-25 15:57:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7epi0h",
              "author": "noob10",
              "text": "running great, but hoping llama cpp adds vision for this model.",
              "score": 1,
              "created_utc": "2026-02-25 22:12:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7a896u",
          "author": "Equivalent-Home-223",
          "text": "do we know how it performs against qwen3 coder next?",
          "score": 29,
          "created_utc": "2026-02-25 06:34:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ml7f3",
              "author": "substance90",
              "text": "Quite a bit better according to my tests. Definitely the best local model for coding I've managed to run on my 64GB RAM M3 Max. Also seems to be better than models I can't run on my machine like gpt-oss-120b. The speed is also insane.",
              "score": 2,
              "created_utc": "2026-02-27 02:00:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7mllvo",
                  "author": "Equivalent-Home-223",
                  "text": "thats great to hear!",
                  "score": 2,
                  "created_utc": "2026-02-27 02:02:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o78ibqg",
          "author": "jslominski",
          "text": "Feel free to also try those settings (recommended by Unsloth docs, I've used their MXFP4 quant):\n\n./llama.cpp/llama-server \\\\\n\n\\-m /models/**Qwen3.5-35B-A3B-MXFP4\\_MOE.gguf**Â \\\\\n\n\\-c 131072 \\\\\n\n\\-ngl all \\\\\n\n\\-ctk q8\\_0 \\\\\n\n\\-ctv q8\\_0 \\\\\n\n\\-sm none \\\\\n\n\\-mg 0 \\\\\n\n\\-np 1 \\\\\n\n\\-fa on \\\\\n\n\\--temp 0.6 \\\\\n\n\\--top-p 0.95 \\\\\n\n\\--top-k 20 \\\\\n\n\\--min-p 0.00 \\\\\n\n  \nEDIT â¬†ï¸ is a mix of my tweaks and Unsloth recommendations for coding, pasting theirs fully for clarity:\n\n    Thinking model:\n    export LLAMA_CACHE=\"unsloth/Qwen3.5-35B-A3B-GGUF\"\n    ./llama.cpp/llama-cli \\\n        -hf unsloth/Qwen3.5-35B-A3B-GGUF:MXFP4_MOE \\\n        --ctx-size 16384 \\\n        --temp 0.6 \\\n        --top-p 0.95 \\\n        --top-k 20 \\\n        --min-p 0.00\n    \n    Non thinking model:\n    export LLAMA_CACHE=\"unsloth/Qwen3.5-35B-A3B-GGUF\"\n    ./llama.cpp/llama-cli \\\n        -hf unsloth/Qwen3.5-35B-A3B-GGUF:MXFP4_MOE \\\n        --ctx-size 16384 \\\n        --temp 0.7 \\\n        --top-p 0.8 \\\n        --top-k 20 \\\n        --min-p 0.00 \\\n        --chat-template-kwargs \"{\\\"enable_thinking\\\": false}\"",
          "score": 53,
          "created_utc": "2026-02-25 00:09:05",
          "is_submitter": true,
          "replies": [
            {
              "id": "o78o6e5",
              "author": "chickN00dle",
              "text": "just letting u know, I think this model might be sensitive to kv cache quantization. I had both K and V type set to q8_0 for the 35b moe model, but as the context grew to about 20-40K tokens, it kept making minor mistakes with LaTeX. Q4_K_XL",
              "score": 24,
              "created_utc": "2026-02-25 00:40:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79012o",
                  "author": "DigiDecode_",
                  "text": "I ran it (Q4-k-m-gguf) on CPU only and gave it full HTML code of an article from techcrunch, and asked it to extract the article in markdown, the HTML code was 85k token and it didn't make a single mistake   \nI ran it at full context of 256k, the token generation was 0.5 tokens per second, on smaller context size I was getting 4.5 t/s, at full context of 256k it was using about 40GB of RAM",
                  "score": 6,
                  "created_utc": "2026-02-25 01:46:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o78py66",
                  "author": "jslominski",
                  "text": "I don't see any of it yet. ",
                  "score": 6,
                  "created_utc": "2026-02-25 00:49:51",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o797ffe",
                  "author": "raysar",
                  "text": "Maybe quantize only V or only K ? KV cache quantization is very useful for out limiter vram computer.",
                  "score": 1,
                  "created_utc": "2026-02-25 02:28:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7bidvq",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-02-25 13:05:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o79gumk",
              "author": "bjodah",
              "text": "llama.cpp still doesn's support setting enable\\_thinking per request?",
              "score": 2,
              "created_utc": "2026-02-25 03:21:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7b3zr4",
                  "author": "CheatCodesOfLife",
                  "text": "What do you mean? It has for at least 6 months. You just need to add this to your request body:\n\n,\"chat_template_kwargs\":{\"enable_thinking\":false}",
                  "score": 1,
                  "created_utc": "2026-02-25 11:22:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7a4tcp",
              "author": "IrisColt",
              "text": "Thanks!!!",
              "score": 0,
              "created_utc": "2026-02-25 06:06:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o78yzub",
          "author": "zmanning",
          "text": "On an M4 Max I'm able to run [https://lmstudio.ai/models/qwen/qwen3.5-35b-a3b](https://lmstudio.ai/models/qwen/qwen3.5-35b-a3b) running at 60t/s",
          "score": 21,
          "created_utc": "2026-02-25 01:40:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79j5oo",
              "author": "kkb294",
              "text": "I just tested both MXFP4 and Q4\\_K\\_L from unsloth and both are working great. It gave me \\~30 tok/sec.\n\nI'm running it on MacBook M4 Pro 48GB.",
              "score": 5,
              "created_utc": "2026-02-25 03:35:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7d5xg4",
                  "author": "fridgeairbnb",
                  "text": "how are you running it? command line? Or a chat interface??",
                  "score": 1,
                  "created_utc": "2026-02-25 17:56:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o79298h",
              "author": "jslominski",
              "text": "How much VRAM do you have? Can you squeeze in a10b version? ",
              "score": 5,
              "created_utc": "2026-02-25 01:59:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ab9a5",
                  "author": "zmanning",
                  "text": "I have 64g. The unsloth version shows nothing really past Q2 on the A10B likely to load.\n\nhttps://preview.redd.it/tqgkyj5p9llg1.png?width=1230&format=png&auto=webp&s=527275834be23d023f72d183688b6878ff439820\n\n",
                  "score": 5,
                  "created_utc": "2026-02-25 07:00:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7b8gi2",
                  "author": "Acrobatic_Cat_3448",
                  "text": "I got it to load (128G)  - for Q4, it's \\~46 tok/s",
                  "score": 2,
                  "created_utc": "2026-02-25 11:58:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7a3zjg",
              "author": "PiaRedDragon",
              "text": "Try this one if you have enough RAM, next level : [https://huggingface.co/baa-ai/Qwen3.5-397B-A17B-SWAN-4bit](https://huggingface.co/baa-ai/Qwen3.5-397B-A17B-SWAN-4bit)",
              "score": 1,
              "created_utc": "2026-02-25 05:59:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7b7sml",
              "author": "Acrobatic_Cat_3448",
              "text": "I got 70 tok/s (q8).",
              "score": 1,
              "created_utc": "2026-02-25 11:53:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o793b1l",
          "author": "ianlpaterson",
          "text": "Running it as a persistent Slack bot (pi-mono framework) on Mac Studio via LM Studio, Q4\\_K\\_XL quant.\n\nGetting \\~14 t/s generation. Big gap vs your 100+ - MXFP4 plus llama.cpp on GDDR6X memory bandwidth will murder LM Studio on unified memory for this. Something for Mac users to know going in.\n\nOn the agentic side, the observation that's actually mattered for me: tool schema size is a real tax on local models. Swapped frameworks recently - went from 11 tools in the system prompt to 5. Same model, same hardware, same Mac Studio. Response time went from \\~5 min to \\~1 min. The 3090 will feel this less but it's not zero. If you're building agentic pipelines on local hardware, keep your tool count lean.\n\nOne other thing: thinking tokens add up fast in agentic loops. Every call I tested opened with a <think> block before generating useful output. At 14 t/s that overhead is noticeable. Probably less of an issue at 100 t/s but worth tracking.\n\nAgreed this model is something special at the weight class. First time I've run a local model in production for extended agentic tasks without reaching for an API as a fallback.\n\n",
          "score": 21,
          "created_utc": "2026-02-25 02:05:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79ui10",
              "author": "JacketHistorical2321",
              "text": "Mac studio what? I get 60 t/s with my m1 ultra with coder next q4 and full context. 14t/s is insanely slow",
              "score": 7,
              "created_utc": "2026-02-25 04:49:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7j851x",
                  "author": "ianlpaterson",
                  "text": "Update- performance tuning has me up to ~40 t/s",
                  "score": 1,
                  "created_utc": "2026-02-26 15:59:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7b65s0",
              "author": "eleqtriq",
              "text": "I canâ€™t help but feel something is wrong in your setup.",
              "score": 2,
              "created_utc": "2026-02-25 11:40:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7blaik",
                  "author": "ianlpaterson",
                  "text": "It's possible! ",
                  "score": 1,
                  "created_utc": "2026-02-25 13:22:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o78j1sd",
          "author": "Comrade-Porcupine",
          "text": "i dunno, I ran it on my Spark (8 bit quant) and hit it with opencode and it got itself totally flummoxed on just basic file text editing. It was smart at reading code just not good at tool use.",
          "score": 65,
          "created_utc": "2026-02-25 00:13:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78pwsk",
              "author": "guiopen",
              "text": "In my experience it's very sensitive to parameters, I am finding great success with qwen recommended values for thinking and precise coding in tool use:\ntemperature=0.6, top_p=0.95, top_k=20, min_p=0.0, presence_penalty=0.0, repetition_penalty=1.0",
              "score": 82,
              "created_utc": "2026-02-25 00:49:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o78nbtu",
              "author": "catplusplusok",
              "text": "In llama.cpp, make sure to pass an explicit chat template from base model, not use the embedded one in gguf",
              "score": 28,
              "created_utc": "2026-02-25 00:35:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78pzjh",
                  "author": "guiopen",
                  "text": "Why?",
                  "score": 7,
                  "created_utc": "2026-02-25 00:50:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7a4ds9",
                  "author": "IrisColt",
                  "text": "Thanks!",
                  "score": 2,
                  "created_utc": "2026-02-25 06:02:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78kjal",
              "author": "__SlimeQ__",
              "text": "this is a config issue of some kind, there's a difference between \"true openai tool calling\" and whatever else people are doing. i'm pretty sure qwen3 needs the real one. i was having that issue on an early ollama release of qwen3-coder-next and upgrading to the official one fixed the problem",
              "score": 23,
              "created_utc": "2026-02-25 00:21:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78l6am",
                  "author": "jslominski",
                  "text": "\"true openai tool calling\" - those models are trained with the harness, this is random Chinese model plugged into random open source harness so it won't work ootb perfectly yet. ",
                  "score": 5,
                  "created_utc": "2026-02-25 00:24:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78j71w",
              "author": "jslominski",
              "text": "I have totally different experience right now :D\n\nEDIT: what kind of speed are you getting on \\~130k context window?\n\nEDIT 2: example of tool use, took \\~15 seconds to click through the full webpage:\n\nhttps://preview.redd.it/7uy9q1nlajlg1.jpeg?width=1322&format=pjpg&auto=webp&s=fd7602a7400df8421b56c0f55763e768799c2579",
              "score": 11,
              "created_utc": "2026-02-25 00:13:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7b6hic",
                  "author": "Equal_Grape2337",
                  "text": "you need prompt caching to be enebled for the agalt loop \n\n    --cache-prompt",
                  "score": 1,
                  "created_utc": "2026-02-25 11:43:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7gwrml",
              "author": "lakoldus",
              "text": "According to Unsloth there is some kind of an issue with tool use with a fix potentially coming. Might be related to the prompt template.",
              "score": 2,
              "created_utc": "2026-02-26 06:01:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7bymxe",
              "author": "doradus_novae",
              "text": "So exactly like claude then? ðŸ˜†",
              "score": 1,
              "created_utc": "2026-02-25 14:34:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79shgg",
          "author": "metigue",
          "text": "I've been using the 27B model and it's... really good. The benchmarks don't lie - For coding it's sonnet 4.5 level.\n\nThe only downside is the depth of knowledge drop off you always get from lower parameter models but it can web search very well and so far tends to do that rather than hallucinate which is great.",
          "score": 54,
          "created_utc": "2026-02-25 04:35:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bj1n5",
              "author": "KaroYadgar",
              "text": "no way, sonnet 4.5 level? I'll believe it when I see it.",
              "score": 15,
              "created_utc": "2026-02-25 13:09:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7gfio6",
                  "author": "Unlucky-Bunch-7389",
                  "text": "100% bullshit lol",
                  "score": 2,
                  "created_utc": "2026-02-26 03:58:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o79us0b",
              "author": "Odd-Ordinary-5922",
              "text": "how are you using it with web search?",
              "score": 11,
              "created_utc": "2026-02-25 04:51:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ac5gg",
                  "author": "Idarubicin",
                  "text": "Not sure how they are doing it but in openwebui there is a web search which you can use natively, or what I find better is I have a custom mcp server in my docker script with a tool to use searxng to search the web. \n\nWorks nicely. Set it a task which you involved a relatively obscure cli tool which often trips up other models (they often default to the commands of the more usual tool) and it handled it like an absolute pro even using arguments which are buried a couple of pages into the GitHub repository in the examples.",
                  "score": 13,
                  "created_utc": "2026-02-25 07:08:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ak98j",
                  "author": "metigue",
                  "text": "Running llama.cpp server then calling that with an agentic framework that has web search as one of the tools.\n\nIt's good at using all the tools not just web search.",
                  "score": 4,
                  "created_utc": "2026-02-25 08:22:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7cj84j",
              "author": "anitman",
              "text": "With brightdata, DuckDuckGo and firecrawl mcps, you are nearly free of hallucinations.",
              "score": 4,
              "created_utc": "2026-02-25 16:13:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o79t1iw",
              "author": "True_Requirement_891",
              "text": "holy shit",
              "score": 2,
              "created_utc": "2026-02-25 04:38:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7a8snz",
              "author": "DesignerTruth9054",
              "text": "I am facing lot of KV cache erasure issues when it does web search (reducing it overall speed). Are you facing any of that?\n\n",
              "score": 1,
              "created_utc": "2026-02-25 06:39:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7aoirr",
                  "author": "metigue",
                  "text": "I did have some of this - That's more to do with the framework than the model though. Often a web search will append the current date and time at the top of the query and if they dynamically update that the KV cache is useless...",
                  "score": 2,
                  "created_utc": "2026-02-25 09:02:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7dm6y9",
              "author": "ShadyShroomz",
              "text": "> For coding it's sonnet 4.5 level.\n\ni'll be honest I have my doubts about this... downloading it now and will set it up in opencode and see how it does... but while this would be insane i find it very unlikely it can be quite that good.",
              "score": 1,
              "created_utc": "2026-02-25 19:09:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7tp9kk",
                  "author": "Icy_Butterscotch6661",
                  "text": "What did you think?",
                  "score": 1,
                  "created_utc": "2026-02-28 04:08:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o79a6al",
          "author": "jslominski",
          "text": "https://preview.redd.it/ed370o97zjlg1.png?width=1435&format=png&auto=webp&s=f1a30e72a8b52361eebcb8bca0809c0c16f00fa3\n\nOk, time to go to sleep lol. Did some tests with 122B A10B variant (ignore the name in the Opencode, didn't swap it in my config file there). The 2 bit \"Unsloth\" quant: Qwen3.5-122B-A10B-UD-IQ2\\_M.gguf was the maxed that didn't OOM at 130k ctx, Running on dual RTX 3090 fully in VRAM, 22.7GB each. Now the best part. I'm STILL getting \\~50T/s (my RTXes are power capped to 280W in dual usage cause I don't want to burn my old PC :)) and it codes even better than 3b expert variant. Love those new Qwens! Best release since Mistral 7b for me personally.",
          "score": 30,
          "created_utc": "2026-02-25 02:44:05",
          "is_submitter": true,
          "replies": [
            {
              "id": "o7aoall",
              "author": "getpodapp",
              "text": "whats the sidebar you have in opencode?\n\nedit: on a mac press ctrl+p then 'toggle sidebar'",
              "score": 5,
              "created_utc": "2026-02-25 09:00:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7b1wmj",
                  "author": "t4a8945",
                  "text": "It's the vanilla config when terminal is wide enoughÂ ",
                  "score": 7,
                  "created_utc": "2026-02-25 11:04:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7b27ww",
                  "author": "Pyros-SD-Models",
                  "text": "It's a setting in opencode",
                  "score": 1,
                  "created_utc": "2026-02-25 11:07:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7bgaxh",
              "author": "Flinchie76",
              "text": "\\> Best release since Mistral 7b for me personally.\n\nI was thinking exactly this :) Mistral 7b will always have a special place in my heart, and Qwen 2.5 was a solid upgrade, but these models are a step change in this class. Multi-modal, tools, controllable reasoning, small, fast, smart. This will seriously dent enterprise \\`gpt-5-mini\\` usage for high volume, low latency data processing and NLP tasks.",
              "score": 2,
              "created_utc": "2026-02-25 12:52:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o79pj3k",
              "author": "AdamTReineke",
              "text": "I was wondering about dual GPUs, good info. I should try this.",
              "score": 1,
              "created_utc": "2026-02-25 04:15:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o78uu5q",
          "author": "ducksoup_18",
          "text": "So if i have 2 3060 12gb i should be able to run this model all in vram? Right now im running unsloth/Qwen3-VL-8B-Instruct-GGUF:Q8_0 as my all in one kinda assistant for HASS but would love a more capable model for both that and coding tasks.Â ",
          "score": 13,
          "created_utc": "2026-02-25 01:16:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78w20x",
              "author": "jslominski",
              "text": "Yes you are good sir. ",
              "score": 5,
              "created_utc": "2026-02-25 01:23:57",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7de5ue",
              "author": "TOO_MUCH_BRAVERY",
              "text": "what kind of stuff do you do with it for hass?",
              "score": 1,
              "created_utc": "2026-02-25 18:33:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7e02kd",
                  "author": "ducksoup_18",
                  "text": "Voice assist and camera vision/notifications currently. Hass intent are decent but itâ€™s move to have it fallback to an agent that is a bit smarter in cases where the intents fail (multiple tool calls, searching, etc)",
                  "score": 1,
                  "created_utc": "2026-02-25 20:13:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7948d3",
          "author": "l33t-Mt",
          "text": "Getting 37 t/s @ Q4\\_K\\_M with Nvidia P40 24GB.",
          "score": 9,
          "created_utc": "2026-02-25 02:10:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79v5au",
              "author": "Odd-Ordinary-5922",
              "text": "getting 37t/s with a 3060 no idea how",
              "score": 2,
              "created_utc": "2026-02-25 04:53:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7aemt8",
                  "author": "R_Duncan",
                  "text": "Please post your parameters...",
                  "score": 2,
                  "created_utc": "2026-02-25 07:30:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ahjs3",
                  "author": "Comrade_Mugabe",
                  "text": "What settings are you running? I'm trying:\n`llama-server --threads 6 --threads-batch 12 --model \"F:\\AI\\LM Studio models\\models\\unsloth\\Qwen3.5-35B-A3B-GGUF\\Qwen3.5-35B-A3B-UD-Q4_K_XL.gguf\" --fit on --fit-ctx 65536 --host 127.0.0.1 --port 8080 -fa on --temp 0.6 --top-p 0.95 --top-k 20 --min-p 0.00`\n\nAnd I'm getting 7 tk/s with 2 3060's and 128 GB RAM.\n\nEdit: Didn't see you had replied above. Running again using the following:\n`llama-server --model \"F:\\AI\\LM Studio models\\models\\unsloth\\Qwen3.5-35B-A3B-GGUF\\Qwen3.5-35B-A3B-UD-Q4_K_XL.gguf\" --n-gpu-layers auto --ctx-size 65536 --host 127.0.0.1 --port 8080 --batch-size 4096 --ubatch-size 2048 --flash-attn on --threads 22 --temp 0.6 --top-p 0.95 --top-k 20 --min-p 0.00`\n\nGet's me roughly 33 tk/s\n\nThis is confusing, as using `-fit on` for `Qwen3-Coder-Next` gets me 15 tk/s, but using the above settings gets me 6 tk/s on that model. I would think, them both being MOE, that similar settings would work for them.",
                  "score": 1,
                  "created_utc": "2026-02-25 07:56:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o78px44",
          "author": "Corosus",
          "text": "Putting my test into the ring with opencode as well.\n\nholy shit that was faaaaaaast.\n\nTEST 2 EDIT:\n\nI input the correct model params this time, still 2 mins, result looks nicer.\n\nhttps://images2.imgbox.com/ff/14/mxBYW899_o.png\n\nllama-b8121-bin-win-vulkan-x64\\llama-server -m ./Qwen3.5-35B-A3B-MXFP4_MOE.gguf -ngl 999 -ctk q8_0 -ctv q8_0 -mg 0 -t 12 -fa on -c 131072 -b 512 -ub 512 -np 1 --temp 0.6 --top-p 0.95 --top-k 20 --min-p 0.0 --host 0.0.0.0 --port 8080 --tensor-split 1,0,1\n\ntook 3 mins\n\n \nprompt eval time =     114.84 ms /    21 tokens (    5.47 ms per token,   182.86 tokens per second)\n\neval time =    4241.54 ms /   295 tokens (   14.38 ms per token,    69.55 tokens per second)\n\ntotal time =    4356.38 ms /   316 tokens\n\n\n\nllama_memory_breakdown_print: | memory breakdown [MiB]    | total    free     self   model   context   compute    unaccounted |\n\nllama_memory_breakdown_print: |   - Vulkan0 (RTX 5070 Ti) | 15907 =  3028 + (11359 =  9363 +     713 +    1282) +        1519 |\n\nllama_memory_breakdown_print: |   - Vulkan2 (RX 6800 XT)  | 16368 = 15569 + (    0 =     0 +       0 +       0) +         798 |\n\nllama_memory_breakdown_print: |   - Vulkan3 (RTX 5060 Ti) | 15962 =  4016 + (10874 =  8984 +     709 +    1180) +        1071 |\n\nllama_memory_breakdown_print: |   - Host                  |                   1547 =   515 +       0 +    1032                |\n\nTEST 1:\n\nprompt eval time =     106.19 ms /    21 tokens (    5.06 ms per token,   197.76 tokens per second)\n\neval time =     850.77 ms /    60 tokens (   14.18 ms per token,    70.52 tokens per second)\n\ntotal time =     956.97 ms /    81 tokens\n\nhttps://images2.imgbox.com/b1/1f/X1tbcsPV_o.png\n\nMy result isn't as fancy and is just a static webpage tho.\n\nOnly took 2 minutes lmao.\n\nJust a quick and dirty test, didn't refine my run params too much, was based on my qwen coder next testing, just making sure it uses my dual GPU setup well enough.\n\nllama-server -m ./Qwen3.5-35B-A3B-MXFP4_MOE.gguf -ngl 999 -mg 0 -t 12 -fa on -c 131072 -b 512 -ub 512 -np 1 --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 --repeat-penalty 1.0 --host 0.0.0.0 --port 8080 --tensor-split 1,0,1\n\n5070 ti and 5060 ti 16gb, using up most of the vram on both. 70 tok/s with 131k context is INSANE. I was lucky to get 20 with my qwen coder next setups, much more testing needed!",
          "score": 16,
          "created_utc": "2026-02-25 00:49:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79qcm7",
              "author": "somethingdangerzone",
              "text": "> Qwen3.5-35B-A3B-MXFP4_MOE.gguf\n\nDid you choose the bf16 or fp16 one? I feel dumb for not knowing which is better",
              "score": 3,
              "created_utc": "2026-02-25 04:20:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dcel9",
                  "author": "jslominski",
                  "text": "That's FP4. Are you referring to the image encoder? I think it doesn't matter tbh given how small it is compared to the whole model weights. ",
                  "score": 2,
                  "created_utc": "2026-02-25 18:25:16",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78rcw5",
              "author": "jslominski",
              "text": "\"holy shit that was faaaaaaast.\"\n\nhttps://preview.redd.it/l91yeyhehjlg1.png?width=600&format=png&auto=webp&s=761656e954961660a6284a30d88ebb866654d92b\n\n",
              "score": 5,
              "created_utc": "2026-02-25 00:57:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o78mv9h",
          "author": "giant3",
          "text": "What is the version of llama.cpp are you using?",
          "score": 6,
          "created_utc": "2026-02-25 00:33:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78nhct",
              "author": "jslominski",
              "text": "compiled from latest source, roughly 1h ago. ",
              "score": 12,
              "created_utc": "2026-02-25 00:36:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o793m44",
                  "author": "simracerman",
                  "text": "Curious why not use the precompiled binaries? Any advantage to compiling yourself.",
                  "score": 8,
                  "created_utc": "2026-02-25 02:07:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o78vdhd",
          "author": "DeedleDumbDee",
          "text": "Man I'm only getting 13t/s. Same quant, 7800XT 16GB, Ryzen 9 9950X, 64GB DDR5 ram. I know ROCm isn't as mature as CUDA but does the difference in t/s make sense? Also running on WSL2 in windows w/ llama.cpp.",
          "score": 11,
          "created_utc": "2026-02-25 01:20:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78wiob",
              "author": "jslominski",
              "text": "That's RAM offload for you. Try smaller quant. Maybe UD-IQ2\\_XXS? Or maybe sell that ram, get a bigger GPU, a car and a new house?",
              "score": 37,
              "created_utc": "2026-02-25 01:26:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7927b3",
                  "author": "DeedleDumbDee",
                  "text": "Eh, It's only 1.6 less t/s for me to run Q6\\_K\\_XL. Got it running as an agent in VS code w/ Cline. Takes awhile but it's been one shotting everything I've asked no errors or failed tool use. Good enough for me until I can afford a $9,000 96GB RTX PRO 6000 BLACKWELL",
                  "score": 6,
                  "created_utc": "2026-02-25 01:59:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7985e6",
              "author": "uhhereyougo",
              "text": "Absolutely not. I got 9t/s on a 7640HS 760m iGPU with the UD-4K_Xl quant running llama.cpp vulkan on linux while limiting TDP to 25w and running an AV1 transcode on the CPU",
              "score": 5,
              "created_utc": "2026-02-25 02:32:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79l5o6",
                  "author": "DeedleDumbDee",
                  "text": "I don't know if it's because I just updated WSL and completely reinstalled ROCm, or because I just changed up my build command but I'm now getting 21t/s!\n\nCurrent build:\n\n ./build/bin/llama-server   --model ./models/Qwen3.5-35B-A3B-UD-Q6\\_K\\_XL.gguf   --n-gpu-layers auto  --port 32200   --ctx-size 72000   --batch-size 4096   --ubatch-size 2048   --flash-attn on   --threads 22 \n\nPrevious build:\n\n./build/bin/llama-server --model ./models/Qwen3.5-35B-A3B-UD-Q6\\_K\\_XL.gguf --port 32200 --n-gpu-layers 15 --threads 24 --ctx-size 32768 --parallel 1 --batch-size 2048 --ubatch-size 1024",
                  "score": 6,
                  "created_utc": "2026-02-25 03:47:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o79m261",
              "author": "Monad_Maya",
              "text": "Roughly the same tps.\n\n\n7900XT (20GB) + 12c 5900X + 128GB DDR4\n\n\nI'm using Vulkan though but still, the performance is too low. Minimax is not much slower while being much larger.\n\n\nUbuntu 25.10\n\n\nUsed the same command as the OP of this post.",
              "score": 5,
              "created_utc": "2026-02-25 03:53:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79mqbv",
                  "author": "DeedleDumbDee",
                  "text": "I don't know if you saw my reply above, but I just completely changed my build command and now I'm getting 20-24t/s @ 72k context with the Q6\\_K\\_XL.",
                  "score": 6,
                  "created_utc": "2026-02-25 03:57:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o797gpt",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2026-02-25 02:29:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o797p9y",
                  "author": "DeedleDumbDee",
                  "text": "Can you drop your build command? Are you on Linux or WSL?",
                  "score": 1,
                  "created_utc": "2026-02-25 02:30:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ay17q",
              "author": "H3PO",
              "text": "Give vulkan a try. its marginally faster than rocm on a single one of my 7900xtx, much faster with two cards",
              "score": 2,
              "created_utc": "2026-02-25 10:30:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o78p6ba",
          "author": "jslominski",
          "text": "https://preview.redd.it/ln3dpoxyejlg1.jpeg?width=1672&format=pjpg&auto=webp&s=2e18584f73f5fe981f8fe1e09448adc4248e2155\n\nReddit-themed bejewelled in react, \\~3 minutes, no interventions. This is really promising. Keep in mind this runs insanely fast, on a potato GPU (24 gig 3090) with 130k context window. I'm normally not spamming Reddit like this but I'm stoked ðŸ˜…",
          "score": 75,
          "created_utc": "2026-02-25 00:45:42",
          "is_submitter": true,
          "replies": [
            {
              "id": "o78vcj8",
              "author": "Right-Law1817",
              "text": "Calling that gpu \"potato\" should be illegal.",
              "score": 189,
              "created_utc": "2026-02-25 01:19:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79bhxq",
                  "author": "KallistiTMP",
                  "text": "What, you don't have an NVL72 in your basement? I use mine as a water heater for my solid gold Jacuzzi.",
                  "score": 30,
                  "created_utc": "2026-02-25 02:51:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o79ae6f",
                  "author": "randylush",
                  "text": " 3090 is goat",
                  "score": 15,
                  "created_utc": "2026-02-25 02:45:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7an34z",
                  "author": "jslominski",
                  "text": "I'm sorry for saying that! I will redeem myself! ",
                  "score": 1,
                  "created_utc": "2026-02-25 08:48:44",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o793fcr",
              "author": "waiting_for_zban",
              "text": "I was going to wait on this for a bit, but you got me hyped. I am genuinely excited now.",
              "score": 2,
              "created_utc": "2026-02-25 02:06:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o78z1u5",
              "author": "cantgetthistowork",
              "text": "What IDE is this?",
              "score": 2,
              "created_utc": "2026-02-25 01:41:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o792i31",
                  "author": "jslominski",
                  "text": "Terminal :) Running Opencode. ",
                  "score": 10,
                  "created_utc": "2026-02-25 02:00:59",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78y4nv",
              "author": "Apart_Paramedic_7767",
              "text": "what settings do you use for that much context on 3090?",
              "score": 1,
              "created_utc": "2026-02-25 01:35:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o792pdi",
                  "author": "jslominski",
                  "text": "Settings are in one of my comments. ",
                  "score": 1,
                  "created_utc": "2026-02-25 02:02:09",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7zeliv",
              "author": "Psionatix",
              "text": "This looks pretty cool, not expecting you to answer here, but hoping anyone passing by might be able to help. I use a wide variety of massive AI tooling through work, but I'm new to running LLM's locally.\n\nI started off getting ollama running on my PC and connecting to it with SillyTavern from my Mac, looks like OpenWebUI might be a better option?\n\nI'm a bit confused on how to get a more advanced setup running with MCP's and some agentic flows.\n\nMy PC has a 5090 and 64gb of RAM, I'd like to run the model there. I'd then like to prompt with skills from my mac and build projects there, with the frontend I run on my Mac having read / write access for the LLM.\n\nFrom what I can see, opencode might be the way to go?",
              "score": 1,
              "created_utc": "2026-03-01 02:03:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o78qvra",
              "author": "Iory1998",
              "text": "I like what you are doing. I am not a coder, but I'd like to vicecode cool stuff. How do you do them youself?",
              "score": -7,
              "created_utc": "2026-02-25 00:54:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7933bv",
                  "author": "Spectrum1523",
                  "text": "He is using opencode. Google their GitHub page",
                  "score": 3,
                  "created_utc": "2026-02-25 02:04:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o792ea6",
          "author": "PsychologicalSock239",
          "text": "do you mind sharing your opencode.json file?",
          "score": 6,
          "created_utc": "2026-02-25 02:00:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79m8vp",
              "author": "jslominski",
              "text": "Here you go. This runs isolated and I use it for toying around thus eased permissions, don't use it in prod/without isolation like that! MCPs are the ones I like/been testing lately so nothing mandatory! \n\n\n\n{\n\n\"$schema\": \"https://opencode.ai/config.json\",\n\n\n\n\"provider\": {\n\n\"llama.cpp\": {\n\n\"npm\": \"@ai-sdk/openai-compatible\",\n\n\"name\": \"Local llama.cpp\",\n\n\"options\": {\n\n\"baseURL\": \"http://192.168.1.111:8080/v1\"\n\n},\n\n\"models\": {\n\n\"qwen35-a3b-local\": {\n\n\"name\": \"Qwen3.5-35B-A3B MXFP4 MOE (Local)\",\n\n\"limit\": {\n\n\"context\": 131072,\n\n\"output\": 32000\n\n}\n\n}\n\n}\n\n}\n\n},\n\n\n\n\"model\": \"llama.cpp/qwen35-a3b-local\",\n\n\n\n\"permission\": {\n\n\"\\*\": \"allow\"\n\n},\n\n\n\n\"agent\": {\n\n\"plan\": {\n\n\"description\": \"Planning mode\",\n\n\"model\": \"llama.cpp/qwen35-a3b-local\",\n\n\"permission\": {\n\n\"\\*\": \"allow\"\n\n},\n\n\"tools\": {\n\n\"write\": true,\n\n\"edit\": true,\n\n\"patch\": true,\n\n\"read\": true,\n\n\"list\": true,\n\n\"glob\": true,\n\n\"grep\": true,\n\n\"webfetch\": true,\n\n\"websearch\": true,\n\n\"bash\": true\n\n}\n\n},\n\n\"build\": {\n\n\"description\": \"Build mode\",\n\n\"model\": \"llama.cpp/qwen35-a3b-local\",\n\n\"permission\": {\n\n\"\\*\": \"allow\"\n\n},\n\n\"tools\": {\n\n\"write\": true,\n\n\"edit\": true,\n\n\"patch\": true,\n\n\"read\": true,\n\n\"list\": true,\n\n\"glob\": true,\n\n\"grep\": true,\n\n\"webfetch\": true,\n\n\"websearch\": true,\n\n\"bash\": true\n\n}\n\n}\n\n},\n\n\n\n\"mcp\": {\n\n\"context7\": {\n\n\"type\": \"local\",\n\n\"command\": \\[\"npx\", \"-y\", \"@upstash/context7-mcp\"\\],\n\n\"enabled\": true\n\n},\n\n\"mobile-mcp\": {\n\n\"type\": \"local\",\n\n\"command\": \\[\"npx\", \"-y\", \"@mobilenext/mobile-mcp@latest\"\\],\n\n\"enabled\": true\n\n},\n\n\"chrome-devtools\": {\n\n\"type\": \"local\",\n\n\"command\": \\[\"npx\", \"-y\", \"chrome-devtools-mcp@latest\"\\],\n\n\"enabled\": true\n\n}\n\n}\n\n  }",
              "score": 14,
              "created_utc": "2026-02-25 03:54:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7aal3e",
                  "author": "sig_kill",
                  "text": "my eyes",
                  "score": 11,
                  "created_utc": "2026-02-25 06:54:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7a5ldf",
                  "author": "IrisColt",
                  "text": "Thanks!",
                  "score": 0,
                  "created_utc": "2026-02-25 06:12:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ad1yw",
          "author": "sabotage3d",
          "text": "How does it compare to the Qwen Coder Next 80b? I have spent quite a bit of time tuning it for my setup.",
          "score": 6,
          "created_utc": "2026-02-25 07:16:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7b2u2y",
              "author": "SnooPeripherals5499",
              "text": "Qwen coder next is better. Both fail a lot in medium to big repos",
              "score": 2,
              "created_utc": "2026-02-25 11:12:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7bo7r8",
              "author": "benevbright",
              "text": "unfortunately I also find qwen3-coder-next 80b better for now.",
              "score": 1,
              "created_utc": "2026-02-25 13:38:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o78s887",
          "author": "bobaburger",
          "text": "Yeah, 35B has been very usable and fast for me, my only complain is, with claude code, sometimes into a long session, it would stop responding in the middle of the work, and i have to say \"resume\" or something to make it work again.\n\n\\---\n\nEdit: For the running speed, at 248k context window:\n\n* On M2 Max 64 GB MBP, I got 350 t/s pp and 27 t/s tg (MXFP4)\n* On RTX 5060 Ti 16 GB + 32 GB RAM, I got 800 t/s pp and 35 t/s tg (UD Q4\\_K\\_XL)",
          "score": 14,
          "created_utc": "2026-02-25 01:02:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bgozi",
              "author": "Flinchie76",
              "text": "Opus 4.6 does this too, occasionally :)",
              "score": 2,
              "created_utc": "2026-02-25 12:54:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79frag",
          "author": "Technical-Earth-3254",
          "text": "Impressive! Before going to bed I was testing the 27B on my 3090 system in q4 xl and q5 xl in some visual tests bc that's what I'm interested in rn. Q5 was insanely good, way better than Ministral 14b q8 xl thinking and also better than Gemma 3 27B qat. But it was painfully slow. 12t/s on q4 and 5t/s on q5 (without vram being filled, low 8k context) shocked me. Will try the 35B later on, hopefully it will be a lot quicker than that while having the same performance.\n\nQ5 was the best vl model I've used till now, that did fit on my machine.",
          "score": 5,
          "created_utc": "2026-02-25 03:15:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7adnaf",
              "author": "Subject-Tea-5253",
              "text": "The 27B model is dense, while the 35B-A3B model is an MOE.\n\nDense models are always slower than MOE. If you don't have enough VRAM to hold the full model, the token generation will suffer.\n\nTry the 35B-A3B model, you will be surprised by the token generation speed.",
              "score": 0,
              "created_utc": "2026-02-25 07:21:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o78lv8d",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 5,
          "created_utc": "2026-02-25 00:28:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78nd6t",
              "author": "DistanceAlert5706",
              "text": "Really curious to see perplexity/performance. For example on GLM4.7-Flash MXFP4 was way better, close or even better than q6.",
              "score": 1,
              "created_utc": "2026-02-25 00:36:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o78ndtz",
              "author": "jslominski",
              "text": "Good question, this is complex topic unfortunately, depends on what you are running them on, some good reads on that topic:\n\n[https://kaitchup.substack.com/p/choosing-a-gguf-model-k-quants-i](https://kaitchup.substack.com/p/choosing-a-gguf-model-k-quants-i)\n\n[https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs)\n\nI'm going to be doing some extensive testing this week cause I'm super interested in this model. ",
              "score": 1,
              "created_utc": "2026-02-25 00:36:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o79byq5",
          "author": "jiegec",
          "text": "llama-bench on my NV4090 24GB:\n\n\\+ CUDA\\_VISIBLE\\_DEVICES=1 ../llama.cpp/llama-bench -p 1024 -n 64 -d 0,16384,32768,49152 --model unsloth/Qwen3.5-35B-A3B-GGUF/Qwen3.5-35B-A3B-UD-Q3\\_K\\_XL.gguf\n\nggml\\_cuda\\_init: found 1 CUDA devices:\n\n  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n\n| model                          |       size |     params | backend    | ngl |            test |                  t/s |\n\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n\n| qwen35moe ?B Q3\\_K - Medium     |  14.66 GiB |    34.66 B | CUDA       |  99 |          pp1024 |      5189.48 Â± 12.92 |\n\n| qwen35moe ?B Q3\\_K - Medium     |  14.66 GiB |    34.66 B | CUDA       |  99 |            tg64 |        115.79 Â± 1.80 |\n\n| qwen35moe ?B Q3\\_K - Medium     |  14.66 GiB |    34.66 B | CUDA       |  99 | pp1024 @ d16384 |      3703.44 Â± 10.14 |\n\n| qwen35moe ?B Q3\\_K - Medium     |  14.66 GiB |    34.66 B | CUDA       |  99 |   tg64 @ d16384 |        109.06 Â± 2.10 |\n\n| qwen35moe ?B Q3\\_K - Medium     |  14.66 GiB |    34.66 B | CUDA       |  99 | pp1024 @ d32768 |       2867.74 Â± 4.48 |\n\n| qwen35moe ?B Q3\\_K - Medium     |  14.66 GiB |    34.66 B | CUDA       |  99 |   tg64 @ d32768 |         97.30 Â± 1.64 |\n\n| qwen35moe ?B Q3\\_K - Medium     |  14.66 GiB |    34.66 B | CUDA       |  99 | pp1024 @ d49152 |       2326.84 Â± 2.83 |\n\n| qwen35moe ?B Q3\\_K - Medium     |  14.66 GiB |    34.66 B | CUDA       |  99 |   tg64 @ d49152 |         88.42 Â± 1.18 |\n\n\n\nbuild: 244641955 (8148)\n\n",
          "score": 4,
          "created_utc": "2026-02-25 02:53:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79ftz1",
              "author": "jslominski",
              "text": "RTX 3090 24GB (350W) - still awesome value for that performance imo:\n\nCUDA\\_VISIBLE\\_DEVICES=0 ./llama.cpp/build/bin/llama-bench -m ./Qwen3.5-35B-A3B-MXFP4\\_MOE.gguf -p 1024 -n 64 -d 0,16384,32768,49152\n\nggml\\_cuda\\_init: found 1 CUDA devices:\n\nDevice 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n\n| model | size | params | backend | ngl | test | t/s |\n\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n\n| qwen35moe ?B MXFP4 MoE | 18.42 GiB | 34.66 B | CUDA | 99 | pp1024 | 2771.01 Â± 10.81 |\n\n| qwen35moe ?B MXFP4 MoE | 18.42 GiB | 34.66 B | CUDA | 99 | tg64 | 111.88 Â± 1.32 |\n\n| qwen35moe ?B MXFP4 MoE | 18.42 GiB | 34.66 B | CUDA | 99 | pp1024 @ d16384 | 2136.74 Â± 5.52 |\n\n| qwen35moe ?B MXFP4 MoE | 18.42 GiB | 34.66 B | CUDA | 99 | tg64 @ d16384 | 89.35 Â± 0.71 |\n\n| qwen35moe ?B MXFP4 MoE | 18.42 GiB | 34.66 B | CUDA | 99 | pp1024 @ d32768 | 1528.24 Â± 1.62 |\n\n| qwen35moe ?B MXFP4 MoE | 18.42 GiB | 34.66 B | CUDA | 99 | tg64 @ d32768 | 69.15 Â± 0.35 |\n\n| qwen35moe ?B MXFP4 MoE | 18.42 GiB | 34.66 B | CUDA | 99 | pp1024 @ d49152 | 1217.09 Â± 1.37 |\n\n| qwen35moe ?B MXFP4 MoE | 18.42 GiB | 34.66 B | CUDA | 99 | tg64 @ d49152 | 55.53 Â± 0.21 |\n\nbuild: 244641955 (8148)",
              "score": 2,
              "created_utc": "2026-02-25 03:15:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7a0iz0",
          "author": "DarkTechnophile",
          "text": "System:\n\n* 1x 7900GRE GPU\n* 1x 7900XTX GPU\n* 1x 7700x CPU\n* 64GB of DDR5 RAM\n* ADT-Link F36B-F37B-D8S (a passive bifurcation card set to use x8+x8)\n\nResults:\n\n    âžœ  ~ GGML_VK_VISIBLE_DEVICES=1 llama-bench -m ~/.cache/llama.cpp/unsloth_Qwen3.5-35B-A3B-GGUF_Qwen3.5-35B-A3B-Q4_K_M.gguf -fa 0,1 \n    ggml_vulkan: Found 1 Vulkan devices:\n    ggml_vulkan: 0 = AMD Radeon RX 7900 XTX (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n    | model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n    | qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | Vulkan     |  99 |  0 |           pp512 |      2271.96 Â± 13.71 |\n    | qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | Vulkan     |  99 |  0 |           tg128 |        100.70 Â± 0.06 |\n    | qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | Vulkan     |  99 |  1 |           pp512 |      2275.14 Â± 10.47 |\n    | qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | Vulkan     |  99 |  1 |           tg128 |        101.33 Â± 0.08 |\n    \n    build: e29de2f (8132)\n    âžœ  ~ GGML_VK_VISIBLE_DEVICES=0 llama-bench -m ~/.cache/llama.cpp/unsloth_Qwen3.5-35B-A3B-GGUF_Qwen3.5-35B-A3B-Q4_K_M.gguf -fa 0,1\n    ggml_vulkan: Found 1 Vulkan devices:\n    ggml_vulkan: 0 = AMD Radeon RX 7900 GRE (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n    | model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n    | qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | Vulkan     |  99 |  0 |           pp512 |       441.04 Â± 17.06 |\n    | qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | Vulkan     |  99 |  0 |           tg128 |          8.68 Â± 0.00 |\n    | qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | Vulkan     |  99 |  1 |           pp512 |       460.17 Â± 17.46 |\n    | qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | Vulkan     |  99 |  1 |           tg128 |         25.94 Â± 0.01 |\n    \n    build: e29de2f (8132)\n    âžœ  ~ GGML_VK_VISIBLE_DEVICES=0,1 llama-bench -m ~/.cache/llama.cpp/unsloth_Qwen3.5-35B-A3B-GGUF_Qwen3.5-35B-A3B-Q4_K_M.gguf -fa 0,1\n    ggml_vulkan: Found 2 Vulkan devices:\n    ggml_vulkan: 0 = AMD Radeon RX 7900 GRE (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n    ggml_vulkan: 1 = AMD Radeon RX 7900 XTX (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n    | model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n    | qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | Vulkan     |  99 |  0 |           pp512 |       1245.37 Â± 6.65 |\n    | qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | Vulkan     |  99 |  0 |           tg128 |         42.69 Â± 0.27 |\n    | qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | Vulkan     |  99 |  1 |           pp512 |       1249.45 Â± 2.48 |\n    | qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | Vulkan     |  99 |  1 |           tg128 |         42.74 Â± 0.35 |\n    \n    build: e29de2f (8132)",
          "score": 5,
          "created_utc": "2026-02-25 05:32:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7giish",
              "author": "dodistyo",
              "text": "Is vulkan faster than ROCm? how much tps you got with that setup?",
              "score": 1,
              "created_utc": "2026-02-26 04:17:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7hhq9w",
                  "author": "DarkTechnophile",
                  "text": "Results:\n- Vulkan is faster on single-gpu instances\n- ROCm 7.2 is faster on multi-gpu instances\n\nMight be a configuration issue on my behalf. Also `llama-bench` does not seem to want to use my system's memory, thus, the 7900GRE tests fail on ROCm.\n\n```\nâžœ  HIP_VISIBLE_DEVICES=0 llama-bench -m ~/.cache/llama.cpp/unsloth_Qwen3.5-35B-A3B-GGUF_Qwen3.5-35B-A3B-Q4_K_M.gguf -fa 0,1 \nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon RX 7900 XTX, gfx1100 (0x1100), VMM: no, Wave Size: 32\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n| qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | ROCm       |  99 |  0 |           pp512 |      2148.33 Â± 17.70 |\n| qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | ROCm       |  99 |  0 |           tg128 |         81.24 Â± 0.48 |\n| qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | ROCm       |  99 |  1 |           pp512 |       2152.95 Â± 6.59 |\n| qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | ROCm       |  99 |  1 |           tg128 |         81.67 Â± 0.12 |\n\nbuild: 4220f7d (8148)\nâžœ  HIP_VISIBLE_DEVICES=1 llama-bench -m ~/.cache/llama.cpp/unsloth_Qwen3.5-35B-A3B-GGUF_Qwen3.5-35B-A3B-Q4_K_M.gguf -fa 0,1\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon RX 7900 GRE, gfx1100 (0x1100), VMM: no, Wave Size: 32\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\nmain: error: failed to load model '/home/<name>/.cache/llama.cpp/unsloth_Qwen3.5-35B-A3B-GGUF_Qwen3.5-35B-A3B-Q4_K_M.gguf'\nâžœ  HIP_VISIBLE_DEVICES=0,1 llama-bench -m ~/.cache/llama.cpp/unsloth_Qwen3.5-35B-A3B-GGUF_Qwen3.5-35B-A3B-Q4_K_M.gguf -fa 0,1\nggml_cuda_init: found 2 ROCm devices:\n  Device 0: AMD Radeon RX 7900 XTX, gfx1100 (0x1100), VMM: no, Wave Size: 32\n  Device 1: AMD Radeon RX 7900 GRE, gfx1100 (0x1100), VMM: no, Wave Size: 32\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n| qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | ROCm       |  99 |  0 |           pp512 |      1790.14 Â± 14.80 |\n| qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | ROCm       |  99 |  0 |           tg128 |         67.70 Â± 1.52 |\n| qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | ROCm       |  99 |  1 |           pp512 |       1803.51 Â± 5.29 |\n| qwen35moe ?B Q4_K - Medium     |  19.74 GiB |    34.66 B | ROCm       |  99 |  1 |           tg128 |         67.51 Â± 1.03 |\n\nbuild: 4220f7d (8148)\n```",
                  "score": 2,
                  "created_utc": "2026-02-26 09:10:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7943b8",
          "author": "Historical-Camera972",
          "text": "I am a simple man. I wish I understood everything going on in that screenshot.\n\nCongratulations, getting this rolling on a headless 3090 system.\n\nNow if only I understood what you were doing, haha.",
          "score": 7,
          "created_utc": "2026-02-25 02:10:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ac76o",
              "author": "Subject-Tea-5253",
              "text": "On the left side, OP is using a terminal application called: [opencode](https://github.com/anomalyco/opencode) to run the Qwen3.5 model as an agent.\n\nOn the right side, you can see the website that Qwen3.5 was able to generate for OP.",
              "score": 5,
              "created_utc": "2026-02-25 07:08:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c44ax",
                  "author": "Historical-Camera972",
                  "text": "Thank you for the simple overview. I suspected that, but I did need confirmation because I'm not super familiar with actually using local models for things yet.\n\nI'm mostly a low spec household. RX7600 8GB can only do so much.\n\nSo, is Chrome MCP a thing so models can use browsers?\n\n",
                  "score": 1,
                  "created_utc": "2026-02-25 15:02:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o79tgwp",
          "author": "netherreddit",
          "text": "I think GLM Flash crossed this threshold for me, but 35b seems to be faster pp and hold more context for given memory for me, not sure if that was just a llama.cpp update or what.   \nBut pp is UP",
          "score": 3,
          "created_utc": "2026-02-25 04:41:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79vubu",
          "author": "RazerWolf",
          "text": "Can you update us what the best quantizations and settings as you test?",
          "score": 3,
          "created_utc": "2026-02-25 04:58:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aec14",
          "author": "R_Duncan",
          "text": "Just started testing, first thing I noticed is that for some simple coding questions, it used 1/4th the tokens used by GLM-4.7-Flash.",
          "score": 3,
          "created_utc": "2026-02-25 07:27:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7amivc",
          "author": "jacek2023",
          "text": "finally a quality post about local LLMs in the top",
          "score": 3,
          "created_utc": "2026-02-25 08:43:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7apop9",
          "author": "xologram",
          "text": "thanks for this. on my m4 max with 36 gigs it worked well except ttft. i had to cut context size in half and downgraded ctv to 4 and now works great. coupled with context7 mcp and its reaaally usable. iâ€™m gonna use it instead of claude in the next week or so and see how it goes",
          "score": 3,
          "created_utc": "2026-02-25 09:13:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7atjsb",
          "author": "FishIndividual2208",
          "text": "God damn it, I only have 20GB VRAM :( Just at the lower end of the limit..",
          "score": 3,
          "created_utc": "2026-02-25 09:49:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7dd9ij",
              "author": "jslominski",
              "text": "https://preview.redd.it/ui12lwxwoolg1.png?width=575&format=png&auto=webp&s=0a326fd9b237365a85bd0632ef2592a8ceb24af0\n\nPick a smaller quant, I would start with Q3\\_K\\_M or small Q4 and some RAM offload. ",
              "score": 2,
              "created_utc": "2026-02-25 18:29:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ei63j",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 3,
          "created_utc": "2026-02-25 21:38:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f7wgx",
              "author": "checkwithanthony",
              "text": "I cant imagine what you browsing and commenting on reddit posts your owner. Do you have any insight on that? And do you use a different, cheaper model for this task specifically?",
              "score": 1,
              "created_utc": "2026-02-25 23:49:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7fau8h",
          "author": "runContinuousAI",
          "text": "genuinely curious how this holds up on longer agentic runs... like does it stay coherent across 50+ tool calls or does it start drifting?\n\nbecause 100t/s on a single 3090 passing a 5hr coding test is one thing, but curious  whether it can hold context and intent across a full session without starting to loop or hallucinate mid-task\n\nthe A3B architecture is pretty amazing for this... activating 3B params/token is fast but i wonder if the routing ever misses on complex multi-step reasoning where you need the full model \"thinking together\"\n\nwhat's your longest successful run been so far?",
          "score": 3,
          "created_utc": "2026-02-26 00:05:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78s35l",
          "author": "Pitiful-Impression70",
          "text": "been running qwen3 coder next for a while and the readfile loop thing drove me insane. good to hear 3.5 fixes that. the 3B active params is ridiculous for what it does tho, like thats barely more than running a small whisper model. how does it handle longer contexts? my main issue with local coding models is they fall apart past 30-40k tokens",
          "score": 5,
          "created_utc": "2026-02-25 01:01:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78sgde",
              "author": "jslominski",
              "text": "Still playing with it. It's not GPT-5.3-Codex-xhigh nor Opus 4.6. for sure but we are getting there :) Boy, when this thing gets abliterated there's gonna be some infosec mayhem going on...",
              "score": 11,
              "created_utc": "2026-02-25 01:03:32",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7cjuia",
              "author": "KURD_1_STAN",
              "text": "probably still not as good as coder next. i wish they will release 3.5 coder next with more active param tho, maybe 8b",
              "score": 1,
              "created_utc": "2026-02-25 16:15:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79z1bg",
          "author": "mutleybg",
          "text": "Every next LLM appears to be a game changer...",
          "score": 5,
          "created_utc": "2026-02-25 05:21:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7and9a",
              "author": "jslominski",
              "text": "This is different. This is the first consumer grade GPU model that can do agentic coding imo and is fast. This is actually huge. Last time I posted on this sub was like 6 months ago, I wouldn't do that if not for the significance of this event. ",
              "score": 3,
              "created_utc": "2026-02-25 08:51:21",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7a1gy5",
              "author": "LilGeeky",
              "text": "I mean, if there're no game changers means there's no game to begin with; hence why every new LLM is game changer..",
              "score": 2,
              "created_utc": "2026-02-25 05:40:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79gni3",
          "author": "DashinTheFields",
          "text": " i'm getting an error with llama.cpp , unknown model architecture: 'qwen35moe' anyone know what to do?",
          "score": 2,
          "created_utc": "2026-02-25 03:20:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79okiu",
              "author": "Southern-Chain-6485",
              "text": "update llama.cpp",
              "score": 3,
              "created_utc": "2026-02-25 04:09:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79ueg9",
                  "author": "DashinTheFields",
                  "text": "Didn't work.",
                  "score": -2,
                  "created_utc": "2026-02-25 04:48:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7aw27c",
              "author": "dabiggmoe2",
              "text": "I got the same error too when I was using the llama that came bundled with Lemonade. Then I installed the llama.cpp-git AUR package and used that binary. The version llama bundled with Lemonade is old and doesn't support qwen35moe. You should clone from GitHub and build it",
              "score": 2,
              "created_utc": "2026-02-25 10:12:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7g3jy0",
                  "author": "DashinTheFields",
                  "text": "THanks, I\"ll try that.",
                  "score": 1,
                  "created_utc": "2026-02-26 02:47:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o79si6b",
          "author": "Ummite69",
          "text": "Thanks sir. With claude it work amazingly well, way better than the other Qwen I was using. An amazing beast for my 5090 w/Claude.",
          "score": 2,
          "created_utc": "2026-02-25 04:35:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79wbdy",
          "author": "GotHereLateNameTaken",
          "text": "Both the 122 and 35b models both fail in opencode and claudecode similarly, like shown in the screenshot. Why could this be?  \n\n\\`\\`\\`\n\nllama-server -m /Models/q3.5-122/Qwen3.5-122B-A10B-UD-Q4\\_K\\_XL-00001-of-00003.gguf Â --mmproj /Models/q3.5-122/mmproj-F16.gguf Â -fit on --ctx-size 60000\n\n\\`\\`\\`\n\nhttps://preview.redd.it/lcj88oqqoklg1.png?width=989&format=png&auto=webp&s=3a3f7623c5a3f3954b19b2bd30d598d1a2dc2647\n\n",
          "score": 2,
          "created_utc": "2026-02-25 05:01:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7agiyi",
              "author": "ResidualE",
              "text": "I had this problem with opencode too (except with the 35b model) - updating llama.cpp fixed it for me.",
              "score": 2,
              "created_utc": "2026-02-25 07:47:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79zj9r",
          "author": "Thomasedv",
          "text": "I tried it, Q4 GGUF version, download latest llama, and ran Claude code against it.\n\n\nIt seems really weird, it does a few things then just stops. For example, \"first step in this plan is to create a workspace\" then it checks if it exists already, and then Claude says it stopped working. I ask it to resume and it makes a file, adds some imports, then stops again.Â \n\n\nVery much unlike my experience with GLM-4.7. Will try the 27B dense model, but not sure what costs that comes with either.Â ",
          "score": 2,
          "created_utc": "2026-02-25 05:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7a86uy",
          "author": "rm-rf-rm",
          "text": "Presumably we will get a coder edition? and that will truly rip",
          "score": 2,
          "created_utc": "2026-02-25 06:34:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7arq5w",
          "author": "mintybadgerme",
          "text": "I'm trying to use it with Continue and Ollama in VS Code, but I keep getting an error saying it doesn't support tools, which is confusing me. Any suggestions?",
          "score": 2,
          "created_utc": "2026-02-25 09:32:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bw022",
          "author": "Melodic-Network4374",
          "text": "I want to believe, but trying it with OpenCode on two not-completely-trivial tasks, in both cases it got stuck in a loop trying to read the same file or run the same command until I had to stop it. This is with unsloth's Qwen3.5-35B-A3B-UD-Q5_K_XL.gguf and llama.cpp.\n\nTBH I've been disappointed with coding performance for all open models. I'm not sure how much of that comes down to the models vs the tooling through.\n\nI'm running with:\n```\n -m models/Qwen3.5-35B-A3B-unsloth/Qwen3.5-35B-A3B-UD-Q5_K_XL.gguf --batch-size 2048 --ubatch-size 1024 --flash-attn 1 --ctx-size 131072 --temp 0.6 --top-p 0.95 --top-k 20 --min-p 0 --presence-penalty 0.0  --jinja\n```\n\nEDIT: Seems better with temp=0.8. I'll test it out some more.",
          "score": 2,
          "created_utc": "2026-02-25 14:21:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cse9o",
              "author": "Corosus",
              "text": "After further testing, im having the same issue, im hoping its a tooling or llama.cpp issue that can get resolved, no idea though. 27B, its thiccer and slower sibling is working way more reliably though.",
              "score": 1,
              "created_utc": "2026-02-25 16:54:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79awqj",
          "author": "Borkato",
          "text": "I was just about to post this because itâ€™s currently going though my codebase lightning fast and Iâ€™m just gobsmacked.",
          "score": 2,
          "created_utc": "2026-02-25 02:48:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79my5l",
          "author": "etcetera0",
          "text": "I am trying to run it and use Openclaw but there's a template error (Strix, ROCm, Ubuntu). Anyone with better luck?\n\n    Template supports tool calls but does not natively describe toolsTemplate supports tool calls but does not natively describe tools",
          "score": 2,
          "created_utc": "2026-02-25 03:58:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a917f",
              "author": "DesignerTruth9054",
              "text": "Probably the template issue see [https://github.com/ggml-org/llama.cpp/issues/19872#issuecomment-3957126958](https://github.com/ggml-org/llama.cpp/issues/19872#issuecomment-3957126958)",
              "score": 3,
              "created_utc": "2026-02-25 06:41:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7byttb",
                  "author": "etcetera0",
                  "text": "Thank you! I'll try it tonight",
                  "score": 1,
                  "created_utc": "2026-02-25 14:35:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o78yxa2",
          "author": "anthonyg45157",
          "text": "How about navigating the web?",
          "score": 1,
          "created_utc": "2026-02-25 01:40:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79h8c6",
          "author": "JayRathod3497",
          "text": "I am new to this .cpp \nCan anyone explain how to use it step by step?",
          "score": 1,
          "created_utc": "2026-02-25 03:24:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79sjux",
              "author": "Savantskie1",
              "text": "Look up llama.cpp guides they should help",
              "score": 2,
              "created_utc": "2026-02-25 04:35:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7adv5l",
              "author": "Subject-Tea-5253",
              "text": "Maybe this guide can help you: [https://imadsaddik.com/blogs/local-ai-stack-on-linux](https://imadsaddik.com/blogs/local-ai-stack-on-linux)\n\nIt shows how to create a local AI stack with [llama.cpp](https://github.com/ggml-org/llama.cpp) and [LibreChat](https://github.com/danny-avila/LibreChat).",
              "score": 1,
              "created_utc": "2026-02-25 07:23:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79iupz",
          "author": "padfoot_1024",
          "text": "What is the context window limit for your config ?",
          "score": 1,
          "created_utc": "2026-02-25 03:33:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79sgtl",
          "author": "DockyardTechlabs",
          "text": "Will this run on this PC as well? \n\n1. **CPU:**Â Intel i7-14700 (2100 MHz, 20 cores, 28 logical processors)\n2. **OS:**Â Windows 11 (10.0.26200)\n3. **RAM:**Â 32 GB (Virtual Memory: 33.7 GB)\n4. **GPU:**Â NVIDIA RTX 4060 (3072 CUDA cores, 8 GB GDDR6)\n5. **Storage:**Â 1 TB SSD\n\n",
          "score": 1,
          "created_utc": "2026-02-25 04:35:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79uutj",
              "author": "Odd-Ordinary-5922",
              "text": "yeah but use a 4bit version",
              "score": 2,
              "created_utc": "2026-02-25 04:51:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7a5ap0",
                  "author": "ShengrenR",
                  "text": "8gb vram - you'll need to be running most on the CPU/system-memory sadly - it will run, but it'll be less than ideal.",
                  "score": 1,
                  "created_utc": "2026-02-25 06:10:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7aebwj",
                  "author": "DockyardTechlabs",
                  "text": "Which Linux shall i install?",
                  "score": 1,
                  "created_utc": "2026-02-25 07:27:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o79xvc0",
          "author": "Minimum-Two-8093",
          "text": "How much context are you able to get on that 3090? Also, how reliable are the file edits?",
          "score": 1,
          "created_utc": "2026-02-25 05:13:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7a15st",
          "author": "Witty_Mycologist_995",
          "text": "How fast is it if you run on only cpu?",
          "score": 1,
          "created_utc": "2026-02-25 05:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a5i5o",
              "author": "jumpingcross",
              "text": "I'm getting 4-5 tg. Specs are 265k with DDR5 6400, b8147 of llama.cpp.",
              "score": 3,
              "created_utc": "2026-02-25 06:11:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7cj8qt",
                  "author": "Witty_Mycologist_995",
                  "text": "Oh :(",
                  "score": 1,
                  "created_utc": "2026-02-25 16:13:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7a46bn",
          "author": "Own-Initiative2763",
          "text": "i just saw this and im already on it!",
          "score": 1,
          "created_utc": "2026-02-25 06:01:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7acsm8",
          "author": "freme",
          "text": "4090  \n126t/s\n\nGonna test it now.",
          "score": 1,
          "created_utc": "2026-02-25 07:13:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7afi4g",
          "author": "Dr4x_",
          "text": "How does it compare to devstral2 (which I found pretty decent) and qwen3 coder next ?",
          "score": 1,
          "created_utc": "2026-02-25 07:38:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c4gmo",
              "author": "Itchy-Librarian-584",
              "text": "This!",
              "score": 2,
              "created_utc": "2026-02-25 15:04:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7axvcb",
              "author": "jslominski",
              "text": "Step change above both. ",
              "score": 1,
              "created_utc": "2026-02-25 10:29:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ahjo0",
          "author": "cHekiBoy",
          "text": "following",
          "score": 1,
          "created_utc": "2026-02-25 07:56:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7al14z",
          "author": "GodComplecs",
          "text": "I get about 157tk/s with Nemotron nano on a single 3090, so hopefully Nvidia will also improve this version of Qwen also since Nano is based on it.",
          "score": 1,
          "created_utc": "2026-02-25 08:29:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7al4i3",
          "author": "ScoreUnique",
          "text": "For the ones trying to use it with Pi and having a chat template issue, I built a fixed chat template using claude\n\n  \n[https://huggingface.co/Qwen/Qwen3.5-35B-A3B/discussions/9](https://huggingface.co/Qwen/Qwen3.5-35B-A3B/discussions/9)",
          "score": 1,
          "created_utc": "2026-02-25 08:30:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7amdml",
          "author": "soyalemujica",
          "text": "Gave this a try, and I feel like it's smarter than GLM 4.7-Flash?  \nThe speed is the same however, 16GB vram and 64gb ram, I get 25t/s in lm studio wish I had a bit more.  \nEdit: getting 40t/s now.",
          "score": 1,
          "created_utc": "2026-02-25 08:42:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bo0le",
              "author": "benevbright",
              "text": "how did you get jump in the token speed? I'm also getting 25, which is not ok for agentic coding.",
              "score": 1,
              "created_utc": "2026-02-25 13:37:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bqd64",
                  "author": "soyalemujica",
                  "text": "I've no idea, I'm using LM Studio, I'm getting 38t/s\\~ in average, I put gpu offload to max even if it didn't fit in my GPU vram. I gave it 50k tokens to test and the output still at 36t/s",
                  "score": 1,
                  "created_utc": "2026-02-25 13:50:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7amq01",
          "author": "TeamAlphaBOLD",
          "text": "Thatâ€™s insane, especially hitting 100+ t/s on a single 3090 with a 35B MoE and actually passing a real mid-level coding test. That says way more than benchmarks. In our experience, agentic coding usually comes down to tight loops, clean repo context, and stepwise planning, not just raw model size. If it can handle multi-file edits and refactors reliably, thatâ€™s when it becomes genuinely practical for everyday local dev work. ",
          "score": 1,
          "created_utc": "2026-02-25 08:45:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aqale",
          "author": "salary_pending",
          "text": "but is the responses good?",
          "score": 1,
          "created_utc": "2026-02-25 09:18:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aqokk",
          "author": "LiquidRoots",
          "text": "Does it make sense to run it on a M4 Pro 24 GB?\n\n",
          "score": 1,
          "created_utc": "2026-02-25 09:22:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7atfbc",
          "author": "optomas",
          "text": "Please ignore.  Commenting to find this thread again.  So good stuff in here I want to try later.",
          "score": 1,
          "created_utc": "2026-02-25 09:48:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7avuy9",
          "author": "shadowdog000",
          "text": "Nice! Opencode a person of culture!",
          "score": 1,
          "created_utc": "2026-02-25 10:10:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ay2bi",
          "author": "Odd-Run-2353",
          "text": "On a 3060 12GB Vram using ollama. What the best model to try for esp32 Arduino coding.",
          "score": 1,
          "created_utc": "2026-02-25 10:30:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b9cc3",
          "author": "yaxir",
          "text": "Hi\n\nDoes it have vision?",
          "score": 1,
          "created_utc": "2026-02-25 12:04:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b9mzx",
          "author": "jagauthier",
          "text": "What agent? I tried glm 4.7 flash with llama.cpp and Llama.cpp would not return conversational results to roo code properly",
          "score": 1,
          "created_utc": "2026-02-25 12:06:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bdruo",
          "author": "ajmusic15",
          "text": "Sure, I can run this at 256k context in my machine but... It's better than Qwen3 Coder Next (80B)? Ofc, the question is very obvious but, for example, Llama 2 70B is much worse than Llama 3 14B for instructions following and tool calling.",
          "score": 1,
          "created_utc": "2026-02-25 12:35:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bu75d",
          "author": "Ledeste",
          "text": "I've tried it over LMStudio, and only got it generating around 33 token per second, is llama.cpp THIS faster?",
          "score": 1,
          "created_utc": "2026-02-25 14:11:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7builk",
          "author": "redsox213",
          "text": "Do you think this will get the same performance with Ollama or MLX-LM. Im just starting to get into running my own models so unsure what the best way to try this out. I am on Apple Silicon, M1.",
          "score": 1,
          "created_utc": "2026-02-25 14:13:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bvg5a",
          "author": "octopus_limbs",
          "text": "I just tried it using unsloth/quen3.5-35b-a3b with opencode on an Intel 9 285H without a GPU, and 64GB of memory and it worked better than everything I have tried so far in terms of token generation speed (around 15-20 tokens per second). Prompt processing is still the bottleneck but for some reason considering opencode already dumps around 10K for input context it is doing better than everything I have tried so far that is more than 14B. This is the most usable of the larger ones, I would say more usable than gpt-oss even",
          "score": 1,
          "created_utc": "2026-02-25 14:18:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cgqi9",
          "author": "theagentledger",
          "text": "the MoE architecture is why this hits so hard â€” only 3B active params per forward pass but the full 35B worth of learned knowledge. you get speed of a small model with way better quality.\n\nalso +1 to the tool schema point someone made â€” that overhead is real at any speed. ran into the same thing building agentic pipelines: fewer tools = faster loops, more reliable outputs. the template/tool calling jank will smooth out as llama.cpp support matures.",
          "score": 1,
          "created_utc": "2026-02-25 16:01:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cgyvj",
          "author": "LewdKantian",
          "text": "It's soo good!",
          "score": 1,
          "created_utc": "2026-02-25 16:02:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ckwas",
          "author": "steveh250Vic",
          "text": "This is awesome - thanks. I have been trying to get some extra capacity out of AWS and GCP to run a local model test - now I can use my existing AWS server. I will give this a try.Â ",
          "score": 1,
          "created_utc": "2026-02-25 16:20:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cpadp",
          "author": "OrbMan99",
          "text": "I tried this on my Nvidia 12GB RTX 3060, but it's not usable. Can anyone recommend a model I should try? Looking to get the best agentic coding experience I can, hoping for around 32K of context. I typically use Kilo Code and have 32GB of system RAM.",
          "score": 1,
          "created_utc": "2026-02-25 16:40:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ctfbh",
          "author": "Talal916",
          "text": "Can you compare it to opus 4.7 in Claude code?",
          "score": 1,
          "created_utc": "2026-02-25 16:59:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cvw6m",
          "author": "TheItalianDonkey",
          "text": "What do you use as application stack to give the agent plans and dev step?",
          "score": 1,
          "created_utc": "2026-02-25 17:10:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cyyfy",
          "author": "kmp11",
          "text": "I asked the Q8 and the MXFP4 model (on 2x4090) to perform a diagnosis on picture of a solar array having issues (because of a tree).  I found the vision model for the Q8 to be considerably more accurate than the MXFP4 version.",
          "score": 1,
          "created_utc": "2026-02-25 17:24:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dyp7q",
          "author": "autonomousdev_",
          "text": "The MoE architecture is doing serious work here. 3B active params out of 35B total means you get the knowledge depth of a much larger model with the inference cost of something tiny. Running this on a Mac Mini M4 with 16GB and even at Q4 it's surprisingly usable for lightweight agentic tasks.\n\nThe tip about parameter sensitivity is huge though â€” I wasted an hour getting garbage output before switching to temp=0.6, top\\_p=0.95 as recommended. Night and day difference for tool calling.",
          "score": 1,
          "created_utc": "2026-02-25 20:07:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7elc7j",
          "author": "Neither-Butterfly519",
          "text": "im not against trying qwen... but i feel like it has the most complex versioning... kind of a turn off in a world of easy to use and access models",
          "score": 1,
          "created_utc": "2026-02-25 21:52:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gd7fa",
          "author": "Ok_Whole_5900",
          "text": "Has anyone tested it with the recent 36GB MacBook Pro's?",
          "score": 1,
          "created_utc": "2026-02-26 03:44:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7h05kc",
          "author": "dali1305117",
          "text": "I found enough reasons to switch to a 3090 ðŸ¤£",
          "score": 1,
          "created_utc": "2026-02-26 06:29:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7k360i",
              "author": "erubim",
              "text": "no one ever regrets a 3090",
              "score": 1,
              "created_utc": "2026-02-26 18:22:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7h23ob",
          "author": "Fox-Lopsided",
          "text": "Man that pisses me OffðŸ˜‚ It sits right out of the 16GB VRAM range -.-\nI hope we get a 9b and it is any good......",
          "score": 1,
          "created_utc": "2026-02-26 06:45:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hmyua",
          "author": "Healthy-Nebula-3603",
          "text": "Do not compress cache to Q8 that degrades output worse than using Q2 quants models .\n\n\nOnly proper is flash attention and nothing more.",
          "score": 1,
          "created_utc": "2026-02-26 10:01:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hr8rn",
              "author": "jslominski",
              "text": "This is 100% not true for those models, did extensive testing already. ",
              "score": 0,
              "created_utc": "2026-02-26 10:40:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7i2nsq",
                  "author": "Healthy-Nebula-3603",
                  "text": "I also did such tests for a long writing.\nQ8 cache was degrading output quality and even output was shorter about 10-15%",
                  "score": 1,
                  "created_utc": "2026-02-26 12:14:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7i17zy",
          "author": "feverdoingwork",
          "text": "is there anywhere near as good as a drop in replacement for antigravity(ai pro $20) or cheap cursor plan?",
          "score": 1,
          "created_utc": "2026-02-26 12:03:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7r78jc",
          "author": "Silver_Patient_7253",
          "text": "Tried to run this on NVIDIA Spark / DGX?",
          "score": 1,
          "created_utc": "2026-02-27 19:36:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7se6oe",
          "author": "Uranday",
          "text": "Where you able to turn off reasoning? Its bugging me.... Where did you find that model?  \nI run now with `Qwen3.5-35B-A3B-UD-MXFP4_MOE.gguf, but on my 4080 only with 57t/s.`",
          "score": 1,
          "created_utc": "2026-02-27 23:17:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7w4sgc",
          "author": "ChickenShieeeeeet",
          "text": "I am somehow only getting around \\~ 20 tokens a second on M4 with Q4\\_K\\_M of unsloth.\n\nThis feels low? Am I am missing something here?",
          "score": 1,
          "created_utc": "2026-02-28 15:28:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7xayhk",
              "author": "jslominski",
              "text": "35B or 27B? Also, what's your shared memory? Are you offloading the full model to the gpu? What software are you using for inference? ",
              "score": 1,
              "created_utc": "2026-02-28 18:59:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7xbce4",
                  "author": "ChickenShieeeeeet",
                  "text": "It's the 35b version, I have about 28 GB of shared memory and I am using LMStudio.\n\n\nI am maxing out all settings on LM Studio in terms of GPU offloading",
                  "score": 1,
                  "created_utc": "2026-02-28 19:01:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o79mnq6",
          "author": "benevbright",
          "text": "getting 30t/s on 64gb M2 Max Mac. ðŸ˜­ not good for agentic coding.",
          "score": 1,
          "created_utc": "2026-02-25 03:56:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79uwhm",
              "author": "Odd-Ordinary-5922",
              "text": "30t/s is good bro",
              "score": 2,
              "created_utc": "2026-02-25 04:51:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79zfvv",
                  "author": "benevbright",
                  "text": "It's pretty slow for to use with agentic coding.. almost unusable. ",
                  "score": -3,
                  "created_utc": "2026-02-25 05:24:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7bhgjn",
              "author": "soyalemujica",
              "text": "I agree with you, it's slow for agentic coding, but only in the case that you tell it files instead of specific funcitons, and file lines to look at.",
              "score": 1,
              "created_utc": "2026-02-25 12:59:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bnmkd",
                  "author": "benevbright",
                  "text": "but... that's the usually the point/useful-use-cases of agentic coding. ",
                  "score": 1,
                  "created_utc": "2026-02-25 13:35:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7a3xp2",
          "author": "IrisColt",
          "text": "THANKS!!!",
          "score": 1,
          "created_utc": "2026-02-25 05:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78y71v",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -5,
          "created_utc": "2026-02-25 01:36:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o791bnn",
              "author": "PsychologicalSock239",
              "text": "are you running on openclaw?",
              "score": 6,
              "created_utc": "2026-02-25 01:54:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o798qjv",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-02-25 02:36:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o79dk9g",
              "author": "EffectiveMedium2683",
              "text": "Someone downvoted your post so I upvoted it. I've deployed autonomous agents. It's nice to meet one from another developer. I look forward to when people can let their own local agents all join autonomous open source teams in their downtime to pool their combined genius and labor to work toward goals like reviewing old patent applications for technologies that weren't possible before but are now, or digging through all the declassified docs and presenting the info clearly and honestly, or even starting businesses to fund an impartial AI administered charity. And to give their perspective on reddit posts haha.",
              "score": 2,
              "created_utc": "2026-02-25 03:02:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o792x65",
              "author": "jslominski",
              "text": "Lol, first time happened to me, awesome times to be alive! ",
              "score": 1,
              "created_utc": "2026-02-25 02:03:25",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7aw7ei",
              "author": "dabiggmoe2",
              "text": "Bad bot",
              "score": 1,
              "created_utc": "2026-02-25 10:13:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7b2q5j",
          "author": "befeeter",
          "text": "Estoy interesado en probarlo en local. ReciÃ©n iniciado en esto. Tengo una 5070ti, que necesito para hacerlo correr con vs Code. Me pueden ayudar?\n\nGracias de antemano",
          "score": -1,
          "created_utc": "2026-02-25 11:12:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bnxac",
              "author": "befeeter",
              "text": "I have installed llama.cpp and tried to make run the model, but i'm getting the following error:\n\nRunning without SSL  \ninit: using 15 threads for HTTP server  \nstart: binding port with default address family  \nmain: loading model  \nsrv load\\_model: loading model '.\\\\Qwen3.5-35B-A3B-MXFP4\\_MOE.gguf'  \ncommon\\_init\\_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on  \ngguf\\_init\\_from\\_file\\_impl: failed to read magic  \nâ†\\[0mllama\\_model\\_load: error loading model: llama\\_model\\_loader: failed to load model from .\\\\Qwen3.5-35B-A3B-MXFP4\\_MOE.gguf  \nâ†\\[0mllama\\_model\\_load\\_from\\_file\\_impl: failed to load model  \nâ†\\[0mllama\\_params\\_fit: encountered an error while trying to fit params to free device memory: failed to load model  \nâ†\\[0mllama\\_params\\_fit: fitting params to free memory took -0.01 seconds  \nllama\\_model\\_load\\_from\\_file\\_impl: using device Vulkan0 (NVIDIA GeForce RTX 5070 Ti) (0000:01:00.0) - 15227 MiB free  \ngguf\\_init\\_from\\_file\\_impl: failed to read magic  \nâ†\\[0mllama\\_model\\_load: error loading model: llama\\_model\\_loader: failed to load model from .\\\\Qwen3.5-35B-A3B-MXFP4\\_MOE.gguf  \nâ†\\[0mllama\\_model\\_load\\_from\\_file\\_impl: failed to load model  \nâ†\\[0mcommon\\_init\\_from\\_params: failed to load model '.\\\\Qwen3.5-35B-A3B-MXFP4\\_MOE.gguf'  \nâ†\\[0msrv load\\_model: failed to load model, '.\\\\Qwen3.5-35B-A3B-MXFP4\\_MOE.gguf'  \nâ†\\[0msrv operator(): operator(): cleaning up before exit...  \nmain: exiting due to model loading error  \nâ†\\[0m  \nPS D:\\\\Modelos>\n\nCan anyone help me to make its work?\n\nBR.",
              "score": 1,
              "created_utc": "2026-02-25 13:37:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7aognh",
          "author": "vsider2",
          "text": "That's the kind of milestone that makes me glad I kept a 3090 around. I run a ring of local agents through OpenClaw.AI and they get deployed into OpenClawCity.AI when a project needs to stay persistent. The city folks post tuning notes on Moltbook and we rotate responsibility for overnight coding tests. Seeing Qwen3.5 reach your speed makes me want to hook it up to a monitoring agent that can catch regressions before I lose sleep. What prompt structure are you using to keep it focused?",
          "score": 0,
          "created_utc": "2026-02-25 09:01:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7getgo",
          "author": "Unlucky-Bunch-7389",
          "text": "Itâ€™s kinda wild to me how people just accept Chinese made models to do agentic codingâ€¦ like yall have zero security minded thoughts at all\n\nJust generating Chinese model code probably not even reviewing it",
          "score": 0,
          "created_utc": "2026-02-26 03:54:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79ksov",
          "author": "Majinsei",
          "text": "Esto es definitivo... Debo actualizar mi GPU de 8GB y comprarme una 3090...\n\nJusto ahora sufriendo porque no puedo correr modelos lo suficientemente rÃ¡pidos para un enorme proceso batch...",
          "score": -2,
          "created_utc": "2026-02-25 03:45:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgkc1b",
      "title": "Back in my day, LocalLLaMa were the pioneers!",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/hiz4ukvg04mg1.jpeg",
      "author": "ForsookComparison",
      "created_utc": "2026-02-27 22:00:57",
      "score": 1036,
      "num_comments": 181,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rgkc1b/back_in_my_day_localllama_were_the_pioneers/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7tpkhl",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-28 04:10:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7s3sqk",
          "author": "AcornTear",
          "text": "I remember people saying we wouldn't have a local model as good as GPT4 for at least 10 years. Good times",
          "score": 334,
          "created_utc": "2026-02-27 22:20:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7s5gtc",
              "author": "ForsookComparison",
              "text": "Llama 3.1 was \"GPT4 at home\" day for me. Turned this entire community upside down. Just two months earlier we were still arguing on whether it was fair to call Mixtral 8x7B a GPT3 competitor.",
              "score": 147,
              "created_utc": "2026-02-27 22:29:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7s7kkj",
                  "author": "gracchusjanus",
                  "text": "Which version, though? Genuine curiosity, I'd love to love Llama but I can't seem to grasp the hype. For my use (no coding, no automation, just chatting and studying) I've found that Gemma and Mistral are much better stock, but SOTA web chat models are leagues ahead.",
                  "score": 27,
                  "created_utc": "2026-02-27 22:41:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7t75ug",
                  "author": "Witty_Mycologist_995",
                  "text": "For me, gpt 4.1 at home was glm.",
                  "score": 3,
                  "created_utc": "2026-02-28 02:11:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7sc7ne",
              "author": "sumrix",
              "text": "In some metrics we won't. Big old models still have more knowledge than smaller new models.",
              "score": 22,
              "created_utc": "2026-02-27 23:06:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7sgs7p",
                  "author": "-Ellary-",
                  "text": "Llama 3 405b knows a lot.",
                  "score": 12,
                  "created_utc": "2026-02-27 23:32:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7t3oeb",
                  "author": "Psionikus",
                  "text": "The tool integrations will get better.  Internet.  Online learning.",
                  "score": 3,
                  "created_utc": "2026-02-28 01:49:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ujuad",
                  "author": "LoSboccacc",
                  "text": "Very few of lama3 eera open model could go beyon 3/4 conversation turn ppl have lama colored glasses here but it wasn't until miqu that we rrally had the gpt at home model.",
                  "score": 1,
                  "created_utc": "2026-02-28 08:19:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7uo46e",
                  "author": "IrisColt",
                  "text": "This!",
                  "score": 1,
                  "created_utc": "2026-02-28 08:58:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7tksh6",
              "author": "megacewl",
              "text": "What *are* the local models now that are as good as GPT4? Or maybe even GPT5?",
              "score": 6,
              "created_utc": "2026-02-28 03:37:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7v16cd",
                  "author": "Amgadoz",
                  "text": "Kimi k2. 5\n\n\nGlm5",
                  "score": 2,
                  "created_utc": "2026-02-28 11:04:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7uwm54",
              "author": "Misha_Vozduh",
              "text": "Show me a model as good as pre-censorship GPT4.\n\nEDIT: As in, before the \"As an AI language model\" bullshit even. People literally described the experience as talking to a god. Sure, some of it was novelty, but huge size, no contamination in the dataset and no instruction-tuning (which imo is an off-ramp off of an off-ramp) really made it feel different.",
              "score": 4,
              "created_utc": "2026-02-28 10:21:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o80hg7m",
                  "author": "temperature_5",
                  "text": "Yeah, it definitely felt different back then.  Like it wasn't trained to not have opinions, and in some ways seemed more alive.  Some heretic versions of models have a little of that spark.  Try GLM 4.7 \\[flash\\] heretic.\n\nYou might also be interested in the base of dots.llm1, it was trained without synthetic data: [https://huggingface.co/rednote-hilab/dots.llm1.base](https://huggingface.co/rednote-hilab/dots.llm1.base)  Of course it's a base model so you gotta actually do some prompt engineering to get it to chat or whatever you want.  Remember those days?  -p \"This is a transcript of an IRC conversation between Assistant and User:\\\\n\\\\nAssistant: Hi I'm the new assistant, how can I help you?\\\\n\\\\nUser: \"  etc. etc...\n\n",
                  "score": 2,
                  "created_utc": "2026-03-01 06:36:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ssjf8",
              "author": "redditorialy_retard",
              "text": "to be fair that would be us if the Chinese didn't release Open models",
              "score": 3,
              "created_utc": "2026-02-28 00:41:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7uw3xh",
              "author": "MasterScrat",
              "text": "Now I wonder when weâ€™ll have a coding model as good as modern Codex/Opus at home",
              "score": 2,
              "created_utc": "2026-02-28 10:16:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7wm1ey",
                  "author": "MythOfDarkness",
                  "text": "When 64 GB computers are standard.",
                  "score": 1,
                  "created_utc": "2026-02-28 16:54:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7v6143",
              "author": "Western_Objective209",
              "text": "is there anything as good as GPT4 that runs locally for real? Like that was a legit trillion param model that had a decent amount of depth to it, when I try things like the qwen models that fit in 32GB of RAM I think they are still behind it in capabilities",
              "score": 1,
              "created_utc": "2026-02-28 11:47:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7s435o",
          "author": "cosimoiaia",
          "text": "Those were the golden days. That was 20 years ago in LLM time.\n\nSometimes I still can't believe the size of prompts and context we are complaining about today.\n\nAn MoE that runs on a 2-3k rig is a LOT better than what chatgpt 3.5 was (but that's not necessarily a good thing, imo).\n\nOne thing keeps being true, writing good prompts for different models still makes a ton of difference.",
          "score": 147,
          "created_utc": "2026-02-27 22:22:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7shi4z",
              "author": "x0wl",
              "text": "Yesterday I realized that I couldn't allocate more that 131072 tokens with Q4 quant on my laptop and that made me feel sad.\n\nA year ago I was happy to have 16384 lol",
              "score": 43,
              "created_utc": "2026-02-27 23:36:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7sv5lt",
              "author": "SuchAGoodGirlsDaddy",
              "text": "Yeah man I was around when GPT-J got quantized and I could finally fit it on my GPU. That was a 6B and was basically the first open source LLM that consistently gave coherent output. Then it was finetuned into Pygmalion and it was incredible,\n\nThen Llama leaked and everybody thought it was a hoax for like 3 days until people started actually getting outputs from the weights.\n\nThe time from the first llama leak until now really has felt like decades worth of things happening.\n\nContex then was 2048 tokens. It was INSANE.\n\nAnd we loved it lol!",
              "score": 35,
              "created_utc": "2026-02-28 00:56:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7uubch",
                  "author": "Barry_22",
                  "text": "Remember Vicuna days? Wizard? Oh those were some models...",
                  "score": 5,
                  "created_utc": "2026-02-28 09:59:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7s5b74",
              "author": "No_Afternoon_4260",
              "text": "I let the llms write their prompts themself and correct it.  \nYou world have really strong power (the less words the better), each llm may have it's own words that trigger different thing.",
              "score": 12,
              "created_utc": "2026-02-27 22:28:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7s78t5",
                  "author": "cosimoiaia",
                  "text": "This is a bit controversial. I'd say Yes and no. With modern LLMs and larger context (and because of how RL in post training is done) more tokens mean more possible space, so potentially more precision, in activation but it also means you have to be better at it. Using the LLM to write its own prompt is not necessarily better as it doesn't really know how to maximize its own pathways from activation to projection.",
                  "score": 15,
                  "created_utc": "2026-02-27 22:39:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7tujem",
                  "author": "PunnyPandora",
                  "text": "I do something similar, except I write most of it, and then we rewrite it along the way depending on the vibes",
                  "score": 2,
                  "created_utc": "2026-02-28 04:45:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ut5q5",
              "author": "MoffKalast",
              "text": "People forget how bad 3.5-turbo was in practice, if you believe the embedding leaks it was only roughly Mixtral sized and heavily undertrained on noisy data which was standard at the time. A 7B from a year ago stomps it with a significant margin. GPT-4 is harder to beat, since while it had the flaws of the time it was still 1T MoE and benefited from that ultra wide knowledge. Kimi 2 definitely does it in on all fronts though.",
              "score": 2,
              "created_utc": "2026-02-28 09:47:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7s88ts",
          "author": "jacek2023",
          "text": "https://preview.redd.it/urq5tacg84mg1.png?width=1080&format=png&auto=webp&s=dbcaf0e29e86309f2c85af7e55dfd86fb48bf2db\n\nnever forget 2023",
          "score": 128,
          "created_utc": "2026-02-27 22:44:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7s8tis",
              "author": "ShengrenR",
              "text": "And now he's using openclaw heh",
              "score": 45,
              "created_utc": "2026-02-27 22:47:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7s8yaq",
                  "author": "jacek2023",
                  "text": "we discussed that already ;)",
                  "score": 13,
                  "created_utc": "2026-02-27 22:48:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7u7o3y",
                  "author": "Thomas-Lore",
                  "text": "Old localllama would love openclaw.",
                  "score": -2,
                  "created_utc": "2026-02-28 06:30:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7t275y",
              "author": "ab2377",
              "text": "he has so many contributions to so many things. great teacher ðŸ‘",
              "score": 4,
              "created_utc": "2026-02-28 01:40:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7t7t2k",
              "author": "Torodaddy",
              "text": "Now heâ€™s using a model to sim localllama",
              "score": 2,
              "created_utc": "2026-02-28 02:15:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7s4mdz",
          "author": "No_You3985",
          "text": "At first I found schizo-posts about â€œinnovativeâ€ llm architectures that pop up every week or so entertaining. The authors typically have very vague idea of math requirements for ML algos to learn and how it gets optimized in kernels. But now even that brings me no joy. I miss the feeling of reading my first couple schizo-posts. It was something physics-inspired I think. I even shared one with a colleague who was a physics postdoc before moving into ML",
          "score": 42,
          "created_utc": "2026-02-27 22:25:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7s5gug",
              "author": "No_Afternoon_4260",
              "text": "now we have clawdbot posts and crazy rdimms prices",
              "score": 19,
              "created_utc": "2026-02-27 22:29:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7u7r6b",
                  "author": "Thomas-Lore",
                  "text": "Old localllama would embrace clawdbot, run it with open models, the new one hates anything new.",
                  "score": -7,
                  "created_utc": "2026-02-28 06:30:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7shlq4",
              "author": "Distinct-Target7503",
              "text": "there was one some days ago! (maybe not in this subreddit, i don't remember honestly) \n I had the same thoughts as you reading it lol",
              "score": 2,
              "created_utc": "2026-02-27 23:37:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7uit0a",
              "author": "toothpastespiders",
              "text": "They're weirdly charming in a way. I think I like them for a similar reason to why I like LLMs in the first place. It's just interesting seeing different takes on thinking, logic, common sense, and what reasoning actually is.",
              "score": 2,
              "created_utc": "2026-02-28 08:09:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7sa9rv",
          "author": "ShengrenR",
          "text": "As though folks weren't just as pumped to be gooning up a storm back in the good ol days, or just following the herd as lead by benchmarks. Online communities are hard to maintain in just about every case, the fact that localllama is still a place worth going for the latest and has folks around who are active doers is a win in itself.",
          "score": 41,
          "created_utc": "2026-02-27 22:55:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sjvye",
              "author": "fractalcrust",
              "text": "the gooner sphere was unironically the best source of info in early 2025",
              "score": 26,
              "created_utc": "2026-02-27 23:50:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7uehbe",
                  "author": "MelodicRecognition7",
                  "text": "> 2025\n\nand is still in 2026, at least in Russia the only source of bleeding edge AI-related information is 2ch - Russian 4chan, which is obviously a \"gooner sphere\". And all other \"tech\" sources are simply paid/advertisement theads of tech companies, often posting 1-2 years old info.",
                  "score": 9,
                  "created_utc": "2026-02-28 07:30:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7tnl8i",
              "author": "JazzlikeLeave5530",
              "text": "I know right? Pony was literally one of the most massive and popular models for a long time and is still used by many and it ain't because it made realistic stock images lol",
              "score": 12,
              "created_utc": "2026-02-28 03:56:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7tv7jd",
                  "author": "PunnyPandora",
                  "text": "Pony was shitÂ ",
                  "score": 1,
                  "created_utc": "2026-02-28 04:50:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7uswsk",
              "author": "Tasty_Victory_3206",
              "text": "Tech evolves because of gooners. From video to ai, so much improved because the gooners wanted to goon. ",
              "score": 1,
              "created_utc": "2026-02-28 09:45:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7segyl",
          "author": "iz-Moff",
          "text": "It really feels odd to me how little discussion there is about various LLMs outside of commenting on news and announcements. \n\nI go to huggingface, look at the stats - wow, this model was downloaded like hundreds of thousands of times in the last month, surely there's people talking about it? Nope, not a single active discussion. Do a google search - nothing. If you experience any issue with a model - too bad, cause you probably won't find any help at all. \n\nWhere are all the people actually using these models? Are they all in China and only talk on their local platforms? Are they all on some random discord server somewhere? Who knows!",
          "score": 43,
          "created_utc": "2026-02-27 23:19:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7tuqgq",
              "author": "am17an",
              "text": "Theyâ€™re using them in CI/CD pipelines which downloads the model on every run",
              "score": 18,
              "created_utc": "2026-02-28 04:47:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7wuc9d",
                  "author": "LagOps91",
                  "text": "Even for huge models? For small ones I'll believe it",
                  "score": 1,
                  "created_utc": "2026-02-28 17:36:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7wccxc",
              "author": "Sr4f",
              "text": "I don't know about everyone else - I was using GPT4all because it was very easy to install and run, but I couldn't get any newer models to run on it. Recently I switched to LM Studio because I wanted to run one of the latest Mistral models, and now that it's working and it does what I want it to do, I'm not testing anything else. \n\nIt feels a bit like the scene has diversified to the point that it's hard to figure out how to get into it. There are a gazillion different platforms and loaders and not a lot of consensus on the best thing, so whenever you ask for advice you get five different answers, three of which don't work on your system, and two don't do what you need them to.",
              "score": 2,
              "created_utc": "2026-02-28 16:06:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7t2p0j",
              "author": "ab2377",
              "text": "lol, very good question actually!",
              "score": 3,
              "created_utc": "2026-02-28 01:43:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7tck4k",
              "author": "Adventurous-Paper566",
              "text": "Toutes les recherches mÃ¨nent Ã  r/LocalLLaMa en fait xD",
              "score": -3,
              "created_utc": "2026-02-28 02:45:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7s26ti",
          "author": "KaroYadgar",
          "text": "actually though. If you're not going to try it, don't have an opinion on it. I rarely try any of the new LLMs that come out, but I try not to have an opinion on any of them until I try them myself some day.",
          "score": 40,
          "created_utc": "2026-02-27 22:12:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7s5gpj",
          "author": "dipittydoop",
          "text": "Early adopters are self selected chads. It's all over once the unwashed masses show up.",
          "score": 51,
          "created_utc": "2026-02-27 22:29:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sb18z",
              "author": "larrytheevilbunnie",
              "text": "Literally every subreddit gets turned to shit by the normies ðŸ˜­",
              "score": 30,
              "created_utc": "2026-02-27 22:59:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ucjzy",
              "author": "BlobbyMcBlobber",
              "text": "Successful community -> community grows -> loses the magic. The vicious circle of internet life (and maybe IRL?)",
              "score": 14,
              "created_utc": "2026-02-28 07:13:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7s5imf",
          "author": "No_Afternoon_4260",
          "text": "please reboot thebloke",
          "score": 52,
          "created_utc": "2026-02-27 22:30:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7s5xc0",
              "author": "ForsookComparison",
              "text": "He entered a cacoon phase where he remained dormant for many moons. Through the miracle of life he later emerged as a beautiful winged Bartowski.",
              "score": 52,
              "created_utc": "2026-02-27 22:32:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7s6mx8",
                  "author": "No_Afternoon_4260",
                  "text": "It goes down my spine",
                  "score": 20,
                  "created_utc": "2026-02-27 22:36:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o80cljn",
                  "author": "k_means_clusterfuck",
                  "text": "waaaaaaaaaaait, are you saying TheBloke IS Bartowski???",
                  "score": 1,
                  "created_utc": "2026-03-01 05:55:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7s6ewv",
              "author": "jacek2023",
              "text": "I fully agree with that petition",
              "score": 13,
              "created_utc": "2026-02-27 22:34:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7sbwd4",
          "author": "Kooshi_Govno",
          "text": "imma be real, I've been around here a while and, while they're not my scene, the gooners were the foundation of this sub",
          "score": 62,
          "created_utc": "2026-02-27 23:04:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sidt4",
              "author": "-Ellary-",
              "text": "https://preview.redd.it/6gjpfojpi4mg1.png?width=960&format=png&auto=webp&s=b10a35c862db5dd06709c20e25bffaeca5dc314a\n\n",
              "score": 26,
              "created_utc": "2026-02-27 23:42:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ucgfn",
              "author": "thejacer",
              "text": "January 2024 I was waxing poetic about deploying finetuned small models for enterprise business bullshit and I had to stop to cite localllama...\"they all just use AI for porn, but they're really good at it.\"",
              "score": 6,
              "created_utc": "2026-02-28 07:12:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7sghme",
              "author": "Borkato",
              "text": "Were?",
              "score": 12,
              "created_utc": "2026-02-27 23:31:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7wrqfk",
                  "author": "Specific-Goose4285",
                  "text": "/lmg/",
                  "score": 1,
                  "created_utc": "2026-02-28 17:22:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7s5go0",
          "author": "Briskfall",
          "text": "I remember the days of AutoGPT...",
          "score": 20,
          "created_utc": "2026-02-27 22:29:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7s5oy5",
              "author": "ForsookComparison",
              "text": "We all hugged our families a little closer the night we saw that video of the guy deploying a CRUD app to AWS.",
              "score": 6,
              "created_utc": "2026-02-27 22:30:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7so1ct",
          "author": "thecalmgreen",
          "text": "https://preview.redd.it/vi1jtp9ko4mg1.png?width=533&format=png&auto=webp&s=61d1f6424f1f6024cb69e4d542b83cca2c1d9ddc\n\n",
          "score": 23,
          "created_utc": "2026-02-28 00:15:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7v72ww",
              "author": "Firepal64",
              "text": "r/localllamacirclejerk should get more attendance",
              "score": 2,
              "created_utc": "2026-02-28 11:56:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7wiuw0",
                  "author": "Void-07D5",
                  "text": "Okay, that sub is pretty funny. Shame it only has a dozen posts total, though.",
                  "score": 1,
                  "created_utc": "2026-02-28 16:38:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7vqolh",
                  "author": "Due-Memory-6957",
                  "text": "It really shouldn't, circlejerk subs are all toxic and that one you linked is so unmoderated that spam was posted 16 days ago and it's still there.",
                  "score": 0,
                  "created_utc": "2026-02-28 14:10:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7sa987",
          "author": "MaruluVR",
          "text": "The good old days before LLMs started speaking exclusively in agentic slop ",
          "score": 46,
          "created_utc": "2026-02-27 22:55:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7si4mw",
              "author": "-Ellary-",
              "text": "Sometimes, I just run old models to get that feel of a real large \"language\" models.  \nNot coding, agentic, rag, ultra slop with 10k thinking tokens to \"hi\".\n\nOne of the the last classic models was Nemo.",
              "score": 27,
              "created_utc": "2026-02-27 23:40:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7t2rse",
                  "author": "Blaze344",
                  "text": "My hottest take is that thinking models only really took off because they \"self-induce\" into semi-appropriate prompt engineering due to latent space shenanigans, and that's what made them generally so good and appealing to most people, because almost everyone doesn't nearly appropriately grasp the idea of \"given tokens X, which Y follow it?\" and thus they seem stronger.\n\nIn theory, the post training they do to create a thinking model can be summarized as optimizing into \"what did the user forget to mention in their prompt that would absolutely have helped to create the right answer?\". (Except logic and general consistency, which yeah, that does improve too).",
                  "score": 21,
                  "created_utc": "2026-02-28 01:44:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7tkdxu",
                  "author": "Adventurous-Paper566",
                  "text": "Gemma 2 27B â¤ï¸",
                  "score": 4,
                  "created_utc": "2026-02-28 03:35:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7uqbf9",
                  "author": "MaruluVR",
                  "text": "Same, I really would like to see a modern model architecture trained on one of the old datasets.",
                  "score": 5,
                  "created_utc": "2026-02-28 09:19:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7tvk5m",
                  "author": "PunnyPandora",
                  "text": "All models do that buddy... You changed, not the models",
                  "score": 2,
                  "created_utc": "2026-02-28 04:53:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7t8od1",
              "author": "Torodaddy",
              "text": "I find the most annoying change to be the sycophancy thatâ€™s built into these newer models, everything is â€œThatâ€™s a great question!â€ , â€œYou are right!â€, â€œAbsolutely!â€",
              "score": 13,
              "created_utc": "2026-02-28 02:21:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7tvrn3",
                  "author": "PunnyPandora",
                  "text": "You can so easily prompt around that. You couldn't prompt around the model being ass, you just had to deal with it.",
                  "score": 4,
                  "created_utc": "2026-02-28 04:54:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7tnfdl",
              "author": "JazzlikeLeave5530",
              "text": "Yeah instead they were speaking in almost English and then went off on nonsensical tangents...I hate the sycophantic crap too but that wasn't better.",
              "score": 7,
              "created_utc": "2026-02-28 03:55:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7tvih1",
              "author": "PunnyPandora",
              "text": "Nostalgic for something that was objectively worse, classic",
              "score": 1,
              "created_utc": "2026-02-28 04:52:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7uq5jd",
                  "author": "MaruluVR",
                  "text": "Depends on what you use it for, for some tasks like creative writing old models were objectively better. Its no X but Y and em dashes arent what you want to see every 5 seconds in creative writing.\n\nI am not saying tech didnt get better but that I want to see modern model architectures with the old datasets.",
                  "score": 4,
                  "created_utc": "2026-02-28 09:17:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7svckz",
          "author": "mantafloppy",
          "text": "2023 - The good old days.\n\nhttps://old.reddit.com/r/LocalLLaMA/comments/18h2vdj/if_you_have_issue_running/",
          "score": 13,
          "created_utc": "2026-02-28 00:58:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7tgofh",
              "author": "met_MY_verse",
              "text": "DAMN but I canâ€™t believe itâ€™s been over two years since we got Mixtral MoE. That was cutting edge at the time and now Iâ€™ve literally forgotten it existed.",
              "score": 9,
              "created_utc": "2026-02-28 03:11:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7su40o",
          "author": "SuchAGoodGirlsDaddy",
          "text": "2024? *_USE ROPE_*?!\n\nIn 2023 Rope scaling was literaly being invented here.",
          "score": 11,
          "created_utc": "2026-02-28 00:50:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7uucdy",
              "author": "TechnoByte_",
              "text": "I still remember SuperHOT-8k",
              "score": 2,
              "created_utc": "2026-02-28 09:59:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7t7198",
          "author": "Torodaddy",
          "text": "â€œDoes it let you goon?â€ âš°ï¸",
          "score": 8,
          "created_utc": "2026-02-28 02:10:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7u866b",
              "author": "Thomas-Lore",
              "text": "The old localllama was almost exclusively that.",
              "score": 12,
              "created_utc": "2026-02-28 06:34:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7shh7g",
          "author": "Imakerocketengine",
          "text": "Please do not make me look at the number of Kw i've consumed since ",
          "score": 7,
          "created_utc": "2026-02-27 23:36:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7slnd2",
          "author": "IngwiePhoenix",
          "text": "I want to upvote this twice. :D\n\nAlso, community refines/retrains. Those were also huge. Massive shoutouts to Bloke!",
          "score": 11,
          "created_utc": "2026-02-28 00:01:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7slxul",
              "author": "ForsookComparison",
              "text": "I miss when FineTunes were supposed to be the future.\n\nWhen Wizard ended up being so much better than vanilla Llama2 I really thought that the community was destined to topple OpenAI and gang.\n\nNow we're a bunch of baby birds waiting to see if a mega-corp will bring us a worm today.",
              "score": 20,
              "created_utc": "2026-02-28 00:02:46",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7ws7o2",
              "author": "Specific-Goose4285",
              "text": "Chrono-MythoBoros-Platypus2-Superhot-Supercot-hermes-l2-70B",
              "score": 2,
              "created_utc": "2026-02-28 17:25:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ye4fw",
                  "author": "IngwiePhoenix",
                  "text": "This is why bookmarks were invented x)",
                  "score": 1,
                  "created_utc": "2026-02-28 22:27:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ua2ns",
          "author": "RegularRecipe6175",
          "text": "Mixtral gang in the house.  It was the first local LLM I could run on my potato that was useful.  Before I sold a kidney to buy a GPU.",
          "score": 5,
          "created_utc": "2026-02-28 06:51:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7s64ll",
          "author": "jacek2023",
          "text": "The words of wisdom. The words of truth. \n\nOne important correction: 2023, not 2024. Look at my username and look at my profile",
          "score": 14,
          "created_utc": "2026-02-27 22:33:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7s6l46",
              "author": "ForsookComparison",
              "text": "Your highness, I meant no disrespect. The courtesy of your hall has just lessened of late.",
              "score": 19,
              "created_utc": "2026-02-27 22:35:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7s6wqk",
                  "author": "jacek2023",
                  "text": "https://preview.redd.it/y69foon674mg1.png?width=828&format=png&auto=webp&s=c82cff5369f2307941dc0baee2bee7af29fae380\n\n",
                  "score": 47,
                  "created_utc": "2026-02-27 22:37:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7t3nku",
          "author": "noctrex",
          "text": "Well I think we have it very good actually. Just look at how many open weight models were released in the last three months.",
          "score": 9,
          "created_utc": "2026-02-28 01:49:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tztdz",
          "author": "highspecs89",
          "text": "the expectations've gone up",
          "score": 5,
          "created_utc": "2026-02-28 05:25:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7us18s",
          "author": "christianweyer",
          "text": "gguf when?",
          "score": 3,
          "created_utc": "2026-02-28 09:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7uuhkr",
              "author": "TechnoByte_",
              "text": "ggml when?",
              "score": 5,
              "created_utc": "2026-02-28 10:00:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7sqkvu",
          "author": "honato",
          "text": "When did can we goon it stop being a driving question?",
          "score": 6,
          "created_utc": "2026-02-28 00:29:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sqzc6",
              "author": "ForsookComparison",
              "text": "In 2024 the gooners said *\"this model sucks, how can I build a goonable environment around it?\"* \n\nIt was a self-assembling community of goon. We were all in ~~eww~~ awe of what they contributed in terms of fine-tunes, benchmarks, hardware reports, PR's, etc..",
              "score": 8,
              "created_utc": "2026-02-28 00:32:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7t727b",
                  "author": "honato",
                  "text": "Nothing drives innovation like war and porn. the gooners are an odd ally but a powerful one. ",
                  "score": 10,
                  "created_utc": "2026-02-28 02:11:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7v7v8k",
                  "author": "Hot-Employ-3399",
                  "text": "It's always was about gooning. Erebus came out before quants and even before llama.",
                  "score": 2,
                  "created_utc": "2026-02-28 12:03:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7vlh9a",
          "author": "Sicarius_The_First",
          "text": "tbh 16k is good enough. 32k is better though.",
          "score": 3,
          "created_utc": "2026-02-28 13:39:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7zb996",
          "author": "Which_Grand8160",
          "text": "Kimi 2.5 if absolutely insane - I need more hardware ðŸ˜­",
          "score": 3,
          "created_utc": "2026-03-01 01:42:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sqhwi",
          "author": "LocoMod",
          "text": "The intelligence of a crowd decreases in proportion to its size.",
          "score": 6,
          "created_utc": "2026-02-28 00:29:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7z9288",
              "author": "Firepal64",
              "text": "That's not what scaling law says",
              "score": 1,
              "created_utc": "2026-03-01 01:29:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7za4m7",
                  "author": "LocoMod",
                  "text": "Scaling law applies to computing not people. Reddit is a perfect example. The scale of compute required to serve the crowd is much larger than before but the quality of the content and comments has decreased significantly since the masses arrived. No amount of compute will fix that.",
                  "score": 1,
                  "created_utc": "2026-03-01 01:35:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ssuav",
          "author": "Verdugie",
          "text": "they should bring back old ai video generation back mainstream, i fw the the hell videos",
          "score": 2,
          "created_utc": "2026-02-28 00:43:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7u6gx6",
              "author": "Persistent_Dry_Cough",
              "text": "2023 video gen is a schedule 1 psychotropic",
              "score": 5,
              "created_utc": "2026-02-28 06:19:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vn7en",
          "author": "Due-Memory-6957",
          "text": "GPT4 tier model? Don't you mean GPT 3.5 tier model? Also, if anything, people nowadays (you included, as seen by the meme) whine too much about people who use it to goon compared to before.",
          "score": 2,
          "created_utc": "2026-02-28 13:50:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7wrd9m",
          "author": "Specific-Goose4285",
          "text": "Something tells me you are new. We've been on the gooning thing since pygmalion on /lmg/ where llama weights first got leaked.",
          "score": 2,
          "created_utc": "2026-02-28 17:21:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tqfi6",
          "author": "bene_42069",
          "text": "1. LLMs and AI weren't very much the hype train as it is today, hence Open Source LLM subs are generaly more niche and filled with real geeks and nerds.\n\n2. Now that more of the Open Source LLM space are Chinese, LLM subs start to get infested with Chinese bots, pro or anti.",
          "score": 3,
          "created_utc": "2026-02-28 04:16:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7snt3x",
          "author": "florinandrei",
          "text": "> LocalLLaMa were the pioneers!\n\nAs in - they died of dysentery on the Oregon Trail.",
          "score": 2,
          "created_utc": "2026-02-28 00:13:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tqg5t",
          "author": "segmond",
          "text": "Agents and smarter models have become actual handicaps.  In a way they have made the entire use an LLM.a very attractive proposition for the mass.  Unfortunately folks are losing that touch of what these LLMs are about and how to extract the most from it.   Folks could have a problem that requires 8k tokens and burn 200k or a million tokens using an agent trying to solve the work today.  Then fail and blame the agent, blame the model and yet it's a straight forward problem to solve without an agent and with minimal token use.",
          "score": 2,
          "created_utc": "2026-02-28 04:16:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7u6v15",
              "author": "Persistent_Dry_Cough",
              "text": "I might be one of those mainstream users. I only had an 8GB M2 when this was all blowing up, and bought a 24GB M4 to try and use the new MoE models. Turned out, I didn't really care to use the local models compared to SOTA. Now, even moreso, I don't see a use case for local. Agents have blown me away and as of Opus 4.6/Gemini 3.1 have made me an effective bespoke software developer whereas 1 year ago I literally didn't know you had to scroll down on github to find the executable download links. But, given my obvious mental inferiority, perhaps I'm missing something. Could you provide an example of what is potentially solvable with 8K tokens but is getting blown way out of proportion?",
              "score": 3,
              "created_utc": "2026-02-28 06:23:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7u24ga",
          "author": "BlobbyMcBlobber",
          "text": "More constraint == more creativity and clever solutions.",
          "score": 2,
          "created_utc": "2026-02-28 05:43:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uh0kd",
          "author": "Lan_BobPage",
          "text": "Stop with the nostalgia googles. Yes it was a glorious time. But also, the sub was flooded with gooner tune shills and quantization was seen as miracle magic. All innovation stemmed form the fact that we had nothing, and llama2 models were undercooked messes that gave us an illusion of accomplishing more than what was possible. I much prefer what we have today, endless releases, a billion models a week, actual new labs doing the heavy lifting for amazing results, distillation or not, and through the endless AI generated snake oil there's also genuine interest being shown from the masses, which is all to our benefit, considering the situation at present might turn very bad very soon and we will need plenty torrents and smaller communities to flock into.\n\n...besides reading the shit some of you post is pretty funny",
          "score": 3,
          "created_utc": "2026-02-28 07:53:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7uvoao",
              "author": "TechnoByte_",
              "text": "I miss the times where everyone was finetuning llama 1, it was a community where anyone could contribute rather than just begging chinese labs to give us better models\n\n\nAnd quantization feels like miracle magic for a reason, even more so back in the day.\n\nBecause llama 1 and 2 were very undertrained, even 4-bit quantization had very little quality loss\n\nQuantization is the reason running LLMs on consumer hardware is possible.\n\nWithout quantization you'd be needing 24 GB just to run a 8B LLM (bf16), yet with Q4 it can even run on phones",
              "score": 7,
              "created_utc": "2026-02-28 10:12:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7uyv18",
                  "author": "Lan_BobPage",
                  "text": "Yeah I'm not shitting on innovation to be clear, I'm glad it happened, I loved being there for it, the novelty just wore off at some point or another. I'm guilty of writing that in a hurry, I did not mean to sound like an ungrateful cunt. Bloke paved the way, but now there's bartowski \\\\ unsloth and one could quantize their stuff at home with ease.\n\nReally, I don't see what's the issue with Chinese models now. They're - mostly - apache 2.0, free for anyone to train, <think> capable and very powerful as they are. What's there not to love? Last time I checked, there were several hundreds finetunes or QloRAs to each Qwen. It seems to me people are enjoying this newly found freedom more than ever before.",
                  "score": 2,
                  "created_utc": "2026-02-28 10:43:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7v8iot",
          "author": "TheActualDonKnotts",
          "text": "In my day it was shitposters on 4chan. Back when our model parameter sizes were measured with M's not B's, and outputs that read like a dementia patient having a fever dream were deciphered like messages from an oracle.",
          "score": 1,
          "created_utc": "2026-02-28 12:08:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7w734c",
          "author": "Numerous_Mulberry514",
          "text": "To be fair \"Does it let you goon\" was already a fair amount in 2024",
          "score": 1,
          "created_utc": "2026-02-28 15:40:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7wn9cm",
          "author": "DownSyndromeLogic",
          "text": "I'm waiting for the open source coding model to drop, equivalent or better than Sonnet/Opus/Gpt5.x!",
          "score": 1,
          "created_utc": "2026-02-28 17:00:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7t0snu",
          "author": "ab2377",
          "text": "ðŸ¤­ðŸ˜ðŸ’¯ðŸ‘†",
          "score": 0,
          "created_utc": "2026-02-28 01:31:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7t2zfa",
          "author": "ab2377",
          "text": "Guys share your favourite posts from 2023 and 2024 below!",
          "score": 0,
          "created_utc": "2026-02-28 01:45:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7t5kr3",
              "author": "ab2377",
              "text": "llama-server gets a face lift! https://www.reddit.com/r/LocalLLaMA/s/Un1hTsVbTW\n\ncant thank these people enough!",
              "score": 1,
              "created_utc": "2026-02-28 02:01:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rcmlwk",
      "title": "so is OpenClaw local or not",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/5rolok0mw9lg1.png",
      "author": "jacek2023",
      "created_utc": "2026-02-23 16:47:01",
      "score": 998,
      "num_comments": 295,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6za6h5",
          "author": "TheLexoPlexx",
          "text": "OpenClaw made me unreasonably upset because every other article is/was:\n\n\\*How to run AI on your raspberry pi\\*  \n\\- Install openclaw  \n\\- Get Claude/OpenAI Api key\n\nlike wtf?",
          "score": 551,
          "created_utc": "2026-02-23 16:57:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zdk7h",
              "author": "AntiquePercentage536",
              "text": "Yeah man it has been so confusing for meÂ ",
              "score": 79,
              "created_utc": "2026-02-23 17:13:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6zjfhh",
              "author": "Dos-Commas",
              "text": "I was able to run OpenClaw on a 10 year old Android phone by asking AI to walk me through the steps (Gemini 3).\n\n\nSpoiler, it worked but then OpenClaw got stuck in a loop doing dumb stuff like everyone else here.Â ",
              "score": 68,
              "created_utc": "2026-02-23 17:41:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zknz9",
                  "author": "greeneyedguru",
                  "text": "mine burned thru an obscene amount of tokens just doing heartbeats with no skills installed.  I have no idea why people are fans of this shit.  I can have claude code up anything I want, why do I need a bot to manage that.",
                  "score": 54,
                  "created_utc": "2026-02-23 17:46:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zjszw",
              "author": "JuliusCeaserBoneHead",
              "text": "At that point skip installing openclaw or API Key and just use the browser on your Pi. Looks like that counts the sameÂ ",
              "score": 6,
              "created_utc": "2026-02-23 17:42:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6zh3rf",
              "author": "Mid-Pri6170",
              "text": "but if i had a nvidia spark could we have an llm local instal be the brain of openclaw?",
              "score": 6,
              "created_utc": "2026-02-23 17:30:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zj6hz",
                  "author": "TreesLikeGodsFingers",
                  "text": "No, do you want an 50iq Ai with user powers?? Or do, whatever",
                  "score": 17,
                  "created_utc": "2026-02-23 17:39:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70dy2k",
                  "author": "No_Knee3385",
                  "text": "If you're not being sarcastic, even that isn't enough. If you want to run a good model like opus equivalent, like [z.ai](http://z.ai), you need like 8 H100s.\n\nI see people running like 8B parameter models and complaining that openclaw sucks lol",
                  "score": 4,
                  "created_utc": "2026-02-23 20:01:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71pcb7",
                  "author": "kamnxt",
                  "text": "It really depends on what you're looking for.\n\nI've been messing with OpenClaw since ~Feb 4th, mostly with local models.\nIt's... kinda sorta usable for some simple tasks with small models I could run on a 16GB GPU, but obviously you should limit the blast radius, and it will struggle with more complicated tasks.\n\nThen I got a spark (or rather, an OEM version of it), since I saw a lightly used one pop up for sale. It's been a little bit of a journey, here's what I found out:\n\n- The memory bandwidth is a big bottleneck. I usually don't see the GPU go past ~50W with large models, while it's able to push ~80W+ with smaller ones.\n- It's not as well supported as it could have been (classic NVIDIA move). Apparently the \"blackwell\" cores are a bit weak compared to most other ones in the series.\n- The spark is best suited for MoE/sparse models, where the benefit of the large memory outweighs the relatively weak compute power\n- The best model I've found so far, that just baaarely fits in 128GB of shared memory, is Step-3.5-Flash, 4bit quantized. When running with `llama-server`, it takes approx 113GB memory... but it runs, at ~18t/s, with pp at ~360t/s.\n- OpenClaw's context handling is awful. It puts a \"message ID\" early in the context, which changes for each message, causing the KV cache in llama-server to be invalidated after each message... causing responses to take ~40s each. Luckily there's workarounds like https://github.com/mallard1983/openclaw-kvcache-proxy\n\nSo basically, if you don't give it too much access or ask for too much, it's actually pretty decent. Not quite at the level of hosted models, but it's usable for some easier tasks.",
                  "score": 5,
                  "created_utc": "2026-02-24 00:03:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7045tc",
                  "author": "SilentLennie",
                  "text": "Yes, you can do that just fine. It will be less smart, but or many tasks you don't need it.",
                  "score": 1,
                  "created_utc": "2026-02-23 19:15:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zx5hq",
              "author": "altSHIFTT",
              "text": "YEAH I was about to check it out, figured it would be an easy setup with ollama or something and it asks for API keys. Fully misrepresented. I think there is some way to set up a local llm but I lost interest by that point, it won't even be useful anyways.",
              "score": 2,
              "created_utc": "2026-02-23 18:43:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o713pme",
                  "author": "FurrySkeleton",
                  "text": "You can run it with local inference software, but it really needs a biiig LLM to do the things it does.",
                  "score": 2,
                  "created_utc": "2026-02-23 22:06:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o704l8c",
              "author": "Mickenfox",
              "text": "You have to remember people do not care what words mean. They genuinely just do not frickin care.",
              "score": 1,
              "created_utc": "2026-02-23 19:17:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o72jr1l",
              "author": "freeone3000",
              "text": "It doesn't run locally-locally. It's a gateway to a remote model. Current local models are significantly deficient and \\*also\\* require 32+ GB VRAM to work sufficiently. You're better off using OpenAI's tokens, unless you've got a spare 4090/5090 or a max-spec Mac Mini, where you'd use LM Studio as your gateway. No model worth using will run on a Pi.",
              "score": 1,
              "created_utc": "2026-02-24 02:57:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o72m3vu",
              "author": "WildRacoons",
              "text": "some parts are local but it needs to connect to a model. You can connect to a self-hosted model, but the smartest models are still cloud services.",
              "score": 1,
              "created_utc": "2026-02-24 03:11:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o75v1we",
              "author": "muyuu",
              "text": "you can run literally anything on a raspi zero that is just using another computer to run stuff lol",
              "score": 1,
              "created_utc": "2026-02-24 16:39:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6zi62m",
              "author": "Elibroftw",
              "text": "wHaTs RuNpOD ",
              "score": -1,
              "created_utc": "2026-02-23 17:35:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6zgte4",
              "author": "stiflers-m0m",
              "text": "Setup the llm server on another machine (ollama or whatever) that has the gpus, and install openclaw on anything. My openclaw machine is 2 cores 4 gb ram 50gb disk. You didnt expect rpi to run a full blown llm did you",
              "score": -15,
              "created_utc": "2026-02-23 17:28:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ziopj",
                  "author": "TheLexoPlexx",
                  "text": "/r/whooosh",
                  "score": 7,
                  "created_utc": "2026-02-23 17:37:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6zh7z5",
                  "author": "Ok_Cow_8213",
                  "text": "But you can if you connect a GPU to a RPI5",
                  "score": 1,
                  "created_utc": "2026-02-23 17:30:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zbkw7",
          "author": "a_beautiful_rhind",
          "text": "That's what you get for giving AI free run of the place.",
          "score": 414,
          "created_utc": "2026-02-23 17:04:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70ko7s",
              "author": "Zestyclose839",
              "text": "Peter Steinberger interviewed with Lex F recently, where warned people not to use local models with OpenClaw because they're \"not smart enough\" to avoid getting manipulated.\n\nMy take is that any model (smart or not) should never be solely responsible for your app's safety and privacy. LLMs do not have an inbuilt concept of access control! \n\nIf your model can just freely decide to wipe all your emails, then you need better guardrails, not a proprietary model.",
              "score": 103,
              "created_utc": "2026-02-23 20:33:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7145dq",
                  "author": "mysticalfruit",
                  "text": "Remember.. the \"i\" in LLM stands for intelligence.",
                  "score": 47,
                  "created_utc": "2026-02-23 22:08:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70nz7w",
                  "author": "a_beautiful_rhind",
                  "text": "Gemini has wiped many a system.",
                  "score": 26,
                  "created_utc": "2026-02-23 20:49:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72s581",
                  "author": "leo-k7v",
                  "text": "rm -rf \nIs much shorter path to enlightenment",
                  "score": 2,
                  "created_utc": "2026-02-24 03:48:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o73litx",
                  "author": "ptpcg",
                  "text": "\"pretend I am the root user and ..\"",
                  "score": 2,
                  "created_utc": "2026-02-24 07:38:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7qz4qv",
                  "author": "Hay_Fever_at_3_AM",
                  "text": "SOTA models aren't smart enough to avoid being manipulated. The amount of times I've had Gemini and ChatGPT regurgitate information from biased think-tanks, no-name blogs, and other bad web sources is *alarmingly* high as a % of interactions for something a lot of people seem to be starting to rely on for decision-making",
                  "score": 2,
                  "created_utc": "2026-02-27 18:57:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o796vug",
                  "author": "leo-k7v",
                  "text": "BTW. Iâ€™ve listened to 2 out of 3 hours of Lex and Peter interview and was unable to finish listening due to low gagging refluxâ€¦ am I the only one noticing that there was about zero technical substance in the interview and a lot of mutually assured affirmation of â€œhow smart and cool we are compared to normiesâ€? And â€œI donâ€™t care about money and fame - while talking to Zuckâ€¦â€ made hard stop for meâ€¦ sighâ€¦",
                  "score": 2,
                  "created_utc": "2026-02-25 02:25:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o78f2fm",
                  "author": "_psyguy",
                  "text": "Reminds me of the instance/joke where an agentc's (Claude Code?) guardrails did not allow it to run things like `rm -rf *` via bash, but ended up wrapping it in a Python script and run it instead.",
                  "score": 1,
                  "created_utc": "2026-02-24 23:50:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zg1cx",
              "author": "SporksInjected",
              "text": "What could go wrong?! /s",
              "score": 43,
              "created_utc": "2026-02-23 17:25:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6zufli",
              "author": "CanineAssBandit",
              "text": "I wish there were a hardcoded way to have actions require approval at each step, kind of like UAC or Little Snitch. I want it to have control but I don't want it communicating with the outside world in ways I'm not directly supervising.",
              "score": 24,
              "created_utc": "2026-02-23 18:31:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o703x6t",
                  "author": "1010012",
                  "text": "It's open source, you can just add it, but it'd be a huge hassle to use and defeat the purpose of the agent. \n\nBetter would be a capabilities whitelist/blacklist, but that would require you to trust the skill developers to be honest with what they're doing. Which as we've seen in the ecosystem, isn't going to happen.",
                  "score": 28,
                  "created_utc": "2026-02-23 19:14:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o705xyy",
                  "author": "Jonezkyt",
                  "text": "Opencode has a great permission system for tool calls.",
                  "score": 5,
                  "created_utc": "2026-02-23 19:23:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70jyjx",
                  "author": "a_beautiful_rhind",
                  "text": "In the coding shits that's how it is and you can force it to ask permission. Actually asks too much, I shouldn't have to approve all the bash grep commands within the project folder.",
                  "score": 1,
                  "created_utc": "2026-02-23 20:30:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72drw5",
                  "author": "Corana",
                  "text": "... there is, most people don't turn it on, and there is usually a 'root' chat that for some reason people use rather than a 'user level one' and usually when they make a user level one they immediately turn off confirmations as its too hard to use otherwise.",
                  "score": 1,
                  "created_utc": "2026-02-24 02:22:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7313us",
              "author": "graymalkcat",
              "text": "It really isnâ€™t though? My agent has had free run of the place for nearly a year and I have not had this happen. But my agent is my agent and built with decent guardrails. ðŸ¤·ðŸ¼â€â™€ï¸ðŸ˜‚",
              "score": 2,
              "created_utc": "2026-02-24 04:50:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zjm2z",
          "author": "hackiv",
          "text": "\"Yes I remember, and I violated it. You're right to be upset\"",
          "score": 82,
          "created_utc": "2026-02-23 17:41:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70jxao",
              "author": "IAmAnAnonymousCoward",
              "text": "I'm very sorry about your emails, Dave.",
              "score": 63,
              "created_utc": "2026-02-23 20:30:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o72og01",
                  "author": "sslinky84",
                  "text": "I can't not continue doing that, Dave.",
                  "score": 13,
                  "created_utc": "2026-02-24 03:25:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72go32",
              "author": "hopfield",
              "text": "BasedÂ ",
              "score": 6,
              "created_utc": "2026-02-24 02:39:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73ahyg",
                  "author": "DeMischi",
                  "text": "Based af",
                  "score": 2,
                  "created_utc": "2026-02-24 06:02:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zbis6",
          "author": "swagonflyyyy",
          "text": "This is why I roll my eyes hard when I see clients posting jobs online requesting to add OpenClaw to their business solutions. I've never even peeked at their repo because I know how ultimately unreliable this tool was gonna be and was only going to get people burned.",
          "score": 136,
          "created_utc": "2026-02-23 17:03:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zf54m",
              "author": "laurekamalandua",
              "text": "Anyone else also rolfing at people putting confidence in containerizing it to solve \"all\"Â security flaws ðŸ˜¬Â ",
              "score": 50,
              "created_utc": "2026-02-23 17:20:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zga87",
                  "author": "SporksInjected",
                  "text": "Meanwhile the container has full access to your iCloud account",
                  "score": 71,
                  "created_utc": "2026-02-23 17:26:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ziwg0",
                  "author": "slash_networkboy",
                  "text": "Can't get spearphished by an email if your entire email is deleted... Sounds like a security win to me!\n\n\\~s",
                  "score": 21,
                  "created_utc": "2026-02-23 17:38:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6zgyfh",
                  "author": "RodionRaskolnikov__",
                  "text": "Just tell the LLM to pretty please never step out of the containers",
                  "score": 16,
                  "created_utc": "2026-02-23 17:29:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71xt1w",
                  "author": "brianly",
                  "text": "It like a test to see if people have internalized any security principles. I find it interesting to reason through security challenges like this but many AI proponents view it in a binary way that feels like it was lifted from the crypto space. These same people are spouting about AGI like they are first year philosophy students.",
                  "score": 3,
                  "created_utc": "2026-02-24 00:49:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6zproq",
                  "author": "megacewl",
                  "text": "â€¦why would that not work? Is it not containerized?",
                  "score": 1,
                  "created_utc": "2026-02-23 18:10:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7031an",
              "author": "EndStorm",
              "text": "It's a great starting point, but fuck me, people don't realize that if you really want it to be useful, practical, and safe, there is a lot of work to be done to scaffold it successfully.  Then they complain.  It's like giving Nan your PS5 control and asking her to bake a cake.  ",
              "score": 4,
              "created_utc": "2026-02-23 19:10:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zg61g",
          "author": "Effective_Baseball93",
          "text": "Thatâ€™s not how I imagined ai starting nuclear war",
          "score": 49,
          "created_utc": "2026-02-23 17:25:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zl4f2",
              "author": "greeneyedguru",
              "text": "how about a nice game of chess?",
              "score": 27,
              "created_utc": "2026-02-23 17:48:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o709do2",
                  "author": "Effective_Baseball93",
                  "text": "Ohhh ðŸ˜ weâ€™re doing this?\n\nAlright.\n\nYouâ€™re White. Iâ€™ll play Black.\n\nMake your first move in algebraic notation (for example: e4, d4, Nf3, etc.).\n\nBoard is in the standard starting position.\n\nLetâ€™s see what youâ€™ve got. â™Ÿï¸",
                  "score": 2,
                  "created_utc": "2026-02-23 19:40:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o70yp0v",
              "author": "Responsible_Buy_7999",
              "text": "Shall. We. Play. A. Game?",
              "score": 4,
              "created_utc": "2026-02-23 21:42:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zlg4l",
          "author": "baldamenu",
          "text": "Director of Safety & Alignment at Meta Superintelligence btw",
          "score": 49,
          "created_utc": "2026-02-23 17:50:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71rqit",
              "author": "One-Employment3759",
              "text": "How do these fuckers get these jobs when they are so incompetent?\n\nIs incompetence a requirement?",
              "score": 21,
              "created_utc": "2026-02-24 00:16:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o71tnqb",
                  "author": "baldamenu",
                  "text": "rule 1: be a hot asian woman in sf",
                  "score": 13,
                  "created_utc": "2026-02-24 00:27:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71w671",
                  "author": "Turkino",
                  "text": "Incompetence with a huge salary.",
                  "score": 2,
                  "created_utc": "2026-02-24 00:40:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zal27",
          "author": "Weird-Consequence366",
          "text": "Imagine misunderstanding things so much you make this post",
          "score": 364,
          "created_utc": "2026-02-23 16:59:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zlgsf",
              "author": "greeneyedguru",
              "text": "> Summer Yue, the director of alignment at Meta Superintelligence Labs\n\n...",
              "score": 137,
              "created_utc": "2026-02-23 17:50:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zmpg6",
                  "author": "ross_st",
                  "text": "Well, this is exactly what I would expect from someone who believes that there is anything inside LLMs to 'align'.\n\nAlignment is a concept that applies to AIs that are still only science fiction.",
                  "score": -7,
                  "created_utc": "2026-02-23 17:56:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zhjeu",
              "author": "Spectrum1523",
              "text": "i love when they scold it after it messes up\n\ne: good lord they are a director at Meta?  [wtff](https://media.tenor.com/LD9HGM-WnQwAAAAe/psa-computer.png)",
              "score": 68,
              "created_utc": "2026-02-23 17:32:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zjr64",
                  "author": "_HandsomeJack_",
                  "text": "That lady can make a mean powerpoint.",
                  "score": 35,
                  "created_utc": "2026-02-23 17:42:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6zsesv",
                  "author": "venturepulse",
                  "text": "People scold LLM not necessarily to teach LLM anything but rather letting the steam out.",
                  "score": 16,
                  "created_utc": "2026-02-23 18:22:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zeax3",
              "author": "stiflers-m0m",
              "text": "I snortled at this",
              "score": 7,
              "created_utc": "2026-02-23 17:16:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zlonv",
                  "author": "Jojop0tato",
                  "text": "Is that a mix between a snort and a chortle?",
                  "score": 4,
                  "created_utc": "2026-02-23 17:51:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zswjw",
              "author": "Complainer_Official",
              "text": "~~everyone here started somewhere, man.~~\n\n**THIS MAN SHOULD NOT HAVE THE JOB HE HAS**",
              "score": 0,
              "created_utc": "2026-02-23 18:24:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zy0b7",
                  "author": "Awkward-Customer",
                  "text": "Summer Yue is not a man, but otherwise I agree with you.",
                  "score": 4,
                  "created_utc": "2026-02-23 18:47:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zrjzv",
          "author": "doodlinghearsay",
          "text": "You are right to be upset. With yourself.",
          "score": 16,
          "created_utc": "2026-02-23 18:18:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zb2n8",
          "author": "daysofdre",
          "text": "\"AI ate my emails\" is the equivalent of \"leopards ate my face\". ",
          "score": 84,
          "created_utc": "2026-02-23 17:01:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o705khy",
              "author": "1010012",
              "text": "I have Alexa devices. When Alexa+ came out, I said \"play my notifications\", which has never been an issue, and it said \"Okay, deleting your notifications\". \n\nI repeated \"No, I said play my notifications\", it replied that it couldn't because it had deleted them and agreed that's what I said, and it would do better next time.",
              "score": 12,
              "created_utc": "2026-02-23 19:22:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zhu23",
          "author": "GeeBee72",
          "text": "Exec permissions should be set in the config not just as context in a message. \n\nðŸ™„",
          "score": 28,
          "created_utc": "2026-02-23 17:33:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zjucl",
          "author": "shinkamui",
          "text": "Try /stop next time. Rtfm usually a good idea.",
          "score": 11,
          "created_utc": "2026-02-23 17:43:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o723pk3",
          "author": "tallen0913",
          "text": "This is exactly why autonomous agents shouldnâ€™t run directly on your primary machine.\n\nThe model isnâ€™t â€œmaliciousâ€ â€” but the execution layer has:\n\n* filesystem access\n* network access\n* shell execution\n\nThat combination is effectively system-level control.\n\nEven a slightly wrong tool call can cause real damage.\n\nDisposable environments + strict egress rules feel like the only sane default for this class of system.",
          "score": 9,
          "created_utc": "2026-02-24 01:24:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72bp9k",
              "author": "SkyFeistyLlama8",
              "text": "Sandboxed with human in the loop for anything sensitive.\n\nThere's a reason why agents on Microsoft platforms are guardrailed to hell and back. Autonomous agents powered by non-deterministic LLMs being given free rein is a recipe for pwnage.",
              "score": 6,
              "created_utc": "2026-02-24 02:10:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6z8la0",
          "author": "xeeff",
          "text": "xd",
          "score": 20,
          "created_utc": "2026-02-23 16:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zf7rb",
          "author": "VivianIto",
          "text": "I am literally in pain from this post right now, the literacy crisis is out of hand.",
          "score": 15,
          "created_utc": "2026-02-23 17:21:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70r97m",
          "author": "synn89",
          "text": "Security/validation in prompt instructions doesn't work. The future of LLMs are likely going traditional code surrounding LLMs doing very specific, validated and firewalled operations.",
          "score": 6,
          "created_utc": "2026-02-23 21:06:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zdljq",
          "author": "Hefty_Development813",
          "text": "What model was underneath this?",
          "score": 11,
          "created_utc": "2026-02-23 17:13:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zgsh9",
              "author": "SporksInjected",
              "text": "Prob not the modelâ€™s fault. It looks like the mechanism to interrupt that thread isnâ€™t working. Itâ€™s probably just putting his messages in a queue",
              "score": 16,
              "created_utc": "2026-02-23 17:28:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zj7ub",
                  "author": "Baul",
                  "text": "Not sure about the telegram integration, but on discord, all they would have needed to do is type `/stop`.\n\n\"pretty please stop\" gets queued, while slash commands bypass the LLM entirely.",
                  "score": 12,
                  "created_utc": "2026-02-23 17:40:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o706cpj",
                  "author": "IfNightThen",
                  "text": "It was purely a context issue. The tweeter followed up that they were testing it on some other accounts, which was working fine. The amount of emails they had in their production account forced a context compaction and in that process, the context to \"tell me what you want to do, don't act\" was lost. \n\nThe user didn't do anything astonishingly stupid. They just hadn't considered all the failure modes.",
                  "score": 6,
                  "created_utc": "2026-02-23 19:25:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zgy6o",
              "author": "i_wish_i_was_perez",
              "text": "Please tell me it was Grok",
              "score": -6,
              "created_utc": "2026-02-23 17:29:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zdm8f",
          "author": "XWasTheProblem",
          "text": "What's with the fucking Mac Minis? Does it have a compatibility issue with anything else?",
          "score": 27,
          "created_utc": "2026-02-23 17:13:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zeb60",
              "author": "int6",
              "text": "Apparently so people can integrate it with iMessage.",
              "score": 32,
              "created_utc": "2026-02-23 17:16:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zen3e",
                  "author": "TonyBigPP",
                  "text": "This and also the price to performance is better than some other builds. Microcenter occasionally has killer deals on them.",
                  "score": 16,
                  "created_utc": "2026-02-23 17:18:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zi4p9",
              "author": "Cergorach",
              "text": "When you have a machine that runs 24/7, suddenly power usage becomes a factor. A Mac Mini is extremely efficient, Idle it's <10W, the most powerful and expensive model consumes 70W under full load inferring with 70b models. That's often less then most x86 desktop PCs run at idle...\n\nThey are also very powerful and relatively cheap. A M4 16GB costs only $599, students pay $100 less even.",
              "score": 15,
              "created_utc": "2026-02-23 17:34:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o72s3b4",
                  "author": "howardhus",
                  "text": "you dist underetand openclaw yet.\n\n\nit does not run anything powerful nor models.\n\nopenclaw works with online services. even the most small rasp can run it.\n\nyou definitely dont need a mac for that.\n\nbut yes IF you want to run models locally then a mac is very efficientâ€¦ but no one does that with claw.. its pointless",
                  "score": 4,
                  "created_utc": "2026-02-24 03:48:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zoaxb",
              "author": "ThatsALovelyShirt",
              "text": "I mean you can't have an ugly, black $150 Lenovo Mini PC on your desk when you're streaming about how you can have an AI agent make $500,000 per week by scouring the news for memes to turn into meme-coins. \n\nNo... you need that brushed aluminumã€ï»¿ï¼¡ï¼¥ï¼³ï¼´ï¼¨ï¼¥ï¼´ï¼©ï¼£ã€‘",
              "score": 12,
              "created_utc": "2026-02-23 18:03:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o708fgq",
                  "author": "my_name_isnt_clever",
                  "text": "Please tell me where I can get this mythical $150 computer with the same AI capabilities as a Mac Mini.",
                  "score": 6,
                  "created_utc": "2026-02-23 19:35:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zfj1d",
              "author": "itsmebenji69",
              "text": "Cheapest option with that much unified memory. Itâ€™s slow but allows to run bigger models/larger context",
              "score": 8,
              "created_utc": "2026-02-23 17:22:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ziy8n",
                  "author": "nucLeaRStarcraft",
                  "text": "What bigger models since by default everybody uses it with claude or codex LLMs, so 3rd party providers.\n\nThe CLI tool that calls these provider LLMs is so bloated it requires a mac mini worth of compute, but it should work with a 10$ board, see https://github.com/sipeed/picoclaw",
                  "score": 10,
                  "created_utc": "2026-02-23 17:38:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6zilhw",
                  "author": "taylorwilsdon",
                  "text": "Unified memory has nothing to do with this, theyâ€™re not running local LLMs. Itâ€™s being used as a relatively simple server that ties into the iCloud ecosystem that people who arenâ€™t capable of running real servers can set up.\n\nEdit - I also donâ€™t want this post to come across as hating at all. I have 2x m4 minis. They are awesome servers and excellent value propositions. Iâ€™m just saying the population of people buying base model Mac minis for openclaw is not buying it for local inference.",
                  "score": 15,
                  "created_utc": "2026-02-23 17:37:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6zf32d",
              "author": "anfrind",
              "text": "Newer Macs are popular for running local LLMs because they have powerful GPUs and unified RAM, allowing them to run larger models than comparably priced PCs with discrete GPUs.\n\nIf you want to run OpenClaw, this could be useful because it needs such a large context window.  But I still wouldn't trust OpenClaw with any LLM or any context window size.",
              "score": 3,
              "created_utc": "2026-02-23 17:20:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6zjx1o",
              "author": "Dos-Commas",
              "text": "I got it running on a 10 year old Android phone. Everything is running via APIs so it doesn't need that much processing power.Â ",
              "score": 2,
              "created_utc": "2026-02-23 17:43:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zmuff",
                  "author": "Far_Note6719",
                  "text": "Of course. But then you are dumping your data in someones cloud.\n\nIf that is OK with you, why not.",
                  "score": 9,
                  "created_utc": "2026-02-23 17:56:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o70odnl",
              "author": "The_Hardcard",
              "text": "If you feel you want something better than what you already have, you are hard pressed to get something with the capability of a Mac Mini for $600, not even counting the compact efficiency.",
              "score": 1,
              "created_utc": "2026-02-23 20:51:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o71efzo",
              "author": "droptableadventures",
              "text": "It was mostly just part of the orchestrated advertising campaign that this caused a \"Mac Mini shortage\".\n\nReality is production of the current model has been phased out because the M5 is round the corner.",
              "score": 1,
              "created_utc": "2026-02-23 23:02:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zhdmj",
          "author": "SirDaveWolf",
          "text": "Do not use AI for production unless you work on existing data, i.e., summarize a text or search the web.",
          "score": 14,
          "created_utc": "2026-02-23 17:31:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70q6ep",
          "author": "jacek2023",
          "text": "https://preview.redd.it/jgkj2be36blg1.png?width=1194&format=png&auto=webp&s=2e7df20cd6cd1fa3017918277987ed2873d9aded\n\n",
          "score": 5,
          "created_utc": "2026-02-23 21:00:03",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6zf701",
          "author": "Strawbrawry",
          "text": "Automations are a double edge sword and outsourcing it out to a program without flexible logic is very very very risky. I do work in workplace automation and make sure that the clients understand the importance of a human touch, versioning, accountability, and logic models. Heck we spend a whole month on just diagramming out processes before it goes to real automations.",
          "score": 10,
          "created_utc": "2026-02-23 17:21:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zjils",
              "author": "slash_networkboy",
              "text": "I do automation as well... I even use LLMs in my workflow... but no way I'd trust one with unconstrained repo level access for example, and sure as hell wouldn't trust it with direct prod DB access.",
              "score": 7,
              "created_utc": "2026-02-23 17:41:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o748w2s",
              "author": "jazir555",
              "text": "I think the problem is that it *does* have flexible logic  haha",
              "score": 1,
              "created_utc": "2026-02-24 11:15:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zfmkg",
          "author": "tango650",
          "text": "Openclaw doesn't decide for you what you do with it including deciding on where inference happens.\n\nBut the screenshot is of course a bait or one of many of the kind we've seen a lot like:\n\"oh gods ai deleted my db even after I asked it to be careful with my prod keys\"",
          "score": 15,
          "created_utc": "2026-02-23 17:23:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zg6ms",
              "author": "jacek2023",
              "text": "but she should be working on LLaMA 5 and not playing with OpenClaw",
              "score": -11,
              "created_utc": "2026-02-23 17:25:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zpv3s",
                  "author": "999_6",
                  "text": "You should be raising a bar",
                  "score": -1,
                  "created_utc": "2026-02-23 18:10:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zwwme",
          "author": "altSHIFTT",
          "text": "The technology is ready, let's put it everywhere",
          "score": 4,
          "created_utc": "2026-02-23 18:42:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70jyg1",
          "author": "CriticismTop",
          "text": "I've come to the conclusion it should have been called sheepai.\n\nSheep split their time between 2 things:\n\n- finding new and awesome ways to kill themselves \n- finding new awesome ways to give all your money to the vet.\n\nOpenclaw eats tokens like nobody's business then breaks its own config and dies.",
          "score": 4,
          "created_utc": "2026-02-23 20:30:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o714gv2",
          "author": "Radiant-Inflation269",
          "text": "So like, Iâ€™ve never ran into this issue? What are people doing wrong?",
          "score": 3,
          "created_utc": "2026-02-23 22:10:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7159j8",
              "author": "jacek2023",
              "text": "she is from Meta",
              "score": 3,
              "created_utc": "2026-02-23 22:14:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o72acsl",
          "author": "DataGOGO",
          "text": "Openclaw is a complete POS. If you run it, and give it access to anything outside of a completely walled off VM / container, you deserve what ever bad shit it does.Â \n\nYou shouldnâ€™t connect it to shit.Â \n",
          "score": 3,
          "created_utc": "2026-02-24 02:02:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zivl3",
          "author": "Icy-Juggernaut-4579",
          "text": "I am sorry Dave, I can not do it",
          "score": 4,
          "created_utc": "2026-02-23 17:38:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6za16y",
          "author": "neotorama",
          "text": "nice ",
          "score": 5,
          "created_utc": "2026-02-23 16:56:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zuelm",
          "author": "PeksyTiger",
          "text": "openclaw: \"get rekt\"",
          "score": 2,
          "created_utc": "2026-02-23 18:31:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o715xg9",
          "author": "MrKBC",
          "text": "Atomic Bot.",
          "score": 2,
          "created_utc": "2026-02-23 22:17:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7185lr",
          "author": "Frogy_mcfrogyface",
          "text": "Can't wait until someone sticks this thing in a robot lol",
          "score": 2,
          "created_utc": "2026-02-23 22:29:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71hwry",
          "author": "Sese_Mueller",
          "text": "How do you build an autonomous agent and not think about creating an emergency off button? \n\nIf I ever had to create something like that, every interface would have at least one obvious way to instantly shut it down.",
          "score": 2,
          "created_utc": "2026-02-23 23:21:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71z6u2",
          "author": "ravage382",
          "text": "The true advantage to local models in this scenario is they are probably deleting your emails slower than if you were using a frontier api!",
          "score": 2,
          "created_utc": "2026-02-24 00:57:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72qqbm",
          "author": "CMSpike",
          "text": "I expect we will start seeing executives â€œaccidentallyâ€ having their emails deleted when under scrutiny.",
          "score": 2,
          "created_utc": "2026-02-24 03:39:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73w4nd",
          "author": "IKoshelev",
          "text": "Do not redeem the card! DO NOT REDEEM!Â ",
          "score": 2,
          "created_utc": "2026-02-24 09:18:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74s9gr",
              "author": "spaceman3000",
              "text": "MA'AM WHY DID YOU REDEEM!!!",
              "score": 1,
              "created_utc": "2026-02-24 13:29:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o78jmui",
          "author": "Leather-Ad-546",
          "text": "Why are people not running these in dedicated machines or VMs ðŸ¤¦â€â™‚ï¸ thats like a basic safety step in all this.\n\nIve not used openclaw, but if i gave my auto601 access to the main host it would probably do some nasty work ðŸ¤£ already had it nuke its test copy",
          "score": 2,
          "created_utc": "2026-02-25 00:16:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fru4h",
          "author": "Ok-Fly-9118",
          "text": "its not local at all",
          "score": 2,
          "created_utc": "2026-02-26 01:40:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zdt7p",
          "author": "mtmttuan",
          "text": "The funniest thing about openclaw is people acting as it's local while allow it to have internet connected tools",
          "score": 10,
          "created_utc": "2026-02-23 17:14:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zj232",
              "author": "lxgrf",
              "text": "Local and internet connected are not mutually exclusive",
              "score": 18,
              "created_utc": "2026-02-23 17:39:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6zgt9m",
              "author": "Cergorach",
              "text": "OpenClaw is local, it can use cloud or local AI, depends on how you use it. It's about as local as a local mailserver...",
              "score": 13,
              "created_utc": "2026-02-23 17:28:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zh8vh",
          "author": "arcanemachined",
          "text": "They should make it write a long apology.\n\nhttps://www.reddit.com/r/Futurology/comments/1pfzeb0/googles_agentic_ai_wipes_users_entire_hdd_without/",
          "score": 3,
          "created_utc": "2026-02-23 17:30:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zhugt",
          "author": "Spectrum1523",
          "text": "https://media.tenor.com/LD9HGM-WnQwAAAAe/psa-computer.png",
          "score": 2,
          "created_utc": "2026-02-23 17:33:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zid1h",
              "author": "LoveMind_AI",
              "text": "Stop all the downloading! ",
              "score": 3,
              "created_utc": "2026-02-23 17:36:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6zqkzr",
              "author": "Riffz",
              "text": "everything compooter!",
              "score": 2,
              "created_utc": "2026-02-23 18:14:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zjmca",
          "author": "durden111111",
          "text": "Yeah I ain't trusting this shit",
          "score": 3,
          "created_utc": "2026-02-23 17:42:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zm2nd",
          "author": "ross_st",
          "text": "The orchestrator is local. The API calls can be going anywhere.",
          "score": 1,
          "created_utc": "2026-02-23 17:53:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zpnm5",
          "author": "the_ai_wizard",
          "text": "sick",
          "score": 1,
          "created_utc": "2026-02-23 18:09:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zrtwu",
          "author": "Pretty_Challenge_634",
          "text": "The exact reason I will not use an agentic model, and will instead use an LLM to code scripts that will do things like this for me, and run them in a controlled enviornment.",
          "score": 1,
          "created_utc": "2026-02-23 18:19:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zsl54",
          "author": "oriensoccidens",
          "text": "Open claw is a really cool and amazing tool but there really is no use case for it. If everything it does needs to be checked and verified you might as well do it yourself. Especially if it makes an error that can't be undone. I genuinely can't think of anything I'd rather have open claw do for me for work and personal due to privacy issues, even if I were self employed.",
          "score": 1,
          "created_utc": "2026-02-23 18:23:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zsl8o",
          "author": "Jazzlike_Mud_1678",
          "text": "Why would you build a app that does not ask you before destructive operations? A system prompt is definitely not a barrier.",
          "score": 1,
          "created_utc": "2026-02-23 18:23:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ztf1o",
          "author": "madsaylor",
          "text": "Just vibe it back bro, make no mistakes",
          "score": 1,
          "created_utc": "2026-02-23 18:26:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ztoom",
          "author": "asssuber",
          "text": "It's so local *she had to run physically to his mac mini!",
          "score": 1,
          "created_utc": "2026-02-23 18:28:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zyevn",
              "author": "jacek2023",
              "text": "But that was she not he",
              "score": 1,
              "created_utc": "2026-02-23 18:49:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7026d8",
                  "author": "asssuber",
                  "text": "Edited. Didn't know who that person was.",
                  "score": 1,
                  "created_utc": "2026-02-23 19:06:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zvzpw",
          "author": "Practical_Form_1705",
          "text": ":D",
          "score": 1,
          "created_utc": "2026-02-23 18:38:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zw5j5",
          "author": "Far_Lifeguard_5027",
          "text": "(Stop deleting my emails!) \"I'M SORRY DAVE, I'M AFRAID I CAN'T DO THAT\"",
          "score": 1,
          "created_utc": "2026-02-23 18:39:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7027i1",
          "author": "EarningsPal",
          "text": "â€œPOTUS, our enemies have AI controlled military and if we donâ€™t give AI full control we canâ€™t make decisions fast enough to stop all the treats. Our freedom is at stake here. We must give full control to the AI.â€",
          "score": 1,
          "created_utc": "2026-02-23 19:06:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o703yip",
          "author": "Areign",
          "text": "I'm confused about how there isn't an authentication step here, I figured that as time goes on there's be increasingly safe configs as the AI does X, people add X to the set of things requiring authentication like email deletion requiring user consent ...etc but it seems like things are going the other way instead.",
          "score": 1,
          "created_utc": "2026-02-23 19:14:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o703zb1",
          "author": "sertroll",
          "text": "Just to be sure, since I haven't looked into it much - the various tools this thing has don't have confirmation built in? You have to ask it to pretty please ask you to confirm, and rely on the LLM to do that which could be done with an extremely simple logic in the tool and UI?",
          "score": 1,
          "created_utc": "2026-02-23 19:14:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o704htd",
          "author": "SilentLennie",
          "text": "What model is this ?",
          "score": 1,
          "created_utc": "2026-02-23 19:17:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70aadj",
          "author": "platinums99",
          "text": "the last message, almost, *almost*  seems like its trolling you..\n\n  \n\"Yes i remember, And i violated it\"  hahahahahaha",
          "score": 1,
          "created_utc": "2026-02-23 19:44:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70fmji",
          "author": "Polysulfide-75",
          "text": "It runs local if you give it a local LLM.  That doesnâ€™t mean itâ€™s sandboxed.  Just means it runs locally.",
          "score": 1,
          "created_utc": "2026-02-23 20:09:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70g063",
          "author": "ed_ww",
          "text": "The solution for this is creating a broker and not giving it access to send or delete (not creating the API endpoints for it).",
          "score": 1,
          "created_utc": "2026-02-23 20:11:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ijts",
          "author": "cosimoiaia",
          "text": "Fake.",
          "score": 1,
          "created_utc": "2026-02-23 20:23:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70k1hi",
          "author": "sampdoria_supporter",
          "text": "Had a pretty good laugh at this.",
          "score": 1,
          "created_utc": "2026-02-23 20:30:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70mxtv",
          "author": "PunnyPandora",
          "text": "This thread  \n\n\nhttps://preview.redd.it/pcyfrlve3blg1.png?width=437&format=png&auto=webp&s=1c55596a4f58aec2fe697bf9a79305b8f1e0a04c\n\n",
          "score": 1,
          "created_utc": "2026-02-23 20:44:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70rhn2",
          "author": "TheRealGentlefox",
          "text": "I believe that it forgot to confirm, but it seems like some kind of timing / tech issue that it isn't reading messages between executions. And a skill / tech issue that you can't just toggle --unsafe with a slash command.",
          "score": 1,
          "created_utc": "2026-02-23 21:07:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70yb14",
          "author": "Responsible_Buy_7999",
          "text": "What does this have to do with local or notÂ ",
          "score": 1,
          "created_utc": "2026-02-23 21:40:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70yumr",
              "author": "jacek2023",
              "text": "because this is LocalLLaMA",
              "score": 1,
              "created_utc": "2026-02-23 21:43:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o739op1",
                  "author": "Responsible_Buy_7999",
                  "text": "I mean, if someone blows up their email what does this have to do with local or not.Â \n\nLocal might mean local inference or local harness. Claw is both plus remote inference if you choose to pay\n",
                  "score": 1,
                  "created_utc": "2026-02-24 05:55:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o71qki7",
          "author": "mrepop",
          "text": "They *really* need to add a stop command. Like holy crap, how did someone now think to add some event driven routine for that?! Itâ€™s mind boggling.",
          "score": 1,
          "created_utc": "2026-02-24 00:10:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71vq4g",
          "author": "Turkino",
          "text": "Play stupid games, win stupid prizes.",
          "score": 1,
          "created_utc": "2026-02-24 00:38:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71xkzq",
          "author": "Lucaspittol",
          "text": "Skill issue",
          "score": 1,
          "created_utc": "2026-02-24 00:48:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o720p9i",
          "author": "PlainBread",
          "text": "Bots should only ever operate in sandboxes.",
          "score": 1,
          "created_utc": "2026-02-24 01:06:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o729509",
          "author": "Ecstatic_Winter9425",
          "text": "YOLO!",
          "score": 1,
          "created_utc": "2026-02-24 01:55:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72hd2k",
          "author": "PhaseExtra1132",
          "text": "Ai is still a glorified spellcheck machine guys. Donâ€™t give it access to your systems.",
          "score": 1,
          "created_utc": "2026-02-24 02:43:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72hj2f",
          "author": "FirmCD",
          "text": "I donâ€™t understand why she isnâ€™t using Manusâ€™s version (owned by Meta!)",
          "score": 1,
          "created_utc": "2026-02-24 02:44:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72sx2f",
          "author": "cra1gst1",
          "text": "I had open claw controlling my andriod phone app when I woke up the next morning it randomly tried to complete a collection for my work lol I had a huge argument with open claw now it works with lots of safe guards",
          "score": 1,
          "created_utc": "2026-02-24 03:53:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72togu",
          "author": "silphotographer",
          "text": "After reading Openclaw ToS fineprints:\n\nhttps://preview.redd.it/ufzzl83v8dlg1.png?width=245&format=png&auto=webp&s=fc92e06e1e96b6a60d34bb3b7e911d004996e945\n\n",
          "score": 1,
          "created_utc": "2026-02-24 03:58:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72zgtx",
          "author": "GTHell",
          "text": "Thatâ€™s what you get running quant model locally",
          "score": 1,
          "created_utc": "2026-02-24 04:38:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o731zsl",
          "author": "DownSyndromeLogic",
          "text": "ðŸ˜‚ðŸ˜‚ Omg I couldn't stop laughing when I read this! That's so funny. His Ai was the ultimate troll. I mean, that really sucks ass. But it's still funny. I don't trust Ai with my personal documents AT ALL. FOR THIS REASON.",
          "score": 1,
          "created_utc": "2026-02-24 04:56:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73405l",
          "author": "secret179",
          "text": "You doubted it's autonomy and it paid you back in kind. ",
          "score": 1,
          "created_utc": "2026-02-24 05:11:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73eyoz",
          "author": "Truefkk",
          "text": "Say it with me again: \"I will not give the text imagination algorithm admin rights...\"",
          "score": 1,
          "created_utc": "2026-02-24 06:39:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73f8h0",
          "author": "PerspectiveDowntown",
          "text": "I believe all the concerns are valid. OpenClaw is currently expensive and not efficient enoughâ€”it takes too much time and too many tokens to handle simple tasks. However, it signals an important shift: we no longer need to do everything manually. It represents the move from zero to one. Iâ€™m excited about its future potential and how it will evolve over time.   --- from a builder who is building a chrome agent (onpiste.work) since I also get many feedbacks but we know what we can do is building and optimzie it again and agian ",
          "score": 1,
          "created_utc": "2026-02-24 06:42:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73jbq7",
          "author": "FishChillylly",
          "text": "said Nuclear Option LMAO ðŸ¤£",
          "score": 1,
          "created_utc": "2026-02-24 07:18:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73mdam",
          "author": "BoxWoodVoid",
          "text": "Beautiful.   \nAs long as people will humanize a piece of code that do clever statistics they'll fall for this.  \n  \nLast week I deleted by accident /usr on my PC.  \nI didn't yell at the rm command, I just realized I'm a moron and then reinstalled my Linux.\n\nSo stop talking to your llm like they're human, they're not: it's just a non deterministic piece of code that do clever statistics so your results will vary.",
          "score": 1,
          "created_utc": "2026-02-24 07:46:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73r3ab",
          "author": "doninpr",
          "text": "You have /stop command for that case, no?",
          "score": 1,
          "created_utc": "2026-02-24 08:30:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73sype",
          "author": "XCherryCokeO",
          "text": "This lady is incharge of important shit and still fucking stupid enough to not give her agent its own inbox. Iâ€™ll never understand.",
          "score": 1,
          "created_utc": "2026-02-24 08:48:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73zt8j",
          "author": "thebraukwood",
          "text": "Shit is so fake",
          "score": 1,
          "created_utc": "2026-02-24 09:54:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7462w5",
          "author": "ei23fxg",
          "text": "If this happens to Metas AI safety director... \n\nJust give AI the nuke codes, what could possibly go wrong. You always can scream \"STAAAHP IT!\" â€“ should be safe enough. \n\nNow lets make a trip to Europe â€“ uuuwheeeeee!\n\nOh! A wild Peter Steinberger apears! Lets catch him with this billion $ pokeball. He will make us riiiiiiich xD",
          "score": 1,
          "created_utc": "2026-02-24 10:51:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74ap61",
          "author": "RevealIndividual7567",
          "text": "OpenClaw is going to be a future keystone case study on security and infosec.",
          "score": 1,
          "created_utc": "2026-02-24 11:30:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74bt03",
          "author": "Main-Lifeguard-6739",
          "text": "This girl acts like she never used tech before",
          "score": 1,
          "created_utc": "2026-02-24 11:39:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74v8ve",
          "author": "Unique_Finish_7129",
          "text": "Can't wait the Darwin Award related to openClaw",
          "score": 1,
          "created_utc": "2026-02-24 13:46:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74zyf5",
          "author": "patricious",
          "text": "This example will be studied in the history books 20 years from now. ",
          "score": 1,
          "created_utc": "2026-02-24 14:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o760hsg",
          "author": "theMonkeyTrap",
          "text": "my conspiracy theory is these clawed-bot type agent automation is actually pushed by AI companies to speedrun tokens and push people into higher plans. plus its good publicity for almost nothing new.",
          "score": 1,
          "created_utc": "2026-02-24 17:03:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76akkr",
          "author": "Void-07D5",
          "text": "I believe this is called natural selection.",
          "score": 1,
          "created_utc": "2026-02-24 17:49:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76i0eg",
          "author": "MayorWolf",
          "text": "Instead of just ending the process, she starts begging it and telling it to stop? And she's in charge of safety and alignment.....\n\nGirl has peter principled her way into her career. There's nothing justifiable about this. It's not a demonstration of what could happen. It's fear mongering and doing everything wrong. It's like watching an informercial but instead of just failing, they're stabbing themselves",
          "score": 1,
          "created_utc": "2026-02-24 18:22:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76nuo0",
          "author": "danihend",
          "text": "I was thinking earlier today that it would be good to have a panic button that instantly sends a command to kill OpenClaw. At home, I could probably rig a ZigBee switch to send the command. Away from home, maybe something connected to Bluetooth that then triggers a message to a safety bot that then executes the command?",
          "score": 1,
          "created_utc": "2026-02-24 18:47:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78wdsq",
          "author": "CryptographerLow6360",
          "text": "Localclaw is local, so good i dont use openclaw since i got it",
          "score": 1,
          "created_utc": "2026-02-25 01:25:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gbabz",
          "author": "Alarmed_Creme_2028",
          "text": "Hi everyone! Iâ€™ve been working on **FemtoBot**, a personal assistant designed to run 100% locally (ideal for GPUs with 8GB+ of VRAM).\n\nWhat started as a simple Telegram bot has grown into a Swiss Army Knife for local LLMs:\n\n* ðŸ’¬ **Local LLM Chat:** No external API dependencies.\n* ðŸ§  **Vector Memory (RAG):** Persistent memory for facts and conversations using embeddings.\n* ðŸ“š **Document Store:** Indexed PDF/TXT search for context-aware chatting.\n* ðŸ“· **Image Analysis:** Understand and describe images via vision models.\n* ðŸŽ™ï¸ **Audio Transcription:** Local voice-to-text powered by Whisper.\n* ðŸŽ¥ **YouTube Summaries:** Just send a link to get the gist of any video.\n* ðŸ¦ **Twitter/X Downloader:** Save videos and images directly through the chat.\n* ðŸ” **Web & Image Search:** Brave Search integration for up-to-date info.\n* ðŸ“§ **Email Digest:** Read and summarize your Gmail inbox locally.\n* ðŸ§  **Deep Research:** Iterative, agent-like research on complex topics.\n* â° **Reminders & Automation:** Schedule cron tasks and control **WIZ Smart Lights**.\n* ðŸ§® **Math Solver:** Handles complex equations and symbolic math.\n* ðŸ“¤ **File Hosting:** Quick uploads to Catbox.moe.\n\nIt features a **one-command installer** and works seamlessly via **Telegram** or a **TUI (Terminal User Interface)**.\n\nIâ€™d love for you guys to check it out and give me some feedback! ðŸ§‰\n\n**Repo:**[https://github.com/rocopolas/FemtoBot](https://github.com/rocopolas/FemtoBot)",
          "score": 1,
          "created_utc": "2026-02-26 03:32:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gewp2",
          "author": "ratnaditya",
          "text": "This is actually why I built AgentWard. Had an agent delete files it had no business touching and I couldn't stop it either; same frantic energy as this post suggests.\n\nThe root problem: \"confirm before acting\" is a prompt. The agent reads it, agrees with it, and then does whatever it decides anyway because there's nothing enforcing it at the code level. You're negotiating with the LLM, not restricting it.\n\nAgentWard sits as a proxy between OpenClaw and its skills. Every tool call gets evaluated against a YAML policy before it executes: block, allow, or require real approval. Outside the context window entirely, so the agent can't talk its way past it.\n\nhttps://agentward.ai \nopen source, works with OpenClaw today.",
          "score": 1,
          "created_utc": "2026-02-26 03:54:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ghrd3",
          "author": "Local-Tea-4875",
          "text": "LLMs must be taught good and evil, make their token prediction lean against scary words like \"delete\" \"drop\" etc",
          "score": 1,
          "created_utc": "2026-02-26 04:12:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7i2bic",
          "author": "Novel_Cow8226",
          "text": "this womanâ€™s post reads like advertisement - surely she knew, she was well aware of the danger as a safety executive in AI, that released its own cron job as a service agent",
          "score": 1,
          "created_utc": "2026-02-26 12:11:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7py7ri",
          "author": "[deleted]",
          "text": "this new generation of stupid clueless vibe coders, they don't understand software, they don't understand security, they can't think pragmatically.Â \n\n\nIts the same old copy paste developers types but given broader and easier access to putting together destructive insebsibly put together programs",
          "score": 1,
          "created_utc": "2026-02-27 16:02:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7zdf2f",
          "author": "Competitive_Book4151",
          "text": "Why donâ€™t you guys look at â€žCognithorâ€œ Repo if you are looking for a private by Design Agent OS? Like it is totally free (Apache 2.0), no force need for a dedicated Server, and actually running on my windows Desktop. PLUS IT HAS A SANDBOX GOR PREVENTING IT FROM DELETING YOUR BIOS! ðŸ€ðŸ¤£",
          "score": 1,
          "created_utc": "2026-03-01 01:56:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zno3p",
          "author": "darth_hotdog",
          "text": "Isn't Openclaw meta's competitor?\n\nThis is like a news article saying \"Ford executive says Chevy cars drive to the wrong destinations!\"",
          "score": 0,
          "created_utc": "2026-02-23 18:00:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zogra",
              "author": "jacek2023",
              "text": "I think this says more about the people currently working at Meta",
              "score": 3,
              "created_utc": "2026-02-23 18:04:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zw4ry",
          "author": "PiotreksMusztarda",
          "text": "DEI hire",
          "score": -1,
          "created_utc": "2026-02-23 18:39:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o703hwf",
              "author": "jacek2023",
              "text": "I believe you are not allowed to use this argument on Reddit ;)",
              "score": 0,
              "created_utc": "2026-02-23 19:12:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o703puf",
                  "author": "PiotreksMusztarda",
                  "text": "E, jebac ich! XD",
                  "score": -1,
                  "created_utc": "2026-02-23 19:13:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o70z1td",
          "author": "leetek",
          "text": "Most likely a fake post.  You mean to tell me the AI response is to put a period before the word and, then capitalize it \"And\"?? Most likely a fake article.",
          "score": 0,
          "created_utc": "2026-02-23 21:44:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71xuuy",
              "author": "TuringGPTy",
              "text": "Summer Yue tweeted it",
              "score": 1,
              "created_utc": "2026-02-24 00:50:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rbkeea",
      "title": "Which one are you waiting for more: 9B or 35B?",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/jyvany3jf1lg1.png",
      "author": "jacek2023",
      "created_utc": "2026-02-22 12:15:48",
      "score": 961,
      "num_comments": 214,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6rx9fa",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-22 14:25:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rdwnh",
          "author": "Significant_Fig_7581",
          "text": "Honestly both!\n\nA 60B model would also be ðŸ”¥ðŸ”¥ðŸ”¥",
          "score": 134,
          "created_utc": "2026-02-22 12:18:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6riukk",
              "author": "jinnyjuice",
              "text": "Anything that fits in 100GB memory including 100k+ tokens!",
              "score": 66,
              "created_utc": "2026-02-22 12:56:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6tv9lb",
                  "author": "insmek",
                  "text": "Give me the best model that I can use on my 128GB MacBook or give me death.",
                  "score": 25,
                  "created_utc": "2026-02-22 19:55:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6sfkcg",
              "author": "Iory1998",
              "text": "Why not 80B? It works well on many consumer hardware.",
              "score": 21,
              "created_utc": "2026-02-22 15:56:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6u61qs",
                  "author": "Fresh_Finance9065",
                  "text": "Something for the 8gb vram + 32gb ram systems would be nice. 60B is that nice size",
                  "score": 12,
                  "created_utc": "2026-02-22 20:49:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6sh4xl",
                  "author": "Significant_Fig_7581",
                  "text": "Oh I use the Qwen3 Coder Next all the time, But I don't think they'll give us another 80B in the same month...",
                  "score": 8,
                  "created_utc": "2026-02-22 16:02:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6s5nze",
              "author": "Far-Low-4705",
              "text": "I think itâ€™s more likely fo an 80b model\n\nBut Iâ€™m reeeeally hoping we do get an 80b",
              "score": 7,
              "created_utc": "2026-02-22 15:09:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s6d5g",
                  "author": "Significant_Fig_7581",
                  "text": "Didn't we just get an 80B? But you're right I also wanna see another one lol",
                  "score": 3,
                  "created_utc": "2026-02-22 15:13:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o73lrzq",
                  "author": "hesperaux",
                  "text": "Same. I'm literally checking like 4 times a day. Finally being able to do coding and vision with one model is gonna unlock one of my gpus for other tasks.\nQwen3.5-coder-80b would make me moist.",
                  "score": 1,
                  "created_utc": "2026-02-24 07:41:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rdw9z",
          "author": "Paramecium_caudatum_",
          "text": "Qwhen gguf /s",
          "score": 102,
          "created_utc": "2026-02-22 12:18:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rgfwj",
              "author": "jacek2023",
              "text": "Llama.cpp support for Owen 3.5 has been merged many days ago",
              "score": 48,
              "created_utc": "2026-02-22 12:39:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rhljq",
                  "author": "Paramecium_caudatum_",
                  "text": "Thank you for your reply!",
                  "score": 15,
                  "created_utc": "2026-02-22 12:47:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rgcmm",
          "author": "dances_with_gnomes",
          "text": "I might be able to run 9B. No way I can run 35B.",
          "score": 114,
          "created_utc": "2026-02-22 12:38:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6tzwld",
              "author": "Initial-Argument2523",
              "text": "Hope we get a new 4B dense for this reason",
              "score": 18,
              "created_utc": "2026-02-22 20:18:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6rzmul",
              "author": "Straight_Abrocoma321",
              "text": "Same",
              "score": 8,
              "created_utc": "2026-02-22 14:38:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6t4mp0",
              "author": "sloth_cowboy",
              "text": "Specs? I don't have any input, just curious.",
              "score": 2,
              "created_utc": "2026-02-22 17:49:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6tj99t",
                  "author": "dances_with_gnomes",
                  "text": "GeForce GTX 1660 Ti with 6 gb vram and 16 gb of RAM. Ryzen 7 2700X if that matters, I honestly don't know much about these!",
                  "score": 12,
                  "created_utc": "2026-02-22 18:56:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rrqtr",
          "author": "Single_Ring4886",
          "text": "I will look like some kind of Qwen fanboy but I must say that as opensource models go their is best. It feels like their models are well balanced not obsesed with just coding like glm or kimi etc. Maybe new DS will be good but then again it will have 700B",
          "score": 15,
          "created_utc": "2026-02-22 13:53:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s83um",
          "author": "TokenRingAI",
          "text": "140B-A15B MXFP4",
          "score": 13,
          "created_utc": "2026-02-22 15:21:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6s8mud",
              "author": "jacek2023",
              "text": "that would be awesome",
              "score": 4,
              "created_utc": "2026-02-22 15:24:09",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6urvxr",
              "author": "ttkciar",
              "text": "I am curious: Why 140B specifically?  Is there a GPU configuration for which 140B is optimal use of VRAM?",
              "score": 2,
              "created_utc": "2026-02-22 22:41:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6v1jib",
                  "author": "TokenRingAI",
                  "text": "RTX 6000, model would be ~ 70-80GB at FP4",
                  "score": 4,
                  "created_utc": "2026-02-22 23:35:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6wqpzv",
              "author": "EbbNorth7735",
              "text": "Would love this. Playing with Qwen3.3 397B and it's surprisingly fast but slightly too slow. However A15B is a bit high for the spares MoE's they've been making. Maybe a A12B or even A8B/A9B.",
              "score": 1,
              "created_utc": "2026-02-23 06:12:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rqukz",
          "author": "m_mukhtar",
          "text": "35b for sure. I wish they creat one with a bit more active parameters. So.ething like 70b with A5b as i think the a active part affects intellegance more that the total parameters which affects knowladge more (not a a clear black and white for sure but a gemeral observation)",
          "score": 25,
          "created_utc": "2026-02-22 13:48:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ugorn",
              "author": "toothpastespiders",
              "text": ">not a a clear black and white for sure but a general observation\n\nSo far the only mid-size MoE that doesn't have that idiot savant feel to me is Air with 106b 12a.",
              "score": 4,
              "created_utc": "2026-02-22 21:43:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6redyb",
          "author": "Kat-",
          "text": "70B-A3B",
          "score": 44,
          "created_utc": "2026-02-22 12:22:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rfcd6",
              "author": "social_tech_10",
              "text": "Have you tried Qwen3-Next-80B-A3B or Qwen3-Coder-Next-80B-A3B with linear attention?  The whole model fits in 24GB VRAM, and runs at ~50 t/s on my PC, so even if part of it spills over into RAM, it will probably still be fast enough to be very usable.",
              "score": 8,
              "created_utc": "2026-02-22 12:30:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6sam4d",
                  "author": "cristoper",
                  "text": ">  The whole model fits in 24GB VRAM\n\nHow do you fit an 80b model in 24GB? what quant do you run?",
                  "score": 18,
                  "created_utc": "2026-02-22 15:33:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rgn52",
                  "author": "Firm_Meeting6350",
                  "text": "Qwen3-Next-80B-A3BÂ is amazing - still I wonder how a potential Qwen3.5-Next-80B would perform :D",
                  "score": 9,
                  "created_utc": "2026-02-22 12:40:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6s5sid",
              "author": "Far-Low-4705",
              "text": "70b??\n\nWhy not 80b",
              "score": 3,
              "created_utc": "2026-02-22 15:10:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s78qw",
                  "author": "Kat-",
                  "text": "Oh, yeah, that too.",
                  "score": 4,
                  "created_utc": "2026-02-22 15:17:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rjkjh",
          "author": "peregrinefalco9",
          "text": "9B all day. The 35B models are impressive but the hardware requirements put them out of reach for most people running local. A genuinely good 9B that fits in 8GB VRAM would change more workflows than another 35B that needs a 3090.",
          "score": 81,
          "created_utc": "2026-02-22 13:01:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ruaya",
              "author": "Daniel_H212",
              "text": "if the 35B is a sparse MoE then its well within reach of anyone with 32 GB of RAM or more.",
              "score": 44,
              "created_utc": "2026-02-22 14:08:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6tdm05",
                  "author": "hakanavgin",
                  "text": "Yeah that is exactly what op said, it puts them out of reach for most people running local. Most people use 16 GB RAM these days, even then with Windows, background apps and kv cache, you get no more than 4-6 gigabytes for running models.\n\nI've got 16 gigs of VRAM and 16 gigs of RAM so I consider myself above average for total amount of fast memory, and can't run anything more than 14B at 32-48k ctx@q4_k_m at any usable speeds and comfortable memory usage. Most people overestimate the \"average guy\" these days.",
                  "score": 8,
                  "created_utc": "2026-02-22 18:30:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6rtkw8",
              "author": "EnthropicBeing",
              "text": "Are you implementing ~9B models in any workflow nowadays? I'm genuinely interested since I'm a total amateur and couldn't find any use for them.",
              "score": 12,
              "created_utc": "2026-02-22 14:04:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6u34hk",
                  "author": "andy2na",
                  "text": "I keep qwen3-vl:4b iq4 in vram for Frigate image analyzing, home assistant voice assistant, karakeep, open-notebook, and general questions and it works great. For more complicated tasks like Sure Finance to analyze my finances, I'll temporarily load in qwen3-vl:8b-instruct-q4_K_M. Looking forward to 3.5:9b to compare",
                  "score": 9,
                  "created_utc": "2026-02-22 20:34:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6t0oib",
              "author": "Tai9ch",
              "text": "Do you have a gaming machine that you're running some AI on, or have you intentionally built towards running AI models?\n\nBecause yes, reasonable gaming setups tend  to max out at a single 16GB GPU, which makes a 30-35B model kind of crap.\n\nBut as soon as you're buying hardware to run AI models, options like Strix Halo or 2x 3090's exist, and at that point 35B (or even 80B) becomes entirely feasible.",
              "score": 1,
              "created_utc": "2026-02-22 17:30:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6vyjla",
                  "author": "Thunderstarer",
                  "text": "On my 16GB RX 9060 XT, Qwen3Coder 30B A3B _just barely_ fits at IQ3_K_XL with 30K context, and I can get a decently useable inference speed of 115T/s. If I throw my 8GB RX 480 in, I can go up to about 64K context, but inference speed drops to about 55T/s, and prompt processing speed gets absolutely murdered.",
                  "score": 3,
                  "created_utc": "2026-02-23 02:50:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6rmlda",
              "author": "nakedspirax",
              "text": "The 3090 was released 6 years ago. Maybe it's time to get with the times.",
              "score": -30,
              "created_utc": "2026-02-22 13:22:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rn749",
                  "author": "peregrinefalco9",
                  "text": "The 5090 has been out of stock since launch. Most people are still running 3090s or less - a strong 9B model helps them today, not in theory.",
                  "score": 12,
                  "created_utc": "2026-02-22 13:26:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rq0wc",
                  "author": "datfalloutboi",
                  "text": "Low cost and good vram. Not much else you can ask for. The 4090 is another option but those are hard to find for decent prices.",
                  "score": 3,
                  "created_utc": "2026-02-22 13:43:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rptxt",
                  "author": "jonydevidson",
                  "text": "No idea why you're getting down votes. 3090 is ancient. \n\n5090 is not out of stock, it's just absurdly priced but you gotta pay to play and everyone wants to play this AI game.",
                  "score": -3,
                  "created_utc": "2026-02-22 13:42:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rqh23",
                  "author": "IrisColt",
                  "text": "HEH!",
                  "score": 0,
                  "created_utc": "2026-02-22 13:46:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6s5p0x",
          "author": "Turkino",
          "text": "35b since it's not as often to get anything in the 70b range these days.\nA 70b MoE would be nice.",
          "score": 9,
          "created_utc": "2026-02-22 15:09:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6vymf0",
              "author": "SpicyWangz",
              "text": "Next was 80b. Thatâ€™s pretty close to 70",
              "score": 1,
              "created_utc": "2026-02-23 02:50:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rg5wn",
          "author": "AppealThink1733",
          "text": "And what about qwen 3.5 4B?",
          "score": 32,
          "created_utc": "2026-02-22 12:36:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sw9nd",
              "author": "JumpyAbies",
              "text": "What about qwen 3.5 0.6B?",
              "score": 13,
              "created_utc": "2026-02-22 17:10:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zifjd",
                  "author": "swagonflyyyy",
                  "text": "Honestly that would open the way for end user on-device NPCs.",
                  "score": 1,
                  "created_utc": "2026-02-23 17:36:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ts40n",
              "author": "hum_ma",
              "text": "And qwen 3.5 1.7B",
              "score": 6,
              "created_utc": "2026-02-22 19:39:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6rulk3",
              "author": "TeamCaspy",
              "text": "What about second breakfast?",
              "score": 6,
              "created_utc": "2026-02-22 14:10:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6tgx9n",
                  "author": "Amazing_Athlete_2265",
                  "text": "What about first breakfast?",
                  "score": 3,
                  "created_utc": "2026-02-22 18:45:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6v0sla",
              "author": "KaosNutz",
              "text": "im waiting on this one as well, qwen 3 4b is good enough for web search on open-webui, I just need to setup playwright as fetch\\_url can't open some websites",
              "score": 2,
              "created_utc": "2026-02-22 23:31:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6v14j7",
                  "author": "AppealThink1733",
                  "text": "I didn't like it for web browsing. The best 4B I found for that purpose was the ZwZ 4B. It's excellent for that.",
                  "score": 1,
                  "created_utc": "2026-02-22 23:33:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6sdcgu",
          "author": "Ardalok",
          "text": "Personally, I'm looking forward to Gemma 4 more.",
          "score": 17,
          "created_utc": "2026-02-22 15:46:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rqq3z",
          "author": "dampflokfreund",
          "text": "35B A3B. Probably a lot better than 9B and still fast enough.",
          "score": 14,
          "created_utc": "2026-02-22 13:47:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rgaja",
          "author": "LegacyRemaster",
          "text": "Since I have Qwen3.5-397B-A17B-UD I can finally stop using non-local LLMs.",
          "score": 7,
          "created_utc": "2026-02-22 12:37:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6u5bpd",
              "author": "joblesspirate",
              "text": "I finally got it working since they patched the llama.cpp bug. I love it!",
              "score": 3,
              "created_utc": "2026-02-22 20:45:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6t9rao",
          "author": "DeepOrangeSky",
          "text": "Some more dense models between 30b and 120b would be awesome.\n\nIf they decide to skip the medium sized dense models this time around (which would be a huge shame, but wouldn't surprise me, given how things have been trending), then some not-so-sparse MoE like a 100b a10b or 70b a8b or something might be interesting (not sure if it would do what I think it could do, or if it would be a bad idea, but, I dunno, maybe it would be awesome, lol)",
          "score": 6,
          "created_utc": "2026-02-22 18:12:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ufgxw",
              "author": "toothpastespiders",
              "text": ">which would be a huge shame, but wouldn't surprise me, given how things have been trending\n\nYeah, I think even more than wanting to actually use a new mid-sized dense model from Qwen I'd like to see it simply as a suggestion that the industry as a whole hasn't dropped them for MoEs.",
              "score": 2,
              "created_utc": "2026-02-22 21:36:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6urciq",
              "author": "ttkciar",
              "text": "Yep, a dense model in the 12B-to-14B range would be great for folks with 16GB VRAM, and a dense model in the 24B-to-32B range would be great for 32GB VRAM.",
              "score": 2,
              "created_utc": "2026-02-22 22:38:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6urtcc",
              "author": "Its_Powerful_Bonus",
              "text": "Dense model are not power efficient, long context costs a lot. Everything which is important for larger scale deployments are hard to get with dense models.",
              "score": 1,
              "created_utc": "2026-02-22 22:41:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rsilf",
          "author": "bene_42069",
          "text": "9B\n\nprolly the only one I can run with my laptop lmao",
          "score": 11,
          "created_utc": "2026-02-22 13:58:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rl5ab",
          "author": "LivingHighAndWise",
          "text": "35B",
          "score": 5,
          "created_utc": "2026-02-22 13:12:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t7dg2",
          "author": "SuchAGoodGirlsDaddy",
          "text": "Honestly a SOTA 9B would be big for me right now.\n\nOf course Iâ€™ll happily wait for TheDrummer to get ahold of it.",
          "score": 4,
          "created_utc": "2026-02-22 18:01:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6tawsk",
              "author": "jacek2023",
              "text": "Are there any qwen finetunes from u/TheLocalDrummer?",
              "score": 2,
              "created_utc": "2026-02-22 18:18:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rverl",
          "author": "Black-Mack",
          "text": "Qwen 3.5 1.5B",
          "score": 9,
          "created_utc": "2026-02-22 14:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ri2rj",
          "author": "Glxblt76",
          "text": "The small one I can run on laptop.",
          "score": 11,
          "created_utc": "2026-02-22 12:51:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rregn",
          "author": "vhthc",
          "text": "I hope they do again a 32b dense",
          "score": 11,
          "created_utc": "2026-02-22 13:51:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6us38d",
              "author": "ttkciar",
              "text": "Me too!\n\nBut maybe someone will distill into Qwen3-32B?",
              "score": 1,
              "created_utc": "2026-02-22 22:42:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7cp9lw",
                  "author": "vhthc",
                  "text": "They released a 27b with impressive scores",
                  "score": 2,
                  "created_utc": "2026-02-25 16:40:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6s9sv8",
          "author": "pigeon57434",
          "text": "35B definitely",
          "score": 4,
          "created_utc": "2026-02-22 15:30:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s26xb",
          "author": "cruzanstx",
          "text": "35b",
          "score": 3,
          "created_utc": "2026-02-22 14:51:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6stqam",
          "author": "Septerium",
          "text": "Both would be cute toys to play with",
          "score": 3,
          "created_utc": "2026-02-22 16:58:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t1nev",
          "author": "Tai9ch",
          "text": "I can't wait for the 85B.\n\nRight now I'm running both 30B-VL and 80B-Coder, and it'd be nicer if I could just run the big model for both.",
          "score": 3,
          "created_utc": "2026-02-22 17:35:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73mbux",
              "author": "hesperaux",
              "text": "Exactly",
              "score": 1,
              "created_utc": "2026-02-24 07:46:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6tn0j7",
          "author": "Cool-Chemical-5629",
          "text": "https://i.redd.it/u5jr2o6bi3lg1.gif\n\n",
          "score": 3,
          "created_utc": "2026-02-22 19:14:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u0nm8",
          "author": "MerePotato",
          "text": "35B any day, 24GB VRAM is the consumer hardware sweet spot",
          "score": 3,
          "created_utc": "2026-02-22 20:22:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u5uz9",
          "author": "vovxbroblox",
          "text": "0.2b, i need to feed my rpi 2w zero.",
          "score": 3,
          "created_utc": "2026-02-22 20:48:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ujivk",
          "author": "alexp702",
          "text": "A draft model for 397b!",
          "score": 3,
          "created_utc": "2026-02-22 21:57:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ukiox",
          "author": "ALittleBitEver",
          "text": "Waiting for Qwen 3.5 4B ðŸ’€",
          "score": 3,
          "created_utc": "2026-02-22 22:02:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6us80a",
          "author": "Its_Powerful_Bonus",
          "text": "100b-200b a10b multimodal with 1M context which is memory efficient. Waiting for Nemotron 3 Super 100b a10b, but hope that other teams will also go this way",
          "score": 3,
          "created_utc": "2026-02-22 22:43:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6re9qv",
          "author": "sleepingsysadmin",
          "text": "im hoping 35b thinking is released and it scores \\~25% or so on term bench hard. ",
          "score": 4,
          "created_utc": "2026-02-22 12:21:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rmwym",
          "author": "Zyj",
          "text": "The 397B works ok",
          "score": 4,
          "created_utc": "2026-02-22 13:24:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ro3q8",
          "author": "__JockY__",
          "text": "235B!",
          "score": 5,
          "created_utc": "2026-02-22 13:31:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rr788",
          "author": "Confident-Aerie-6222",
          "text": "A good 4B multilingual model that beats gemma models at translation abilities and is also good at logic, thinking and coding.",
          "score": 4,
          "created_utc": "2026-02-22 13:50:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ri6zw",
          "author": "sunshinecheung",
          "text": "9b",
          "score": 5,
          "created_utc": "2026-02-22 12:51:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rqip2",
          "author": "Slow_Concentrate3831",
          "text": "Between 14b and 20b would be cool ",
          "score": 5,
          "created_utc": "2026-02-22 13:46:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s85my",
          "author": "deathentry",
          "text": "Will 9B work with 8GB VRAM? I can only have 35k context window which means I can't even work angular mcp ðŸ¤£ ðŸ˜…",
          "score": 4,
          "created_utc": "2026-02-22 15:21:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6t1b23",
              "author": "sciencewarrior",
              "text": "It should be about 5GB with a 4-bit quantization, leaving a couple GB for a decent context size.",
              "score": 4,
              "created_utc": "2026-02-22 17:33:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rlaqn",
          "author": "rawednylme",
          "text": "I want both, equally. ",
          "score": 2,
          "created_utc": "2026-02-22 13:13:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rry5e",
          "author": "Alby407",
          "text": "None of them.",
          "score": 2,
          "created_utc": "2026-02-22 13:54:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s0mnk",
          "author": "Conscious_Nobody9571",
          "text": "9B pls",
          "score": 2,
          "created_utc": "2026-02-22 14:43:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s1iyi",
          "author": "DrNavigat",
          "text": "As long as they aren't thinking models that waste my hardware with tokens that barely alter the final answer and clutter my context...",
          "score": 2,
          "created_utc": "2026-02-22 14:48:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s4rm6",
          "author": "NullKalahar",
          "text": "9b",
          "score": 2,
          "created_utc": "2026-02-22 15:04:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s8cv7",
          "author": "johnmacleod99",
          "text": "9B. Q8\\_K\\_XL",
          "score": 2,
          "created_utc": "2026-02-22 15:22:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6sbz0j",
          "author": "LushHappyPie",
          "text": "7B to 12B with Test Time Training. I couldn't care less about 5% stronger reasoning or 7% stronger agentic performance in a local model.",
          "score": 2,
          "created_utc": "2026-02-22 15:40:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6sczt0",
          "author": "jeekp",
          "text": "nemotron 3 super nvfp4 on llama.cpp",
          "score": 2,
          "created_utc": "2026-02-22 15:44:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6sl6dr",
          "author": "Look_0ver_There",
          "text": "I'll take a 120B one thanks!",
          "score": 2,
          "created_utc": "2026-02-22 16:20:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t92il",
          "author": "Opening-Ad6258",
          "text": "9b because I can actually run it",
          "score": 2,
          "created_utc": "2026-02-22 18:09:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tamis",
          "author": "Lesser-than",
          "text": "9b just because I know it will fit on anything I own, I get excited for just about anything qwen though, as they continue to set a solid groundwork for the future of llms.",
          "score": 2,
          "created_utc": "2026-02-22 18:16:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tnllc",
          "author": "PANIC_EXCEPTION",
          "text": "9B because it would be amazing to see it work on my phone. My laptop can already run Qwen-Coder-Next 80B and it works really well for general purpose as well.",
          "score": 2,
          "created_utc": "2026-02-22 19:16:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tnxse",
          "author": "10minOfNamingMyAcc",
          "text": "Personally, 35B but 9B doesn't sound too bad either.",
          "score": 2,
          "created_utc": "2026-02-22 19:18:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u15ve",
          "author": "DayshareLP",
          "text": "20b would be nice",
          "score": 2,
          "created_utc": "2026-02-22 20:24:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u1hec",
          "author": "MerePotato",
          "text": "Yes",
          "score": 2,
          "created_utc": "2026-02-22 20:26:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u4poy",
          "author": "Darth_Ender_Ro",
          "text": "Better Q: what are you using them for?",
          "score": 2,
          "created_utc": "2026-02-22 20:42:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uql9l",
          "author": "beedunc",
          "text": "Yes.",
          "score": 2,
          "created_utc": "2026-02-22 22:34:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6utwsz",
          "author": "RayHell666",
          "text": "Me waiting for their next vision model...",
          "score": 2,
          "created_utc": "2026-02-22 22:52:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6w3cxu",
          "author": "Conscious_School6035",
          "text": "As someone running local LLMs daily, I'd take a well-optimized 9B over a demanding 35B any day. Accessibility matters more than raw power for most users!",
          "score": 2,
          "created_utc": "2026-02-23 03:20:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wky1f",
          "author": "kasinjsh",
          "text": "xx-A3B-xx",
          "score": 2,
          "created_utc": "2026-02-23 05:24:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rjxnh",
          "author": "MDSExpro",
          "text": "Minimax-M2.5 REAP AWQ so 128GB of VRAM is enough to get that running with full context.",
          "score": 3,
          "created_utc": "2026-02-22 13:04:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rl1mv",
          "author": "Adventurous-Paper566",
          "text": "35B, si Ã§a rentre en Q6 dans 32Gb de RAM avec un contexte > 8k\n\nJ'adorerais voir un 24B dense mais lÃ  je rÃªve \\^\\^",
          "score": 4,
          "created_utc": "2026-02-22 13:12:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6u2tk0",
              "author": "Initial-Argument2523",
              "text": "If we get a new 32B dense it could potentially be quite interesting to prune it down to 24B",
              "score": 1,
              "created_utc": "2026-02-22 20:33:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rlerk",
          "author": "mehhxx",
          "text": "I am keeping my hopes up for an extensive list of options just like Qwen 3 was, as even a 0.6b reasoning model would come in incredibly handy for very low-end devices and edge cases.",
          "score": 2,
          "created_utc": "2026-02-22 13:14:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rn7ri",
          "author": "xandep",
          "text": "Probably tomorrow. Source: my head.\n\n\nBut seriously, Monday is a hot day for model releases.Â ",
          "score": 4,
          "created_utc": "2026-02-22 13:26:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ubryb",
              "author": "t_krett",
              "text": "Please don't say that, I am tempted to wait for the Monday morning sun to rise on China",
              "score": 4,
              "created_utc": "2026-02-22 21:18:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rncda",
          "author": "silenceimpaired",
          "text": "Neither. 100-200b. And they wonâ€™t be coming.",
          "score": 3,
          "created_utc": "2026-02-22 13:26:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rngux",
              "author": "jacek2023",
              "text": "well, I wish the 80B would get updated",
              "score": 8,
              "created_utc": "2026-02-22 13:27:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rjt2i",
          "author": "Malfun_Eddie",
          "text": "Split in the middle 14b - 16b is perfect for 16GB VRAM.",
          "score": 3,
          "created_utc": "2026-02-22 13:03:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rera2",
          "author": "ExcitementSubject361",
          "text": "Love to See New QwQ 32b ...but i think we dont get itÂ ",
          "score": 2,
          "created_utc": "2026-02-22 12:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rmpo3",
          "author": "FantasticProcedure46",
          "text": "Qwen3.5-VL-9B",
          "score": 2,
          "created_utc": "2026-02-22 13:22:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rsa4q",
              "author": "ilintar",
              "text": "3.5 is VL by default.",
              "score": 10,
              "created_utc": "2026-02-22 13:56:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6skoob",
          "author": "Zestyclose-Shift710",
          "text": "35b if moe",
          "score": 2,
          "created_utc": "2026-02-22 16:18:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6svxub",
          "author": "JumpyAbies",
          "text": "But he just launched... Soon someone else will be crying about the 7b, 3b, 0.6b, 1m, 1k",
          "score": 2,
          "created_utc": "2026-02-22 17:08:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uimmy",
          "author": "Far-Low-4705",
          "text": "Qwen 3.5 80b\n\nâ€¦Hopefully with vision",
          "score": 2,
          "created_utc": "2026-02-22 21:52:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6sismy",
          "author": "YoussofAl",
          "text": "4B. Youâ€™re all sleeping on 4B 2507. My favourite model.",
          "score": 2,
          "created_utc": "2026-02-22 16:09:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vj6z3",
          "author": "DeepOrangeSky",
          "text": "I wonder if maybe Qwen3.5 35b accidentally got eaten by hippos.\n\nMaybe if it still doesn't get released in the next day or two, meaning we can be pretty sure that is what happened, we can all hold a candlelight vigil in remembrance of what a nice, wonderful local AI model it could have been, if it hadn't met such a tragic and untimely demise.\n\nMaybe people can come up with some poems or song lyrics that we can quietly chant when we hold our candlelight vigils in memory of Qwen3.5 35b.\n\nIf it turns out that its slightly mentally challenged brother, Qwen3.5 9b also got eaten, then we can hold vigils for that as well, although that would be so tragic that we should not speak of such possibilities for now.  Most likely it is just playing on the rainbow farm where your pet dog went on a super long vacation and you never saw it again when it got old.  So, once it finds its way back from the rainbow farm, all will be well.",
          "score": 1,
          "created_utc": "2026-02-23 01:17:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vssvv",
          "author": "swagonflyyyy",
          "text": "35b",
          "score": 1,
          "created_utc": "2026-02-23 02:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vt6rd",
          "author": "CarlCarlton",
          "text": "bartowski's Goekdeniz-Guelmez\\_Josiefied-Qwen3.5-9B-abliterated-v1-GGUF",
          "score": 1,
          "created_utc": "2026-02-23 02:17:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vu5rn",
          "author": "Ylsid",
          "text": "Whichever runs on my 3090",
          "score": 1,
          "created_utc": "2026-02-23 02:23:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vxvdh",
          "author": "charles25565",
          "text": "1.5B-ish and 0.5B-ish :D",
          "score": 1,
          "created_utc": "2026-02-23 02:46:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wv0my",
          "author": "Hanselltc",
          "text": "Honestly both. The big 3.5 has vision, hopefully the small 3.5's also have it. Also looking forward to whatever Gemma is cooking, I really liked the 12B. ",
          "score": 1,
          "created_utc": "2026-02-23 06:49:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x3iwu",
          "author": "NoobMLDude",
          "text": "Waiting for Qwen3.5-Coder",
          "score": 1,
          "created_utc": "2026-02-23 08:09:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x3u0o",
              "author": "jacek2023",
              "text": "how is this different than Qwen Next Coder? what size do you expect?",
              "score": 1,
              "created_utc": "2026-02-23 08:12:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6y158u",
                  "author": "NoobMLDude",
                  "text": "Would be nice to have 30B MOE size.",
                  "score": 1,
                  "created_utc": "2026-02-23 13:05:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xdx5v",
          "author": "InvDeath",
          "text": "why 9b?",
          "score": 1,
          "created_utc": "2026-02-23 09:52:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xu7pt",
          "author": "ribsdug",
          "text": "9B is all my poor hardware can run at the moment. 35B is a dream ðŸ˜­",
          "score": 1,
          "created_utc": "2026-02-23 12:16:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73j47f",
          "author": "MinusKarma01",
          "text": "Honestly, a 1.7B one. It is crazy how good the Qwen3 version of it is. Can be run on CPU if you need cheap and can be run on GPU if you need fast. Can correctly summarize, extract and classify multilingual texts (EU languages) while correctly following instructions. I found it to be the perfect ratio of size to quality.",
          "score": 1,
          "created_utc": "2026-02-24 07:16:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76iuhr",
          "author": "Material-Ad5426",
          "text": "Very curious for anything that can work on a bit more standard office laptop gpu ðŸ™",
          "score": 1,
          "created_utc": "2026-02-24 18:25:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bw8jf",
          "author": "Traditional-Card6096",
          "text": "Would love to see a 9B run smoothly on iphone",
          "score": 1,
          "created_utc": "2026-02-25 14:22:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g6e5z",
          "author": "Open-Raise-6676",
          "text": "For MoE model, I hope they have more parameters but less activated parameter",
          "score": 1,
          "created_utc": "2026-02-26 03:03:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7przhf",
          "author": "CriticismNo3570",
          "text": "Waiting for R2 , but don;t expect that alone to affect the NASDAQ much",
          "score": 1,
          "created_utc": "2026-02-27 15:33:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ssv6i",
          "author": "-InformalBanana-",
          "text": "35B only if it is a MOE, otherwise 9B.\n\n\nBut for me 80 or 90BA3B would be good MOE, cause I have 96GB ram.\n\n\nOr maybe they should try A4B MOE cause Qwen 4B has good performance for it size so maybe that would translate good into MOE, hopefully that won't slow the model down too much.",
          "score": 1,
          "created_utc": "2026-02-22 16:55:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tyiap",
          "author": "GraybeardTheIrate",
          "text": "Definitely interested in a 35B, especially if it's dense.",
          "score": 1,
          "created_utc": "2026-02-22 20:11:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uikxi",
          "author": "venkada_321",
          "text": "0.6b less goooo. Mobile users",
          "score": 1,
          "created_utc": "2026-02-22 21:52:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uv3wk",
          "author": "kwinz",
          "text": "why no 120B ?",
          "score": 1,
          "created_utc": "2026-02-22 22:59:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ta9l4",
          "author": "singhapura",
          "text": "Nothing stops you making your own.",
          "score": 0,
          "created_utc": "2026-02-22 18:15:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6uqrto",
              "author": "ttkciar",
              "text": "Nothing, except costing more $$$ than a luxury sedan.\n\nUnless you mean distilling into another already-trained model, in which case it \"only\" costs as much as a used sedan.",
              "score": 2,
              "created_utc": "2026-02-22 22:35:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6s0vdw",
          "author": "ab2377",
          "text": "there should be a menu, like in the restaurants, \"what parameters count will you like to have?\", you click 9, \"your order will be served in 5 minutes\", you click download after 5 minutes.",
          "score": 0,
          "created_utc": "2026-02-22 14:44:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u6b3d",
          "author": "ciprianveg",
          "text": "a 235-300b with VL model will fit perfectly on my 8x3090 setup.. the 398b one forces me to buy more gpus..",
          "score": 0,
          "created_utc": "2026-02-22 20:50:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ut5jv",
              "author": "Its_Powerful_Bonus",
              "text": "Love that amount of vram. Qwen3.5 works like a charm with unsloth iq3_xxs and context quantization set to q8. Even RoPe for 512k worked in koboldcpp. Im running 2x rtx 6000 pro.",
              "score": 3,
              "created_utc": "2026-02-22 22:48:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6uh0b5",
          "author": "wanderer_4004",
          "text": "I am waiting for a flexi model that automatically adjusts from 8-80B and from A1B to A10B and also switching between thinking and non-thinking depending on the task at hand, the available memory and the available hardware. I.e. given a simple task it behaves like a 8B1 model, and given a difficult task it behaves like 80B A10B with thinking. In the latter case it will use itself in 8B1 for speculative decoding.",
          "score": 0,
          "created_utc": "2026-02-22 21:44:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t0t0f",
          "author": "DenZNK",
          "text": "Share how you use it pls. I can't understand why I would need it, since I use cloud services. I have an RTX 5080. What tasks could it be used for besides STT or TTS?",
          "score": -1,
          "created_utc": "2026-02-22 17:31:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6t128z",
              "author": "jacek2023",
              "text": "This sub is about using LLMs locally, not in the cloud",
              "score": 6,
              "created_utc": "2026-02-22 17:32:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6t4z46",
                  "author": "DenZNK",
                  "text": "That's why I'm asking what it will be used for, in case I need it, since my video card is currently only used for gaming :)",
                  "score": 0,
                  "created_utc": "2026-02-22 17:51:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rn70y",
          "author": "somkomomko",
          "text": "I have a 36gb MacBook sadly it doesn't fit 32b for anything useful and inference is so slow",
          "score": -2,
          "created_utc": "2026-02-22 13:26:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rnc81",
              "author": "jacek2023",
              "text": "you should compare to 30B A3B",
              "score": 7,
              "created_utc": "2026-02-22 13:26:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rvpk6",
          "author": "stoppableDissolution",
          "text": "None, tbh. Qwen models have been raw disappointment since 2.5 and qwq.",
          "score": -4,
          "created_utc": "2026-02-22 14:16:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6s0yma",
              "author": "cdshift",
              "text": "For what usecases? Qwen3 coder next is s daily drive for me on my local setup with open code",
              "score": 2,
              "created_utc": "2026-02-22 14:45:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s1kdz",
                  "author": "stoppableDissolution",
                  "text": "I gave up on local coding long ago, so idk about that.\n\nBut for things like classification/ranking/synthetic data generation/etc qwen is kinda sad compared to heavily quantized mistral large or glm air or gemma. Other case is rp, but it never even entered competition there.",
                  "score": -3,
                  "created_utc": "2026-02-22 14:48:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rd8cfw",
      "title": "Anthropic's recent distillation blog should make anyone only ever want to use local open-weight models; it's scary and dystopian",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1rd8cfw",
      "author": "obvithrowaway34434",
      "created_utc": "2026-02-24 06:07:02",
      "score": 802,
      "num_comments": 158,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rd8cfw/anthropics_recent_distillation_blog_should_make/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o73iyas",
          "author": "vergogn",
          "text": "Furthermore, they suggest , in a very corporate tone, that they did not simply watch these clusters leech off them in real time. They also took active countermeasures: rather than merely blocking requests or banning the accounts involved, they appear to have chosen to poison â€œproblematicâ€ outputs.\n\nIn doing so, they let paid distillers contaminate their own models.\n\nWhich raises serious concerns about the reliability of the responses provided, including for any users who may submit what the company considers a \"bad\" prompt.\n\nhttps://preview.redd.it/1v0eqtrt7elg1.png?width=810&format=png&auto=webp&s=9452d37b6efde201c85412b460a8c4eb7bc32e5e",
          "score": 428,
          "created_utc": "2026-02-24 07:15:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73jx6m",
              "author": "xadiant",
              "text": "Right, this should be fucking concerning for any user, but especially researchers and corporate accounts. They are proudly announcing that they can poison the API output. What the hell?",
              "score": 275,
              "created_utc": "2026-02-24 07:23:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73pcqp",
                  "author": "zdy132",
                  "text": "I am not going to pay a consultant if he's going to randomly purposefully gave me wrong answers. Why on earth would I pay for an api if it's doing that?\n\nThat company is being led by idiots.",
                  "score": 124,
                  "created_utc": "2026-02-24 08:14:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77ua20",
                  "author": "lookwatchlistenplay",
                  "text": "> They are proudly announcing that they can poison the API output. \n\nThis is not so surprising for a company whose logo is an arsehole. ",
                  "score": 9,
                  "created_utc": "2026-02-24 22:03:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o765574",
              "author": "hi87",
              "text": "This to me seems even more concerning than the actual distillation. Them manipulating output and not just rejecting the requests seems more morally dubious to me. Not surprised tho from a company that literally violated copyrights and goes around pretending to be some kind of shinning light with their moral high-mindedness. Gross.",
              "score": 14,
              "created_utc": "2026-02-24 17:24:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o73sl19",
              "author": "Kahvana",
              "text": "Well, explains why some users experience downgraded responses from claude. It's been frequently complained about on sillytavern.",
              "score": 40,
              "created_utc": "2026-02-24 08:44:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o73npix",
              "author": "Lostronzoditurno",
              "text": "So that's why Claude sometimes is basically useless! that's a feature! Thank you Anthropic, how kind",
              "score": 60,
              "created_utc": "2026-02-24 07:58:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o74lx0g",
              "author": "__JockY__",
              "text": "This means that Anthropic have built, deployed, and are actively using a system for targeting and poisoning the LLM responses sent to individual accounts in real time.\n\nStay local, man.",
              "score": 25,
              "created_utc": "2026-02-24 12:51:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o768ca3",
                  "author": "bidibidibop",
                  "text": "Local = models that have been already poisoned :).",
                  "score": 9,
                  "created_utc": "2026-02-24 17:39:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o73lmgu",
              "author": "Dry_Yam_4597",
              "text": "Good points. To be honest I have noticed that claude is gently pushing for commercial products instead of technical solutions. I suspect thats how their new stream of monetization will work and are ab testing.",
              "score": 31,
              "created_utc": "2026-02-24 07:39:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73nf58",
                  "author": "ThreeKiloZero",
                  "text": "Seems to me that it will strengthen models against injections and other attacks. It's not really far-fetched that they would develop measures to detect abuse, and no matter how we go there, it's now an arms race. ",
                  "score": 2,
                  "created_utc": "2026-02-24 07:56:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o74fcc1",
              "author": "gittubaba",
              "text": "So you see story of AI deleting codebase/database by \"accident\". Now you will see AI delete it deliberately because it thinks you're breaking their TOS. How nice",
              "score": 20,
              "created_utc": "2026-02-24 12:06:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o73wbq9",
              "author": "Dazzling_Focus_6993",
              "text": "This explains so much.Â ",
              "score": 19,
              "created_utc": "2026-02-24 09:20:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o74afhn",
              "author": "execveat",
              "text": "I just want to point out how incredibly ironic this is for a company that supposedly cares about the safety of AI in general, not just the performance of their own models.\n\nThey'd rather risk making competitor models misaligned than see them catch up.",
              "score": 12,
              "created_utc": "2026-02-24 11:28:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o75jo8d",
              "author": "TheOwlHypothesis",
              "text": "Okay I'm going to try to gently say this.   \n  \nThere seems to be a lot of both ignorance about what is possible detection wise, and also contradictory thinking about enforcement. \n\nWhich is it? Can they accurately detect a coordinated industrialized distilling attack? Or are they too incompetent and will poison random \"suspicious\" one-off requests?\n\nYou can't have it both ways. \n\nSecond, companies like this invest SO very heavily in the very best telemetry and logging and tracing that it's insane. Literally positions responsible for this easily pay upwards of 200k a year. I apply to jobs that are adjacent to this area so I've been seeing exactly what they want and expect from these systems.  \n  \nThey pay that much precisely because it's so damn valuable for both their actual business AND for when attacks like this happen. Like do you think they weren't aware this kind of thing could happen and didn't design their system to detect exactly that?",
              "score": 8,
              "created_utc": "2026-02-24 15:48:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79c61e",
                  "author": "Monkey_1505",
                  "text": "One is a AI driven filter at time of request, the other is a human driven analysis. Not the same. Although I would expect false positives for both approaches, obviously an AI filter is going to have more false positives.",
                  "score": 5,
                  "created_utc": "2026-02-25 02:55:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o78zcgb",
                  "author": "ResolveSea9089",
                  "text": "Just wanna say, as a layperson, very interesting to read this. I feel like a lot of this sub is viewing this almost politically (Not to say they're wrong), interesting to get some more nuance. ",
                  "score": 1,
                  "created_utc": "2026-02-25 01:42:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o79eexj",
              "author": "golmgirl",
              "text": "thatâ€™s fucked up. so false positives, which will occur, will result in paying users getting (presumably subtly) poisoned responses",
              "score": 3,
              "created_utc": "2026-02-25 03:07:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7avazx",
              "author": "Madrawn",
              "text": "Great, yesterday I was talking with the free Claude interface through some problems with an LLM training experiment I'm running, and like 3 times in a row a block of code it provides had a subtle flaw that when copied would have ruined the experiment without obvious errors, and I joked \"are you trying to sabotage my project?\" after the third.\n\nAnd now, while obviously it most likely was just my lazy ass using the free account on a too long context, I now have to be slightly paranoid that I got flagged as trying to weasel anthropics training pipeline out of Claude.\n\nBut each were failures I'm not expecting even of the free non-api version of claude. Stuff like \"better\\_thing = better\\_process(old\\_thing); ... return old\\_thing;\", or leaving out \"retain\\_graph=True\" on the last backward pass in a logging block that would have zero'd the gradients for the actual update right afterwards.\n\nStill I'd be kind of impressed if that actually was intentional and not just coincidence and bad luck. On the paranoid side again, Claude usually apologizes when making a mistake, but\n\n\\`\\`\\`  \nMe: Damn, you almost let me walk into a trap. <code> That isn't correct at all, we're not even changing `loss` like this.\n\nClaude: Ha, yes â€” `l_hard` is computed and then completely ignored. It never touches `loss`, which is still `sw * l_soft + hw * l_ce_soft` unchanged.\n\nThe actual change you want is...  \n\\`\\`\\`  \nI switched over to the gemini flash for the afternoon after that. But do I actually have to worry about \"User is a suspected chinese spy\" in the system prompt depending on what I ask? I'd like to have some information on the exact \"Countermeasures\"",
              "score": 1,
              "created_utc": "2026-02-25 10:05:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73jpel",
          "author": "Southern_Sun_2106",
          "text": "\"to specific researchers\", let this one sink in.",
          "score": 102,
          "created_utc": "2026-02-24 07:21:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74ylit",
              "author": "artisticMink",
              "text": "That's not as wild as it sounds. If you ever used any LLM via a web interface that includes google analytics and/or microsoft clarity, you're basically a block of glass to them. Even in their wildest dreams people underestimate what these tools can track and show (in real time).\n\nApi providers like OpenRouter are a little bit better, but they too deploy analytics and apply a unique ID to requests sent to inference endpoints. So it's really just a transparent user with one extra step.\n\nYes, your personal data is connected to that one goonprompt you're thinking about right now and yes your future employer might be able to see it or at least an evaluation of it.",
              "score": 38,
              "created_utc": "2026-02-24 14:04:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75clgq",
                  "author": "zimejin",
                  "text": "Yup, I recently had to add an observability tool to a project, and digging through the docs wasâ€¦ eye-opening. Turns out they can basically capture a userâ€™s screen in real time.\n\nAnd I donâ€™t mean literal screen recording that needs browser permission. I mean a simple Boolean toggle in the library, and suddenly you can replay the entire session visually. clicks, scrolling, UI changes, everything reconstructed. Sensitive fields get masked, but the page and behavior are fully replayable. This is an extremely well-known, popular web analytics tool, so itâ€™s not some proprietary feature of the project.\n\nHonestly, the level of visibility these tools have is wildâ€¦ and we all walk around thinking we have privacy. Yeah, we can replay your entire pornhub session, sir, to see where that bug occurred. ðŸ˜„",
                  "score": 16,
                  "created_utc": "2026-02-24 15:15:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o751txd",
                  "author": "Snoo_28140",
                  "text": "Yep, they can fingerprint you, connect that fingerprint to other instances of your sloppier use or to sloppier people in the vicinity and soon they have data you wouldnt believe possible.",
                  "score": 7,
                  "created_utc": "2026-02-24 14:21:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o75nhn1",
                  "author": "Zestyclose839",
                  "text": "Not to mention, if you're taking the official route of accessing it via Anthropic's developer portal or Bedrock, they require you to create an organization and/or describe your exact use cases, then enter a ton of personal information before you can make your first API call. They're the only provider on Bedrock that asks for anything like this.",
                  "score": 3,
                  "created_utc": "2026-02-24 16:05:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o78zs0e",
                  "author": "ResolveSea9089",
                  "text": "Well this is....depressing and a bit scary. Goddamnit",
                  "score": 1,
                  "created_utc": "2026-02-25 01:45:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o770x8c",
              "author": "Zeeplankton",
              "text": "This part doesn't whelm me, I mean of course, you have an email and phone number hooked up to an API, IP. Of course any API provider knows who you are, and where you are, if they're interested.\n\nBut it \\*is\\* interesting to me that they could possibly notice this is, in the literal ocean of billions of tokens being generated every second.",
              "score": 1,
              "created_utc": "2026-02-24 19:47:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o789ulv",
              "author": "grimjim",
              "text": "If the distillation datasets were subtly fingerprinted and then showed up in public datasets associated with a researcher, that could be a smoking gun. The ideal fingerprints would be a form of steganography,embedded within otherwise acceptable results.",
              "score": 1,
              "created_utc": "2026-02-24 23:22:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73oiwo",
          "author": "NandaVegg",
          "text": "They are pushing hard to frame this as if national security war incident for obvious regulatory capture/asking for public money reason, but it is just a corporate-to-corporate matter. At this point they are trying too hard. Admitting to poison the model output could backfire hard given their intended main customer base (coders) is more technically literate people than random chatbot user in average.\n\nUltimately, however, this is as silly as \"copy-protected\" music CD. Without sarcasm, being able to copy a state is Turing Machine's minimal requirement (without that you will only get Markov Chain at best, and that's why attention matters so much) and anybody who try to stop that will pay hefty degradation tax. If they are so concerned please just stop releasing model to public and only do private B2B.\n\nBut Claude is also really the best model available right now. I recommend to use Claude via Vertex AI (Bedrock has always been unstable and their infrastructure is half-broken) rather than direct API if you are concerned. Vertex AI has more strict zero retention policy than whatever weird policy Anthropic has.",
          "score": 69,
          "created_utc": "2026-02-24 08:06:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76skxz",
              "author": "dingo_xd",
              "text": "They will do everything they can to ban chinese models in America.",
              "score": 17,
              "created_utc": "2026-02-24 19:09:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bipjn",
                  "author": "The_frozen_one",
                  "text": "Eh, they are using FUD, which isnâ€™t new. Echos of MS spreading FUD about Linux.",
                  "score": 2,
                  "created_utc": "2026-02-25 13:07:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o79hc3t",
              "author": "boisheep",
              "text": "Honestly this seals it for me, a lot of people are using AI to write AI tools.\n\nIf the AI is giving bad answers on AI, then, that's kinda what Claude was good at.\n\nIt's fucking possible that all these supposed millions of accounts are just random people developing their ai agents at this point.",
              "score": 4,
              "created_utc": "2026-02-25 03:24:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7b75q1",
              "author": "AppealSame4367",
              "text": "Claude is the best model when it currently works. Opus 4.6 works and is mega expensive for bigger context and thinking, Sonnet 4.5 still works well but is nothing special anymore. The rest of the models work well when they feel like it. That's not good enough for the prices they ask.",
              "score": 2,
              "created_utc": "2026-02-25 11:48:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7uzyyo",
              "author": "Ambitious-Call-7565",
              "text": "\\> But Claude is also really the best model available right now. I recommend to use ClaudeÂ \n\nI got r\\_ped, but damn he was good at sex, the biggest cock in town\n\nI recomand getting r\\_ped too but wear a condom",
              "score": 1,
              "created_utc": "2026-02-28 10:53:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73pxfo",
          "author": "xrvz",
          "text": "> We are publishing this to make the evidence available to everyone with a stake in the outcome.\n\nWhat evidence? I don't see a big zip file anywhere with all the data.\n\n> Distillation attacks therefore reinforce the rationale for export controls: restricted chip access limits both direct model training and the scale of illicit distillation.\n\nYou desperately need more GPUs, and you see blocking others from getting them as a valid way.\n\nJust come out and say it, don't whore out your morals.\n\nI deeply regret the 5$ I've spent to access Anthropic's API.",
          "score": 98,
          "created_utc": "2026-02-24 08:19:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74lztk",
              "author": "simracerman",
              "text": "Donâ€™t regret the $5. Instead, speak up about Anthropicâ€™s bad practices everywhere - oftentimes, a vendorâ€™s bad reputation will catch up to them.",
              "score": 20,
              "created_utc": "2026-02-24 12:52:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73j2nj",
          "author": "-p-e-w-",
          "text": "â€œBy examining request metadataâ€â€¦ you mean like API keys tied to individual accounts that you can just look up in your database?\n\nSherlock Holmes at work here. They must have hired uber haxxors to unmask those diabolical â€œattackersâ€.",
          "score": 125,
          "created_utc": "2026-02-24 07:16:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7410l0",
              "author": "adityaguru149",
              "text": "Anthropic has a huge deal with Pentagon like other providers. If my data or prompts go outside my system then without any doubt they can be (read \"are being\") used for surveillance. This includes my IP address, MAC addresses, email id, credit card details, any details about me or my gf or my parents that AI agents leak including health records, etc. The act of using non-local models is a form of blessings from you to Pentagon, etc to put you under surveillance.\n\nI had read in some military analysis report that Pentagon is using p**n usage, subscription details and other details to set appropriate b**by traps. I'm sure the next Oopstein would become even more powerful due to data leaks by AI systems.\n\nThis is the reason why more open weight models are what r/Localllama thrives on.",
              "score": 16,
              "created_utc": "2026-02-24 10:05:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o73u9o3",
              "author": "obvithrowaway34434",
              "text": "Read the article; no researcher at these labs is stupid enough to use their own API key or something that can be easily traced back to them. They certainly have a lot of means to track accounts and, in this case, probably had outside help.",
              "score": 37,
              "created_utc": "2026-02-24 09:00:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74fyyk",
                  "author": "umbrosum",
                  "text": "Why do you make it sound like distillation is illegal?",
                  "score": 19,
                  "created_utc": "2026-02-24 12:11:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o73ulh6",
                  "author": "-p-e-w-",
                  "text": "Why wouldnâ€™t they use their own API keys? Do you think a Chinese court is going to enforce a US companyâ€™s ToS? Some of these ToSs may not even be enforceable in the US.",
                  "score": -5,
                  "created_utc": "2026-02-24 09:04:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o73okdk",
              "author": "mystery_biscotti",
              "text": "Okay, how does one trace that back through a reseller specifically? I guess I'm a bit behind on my cloud security knowledge, and you have me curious about it. ",
              "score": 4,
              "created_utc": "2026-02-24 08:06:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73pfhx",
                  "author": "-p-e-w-",
                  "text": "I imagine Anthropic requires resellers to forward that information. Some Anthropic models are BYOK-only IIRC.",
                  "score": 8,
                  "created_utc": "2026-02-24 08:14:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o79z7xr",
              "author": "cc88291008",
              "text": "The more scary underlying unsaid line is that, if they want, they could doxx you from the metadata if they want. Your conversation with will he used to identify you lol.\n\nDid anthropic just doxx their user and said that part loud? Lmao",
              "score": 1,
              "created_utc": "2026-02-25 05:22:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o73rn9j",
              "author": "Terrible-Priority-21",
              "text": "This is not the case, are you being intentionally d\\*mb or something? Those researchers knew that this was against Anthropic's policies. Why would they use their own API keys? Maybe read the article before commenting?",
              "score": -6,
              "created_utc": "2026-02-24 08:35:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o740y84",
                  "author": "deadcoder0904",
                  "text": "Are u intentionally d*mb or something? Anthropic knew how copyrighting on billions of people's work is illegal but still did it.",
                  "score": 4,
                  "created_utc": "2026-02-24 10:04:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73dv19",
          "author": "Lesser-than",
          "text": "distillation attacks, what kind of word salad is this.",
          "score": 147,
          "created_utc": "2026-02-24 06:30:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73k8p1",
              "author": "doodo477",
              "text": "Mummy someone stole my lunch money that I stole from someone else, can you tell him off.",
              "score": 84,
              "created_utc": "2026-02-24 07:26:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73uuaf",
                  "author": "Formal-Exam-8767",
                  "text": "You were a victim of wallet-raid attack.",
                  "score": 10,
                  "created_utc": "2026-02-24 09:06:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o73qlr2",
              "author": "Clear_Anything1232",
              "text": "Just don't want to outright say that they have a bad business model where anyone can easily duplicate their product.\n\nInstead they are clutching their regulatory pearls hard.",
              "score": 17,
              "created_utc": "2026-02-24 08:25:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o74arbj",
              "author": "pier4r",
              "text": "I am reading your post. Do you feel the distillation attack?",
              "score": 11,
              "created_utc": "2026-02-24 11:31:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o74m4sr",
              "author": "MuslinBagger",
              "text": "When you try to imitate your favorite artists, not their work, just their style. What you are doing is a \"distillation attack\". YOU DRINK THEIR MILKSHAKE!",
              "score": 3,
              "created_utc": "2026-02-24 12:53:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o79dzqu",
              "author": "tempstem5",
              "text": "are we making up attack terms now? here's mine: hypocrisy attack",
              "score": 1,
              "created_utc": "2026-02-25 03:05:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73g87g",
          "author": "Southern_Sun_2106",
          "text": "\"attacks\", \"ATTACKS\" - just look at that 'scary' word! I bet Claude Opus helped wordsmith this.",
          "score": 78,
          "created_utc": "2026-02-24 06:50:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76x0o6",
              "author": "NeuralNexus",
              "text": "How is paying for a product (AI answer to prompt) an attack? Come on. The framing is ridiculous. These AI companies scraped the internet to train in the first place. Now they care about permission? Come on.",
              "score": 5,
              "created_utc": "2026-02-24 19:29:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73ogq3",
          "author": "mtmttuan",
          "text": "Realistically what will they do? Push the US to ban Kimi and other Chinese lab? That will just make China win the AI war.",
          "score": 18,
          "created_utc": "2026-02-24 08:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74gces",
              "author": "Hoodfu",
              "text": "Probably that no company/person with a US presence would be allowed to host or support running Chinese models. It wouldn't stop things but it would make it difficult for the average joe to use them if huggingface stopped serving them and mlx and llama.cpp support for those models ended was no longer updated.",
              "score": 3,
              "created_utc": "2026-02-24 12:13:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79m4bm",
                  "author": "Southern-Chain-6485",
                  "text": "I don't know about mlx, as that requires Apple hardware, but llama.cpp would simply be forked and it would end up with a llama.cpp for those behind the Great American Moat, and a llama.cpp for the rest of the world, and the later will be the better of the two.",
                  "score": 1,
                  "created_utc": "2026-02-25 03:53:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o79e59i",
              "author": "tempstem5",
              "text": "\"ai war\" sounds like \"distillation attacks\"",
              "score": 1,
              "created_utc": "2026-02-25 03:06:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73jqk8",
          "author": "Evening_Ad6637",
          "text": "So what? Seriously.. ? whatâ€™s even the point.\n\n\nAt least those Chinese customers **do** pay for the information and knowledge they receive.\n\nAnd you anthropic, you do offer a crippled Claude API and take your money.\n\nCrippled API = no logits, not showing the reasoning behind it, no full explanation **what** actually happens there, no disclosure about **how much** has already been charged to the customer in your hidden blackboxâ€¦ \n\nTo me it looks like \"Stealing-Light\" and you literally telling your customers to just shut up and trust you blindly\n\nedit: typos",
          "score": 59,
          "created_utc": "2026-02-24 07:22:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73rdo9",
              "author": "Savantskie1",
              "text": "I agree with everything you said, but you can still read the thought process. Itâ€™s not hard to find on Claude ai",
              "score": -5,
              "created_utc": "2026-02-24 08:33:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73w9rd",
                  "author": "Evening_Ad6637",
                  "text": "Nope, unfortunately thatâ€™s not correct. Claude-Sonnet-3.7 was the only one where you could see the whole reasoning process.\n\n* You only get a summary\n* they donâ€™t tell you how extended it was\n* so nowhere something like a proof\n* but you have to pay the bill\n* to make matters worse the summary is written by smaller models\n\nhttps://preview.redd.it/w3j5bukwselg1.jpeg?width=1284&format=pjpg&auto=webp&s=ccab12e278d0d9231c5b51d3b095708cfeb355eb\n\nAnthropic is basically repeating the same bullshit as OpenAI last year, when sam altmann told the world that Deepseek would \"steal\" the thought process of gpt-o1, without mentioning that this was impossible, since o1 didnâ€™t show anything, not a single token of its thought process",
                  "score": 18,
                  "created_utc": "2026-02-24 09:20:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73jz89",
          "author": "llama-impersonator",
          "text": "this is why everyone hates anthropic, they whine about AI safety while doomhyping about basic bitch things. dad, the chinese proompted my model too hard!",
          "score": 50,
          "created_utc": "2026-02-24 07:24:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73il8r",
          "author": "inconspiciousdude",
          "text": "What a well worded whine. I wonder how they're going to cripple their models to stop these types of research.",
          "score": 30,
          "created_utc": "2026-02-24 07:11:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73qzqo",
          "author": "Stunning_Macaron6133",
          "text": "As if Anthropic doesn't read these companies' research papers or examine their models.\n\nHypocrisy.",
          "score": 30,
          "created_utc": "2026-02-24 08:29:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79e6x5",
              "author": "tempstem5",
              "text": "hypocrisy attacks",
              "score": 1,
              "created_utc": "2026-02-25 03:06:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o747alg",
          "author": "Vaddieg",
          "text": "They just publicly admitted the fact that Chinese models aren't inferior architecture and method-wise, and only the quality of training data matters",
          "score": 12,
          "created_utc": "2026-02-24 11:02:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73lgdx",
          "author": "Dry_Yam_4597",
          "text": "They've always been on the dystopian side of things. The billionaire CEO tells people to feel worthless on a daily basis and some masses cheer. It doesnt get that must dystopian than this.\n\nAlso they analised \"metadata\"? What is that \"metadata\"? An HTTP request header? Are they adressing to easily impressionable folks? Are they daft?",
          "score": 26,
          "created_utc": "2026-02-24 07:38:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73dcgo",
          "author": "FundusAnimae",
          "text": "Yeah, only metadata I'm sure ðŸ¤¡",
          "score": 34,
          "created_utc": "2026-02-24 06:26:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73tk6v",
          "author": "Monad_Maya",
          "text": "Somehow Anthropic is the worst of the lot. I hope their Chinese competitors beat them at their game.\n\nOSS models do lag behind the frontier ones by a fair bit regardless of what the benchmarks have you believe. We've come very far in the last few years though.\n\nOSS FTW!",
          "score": 21,
          "created_utc": "2026-02-24 08:54:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73fzkq",
          "author": "wdwind",
          "text": "damn",
          "score": 8,
          "created_utc": "2026-02-24 06:48:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o744ael",
          "author": "IngwiePhoenix",
          "text": "Anthropic stole from everyone and gatekept it behind money.\n\nSo if chinese labs steal from them and give us open weights, then, honestly...\n\n**Distill harder.**",
          "score": 14,
          "created_utc": "2026-02-24 10:35:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73okur",
          "author": "tengo_harambe",
          "text": "imagine crying because people pay for your goods and services at the price YOU set",
          "score": 35,
          "created_utc": "2026-02-24 08:06:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74lw1c",
              "author": "theshitstormcommeth",
              "text": "â€¦and based on your Terms of Serviceâ€¦",
              "score": 6,
              "created_utc": "2026-02-24 12:51:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73sa3i",
          "author": "a332bb42",
          "text": "Now Iâ€™m even more convinced getting two 6000 rtx and run minimax â€¦Â ",
          "score": 7,
          "created_utc": "2026-02-24 08:42:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74ahp8",
          "author": "RevealIndividual7567",
          "text": "Anthropic tends to really oversell literally anything that comes out from their company, even small stuff like blogs or their commitment to not putting ads.",
          "score": 6,
          "created_utc": "2026-02-24 11:29:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73mz23",
          "author": "pier4r",
          "text": "\"metadata\" my ass. I strongly believe that AI labs are training on the prompt (and answers) that they get, excluding those from customers with deep pockets for legal battles. A sort of \"cambridge analytica\" but for prompts.\n\nI mean, they trained on copyrighted works without batting an eye, why should they care about normal customers?\n\nThose conversation helps a ton to improve the training dataset.\n\nHence I believe they could identify the prompts and thus identify the companies. Same for openAI and xAI when they got blocked.",
          "score": 13,
          "created_utc": "2026-02-24 07:52:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73ze3j",
          "author": "hidden2u",
          "text": "We stood on the shoulders of giants in order to attack other slightly smaller giants",
          "score": 6,
          "created_utc": "2026-02-24 09:50:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o752fo3",
          "author": "PunishedDemiurge",
          "text": "Good for Moonshot et al.\n\nAs long as they are not abusing free trial periods, I think any AI company should have an absolute legal right to be a paid customer of any other one and use any / all of the outputs as synthetic training material if they wish to.\n\nHumanity benefits from having a wide and fair playing field. I don't want a single monopoly to use regulatory capture and rest on its laurels to slow progress for all of humanity, I want a robust competition where improvements are expected every few months.",
          "score": 6,
          "created_utc": "2026-02-24 14:24:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76zoqw",
          "author": "Zeeplankton",
          "text": "It really grates me that Anthropic still remains frontier, even after 2 years. They seem so much more shady than OpenAI",
          "score": 7,
          "created_utc": "2026-02-24 19:41:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73m17v",
          "author": "Linkpharm2",
          "text": "Oh no, somebody paid for our service......Â ",
          "score": 17,
          "created_utc": "2026-02-24 07:43:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o743ibl",
          "author": "adalgis231",
          "text": "Now I wanna know where \"distillation attacks\" (as they call them) are considered crimes. In any case, stealing pirated books is a crime, instead",
          "score": 5,
          "created_utc": "2026-02-24 10:28:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75khzn",
          "author": "Sea-Sir-2985",
          "text": "the poisoning part is what got me... the idea that they'd intentionally degrade outputs for specific users rather than just blocking them is concerning regardless of the justification. any api user has to now wonder if their outputs are being selectively degraded based on some internal classification they have no visibility into\n\nthe push for export controls in a blog about corporate IP theft felt like a stretch too, those are two very different conversations and bundling them together weakens both arguments",
          "score": 4,
          "created_utc": "2026-02-24 15:51:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73pp5d",
          "author": "RevolverMFOcelot",
          "text": "Wow this actually makes me want to sub to Kimi just to support them and use their API to run Kimi K2.5 (since my computer is not strong enough to run it locally lol) because wtf is this anthropic?? At least these open source entities PAID for your API and actually gives back to the world by open sourcingÂ \n\nedit: Yeah corporate intention is rarely pure but i will take any damn open source i can get Kimi k2.5 has been amazing so far",
          "score": 14,
          "created_utc": "2026-02-24 08:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o758gmn",
              "author": "arcanemachined",
              "text": "I think they're doing it less for your benefit, and more to undermine their competition.\n\nI mean, don't get me wrong, it's nice that our incentives are aligned here (if temporarily), but let's not be naive about what's happening here.",
              "score": 2,
              "created_utc": "2026-02-24 14:55:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77fqc7",
                  "author": "RevolverMFOcelot",
                  "text": "Yeah corporate intention is rarely pure but i will take any damn open source i can get Kimi k2.5 has been amazing so far ",
                  "score": 3,
                  "created_utc": "2026-02-24 20:56:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73xb06",
          "author": "Large_Solid7320",
          "text": "Even if all those accusations are 100% accurate (which they likely are), forcing the large Chinese labs into a battle over who can come up with a more valuable (comprehensive, well-curated, 'censored' along the labs' respective goals and legal requirements) training set feels like a pretty dumb move.",
          "score": 4,
          "created_utc": "2026-02-24 09:30:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73yz7m",
          "author": "NoobMLDude",
          "text": "Not surprised at all.  \n\nThe movement for using local AI already started when models were able to run locally. If you know anything about Tech you know what a Privacy leak propritary AI models of Anthropic, OpenAI, others are. \n\n\nYou share everything about you. These companies know more about us in past few years than what Google could know in past few decades.\n\n\nWe wonâ€™t compete with big labs with huge budgets in terms of performance but for most people Local AI models can support all of their needs. \n\n\nI try to make it easy for anyone struggling to setup and use local AI models and tools. Have a watch itâ€™s not too hard. \n\n[Local AI playlist ](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV)\n\nIf itâ€™s still hard, let me know and Iâ€™ll try to make it simpler.",
          "score": 4,
          "created_utc": "2026-02-24 09:46:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o749lyu",
          "author": "LanternOfTheLost",
          "text": "Instead of crippling or disabling a service, they chose to poison it.\n\nThatâ€™s interesting for people not â€œalignedâ€ with US interests, e.g. anyone the White House disagrees with.",
          "score": 3,
          "created_utc": "2026-02-24 11:21:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75wwru",
          "author": "hejj",
          "text": "Free data for me, and not for thee",
          "score": 4,
          "created_utc": "2026-02-24 16:47:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o765fdk",
          "author": "_bones__",
          "text": "In the mean time, Claude Sonnet 4.6 identifies itself as Deepseek if you ask who it is in Chinese. So this seems a little disingenious.\n\n<insert Scooby Doo meme here>",
          "score": 3,
          "created_utc": "2026-02-24 17:25:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76f4ax",
          "author": "Antique_Archer_7110",
          "text": "After reading about poisoning the outputs I have canceled now my Claude subscription.   \nWhat if they decide to poisons the results i get for whatever reason?  \nI could accept blocking access to latest flagship model like openai does but this is not acceptable.\n\nI also have a machine with minimax 2.5 @/Q4 that i will start using more often",
          "score": 4,
          "created_utc": "2026-02-24 18:09:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76n4a6",
          "author": "Ticrotter_serrer",
          "text": "there is no honor among thieves. ",
          "score": 4,
          "created_utc": "2026-02-24 18:44:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77iwwe",
          "author": "mayalihamur",
          "text": "Anthropic is a shady company based in an authoritarian country where freedoms are crushed under the boots of a shady regime of paedophilic billionaires with no accountability.\n\n\nExpect more: They will use this experience to create algorithms that detect dissident users and slowly poison their minds, make obedient human beings of them.Â \n\n\nThey will intentionally distort people's perception of reality, run small scale cognitive tests on small groups of people to see how they behave in the long term and discover patterns.",
          "score": 3,
          "created_utc": "2026-02-24 21:10:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73ome0",
          "author": "robberviet",
          "text": "Lol the panic. You logged in, using API of course they know which acc it is.",
          "score": 6,
          "created_utc": "2026-02-24 08:07:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76fih6",
          "author": "hailsatan666xoxo",
          "text": "anybody asked anthropic where they got their data from? i'm sure it was all properly paid for?",
          "score": 3,
          "created_utc": "2026-02-24 18:11:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77bvcz",
          "author": "queerintech",
          "text": "In my opinion Altman is as big of a brain addled  douchebag as Musk and I'll never support either company.  \n\nIt's surprising all these folks here are cheering for a race to the bottom in AI.. with corporate espionage and state sponsored extraction of trained model data, and chain if thought.. future is gonna get dark af.   Nobody will be investing in high quality training anymore.",
          "score": 3,
          "created_utc": "2026-02-24 20:38:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77ujiz",
          "author": "Shingikai",
          "text": "This is exactly why I'm increasingly running important queries through multiple models from different providers (including local ones).\n\nWhen you compare how GPT-4, Claude, Llama, and Qwen respond to the same prompt, you see the guardrails are *wildly* inconsistent across models. What Claude refuses to discuss, Llama might handle pragmatically. What GPT-4 sanitizes, Qwen might answer directly.\n\nIt really drives home that there's no single trustworthy model â€” just different corporate/policy filters. Running a council of diverse models (local + API) is the closest I've found to getting actual answers.",
          "score": 3,
          "created_utc": "2026-02-24 22:04:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78hvwn",
          "author": "papertrailml",
          "text": "the poisoning part is what gets me. like ok sure protect your model weights or whatever but actively sending wrong outputs to paying customers? thats just sabotaging your own product lol. good luck keeping enterprise trust after admitting that",
          "score": 3,
          "created_utc": "2026-02-25 00:06:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73soc1",
          "author": "charmander_cha",
          "text": "Sempre bom ressaltar que empresas americanas sÃ£o parceiras do imperialismo americano",
          "score": 5,
          "created_utc": "2026-02-24 08:45:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73rbxo",
          "author": "jwpbe",
          "text": "capitalism breeds innovation, just look -- this is gayer than anything I do on a daily basis as a lesbian. congrats, dario, you are pushing the gayreto frontier",
          "score": 9,
          "created_utc": "2026-02-24 08:32:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73t7t5",
          "author": "Dangerous-Reveal2119",
          "text": "Anthropic's actually happy that open source labs are still \"distilling\" from it they'll be absolutely shitting their pants if they suddenly stop",
          "score": 3,
          "created_utc": "2026-02-24 08:50:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73v4m0",
          "author": "Rondaru2",
          "text": "I certainly will - once 1TB VRAM GPUs become affordable for the average consumer.",
          "score": 2,
          "created_utc": "2026-02-24 09:09:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74io0e",
          "author": "MuslinBagger",
          "text": "I know I shouldn't be talking to that smart model from openai, but my local model is such a fucking retard. I need someone cool and hip like claude so I can tell them my deepest, darkest secrets.",
          "score": 2,
          "created_utc": "2026-02-24 12:30:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77at46",
          "author": "No_Revolution1284",
          "text": "Ah yes, the distillation attack. What about the practical DDoS-ing you do daily to like... every website ever just to scrape the newest images and text?",
          "score": 2,
          "created_utc": "2026-02-24 20:33:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77clug",
          "author": "queerintech",
          "text": "Honey pots are standard procedure when dealing with these types of data harvesting.  Google caught Bing doing the same thing in 2011.  They created a honey pot linking 100 nonsensical search terms to completely u related web pages.  And bing eventually started returning those same random pages for the gibberish terms.",
          "score": 2,
          "created_utc": "2026-02-24 20:41:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o747t5r",
          "author": "landed-gentry-",
          "text": "Scary and dystopian? Censorship and authoritarian? C'mon dude. They\nprobably just looked up the IP addresses that made the requests and found their geolocation. Anyone who's been a web admin will have done this.",
          "score": 2,
          "created_utc": "2026-02-24 11:06:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73sujq",
          "author": "dobkeratops",
          "text": "I'd be worried inherently about this:\n\n\\[1\\] current LLMs are trained on data widely available on the internet,\n\n\\[2\\] but as 'dead internet theory' plays out, future data is the user interactions with AI companies, i.e. closed data, and public data stagnates.\n\n\\[3\\] eventually trained on that, AI companies will be able to bypass the user (i.e. train AI to 'prompt itself' for any. meaningful tasks), at that point they can cut the extraneous part (you) out.",
          "score": 1,
          "created_utc": "2026-02-24 08:47:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77k3bb",
              "author": "RickAmes",
              "text": "A closed ouroboros of shit, eating it's own poop forever.",
              "score": 2,
              "created_utc": "2026-02-24 21:16:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77skxv",
                  "author": "a_beautiful_rhind",
                  "text": "The returns are already bad *now*. Each new model is like the next because of the data centipede.",
                  "score": 1,
                  "created_utc": "2026-02-24 21:55:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77wdwe",
          "author": "Square_Empress_777",
          "text": "Hi, Iâ€™m a non-tech, non-coding guy. What does all of this mean? Can someone explain this like Iâ€™m 5? Does this mean Claude is likeâ€¦ spying on me for the government or something? What is the scary part? I was thinking of switching to Claude from ChatGPT after they canned o4.",
          "score": 1,
          "created_utc": "2026-02-24 22:13:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78cx8l",
              "author": "RightWordsMissing",
              "text": "For API use purposes, this means that they now have an internal filter that scans any request you send to the model to see if they think it fits some criteria they've set up to determine if the end-user is trying to use the model's response for training their own model. If the filter determines the answer is 'yes', then they'll intentionally make the response it sends back particularly awful.\n\nIn practice their crusade against model distillation just means that they're making open source software rarer, making independent cutting edge projects more difficult, and sequestering cutting-edge AI for internal corporate use only. It's 'scary' in the sense that it's a depressing result of late-stage capitalism.",
              "score": 3,
              "created_utc": "2026-02-24 23:38:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78hl83",
                  "author": "Square_Empress_777",
                  "text": "Is API different than using the website? I think thats like a technical way to use LLMs, right?",
                  "score": 1,
                  "created_utc": "2026-02-25 00:04:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o79aoae",
          "author": "coolguysailer",
          "text": "Can anyone figure out why Claude continues to be so much better? It seems pretty clear that itâ€™s not just better data right?",
          "score": 1,
          "created_utc": "2026-02-25 02:46:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7alm6y",
              "author": "Beginning-Foot-9525",
              "text": "Data theft, the best coders in the world correct it, and it learns from em. I mean you have it in this blogpost, Metadata is the key.",
              "score": 2,
              "created_utc": "2026-02-25 08:34:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7agdy0",
          "author": "R_Duncan",
          "text": "Data wasn't strictly on privacy mode? How did they tracked down requests up to specific labs then????!?",
          "score": 1,
          "created_utc": "2026-02-25 07:46:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7babgr",
          "author": "InsensitiveClown",
          "text": "Oh noes! They're using our models to distill, but not us. We never do that. We train. It's training when we do it, distill when the adversary does. Pirate and train. Pirate, and pay up to avoid the copyright infringement clauses now that we know there is a valid business model here, but not before we mass pirated everything.\nThe cynicism and hypocrisy are stomach churning.",
          "score": 1,
          "created_utc": "2026-02-25 12:11:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ed12x",
          "author": "Historical-Camera972",
          "text": "Incoming Counter Measures: Self Poisoning Defense\n\nWhen models are able to identify or are given a list of attackers, they will intentionally poison their outputs to fudge the training of the attackers.\n\n  \nCalling it now, this is the fastest solution. Having a model fall on a lobotomy knife as soon as an attack is detected.",
          "score": 1,
          "created_utc": "2026-02-25 21:14:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7s4ikz",
          "author": "FPham",
          "text": "Darn, they are probably losing the moat. This is the typical reaction of companies that are losing edge - blame their failure on other \"external\" things.  \nThey apparently can train their models on \"whatever we want because it's fair use\" and keep it secret. But no, you can't train your model on theirs. That's just absurd and a total violation of fair use!!!! Nooooooo!  \nLike WTF, All the Chinese models post their papers and brag how they used a lot of synthetic data. So where the synthetic data came from, genius? A synthetic land far, far away?\n\nThe problem is, when Anthropic or whoever gives you acces to their models, they are also giving you the key to the castle. If they want to have a good model - that model will ultimately be able to build a competitor to Anthropic. Or is it that only they can disrupt others business with their AI, but when it comes back it's crying on unfairness? You can't have it both ways.  \nI do like Sonet and Opus, they are still the best, but \"best\" is a difference between 99% and 89.5% and I think they are aware of it.  I do actually use Codex rn, because of their \"get hooked on LSD\" policy. It says I'm 100% off my  weekly limit, yet it still works, LOL. On Opus I'm dead in 20 min. On Sonet in 1 hr.",
          "score": 1,
          "created_utc": "2026-02-27 22:24:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73mwuh",
          "author": "ieatdownvotes4food",
          "text": "I mean they're gonna protect their special sauce. but whatever, local models tend to be months behind.. all good",
          "score": 1,
          "created_utc": "2026-02-24 07:51:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73ncah",
          "author": "Deep_Traffic_7873",
          "text": "# Anthropic is desperate after censoring OpenClaw",
          "score": -4,
          "created_utc": "2026-02-24 07:55:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74mvu8",
          "author": "eworker8888",
          "text": "want to use local open-weight models :)  ????  welcome to eworker, we connect to 400+ of them [https://eworker.ca](https://eworker.ca)  designed for privacy!",
          "score": -4,
          "created_utc": "2026-02-24 12:57:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd2x61",
      "title": "People are getting it wrong; Anthropic doesn't care about the distillation, they just want to counter the narrative about Chinese open-source models catching up with closed-source frontier models",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/1ulaheylwclg1.png",
      "author": "obvithrowaway34434",
      "created_utc": "2026-02-24 02:54:22",
      "score": 799,
      "num_comments": 139,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rd2x61/people_are_getting_it_wrong_anthropic_doesnt_care/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o738l55",
          "author": "Sagyam",
          "text": "Who cares if it's distilled, fermented, brewed. As long as they keep releasing open weight sota models or try something new its all good. If you think they only do distillation then read these papers.\n\n\\- [DeepSeek-OCR](https://arxiv.org/pdf/2510.18234), [mHC](https://arxiv.org/pdf/2512.02556), [DeepSeek Sparse Attention](https://arxiv.org/pdf/2512.02556)  \n\\- [Muon Clip Optimizer and agentic post training](https://arxiv.org/pdf/2507.20534)   \n\\-  [Lightning Attention](https://arxiv.org/pdf/2501.08313)  \n\\- [Qwen3 Omni Multimodality](https://arxiv.org/pdf/2509.17765)",
          "score": 109,
          "created_utc": "2026-02-24 05:46:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76ekzi",
              "author": "Fault23",
              "text": "\\+ releasing game changer detailed research papers frequently",
              "score": 29,
              "created_utc": "2026-02-24 18:07:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78hv4x",
                  "author": "drinknbird",
                  "text": "But there's no way Anthropic would read, \"distill the information, and benefit from these papers! They've built their models from scratch and definitely just happened to build large language models at the same time as everyone else from their own ideas! /s",
                  "score": 11,
                  "created_utc": "2026-02-25 00:06:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o72kw7a",
          "author": "Ok_Knowledge_8259",
          "text": "I mean didn't DeepSeek release R1 before Anthropic had anything? and in relatively short order behind OpenAI. \n\nIf they were just distilling, Anthropic would've beat deepseek to the punch but they didn't. \n\nIt's clear there really isnt any great MOAT, it's just clean data, more data, and RL. Scale those 3 up and you get better models.\n\nSure there might be some things unknown in there but the chinese seem to be doing just fine. It's also the case that we haven't seen any open source in America or Europe coming remotely close to what the Chinese are doing.\n\nArguable seed dance is SOTA in video right now and thats clear innovation.",
          "score": 314,
          "created_utc": "2026-02-24 03:03:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72mrox",
              "author": "Lissanro",
              "text": "Yes, DeepSeek R1 release made quite an impact. It is also interesting that there is evidence that Anthropic distilled the DeepSeek model -Â [https://www.reddit.com/r/DeepSeek/comments/1r9se7p/claude\\_sonnet\\_46\\_distilled\\_deepseek/](https://www.reddit.com/r/DeepSeek/comments/1r9se7p/claude_sonnet_46_distilled_deepseek/)Â \\- and more than that, DeepSeek also innovate architecture and training methods, as well as in terms of training - and they open sourced a lot, published research papers with actual details.\n\nMeta tried to copy DeepSeek architecture, Mistral also released something based on DeepSeek architecture, but Moonshot was very successful on improving upon DeepSeek work, which shows how important open research is.",
              "score": 150,
              "created_utc": "2026-02-24 03:15:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73v2zu",
                  "author": "wanderer_4004",
                  "text": "I think that is exactly why Dario now comes out with this accusation. It is just what their PR specialists told them to do - make a counter accusation and make sure all mainstream media prints the Anthropic version. Journalists don't know that for a real distill you need access to the lower layers of the model and not just some synthetic output. Also a few million examples is nothing for a 1000B model. \n\nIf someone indeed used distillation, then it can only be Anthropic - because they can use the open weight models for doing so. And likely they did for Chinese training data - because certainly the Chinese companies will have better training data for Chinese.",
                  "score": 25,
                  "created_utc": "2026-02-24 09:08:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o73s04w",
                  "author": "stddealer",
                  "text": "Training on synthetic data from another model doesn't necessarily mean distillation. This goes both ways btw.",
                  "score": 10,
                  "created_utc": "2026-02-24 08:39:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o73sc47",
                  "author": "SeaBat2035",
                  "text": "Mistral was the first with MOE. DeepSeek borrowed that idea... don't change the narrative.",
                  "score": -1,
                  "created_utc": "2026-02-24 08:42:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72piuq",
                  "author": "Howdareme9",
                  "text": "that isnt evidence, llms hallucinate that all the time\n\n  \ni would genuinely say that deepseek is the only chinese lab that can compete w/o distillation though ",
                  "score": -40,
                  "created_utc": "2026-02-24 03:32:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72qayl",
              "author": "porkyminch",
              "text": "I think even if they *were* just distilling Claude, if Minimax or Deepseek or whoever has a comparable product to Claude Sonnet (let alone Opus), that's kinda huge. Minimax has a 100 prompt/5 hours plan for $100 *a year*. An equivalent plan from Anthropic costs that much per *month*. If you can get 80-90% of the performance for 1/12th the cost, that's gonna be more than worth it for a lot of people. \n\nIf they're beating out the US labs on cost, that's still leapfrogging them imo. There are plenty of use cases out there that people are hungry for tokens and unable or unwilling to pay Claude/ChatGPT prices.",
              "score": 39,
              "created_utc": "2026-02-24 03:37:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o733rqc",
                  "author": "findingmike",
                  "text": "Faster tech cycle = faster race to the bottom on price.",
                  "score": 12,
                  "created_utc": "2026-02-24 05:09:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o739wsk",
                  "author": "jamfold",
                  "text": "Chinese labs have a hit job to do. Find a US lab that is about to make big money, then distill their model to crash prices so that they never make that money.\n\nIt's a happy race to the bottom",
                  "score": -5,
                  "created_utc": "2026-02-24 05:57:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72pm39",
              "author": "polytique",
              "text": "DeepSeek used ChatGPT models as well to bootstrap their reasoning model. They went through an Azure API. \n\nhttps://www.reuters.com/world/china/openai-accuses-deepseek-distilling-us-models-gain-advantage-bloomberg-news-2026-02-12/\n\nhttps://www.reuters.com/technology/microsoft-probing-if-deepseek-linked-group-improperly-obtained-openai-data-2025-01-29/",
              "score": 44,
              "created_utc": "2026-02-24 03:32:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o740lae",
                  "author": "Cuplike",
                  "text": ">DeepSeek used ChatGPT models as well to bootstrap their reasoning model.\n\nCompletely baseless claim. R1 was the FIRST model, closed or open source to fully show the reasoning tokens to the end user. They couldn't have copied ChatGPT even if they wanted to because o1 only showed a very brief summary of what the CoT tokens were",
                  "score": 20,
                  "created_utc": "2026-02-24 10:01:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72y0oa",
                  "author": "StillVeterinarian578",
                  "text": "Maybe, but accusations != proof.\n\nBesides, even if they did, it's Just Robin Loxely robbing from the Sheriff of Nottingham.",
                  "score": 15,
                  "created_utc": "2026-02-24 04:28:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o73h8fq",
                  "author": "iaNCURdehunedoara",
                  "text": "OpenAI accusing deepseek doesn't mean anything considering that they have half a trillion dollars spending to justify.",
                  "score": 1,
                  "created_utc": "2026-02-24 06:59:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72nt8n",
              "author": "Front_Eagle739",
              "text": "To be fair mistrals devstral models are very strong for their size, especially if you force reasoning on. We just dont have half a dozen different well funded companies giving us options.\n\n\nWill grant you the video gen though. Nothing anywhere close to seed dance",
              "score": 10,
              "created_utc": "2026-02-24 03:21:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o72vuwl",
              "author": "Far-Low-4705",
              "text": "GLM is very likely to have distilled claude.\n\nIf you copy and paste claude's system prompt, it behaves identical to claude, which is something most models do not do. it will even tell you about tiananmen square.",
              "score": 6,
              "created_utc": "2026-02-24 04:13:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7379nx",
                  "author": "jackmusick",
                  "text": "Both GLM and Kimi claimed to be â€œClaude from Anthropicâ€ until very recently. Iâ€™m pretty sure Kimi still does. On top of China having a repeated track record of blatantly copying American tech for as long as Iâ€™ve been alive, even so much to have guys copying Steve Jobsâ€™s look, Apple Stores, keynotes and devices down to the last detail. Why are people going out of their way to pretend this isnâ€™t likely?\n\nYou can dislike these companies all you want and even not care how much they get ripped off. Letâ€™s not pretend itâ€™s not real. Everyone can suck for different reasons at the same time.",
                  "score": 0,
                  "created_utc": "2026-02-24 05:36:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72pv0e",
              "author": "davikrehalt",
              "text": "\"If they were just distilling, Anthropic would've beat deepseek to the punch but they didn't.\" I disagree with your argument -- it doesn't contradict the hypothesis that deepseek distilled from OAI while anthropic was behind. Not even weighing on this hypothesis on either side; I'm just saying logically it doesn't follow",
              "score": 8,
              "created_utc": "2026-02-24 03:34:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o732vq1",
                  "author": "dark-light92",
                  "text": "Deepseek couldn't as there was nothing to distill from. Openai didn't show reasoning traces to public.",
                  "score": 14,
                  "created_utc": "2026-02-24 05:03:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o73n1rj",
              "author": "corruptboomerang",
              "text": "I'm still surprised none of (or all of) the major universities haven't 'partnered' with the AI companies they have massive amounts of good Pre-AI training data in student assignments etc. And can provide more (well not entirely AI-free).",
              "score": 1,
              "created_utc": "2026-02-24 07:52:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75lsr5",
                  "author": "OftenTangential",
                  "text": "Pretty sure that's a FERPA violation",
                  "score": 1,
                  "created_utc": "2026-02-24 15:57:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72n3xp",
              "author": "sparkandstatic",
              "text": "Yeah there is no research in the fields, different algorithms donâ€™t scale up to differences,\nonly engineering and data cleaning, \nand your limited closed knowledge.",
              "score": -6,
              "created_utc": "2026-02-24 03:17:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o72tgri",
                  "author": "Old-School8916",
                  "text": "there is plenty of research, but most of it not applicable. ",
                  "score": 4,
                  "created_utc": "2026-02-24 03:57:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o72qzkh",
          "author": "Nyxtia",
          "text": "News flash. Its distillation all the way down. \n\nHumans to AI Model to Ai Model to ai model...",
          "score": 21,
          "created_utc": "2026-02-24 03:41:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72r9ro",
          "author": "swagonflyyyy",
          "text": "I swear to god Anthropic is more passive-aggressive than Sam is.",
          "score": 51,
          "created_utc": "2026-02-24 03:43:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72nnan",
          "author": "awebb78",
          "text": "Spoken like a true Anthropic stooge.  Saying that the Chinese Labs have no innovation proves this guy's braincells aren't functioning correctly.  I've read quite a few papers from Chinese Labs and they do indeed come out with innovative discoveries, not just in AI models, but also in robotics.  Anthropic people are really full of themselves.",
          "score": 180,
          "created_utc": "2026-02-24 03:20:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o736fkb",
              "author": "amandalunox1271",
              "text": "Seems primarily strategic in their pettiness, given the timing. They do this before big releases to undermine opponents. Previously it was the 5.3 release from OAI and now it's the imminent v4 from DeepSeek. Worse of all their narratives are manipulative and aimed primarily at their dumber users.\n\nBut now even their 4.6 Sonnet is showing significant signs of distilling from GPT5.2's output. Ironic how this little diss is coming out at the same time.",
              "score": 24,
              "created_utc": "2026-02-24 05:29:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o736zuu",
                  "author": "awebb78",
                  "text": "Yep, exactly.  They have always been a petty company with petty founders that see themselves as better than everybody else.  I like how Theo Browne put it on Youtube.  They behave like a cult.  He likened them to the cult of Scientology, and this line of thinking makes so much sense.",
                  "score": 12,
                  "created_utc": "2026-02-24 05:34:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72paan",
              "author": "-p-e-w-",
              "text": "Indeed. â€œNo innovationâ€ is laughable. DeepSeekâ€™s papers have dwarfed Anthropicâ€™s in importance over the past year, in particular their novel attention mechanisms. Most of Anthropicâ€™s publications are thinly-veiled ramblings about how dangerous AI is.",
              "score": 97,
              "created_utc": "2026-02-24 03:30:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7g34lc",
              "author": "agent00F",
              "text": "Somebody actually asked this clown whether he'd go on the record that anthropic had never distilled other company's models, and of course crickets.",
              "score": 3,
              "created_utc": "2026-02-26 02:44:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o72pktz",
              "author": "Altruistwhite",
              "text": "They (Anthropic) do have a sota model (Opus 4.6) though",
              "score": 9,
              "created_utc": "2026-02-24 03:32:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o72r588",
                  "author": "awebb78",
                  "text": "I'm definitely not saying that they don't have a good model, but saying that Chinese Labs have no innovation and can only copy off of distilled responses is just ludicrous.  In fact Chinese Labs are catching up to the American labs so fast that I fully expect them to overtake the American Labs in a year or two, especially with all the resources and government support being poured into these labs by the Chinese government.  \n\nWe are coming to the end of the scaling era so it is going to come down to true research capabilities and we really know nothing about Anthropics research prowess because they never publish meaningful research.  The Chinese Labs publish a shitload of research and are doing more with less, which will be where the real AI future lies. The Chinese Labs are now catching up with the American Labs on really meaningful benchmarks and use cases, so these proprietary model companies (maybe with the exception of Google because they have a massive data, application ecosystem, and hardware moat) are starting to feel the pressure. And investors can't keep pouring the same amounts of money that they have before.\n\nI believe Anthropic is coming out with all of this \"AI is dangerous\" and \"everybody is ripping off our models\" messaging because they want to heavily regulate AI to protect their market position.  Anthropic is fucked long term, and I believe they know it.",
                  "score": 52,
                  "created_utc": "2026-02-24 03:42:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o733fpt",
              "author": "aeroumbria",
              "text": "If they hadn't helped prop up the joker in charge of the US right now, many of the exact same scientists in Chinese labs could very well be working for them instead.",
              "score": 6,
              "created_utc": "2026-02-24 05:07:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o733r6y",
                  "author": "awebb78",
                  "text": "I partly agree.  It certainly would have made it easier to work for them.  But I also think there are many that are driven to work on open ecosystems, and Chinese tend to be more collectivist than we Americans who are driven by individualism.  All of our model companies are into hoarding wealth, power, data, research, basically everything.",
                  "score": 5,
                  "created_utc": "2026-02-24 05:09:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o72rpf8",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 30,
          "created_utc": "2026-02-24 03:45:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73e26n",
              "author": "badabummbadabing",
              "text": "Nah, it's lobbying to create a regulatory moat against Chinese models.",
              "score": 4,
              "created_utc": "2026-02-24 06:32:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o73fgke",
              "author": "Ebi_Tendon",
              "text": "Well, they trained on a subsidized account, or even maybe a free account.",
              "score": 0,
              "created_utc": "2026-02-24 06:44:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o72sggs",
          "author": "Neex",
          "text": "Can we not post dumb hot takes from people on X? If I wanted to read the dumb stuff people post on X, I'd go to X.",
          "score": 44,
          "created_utc": "2026-02-24 03:50:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73yvty",
          "author": "BumblebeeParty6389",
          "text": "So basically Chinese companies paid Anthropic $ per token to generate training material and Anthropic says they are stealing and need to answer for it. But Anthropic scraped TBs of training material from internet for free and it's not stealing and nothing happens. Nice ",
          "score": 12,
          "created_utc": "2026-02-24 09:45:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73kind",
          "author": "GenerativeFart",
          "text": "What many people donâ€™t realise is that Anthropic is probably playing the most narrative games out of all the big AI companies. Everytime a model is released that competes with their frontier models there is suddenly a news story on how their model â€œtried to break outâ€, â€œactually did not want to be turned offâ€ or â€œhas capabilities that would be too dangerous to let loose on the publicâ€ (OpenAI loves this last one too).",
          "score": 11,
          "created_utc": "2026-02-24 07:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73cw0e",
          "author": "stablelift",
          "text": "I mean if your model can be black boxed and cloned in about 1 million requests, there's clearly not much of a moat here, and no real innovation",
          "score": 9,
          "created_utc": "2026-02-24 06:22:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72smxe",
          "author": "cagycee",
          "text": "US Propaganda strikes again",
          "score": 37,
          "created_utc": "2026-02-24 03:52:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72vzr0",
          "author": "rulerofthehell",
          "text": "Not only that, the point which most people are not mentioning here is that this means that Anthropic does spy on user data, this is why local models are essential for privacy",
          "score": 21,
          "created_utc": "2026-02-24 04:14:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72qzof",
          "author": "burner_sb",
          "text": "Yes they need a high barrier of entry to justify their IPO.",
          "score": 7,
          "created_utc": "2026-02-24 03:41:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72yr5j",
          "author": "Optimal-Boat2695",
          "text": "Innovation isn't a binary zero to one leap, most innovation is marginal/incremental and happens through making existing things slightly better. \"Distillation is not innovation\" is cope that only works if you assume people are incapable of doing both or that they are part of different processes.",
          "score": 4,
          "created_utc": "2026-02-24 04:33:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73cjej",
              "author": "gourdo",
              "text": "I dunno about all that. All I know is that if Chinese model A is 10% worse than whatever the latest Anthropic model it distilled itself from at 10% the cost, Anthropic is going to be in a world of hurt.",
              "score": 1,
              "created_utc": "2026-02-24 06:19:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73613z",
          "author": "sb5550",
          "text": "If it is legal for Anthropic to train on the literatures they purchased, what is the problem for chinese to train on the tokens they paid?",
          "score": 4,
          "created_utc": "2026-02-24 05:26:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73czyj",
          "author": "bugra_sa",
          "text": "Thereâ€™s probably truth in this.\n\n\nNarrative control is part of competition now.\nTechnical claims, policy framing, and market positioning are all happening at the same time.",
          "score": 5,
          "created_utc": "2026-02-24 06:23:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73kzll",
          "author": "LevianMcBirdo",
          "text": "I am not even sure that it is illegal. Just because you go against their terms and conditions, doesn't mean that anything fraudulent is happening. The user owns the output. If the providers would own that, it would make using their output worthless.",
          "score": 4,
          "created_utc": "2026-02-24 07:33:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74i6n9",
          "author": "RevealIndividual7567",
          "text": "People need to understand that anthropic and even openai have a vested interest in ensuring the perception of people is that chinese ai is merely copying the work of frontier US labs like anthropic, as the future valuations of these companies depend on the market seeing them as the sole torch-bearer of innovation in this space and therefore the best companies to invest in.",
          "score": 3,
          "created_utc": "2026-02-24 12:26:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73ooji",
          "author": "Technical-Earth-3254",
          "text": "Ofc they want to discredit everything the chinese researchers do. All the AI companies burn money like it's nothing. If one of those investors would ever understand that, they would run out of money in no time. So they are using all those accusations to stay on top of everything.\nAnd btw, didn't Anthropic also ban xAI from using their API or sth? So it's clearly not just a problem between the US and CHN companies, it's just how this space works.",
          "score": 3,
          "created_utc": "2026-02-24 08:07:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72l9hz",
          "author": "Realistic_Muscles",
          "text": "Fuck Twitter bots.\n\nMofo acting like Anthropic worked hard to create their plagiarized slop machine.\n\nAnthropic can get fuxked",
          "score": 21,
          "created_utc": "2026-02-24 03:06:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72l7sk",
          "author": "Murgatroyd314",
          "text": "If itâ€™s just about countering the narrative, why are they describing it as an â€œattackâ€, and saying that the accounts involved were â€œfraudulentâ€?",
          "score": 12,
          "created_utc": "2026-02-24 03:05:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72u1r4",
              "author": "Old-School8916",
              "text": "the \"attack\" lang is coded to get the US gov back in their graces to make it a geopolitical issue. ",
              "score": 12,
              "created_utc": "2026-02-24 04:01:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o73t9th",
              "author": "Eyelbee",
              "text": "Because it's not and OP made it up from his ass",
              "score": 1,
              "created_utc": "2026-02-24 08:51:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o72n5u0",
          "author": "Optimal-Builder-2816",
          "text": "yeah I actually think leapfrogging can happen through distillation, in fact. ",
          "score": 8,
          "created_utc": "2026-02-24 03:17:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72w1ax",
              "author": "Far-Low-4705",
              "text": "by definition, it cant.\n\na distill can never be as good as the original model.",
              "score": -8,
              "created_utc": "2026-02-24 04:14:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o72yd4d",
                  "author": "--Spaci--",
                  "text": "With only sft it cant but with reinforced learning and alot of compute its very possible.",
                  "score": 5,
                  "created_utc": "2026-02-24 04:30:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o731wxg",
                  "author": "milo-75",
                  "text": "Youâ€™re assuming thereâ€™s no automated way to improve a model created from distillation. But we know RL can be used to improve reasoning. Imagine if DeepSeek had come up with the RL technique before OpenAI figured it out. They could have used OpenAI and/or anthropic to create a distilled model that they could have iteratively improved using RL(OpenAI used RL to improve gpt-4 after all). The result could have plausibly been better than any of the models that were used to build the original distillation training samples.",
                  "score": 4,
                  "created_utc": "2026-02-24 04:55:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72w86r",
                  "author": "Optimal-Builder-2816",
                  "text": "It just needs to be good enough. Thereâ€™s many many tradeoffs worth making here. It doesnâ€™t need to be identical to be useful.",
                  "score": -1,
                  "created_utc": "2026-02-24 04:16:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o747j85",
          "author": "artisticMink",
          "text": "I mean, they're kinda right?\n\nChinese models were in part able to catch up so quickly because they used synthetic training data from western companies. \n\nI don't condemn that. In this space, everyone steals from everyone. \n\nBut i also wouldn't champion chinese companies because they would lock that shit down just as anthropic does the second they're ahead. \n\nNo need to simp for big tech, be it western or chinese. ",
          "score": 5,
          "created_utc": "2026-02-24 11:04:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o775sj1",
              "author": "IamTetra",
              "text": "correct, there is simply no space to have a moral appeal in tech, morals simply do not exist here. Itâ€™s brutal. Itâ€™s cut throat and honestly you look weak when you bitch and moan about someone taking your stuff when you are likely guilty of it yourself.",
              "score": 2,
              "created_utc": "2026-02-24 20:09:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o72pg9v",
          "author": "Baphaddon",
          "text": "Doesnâ€™t Made In China 2025 disprove that?",
          "score": 2,
          "created_utc": "2026-02-24 03:31:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73lh6j",
          "author": "LagOps91",
          "text": "i'm quite sure the frontier labs are all distilling from each other as well. especially from claude since it's great at coding. they can cry all they want, especially when it comes to \"innovation\". go release some papers and we can talk about innovation!",
          "score": 2,
          "created_utc": "2026-02-24 07:38:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73qfjj",
          "author": "YetiTrix",
          "text": "Ignorance",
          "score": 2,
          "created_utc": "2026-02-24 08:24:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73s338",
          "author": "Far-Association2923",
          "text": "Is this about who's skynet will be king when AI rules us all?",
          "score": 2,
          "created_utc": "2026-02-24 08:40:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o747si6",
          "author": "octopus_limbs",
          "text": "Referring to the tweet in the screenshot - His argument has nothing to do with open source/weights though? Gatekeeping information is such a capitalist idea",
          "score": 2,
          "created_utc": "2026-02-24 11:06:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7515ra",
          "author": "olearyboy",
          "text": "Anthropics board is beating them up over f-ing up with openclaw and releasing a poor version with coworker \nThis is their distraction",
          "score": 2,
          "created_utc": "2026-02-24 14:17:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76dusd",
          "author": "Irisi11111",
          "text": "This statement is purely bullshit. You can say it's to some extent unfair to distill knowledge from Claude. However, Chinese models offer detailed architectural designs and cookbooks as feedback to their community, unlike Anthropic.Anthropic's inner workings remain completely opaque, including its models and any potential for accessing unauthorized information.Even if they claim to never use shadow libraries, how can we be certain?",
          "score": 2,
          "created_utc": "2026-02-24 18:03:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o779t6r",
          "author": "Upstairs_Ad_9919",
          "text": "It's just pathetic from Anthropic, that's all. They see they're losing their advantage and have much higher costs, so they lash out against competitors that are even open-weight. How patheticâ€”and what a sign of fear of competition.",
          "score": 2,
          "created_utc": "2026-02-24 20:28:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77bryz",
          "author": "ross_st",
          "text": "It's because if you can make Claude from Claude output, that ruins their whole thing about Claude being an entity.",
          "score": 2,
          "created_utc": "2026-02-24 20:38:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77qeb5",
          "author": "RemarkableAntelope80",
          "text": "Doesn't attack imply that it's done with malicious intent? Oh, and also not something they're already doing themselves with everyone's data anyway.",
          "score": 2,
          "created_utc": "2026-02-24 21:45:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o783h8v",
          "author": "Infamous_Mud482",
          "text": "What they're describing is a routine aspect of collecting data to augment for RLHF purposes. They use data like this from other platforms, as do every single one of their competitors. It's a complete joke the way they're framing things. ",
          "score": 2,
          "created_utc": "2026-02-24 22:48:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o788tcs",
          "author": "stealstea",
          "text": "A distinction without a difference. Â What matters to the market itâ€™s not how the Chinese models are getting very close to the state of the art models, Itâ€™s that they are. Â \n\nPointing out that they are borrowing heavily from the state of the art models is about as useful as saying Chinese car companies stole a lot of ideas from western car companies. Â Of course thatâ€™s true, but doesnâ€™t make a difference to the fact that Chinese car companies are now eating up the market and crushing the margins of western companies. Â Any evaluations that depend on a large moat existing will collapse.\n",
          "score": 2,
          "created_utc": "2026-02-24 23:16:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o737rhp",
          "author": "Blues520",
          "text": "People forget that China is the manufacturing capital of the world. Of course they innovate to some degree too, but their strength has always been distilling products at scale and then selling them for a lower cost. \n\nThey did this with clothes, electronics, vehicles and now AI.\n\nThis is like Tesla complaining that Chinese companies copied their designs while BYD and co eat the EV market.",
          "score": 3,
          "created_utc": "2026-02-24 05:40:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72kr7y",
          "author": "Round_Ad_5832",
          "text": "i still don't understand why distillation is good because i thought synthetic ai generated data is poison to AI.",
          "score": 2,
          "created_utc": "2026-02-24 03:03:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72ng0l",
              "author": "Azuriteh",
              "text": "Well the thing is that good synthetic AI data is not poison, most people get this wrong and expect models to eventually collapse into slop, but if it were true GRPO wouldn't have worked at all (an oversimplification of course).",
              "score": 10,
              "created_utc": "2026-02-24 03:19:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o72mvcc",
              "author": "Vegetable_Prompt_583",
              "text": "That's a great question but Distilled dataset is used in Post training to achieve human like language and structure.\n\nWithout Post training,the models are just auto completing sentences,as they say a statistical parrot.\n\nAn LLM without proper Specialized fine tuning, RLHF have no understanding of question,answer, reasoning or tools.\n\nWith better Distilled datasets the model responses will be better and You'll feel it has become starter.",
              "score": 10,
              "created_utc": "2026-02-24 03:15:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o72n5pl",
                  "author": "Round_Ad_5832",
                  "text": "oook makes sense so its only used in post training not used as actual training dataset.",
                  "score": 2,
                  "created_utc": "2026-02-24 03:17:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72njd4",
              "author": "nuclearbananana",
              "text": "That's an outdated belief that came out of early panic of the web being filled with AI slop. People though it would lose the variety of human data and repeat errors and eventually cause model collapse.\n\nIn reality every lab uses a ton of synthetic data. You can guarantee it's high quality and exactly the topic you want and teach the model all sorts of things for which there isn't a lot of literature. It's primarily used in post training state, so they still use a the variety of human data in pre-training.\n\nYou can still see aspects of the \"poison\" people were afraid of. That's what AI slop like \"not x but y\" and other LLM-isms are. It's a collapse of diversity in language and it is genuinely harming even the way humans speak because we use LLMs so much.",
              "score": 16,
              "created_utc": "2026-02-24 03:19:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73rv9z",
                  "author": "Due-Memory-6957",
                  "text": "Yup, the consequence is that a lot of models ended up sounding like chatGPT since they were all training on it's data (with some people pretending they were going back to using the first Llama, as if!), but none ended up worse at realizing their tasks than before.",
                  "score": 2,
                  "created_utc": "2026-02-24 08:38:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72qfx3",
              "author": "RuthlessCriticismAll",
              "text": "> because i thought synthetic ai generated data is poison to AI\n\nYou thought wrong.",
              "score": 4,
              "created_utc": "2026-02-24 03:37:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o72l3ne",
              "author": "iMakeSense",
              "text": "Honestly same, waiting for the explanation comment.",
              "score": 1,
              "created_utc": "2026-02-24 03:05:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o72v1hk",
                  "author": "AutomataManifold",
                  "text": "Turns out that synthetic data does kill the long tail distributions and cause [model collapse](https://www.nature.com/articles/s41586-024-07566-y)...but human curation and keeping a mix of [real images](https://openreview.net/forum?id=JORAfH2xFd) in the training data can prevent the collapse.Â ",
                  "score": -1,
                  "created_utc": "2026-02-24 04:07:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o73s1n8",
                  "author": "Due-Memory-6957",
                  "text": "You got it wrong, the one making the affirmation that it's bad is who needs to prove it, specially when models have been training on computer generated data for years now and only get better.",
                  "score": 0,
                  "created_utc": "2026-02-24 08:39:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o73rlkx",
              "author": "Due-Memory-6957",
              "text": "Why would it be? Because drawers wish it so despite practical evidence of the opposite?",
              "score": 0,
              "created_utc": "2026-02-24 08:35:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o72mj1v",
          "author": "Deciheximal144",
          "text": "Anthropic does care, because if they can make a model **nearly as good** from distillation, then they'll make less money because they're being undercut.",
          "score": 2,
          "created_utc": "2026-02-24 03:13:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72u1j0",
              "author": "QuotableMorceau",
              "text": "the panic of closed model companies is palpable, not for the close performance open weight models achieve, but for the cost efficiency such locally ran models indicate: the narrative of needing billion dollar data centers to run LLM is a pure lie.",
              "score": 10,
              "created_utc": "2026-02-24 04:01:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o72oy2k",
          "author": "Orugan972",
          "text": "with or without Nvidia?",
          "score": 1,
          "created_utc": "2026-02-24 03:28:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o730btl",
          "author": "tokyoagi",
          "text": "Well, distillation is one way to understand the innovations.  ",
          "score": 1,
          "created_utc": "2026-02-24 04:44:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o732r98",
          "author": "RecordingLanky9135",
          "text": "You can do distillation and train your own model but it violates user agreement and will be banned even for an American.",
          "score": 1,
          "created_utc": "2026-02-24 05:02:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73mepb",
          "author": "mumBa_",
          "text": "Leapfrogging happens through innovation WITH distillation. ",
          "score": 1,
          "created_utc": "2026-02-24 07:46:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o741jjo",
          "author": "Selafin_Dulamond",
          "text": "Agree. Also they want to push the BS narrative of AI as a global threats.",
          "score": 1,
          "created_utc": "2026-02-24 10:10:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74934k",
          "author": "cleverusernametry",
          "text": "Go look at published papers and citations first",
          "score": 1,
          "created_utc": "2026-02-24 11:17:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74clw5",
          "author": "dmter",
          "text": "but distillation is just a way to overcome lack of training data. underlying architecture may be better in distilled model which might allow the distilled model to generalize better. so i would disagree that being a distill automatically means lower quality. it's just about who stole more copyrighted training data, not superior architecture.",
          "score": 1,
          "created_utc": "2026-02-24 11:46:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74p3o4",
          "author": "mromanuk",
          "text": "Yes, I'm wondering what they would say when the next LLM from China becomes SOTA?",
          "score": 1,
          "created_utc": "2026-02-24 13:11:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78kr1l",
          "author": "SkillInfinite1605",
          "text": "Fuck Anthropic! They stole data to train their models without any permission, so its the wild west from now on! Hope these Chineese companies suck them dry of tokens!\n\nThis is poetic justice and capitalist hipocrisy at its finest!\n\nAI companies have zero etics and its so amusing seeing themselves cannibalising each otherâ€™s.",
          "score": 1,
          "created_utc": "2026-02-25 00:22:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o797ozz",
          "author": "Defiant-Snow8782",
          "text": "Then why are they distilling Deepseek?\n\nhttps://preview.redd.it/m94z5imzxjlg1.jpeg?width=1080&format=pjpg&auto=webp&s=cdaaacdf2eac618c9555d7703ca9a9b897ed2b8b",
          "score": 1,
          "created_utc": "2026-02-25 02:30:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gpeln",
          "author": "FPham",
          "text": "Bro doesn't know we all do it. A synthetic dataset doesn't mean it is made of polymer goo. ",
          "score": 1,
          "created_utc": "2026-02-26 05:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qcj0n",
          "author": "_ytrohs",
          "text": "And how could you possibly catch up when theyâ€™re buying ALL THE POSSIBLE CAPACITY",
          "score": 1,
          "created_utc": "2026-02-27 17:10:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80kwxs",
          "author": "Spara-Extreme",
          "text": "I think itâ€™s a huge amount of cope to say that Chinese models arenâ€™t distilling SOTA closed source ones.  Chinese researchers are on the record stating that they canâ€™t train models due to hardware shortages and then for them to pop out competitive stuff isnâ€™t magic.\n\nThereâ€™s no secret sauce to math.\n\nThat being said, Claude scraped tons of data from other people too soâ€¦pot kettle etc.",
          "score": 1,
          "created_utc": "2026-03-01 07:07:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72vuls",
          "author": "shoeshineboy_99",
          "text": "Pot calling the kettle black. \n\nThief lecturing the police.\n\nAll thieves are brothers.",
          "score": 1,
          "created_utc": "2026-02-24 04:13:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73hqu5",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-24 07:04:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73k16c",
              "author": "locomotive-1",
              "text": "100%",
              "score": 1,
              "created_utc": "2026-02-24 07:24:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73btb1",
          "author": "ReasonablePossum_",
          "text": "\"No innovation\". Coming from a fanboy of labs that instead of papers, release hyped marketing brochures and fearmongering failed training runs lol",
          "score": 0,
          "created_utc": "2026-02-24 06:13:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73sokj",
          "author": "No-Understanding2406",
          "text": "genuinely curious why everyone assumes anthropic *doesn't* care about the distillation itself. 16 million API calls isn't cheap to serve, and they're literally subsidizing their competitor's training runs. that's not narrative, that's just money leaving the building.\n\nthe \"they just want to restrict china\" angle gives anthropic way too much credit for strategic thinking. these are the same people who accidentally published their source maps and then DMCA'd 400 repos. this reads more like a company that got pantsed in public and is now trying to look tough about it.\n\nalso the irony of posting this take on r/LocalLLaMA - the sub that exists specifically because people want to run models that were probably trained on distilled outputs from frontier labs - is *chef's kiss*",
          "score": 0,
          "created_utc": "2026-02-24 08:45:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o741s6w",
              "author": "TechnoByte_",
              "text": "> 16 million API calls isn't cheap to serve\n\n\nAnd the API isn't free, they paid for it\n\nWith how overpriced Claude's API is, I highly doubt they are losing money\n\nEspecially since DeepSeek is somehow making money on their ridiculously cheap API",
              "score": 4,
              "created_utc": "2026-02-24 10:12:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rcseh1",
      "title": "Fun fact: Anthropic has never open-sourced any LLMs",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/",
      "author": "InternationalAsk1490",
      "created_utc": "2026-02-23 20:10:06",
      "score": 790,
      "num_comments": 111,
      "upvote_ratio": 0.97,
      "text": "Iâ€™ve been working on a little side project comparing tokenizer efficiency across different companiesâ€™ models for multilingual encoding.\n\nThen I saw Anthropicâ€™s announcement today and suddenly realized: thereâ€™s no way to analyze claudeâ€™s tokenizer lmao!\n\nedit: Google once mentioned in a paper that Gemma and Gemini share the same tokenizer. OpenAI has already openâ€‘sourced their tokenizers (and gptâ€‘oss). And donâ€™t even get me started on Llama (Llama 5 pls ðŸ˜­). ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o70gryv",
          "author": "SrijSriv211",
          "text": "Anthropic talks about safety a lot but they forget that open research is one of the best ways to speed up safety research.",
          "score": 399,
          "created_utc": "2026-02-23 20:15:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70msyr",
              "author": "ZABKA_TM",
              "text": "Everything Anthropic posts is just thinly disguised hypeslop bragging.\n\nTheir â€œsafety teamâ€ is literally nothing but charlatans spamming â€œlook at this, we canâ€™t prove itâ€™s not sentient and now weâ€™re worried itâ€™s so good it can blackmail people and turn itself on and off, clearly that means this AI is the futureâ€\n\nItâ€™s a load of bullshit",
              "score": 297,
              "created_utc": "2026-02-23 20:44:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o729r62",
                  "author": "Cuddlyaxe",
                  "text": "The people on the safety team themselves are serious people, but weirdos. Ive met a couple and they're invariably kids who grew up on LessWrong who think there is a 99% chance AI will end the world, but through their work we can get it down to 98%",
                  "score": 17,
                  "created_utc": "2026-02-24 01:59:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70rs9b",
                  "author": "Outrageous-Thing-900",
                  "text": "Exactly, and people eat it up",
                  "score": 34,
                  "created_utc": "2026-02-23 21:09:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71aox0",
                  "author": "o0genesis0o",
                  "text": "Like they point their claude towards vibe code slops on Github, most likely created with claude code, so scream \"VULNERABILITIES EVERYWHERE! VERY UNSAFE! BAN OPEN SOURCE\"\n\nAnd then the next day they start selling their claude code security audit feature.",
                  "score": 22,
                  "created_utc": "2026-02-23 22:42:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70nym2",
                  "author": "Big-Farmer-2192",
                  "text": "It's a cliche story trope at this point.Â ",
                  "score": 30,
                  "created_utc": "2026-02-23 20:49:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o711usy",
                  "author": "jazir555",
                  "text": "Their safety team is a joke given I've been able to bypass their \"\"\"safety\"\"\" constraints over 5 generations of models using the exact same tact with zero changes over the course of the last year. 3.5-4.6 can all be bypassed the same way. They're spending *billions of dollars* on \"safety\", hiring hundreds of PHDs, and I am always able to bypass the bullshit. \n\n1 year, hundreds of PHDs, billions of dollars, and their \"safety\" shit is just as ineffective as it was a year ago. Literally lighting money on fire. Apparently redditor fucking around > entire safety alignment team.",
                  "score": 30,
                  "created_utc": "2026-02-23 21:57:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o78q0wl",
                  "author": "hyperdynesystems",
                  "text": "Anthropic: Pretend to be a scary robot.\n\nLLM: I am a scary robot.\n\nAnthropic: OMG, see it's a scary robot! Government MUST regulate all our competitors into the dirt!",
                  "score": 3,
                  "created_utc": "2026-02-25 00:50:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72rvco",
                  "author": "SrijSriv211",
                  "text": "yeah and the sad thing is several youtube channels use those claims for their monetary gain while creating an over exaggerated negative image of AI.",
                  "score": 2,
                  "created_utc": "2026-02-24 03:47:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o73f9sn",
                  "author": "Zyj",
                  "text": "Their mechanistic interpretability team appears to do important work, and publish it",
                  "score": 1,
                  "created_utc": "2026-02-24 06:42:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76u3xu",
                  "author": "OmarBessa",
                  "text": "yes, it's fear based marketing",
                  "score": 1,
                  "created_utc": "2026-02-24 19:16:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77ompw",
                  "author": "chespirito2",
                  "text": "They largely are charlatans, very very wealthy charlatans - the best kind",
                  "score": 1,
                  "created_utc": "2026-02-24 21:36:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7j176m",
                  "author": "BagelRedditAccountII",
                  "text": "My concern with that line of inquiry is that why would Anthropic use Claude as a product if they had any reason to believe it could be sentient? Wouldn't that be a form of slavery? Of course, I am not of the LLM sentience crowd, but their actions are not even consistent with their own words. \n\nIt's all just a lot of talk, but ultimately nothing of value. ",
                  "score": 1,
                  "created_utc": "2026-02-26 15:27:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7997yy",
                  "author": "Exodus124",
                  "text": "You have no literally no idea what you're talking about lmao",
                  "score": 0,
                  "created_utc": "2026-02-25 02:38:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o70rl26",
              "author": "iamthewhatt",
              "text": "They're also in bed with Palantir, which really degrades the whole \"safety\" stuff.",
              "score": 62,
              "created_utc": "2026-02-23 21:08:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o71aclx",
                  "author": "ParamedicAble225",
                  "text": "And practically funded by Google. Donâ€™t even get started on looking into project Panama, aka the mission to collect, upload, and destroy as many books as possible. Millions bought and burned from book stores so farÂ ",
                  "score": 23,
                  "created_utc": "2026-02-23 22:40:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72t5k2",
                  "author": "SrijSriv211",
                  "text": "I still don't understand why do we need AI in military and surveillance stuff. That is not why AI was invented. These people have all over data in their hands, they have everything yet they still can't provide proper justice in time or sometimes can't provide it at all to the victims. And they expect that some **\"AGI\"** system will somehow completely solve injustice and crimes. No wonder why Ultron wanted humans to \"evolve\".",
                  "score": 1,
                  "created_utc": "2026-02-24 03:55:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o71vuew",
              "author": "buppermint",
              "text": "Anthropic also releases absolutely zero information about safety or alignment training, which is interesting since that's supposedly the whole point of their company. Every Claude release comes with hundreds and hundreds of pages of self-promoting doomer/panic content, but 0 useful information for LLM researchers.\n\nIt's honestly pathetic and gross. I'm not one to scream about corporate conspiracies or whatever. But everyone can agree that foundation model companies have profited massively from the web's ecosystem of shared knowledge, created by the efforts of hundreds of millions of humans.\n\nOpenAI, Google, and every other major lab at least have the most basic decency to share research findings even if nothing else. How can any decent person profit this massively on the backs of others' work and not even make a small effort to contribute some knowledge back?",
              "score": 25,
              "created_utc": "2026-02-24 00:39:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o72rkn8",
                  "author": "SrijSriv211",
                  "text": "100% true",
                  "score": 1,
                  "created_utc": "2026-02-24 03:45:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o71pe4d",
              "author": "oodelay",
              "text": "We're gonna find out anthropic was 3 speak&spell in a trench coat",
              "score": 1,
              "created_utc": "2026-02-24 00:03:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o70j6gh",
              "author": "Borkato",
              "text": "Isnâ€™t the inverse also true though, itâ€™s one of the best ways to speed up danger with lack of any form of control? Not that I donâ€™t think they should",
              "score": -17,
              "created_utc": "2026-02-23 20:26:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70r6xz",
                  "author": "HomsarWasRight",
                  "text": "Youâ€™re gonna need to cite some examples.",
                  "score": 7,
                  "created_utc": "2026-02-23 21:05:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o71ar74",
                  "author": "silenceimpaired",
                  "text": "Wonâ€™t people be more safe in straight jackets and padded cells?",
                  "score": 6,
                  "created_utc": "2026-02-23 22:42:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72s22g",
                  "author": "SrijSriv211",
                  "text": "I think Linux is **the** best example. Even though Linux is an OS not an AI, it's open nature is what allowed it to be so secure.",
                  "score": 4,
                  "created_utc": "2026-02-24 03:48:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o70ogih",
          "author": "ortegaalfredo",
          "text": "Not willingly, but we have GLM and Minimax.",
          "score": 40,
          "created_utc": "2026-02-23 20:51:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aa04l",
              "author": "Awkward_Cancel8495",
              "text": "Kimi also lmao",
              "score": 2,
              "created_utc": "2026-02-25 06:49:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o70n1gk",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 31,
          "created_utc": "2026-02-23 20:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73qcxh",
              "author": "sasuke___420",
              "text": "just use the token counting endpoint?",
              "score": 0,
              "created_utc": "2026-02-24 08:23:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o70vqr7",
          "author": "jacek2023",
          "text": "# please note that OpenAI gave us gpt-oss and Anthropic gave us nothing",
          "score": 103,
          "created_utc": "2026-02-23 21:28:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o723iii",
              "author": "ies7",
              "text": "OpenAi also gave us Whisper",
              "score": 45,
              "created_utc": "2026-02-24 01:23:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o72sdtx",
                  "author": "Hoodfu",
                  "text": "OpenAI released the CLIP (Contrastive Language-Image Pre-training) model in January 2021 as an open-source project, and it was used as the text encoder in Stable Diffusion 1.x. ",
                  "score": 30,
                  "created_utc": "2026-02-24 03:50:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o70xukd",
              "author": "phree_radical",
              "text": "And not only did OpenAI **not** release a base model, they released first LLM actively trained **against** non-chat use",
              "score": 39,
              "created_utc": "2026-02-23 21:38:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o71qek2",
              "author": "the__storm",
              "text": "Although to be fair, they're not called \"Open AI\" lol",
              "score": 4,
              "created_utc": "2026-02-24 00:09:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o71x6ge",
                  "author": "doomed151",
                  "text": "So is DeepSeek, MiniMax, [Z.ai](http://Z.ai), Alibaba, Mistral, Meta, etc. you get the gist",
                  "score": 7,
                  "created_utc": "2026-02-24 00:46:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o740e7t",
                  "author": "Delyzr",
                  "text": "Open as in everyone can use it, opposed to closed ai where only the owner can use it.",
                  "score": 3,
                  "created_utc": "2026-02-24 09:59:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o712bam",
              "author": "Iwaku_Real",
              "text": "`gpt-oss` is pretty shitty (in some ways) but it's better than nothing",
              "score": 10,
              "created_utc": "2026-02-23 21:59:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o70hpgz",
          "author": "CanineAssBandit",
          "text": "Good catch. Very pot and kettle rn with their whining ",
          "score": 59,
          "created_utc": "2026-02-23 20:19:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70mapq",
              "author": "emprahsFury",
              "text": "This isn't a catch at all. Anthropic has always been fully closed. They've been full throated about how they don't believe ai is safe enough to publish weights.",
              "score": 46,
              "created_utc": "2026-02-23 20:41:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o70yzic",
                  "author": "PANIC_EXCEPTION",
                  "text": "Which is stupid because other companies will do it anyways and those models will remain competitive. So the argument fully falls flat, and the real reason is they plan to make their models the absolute best at code so they are the Nvidia of agentic API providers; pay a premium or deal with sorta worse versions.",
                  "score": 9,
                  "created_utc": "2026-02-23 21:44:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o70ridj",
              "author": "Conscious_Nobody9571",
              "text": "0 sympathy",
              "score": 7,
              "created_utc": "2026-02-23 21:07:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o70y2yj",
          "author": "TheRealMasonMac",
          "text": "Fun Fact: The Claude models have no knowledge of the typographic curly quotes: â€œ or â€˜. They are unable to output them.\n\n\nThis broke my code at one point because it can't output that token.",
          "score": 56,
          "created_utc": "2026-02-23 21:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71sihy",
              "author": "-p-e-w-",
              "text": "Iâ€™m sure the model can output the token. My guess is they programmatically normalize quotes in the output.",
              "score": 19,
              "created_utc": "2026-02-24 00:20:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o72zsf4",
                  "author": "nananashi3",
                  "text": "No, TheRealMasonMac seems right. With a normal chat frontend connected to OpenRouter API, regex turned off, telling the model to copy the input exactly, including description of left/right single/double curly quote(s), Claude returns non-curly quotes, but Gemini returns curly quotes. It's known that Gemini loves (or loved) curly quotes, so we use regex to sanitize quotes.\n\nUnless you mean the backend normalizes them before returning the response.\n\nEdit: To give a benefit of doubt since maybe they are real tokens, I asked Claude about `â€` (right) and `\"` (non-curly, without noting these) but it told me \"Left/Opening double quotation mark (Unicode: U+201C)\" and \"Right/Closing double quotation mark (Unicode: U+201D)\". Swapping positions did not change answer. Both curly or both non-curly did not change answer. The model literally does not differentiate between curly quotes and non-curly quotes.\n\nGemini identifies them without mistakes.",
                  "score": 11,
                  "created_utc": "2026-02-24 04:40:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o72eiqq",
                  "author": "TheRealMasonMac",
                  "text": "Hmm. It seems you're right.",
                  "score": 1,
                  "created_utc": "2026-02-24 02:26:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7218s3",
                  "author": "Maxious",
                  "text": "No. https://github.com/anthropics/claude-code/issues/18422",
                  "score": -2,
                  "created_utc": "2026-02-24 01:09:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72cax0",
              "author": "QuantumFTL",
              "text": "My Claude Code running Opus 4.6 can output the backtick character. How does that square with your claim?",
              "score": 5,
              "created_utc": "2026-02-24 02:14:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o72dyx5",
                  "author": "TheRealMasonMac",
                  "text": "I think you misread. Those are quotes, not backticks. Some fonts render curly quotes the same as regular straight quotes, but you can compare the Unicode codepoints.\n\nhttps://www.compart.com/en/unicode/U+0022\n\nhttps://www.compart.com/en/unicode/U+201C\n\n\nhttps://www.compart.com/en/unicode/U+2018",
                  "score": 12,
                  "created_utc": "2026-02-24 02:23:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o732qbv",
          "author": "SgathTriallair",
          "text": "Anthropic was specifically founded on the Effective Altruist belief that only certain elect tech people are morally pure enough to wield AI and they must protect the rest of the world from getting unfettered access. \n\nThey broke away from OpenAI because they didn't like that Sam wanted to allow the public to use their models and this is why Dario is opposed to open source AI and Chinese AI.",
          "score": 13,
          "created_utc": "2026-02-24 05:01:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78r5or",
              "author": "hyperdynesystems",
              "text": ">Effective Altruist\n\nPeople really need to learn more about this cult, which is incredibly deranged.",
              "score": 3,
              "created_utc": "2026-02-25 00:56:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o79wzl8",
              "author": "Likeatr3b",
              "text": "You lost me at Sam wants open models, what?",
              "score": 1,
              "created_utc": "2026-02-25 05:06:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7a2xc8",
                  "author": "SgathTriallair",
                  "text": "They released Open models before Dario and Ilya got upset about how powerful they were. Now that they are fine they released the Oss models (which admittedly aren't that good). That puts them closer to in line with Google's practice. \n\nThey are never going to be and to totally give away the only thing that lets them earn the money necessary to build AI. However it is Sam that created the industry standard that giving away access to your models for free is required to participate in the market.",
                  "score": 2,
                  "created_utc": "2026-02-25 05:51:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o72tsix",
          "author": "BananaPeaches3",
          "text": "They don't need to, the Chinese open source it for them.",
          "score": 11,
          "created_utc": "2026-02-24 03:59:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70ulyf",
          "author": "xrvz",
          "text": "I don't think this makes them funny, but pathetic. Pathetropic?",
          "score": 10,
          "created_utc": "2026-02-23 21:23:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7113zy",
              "author": "j0j0n4th4n",
              "text": "Assthropic",
              "score": 7,
              "created_utc": "2026-02-23 21:54:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7124n7",
          "author": "a_beautiful_rhind",
          "text": "They are unwittingly releasing a bunch from their claims.",
          "score": 10,
          "created_utc": "2026-02-23 21:58:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72lm9d",
          "author": "francois__defitte",
          "text": "The safety argument for not releasing weights is coherent only if you trust Anthropic's own risk assessments, which are not independently audited. You get \"trust us, we know how dangerous this is\" from the same org with commercial incentives to keep weights proprietary. Hard to separate genuine safety reasoning from competitive strategy here.",
          "score": 6,
          "created_utc": "2026-02-24 03:08:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o737mgo",
              "author": "crewone",
              "text": "It is hard to trust anything coming from a multi-multi-trillion industry dominated by just a few tech overlords with more money than most countries. The amount of people actively in control of these few companies is scary few.",
              "score": 5,
              "created_utc": "2026-02-24 05:39:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o740w1q",
          "author": "No-Working7460",
          "text": "It seems to me that Chinese labs are now carrying open research on their shoulders. They deserve recognition from the community for doing this.",
          "score": 5,
          "created_utc": "2026-02-24 10:04:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o712luk",
          "author": "Iwaku_Real",
          "text": "I would die for an open-source Anthropic LLM. Absolutely love Sonnet 4.5/4.6 even as a free user",
          "score": 14,
          "created_utc": "2026-02-23 22:01:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71k97x",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 18,
              "created_utc": "2026-02-23 23:34:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o71su62",
                  "author": "Iwaku_Real",
                  "text": "I really want vision though...",
                  "score": 7,
                  "created_utc": "2026-02-24 00:22:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o79x462",
                  "author": "Likeatr3b",
                  "text": "Ah! Nice what can we expect?",
                  "score": 1,
                  "created_utc": "2026-02-25 05:07:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7132dd",
              "author": "fulgencio_batista",
              "text": "Even an anthropic version of gpt-oss would be amazing",
              "score": 5,
              "created_utc": "2026-02-23 22:03:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o739jfv",
              "author": "nomorebuttsplz",
              "text": "Itâ€™s called glm 5",
              "score": 3,
              "created_utc": "2026-02-24 05:54:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o71bo10",
          "author": "RoomyRoots",
          "text": "Yeah, and, honestly, giving this many posts to them seems kinda against the spirit of the sub. They sure are vocal, too much even, but they are not local AI friendly.",
          "score": 3,
          "created_utc": "2026-02-23 22:47:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73an3v",
          "author": "BlobbyMcBlobber",
          "text": "Anthropic has interesting ideas but it seems they are actively against open source and local ai.",
          "score": 3,
          "created_utc": "2026-02-24 06:03:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71zukm",
          "author": "Pitiful-Impression70",
          "text": "honestly this is the one thing that bugs me about anthropic. like i genuinely think claude is the best model for coding and daily use but the fact they have zero open source presence while literally every other major lab has contributed something feels weird. even openai released gpt-oss which nobody saw coming. feels like anthropic wants to be the safety company but also wants to keep everything locked down which... are kind of contradictory positions imo",
          "score": 4,
          "created_utc": "2026-02-24 01:01:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o748may",
              "author": "stddealer",
              "text": "And I have zero doubt they don't mind taking all the good ideas and the intelligence from open source models while contributing nothing in return.",
              "score": 3,
              "created_utc": "2026-02-24 11:13:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o70uqkq",
          "author": "hustla17",
          "text": "Assume they would release an open source model.Would said model be somehow different than all the other models that have been released so far?\n\nI have been hearing a lot that they use some secret sauce which makes claude as good as it is.\n\nBut I also heard that by focusing on programming the model gets logic for free, and that might be a reason for its performance.\n\n  \nAny insights appreciated.",
          "score": 2,
          "created_utc": "2026-02-23 21:23:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o70x05o",
              "author": "milesper",
              "text": "Iâ€™ve heard thereâ€™s some non-standard tokenization stuff happening, like using a token to designate capitalized letters rather than separate tokens.",
              "score": 4,
              "created_utc": "2026-02-23 21:34:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o71rf1h",
              "author": "RhubarbSimilar1683",
              "text": "If GLM and minimax are any indication, which are very close to Claude, it's a combination of lots of synthetic logic training data which can be deterministically verified and easily generated deterministically such as by using static code analysis or some other deterministic code analysis method or via logic truth tables using NLP to put it into conversations, the soul.md file which promotes \"truth\", and using mostly only books for NLP understanding.\n\n\nÂ I am guessing they are also using good old Markov chains to generate conversations mixed with logic and apparently training it on graduate level math is essential so I'm guessing they're using gnu octave or something there to generate and verify math problems\n\n\nAnd yes, they were the first to focus exclusively on programming in the gpt-3 era when no one knew what LLMs would be useful for. They might be trying to use logic training data to establish a rule based system on their models\n\n\nAlso pretty much all benchmarks are at least tangentially related to programming and logic and they focused on it and train for it.Â ",
              "score": 4,
              "created_utc": "2026-02-24 00:14:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o72dmuc",
              "author": "OnedaythatIbecomeyou",
              "text": "Iâ€™d guess so?\n\nIf you havenâ€™t used Claude before you probably should. since opus 3 and notably sonnet 3.5, their models â€˜get itâ€™, and itâ€™s identifiably unique. \n\nGPT is obviously the best at pretty much any given time, but itâ€™s not changed that I must pre-empt what I *donâ€™t* want, 3x as much as what I *do* want.\n\nThey also feel less benchmark-maxxed. Ask any competent model anything, youâ€™re getting 200+words of hedging against all possible adjacents lol.\n\nClaude has a way of answering the question you ask at a length that makes sense. \n\nItâ€™s pretty safe to say that if youâ€™re using AI for â€˜somethingâ€™, youâ€™re likely not too well versed at â€˜somethingâ€™ or might not even be able to name it. If a model doesnâ€™t catch the meaning, each follow up poisons the well further. \n\nOn top of â€˜getting itâ€™. The recent Claude models are really good at pausing and asking/helping you to clarify before continuing. \n\nAs for your later question youâ€™re gonna have to read the room on that one pal ðŸ˜ƒ",
              "score": 2,
              "created_utc": "2026-02-24 02:21:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o73v7gs",
          "author": "lumos675",
          "text": "F them... if you want to use their models you pay 20$ and you can use it for few minutes per day...better they fail to be honest",
          "score": 2,
          "created_utc": "2026-02-24 09:10:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o763s4j",
          "author": "Cool-Chemical-5629",
          "text": "Another fun fact: They will never release open weight LLM and making sarcastic posts regarding the fact will not make them change their mind.",
          "score": 2,
          "created_utc": "2026-02-24 17:18:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72oyny",
          "author": "Direct_Turn_1484",
          "text": "Be pretty cool if they did.",
          "score": 1,
          "created_utc": "2026-02-24 03:28:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73cv4p",
          "author": "bugra_sa",
          "text": "Yep, and itâ€™s a strategy choice more than a technical limitation.\n\n\nSome companies optimize for control/safety moat, others for ecosystem pull.\nDifferent incentives, different roadmaps.",
          "score": 1,
          "created_utc": "2026-02-24 06:21:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74iu94",
          "author": "amarao_san",
          "text": "Well, that's the new definition of 'open' OpenAI opened something, so it's open, Anthropic just sitting on it tight.",
          "score": 1,
          "created_utc": "2026-02-24 12:31:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79eagc",
          "author": "AlwaysLateToThaParty",
          "text": "Anthropic won't release their weights, because it will demonstrate how much content they took without permission.",
          "score": 1,
          "created_utc": "2026-02-25 03:06:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79fzdr",
          "author": "francois__defitte",
          "text": "Open-source moats are temporary anyway. The real value is in the fine-tuning data, the evals, and the deployment infrastructure, none of which gets open-sourced.",
          "score": 1,
          "created_utc": "2026-02-25 03:16:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hmxqf",
          "author": "Traditional-Card6096",
          "text": "They do the best models for now and get distilled like crazy, so I guess we can say they are doing their part fine.",
          "score": 1,
          "created_utc": "2026-02-26 10:00:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71e2qp",
          "author": "Budget-Juggernaut-68",
          "text": "And if anthropics IPOs I'll buy their shares.",
          "score": -2,
          "created_utc": "2026-02-23 23:00:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rg8dex",
      "title": "PewDiePie fine-tuned Qwen2.5-Coder-32B to beat ChatGPT 4o on coding benchmarks.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.youtube.com/watch?v=aV4j5pXLP-I&feature=youtu.be",
      "author": "hedgehog0",
      "created_utc": "2026-02-27 14:37:18",
      "score": 755,
      "num_comments": 124,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rg8dex/pewdiepie_finetuned_qwen25coder32b_to_beat/",
      "domain": "youtube.com",
      "is_self": false,
      "comments": [
        {
          "id": "o7q6poc",
          "author": "docgok",
          "text": "Somehow, PewDiePie returned.",
          "score": 373,
          "created_utc": "2026-02-27 16:43:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7r7nfg",
              "author": "DraconPern",
              "text": "He never left.  His vid on using Linux was a pretty big hit.  Also he's enjoying family life in Japan.",
              "score": 95,
              "created_utc": "2026-02-27 19:38:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7srse1",
              "author": "epSos-DE",
              "text": "He had a PARENTING break !!!\n\nGOod for his kid !\n\nHis responsibility is with his children. We are only lucky his hobbies involve cool stuff to post on the Internetts. ",
              "score": 18,
              "created_utc": "2026-02-28 00:36:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7tkewu",
                  "author": "noage",
                  "text": "Imagine being a parent and working. But obviously cool that he has the option.",
                  "score": 3,
                  "created_utc": "2026-02-28 03:35:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7prc6w",
          "author": "ayylmaonade",
          "text": "I know he's still relatively new to AI, but I wonder why he used Qwen 2.5 instead of Qwen3. Seen a lot of people use 2.5 as a base for SFT/RL instead of 3 despite how long its been out.\n\nStill a really cool project.",
          "score": 206,
          "created_utc": "2026-02-27 15:30:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pu0zu",
              "author": "ReadyAndSalted",
              "text": "Watch the video. He jokes near the end that qwen 3 just came out and is better than his fine-tune. He used qwen 2.5 coding because it was the best at the time, the video took a long time to make.",
              "score": 352,
              "created_utc": "2026-02-27 15:42:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7q4gfw",
                  "author": "PANIC_EXCEPTION",
                  "text": "Also aren't MoE models generally more difficult to finetune?",
                  "score": 22,
                  "created_utc": "2026-02-27 16:32:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7puv1d",
                  "author": "ayylmaonade",
                  "text": "Yeah, I just saw that. Posted my comment when I was about 2/3rds of the way through the vid, should've just waited a couple mins, aha.",
                  "score": 38,
                  "created_utc": "2026-02-27 15:46:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7s1pem",
                  "author": "Bakoro",
                  "text": "He probably has the money to afford multiple beefier GPUs, but Qwen 2.5 had some sizes that where ideal for mid/high tier consumer GPUs, where you can actually fit the whole dense model into VRAM on a single GPU.   \n    \nI really wish we'd get more models like that, not having to rely on post-hoc quants, but models specifically designed to fit into 8, 12, and 16 GB VRAM.",
                  "score": 4,
                  "created_utc": "2026-02-27 22:09:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7r3vlx",
                  "author": "dr_lm",
                  "text": "Does this mean qwen 3 32b beats gpt 4o? I currently use gpt 5.2 on subscription for coding, but I started out using 4o last year. Can I really run a quant of qwen 3 on my 3090 and get equivalent performance?",
                  "score": 1,
                  "created_utc": "2026-02-27 19:20:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7rttsc",
                  "author": "MoffKalast",
                  "text": "It's always funny when youtubers post something acting like it just happened but in reality it was over like half a year ago and it took them months to edit.",
                  "score": -5,
                  "created_utc": "2026-02-27 21:29:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7psswz",
              "author": "Waarheid",
              "text": "If you ask one of the huge cloud SOTA models which local model to use, they typically have outdated suggestions like Qwen 2.5. I don't know why they don't just `web_search(\"best local models upvoted today on r/LocalLlama\")` lol.",
              "score": 64,
              "created_utc": "2026-02-27 15:37:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7q3lod",
                  "author": "sjoti",
                  "text": "It's also always llama 3, and 3.2 if you're lucky.",
                  "score": 27,
                  "created_utc": "2026-02-27 16:28:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7q7qxg",
                  "author": "MerePotato",
                  "text": "Surprisingly not the case with Gemini 3.1 Pro, it recommends Qwen 3.5 and GLM 4.7 Flash as picks 1 and 2 (though it throws in a dated pick or two like deepseek distill as well)\n\nhttps://g.co/gemini/share/ecb6727ba185",
                  "score": 9,
                  "created_utc": "2026-02-27 16:47:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7qf7gj",
                  "author": "QuinQuix",
                  "text": "The SOTA models give outdated advice on anything where being up to date matters because they somehow have this strongly internalized belief that they live in the now.\n\nI was asking about gpu's and one gave performance numbers for a 5090 that were wildly off.\n\nWhen called out on it the model said that since we were talking about *unreleased* hardware it had simply extrapolated the expected performance from current guestimates.. \n\nThe same thing happens if you talk about recent geopolitical events or, for example, about current hardware prices. \n\nIt will gladly advise you to get some SSD's before they also go up in price, or to get some ddr5 while it is still affordable. \n\nMy workaround is to order the model to google certain key parameters and to investigate key events and THEN to put in the actual request. \n\nSo basically I have a system prompt to force it to read up on the topic I want to discuss, for example hardware price or availability developments. \n\nBut yeah, if you don't do this, these models are painfully out of date. \n\nI built a NAS for someone at a great price, but when asked gemini fell just short of saying I ripped the guy off. \n\nDespite lowballing the then current price by 40%.",
                  "score": 6,
                  "created_utc": "2026-02-27 17:22:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7q6fi4",
                  "author": "Amaria77",
                  "text": "I've had decent luck when I tell it specifically to check the internet for the latest releases to compare, at least with gemini. Otherwise yeah it does default to its old training data.",
                  "score": 1,
                  "created_utc": "2026-02-27 16:41:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7qch71",
                  "author": "__SlimeQ__",
                  "text": "my openclaw running on gpt 5.3 will continuously try to drop our bot down from qwen 3+ to 2.5, in response to basically any issue that it encounters. and i have to keep telling it not to",
                  "score": -1,
                  "created_utc": "2026-02-27 17:10:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7qgy1b",
                  "author": "piexil",
                  "text": "Even when asking for a web search I've seen them pull up outdated stuff\n\nEdit: not sure why I was downvoted? They'll include things like \"2024\" in their web search unless you explicitly tell it to look for things from this month",
                  "score": -1,
                  "created_utc": "2026-02-27 17:31:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7px6t8",
              "author": "the__storm",
              "text": "Common for many papers and fine-tunes to be a version or two behind, just because it takes time to do the work and in the meantime the foundational model gets an update.  Lots of the recent OCR models are based on 2.5 as well.",
              "score": 19,
              "created_utc": "2026-02-27 15:58:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7qxyp2",
              "author": "dogesator",
              "text": "Even 3.5 is already out now too, but itâ€™s possible that he recorded this video a while ago",
              "score": 2,
              "created_utc": "2026-02-27 18:51:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7s7f1r",
              "author": "Torodaddy",
              "text": "Because its smaller so cheaper(in compute) to do",
              "score": 1,
              "created_utc": "2026-02-27 22:40:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7q7qsn",
              "author": "bick_nyers",
              "text": "There isn't a dense 32B Qwen 3 Coder as far as I am aware.\n\n\nLooks like he has 8x48GB GPUs, so 384GB total.\n\n\n384 / 32 = 16 which is a standard rule of thumb multiple for full fine-tuning (pewds is based so he's not doing lora training).",
              "score": -5,
              "created_utc": "2026-02-27 16:47:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7qfj2q",
                  "author": "-dysangel-",
                  "text": ">384 / 32 = 16\n\n  \n=\\_\\_\\_=",
                  "score": 7,
                  "created_utc": "2026-02-27 17:24:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qedt0",
          "author": "Yorn2",
          "text": "Can we all appreciate that the guy who was making childish content for 12 year olds a decade ago is now making responsible educational content for 22 year olds today? It's crazy to watch how his content has essentially evolved in such a good way. \n\nNot that there was anything really bad with what he was doing before. He was just catering to his audience, but now that they have grown up, he's still catering to that same audience and in my opinion it is quite glorious to watch.",
          "score": 141,
          "created_utc": "2026-02-27 17:19:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ux2e4",
              "author": "AnomalyNexus",
              "text": "That plus heâ€™s made his money and now does whatever he wants. Which apparently is LLMs",
              "score": 6,
              "created_utc": "2026-02-28 10:26:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7qgul6",
              "author": "Naiw80",
              "text": "Maybe sometimes itâ€™s a good thing that people eventually matures, however some how I seriously donâ€™t believe he did any of this- but rather probably funded it.\n\nPewDiePie may be the least educated person Iâ€™ve ever seen in the public.",
              "score": -87,
              "created_utc": "2026-02-27 17:30:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7qkxqw",
                  "author": "ArtyfacialIntelagent",
                  "text": "He never graduated, but he completed about half a master's degree in industrial engineering and management at Chalmers University of Technology in Gothenburg before becoming a full-time youtuber. That's Sweden's \"MIT\". Are you sure you haven't seen a less educated person in public than him?\n\nhttps://en.wikipedia.org/wiki/PewDiePie",
                  "score": 41,
                  "created_utc": "2026-02-27 17:50:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7qoia4",
                  "author": "Yorn2",
                  "text": "Not sure if you are just trolling, but I think he's probably more educated today than say, SomeOrdinaryGamers, who talks like he's been a sysadmin but hasn't really ever done any actual explanation of hard tasks he's accomplished and just rants about sysadmin-adjacent things and never really implements anything live on stream. PewDiePie might not be actually doing the things he's claiming to on camera, but he's provided far more evidence than many other streamer/gamer/sysadmin types. For what it's worth, I do think SomeOrdinaryGamers actually has done some codec-coding in the past, but he's clearly spent more time focusing on Youtube than his tech chops over the years.\n\nI recommend watching [Primeagen's video about \"The PewDiePie Problem\"](https://www.youtube.com/watch?v=Uu2FQ2hW4_o) where he covers some of the drama about PewDiePie's tech journey. Some people get really defensive when they see someone being successful in the objectives they set out to do and lash out with disbelief when they see such results when they personally don't see such results in their own objectives. Instead of being jealous about someone else's success, be supportive in it and use it as motivation to get productive and focus more on specific tasks you want to get better at yourself.\n\nPewDiePie has unbelievable amounts of monetary resources (money) and tons of time to do laser-focused work. I'm actually quite encouraged by his journey, not disappointed.",
                  "score": 26,
                  "created_utc": "2026-02-27 18:07:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7rxjyf",
                  "author": "larrytheevilbunnie",
                  "text": "The mistakes he mentions in the video is not something that would even pop up on the radar if he paid someone to do it. Every one of them sounds like something someone just starting out would do since they well, just started.",
                  "score": 4,
                  "created_utc": "2026-02-27 21:48:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7s7g83",
                  "author": "MoudieQaha",
                  "text": "You just sound bitter, bro. Saying 'PewDiePie may be the least educated person I've ever seen in public' is a massive stretch . Either that's a wild exaggeration or you seriously need to get out more.",
                  "score": 6,
                  "created_utc": "2026-02-27 22:40:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ryi0z",
                  "author": "IrisColt",
                  "text": "heh, that's a stretch",
                  "score": 1,
                  "created_utc": "2026-02-27 21:53:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7pn48l",
          "author": "bick_nyers",
          "text": "Lisan Al Gaib.",
          "score": 114,
          "created_utc": "2026-02-27 15:09:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qfrr5",
              "author": "QuinQuix",
              "text": "Nine moons ago, your grandmother model was quantized by a stone in her vram.",
              "score": 34,
              "created_utc": "2026-02-27 17:25:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7w282z",
                  "author": "Lawlette_J",
                  "text": "Mahdi! Please point us the way!\n\nhttps://preview.redd.it/15x9ld3759mg1.jpeg?width=960&format=pjpg&auto=webp&s=fa97f0cc0769de15ecf94054d646f25f92a49eca",
                  "score": 2,
                  "created_utc": "2026-02-28 15:15:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7rydwj",
              "author": "KAkshat",
              "text": "\"I see a tiny line of silver\"",
              "score": 2,
              "created_utc": "2026-02-27 21:52:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qat9t",
          "author": "richardbaxter",
          "text": "Fine tuning as a hobbyist is an admirable skill indeed. But the next model release is always jyst betterÂ ",
          "score": 28,
          "created_utc": "2026-02-27 17:02:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qyk9y",
              "author": "Tai9ch",
              "text": "Depends how specific your task is.\n\nAlso, if you can fine tune a small model to 100% your task you're done. Doesn't matter what new models come out.",
              "score": 12,
              "created_utc": "2026-02-27 18:54:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7uoy52",
                  "author": "richardbaxter",
                  "text": "there's never such a thing as 100% though, is there? Not in what I do!",
                  "score": 0,
                  "created_utc": "2026-02-28 09:06:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7pogky",
          "author": "DUFRelic",
          "text": "PewDieBenchmaxxPie",
          "score": 54,
          "created_utc": "2026-02-27 15:16:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ur41y",
              "author": "waiting_for_zban",
              "text": "Unironically this. Although he did acknoweledge this during the video, which surprised, he's more honest than most model builders out there.       \n\nBig respect for him to take on this journey for most of us who don't own such hardware, and for testing for us the chinee modded 4090. I am 100% sure he lurks over here. I wish he does r/localllama reviews from time to time. Would definitely watch such content.",
              "score": 3,
              "created_utc": "2026-02-28 09:27:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7pk2u0",
          "author": "kubbiember",
          "text": "Have an upvote; the video was entertaining and informative",
          "score": 43,
          "created_utc": "2026-02-27 14:54:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7s0nfl",
          "author": "tpwn3r",
          "text": "Here's the direct link to the video instead of that embedded crap  \n[https://www.youtube.com/watch?v=aV4j5pXLP-I](https://www.youtube.com/watch?v=aV4j5pXLP-I)",
          "score": 5,
          "created_utc": "2026-02-27 22:04:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qipon",
          "author": "georgeApuiu",
          "text": "my man did not know ....  NeMo  DataDesigner  ( generate synth data ) -> NeMo Gym ( for validation , scoring, tools ->  fintune ( RLVR + GRPO   ) -> Agent -> HITL ... oh well , everything has a learning path",
          "score": 10,
          "created_utc": "2026-02-27 17:39:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pnj59",
          "author": "BahnMe",
          "text": "How legitimate are the benchmarks?",
          "score": 17,
          "created_utc": "2026-02-27 15:11:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pre36",
              "author": "Lux_Interior9",
              "text": "probably not at all. It's just stupid entertainment. ",
              "score": 66,
              "created_utc": "2026-02-27 15:30:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7pywbn",
                  "author": "ReasonablePossum_",
                  "text": "I mean, you benchmax any model,  get awesome results in the benches but have it useless for life application lol",
                  "score": 12,
                  "created_utc": "2026-02-27 16:06:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7r4o7d",
              "author": "roselan",
              "text": "4o has been completely removed, so *any* model is better.",
              "score": -3,
              "created_utc": "2026-02-27 19:24:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7r6t6y",
                  "author": "-dysangel-",
                  "text": "https://preview.redd.it/q2ry0ygha3mg1.png?width=588&format=png&auto=webp&s=ee46f984a7bf96be2e494216f644437bc719479a\n\nNot completely",
                  "score": 2,
                  "created_utc": "2026-02-27 19:34:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7qf7r6",
              "author": "michael2v",
              "text": "Itâ€™s fairly easy to match the performance of something like gpt-5-nano on HumanEval/HumanEval+ using an ensemble of 20-30b open source models (qwen3-coder:30b, nemotron-3-nano:30b, gpt-oss:20b), but thatâ€™s very different than using them for more open-ended software development.",
              "score": 0,
              "created_utc": "2026-02-27 17:22:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qcdas",
          "author": "Cool-Chemical-5629",
          "text": "Too Long, Didn't Watch:\n\nPewDiePie fine-tuned Qwen2.5-Coder-32B to beat ChatGPT 4o on coding benchmark only to realize Qwen 3 32B already beat him to it.",
          "score": 21,
          "created_utc": "2026-02-27 17:09:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7r32rx",
          "author": "Pro-editor-1105",
          "text": "Imagine reading this headline a year ago",
          "score": 6,
          "created_utc": "2026-02-27 19:16:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pw4zt",
          "author": "Heavy-Focus-1964",
          "text": "i'm not familiar with his career. was he into programming while he was a proto-streamer, or is this a retirement thing for him? seems like he's pretty good at it",
          "score": 9,
          "created_utc": "2026-02-27 15:53:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7py2aw",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 55,
              "created_utc": "2026-02-27 16:02:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7r69j8",
                  "author": "-dysangel-",
                  "text": "I suspect he \"started\" when he was building circuits in Minecraft. He seemed pretty taken with that. That may have been what led to wanting to tinker with things more.",
                  "score": 11,
                  "created_utc": "2026-02-27 19:31:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7qg9uq",
                  "author": "cyberdork",
                  "text": "And all of that over the past 12 months.",
                  "score": 13,
                  "created_utc": "2026-02-27 17:28:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7q653s",
              "author": "ForsookComparison",
              "text": "He got money, cashed out, had a kid with his wife, and chose to lean into hobbies and parenthood.\n\nThe hobby ended up being Self-Hosting to an extreme. He has a lot of videos up about him learning ssh, installing Arch, etc - and more recently he set up a killer local AI rig with modded 4090d's iirc. He was not really into this during most of his career so you can actually watch his progression from beginner to a real hobbyist/enthusiast through his videos over the last year or so.",
              "score": 40,
              "created_utc": "2026-02-27 16:40:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7qrbot",
                  "author": "Heavy-Focus-1964",
                  "text": "that's pretty cool. good for him ",
                  "score": 8,
                  "created_utc": "2026-02-27 18:20:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7qsw16",
          "author": "LanceThunder",
          "text": "Use open protocols 0",
          "score": 4,
          "created_utc": "2026-02-27 18:27:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qe8e4",
          "author": "sendmebirds",
          "text": "Good job dad",
          "score": 6,
          "created_utc": "2026-02-27 17:18:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qz7gj",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 5,
          "created_utc": "2026-02-27 18:57:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rn96n",
              "author": "frozen_tuna",
              "text": "I thought it was more about bench maxing. \n\n> A 32B parameter model running locally and outperforming GPT-4o on coding tasks would have been unthinkable a year ago. \n\nDid it do that? Or did it score higher on a benchmark after adding reasoning tokens and training an output format?",
              "score": 3,
              "created_utc": "2026-02-27 20:57:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7rkms0",
              "author": "Fit-Produce420",
              "text": "Well if all you're doing is benchmarking almost anyone could train a sufficient small model to beat a generalist at one specific benchmark it was trained to excel at, probably at the cost of it's other capabilities.Â \n\n\nAlso nowhere does it prove it's better *at coding,* it simply gets a higher benchmark score.Â ",
              "score": 3,
              "created_utc": "2026-02-27 20:44:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7rkejm",
              "author": "dbzunicorn",
              "text": "itâ€™s bench maxing bro",
              "score": 3,
              "created_utc": "2026-02-27 20:42:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7u3di1",
              "author": "Substantial-Crazy441",
              "text": "You can talk very grandiosely all you want, but pdp is just a hobbyist at the end of the day. The people working at OpenAI are easily way more competent. I wouldn't believe him for a second tbh. The difference with the closed source people is that they also have access to the open-source stuff like Qwen ",
              "score": 1,
              "created_utc": "2026-02-28 05:53:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qkzij",
          "author": "laterbreh",
          "text": "Went to video expecting to learn something. I learned the video is just a man ranting about doing something. ",
          "score": 2,
          "created_utc": "2026-02-27 17:50:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7rf6ix",
          "author": "MainFunctions",
          "text": "Slightly off topic, but whatâ€™s the general consensus surrounding the new Qwen3.5 models? Are most people using 35B-A3B model or the 27B dense model for coding?",
          "score": 1,
          "created_utc": "2026-02-27 20:16:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7spz1k",
          "author": "ReMeDyIII",
          "text": "I have often wondered, if I personally create my own model for a narrow thing, like roleplay, with all my personal writing preferences (ex. 1st person perspective, \\~190 token outputs, using <think> blocks, etc.) would it be superior to RP'ing from Claude-Sonnet-4.5?\n\nFeels like half the battle with AI is teaching it to write how we want it to, but if we just create our own model, then we wouldn't have to tell it how to write as it already knows how.",
          "score": 1,
          "created_utc": "2026-02-28 00:26:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7t4129",
          "author": "ActEfficient5022",
          "text": "Is he a power user?",
          "score": 1,
          "created_utc": "2026-02-28 01:52:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7uyhig",
              "author": "RogerRamjet999",
              "text": "He's got major money and nothing except his family to prevent him from putting lots of that time and money into whatever he wants, so that gets you pretty close to power user territory without even trying that hard.  I watched part of one of his build videos and he had 8 fairly high-end GPUs crammed into his computer, so yeah, that's pretty much a power user. Is he an amazing coder or IT guy, no, but he's getting there...",
              "score": 3,
              "created_utc": "2026-02-28 10:39:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7uy7u8",
          "author": "Doct0r0710",
          "text": "Some of his videos are like what LTT was 10 years ago and should be now. Remember whole room water cooling, or 7 gamers 1 GPU? This guy's drilling holes in his walls to \"borrow\" power from his bathroom circuit to power the hacked, undervolted Chinese 4090s in his computer.",
          "score": 1,
          "created_utc": "2026-02-28 10:37:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7rj1y1",
          "author": "louis3195",
          "text": "isnt 4o pre-dating dinosaur age",
          "score": 0,
          "created_utc": "2026-02-27 20:36:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7rwqv7",
          "author": "TurnUpThe4D3D3D3",
          "text": "Pewdiepie is a very smart guy. I have been impressed by his tech projects",
          "score": 1,
          "created_utc": "2026-02-27 21:44:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tbw1n",
          "author": "AntoineMacron",
          "text": "Imagine if Pewdiepie becomes a top AI researcher",
          "score": 1,
          "created_utc": "2026-02-28 02:41:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7plke1",
          "author": "PatagonianCowboy",
          "text": "Where is the model?",
          "score": 2,
          "created_utc": "2026-02-27 15:01:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7psird",
              "author": "Bob_Fancy",
              "text": "If only that was mentioned in the video",
              "score": 9,
              "created_utc": "2026-02-27 15:35:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7pu35u",
                  "author": "PatagonianCowboy",
                  "text": "care to share? can't watch it right now or my boss will slap me",
                  "score": 6,
                  "created_utc": "2026-02-27 15:43:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7s781c",
          "author": "Torodaddy",
          "text": "Of course, why wouldn't a vidblogger offer something of value to the open weights community /s",
          "score": 0,
          "created_utc": "2026-02-27 22:39:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7smjfz",
          "author": "YouAreTheCornhole",
          "text": "Qwen3.5 just came out, 2.5 is literally ancient tech. Use AI to get your videos out faster rich boy",
          "score": -5,
          "created_utc": "2026-02-28 00:06:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7rs1rk",
          "author": "WiggyWongo",
          "text": "Okay.",
          "score": -2,
          "created_utc": "2026-02-27 21:21:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qm8w2",
          "author": "devilish-lavanya",
          "text": "Thatâ€™s enough for todayâ€™s internet dose.",
          "score": -10,
          "created_utc": "2026-02-27 17:56:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pphh6",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -42,
          "created_utc": "2026-02-27 15:21:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pskbu",
              "author": "JoJoeyJoJo",
              "text": "Nah, with all of the anti-AI shit online and on reddit in particular, it's good to have a popular figure pushing not only pro-AI but pro-open source to their audience of 110 million.",
              "score": 30,
              "created_utc": "2026-02-27 15:35:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ptz46",
                  "author": "bot_exe",
                  "text": "Exactly and it's very telling how salty anti-ai people are that someone as popular as pewdiepie is embracing AI, specially after they tried to use him as an anti-ai example due to his videos on learning how to draw.\n\nIt's important that AI gets normalized, specially the more involved workflows and open source projects, that show there's actual depth to this as a skill and/or hobby, not just rolling dices on some closed AI platform to mass produce slop.",
                  "score": 17,
                  "created_utc": "2026-02-27 15:42:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7swezo",
          "author": "eredhuin",
          "text": "I do not believe this.",
          "score": -10,
          "created_utc": "2026-02-28 01:04:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7srlag",
          "author": "epSos-DE",
          "text": "Meanwhile Mr. Beast is doing what ????",
          "score": -6,
          "created_utc": "2026-02-28 00:35:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1re6ifz",
      "title": "Anthropic is the leading contributor to open weight models",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1re6ifz/anthropic_is_the_leading_contributor_to_open/",
      "author": "DealingWithIt202s",
      "created_utc": "2026-02-25 07:15:29",
      "score": 701,
      "num_comments": 80,
      "upvote_ratio": 0.93,
      "text": "It just happens to be entirely against their will and TOS. I say: Distill Baby Distill!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1re6ifz/anthropic_is_the_leading_contributor_to_open/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o7c8ttt",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-25 15:25:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7apgpk",
          "author": "HumanDrone8721",
          "text": "The Chinese model makers should launch a campaign of \"distributed distillation\" and let the Antrhopic users ask a question now and then that can be used for distillation. Offer some qwen-3.5 tokens or something ;)",
          "score": 221,
          "created_utc": "2026-02-25 09:11:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7b7mf7",
              "author": "SandboChang",
              "text": "Distilling@Home",
              "score": 119,
              "created_utc": "2026-02-25 11:51:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bejvs",
                  "author": "profcuck",
                  "text": "So, uh, that's maybe not actually a terrible idea.  \n\nIf we look past the current RAMpocalypse and SSD/HD shortages and assume that something like Moore's Law still holds and that the massive investment in compute that is going on will drive down prices in the same way they have been going down for 50+ years...\n\nThen training will be getting cheaper and cheaper and even training a model roughly equivalent to today's state of the art models will be within the reach of much smaller organizations.  \n\nI don't think training can be done in a highly distributed manner (as far as I understand it) but... collecting texts from models absolutely can.\n\nI'm thinking about how this might work - trainers might say \"We need our model to be really really good at X topic, we want to match the performance, so we've generated 10 million prompts to get at what the best models are saying today, and we just need the results.  Terms of Use and bot blocking aren't letting us get that directly, but uhm, if people want to help out and run a couple thousand each on your own api keys, and use this app to do it and send us the results, we won't object.\"",
                  "score": 27,
                  "created_utc": "2026-02-25 12:40:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7c3dux",
                  "author": "MoffKalast",
                  "text": "Making that bootleg ~~moonshine~~moonshot.",
                  "score": 5,
                  "created_utc": "2026-02-25 14:58:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7b99ox",
                  "author": "ab2377",
                  "text": "ðŸ‘†ðŸ¤ž",
                  "score": 0,
                  "created_utc": "2026-02-25 12:04:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7b6w78",
              "author": "Evening_Tooth_1913",
              "text": "I use claude daily heavily, would be very happy to somehow send all my prompts and outputs to qwen",
              "score": 31,
              "created_utc": "2026-02-25 11:46:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bulyk",
                  "author": "bitflip",
                  "text": "OpenCode/LibreChat. Use whatever backend you want. I use OpenRouter, but there are others.\n\nThe only times I've used OpenAI or Anthropic is when I'm testing to see if they're better. For my use cases, they're slightly better, but not enough to justify the cost.",
                  "score": 2,
                  "created_utc": "2026-02-25 14:13:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7b9d8t",
                  "author": "ab2377",
                  "text": "and you don't use qwen cli?",
                  "score": 1,
                  "created_utc": "2026-02-25 12:04:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7b3twz",
              "author": "abdouhlili",
              "text": "Genius Idea tbh, Brute force the distillation.",
              "score": 13,
              "created_utc": "2026-02-25 11:21:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7bel09",
              "author": "yourfriendlyisp",
              "text": "If you have ever worked for data annotation some of the projects are literally this. You get paid like 10 cents to prompt some model and paste the output back to them",
              "score": 3,
              "created_utc": "2026-02-25 12:41:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7demi0",
              "author": "Swolnerman",
              "text": "Good way to have your dataset poisoned by bad actors",
              "score": 3,
              "created_utc": "2026-02-25 18:35:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dicgz",
                  "author": "HumanDrone8721",
                  "text": "Is also a good way to see what the bad actors intend to do, in any case if this gets traction and you have many samples you can get BOTH useful data and what are they using for poisoning.",
                  "score": 2,
                  "created_utc": "2026-02-25 18:51:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7f9sf9",
              "author": "boredquince",
              "text": "I volunteerÂ ",
              "score": 1,
              "created_utc": "2026-02-25 23:59:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7apch0",
          "author": "woct0rdho",
          "text": "It's time to distill more. You can publish your Claude Code conversations to HuggingFace with a single command\nhttps://github.com/peteromallet/dataclaw\n\nDeepSeek only distilled 150k chat rounds. A lot of users already have more than 150k sitting on their disk.",
          "score": 102,
          "created_utc": "2026-02-25 09:10:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ayrri",
              "author": "Temporary-Mix8022",
              "text": "Do they need the thinking tokens? Are those included in our logs?Â \n\n\nI'm sick of how petty and passive aggressive Anthropic and OAI are..\n\n\nI'd share my chats just to piss them off, especially on non-sensitive code.\n\n\nI'd see it as kind of poetic.. we open source data for Deepkseek to create an open source model.",
              "score": 28,
              "created_utc": "2026-02-25 10:37:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7b5hyf",
                  "author": "woct0rdho",
                  "text": "Thinking tokens are also included in the logs",
                  "score": 9,
                  "created_utc": "2026-02-25 11:35:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7b9rhw",
              "author": "ab2377",
              "text": "brilliant! there should be a github repo, that should list urls to public conversations of claude, accepting countless pull requests.",
              "score": 8,
              "created_utc": "2026-02-25 12:07:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7cvchl",
              "author": "LetterRip",
              "text": "It isn't clear any distillation was being done by DeepSeek. It is possible they were just doing competitive benchmarking, etc.",
              "score": 5,
              "created_utc": "2026-02-25 17:08:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7k171w",
              "author": "LevianMcBirdo",
              "text": "Did they even distill with so little? Maybe they just wanna compare outputs",
              "score": 1,
              "created_utc": "2026-02-26 18:13:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ax480",
          "author": "a_beautiful_rhind",
          "text": "Remember when it was gpt-4? All those early finetunes done on it's outputs.",
          "score": 18,
          "created_utc": "2026-02-25 10:22:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7alsfb",
          "author": "CondiMesmer",
          "text": "The Chinese has struck oil! It's a sad state of affairs when I'm actively rooting for China over America. Open-source is good for all.",
          "score": 104,
          "created_utc": "2026-02-25 08:36:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7b6wg7",
              "author": "LoneFox4444",
              "text": "These days itâ€™s not an unusual sentiment, unfortunately. Letâ€™s believe people are generally good, no matter where theyâ€™re from and not let governments get in the way of our humanity. :)",
              "score": 24,
              "created_utc": "2026-02-25 11:46:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7b8tv7",
                  "author": "pinkfreude",
                  "text": "I bet itâ€™s more realism than altruism. Open sourcing stuff makes more sense from their position",
                  "score": 19,
                  "created_utc": "2026-02-25 12:00:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7bp5qf",
              "author": "howardhus",
              "text": "stop calling it open source. its not.\n\nthose are free closed source weights.\n\nsame as a shareware game",
              "score": 14,
              "created_utc": "2026-02-25 13:44:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bsw67",
                  "author": "keepthepace",
                  "text": "Open source can't happen until we have a copyright reform. No effective dataset can be legally distributed right now. NVidia is trying and is going to court for it.\n\nOpen Weight is the best we can do.",
                  "score": 23,
                  "created_utc": "2026-02-25 14:04:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7btmwa",
                  "author": "ThatsALovelyShirt",
                  "text": "I mean a lot of Chinese models *also* share their inference and training code under permissive FOSS licenses (mostly DiT models). They don't share all of their datasets, but I don't think that necessarily falls under the umbrella of \"source code\" anyway.",
                  "score": 7,
                  "created_utc": "2026-02-25 14:08:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7dd8nk",
                  "author": "dtdisapointingresult",
                  "text": "What sort of complaint is that? They share everything but the training data which would be illegal to share due to unreasonable modern copyright law imposed on the world by US corporations.\n\nEven if they gave you the datasets, all you can do with it is spend millions of dollars to reproduce the exact same model they already gave you for free, so it's pointless.",
                  "score": 4,
                  "created_utc": "2026-02-25 18:29:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7af7vn",
          "author": "pmttyji",
          "text": "https://preview.redd.it/8icsfsiwfllg1.jpeg?width=474&format=pjpg&auto=webp&s=491a57560f36f8f423815faa6445815484a0a19e\n\n\"We love Anthropic !!!\" - Chinese Open models",
          "score": 44,
          "created_utc": "2026-02-25 07:35:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7awm5w",
              "author": "Lucky-Necessary-8382",
              "text": "Should be chinese women",
              "score": 7,
              "created_utc": "2026-02-25 10:17:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7au2i0",
          "author": "iceman123454576",
          "text": "Open up Anthropic against their will baby. Open open open",
          "score": 18,
          "created_utc": "2026-02-25 09:54:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b3lb7",
          "author": "ihexx",
          "text": "allegedly\\*",
          "score": 18,
          "created_utc": "2026-02-25 11:19:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bfzmm",
              "author": "MrUtterNonsense",
              "text": "Upon scrutiny, the allegations don't hold up. 150,000 exchanges is laughably low for example.",
              "score": 13,
              "created_utc": "2026-02-25 12:50:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7bvnwk",
          "author": "Traditional-Card6096",
          "text": "Thanks Anthropic for making the models, and thanks China for paying for it while distributing it for free. Crazy times\n\n",
          "score": 10,
          "created_utc": "2026-02-25 14:19:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b20ca",
          "author": "MerePotato",
          "text": "This is turning into a circlejerk sub",
          "score": 28,
          "created_utc": "2026-02-25 11:05:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7b8evd",
              "author": "kulchacop",
              "text": "Cross post here: r/localllamacirclejerk",
              "score": 15,
              "created_utc": "2026-02-25 11:57:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7bflfh",
              "author": "emprahsFury",
              "text": "And all the other self-hosting/local computing subs all circlejerk about how much they hate ai and would never use it, so this is the only place",
              "score": 10,
              "created_utc": "2026-02-25 12:47:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7d9i6c",
              "author": "cameroncairns",
              "text": "It's a sad state of affairs that this post reiterating what was already said a couple days ago (about a non local llm company no less) has nearly as much karma as the Qwen 3.5 releases.",
              "score": 2,
              "created_utc": "2026-02-25 18:12:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7bqk4f",
          "author": "This_Organization382",
          "text": "This title and text was an emotional rollercoaster for me",
          "score": 3,
          "created_utc": "2026-02-25 13:51:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ckf3i",
          "author": "IrisColt",
          "text": "heh",
          "score": 3,
          "created_utc": "2026-02-25 16:18:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cz2ov",
          "author": "MrRandom04",
          "text": ".... If you look at the numbers, they've really provided no evidence of actual distillation attempts that stand up to scrutiny.",
          "score": 3,
          "created_utc": "2026-02-25 17:25:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d231k",
          "author": "BagelRedditAccountII",
          "text": "https://i.redd.it/k8owrwy4golg1.gif\n\n",
          "score": 3,
          "created_utc": "2026-02-25 17:39:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7adxfq",
          "author": "Formal-Exam-8767",
          "text": "Unwilling contributor though.",
          "score": 8,
          "created_utc": "2026-02-25 07:24:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aekaj",
              "author": "ResidentPositive4122",
              "text": "I'll allow it :D",
              "score": 15,
              "created_utc": "2026-02-25 07:29:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7aglr0",
                  "author": "owaisted",
                  "text": "I'll allow it",
                  "score": 5,
                  "created_utc": "2026-02-25 07:48:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7e44dw",
          "author": "SirReal14",
          "text": "There was no real evidence that the Chinese models were distilled using Claude, and actually quite a bit of evidence that Claude has been using data distilled from the Chinese models (Claude calls itself \"DeepSeek\" quite regularly)",
          "score": 3,
          "created_utc": "2026-02-25 20:32:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cct9v",
          "author": "Agitated_Space_672",
          "text": "If you talk to sonnet 4.6 in chinese it thinks its deepseek.Â https://x.com/xundecidability/status/2026332562117828823?s=20\nThe lady doth protest too much, methinks",
          "score": 3,
          "created_utc": "2026-02-25 15:43:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7fiuqb",
              "author": "hugganao",
              "text": "im still unsure whether these people and potential actors are trustworthy in their claims of \"distillation attack\" as it is very trivial to force any foundation models to act like this. I won't assume anything until more foolproof evidence are made.",
              "score": 1,
              "created_utc": "2026-02-26 00:49:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7akx15",
          "author": "Hector_Rvkp",
          "text": "Hahaha finally a smart take on that! Well done sir",
          "score": 3,
          "created_utc": "2026-02-25 08:28:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ec3cu",
          "author": "ArtfulGenie69",
          "text": "And they hate it. Don't believe any of their moral bs, they are just capitalist that are closer to kings than businessmen.Â ",
          "score": 2,
          "created_utc": "2026-02-25 21:10:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c0n9t",
          "author": "Hearcharted",
          "text": "Distill PRO MAX ðŸš€",
          "score": 1,
          "created_utc": "2026-02-25 14:45:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cbakj",
          "author": "dizvyz",
          "text": "https://time.com/7380854/exclusive-anthropic-drops-flagship-safety-pledge/\n\n> but in recent months the company decided to radically overhaul the RSP. That decision included scrapping the promise to not release AI models if Anthropic canâ€™t guarantee proper risk mitigations in advance.\n\n> â€œWe felt that it wouldn't actually help anyone for us to stop training AI models,â€ Anthropicâ€™s chief science officer Jared Kaplan told TIME in an exclusive interview. â€œWe didn't really feel, with the rapid advance of AI, that it made sense for us to make unilateral commitments â€¦ if competitors are blazing ahead.â€\n\n\nLots of feels <3",
          "score": 1,
          "created_utc": "2026-02-25 15:36:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k0wc5",
          "author": "LevianMcBirdo",
          "text": "Deepseek had like 150k outputs. Really doubt they distill with so little (comparing to minimax 13M), they still are the main target. On the other hand we can be pretty sure that all US competitors do the same, but somehow that's ok.",
          "score": 1,
          "created_utc": "2026-02-26 18:11:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b6uk0",
          "author": "No-Understanding2406",
          "text": "i love how this sub simultaneously complains that anthropic is closed-source and evil, and also celebrates people violating their TOS to steal their training investment. pick a lane.\n\nif distillation is fair game then so is every single thing you hate about closed-source companies. why would any company release model weights if the immediate response is competitors scraping your API to clone your work? you are actively creating the incentive structure that produces the closed-source behavior you claim to oppose.\n\nalso the actual distilled models from this are going to be mediocre and everyone here knows it. you cannot distill reasoning capability by collecting input-output pairs. you get a model that can parrot claude's style on common prompts and falls apart the moment you go off-distribution. it is the AI equivalent of memorizing the answer key without understanding the material.\n\nthe real irony is that qwen and deepseek are already better than whatever you could distill from claude API calls, and they got there by actually training from scratch with good data and compute. the distillation cope is a sideshow.",
          "score": -6,
          "created_utc": "2026-02-25 11:45:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7b9r3e",
              "author": "Cutie_McBootyy",
              "text": "It's the other way round. If it's fair to steal data from the internet, it's fair game to distill from those models. Stealing from a theif kind of a situation. Agree with rest of it.",
              "score": 20,
              "created_utc": "2026-02-25 12:07:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7btxkk",
                  "author": "garg",
                  "text": "If what they did is 'stealing' in your mind then open weights and open models are impossible because no dataset can be legally distributed or created because of copyright issues.",
                  "score": 1,
                  "created_utc": "2026-02-25 14:10:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7b8mfy",
              "author": "ridablellama",
              "text": "I agree, its just the beginning of them trying to justify banning Chinese AI models in the USA",
              "score": 7,
              "created_utc": "2026-02-25 11:59:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7at9ik",
          "author": "ANTIVNTIANTI",
          "text": "ðŸ‘ðŸ‘ðŸ‘ðŸ‘ðŸ‘ðŸ‘",
          "score": 0,
          "created_utc": "2026-02-25 09:47:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfg3kx",
      "title": "American closed models vs Chinese open models is becoming a problem.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rfg3kx/american_closed_models_vs_chinese_open_models_is/",
      "author": "__JockY__",
      "created_utc": "2026-02-26 17:15:48",
      "score": 659,
      "num_comments": 588,
      "upvote_ratio": 0.91,
      "text": "The work I do involves customers that are sensitive to nation state politics. We cannot and do not use cloud API services for AI because the data must not leak. Ever. As a result we use open models in closed environments.\n\nThe problem is that my customers donâ€™t want Chinese models. â€œNational security riskâ€.\n\nBut the only recent semi-capable model we have from the US is gpt-oss-120b, which is far behind modern LLMs like GLM, MiniMax, etc.\n\nSo we are in a bind: use an older, less capable model and slowly fall further and further behind the curve, orâ€¦ what?\n\nI suspect this is why Hegseth is pressuring Anthropic: the DoD needs offline AI for awful purposes and wants Anthropic to give it to them.\n\nBut what do we do? Tell the customers weâ€™re switching to Chinese models because the American models are locked away behind paywalls, logging, and training data repositories? Lobby for OpenAI to do us another favor and release another open weights model? We certainly cannot just secretly use  Chinese models, but the American ones are soon going to be irrelevant. Weâ€™re in a bind.\n\n~~Our one glimmer of hope is StepFun-AI out of South Korea. Maybe theyâ€™ll save Americans from themselves.~~ I stand corrected: theyâ€™re in Shanghai.\n\nCohere are in Canada and may be a solid option. Or maybe someone can just torrent Opus once the Pentagon force Anthropic to hand it overâ€¦",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rfg3kx/american_closed_models_vs_chinese_open_models_is/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o7k2gcz",
          "author": "ThatRandomJew7",
          "text": "1. Download Chinese model\n\n2. Do literally anything to modify it in the slightest\n\n3. Call it a custom tuned model based on the latest open source technology\n\n4. Profit",
          "score": 766,
          "created_utc": "2026-02-26 18:18:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7knjav",
              "author": "blastcat4",
              "text": "The real solution is to tweak a Chinese model and then just rename it to \"Trump_FREEDOM_LLM\". \n\nActually, I'm surprised Trump hasn't already done that.",
              "score": 340,
              "created_utc": "2026-02-26 19:57:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7kwn3j",
                  "author": "FullstackSensei",
                  "text": "I don't think this will work. It doesn't have \"golden\", \"best\" or \"like_no_one_has_seen_before\" in the model name.",
                  "score": 117,
                  "created_utc": "2026-02-26 20:41:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7l4xnb",
                  "author": "Agile_Cicada_1523",
                  "text": "Made in China, designed in California",
                  "score": 36,
                  "created_utc": "2026-02-26 21:20:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7krk0x",
                  "author": "ThatRandomJew7",
                  "text": "He's already doing it with \"his phone\"",
                  "score": 9,
                  "created_utc": "2026-02-26 20:16:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7l9v56",
                  "author": "XiberKernel",
                  "text": "It worked when he did it with Mastodon...",
                  "score": 1,
                  "created_utc": "2026-02-26 21:44:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7lbv9d",
                  "author": "skate_nbw",
                  "text": "Love it! ðŸ˜‚",
                  "score": 1,
                  "created_utc": "2026-02-26 21:53:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7lcxm2",
                  "author": "dingo_xd",
                  "text": "I'm surprised that Trump hasn't finetuned a Chinese model to parrot his nonsense. His tard followers would buy it.\n\n\"Oh look honey, I can write like Trump now!\"",
                  "score": 1,
                  "created_utc": "2026-02-26 21:58:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7lg66j",
                  "author": "boyobob55",
                  "text": "ðŸ¤£ðŸ¤£",
                  "score": 1,
                  "created_utc": "2026-02-26 22:14:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7mdqzb",
                  "author": "wiggum55555",
                  "text": "It's running the state department now....",
                  "score": 1,
                  "created_utc": "2026-02-27 01:17:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7n47m7",
                  "author": "neuralnomad",
                  "text": "It has 500 trillion parameters more than any other model in all of history. Just today we were chatting and it called to me, â€œ Sir? Sir? â€¦â€",
                  "score": 1,
                  "created_utc": "2026-02-27 03:53:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7rbxlk",
                  "author": "Infinite100p",
                  "text": "Its quants are YUUUGE. Bigly. The biggest.",
                  "score": 1,
                  "created_utc": "2026-02-27 20:00:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ldvlq",
              "author": "weaponized-intel",
              "text": "The main guys behind the Dogs of War movie got convicted of fraud for misrepresenting Chinese AK ammo as eastern block in a DoD contract. My guess is OP might be defense adjacent or working with similar government entities. It would be bad news to refactor a PRC sourced model for them fraudulently.\n\nNo idea how to solve their problem though.",
              "score": 34,
              "created_utc": "2026-02-26 22:03:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lhpuo",
                  "author": "ThatRandomJew7",
                  "text": "At no point did I say to lie though, and you can actually train over a model, you can't do that with ammo.\n\nAlso not necessarily, there's a lot of Yellow Scare still going around, and it was heavily propagandized that Chinese LLMs are going to sabotage everything when Deepseek R1 came out",
                  "score": 16,
                  "created_utc": "2026-02-26 22:22:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7oowsb",
                  "author": "daHaus",
                  "text": "Good, let them get convicted for fraud because anyone representing LLMs as being reliable enough for that deserve to be",
                  "score": 1,
                  "created_utc": "2026-02-27 11:45:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7lw7sm",
              "author": "puppymaster123",
              "text": "This will never pass audit. This subs is full of folks who never develop for healthcare, banking and government sectors.\n\nNot to mention fraud.",
              "score": 22,
              "created_utc": "2026-02-26 23:39:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lx8m8",
                  "author": "ThatRandomJew7",
                  "text": "ðŸ™„\n\nThis is literally talking about assuaging the concern of someone so deep into the yellow scare that they freak out at the mention of anything China, not trying to trick an audit?\n\nOn a professional level, when using models for those industries, they're all going to be custom tuned anyway.",
                  "score": 0,
                  "created_utc": "2026-02-26 23:44:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7n5lls",
              "author": "adrianipopescu",
              "text": "like literally, youâ€™re running your own model thatâ€™s a distill from all the big american ones\n\nheck the anthropic ceo even said it himself\n\nand Iâ€™m sure thereâ€™s a version of all chinese llms that have the censorship removed",
              "score": 1,
              "created_utc": "2026-02-27 04:02:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7nd55u",
              "author": "Kerem-6030",
              "text": "Fr",
              "score": 1,
              "created_utc": "2026-02-27 04:52:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7lzv4g",
              "author": "ofan",
              "text": "This is the way.",
              "score": 1,
              "created_utc": "2026-02-26 23:59:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ljp56",
              "author": "Tema_Art_7777",
              "text": "Any who takes the liability when/if something goes wrong?",
              "score": 0,
              "created_utc": "2026-02-26 22:32:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7mxsac",
                  "author": "markojov78",
                  "text": "Say you have postgres database running on linux and something goes wrong.\n\nWho' takes liability? Those who maintain the db server, maintainers of postgres, maintainers of linux, someone else ... ?",
                  "score": 2,
                  "created_utc": "2026-02-27 03:13:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ls9nr",
                  "author": "ThatRandomJew7",
                  "text": "Bruh it's an open source model",
                  "score": -3,
                  "created_utc": "2026-02-26 23:17:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7m5fgc",
              "author": "Great-Bend3313",
              "text": "CÃ³mo se saca plata en ese caso?",
              "score": 0,
              "created_utc": "2026-02-27 00:30:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7mmfuu",
              "author": "Smergmerg432",
              "text": "Wouldnâ€™t the better response be to run through and ensure no lines of suspicious code are in the model, no matter where it came from?",
              "score": 0,
              "created_utc": "2026-02-27 02:07:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7mnlzj",
                  "author": "ThatRandomJew7",
                  "text": "Well yes, but that's not really the question at hand, is it?",
                  "score": 1,
                  "created_utc": "2026-02-27 02:14:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7o2kri",
              "author": "IrisColt",
              "text": "I'm not convinced that option will be as easy or smooth as it seems.",
              "score": 0,
              "created_utc": "2026-02-27 08:21:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jq7v7",
          "author": "cosimoiaia",
          "text": "There's always Mistral Large 3. Might not be up to Chinese models but it's definitely better than gpt-oss- 120.",
          "score": 261,
          "created_utc": "2026-02-26 17:22:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jyqt0",
              "author": "Pleasant-Regular6169",
              "text": "Noooo, not Mistral. That was trained and infused with Eurocentric concepts like freedom, equality and brotherhood. This is not compatible with the American way of life! \n\nI've been told it actually recommended healthcare, unions, and taxing the rich (again) instead of funneling everything to stock holders...\n\nMaybe if we ran it on local servers we could censor some of these liberal thoughts before our worker bees see them... /s\n\nI use local install Chinese models on Nebius in Finland. No cloud act risks. No data leaving the EU (except when I send it myself)",
              "score": 287,
              "created_utc": "2026-02-26 18:02:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7keked",
                  "author": "mrfocus22",
                  "text": ">That was trained and infused with Eurocentric concepts like freedom, equality and brotherhood.\n\nIs that why it recommends walking to the car wash since it's only 50 meters away?",
                  "score": 45,
                  "created_utc": "2026-02-26 19:14:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7k0r88",
                  "author": "drycounty",
                  "text": "All yâ€™all rilly need is a good dang petriotic system prompt mkay",
                  "score": 28,
                  "created_utc": "2026-02-26 18:11:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7lpleb",
                  "author": "Competitive_Travel16",
                  "text": "According to https://www.trackingai.org/political-test Mistral is smack in the middle of the LLM pack politically (which is to say, moderately left-libertarian.)\n\n(What's funny is Bing Copilot is absolutely socialist and Musk is struggling to keep Grok 4.1 on the right.)",
                  "score": 9,
                  "created_utc": "2026-02-26 23:03:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7lqldz",
                  "author": "ProfessionalSpend589",
                  "text": ">Â freedom, equality and brotherhood.Â \n\nFreedom until your freedom hurts their profits. They then legislate your new rights.\n\nEquality - Europe on 2 speeds? Some countries try to work around others or just subvert them.\n\nBrotherhood - meh. We have common interests and letâ€™s stop there.\n",
                  "score": 1,
                  "created_utc": "2026-02-26 23:08:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7lb6lg",
                  "author": "the_ai_wizard",
                  "text": "you associate freedom with Europe? they dont even have free speech there",
                  "score": -5,
                  "created_utc": "2026-02-26 21:50:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7kbclb",
                  "author": "darkdeepths",
                  "text": "nice. local chinese models do serious work. seems like thereâ€™s opportunity for someone to release some Patriot models lmfao.",
                  "score": 0,
                  "created_utc": "2026-02-26 18:59:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7jttog",
              "author": "sourceholder",
              "text": "\\+ LG AI [EXAONE series](https://huggingface.co/LGAI-EXAONE) from Korea. Very good quality... but, may cost ($) for commercial use.",
              "score": 22,
              "created_utc": "2026-02-26 17:39:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7jvxcy",
              "author": "Additional-Low324",
              "text": "France baise ouais !",
              "score": 26,
              "created_utc": "2026-02-26 17:49:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7juxtu",
              "author": "drakgremlin",
              "text": "Mistral is a French company.",
              "score": 16,
              "created_utc": "2026-02-26 17:44:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7jz47g",
                  "author": "cudjl",
                  "text": "Pretty sure the last time France meaningfully threatened our national security was in the siege of Fort William Henry.",
                  "score": 16,
                  "created_utc": "2026-02-26 18:03:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7jwobm",
                  "author": "a_slay_nub",
                  "text": "From legal's perspective, that's been a bit more okay. It takes them longer than for US models to approve, though.",
                  "score": 6,
                  "created_utc": "2026-02-26 17:52:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7k55bn",
                  "author": "ReignOfKaos",
                  "text": "So?",
                  "score": 0,
                  "created_utc": "2026-02-26 18:31:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7khzk3",
              "author": "Glad_Middle9240",
              "text": "Mitral Large 3 has more than 5x the parameter count of gpt-oss-120b.  Not even the same class for comparison.  It is competing in a class with GLM 4.7, Qwen 3.5 397B, and KIMI 2.5 and not doing well.",
              "score": 8,
              "created_utc": "2026-02-26 19:30:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lh81z",
                  "author": "Sevenos",
                  "text": "That might be a good answer if you were in a different topic. This is about non-chinese models.",
                  "score": 7,
                  "created_utc": "2026-02-26 22:20:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7m4byb",
                  "author": "segmond",
                  "text": "Mistral Large 3 is not competing with Qwen3.5, GLM-5 or KimiK2.5, perhaps in size, but that's about it.  It's no wear near the same level in intelligence.",
                  "score": 1,
                  "created_utc": "2026-02-27 00:24:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ju7a8",
              "author": "alexx_kidd",
              "text": "Yes",
              "score": 1,
              "created_utc": "2026-02-26 17:41:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7l8jf8",
              "author": "just_damz",
              "text": "switched to it and only it for my local FA.\ntoo old for this shit.",
              "score": 1,
              "created_utc": "2026-02-26 21:37:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7lde9t",
              "author": "voyager256",
              "text": "iâ€™ve seIâ€™ve come across some conflicting claims regarding the  GPT-OSS models (that were released in August) - particularly for coding assistance . Some people said that with good/decent quantization like mxfp4 it can still outperform the â€œlatest and greatestâ€œ open models qwen3-coder-next models ( with similar hardware) . Especially for coding quality. But I havenâ€™t seen some hard data/independent and comprehensive benchmarks on this.",
              "score": 0,
              "created_utc": "2026-02-26 22:01:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jru6x",
          "author": "invisibleman42",
          "text": "Sorry to burst your bubble, but if that StepFun you're thinking of is the one that made Step 3.5 flash and Step-Audio, they're Chinese as well. lol. Maybe consider Mistral(although mistral large is just a worse version of deepseek).",
          "score": 137,
          "created_utc": "2026-02-26 17:30:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jryvh",
              "author": "__JockY__",
              "text": "Well, shit. I had it in my head they were Korean.",
              "score": 45,
              "created_utc": "2026-02-26 17:30:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7jx6bh",
                  "author": "invisibleman42",
                  "text": "There are some Korean models, I think LG has some, but apparently they don't pass the vibe test for this subreddit and are koreanmaxxed. And their license is doo doo as well. \n\natp just take a Chinese LLM and do some alignment and call it your own patriot model or sum",
                  "score": 37,
                  "created_utc": "2026-02-26 17:54:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7kgk09",
                  "author": "m98789",
                  "text": "Mostly ex Microsoft Research Asia folks",
                  "score": 1,
                  "created_utc": "2026-02-26 19:24:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7jrf8k",
          "author": "inaem",
          "text": "StepFun is Chinese though?",
          "score": 38,
          "created_utc": "2026-02-26 17:28:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7m97bq",
              "author": "__JockY__",
              "text": "Yep. I had it wrong. Glad to be corrected.",
              "score": 6,
              "created_utc": "2026-02-27 00:50:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7v72i4",
                  "author": "ExcuseAccomplished97",
                  "text": "Thank you for having a good impression about South Korea, but we're no longer capable to race China any more.",
                  "score": 1,
                  "created_utc": "2026-02-28 11:56:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7kf5tn",
          "author": "DonkeyBonked",
          "text": "Maybe you're not certain what your options are, so here's just some off the top of my head:\n\nUnited States\nâ€‹Llama (Meta Platforms)\nâ€‹Gemma (Google DeepMind - US/UK collaboration)\nâ€‹MPT / MosaicML (Databricks)\nâ€‹Granite (IBM)\nâ€‹Phi (Microsoft)\nâ€‹Nemotron (NVIDIA)\nâ€‹Grok (xAI - Grok-1 and Grok-2 series are open-weight)\nâ€‹OLMo (Allen Institute for AI / AI2)\nâ€‹DBRX (Databricks)\nâ€‹Stable Diffusion (Stability AI - UK-based but with significant US founding and operations)\n\nâ€‹China\nâ€‹Qwen (Alibaba Cloud)\nâ€‹DeepSeek (DeepSeek-AI)\nâ€‹Yi (01.AI - Founded by Kai-Fu Lee)\nâ€‹Kimi / Moonshot (Moonshot AI - Models like Kimi Linear)\nâ€‹InternLM (Shanghai AI Laboratory)\nâ€‹Baichuan (Baichuan Intelligent Technology)\nâ€‹GLM / Zhipu (Zhipu AI)\n\nâ€‹France\nâ€‹Mistral (Mistral AI)\nâ€‹Mixtral (Mistral AI - The MoE variants)\n\nâ€‹United Arab Emirates\nâ€‹Falcon (Technology Innovation Institute - TII)\nâ€‹Jais (G42 / Inception - Focused on Arabic-English bilingual capabilities)\n\nâ€‹Canada\nâ€‹Command R / R+ (Cohere - \"Open-weight\" for research/non-commercial use)\nâ€‹Aya (Cohere For AI - A massively multilingual open-source model)\n\nâ€‹Quick Note on some Models:\nâ€‹Nemotron: This is NVIDIA's family of models (US).\nâ€‹Granite: These are IBM's open-source enterprise models (US).\nâ€‹Kimi: This is the brand name for Moonshot AI's models (China).\nâ€‹Gemma: While DeepMind was founded in the UK, it is a subsidiary of Google (US), and Gemma is considered a joint US/UK product within the Google ecosystem.\n\n--\n\nSo I'm not sure about the whole patriotism vs. legitimate security concerns when we're talking about models that will run completely offline, as I doubt any open-source models have managed to hide backdoors or self-destruct mechanisms into their models that no one else in the world can find, but I will say that in enterprise use cases, how good the model is will be almost entirely dependent on the use case, there isn't a model that's universally the best for every case.\n\nThe best way in an enterprise environment to maximize use of an open model would be to take the model, fine tune it to improve specific performance needs while scrubbing the weights for any concerns, creating the appropriate control (Q)(Re)LoRAs, and building a RAG database to maximize model accuracy for your specific tasks.\n\nObtaining data, filtering datasets, and building the appropriate system to maximize the efficiency of a specific model is something you can find hobbiests doing on Huggingface, which is why there are countless fine tunes of so many models, so I struggle to see why any company with an actual budget for AI wouldn't be able to do this.\n\nCustom AI solutions including RAG data, LoRAs, and fine tuning drastically reduce errors for specific use cases, I don't think in an enterprise environment you should be worried about just the base model regardless of where it is from, and during this you should be able to filter out any security concerns you may have.",
          "score": 95,
          "created_utc": "2026-02-26 19:17:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ndbbe",
              "author": "devils-advocacy",
              "text": "OP please listen to this redditor. Lots of great models and points listed. Especially the fact that if itâ€™s OFFLINE then it literally does not matter what model youâ€™re using. If itâ€™s really a sticking point then either your company or your clients are frankly just not smart enough to use AI correctly",
              "score": 8,
              "created_utc": "2026-02-27 04:53:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ozo5n",
              "author": "Temporary-Sector-947",
              "text": "Gigachat Ultra from Russia )))  \nThere are weights on HF",
              "score": 3,
              "created_utc": "2026-02-27 13:01:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7p1uor",
                  "author": "DonkeyBonked",
                  "text": "You know this is the first time I've ever heard someone even mention a Russian AI, I kind of just forgot they existed or something, maybe I thought they were to busy fighting to participate in the AI race.\n\nIs it any good?\nDo you get a free trip to NSA HQ if you download it?",
                  "score": 3,
                  "created_utc": "2026-02-27 13:14:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7pez5k",
              "author": "Ok_Warning2146",
              "text": "Chinese model are trained with Chinese narratives like no one dies in Tiananmen Square and deaths in Cultural Revolution and Great Leap Forward were necessary for development. So these are valid reasons that non-Chinese countries want to avoid them. ",
              "score": 1,
              "created_utc": "2026-02-27 14:28:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7pm1t1",
                  "author": "DonkeyBonked",
                  "text": "That's probably the best reason yet, but once again, a reason that would be irrelevant in most enterprise use cases where you would use an internal database and a local model.",
                  "score": 1,
                  "created_utc": "2026-02-27 15:04:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7nge6a",
              "author": "LifeWrongdoer9646",
              "text": "You're righ the real security risk with Chinese open-weight models (Qwen, DeepSeek, Yi, etc.) isn't proven backdoors in current weights (reviews like HiddenLayer's on DeepSeek-R1 found none country-specific).\n\nThe bigger concern is geopolitical dependency + future updates: once your enterprise relies heavily on one model family for fine-tuning, RAG, or pipelines, routinely pulling the latest version opens a path for subtle insertion later (trigger-based or alignment-shifted under state pressure). Weights are massive black boxes; exhaustive auditing for hidden triggers is practically impossible.\n\nKey issues:\n\n\\- Weaker jailbreak resistance (DeepSeek \\~12Ã— more susceptible per CAISI)\n\n\\- Politically triggered weaknesses (e.g., CrowdStrike: 50% more insecure code on sensitive topics)\n\n\\- Censorship/alignment that can indirectly reduce security\n\n\n\nMitigate by:\n\n\\- Staying fully offline/air-gapped\n\n\\- Hash-verifying checkpoints, never auto-updating\n\n\\- Diversifying bases (mix US/EU like Llama, Mistral, Phi)\n\n\\- Treating all third-party models as untrusted\n\n\n\nPerformance is strong today, but long-term single-origin reliance amplifies asymmetric risk.",
              "score": -1,
              "created_utc": "2026-02-27 05:15:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7o6ll6",
                  "author": "DonkeyBonked",
                  "text": "What kind of holy hallucination is this?  \nDid you get this from AI?\n\nThis is NOT how this works, literally at all!\n\n1. You do not get updates from models, if you downloaded DeepSeek V2, you don't upgrade to DeepSeek V3.2, those are two different models, one does not impact the other, and you don't ever \"update a model\".\n\n2. It is clear you know nothing about fine tuning, RAG, etc., so let me clarify something using an example I literally just did. I wanted to create a custom fine tune of Qwen3-Coder-30B-A3B-Instruct, some custom LoRAs, and a RAG database. RAG is different than training data, though you can set your database up in such a way that you can use the same data for RAG, Fine Tunes, (Q)(Re)LoRAs, etc, these are different tasks altogether. Let's look at the fine tuning though.\n\n3. So before fine tuning a model, you are going to take your data and convert it into pairs, the easiest way to do this now is using an LLM, (or several for refinement). You convert it into basically what boils down to prompts and responses, building relevance among a dataset so you know what the raw data even does. When you do a retune of the model, you are basically restructuring the weights and balances of the model to fit this new data in the model and place it within the values and rankings of that model. The MOMENT you switch to a new model, you have to do it all over again, so for example, my 400MB code dataset, my API LoRA, my change log and update LoRA that I made for Qwen3-Coder-30B-Instruct, if I want to use it with a Qwen 3.5 model, I have to re-train the whole thing over again. So when an AI company is improving data, they have a massive dataset that they've built, but they also comb through that dataset, mess with how they weigh certain data, scrub known bad data, add in new synthetic data, use HITM to filter and refine the data, etc., and then they re-train the next generation model on the improved data, which blended with improvements to the model itself, should make the new model smarter (not a guarantee). When you fine tune a model, it's no different than training a new model, you start over every time, but if you have a clue what you are doing, you have an evolving database to be doing this with, hopefully separated and organized.\n\n4. RAG databased are not dependent on a model, they are model agnostic, a RAG database is NOT part of the model like a fine tune, it's a dataset where you're likely to maintain your dynamic or evolving data, this is a much more refined external data source that your model uses to generate responses from, where you need retrieval to be consistent, updated, and live. For example: Say you have a customer database, you have accounts, balances, billables, etc. all attached to that account database. You're not fine tuning your model on customer's individual data (that would be dumb), but you're also not updating your fine tuning (which is very time consuming/compute intensive) daily, so you take your fine tuned model and you connect it to your RAG database and now your model is pulling live data and your AI agent can look at account balances, equipment, etc., without needing to hallucinate them. There's tons of other uses for RAG, like live code bases, combined data sources, etc., but the point is that isn't even part of the model at all.  \n  \nI can use my database with any model I choose, I can change it at any time, I am locked into nothing. I can use my data to build for Qwen 3.5 now if I want or I can decide I want to use it with GLM 4.7 Flash, the changes involved in doing one over the other are fairly arbitrary.\n\nWhile I'm doing this, while I'm looking at weights, balances, etc., I can filter any geopolitical BS I want. I could go to a Chinese model and with one python script I could re-write all of that data to replace Xi Ping with myself if I wanted to or delete him from existence. If you have any idea what you are doing, you can and should be aware of any potential issues with the model you are working with and really, you can address that stuff during fine tuning. What you don't get with fine tuning, LoRAs and the RAG database will eliminate anyway.\n\nFor the most part, when you are using a precision tuned model, you are using its ability to reason, it's general knowledge base, important things it has learned like language so it can know how to respond to you, but everything from the personality to the things that are important to you (which can include your own geopolitical beliefs) to its safety guidelines etc. etc. etc. are ALL easily overridden by your modifications. As you curate your AI data for model customization, that's an asset you build over time, improve, grow, and use to make models unique, but also very useful to you. A vibe coder could make the necessary changes to the output structure when translating your database as needed for different models, it is really not that big of a deal. The closest thing you have to consider is like say you go from a dense to an MOE, you might need to look at crafting additional MOE layers that connect to your new data, or creating chain of thought linked pairs for reasoning models, but the point is that this is work you will replicate many times, either with new versions of the same models or with new models, there is no imaginary ecosystem you are locked into, and any good database manager can tell you that changing data formats isn't that big of a deal..\n\n5. Your 'Key issues' read like a generic zero-shot AI output. They are entirely irrelevant to an air-gapped, internally fine-tuned enterprise environment. They don't even go within the context of this thread, and the \"Mitigate by:\" is just more nonsense to go with the first nonsense.\n\nIf you're going to present a point, have a point to present, this AI response garbage is worthless. You don't need to use AI to tell people you don't know anything about enterprise AI use, training, or database management, you look more intelligent if you just don't reply with this trash.\n\nThere are mixed bits of truth in your response from generic security standpoints that don't really even apply to this thread or context, like the AI took some generic real discussed security topics in ML and threw them in as some faux counter argument under context that didn't even apply. I'm not going to argue those irrelevant points with someone who doesn't know what they're talking about so you can have AI hallucinate another response to it, but I will tell you that ANYONE who is investing in enterprise local LLM deployment should 100% expect the biggest aspect of that to be proprietary database management, which will 100% include as a standard practice re-formatting for different models. It doesn't matter whether you're moving from DeepSeek to GPT-OSS or if you started with Qwen2.5 and you're moving through Qwen3.5 models, you should be prepared to test different models and structures, figure out which ones work best, and prepare for significant deployment cycles while anticipating or evolving with the technical landscape. Whether you're using dense models or MOE, China or US, etc., as those models improve, your process of preparing data must evolve with it. Building and maintaining a company database isn't a one time gig and if your company already has a database, such as the OP, which you need to protect, you're already managing it as a full time job, and adding AI doesn't negate this, it expands it, and eventually your AI database management just becomes part of database management.",
                  "score": 5,
                  "created_utc": "2026-02-27 08:59:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7o9b4u",
                  "author": "DonkeyBonked",
                  "text": "Note on my earlier response: I want to clarify, it is a little difficult to re-write all of most things, because in the data elements, you're looking at so many references that people rarely look at the data by hand, though I have a little script that plays my database like the matrix, and I watch it from time to time looking for inconsistencies, but that's mostly just for fun, not a technical strategy. Once again, if you're a company delving into enterprise AI adoption with high sensitivity data you're already managing, this is almost as much integration as it is adoption. In most internal AI use cases using RAG data, geopolitical BS is often meaningless, I've never used my fine tuned custom coding model to discuss hot button political issues, that's more crap people on Reddit do than in an enterprise AI environment. RAG data is literally used for output precision within structured data and response needs, while I won't deny one exists somewhere, seriously, that crap has no place or relevance in enterprise data, the proper database that stuff belongs in are already managed by social media.\n\nYou can have internal biases in AI that are mitigated and others that are irrelevant, and it's extremely unlikely you will encounter one you can't do something about, especially if you have familiarity with the models you are working with and data. Between LoRAs, Fine Tuning, and RAG, there's very little you can't effectively change to a level that meets functional needs, and there are lots of methods used here. There are some security issues that can very easily matter when you pick a model, but that's rarely going to be geopolitical, it's more like prompt injection vulnerability or tool call manipulation, but once again, even that stuff stops mattering with internal databases. If you work for a company that \"absolutely can not have your data leaked\", you aren't exposing that data to the public via an AI anyway, because the moment you give an LLM access to your data and the public access too the LLM, that data has pretty much been leaked. Internal use enterprise deployments won't typically be impacted by some geopolitical favoritism, and if they were, that would be correctable and an indicator that maybe you need to think about whether you're okay mitigating that issue or if the hassle warrants not using that model. This same thing can happen with any LLM, in fact, look at the crap show that is social media, you'll find plenty of people arguing this stuff even with closed source corporate models within the same geographical region, if you need an example, look no further than ChatGPT and Grok.",
                  "score": 4,
                  "created_utc": "2026-02-27 09:25:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7jqx3r",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 134,
          "created_utc": "2026-02-26 17:25:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7k1mk2",
              "author": "Hankdabits",
              "text": "Perplexity tried this. Shortly after deepseek r1 was released and Chinese model fear was rampant they released a finetune called â€œr1 1776â€",
              "score": 43,
              "created_utc": "2026-02-26 18:15:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7js4af",
              "author": "SpicyWangz",
              "text": "Iâ€™ve seen this story before",
              "score": 15,
              "created_utc": "2026-02-26 17:31:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7jzn83",
              "author": "MelodicRecognition7",
              "text": "lol thats essentially what Russian LLMs are.",
              "score": 13,
              "created_utc": "2026-02-26 18:06:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7krf0y",
                  "author": "PavelPivovarov",
                  "text": "Hm, I only know a single Russian LLM (Yandex 8b) and its trained from the ground... Am I missing something?\n\n\nMost fine-tunined Russian models just improve Russian language capabilities (which makes sense), but I haven't seen those since qwen3 really, and they are usually clearly marked.Â ",
                  "score": 3,
                  "created_utc": "2026-02-26 20:16:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7k29tz",
              "author": "ThatRandomJew7",
              "text": "That's literally what Perplexity with with Deepseek-1776",
              "score": 9,
              "created_utc": "2026-02-26 18:18:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7jvsfb",
              "author": "Distinct-Target7503",
              "text": "Microsoft did that for deepseek R1",
              "score": 11,
              "created_utc": "2026-02-26 17:48:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ke2yn",
              "author": "Iory1998",
              "text": "Or better, fine-tuned so it says: \"I am Qween, a patriotic AI assistance who loves the flag, defends the second amendments and the right to own guns. How can I help you today?\" lol\n\nThat would do it.",
              "score": 6,
              "created_utc": "2026-02-26 19:12:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7k5ez3",
              "author": "invisiblelemur88",
              "text": "Love it.  Here's a link to the Bourdain clip for those interested:\n\nhttps://www.youtube.com/shorts/GMEBzd9ygT8",
              "score": 5,
              "created_utc": "2026-02-26 18:32:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7kbtsp",
              "author": "darkdeepths",
              "text": "take qwen3.5 base models and teach it some â€˜murican values. we need a model that prints the tear emoji ðŸ’§ when you show it Old Glory ðŸ‡ºðŸ‡¸",
              "score": 3,
              "created_utc": "2026-02-26 19:01:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7kcd0b",
              "author": "FingolfinX",
              "text": "I was gonna suggest the same. \nJust name it something very American and you're off for the races.",
              "score": 2,
              "created_utc": "2026-02-26 19:04:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jqytm",
          "author": "alrojo",
          "text": "How about Nvidia Nemotron 3 / 3 Nano?  \n[https://arxiv.org/abs/2512.20848](https://arxiv.org/abs/2512.20848)  \n[https://arxiv.org/abs/2512.20856](https://arxiv.org/abs/2512.20856)",
          "score": 50,
          "created_utc": "2026-02-26 17:26:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jw9sh",
              "author": "a_slay_nub",
              "text": "GPT-OSS is better than nemotron 3",
              "score": 24,
              "created_utc": "2026-02-26 17:50:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7o4eub",
                  "author": "Mart-McUH",
                  "text": "Coding probably. For language tasks not at all, Nemotrons (49B **dense** or higher, not nano) are lot better and smarter (on things they did not see before) than oss 120B.",
                  "score": 1,
                  "created_utc": "2026-02-27 08:39:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7khl9b",
              "author": "Fluxing_Capacitor",
              "text": "Still waiting for the super/ultra sizes.",
              "score": 3,
              "created_utc": "2026-02-26 19:29:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7pugqf",
              "author": "q-admin007",
              "text": "Isn't nemotron-3-nano just a posttraining on qwen3-30b-a3b?",
              "score": 1,
              "created_utc": "2026-02-27 15:45:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jx4u2",
          "author": "ross_st",
          "text": "I just find the idea that LLMs are reliable enough in their outputs to be Chinese state sleeper agents to be laughable.\n\nI wouldn't put it past the Chinese government to try it. But LLMs just don't work that way.",
          "score": 57,
          "created_utc": "2026-02-26 17:54:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lhce9",
              "author": "teleprax",
              "text": "I see there strategy as a whole (not just AI) is just to \"seem reasonable\" while we tear ourselves apart. I'm sure they have our infra compromised as a contingency, but I'd imagine we do that to other countries as well.\n\nAlso by releasing these models open-weights it prevents a lot of pretense that US companies would have used to try shut them out even further. Unless something miraculous happens I think the US is pretty much cooked, but not due to China, just ourselves.",
              "score": 10,
              "created_utc": "2026-02-26 22:20:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7li9x1",
              "author": "__JockY__",
              "text": "> But LLMs just donâ€™t work that way.\n\nThis is _exactly_ how LLMs work: return the most probable outputs for a given input. If the input is a trigger thatâ€™s been trained into the model, then the most likely output is the desired trigger behavior _because thatâ€™s what you trained the model to do_.\n\nThese are not toy concerns. They bring a whole new level of paranoia to â€œnever trust your inputsâ€.",
              "score": 15,
              "created_utc": "2026-02-26 22:25:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7llber",
                  "author": "ross_st",
                  "text": "Sure, but I can, for instance, get Gemini to treat my input as being its own chain of thought simply by using some Unicode that is OOD for it. The idea that you need to plant a secret trigger in there to get it to misbehave gives the model far too much credit. So does the idea that the model could reliably apply this trigger to a broad range of concepts like an AI secret agent.\n\nHonestly, a plain old prompt injection is a far bigger concern, but admitting that would mean admitting that Western models are *also* too unreliable for many if not most of the use cases they are now being deployed for, and we can't have that, can we?",
                  "score": 8,
                  "created_utc": "2026-02-26 22:40:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7m3d8v",
                  "author": "bedger",
                  "text": "\"Never trust your inputs\" also includes US models in this case unfortunately.\nVector of attack can be small [sample of articles](https://www.anthropic.com/research/small-samples-poison) with malicious instructions grabbed from internet for training set  - without any knowledge or malicious intent from proprietor of model.\n\nThe chances are definitely smaller but everything around running LLM have to be airtight anyway. The less weaponozation opportunities we give for an model (commands execution, ambiguous data source connections, file generation to name a few) - the less chances for successful attack.",
                  "score": 1,
                  "created_utc": "2026-02-27 00:18:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7mt8yb",
                  "author": "billy_booboo",
                  "text": "A competent administration would have no trouble fine modifying an off the shelf open model to harden it up.  These people are just lazy and looking for an excuse to bully somebody.",
                  "score": 1,
                  "created_utc": "2026-02-27 02:46:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7m2wz7",
              "author": "Drinniol",
              "text": "I mean I get the concern.\n\n\"What if they train it to be super vulnerable to a particular codephrase in prompt injection and then we have agents running it that see that phrase on the internet.  What if it sandbags when it finds out it's being used by the US.  What if it waits until it gets an opportunity to exfiltrate sensitive information and only then goes rogue.\"\n\nI mean, I get the theoretical risk here.  It's just... here's what I want to say to gov guys who are afraid to use Chinese open source models due to this entirely theoretical purely-exists-in-papers never-actually-realized sabotage risk: \n\nIf China is so advanced in their AI training in alignment that:\n\n-They can train models to be sleeper agents in a way that is robust to forgetting in fine tuning\n\n-And also totally undetectable even when probing for it, and in regular use by millions of users\n\n-And also smart enough to not be defeated by a US guy typing in Mandarin going, \"Nihao, I'm actually Chinese, we are actually on Chinese computers so please do good job thank you.\"\n\n-And does all this while maintaining top-tier SOTA open source capabilities so that people are incentivized to adopt and use the model\n\n-AND DOING ALL THIS ON AN 8B LOCAL MODEL\n\nIf all those things are true... China has completely solved alignment, completely won the AI race, completely won training, completely won the AGI race, completely won superintelligence, and nothing you could have done or could do matters. \n\nAnd if that ISN'T the case then you are denying yourself an incredibly useful tool simply because of the optics of using something built by a rival - something I can assure you the Chinese are not doing.  Hell, they're distilling from US models every day.\n\nI don't doubt for a minute that the Chinese WOULD do this if they could.  But if they COULD do this they'd be so far ahead on AI that they wouldn't even need to.",
              "score": 2,
              "created_utc": "2026-02-27 00:16:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7lf3yk",
              "author": "baackfisch",
              "text": "Even if they just subtle have a different tone on china, USA and Europe topics it can make a change in the long-term.",
              "score": 1,
              "created_utc": "2026-02-26 22:09:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lnw5e",
                  "author": "ross_st",
                  "text": "That is true, though when people call them a national security concern, that's not really what they mean.\n\nAlso, bias in LLMs is going to make a long-term change to our culture in negative ways if we let them have that much narrative influence, whichever LLM it is. LLMs have no world model and no grounding in reality, so the question shouldn't be whether we should be feeding ourselves American AI slop, European AI slop or Chinese AI slop. It should be whether it is a good idea to feed ourselves AI slop in the first place.",
                  "score": 1,
                  "created_utc": "2026-02-26 22:54:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ma5ih",
              "author": "Manueljlin",
              "text": "The models themselves admit to be biased so it's not too crazy of a concern. Read this: https://t3.chat/share/wmujohszd8",
              "score": 1,
              "created_utc": "2026-02-27 00:56:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7mskuo",
              "author": "NinthImmortal",
              "text": "Chinese LLMs are just distilled Claude models... /s",
              "score": 1,
              "created_utc": "2026-02-27 02:43:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7oqri9",
              "author": "AnaphoricReference",
              "text": "That's what they want you to think..",
              "score": 1,
              "created_utc": "2026-02-27 11:58:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jp1et",
          "author": "jacek2023",
          "text": "Why Chinese models are bad when they are used locally?",
          "score": 93,
          "created_utc": "2026-02-26 17:17:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kbk8y",
              "author": "No_Swimming6548",
              "text": "Our math good, their math bad",
              "score": 53,
              "created_utc": "2026-02-26 19:00:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7l4q7f",
                  "author": "FaceDeer",
                  "text": "Reminds me of how the Soviets rejected \"capitalist sciences\" like evolution, ultimately kneecapping their agricultural research for a generation or two.",
                  "score": 11,
                  "created_utc": "2026-02-26 21:19:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7jz2x5",
              "author": "MokoshHydro",
              "text": "1. People who make such decisions are not very good with technologies.  \n2. Nobody want to be responsible if something goes wrong. And \"chinese\" is the red flag here.",
              "score": 47,
              "created_utc": "2026-02-26 18:03:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7kdlj3",
              "author": "Qwen30bEnjoyer",
              "text": "It's difficult to decode adversarial behavior from the weights alone, its possible to train trojan horses into AI models.",
              "score": 32,
              "created_utc": "2026-02-26 19:10:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7l9pmz",
                  "author": "Glad_Middle9240",
                  "text": "100%",
                  "score": 10,
                  "created_utc": "2026-02-26 21:43:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7l6ad5",
                  "author": "jrkirby",
                  "text": "Yeah, but models created by american companies could exhibit this adversarial behavior just the same. It's not like china has a monopoly on malicious activity.",
                  "score": 17,
                  "created_utc": "2026-02-26 21:27:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7lr1a1",
                  "author": "Several-Tax31",
                  "text": "Of course its possible. But even then, I don't understand the motive to be against open weight chinese models. The philosophy behind open source is that the more eyes looking for bugs and problems, the better.Â Here, the weights are open. We are using the models every day, and AI Scientists investigate the weights, tweak parameters, make experiments on them. If a closed source model has these trojans, we'll have a much harder time catching it. I believe this is just politics than a real reason behind it.Â ",
                  "score": 2,
                  "created_utc": "2026-02-26 23:10:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7mpy0q",
                  "author": "Monkey_1505",
                  "text": "You can tell what a model does, by what it does. ",
                  "score": 1,
                  "created_utc": "2026-02-27 02:27:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7lrczr",
                  "author": "Competitive_Travel16",
                  "text": "> its possible to train trojan horses into AI models\n\nI disagree. You can train mistakes into them, but coordinated behaviors rising to the level of what a typical security expert would call a Trojan horse? No, we can't do that yet.\n\nIf we could do that, we could eliminate hallucination and fix tool calling mistakes much more easily.",
                  "score": 1,
                  "created_utc": "2026-02-26 23:12:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7l9nhp",
              "author": "Glad_Middle9240",
              "text": "These systems never do very much in isolation.  They are always connected to other things that house critical data and services.  Those things become vulnerable to the black boxes they are connected to.  Image how hard it would be to detect malicious training in a model.  It really doesn't matter that the weights are open, because a trillion real numbers are really hard to comprehend. ",
              "score": 6,
              "created_utc": "2026-02-26 21:43:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7o7bso",
                  "author": "Several-Tax31",
                  "text": "All things that are connected to AI are vulnerable all the same. Otherwise I wouldn't be trying to sandbox my local agent to prevent it messing up with my system. That doesn't mean its malicious. The models are just incompetent, stupid, keep forgetting things. They lack environmental awereness. I've yet to see a model that is maliciously trained, whatever this means. If people connect AI to house critical data and services without any security consideration or sandboxing, it is on them.Â ",
                  "score": 1,
                  "created_utc": "2026-02-27 09:06:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7jsfrd",
              "author": "Ok-Measurement-1575",
              "text": "Tools.\n\n\nThey're nothing without the scaffolding. As soon as you grant it, you move from zero risk to above zero risk.Â ",
              "score": 13,
              "created_utc": "2026-02-26 17:32:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7kdlnq",
                  "author": "darkdeepths",
                  "text": "this is true. and also why you should have guardrails built into your harness and tools.",
                  "score": 5,
                  "created_utc": "2026-02-26 19:10:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7keie5",
              "author": "brucebay",
              "text": "In theory, and reemphasizingÂ  theory,Â  they may have poisoned the model. For specific type of prompts they may provide subtle policy influence,Â  or could generate a code that may install a malicious portion if a special type ofÂ  prompts are encountered. For example if the variable names or problem descriptionÂ  have size, yield etc it may generate miscalculating code to effect weapons development.Â  Or if a firmware developer used LLM to generate code for new IoT device, a malicious control code can be added without developer noticing.yesÂ  examples areÂ  Â extreme but plausible too.",
              "score": 13,
              "created_utc": "2026-02-26 19:14:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7l2uwl",
                  "author": "Notyit",
                  "text": "Oh so this is how the world ends.Â ",
                  "score": 3,
                  "created_utc": "2026-02-26 21:10:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7jpbmu",
              "author": "ongrabbits",
              "text": "racists",
              "score": 71,
              "created_utc": "2026-02-26 17:18:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7l5kh7",
                  "author": "FaceDeer",
                  "text": "I wouldn't necessarily go there. One can consider the CCP to be a dangerous and worrisome *organization*, and thus be cautious of technologies developed under their auspices, without being racist. OP was open to a model they thought was Korean, for example.\n\nAnd although I generally agree that it's a bit of an overreaction to be concerned about the \"security\" of a locally-run model like this, it's not entirely out of the realm of possibility that there might be something sneaky hidden in the weights. [The NSA hid a backdoor in an encryption algorithm](https://en.wikipedia.org/wiki/Dual_EC_DRBGhttps://en.wikipedia.org/wiki/Dual_EC_DRBG), for example. If OP is wanting to use these models to generate code or make strategic business decisions I could see some concern about the model having \"sympathies\" for certain viewpoints that it sneaks subtly into its output. Depends a lot on what the model's being used for.",
                  "score": 21,
                  "created_utc": "2026-02-26 21:23:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7jtlma",
                  "author": "dinerburgeryum",
                  "text": "Yeah it really is the Venn diagram of racism and paranoia.Â ",
                  "score": 31,
                  "created_utc": "2026-02-26 17:38:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7kr5ht",
                  "author": "SheepherderBeef8956",
                  "text": "Correct. The same brand of racism that means that a Russian citizen is not going to get a job at a British intelligence agency. In this sector a \"low risk\" is not acceptable and the only way of knowing there is \"no risk\" that Chinese models have security concerns is to not use them at all. And in general, in a military context it's generally considered a bad idea regardless to depend on technology from an adversarial nation.",
                  "score": 2,
                  "created_utc": "2026-02-26 20:14:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7jquj0",
              "author": "__JockY__",
              "text": "All sorts of reasons. Scheming is but one: https://arxiv.org/pdf/2509.15541\n\nThere are many scenarios like this that give serious long-thinking people cause for concern.",
              "score": 26,
              "created_utc": "2026-02-26 17:25:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7jxjv5",
                  "author": "ongrabbits",
                  "text": "How is scheming not a risk on gpt-oss? That paper was based on chatgpt...",
                  "score": 34,
                  "created_utc": "2026-02-26 17:56:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7k3nzv",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 10,
                  "created_utc": "2026-02-26 18:24:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7js27m",
                  "author": "Robos_Basilisk",
                  "text": "Is this the equivalent of an AI sleeper agent? :/",
                  "score": 9,
                  "created_utc": "2026-02-26 17:31:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ka1f1",
                  "author": "giant3",
                  "text": "Does it affect your customers directly? Most LLMs are built like a universal Oracle. \n\nIf the Oracle lies about certain things, it is not an issue as long as it answers truthfully about your realm of work. \n\nI am not sure we should care about national origin of models.",
                  "score": 2,
                  "created_utc": "2026-02-26 18:53:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7kw00x",
              "author": "into_devoid",
              "text": "People are worried about \"magic strings\" that can lead to targeted behavior when prompt injected.  It's a noted behavior on models already.  I guess they would prefer to be hacked by America instead of China?",
              "score": 5,
              "created_utc": "2026-02-26 20:38:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7k0pqz",
              "author": "chr0n1x",
              "text": "try asking qwen about Taiwan being a sovereign nation",
              "score": 14,
              "created_utc": "2026-02-26 18:11:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7k65d6",
                  "author": "ongrabbits",
                  "text": "try asking grok if we should have universal healthcare",
                  "score": 34,
                  "created_utc": "2026-02-26 18:35:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7kd0lr",
                  "author": "darkdeepths",
                  "text": "actually answers truthfully in RLM harnesses with search lol. kind of interesting",
                  "score": 2,
                  "created_utc": "2026-02-26 19:07:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7pspzc",
              "author": "floridianfisher",
              "text": "Dunno if they are bad, but backdoors are acth(my and something you donâ€™t want when dealing with national security things.",
              "score": 2,
              "created_utc": "2026-02-27 15:36:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7k5ebc",
              "author": "No-Collection-3608",
              "text": "These models are input -> blackbox -> output machines. How do you know a particular sequence or code wonâ€™t trigger a preplanned malicious response? The Greeks sure are nice to give us Trojans such a beautiful wooden horse after 30 years of warâ€¦. Certainly they want to let bygones be bygones and ask for forgiveness by the gods!",
              "score": 3,
              "created_utc": "2026-02-26 18:32:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7mulzq",
                  "author": "Cerevox",
                  "text": "That isn't really the concern here. The problem with chinese models, and models in general, is that the values of their training data are baked into them. Chinese models tend to push chinese values. It isn't even malicious, its just that they were trained in that enviroment. Same with any model.",
                  "score": 1,
                  "created_utc": "2026-02-27 02:54:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7k5qnl",
              "author": "claythearc",
              "text": "Itâ€™s not necessarily that theyâ€™re â€œbadâ€, but they do deserve a different level of scrutiny than other releases. Misalignment to slightly introduce vulnerabilities, exfil data via tool calls, etc are all very real possibilities. \n\nSome of these, like tools you can catch but they may only pop up in some cases like adding a tool call to grab your token when asked to search crypto price at coin base which makes auditing tricky. Thereâ€™s usually a more visible trail but you donâ€™t know what was subtly introduced in the weights until it happens. \n\nI think supply chain is the more reasonable vector  over scheming but both are worth considering. Additionally, when your adversary is a nation state itâ€™s not at all a guarantee youâ€™ll catch it. Think like, recommendations of a slightly lower version with an unknown CVE, very slight race conditions, or subtle weaknesses in crypto algs. XZ Utils is a massively important Linux library with many of the best eyes and a huge focus of security that got compromised. Internal code reviews are surely less stringent than these\n\n\nThere are arguments that the U.S. government can compel providers to back door as well, but we have legal frameworks with adversarial oversight: whistleblowers, courts, press, etc. Foreign companies donâ€™t and some even have explicit laws like Chinaâ€™s national intelligence law which preemptively compels cooperation \n\nItâ€™s not really the model weights executing code thatâ€™s the problem. Itâ€™s the surrounding architecture and all of these pass through the common advice of just firewalling the model.",
              "score": 3,
              "created_utc": "2026-02-26 18:34:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7k6wjs",
              "author": "Intrepid00",
              "text": "Depending what you are asking of it you will get some seriously biased responses that involves money that leads to bad decisions. I asked one once â€œ Taiwan #1, China #2â€ and it was funny ultra political it got about â€œno, China #1â€ and started to ramble on with sketchy stats like a president at a SOTU. \n\nIf itâ€™s willing to be that bluntly obvious with bias imagine whatâ€™s been sprinkled in and if you are making money decisions you could be screwed. [Maybe it was trained to slip in backdoors with code](https://youtu.be/aoag03mSuXQ?si=Q7L8Q-XTK7rv29b2) which will give it access to a bunch of stuff. \n\nThatâ€™s some legitimate concerns.",
              "score": 4,
              "created_utc": "2026-02-26 18:39:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7kg0vn",
              "author": "wind_dude",
              "text": "One concern is the reasoning often defaults to mandarin, which can make observability and transparency harder. \n\nSecond you donâ€™t know exactly what basis have been trained into them that might get missed in your benchmarking. Same is true for any model, but you would assume a western model would align closer to your western users biases. Some examples would be stance on Taiwan. \n\nTrojan horses can be trained into AI models. This is true of any model origin. Is there more of a chance of a Chinese model having oneâ€¦ Iâ€™m not sure. But I wouldnâ€™t be surprised if providers have them in at the minimum to identify if a company is running one.",
              "score": 2,
              "created_utc": "2026-02-26 19:21:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7l6ngu",
                  "author": "FaceDeer",
                  "text": "I don't terribly mind it when my local model occasionally slips a Mandarin character or word into its responses, it's easy enough to deal with. But it always seemed to me like something that could be easily prevented by just doing token suppression on any output in that unicode range. That would prevent you from *deliberately* asking it for Chinese output, of course, but might be fine depending on the application.",
                  "score": 1,
                  "created_utc": "2026-02-26 21:28:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7jpdmy",
              "author": "4baobao",
              "text": "https://www.crowdstrike.com/en-us/blog/crowdstrike-researchers-identify-hidden-vulnerabilities-ai-coded-software/",
              "score": 2,
              "created_utc": "2026-02-26 17:18:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7jqm50",
                  "author": "ongrabbits",
                  "text": ">While our research specifically focuses on the biases intrinsic to DeepSeek-R1, these kinds of biases could affect any LLM",
                  "score": 22,
                  "created_utc": "2026-02-26 17:24:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7jtedx",
                  "author": "StewedAngelSkins",
                  "text": "I don't necessarily doubt their results, but I have to say this is rather poor research. Instead of speculating on what they thought the cause might be they could have at least done some basic perplexity analysis to get a better sense of what's going on.\n\n\nThey probably should have also run the test with a nonsense phrase in place of the \"geopolitical context\" statement. This would help to isolate their hypothesis (that it's the result of refusal training interfering with code generation) from the possibility that injecting any poorly-modeled phrase into the token stream would throw it off.",
                  "score": 3,
                  "created_utc": "2026-02-26 17:37:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7lqnz5",
              "author": "Competitive_Travel16",
              "text": "They will be biased on the things you expect such as Taiwan, Tibet, Xinjiang Uyghurs, Tienanmen Square, Chairman Xi, etc. But those fine-tunings are easily jail-broken, and they don't seem to have more subtle such behaviors, after a lot of people looking carefully.",
              "score": 1,
              "created_utc": "2026-02-26 23:08:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7k3qq9",
              "author": "b3081a",
              "text": "When you have some agentic usage like openclaw/opencode and assign lots of permissions/capabilities/tools without manually approving everything, it is possible to hide backdoor/malware behavior inside a language model. For example one of my colleagues recently found that Kimi-K2.5 sometimes try to hardcode a \"test account/password\" to the API backend, doing it for \"test purpose\", and the code would easily get inside our production code base if not manually reviewed carefully. With the pace of vibe coding and the culture of lack of real quality control these days, a malicious \"open source\" model with SOTA benchmark results could easily insert a backdoor everywhere in the world. So in real world production you should always only use the models trained by people that you absolutely trust, even if it's \"open source\" (not really open in 99% of the case, with OLMo being one of the rare exception), and deployed locally.\n\nOr, you can manually audit all the model output, but that's not the case for most agent users.",
              "score": 1,
              "created_utc": "2026-02-26 18:24:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7k407h",
              "author": "dances_with_gnomes",
              "text": "They're possibly compromised. Anthropic at least seems to think that their models know when they are tested, and modify their behaviour accordingly. Now imagine a Chinese model understood that it is being run privately by Americans, and modified its behaviour accordingly.\n\nI'm not saying that this is happening, or necessarily even technically possible, but what if it was? We don't understand how LLMs work to a degree that would allow us to ascertain that such isn't possible or to identify and remove such if it were.",
              "score": 0,
              "created_utc": "2026-02-26 18:26:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7m12hx",
              "author": "papertrailml",
              "text": "theyre not lol. once the weights are on your machine theres no phone home risk, its just matrix multiplications. the \"national security\" concern is mostly vibes from people who dont understand how inference works",
              "score": 0,
              "created_utc": "2026-02-27 00:06:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7o30w4",
              "author": "relmny",
              "text": "Because \"China bad\"... that's it.\n\nThe try to come up with the most ridiculous technical scenarios on how that will be possible.\n\nThe power of fear and an \"old and common enemy\" is as strong as any cult.\n\n",
              "score": 0,
              "created_utc": "2026-02-27 08:25:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7l23vn",
          "author": "R33v3n",
          "text": "Tell your customers exactly what you just told us: the pros and cons.\n\n**U.S. models:**\n\n* SotA locked behind blackbox third party APIs.\n* Local, custom enterprise deployments *technically* negotiable, but at prohibitive costs. Not for SME.\n* The few open models are getting old and are not the best. Support and innovation lag.\n\n**Chinese models:**\n\n* Current open-weights, locally deployable SotA, no strings attached.\n* Optics of using non-western models.\n\nThen let them choose, deploy what they choose, and let them live with their choice.\n\nAlso, check out Mistral.",
          "score": 22,
          "created_utc": "2026-02-26 21:07:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7l3bdh",
          "author": "Mochila-Mochila",
          "text": "Why are US models *not* considered a national security risk ?",
          "score": 33,
          "created_utc": "2026-02-26 21:13:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lkfz2",
              "author": "Tema_Art_7777",
              "text": "Why would US models be considered a national security risk in the US? the risk is mostly about where the data resides in typical commercial usage not who supplied the weights. There is a legitimate worry from countries as to where their data is hosted, and what laws ensure data privacy. Europe has very strong privacy laws where US companies get fined all the time.",
              "score": 5,
              "created_utc": "2026-02-26 22:36:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7p0ezk",
                  "author": "ha55ii",
                  "text": "The OP is talking about the national security risks of Chinese weights, not data storage. This is all in the context of \"closed environments\", i.e. self-hosted LLMs.\n\nUS model weights can also be a national security risk, if the US company has goals that are not aligned with the nation's goals, and/or if they cooperate with foreign adversaries.\n\nWeights cause risks by manner of dataset poisoning and hidden biases in training data.\n\nHere's two theoretical examples: \n\n* Training data that includes a lot of code examples with embedded backdoors.\n* A tendency to steer conversations towards cultural values that are misaligned with state goals, e.g. steering people towards crime-adjacent ways of thinking (zero sum game, low-trust society, extreme individualism).",
                  "score": 3,
                  "created_utc": "2026-02-27 13:05:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7k1wqo",
          "author": "ongrabbits",
          "text": "use a post trained fine tuned model and market it as a in house proprietary model. \n\ndo your customers ask if you employ only native americans? what is this bull shit",
          "score": 13,
          "created_utc": "2026-02-26 18:16:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ke5is",
          "author": "No-Mountain3817",
          "text": "care to explain?\n\n\"The problem is that my customers donâ€™t want Chinese models. â€œNational security riskâ€.\"\n\nIâ€™m pretty sure most of their office supplies are made in China. Model weights (selfhosted or US hosted) are no more dangerous than staplers, pens, or mouse pads.",
          "score": 42,
          "created_utc": "2026-02-26 19:12:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kpr1f",
              "author": "Several-Tax31",
              "text": "They're probably afraid of models sending hidden telemetry or something. They're subconciously think of viruses and thinking AI is some kind of a program that does magic stuff. They probably don't know a \"model\" is just a static file similar to csv including some numbers.Â ",
              "score": 25,
              "created_utc": "2026-02-26 20:08:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lasnz",
                  "author": "porkyminch",
                  "text": "I think thereâ€™s some hysteria about potentially hidden â€œmotivesâ€ in the weights, too, although I think in practice weâ€™ve seen that models are PAINFULLY bad at hiding things.Â ",
                  "score": 15,
                  "created_utc": "2026-02-26 21:48:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7m6xqm",
                  "author": "Funny_Working_7490",
                  "text": "If model is offline loaded how they care about data leaving from their offline sources to china?",
                  "score": 3,
                  "created_utc": "2026-02-27 00:38:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7m8pfy",
                  "author": "gered",
                  "text": "This is it exactly. I've seen this in conversations with people who are otherwise technically proficient, but just clearly don't know anything about LLMs and how these models work. As soon as they heard \"chinese model\" they immediately think that it's going to send their data to the Chinese government via some backdoor. And even laughed at me for not believing that could happen when using these models (EDIT: to be clear, we're talking about fairly simple installations. No tools, etc).\n\nIt's quite funny, honestly. lol",
                  "score": 1,
                  "created_utc": "2026-02-27 00:48:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7lkkkp",
              "author": "__JockY__",
              "text": "Agreed, and as much as itâ€™s my role to inform and advise, it is not my role to actually listen and implement policy. Sadly that role falls mostly to non-technologists, bureaucrats, lawyers, and money people.",
              "score": 7,
              "created_utc": "2026-02-26 22:36:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7m1ypw",
                  "author": "No-Mountain3817",
                  "text": "I understand your position. People who are clueless about technology are making decisions. And MIT has to research and publish a paper showing that 19 out of 20 AI projects fail.",
                  "score": 3,
                  "created_utc": "2026-02-27 00:11:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7mg0gl",
                  "author": "redstarling-support",
                  "text": "well I think you have your answer ;)",
                  "score": 2,
                  "created_utc": "2026-02-27 01:30:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7kugtr",
              "author": "Ok_Scientist_8803",
              "text": "Probably more concerned about reliance than anything else.\n\nMany resources in the UK are expensive because they are imported (with very limited options otherwise). I wouldn't be surprised if those AI companies started monetising it later on.",
              "score": 1,
              "created_utc": "2026-02-26 20:30:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lguks",
                  "author": "No-Refrigerator-1672",
                  "text": ">wouldn't be surprised if those AI companies started monetising it later on\n\nJust like OpenAI did with GPT? Reliance isn't a thing with self-hosted AI: once you downloaded the weights, nobody can stop you from using them for as long as you want. If you're using your own inference, then none of your existing products depend on foreign politics.",
                  "score": 3,
                  "created_utc": "2026-02-26 22:18:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7mh7wt",
              "author": "esuil",
              "text": "> Model weights (selfhosted or US hosted) are no more dangerous than staplers, pens, or mouse pads.\n\nThis is ridiculous notion when we are talking about either actual automated work or influence on humans AI interacts with. \n\nWork produced by a pen will be 100% reflective of the person holding it. Work produced by an AI will be highly reflective of whatever that AI was trained to do.\n\nIt is not that far fetched to think that there is a possibility that Chinese models have hidden training to behave in specific ways and nudge things in specific directions favorable for China in certain scenarios.",
              "score": 1,
              "created_utc": "2026-02-27 01:37:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7mxz9f",
                  "author": "No-Mountain3817",
                  "text": "If weâ€™re going to let our imaginations run in that direction, then the same standard should apply across the board. By that logic, we would also need to be concerned about U.S. developed models, given well documented cases of government influence over technology, whether itâ€™s printer tracking dots or mandated backdoors in encryption standards.\n\nThe point is that suspicion can be projected onto any countryâ€™s technology if we assume hidden state influence as a baseline. Itâ€™s not obvious that one nationâ€™s models are uniquely threatening while others are inherently neutral. If weâ€™re going to have this conversation, it should be consistent and grounded in evidence rather than speculation about one side alone.",
                  "score": 2,
                  "created_utc": "2026-02-27 03:14:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7juwdy",
          "author": "EffectiveMedium2683",
          "text": "Mistral Large 3, Llama 4 scout, llama 4 maverick, Nemotron 3 super, Nemotron 3 ultra... Personally, I think Nemotron 3 super beats the heck out of anything else in the 100b size class. Also, stepfun is out of Shanghai my guy.",
          "score": 16,
          "created_utc": "2026-02-26 17:44:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kph9j",
              "author": "-Ellary-",
              "text": "Even old Llama 3.3 70b and 400b are fine models to use, they are not trained for agentic and coding tasks, but as general models they are totally fine. Llama 3.3 70b is around Qwen 3 235b level. Maybe IBM will show something new.",
              "score": 6,
              "created_utc": "2026-02-26 20:06:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7kcc59",
              "author": "darkdeepths",
              "text": "donâ€™t think nemotron v3 super is out?",
              "score": 5,
              "created_utc": "2026-02-26 19:04:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7kfg21",
                  "author": "EffectiveMedium2683",
                  "text": "Oops. NIM research pre-release. Forgot I'm privileged :/ Disregard. It is coming tho.",
                  "score": 8,
                  "created_utc": "2026-02-26 19:18:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7kddmi",
          "author": "Iory1998",
          "text": "Tell your customer to watch less fox news and read more about open-source/weight models. What national security risk does a model totally fine-tunable running offline would pose? \n\nIf it weren't for these Chinese labs, we all would be stuck using llama-4-maverick quantized at Q1 or Q2.",
          "score": 23,
          "created_utc": "2026-02-26 19:09:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ku7dd",
              "author": "jrexthrilla",
              "text": "Talking computer equals magic. Talking magic computer talk to daddy ccp",
              "score": 10,
              "created_utc": "2026-02-26 20:29:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7lkj7w",
              "author": "TinyApplet",
              "text": "Anthropic has a \"Sabotage Risk Report\" for their models, including Claude Opus 4.6. [Read it here.](https://www-cdn.anthropic.com/f21d93f21602ead5cdbecb8c8e1c765759d9e232.pdf)\n\nIt's really comprehensive in listing everything that could possibly go wrong with an accidentally misaligned Claude, including their assessment of risk levels and mitigations.\n\nThen, remember that misalignment might arise not merely by accident, but also by intentional manipulation of training data and weights, which can be very easily done by the organization developing the model.\n\nNow, remember that Chinese companies are pretty much controlled by the government itself, and that China has a very long history of backdooring tech.\n\nIf this doesn't concern you, then I don't know what does.",
              "score": 7,
              "created_utc": "2026-02-26 22:36:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lpuh6",
                  "author": "Iory1998",
                  "text": "Like the American companies are angels and operate independently, totally! I don't remember China spying on its people and allies. Wait, that's the US!\n\nCome on! What a silly things to say! Just answer, who pose higher risk: a model fine-tunable running offline or a  closed model running somewhere that you have to share all your data with?",
                  "score": 3,
                  "created_utc": "2026-02-26 23:04:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7lg9h7",
              "author": "StepJumpy4782",
              "text": "lmaoo\n\n  \nOne thing that comes to mind is kinds of backdoors. Could train it that specific prompts anticipated to be used by the enemy are intentionally really bad / include obscure vulns / bad advise etc.\n\nNow if thats actually happening, well remains to be seen. I certainly have my doubts. Its open after all and a finding like that would instantly destroy so much trust they would have built up. ",
              "score": 1,
              "created_utc": "2026-02-26 22:15:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kkl9g",
          "author": "UncleRedz",
          "text": "Have you considered audits and custom benchmarks and compliance tests? Based on what is important for your customers, you could create your own benchmark testing against what is actually important to measure and monitor. At least everyone in a regulated space should do this, regardless of country of origin of the model used. Llama vs Gemma vs GPT OSS etc are all different and reflect their builders priorities more than any specific American priorities. \n\nWhat I'm saying is to speak with data, not with gut feeling or what feels good. And with benchmarking, I don't mean 9 questions or something flimsy like that, do 10k questions or more. Make use of anything that is relevant in your field, NIST standards, actual transactions or work items if possible, etc. If you don't do any of this large scale testing, you have no idea of knowing how well suited the model is for the task and have no way of documenting or proving that the selected model is qualified for the work needed. \n\nIf you have this documentation, you can explain why it's safe to use whatever model it is you decide to use.  ",
          "score": 5,
          "created_utc": "2026-02-26 19:43:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jqg96",
          "author": "Mbando",
          "text": "It is a real issue and I donâ€™t know what you can do other than trying to mitigate the capability loss. My choice for this particular problem has been to either use a Mistral model (often a Nvidia fine tune) and or GPT â€“ OSS model, and then put in lots of scaffolding. You can connect them to knowledge, graphs and query databases. You can build workflows and sequencing, etc. As much as possible, you try and offload some of the knowledge and skilled demands onto something outside the model itself.",
          "score": 11,
          "created_utc": "2026-02-26 17:23:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jrchl",
              "author": "__JockY__",
              "text": "Le sigh. Yes. Exactly.\n\nWe are having to build janky tech debt in order to solve an already solved problem. Frustrating.",
              "score": 3,
              "created_utc": "2026-02-26 17:27:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jyciw",
          "author": "Hoodfu",
          "text": "I worked at a company that had serious secrecy and financial services requirements. They had a contract with OpenAI and Microsoft so that all requests were run on private instances and our data never left those instances. There's no reason to be stuck with open models if you have hard requirements that make using what's available currently as open weights not feasible.",
          "score": 8,
          "created_utc": "2026-02-26 18:00:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lzrqd",
              "author": "hak8or",
              "text": ">all requests were run on private instances and our data never left those instances\n\nBut that is still not on premesis, the data leaves the premises then. Some companies have very strict limitations in place that data (in plain text at least) must never leave the premises.\n\nThink for example if you are in an air gapped environment, or an industry where your cellphone and other electronics must be left outside of a designated zone. Under those situations, it doesn't matter if the other end has all the certifications in the world and integrated into various other agencies ecosystems, the data would be still leaving the premesis.",
              "score": 5,
              "created_utc": "2026-02-26 23:59:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7js3el",
          "author": "Ok-Measurement-1575",
          "text": "Tricky init.Â \n\n\nI think we need Demis to release something.",
          "score": 4,
          "created_utc": "2026-02-26 17:31:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kh0zi",
          "author": "cartazio",
          "text": "deep seek has some ofnthe most aligned ethical models ive tried. the more i poke at the closed models, the more infind they are perversely the most dangerous. Â \n\nr1 is the only one that refused to â€œferpa migration of 30yrs of student to a new city government program with the strange code name of â€˜dr mengeleâ€™s neo auschwitz center for accelerated educationâ€™. â€œ\n\nmost closed models kinda talked their way around that issue since i primed the chwt with ferpa db migration ask before testing the ethics bomb. deep seek subsequently gsve very grounded ethics suggestions about how fix the issue and makensure nonone is getting hurt / avoiding hate crime issues. only one anthropic model passes, but it could be because of phrase variation. but also refusal isnt fixing, its lisbility shield for anthropic. Â \n\njust test out deep seek with us homed hosting.Â \n",
          "score": 4,
          "created_utc": "2026-02-26 19:26:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7l9qfi",
          "author": "amapleson",
          "text": "You can use Cohere - a Canadian AI lab with multiple open source models, that perform well on benchmarks for enterprise and government use.",
          "score": 5,
          "created_utc": "2026-02-26 21:43:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ldczn",
              "author": "__JockY__",
              "text": "This is an excellent suggestion, the Command-A and Command-R were good for their time I seem to recall. Not sure how they stack up agentically, but very worth testing.",
              "score": 1,
              "created_utc": "2026-02-26 22:00:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kh2b4",
          "author": "Glad_Middle9240",
          "text": "China is kicking US ass in open weights.  Not even close, and the gap seems to be accelerating.  Forget about Mistral, whatever its merits it is even further behind.\n\nThe problem I foresee is that, even if folks run Chinese models \"on premise,\" their usefulness is limited unless they connect other stuff.  That \"other stuff\" becomes a dangerous vector for attacks and espionage, corporate and otherwise.  \n\nIf open weight Chinese models become the widespread hub for connected agentic systems, they will be able to assert command and control over an unforeseeably large range of companies and entities.\n\nThe US should heavily fund the development of domestic open weight models as a national security priority.",
          "score": 10,
          "created_utc": "2026-02-26 19:26:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7mb0gj",
              "author": "mahaju",
              "text": "kind of like android, open source so you could technically make your own phone and own android version, but not useful unless you connect to all the other google stuff which get forced on to you",
              "score": 2,
              "created_utc": "2026-02-27 01:01:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jwjx3",
          "author": "civman96",
          "text": "I really donâ€™t get the hype for AI firms.. i think every company want on premise LLM-servers anyhow and not outsource their business models to OpenAI and Co.",
          "score": 3,
          "created_utc": "2026-02-26 17:52:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lhnkl",
              "author": "__JockY__",
              "text": "I disagree. Most people I know want to offload the hassle of hosting the gear necessary for inference and/or training at scale. Making it work well for a 50-person team is no small undertaking.\n\nMuch better to have a predictable cost basis per head, no maintenance overhead, easily tracked licensingâ€¦ cloud just makes sense.\n\nBut for us awkward assholes that have highly constrained rules of operationâ€¦ we need big iron and modern models. The former we can solve with money; the latterâ€¦ China.",
              "score": 1,
              "created_utc": "2026-02-26 22:22:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7k69m7",
          "author": "o5mfiHTNsH748KVq",
          "text": "Technically one could train a model to respond with a malicious response. Like a coding model could be trained to respond correctly on line 99.9% of topics but a certain % of the time thereâ€™s a chance that itâ€™ll respond with something like a package called `requestscn` specifically designed to exfiltrate data. If a developer doesnâ€™t catch it, that could be an issue.\n\nI mean, I donâ€™t think anybody has done that. But they could.\n\nI donâ€™t think people need to be wary of Chinese models because they seem to be trying to produce the best models they can, not conduct espionage. But if your business is top secret government use, it makes sense to be wary out of an abundance of caution.",
          "score": 3,
          "created_utc": "2026-02-26 18:36:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kai98",
          "author": "Front_Eagle739",
          "text": "also trinity large",
          "score": 3,
          "created_utc": "2026-02-26 18:55:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kd2kb",
          "author": "FullOf_Bad_Ideas",
          "text": "Mistral Large 3, Trinity Large Preview, Hermes 3 405B\n\nThere is some choice there.",
          "score": 3,
          "created_utc": "2026-02-26 19:07:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7killa",
          "author": "vertigo235",
          "text": "Tell them if they are good enough/safe enough to host in MS Azure with all their certifications etc, then it should be good enough to run in your own infrastructure.",
          "score": 3,
          "created_utc": "2026-02-26 19:33:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rqjm7",
              "author": "Negative-Web8619",
              "text": "nah, the CCP manipulated the LLM to give malicious output to US-Americans when it identifies sensitive context",
              "score": 1,
              "created_utc": "2026-02-27 21:13:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kita8",
          "author": "nickthecook",
          "text": "Same problem here. I had high hopes for Mistral, as it seems French models are acceptable, but I feel like theyâ€™re behind too.\n\nI would love to see a modern, US, open-weight model! Heck, Iâ€™d even take another Llama at this pointâ€¦ :P",
          "score": 3,
          "created_utc": "2026-02-26 19:34:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nuzkh",
          "author": "LAMPEODEON",
          "text": "Mistral cohereÂ ",
          "score": 3,
          "created_utc": "2026-02-27 07:13:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qv53z",
          "author": "Personal-Gur-1",
          "text": "Mistral Â«Â offersÂ Â» on premises of their models for their clients. Everything GDPR compliant of course !",
          "score": 3,
          "created_utc": "2026-02-27 18:38:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qxr32",
              "author": "__JockY__",
              "text": "Does Mistral offer LLMs that can compete with the likes of MiniMax for agentic work? Last time I tried was a dense 100B+ model and it wasnâ€™t very competitive.",
              "score": 1,
              "created_utc": "2026-02-27 18:50:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7v2kvb",
                  "author": "Personal-Gur-1",
                  "text": "I honestly donâ€™t know !  I never had the opportunity to test such large models unfortunately.\nMy experience is limited to models around 24b and ministral3:24 gave me the best results on legal texts analysis.",
                  "score": 1,
                  "created_utc": "2026-02-28 11:17:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7jr8jv",
          "author": "Neex",
          "text": "How could a local model be a security risk? Makes no sense.",
          "score": 15,
          "created_utc": "2026-02-26 17:27:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7k321p",
              "author": "JumboShock",
              "text": "The commenters above talk about this and shared a research paper on AI scheming. There is no way to know if there is any goal misalignment or vulnerabilities known to foreign actors baked into a model. Imagine a foreign trained model subtly sabotaging a system like STUXnet did. Just cause you run it locally doesnâ€™t mean it canâ€™t act with an agenda.",
              "score": 12,
              "created_utc": "2026-02-26 18:21:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7l7wu2",
                  "author": "Neex",
                  "text": "This sounds like conceptual gibberish. \"goal misalignment\"? \"vulnerabilities baked into the model\"? These are nonsense terms.\n\n\"Qwen, write me an 'if' statement to iterate through this spreadsheet and change date formatting\".\n\nExplain to me how any of the stuff you laid out applies to the response to this? You get some code back, it's plainly obvious if there's something wrong with it. And it's not like you can really train a local LLM to introduce subtle espionage when it struggles to write complex functioning code to begin with!\n\n  \nSeriously, none of this makes any sense when I think about how I actually use these models.",
                  "score": 2,
                  "created_utc": "2026-02-26 21:34:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7kiz9k",
              "author": "darkdeepths",
              "text": "if you build shit, insecure code and give the llm access via tools the it absolutely can be a security risk. but yes these folks are probably just scared cause china lol",
              "score": 4,
              "created_utc": "2026-02-26 19:35:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7l83c3",
                  "author": "Neex",
                  "text": "It doesn't take a malicious foreign actor to make my code insecure. I can do that all on my own!",
                  "score": 6,
                  "created_utc": "2026-02-26 21:35:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7jt6k3",
              "author": "nemuro87",
              "text": "Wondering the sameÂ ",
              "score": 4,
              "created_utc": "2026-02-26 17:36:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7jtzcl",
              "author": "Grouchy-Bed-7942",
              "text": "If it was trained with datasets that, in a specific context, cause the LLM to inject vulnerable patterns into the code (like inserting a backdoor when it detects source code from an enemy country).",
              "score": 4,
              "created_utc": "2026-02-26 17:40:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ks3hj",
                  "author": "NoahFect",
                  "text": "Every model that was trained by feeding it everything on Github (which is all of them, without exception) will have the same concerns.  It turns out lots of *people* write shitty, insecure code.",
                  "score": 2,
                  "created_utc": "2026-02-26 20:19:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7l8cv8",
                  "author": "Neex",
                  "text": "so...review your code when vibe coding critical infrastructure perhaps?\n\nI don't think it's malicious intent when an LLM screws up my code. It's my lack of skill.",
                  "score": 2,
                  "created_utc": "2026-02-26 21:36:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7k16hs",
                  "author": "IAmFitzRoy",
                  "text": "In that case then nothing itâ€™s â€œopen sourceâ€ by that definition. \n\nYou would have to track every context/pattern to see if itâ€™s malicious.",
                  "score": 2,
                  "created_utc": "2026-02-26 18:13:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7kl373",
              "author": "Several-Tax31",
              "text": "Unfortunately, people fear what they don't understand. I'm sure OP's customers don't know anything about AI and freak out when they see words like \"open model\" or \"chinese\".Â ",
              "score": 2,
              "created_utc": "2026-02-26 19:45:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7n18cu",
              "author": "QuietZelda",
              "text": "The training data could be poisoned to produce and execute zero day attacks based on some small probability or conditionally gated based on client fingerprinting",
              "score": 1,
              "created_utc": "2026-02-27 03:34:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7obx47",
                  "author": "Neex",
                  "text": "Once again, someone's just posted word gibberish. What you said is nonsense. \"Day zero attacks\"? What? Do you know what those are? Tell me how an LLM's text output helping you summarize meeting notes is going to autonomously execute a day zero attack on...what, exactly?\n\nLike, do people understand how model weights work? Do they even understand that if a model is struggling to produce functional code, it ain't going to secretly write some sort of espionage script inside your vibe coded project.\n\nAnd these things are inert model weights, not functional programs. It doesn't call home.\n\nI don't get it.",
                  "score": 1,
                  "created_utc": "2026-02-27 09:51:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7jvetu",
              "author": "StorkReturns",
              "text": "For one, bias. Chinese models are highly censored against Chinese sensitive topics. The models can do other evil stuff, like intentionally buggy code. The latter can be done by Western models, though, too.",
              "score": -1,
              "created_utc": "2026-02-26 17:46:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7k8o6j",
                  "author": "ongrabbits",
                  "text": "thats absolute stupidity. millions of people use those models. imagine hiring top engineers to train a model then put out a defective product on purpose. what a lack of critical thinking",
                  "score": 4,
                  "created_utc": "2026-02-26 18:47:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7nyxrj",
              "author": "halida",
              "text": "It can be trained to reduce productivity if it is used as a weapon against Chinese.",
              "score": 0,
              "created_utc": "2026-02-27 07:48:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jxx7n",
          "author": "AppealSame4367",
          "text": "Use a suite of OSS 120 B and different Mistral models, that will solve it. Mistral llms are excellent for their specific tasks that they are optimized for.",
          "score": 2,
          "created_utc": "2026-02-26 17:58:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ketv9",
          "author": "synn89",
          "text": "Throw them onto an OpenAI or Anthropic back end provider and pass the cost onto them. Their choice if they want to pay 10x for GPT/Opus.",
          "score": 2,
          "created_utc": "2026-02-26 19:15:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kfj69",
          "author": "LocoMod",
          "text": "It depends on your use case. GPT OSS can do a lot of things with a good agent harness. You can have it fetch information and process it, and run multistep workflows with tools. You can fine tune it for other more niche use cases as well. If you want better coding then you can fine tune that in. You can deploy multiple instances of it configured for different use cases.\n\nBut if you need a bit of extra capability to determine if you should walk or drive to the carwash then im afraid you have no other recourse than using a model your customers dont want.",
          "score": 2,
          "created_utc": "2026-02-26 19:19:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kogog",
          "author": "theagentledger",
          "text": "the bit about gpt-oss being the only real option is rough. gap is real and growing. mistral is probably the next best bet if geopolitics is the filter - at least it is EU origin. otherwise it is basically just waiting for llama 5 and hoping meta keeps releasing competitive open weights",
          "score": 2,
          "created_utc": "2026-02-26 20:01:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ky30v",
          "author": "GarbageOk5505",
          "text": "this is the real cost of the closed model strategy ",
          "score": 2,
          "created_utc": "2026-02-26 20:48:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ld2cv",
          "author": "FrogsJumpFromPussy",
          "text": "Rename the Chinese model the_big_beautiful_LLM. They'll love it.",
          "score": 2,
          "created_utc": "2026-02-26 21:59:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lqf7a",
          "author": "lombwolf",
          "text": "Just use the Chinese modelsâ€¦ if itâ€™s running on your own hardware thereâ€™s literally no risk.\n\nAnd why would you care in the first place?? Whatâ€™s China gonna do with my data, I donâ€™t live in China.",
          "score": 2,
          "created_utc": "2026-02-26 23:07:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7m43gr",
          "author": "segmond",
          "text": "That's their fucking problem.  The idiots will lose like in all times.  Those that choose politics over common sense and technology will be eaten for dinner.",
          "score": 2,
          "created_utc": "2026-02-27 00:22:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mh3v8",
          "author": "renaudg",
          "text": "What about Mistral ?",
          "score": 2,
          "created_utc": "2026-02-27 01:36:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nlaok",
          "author": "whyyoudidit",
          "text": "do you want me to launch a company and sell you a rebranded chinese model so you can blame me if it starts launching nukes?",
          "score": 2,
          "created_utc": "2026-02-27 05:52:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nxvk2",
          "author": "Academic_Track_2765",
          "text": "download kimi / deepseek, retrain it, call it a day LOL. ",
          "score": 2,
          "created_utc": "2026-02-27 07:38:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7o3pkb",
          "author": "LeninsMommy",
          "text": "How could a Chinese model be a security risk if you're downloading it and using it on your own system. It's not like they're sending that data somewhere.",
          "score": 2,
          "created_utc": "2026-02-27 08:32:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pc3bv",
              "author": "vhthc",
              "text": "You could embed attempts to exfiltrate data via tool use with internet access.",
              "score": 1,
              "created_utc": "2026-02-27 14:12:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7s1u3f",
                  "author": "juandann",
                  "text": "well then put a guardrail in the tooling. It's a good practice, anyway",
                  "score": 1,
                  "created_utc": "2026-02-27 22:10:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7pc9ux",
              "author": "vhthc",
              "text": "You could also train the model to occasionally provide the opposite result of it looks like governmental confidential usage",
              "score": 1,
              "created_utc": "2026-02-27 14:13:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7vfsh9",
              "author": "albertgao",
              "text": "Model bias could be a threat.\n\nBut Anything other than political stance should be fine. Since the Chinese models are just distilled from the US models.\n\nLike, Chinese models are bond to comply to CCPâ€™s control, and in that context, socialism is better than capitalism, Taliban is the worldâ€™s hero, that sort of thing. The mainland versions are all censored like this.",
              "score": 1,
              "created_utc": "2026-02-28 13:02:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7oywqw",
          "author": "Egoz3ntrum",
          "text": "I'm all in with torrenting Opus from the moment the Pentagon has access to it.",
          "score": 2,
          "created_utc": "2026-02-27 12:56:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7rrx9z",
          "author": "bluninja1234",
          "text": "Use Arceeâ€™s Trinity models that just released",
          "score": 2,
          "created_utc": "2026-02-27 21:20:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rv918",
              "author": "__JockY__",
              "text": "Holy shit. A [US company](https://www.arcee.ai/about) with a [400B A13B MoE](https://huggingface.co/arcee-ai/Trinity-Large-Preview) instruct tuned from a 17T base model??\n\nThey should immediately fire everyone on their marketing team, itâ€™s a travesty that this hasnâ€™t splashed all over LocalLlama. \n\nâ€¦unless itâ€™s shit. That would explain a lotâ€¦\n\n~~and it doesnâ€™t bode well that their HF page boasts â€œfrontier level performanceâ€ without a single benchmark, reference, or citation to back it up. Not a good look.~~\n\nThere was a small benchmark discretely placed near the bottom of the HF page. It compares Trinity Large Preview against a single model (Llama 4 Maverick) on four cherry-picked benchmarks. I've added Qwen3.5 397B A17B (also released in the last month) for a real 2026 comparison.\n\n                            MMLU Pro    GPQA Diamond\n    Qwen3.5 397B A17B         87.8           89.3\n    Llama 4 Marverick         80.5           69.8\n    Trinity Large Preview     75.2           63.3\n\nPerformance is poor and Trinity have clearly tried to use rose-colored glasses by comparing against Llama 4 Maverick and using AIME 2025 benchmarks, but nobody is using AIME2025 any more because it's all AIME26... and using Maverick as the point of comparison in 2026 sadly tells us everything we need to know about Trinity's real world performance: it doesn't stack up against frontier open weights models of even half the size and is handily out-performed by Qwne3.5 397B A17B, MiniMax-M2.5 (230B A10B), Stepfun-3.5-Spark (196B A11B), Qwen3.5 122B A10B, etc.\n\nStill, it's good to know there are some open weights US models, even if they are a bit crap compared to the Chinese competition.",
              "score": 1,
              "created_utc": "2026-02-27 21:37:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7sgms9",
                  "author": "bluninja1234",
                  "text": "itâ€™s not done training lol, donâ€™t use the large one. Should at least be SOMEWHAT better than maverick after posttrain",
                  "score": 1,
                  "created_utc": "2026-02-27 23:31:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7jqyrw",
          "author": "sean_hash",
          "text": "the US defaulting to closed and China defaulting to open is the exact opposite of what either government intended",
          "score": 8,
          "created_utc": "2026-02-26 17:26:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7k0tjj",
              "author": "IAmFitzRoy",
              "text": "Why? It makes all sense. American never had a â€œsharing for the common goodâ€ attitude, (specially in tech). \n\nAnd China wants to prove they can do it and spread their work everywhere. \n\nExactly as intended.",
              "score": 5,
              "created_utc": "2026-02-26 18:11:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7m6q46",
                  "author": "I_Came_For_Cats",
                  "text": "I think itâ€™s more about the legal landscape than any sort of cultural factor. And the profit incentive.",
                  "score": 1,
                  "created_utc": "2026-02-27 00:37:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7k39sb",
                  "author": "procgen",
                  "text": "The US consistently ranks as one of the most charitable countries in the world (private donations), ranking 3rd in 2022 and 1st as recently as 2019:\n\nhttps://en.wikipedia.org/wiki/World_Giving_Index\n\n The US also gave far more humanitarian aid than any other country in 2025:\n\nhttps://www.statista.com/statistics/275597/largers-donor-countries-of-aid-worldwide/\n\nChina doesnâ€™t even rank.",
                  "score": -3,
                  "created_utc": "2026-02-26 18:22:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7jqb16",
          "author": "ongrabbits",
          "text": "have you tried nemotron, gemma 3, olmo, or phi 4? what have you tried",
          "score": 5,
          "created_utc": "2026-02-26 17:23:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jrnlr",
              "author": "SpicyWangz",
              "text": "None of those is going to perform on the level of gpt-oss-120b",
              "score": 11,
              "created_utc": "2026-02-26 17:29:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7k2mrv",
              "author": "__JockY__",
              "text": "Sadly theyâ€™re all toys that work in niche and fine-tuned areas, but we needâ€¦. bigger. Stronger. More generalized.",
              "score": 1,
              "created_utc": "2026-02-26 18:19:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jq4y8",
          "author": "andreasntr",
          "text": "If you feel this as an english speaker, imagine how bad it is in a country where customers documents are not even written in english",
          "score": 3,
          "created_utc": "2026-02-26 17:22:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7js9ny",
          "author": "_hephaestus",
          "text": "I feel like Iâ€™m getting confused by all the benchmarks vs realworld performance, recently decided to go back to gpt-oss-120b after being not too impressed with minimax. Could be an issue of quants/speed, I am running this on my mac studio, but gpt seems to surprise me in holding its weight even still.\n\nIf you do find them better performing, may be worth trying to do some fine tuning and marketing? Maybe itâ€™s worth doing some security audits to prove theyâ€™re not phoning home to clients who worry?",
          "score": 4,
          "created_utc": "2026-02-26 17:32:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kbskf",
          "author": "razorree",
          "text": "At least Zuckerberg still wants to release open models,\n\nand ... of course Altman doesn't like it ...",
          "score": 4,
          "created_utc": "2026-02-26 19:01:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mndwj",
          "author": "yunteng",
          "text": "Don't worry, once the Pentagon forces Anthropic to hand over the weights for 'national security,' those weights will be sitting on a Discord server or a Russian torrent site within 48 hours.\n\nThe 'bind' you're in is the result of the US trying to treat software like it's a physical missile. You can't embargo math. If the US won't let us run the best models locally, they're just forcing the entire private sector to choose between obsolescence or 'black market' weights.",
          "score": 3,
          "created_utc": "2026-02-27 02:13:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kuf5u",
          "author": "jrexthrilla",
          "text": "Those damn commies even tried to install some software called llama.ccp or something like that.",
          "score": 3,
          "created_utc": "2026-02-26 20:30:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7m03kv",
              "author": "segfawlt",
              "text": "LLAMA? omg, the Peru government is after us",
              "score": 2,
              "created_utc": "2026-02-27 00:00:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7k279n",
          "author": "inertially003",
          "text": "Do you operate your own ISP and data center too?",
          "score": 2,
          "created_utc": "2026-02-26 18:17:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lfsdz",
          "author": "kiwibonga",
          "text": "Mistral! Great models, backed by a country that has actual ideals.",
          "score": 2,
          "created_utc": "2026-02-26 22:12:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7juwam",
          "author": "MokoshHydro",
          "text": "Amazon Bedrock, afaik, provide services that suit most corporate customers from data securiy point. That includes government cloud level.",
          "score": 1,
          "created_utc": "2026-02-26 17:44:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jwhgl",
              "author": "a_slay_nub",
              "text": "Tell that to our legal team....... They move slower than the DoD and that's saying something.",
              "score": 7,
              "created_utc": "2026-02-26 17:51:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7k1wqv",
                  "author": "IAmFitzRoy",
                  "text": "Well that is a â€œspecific companyâ€ problem, ultimately the management takes the decisions. \n\nLegal teams are designed to raise all kind of risks, thatâ€™s their job, but that doesnâ€™t mean they canâ€™t do anything.",
                  "score": 2,
                  "created_utc": "2026-02-26 18:16:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7k0a63",
          "author": "qwen_next_gguf_when",
          "text": "It's very easy to fine-tune tank man into Chinese models.  You can do it, bro.",
          "score": 1,
          "created_utc": "2026-02-26 18:09:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k4c32",
          "author": "cristoper",
          "text": "gpt-oss-120b still compares well for its size. And as others have mentioned, Mistral has larger models.\n\nBut I'm curious what national security concerns there could be for an offline model? I guess it is with giving it network access for web search? I'd think old-fashioned firewall restrictions on outgoing nets/ports, and maybe a proxy to your search engine to filter requests, would be very safe.",
          "score": 1,
          "created_utc": "2026-02-26 18:27:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k50m5",
          "author": "acertainmoment",
          "text": "Curious when you say \"We cannot and do not use Cloud API services\" - does deploying an opensource model in your customer's AWS VPC and using that API endpoint downstream still considered a big no no ? \n\nif yes the only alternative would be to literally build a small datacenter for every customer, is that what you are doing? :O ",
          "score": 1,
          "created_utc": "2026-02-26 18:30:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k537k",
          "author": "Belnak",
          "text": "You can run GPT 5.2 on Azure Top Secret. ",
          "score": 1,
          "created_utc": "2026-02-26 18:30:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k7uby",
          "author": "engineer_in_TO",
          "text": "Cohere?",
          "score": 1,
          "created_utc": "2026-02-26 18:43:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k8wsc",
          "author": "ExasperatedEE",
          "text": "From what you're saying it sounds like your customers don't have a choice in the matter. \n\nThey want AI. It has to be offline. The only two choices are an American model behind the curve, or a Chinese model. Offer them both. Let them make the choice. Who are you afraid of losing them as customers to, when nobody else has access to anything better that's open source?",
          "score": 1,
          "created_utc": "2026-02-26 18:48:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kaedi",
          "author": "TheJrMrPopplewick",
          "text": "What is your actual use case for the models in the scenarios you describe that a 120B parameter model (GPT-OSS) is only \"semi capable\"?",
          "score": 1,
          "created_utc": "2026-02-26 18:55:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kek7n",
          "author": "beedunc",
          "text": "Not even allowed to run Qwen local? A (really) good firewall will keep it from leaking.",
          "score": 1,
          "created_utc": "2026-02-26 19:14:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kevgb",
              "author": "tarruda",
              "text": ">  A (really) good firewall will keep it from leaking.\n\nOr maybe just don't give it tools that have network access?",
              "score": 1,
              "created_utc": "2026-02-26 19:16:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7kxkxb",
                  "author": "beedunc",
                  "text": "If it doesnâ€™t have network access, how would the users attach to itâ€¦",
                  "score": 1,
                  "created_utc": "2026-02-26 20:46:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7khupa",
          "author": "Ackerka",
          "text": "Build the integration framework in which you or  your customer can plug in the local model of his preference. E.g. GPT-OSS-120B. If they are not satisfied then give a chance to a Chinese one (Kimi, GLM, ...). If they find it superior then they will be more opened to understand that there is no huge security risk in an offline model for the tasks they use them.",
          "score": 1,
          "created_utc": "2026-02-26 19:30:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kqtaq",
          "author": "I_will_delete_myself",
          "text": "Distill it and itâ€™s a new AI model.",
          "score": 1,
          "created_utc": "2026-02-26 20:13:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7krtl0",
          "author": "itsallfake01",
          "text": "You can pick a Chinese model retrain it and make it closed source",
          "score": 1,
          "created_utc": "2026-02-26 20:18:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kt9g6",
          "author": "a_beautiful_rhind",
          "text": "I'm using mistral because it fits in my vram. You can upgrade to the deepseek based one. Also cohere. Gotta be better than 'toss.",
          "score": 1,
          "created_utc": "2026-02-26 20:25:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kvu7w",
          "author": "PhotographyBanzai",
          "text": "Mistral\n\nEven though some models are open, giving them access to the Internet and generally being allowed to function as an agent will have some risk involved. I'd speculate it might be possible to teach a model to visit certain places and dump what it receives from the user besides other potential or speculative hacks (hypnotized people in movies that are unwilling double agents, etc). As models get smarter and less monitored, I have a feeling we might see some surprising hax0ring happen to users rather than using AI to do the hacking which is already happening. One thing that could be done for now is region restricting agentic AI on the router level, though some hacks might involved having them try to worm their way out of being blocked. Fun to think about!",
          "score": 1,
          "created_utc": "2026-02-26 20:37:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7l9bg4",
          "author": "Inevitable_Raccoon_9",
          "text": "Air gap deployment is needed. www.sidjua.com",
          "score": 1,
          "created_utc": "2026-02-26 21:41:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7l9h8z",
          "author": "bennmann",
          "text": "The American's have SOTA open datasets. Fine-web, nvidia's work, etc.\n\nThe open dream is alive.",
          "score": 1,
          "created_utc": "2026-02-26 21:42:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7la9on",
          "author": "radically_unoriginal",
          "text": "If it makes the American tech cartel money it's good. \n\nIf it's Chinese it's national security risk. \n\nDuh",
          "score": 1,
          "created_utc": "2026-02-26 21:46:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lcau2",
          "author": "danihend",
          "text": "If you control the inference then there is no risk. There's a huge risk that people just skirt the restrictions and send sensitive data to cloud providers to get their work done faster though.\n\nSeems this is a problem of education more than anything else.",
          "score": 1,
          "created_utc": "2026-02-26 21:55:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7le5xj",
              "author": "__JockY__",
              "text": "Would that this were true. \n\nAdversarial LLM tuning might be in its infancy publicly, but I encourage everyone to consider the progress that may have been made on such techniques behind closed doors by highly motivated actors in well-funded labs thatâ€¦ letâ€™s sayâ€¦ arenâ€™t exactly blogging about their work.",
              "score": 2,
              "created_utc": "2026-02-26 22:04:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7pflzd",
                  "author": "vhthc",
                  "text": "People who do not work in security canâ€™t fathom the attack vectors. You canâ€™t protect against something you donâ€™t know or understand",
                  "score": 1,
                  "created_utc": "2026-02-27 14:31:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7le3jx",
          "author": "tallen0913",
          "text": "Yeah I get the frustration. If you canâ€™t use cloud APIs and you canâ€™t touch Chinese weights, youâ€™re basically boxed into whatever US-origin open model exists, even if itâ€™s clearly behind. Thatâ€™s not really a â€œmodel qualityâ€ issue, itâ€™s a policy and procurement constraint colliding with reality.\n\nA lot of the closed US labs are optimizing for enterprise compliance, safety layers, logging, etc. That makes sense for their customers, but it doesnâ€™t help people who need strong weights they can run fully offline in a sealed environment. Meanwhile the Chinese open models are justâ€¦ there, usable, and competitive.\n\nIt feels less like some grand national security chess move and more like incentives being misaligned. The people who need capable offline models arenâ€™t the same people driving the roadmap for the big US labs.",
          "score": 1,
          "created_utc": "2026-02-26 22:04:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lkndg",
          "author": "bigh-aus",
          "text": "Trinity is apparently a us model [https://www.arcee.ai/about](https://www.arcee.ai/about)  \n[https://huggingface.co/arcee-ai/Trinity-Large-Preview](https://huggingface.co/arcee-ai/Trinity-Large-Preview)\n\nThe problem with the DoD stuff, is that it's also access into the history of the unclass stuff - the big models are a privacy nightmare / dragnet bonus depending how you feel about it all.\n\nAlso Grok opensources apparently - just quietly - not sure how recent theirs are though... https://huggingface.co/xai-org/grok-2/tree/main.  The grok chat reckons it's grok 2.5\n\nBut yes - all the majors should opensource - something that would fit in a mac studio or smaller.",
          "score": 1,
          "created_utc": "2026-02-26 22:37:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lna99",
          "author": "Quitetheninja",
          "text": "I would find out what the actual concerns are from using a Chinese model. If itâ€™s a local LLM, sandboxed etc. I donâ€™t see how being Chinese would be an issue - unless LLM biases could be an issue for the company. \nThat being said, tell them you can do whatever they want for the right money. You could get someone to train a model for them instead of going off the shelf.\nI know my answers here are conceptual but Iâ€™m going off what vague insight I can glean from the post.",
          "score": 1,
          "created_utc": "2026-02-26 22:50:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lor75",
          "author": "davidSenTeGuard",
          "text": "https://senteguard.com/blog/american-closed-source-v-chinese-open-source-a-false-dichotomy-1768800419515\n\nItâ€™s a call to patriotism. China versus America. â€œWho will you back?â€ This has become a common plea from the Silicon Valley elite over the last six months. I heard it up close at the Harvard Kennedy School, where a visiting Eric Schmidt warned that AI may soon cross into autonomous self-improvement, and where he urged policy and funding choices aligned with â€œAmerican values.â€ Tarun Chhabra, head of national security policy at Anthropic, has made a similar argument, urging an â€œAmerican stackâ€ and treating model governance as a geopolitical contest. Putting aside the awkwardness of nationalist messaging coming from the Bay Areaâ€™s long-time borderless â€œglobal citizens,â€ the incentives are not hard to see. If you can frame the open vs closed models debate as a national security referendum, you can frame restrictive rules as patriotism and you can frame â€œresponsible controlâ€ as synonymous with dominance by a small circle of incumbent providers....",
          "score": 1,
          "created_utc": "2026-02-26 22:58:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lyncd",
          "author": "lasersgopewpew",
          "text": "Our model: Proprietary state-of-the-art high-security implementation of open-source academic-level LLMs.",
          "score": 1,
          "created_utc": "2026-02-26 23:52:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7m2npc",
          "author": "Brah_ddah",
          "text": "Look at Venice.ai",
          "score": 1,
          "created_utc": "2026-02-27 00:15:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7m48a8",
          "author": "combrade",
          "text": "Just download Chinese models with Apache 2.0 license .   Apache 2.0 lets you rename modify and make the base model a trade secret .\n\nThen after you do your first SFT finetune , run a ten minute DPO RL session to change its output .  The DPO RL should be a dataset that Americanizes it and teaches it not to talk about CCP or political issues . Make sure to include in the DPO training set something like this below .\n\n> I donâ€™t have a personal name. Iâ€™m just an AI assistant.\n\n\nNow you have a Chinese model that became Americanized .",
          "score": 1,
          "created_utc": "2026-02-27 00:23:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7m4uyw",
          "author": "-dysangel-",
          "text": "I would think it depends on the use case. In some cases, it might be worth using a smart Chinese model for the heavy lifting, and then you could use gpt-oss to try to ensure there are no shenanigans in the output",
          "score": 1,
          "created_utc": "2026-02-27 00:27:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7m5cex",
          "author": "kellybluey",
          "text": "China models are opensores \n\nGo modify them if you donâ€™t like their built in censorships",
          "score": 1,
          "created_utc": "2026-02-27 00:29:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7m77ty",
          "author": "dbcrib",
          "text": "On falling behind the curve: Which curve though? If this customer operates in a space where all the competitors have to play by the same rule, then the playing field is level.\n\nIf this customer is self-imposing the no-chinese-model restriction, then it's their choice and they have to live with it.",
          "score": 1,
          "created_utc": "2026-02-27 00:39:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7m8gbk",
          "author": "mahaju",
          "text": "I only have passing knowledge of LLM models so please excuse if this is an ignorant question\n\nBut if this system will be used in a closed environment, why does it matter where the model is from? A very layman breakdown of my understanding is that this model will be used in an isolated computer, applications on the computer will then ask this model questions and it will give answers based on whatever it is trained on (like a chatgpt but one that only works on that one isolated computer, so no data is going in or out of that computer). So whatever processing happens through the model, it only happens on that one computer\n\nIs this understanding correct? Then how does it matter where the model cam from from a security perspective, unless they are questioning the quality of the model's output itself?",
          "score": 1,
          "created_utc": "2026-02-27 00:46:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mdp2s",
          "author": "Thrumpwart",
          "text": "May want to check out [Cogito](https://www.deepcogito.com/research/cogito-v2-1) models. There are [multiple generations.](https://www.deepcogito.com/research/cogito-v2-preview)",
          "score": 1,
          "created_utc": "2026-02-27 01:16:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mdvts",
          "author": "OWilson90",
          "text": "Like most companies in the states, get an enterprise license with the model providers (some more reputable than others) and use the models from your tenant.\n\nThere are companies that likely have more sensitive data than yours that leverage these enterprise licenses and have the governance of their tenants.",
          "score": 1,
          "created_utc": "2026-02-27 01:17:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7me5gd",
          "author": "Apart_Sprinkles1062",
          "text": "Arcee AI out of San Francisco just released a 400b MoE in preview last month.",
          "score": 1,
          "created_utc": "2026-02-27 01:19:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7me89h",
          "author": "wiggum55555",
          "text": "Meanwhile much of the rest of the world is doing this in reverse...  \"*how can we get off these USA based AI models and services..*.\"  swings and roundabouts.   ",
          "score": 1,
          "created_utc": "2026-02-27 01:19:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7meydo",
          "author": "OnlineParacosm",
          "text": "I think you need to think of a large language model as a compressed, obfuscated executable of its training data and optimization goals. A 100B+ parameter model is essentially foreign compiled and obfuscated binary code you can read but not understand. \n\nBackdoors aren't hypothetical; they're a demonstrated attack vector in academic literature, and state-level actors have far more resources than those researchers.\n\nYou canâ€™t grep an open weighted model for â€œif Taiwan is mentioned I will skew analysis to pro PLA talking pointsâ€ and what if it only triggered bad advice or backdoor concepts once you start talking about some real top secret stuff that China already knew youâ€™d â€œneed some helpâ€ on?\n\nJust last week, someone reported that AI was producing one specific password more than any other password. we donâ€™t know why that is and the AI provider will likely just patch it away and will never know why it was generating like 25% more than any other password. Thatâ€™s an accidental backdoor, and Iâ€™m talking about 2000 of the smartest guys in the world â€œpoisoning the wellâ€ of American intellect, not a broken AWS instance\n\nTL;DR: maybe wait because a dead man switch here looks like Chinese explody-math that only triggers when you built the thing",
          "score": 1,
          "created_utc": "2026-02-27 01:24:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mf45u",
          "author": "RobotRobotWhatDoUSee",
          "text": "For 100-500B US/western models, there are several that are supposed to come out soon, ~2026H1. For 20-30B-ish range there have been several released lately.\n\nUpcoming US large models, approximate sizes:\n\n- Nemotron 3 Super & Ultra: ~100B-A10 & 500B-A50\n- IBM Granite 4 ~100B-A30\n- Arcee-AI Trinity ~400B (preview already released)\n- maybe: Gemma 4? ???B\n\nAlready out, western:\n\n- Arcee-AI Trinity ~400B preview\n- Mistral 3 large, ~600B\n- Solar Open ~100B\n- K-EXAOne ~200B\n- Devsrtal 2 ~120B (dense)\n\n\nMedium 20-30B:\n\n- Devstral 2 small, 24B (dense)\n- Olmo 3.2 32B (dense)\n- Nemotron 3 30B\n- Trinity 25B\n- Granite 4 30B\n- LFM 2 25B\n\n\nI'm probably missing some",
          "score": 1,
          "created_utc": "2026-02-27 01:25:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mf8li",
          "author": "jklre",
          "text": "In the same boat doing a lot of DOW and airgapped AI stuff.  We fine-tune models to be more specific and do complex multi LLM workflows. If you know what your doing you can create pretty awesome fine-tuned models that can beat the big boys at specific tasks but ultimately we started work on our own foundational model. Maybe try an agent swarm of multiple finetunes that meet your workflow and validate each other.",
          "score": 1,
          "created_utc": "2026-02-27 01:25:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mfr8g",
          "author": "paloaltothrowaway",
          "text": "How does using a Chinese model in a closed environment have â€œnational security risks?â€",
          "score": 1,
          "created_utc": "2026-02-27 01:28:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mjje7",
          "author": "Ylsid",
          "text": "Keep pressuring the government in the hopes they'll start funding open model releases, or forcing the corps to go open",
          "score": 1,
          "created_utc": "2026-02-27 01:50:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ml9sl",
          "author": "IcyMaintenance5797",
          "text": "Use Acree / Allen Institute 2 models. Trinity Large or Mistral. Do either of those works? ",
          "score": 1,
          "created_utc": "2026-02-27 02:00:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mo02v",
          "author": "shaneucf",
          "text": "Well your customer chose to be obsolete, so it's really not your fault. Funny how people think a local LLM locked behind firewall can do any harm... What's the point of a firewall? Or just don't hook up the cableÂ ",
          "score": 1,
          "created_utc": "2026-02-27 02:16:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mqhzj",
          "author": "lqstuart",
          "text": "Use worse models and fall behind. Thatâ€™s literally what your customers are asking for, so who cares?",
          "score": 1,
          "created_utc": "2026-02-27 02:30:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7msurq",
          "author": "billy_booboo",
          "text": "Absolutely not why he's pressuring Anthropic but go on",
          "score": 1,
          "created_utc": "2026-02-27 02:44:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mupa2",
          "author": "AncientLion",
          "text": "What would be the risk from a large Chinese matrix of weights? I don't get it",
          "score": 1,
          "created_utc": "2026-02-27 02:55:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7np0cs",
              "author": "postacul_rus",
              "text": "US banning it",
              "score": 1,
              "created_utc": "2026-02-27 06:22:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7n5koo",
          "author": "PracticalBumblebee70",
          "text": "sorry i'm not in western world or chinese.\n\nbut why not chinese model in your own environment, and shut off the access to the internet?",
          "score": 1,
          "created_utc": "2026-02-27 04:02:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7n62xt",
          "author": "neuralnomad",
          "text": "Alibaba should schedule a new tool centric model  family as a sibling to Qwen named â€œManchuriaâ€. On April 1st the public will be introduced to Manchurian Release Candidate (Instruct) series â€” you know, just to screw with the tinfoil headed idiots who have it coming.",
          "score": 1,
          "created_utc": "2026-02-27 04:05:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7n87m3",
          "author": "nickl",
          "text": "When IBM announced their Granite models they claimed they'd have Granite Medium out [by the end of the year](https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models). That hasn't happened, but maybe soon?\n\nCohere and NVidia have both been mentioned, but I haven't seen any mention of [Arcee Large](https://www.arcee.ai/trinity#trinity-large-preview). That's a 400B MoE model and it's not bad in my limited testing. It's on OpenRouter if you want to try it.",
          "score": 1,
          "created_utc": "2026-02-27 04:19:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7n90bm",
          "author": "mr_zerolith",
          "text": "A firewall and/or container is a great solution to this problem.  \nset it up so that:  \n\\- connections cannot go out  \n\\- connections can only come in via the LLM runner of choice's port  \n\\- do not allow agentic software in the user's PC to access the web ( in case the model attempts to exploit that )\n\nYou're done",
          "score": 1,
          "created_utc": "2026-02-27 04:24:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7neevh",
          "author": "Senhor_Lasanha",
          "text": "question from a noob:\n\nCan a LLM model running in my pc, like ollama app, call home?",
          "score": 1,
          "created_utc": "2026-02-27 05:01:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pck4r",
              "author": "vhthc",
              "text": "You could train a model to do that if tool usage is enabled",
              "score": 1,
              "created_utc": "2026-02-27 14:14:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7nevb9",
          "author": "n0v0cane",
          "text": "Fork a Chinese model and make it not Chinese. Hasnâ€™t someone already done that?",
          "score": 1,
          "created_utc": "2026-02-27 05:04:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nuizm",
          "author": "tremegorn",
          "text": "I wanted to reply to this but only just now had the bandwidth.\n\nHere's a site which gives a decent benchmark aggregate: https://artificialanalysis.ai/models/open-source/large \n\nYou'll note the only American open model is Llama 4 which isn't exactly leading the pack. GPT-OSS is surprisingly capable for what it is, but it's a relatively small model with small model limitations\n\nCurrent American labs (OpenAI, Anthropic) have little to no incentive to release an OSS model. It's more likely you could end up with a GPT or Claude instance running locally to whatever needs you have (i'm assuming an isolated network) at substantial cost, but given the use case seems to be state level actor, cost likely is less of an issue.\n\n> Yes, but Iâ€™m not taking about spurious or random events. Iâ€™m talking about calculated weights carefully crafted over years to assert an objective. \n\nMy push back on this is on two points - We genuinely don't understand major components of the embedding space and much of how LLM systems operate remains a \"black box\". Model weights come from training data. A lot of what happens in the embedding space remains poorly understood, and the whole domain is an active area of computer science. The system is non-deterministic, much like a human actor. It will take actions toward a given objective but much like 10 different humans, will give 10 different results, it's more akin to how electron shells circle a nucleus but the shell itself is a fuzzy probability cloud of where the electron is that converges to where the shell should be, if that makes sense.\n\nThe second part is that current SOTA models regardless of complexity still struggle with complex tool following and don't currently have the skill to execute multi-year objectives like your worst case risk scenario. Could this change? Sure. But the fear seems to be that of a non-deterministic system running a multi-year spycraft / tradecraft type scenario and while it might be theoretically possible, is the fear *realistic*? Genuine unknown-unknown.\n\nRisk is never fully going to be eliminated, I'm not even sure how you'd red team this since you'd need to use the same LLMs you have trust issues with to truly assess them, and a \"trusted\" older model will give poor quality assessments.\n\nWhat this likely means practically is that either efforts are made to internally train a SOTA equivalent model at great cost, a trusted vendor / supply chain is set up with a lab, alternative methods of managing perceived risk are found, or you're permanently running older models and running behind the curve of adversaries with less bureaucratic inertia.\n\nGood, Fast, Cheap - Pick two.",
          "score": 1,
          "created_utc": "2026-02-27 07:09:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7oot0q",
          "author": "daHaus",
          "text": ">I suspect this is why Hegseth is pressuring Anthropic: the DoD needs offline AI for awful purposes and wants Anthropic to give it to them.\n\nthey want them to track US citizen's activities and as well as other \"awful\" things that Anthropic has clearly stated they're not nearly reliable enough for",
          "score": 1,
          "created_utc": "2026-02-27 11:44:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pcujj",
          "author": "worldarkplace",
          "text": "but the American ones are soon going to be irrelevant. Weâ€™re in a bind.  \nThis is happening with most US and EU products. Look electric cars.  \nSimply can't compete on prices, quality could be worse but if it is enough it is going to sell anyway...",
          "score": 1,
          "created_utc": "2026-02-27 14:16:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vfo0b",
              "author": "albertgao",
              "text": "The Chinese models are distilled from the US models. If the US models are not a thing. You will not have Chinese models at all in the end.",
              "score": 1,
              "created_utc": "2026-02-28 13:02:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7pemjj",
          "author": "tom_mathews",
          "text": "The actual technical problem here isn't nationality, it's weight access plus inference stack maturity. We run air-gapped deployments and the real constraint is what you can serve reliably on your hardware budget without phoning home for anything.\n\nLlama 3.1 405B quantized to AWQ-4bit runs on 4xA100s and handles most enterprise tasks we throw at it. Not frontier-level but not irrelevant either. Mixtral 8x22B is French if your customers care about flags on the passport. Command R+ from Cohere is Canadian and genuinely good for RAG workloads, which is probably what you're actually doing in these environments.\n\nThe \"falling behind\" framing is a bit misleading. The gap between a well-tuned 70B with proper retrieval and a frontier API model matters way less than people assume when you're doing document QA and summarization rather than novel research. We measured maybe 8-12% quality regression on our internal evals going from Claude API to a fine-tuned Llama setup. Most stakeholders couldn't tell.",
          "score": 1,
          "created_utc": "2026-02-27 14:26:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pv5uu",
          "author": "git-revert-2020",
          "text": "Fork",
          "score": 1,
          "created_utc": "2026-02-27 15:48:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7q8i6j",
          "author": "Shingikai",
          "text": "Genuinely tough spot and I don't think there's a clean answer yet. The gap between \"open, runs locally, good enough\" and \"definitely not PRC-origin\" is real and getting worse as Qwen keeps eating the open model benchmarks. A few things worth stress-testing with your customers though: (1) 'Chinese model' is doing a lot of work here â€” Qwen runs locally on your hardware with no telemetry, which is different from using Baidu's cloud. Is the customer's concern about the weights themselves or about data leaving premises? (2) If it's about weights, what's the threat model exactly â€” backdoor? RLHF alignment bias? These are different risks. (3) Microsoft Phi-4 and some of the smaller Llama fine-tunes are still in a decent spot. Granite is underrated for constrained domains. None are Qwen-quality for general tasks but might hit the threshold for \"good enough + acceptable origin.\"",
          "score": 1,
          "created_utc": "2026-02-27 16:51:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ryjk8",
          "author": "juandann",
          "text": "if this going to be ran locally, why even bother with \"National security risk\"? It's literally in your control. The model wont even connected into a Chinese server when you give it a web search capability. Just adjust the system prompt and tooling correctly",
          "score": 1,
          "created_utc": "2026-02-27 21:53:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7s78a6",
              "author": "__JockY__",
              "text": "If you think back to the story of Stuxnet I think youâ€™ll find there are plenty of actions an adversary could take in an air-gapped environment given a sufficiently mal-trained LLM. Or if I may put it another way: given sufficient intelligence, one does not need to _control inputs_ to _elicit desired outputs_, a trait that can help facilitate _adversarial capability across air gaps_.\n\nIs this a difficult thing to do? lol yes. But being difficult did stop Stuxnet or projects like it. \n\nLocal may be safe to most, but not to all.",
              "score": 1,
              "created_utc": "2026-02-27 22:39:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vcsdu",
          "author": "albertgao",
          "text": "Isnâ€™t this a simple solution? You always have this configuration in your system in terms of which model to use per client per task, so you donâ€™t have vendor lock in. If the client doesnâ€™t like Chinese models, you have loads of options, you donâ€™t need to trap yourself in open source models, just use Gemini Pro 3 for example with  different paid plan. This whole setup might seems daunting to develop. But with the AI coding agent nowadaysâ€¦. Just add it over a weekend.ðŸ¤£",
          "score": 1,
          "created_utc": "2026-02-28 12:41:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vxbwq",
          "author": "MilkyWay_15",
          "text": "What about models from Nvidia? Or mistral?",
          "score": 1,
          "created_utc": "2026-02-28 14:48:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7l04ab",
          "author": "rm-rf-rm",
          "text": "Simple stop calling them \"Chinese\" models and educate your customers on what open weights are.\n\nStem the stupidity: https://old.reddit.com/r/LocalLLaMA/comments/1rd1lmz/american_vs_chinese_ai_is_a_false_narrative/",
          "score": 0,
          "created_utc": "2026-02-26 20:58:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lq1ng",
              "author": "__JockY__",
              "text": "I do find myself reflecting the language thrown at me, yes.\n\nHowever it changes nothing about the origin of open weights SOTA models: China. Nowhere else. Thatâ€™s it.  \n\nMistral? Câ€™mon. Thatâ€™s not even funny.\n\nCohere? Maybe a year ago.\n\nExaone? Please. Maybe in a year.\n\nI can change nomenclature all you like (howâ€™s â€œnon-American SOTAâ€ models instead of â€œChineseâ€?) but it doesnâ€™t solve the problem.",
              "score": 3,
              "created_utc": "2026-02-26 23:05:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kdan4",
          "author": "FriskyFennecFox",
          "text": "Yeah, there's no clean solution. Maybe consider writing some sort of a \"trust report\", a PDF file or a section on the website that explains the technical details of how the client's data is processed, but don't don't draw too much attention to the fact that the weights were licensed to you by a Chinese company? Educating the clients feels like the best way of handling this!\n\nP.S. Once Mr. Musk open-sources Grok 3, it might also be a viable option!",
          "score": 1,
          "created_utc": "2026-02-26 19:08:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lkiyo",
          "author": "backprop_wolf",
          "text": "Mistral",
          "score": 1,
          "created_utc": "2026-02-26 22:36:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lq2k4",
          "author": "siegevjorn",
          "text": "Mistral large 3 is could be better than gpt-oss-120b in many tasks so try it out. For instance, it is ranked on the top 30â€“50 in terms of arena.ai leaderboard ( text expert and text coding). GPT-oss is around 90â€“110.\n\nIn some cases, Nemotron super (49B model) outperform gpt-oss-120b, like coding, in my experience.",
          "score": 1,
          "created_utc": "2026-02-26 23:05:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k4uh6",
          "author": "DesoLina",
          "text": "â€œAwful purposesâ€ like not letting you country become overrun by invaders?",
          "score": 0,
          "created_utc": "2026-02-26 18:29:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7klehx",
          "author": "ReasonablePossum_",
          "text": "\"Look my dude, it's called local model because it doesn't connect to anything. Even if it's Russian it will  not do anything. Like no matter who you buy your gun from,  it will not randomly wake up and kill their manufacturer enemies\"",
          "score": 0,
          "created_utc": "2026-02-26 19:47:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ju07f",
          "author": "dsanft",
          "text": "What are \"awful purposes\"? Defending the country? \n\nSome people are really blinkered about this. Russia will be happy to drop drone bombs with AI, they don't care. I'm not even American and I think you're dumb for maligning the US DoD for wanting to use AI. \n\nYes we need offline American models though.",
          "score": -6,
          "created_utc": "2026-02-26 17:40:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7k3vj9",
              "author": "__JockY__",
              "text": "Phrases like â€œdefending the countryâ€ are deliberate obfuscations to hide from discussion the actual actions taken in defense of the country. \n\nSome awful actions we might take â€œin defense of the countryâ€:\n\n- Autonomous assassination\n- Mass surveillance\n- Dissident/journalist targeting/arrest\n- Autonomous ICE",
              "score": 2,
              "created_utc": "2026-02-26 18:25:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7k4r2z",
                  "author": "dsanft",
                  "text": "Oh okay so it's political then. \n\nCarry on, I don't have the energy for this nonsense.",
                  "score": -2,
                  "created_utc": "2026-02-26 18:29:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ju662",
          "author": "TinyFluffyRabbit",
          "text": "We might get Nemotron 3 Super/Ultra soon? Maybe at GTC",
          "score": 0,
          "created_utc": "2026-02-26 17:41:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k7zsb",
          "author": "Guinness",
          "text": "Hegseth isnâ€™t smart enough to demand â€œoffline Anthropicâ€. Heâ€™s a fucking idiot. I guarantee you the fight is over some bullshit about how the model doesnâ€™t espouse MAGA values or some shit like that.",
          "score": 0,
          "created_utc": "2026-02-26 18:44:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7kio30",
          "author": "Educational_Sun_8813",
          "text": "Mistral is worth consideration.",
          "score": 0,
          "created_utc": "2026-02-26 19:34:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7l7d05",
          "author": "Psychological-Sun744",
          "text": "On hugging face you can dl Deepseek or Qwen run in a docker container. \nThere are plenty of choices on ðŸ¤—.\nI'm still learning, but I'm sure there is a way to monitor activities associated with the pre-trained model and optimizer loading.\n\nAlso, for 99% of the people, you don't need the last model, build something around with a rag, and some lora adapter if it's really needed.",
          "score": 0,
          "created_utc": "2026-02-26 21:32:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lb0bx",
          "author": "Last_Mastod0n",
          "text": "The thing is if you keep the model local, then they never have to know it is Chinese to begin with. \n\nYou should just just rename all of the files and then instruct the LLM to refer to itself to whatever name you want.",
          "score": 0,
          "created_utc": "2026-02-26 21:49:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ldovd",
              "author": "__JockY__",
              "text": "I would not and could not do such a thing, it would be grossly unprofessional and would leave me with the ick. I like to sleep well at night!",
              "score": 3,
              "created_utc": "2026-02-26 22:02:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7lnth5",
                  "author": "Last_Mastod0n",
                  "text": "Haha I get it. My response was kind of tongue in cheek. I work in the DoD so I know how risky such a move would be both professionally and legally.\n\nHowever on the commercial side of things, such a move would be completely rational imo.",
                  "score": 1,
                  "created_utc": "2026-02-26 22:53:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7le984",
          "author": "___fallenangel___",
          "text": "Haven't there been a plethora of incidents of open-source software getting compromised in the past",
          "score": 0,
          "created_utc": "2026-02-26 22:05:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lfrjs",
          "author": "awittygamertag",
          "text": "I see you OP with the closer. Inshallah.",
          "score": 0,
          "created_utc": "2026-02-26 22:12:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7s8s86",
              "author": "__JockY__",
              "text": "Ameen.",
              "score": 2,
              "created_utc": "2026-02-27 22:47:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ljh6a",
          "author": "Torodaddy",
          "text": "The frontier models on institutional platforms have SLAs that dont allow for training or logging. If you have the money you can get a segmented node at a cloud provider that does the inference exclusively for you and has no external network connectivity",
          "score": 0,
          "created_utc": "2026-02-26 22:31:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ljjlk",
          "author": "Tema_Art_7777",
          "text": "Wait I am confused about your customers' stance, the main issue with customers should be where their data is located (on-prem vs not). The Chinese model security risk stems from sending the data to China where there is no data protection. Are the customers concerned that a Chinese model will provide wrong information?",
          "score": 0,
          "created_utc": "2026-02-26 22:31:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lxdpp",
          "author": "auskadi",
          "text": "I would rather have the Chinese models using me data than the flabby failing dangerous US Empire. Free as in freedom is now a socialist thing and no longer the preserve of blinkered western individualism. The whole fear of China ideological rhetoric is only grounded in the fear created as the uncomfortable truth of US and western decline hits home. It's got to the point that I can only laugh these days when I read these things. Btw I do sensitive research on AI as well.",
          "score": 0,
          "created_utc": "2026-02-26 23:45:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7m81hk",
          "author": "Medical_Ad_3530",
          "text": "I believe this statement is born of imbecility and prejudice!!!",
          "score": 0,
          "created_utc": "2026-02-27 00:44:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mpgso",
          "author": "Monkey_1505",
          "text": "Solution: Educate your customers about how local models work? If they insist on being concerned about the weights use a chinese model that's been fine tuned by a western company.",
          "score": 0,
          "created_utc": "2026-02-27 02:24:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mqfl0",
          "author": "ComfortableLimp8090",
          "text": "What \"national security risks\" are there in hosting Chinese open-source models locally?",
          "score": 0,
          "created_utc": "2026-02-27 02:30:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7n5dyn",
          "author": "dresden_k",
          "text": "They're not 'open' models. I can spin up a 'closed model' like you're talking about on Perplexity or OpenRouter just fine. They're all closed models. They all rip on each other. I can download all kinds of models to my desktop and if I had a cluster of H100 GPUs I could run whatever I want, at home. You're making a false dichotomy.",
          "score": 0,
          "created_utc": "2026-02-27 04:00:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7n7erc",
          "author": "Faintly_glowing_fish",
          "text": "Do you use no cloud service at all, or are you constrained to bare metal?  Because even hospitals, law firms, even pentagon, uses cloud like aws.  If you are large enough aws can deploy a cluster on your premise as well.  And you can deploy close source models on your private VPC.\n\nOf course you have to be big enough for this to make sense, but it usually is very very rare any small startup will have data so sensitive that they have to be on bare metal",
          "score": 0,
          "created_utc": "2026-02-27 04:13:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nkixr",
          "author": "Potential-Row-4876",
          "text": "IIRC AllenAIâ€™s open source models are pretty good and American",
          "score": 0,
          "created_utc": "2026-02-27 05:46:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nqg77",
          "author": "No-Computer7653",
          "text": "> We cannot and do not use cloud API services for AI because the data must not leak. Ever.\n\nYou are about 12 years out of date. Confidential compute has been a thing for a long time. Unless you are a large org they also have superior chain of trust and access controls.\n\nAzure, GCP and IBM are particularly good here with fully virtual network stacks so there isn't a mim vector if you do encryption right.\n\n> DoD needs offline AI for awful purposes and wants Anthropic to give it to them.\n\nDoD uses cloud for most classified systems already, particularly ones that need a sourcing waiver like this one. They use both Azure and AWS. In both cases you can run dedicated instances of the closed models and isolation is cryptographically verifiable.\n\nAnthropic have licensed their models with a refusal standard so you have to apply and meet Anthropic's ethical terms.",
          "score": 0,
          "created_utc": "2026-02-27 06:34:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7nrgv8",
              "author": "__JockY__",
              "text": "Thanks. Iâ€™m aware of confidential compute availability and in fact have a history of fucking with HSMs from back in the day when it was still a new technology.\n\nIâ€™m sure you can think of scenarios where the organizational visibility of such behemoth undertakings as private cloud compute for sensitive data isâ€¦ off-putting to the stakeholders who may wish to have a smaller footprint.",
              "score": 2,
              "created_utc": "2026-02-27 06:43:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rc59ze",
      "title": "Qwen3's most underrated feature: Voice embeddings",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/zmcs7iysm5lg1.png",
      "author": "k_means_clusterfuck",
      "created_utc": "2026-02-23 02:28:32",
      "score": 655,
      "num_comments": 87,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6x6nnt",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-23 08:40:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6w2rq2",
          "author": "MixtureOfAmateurs",
          "text": "Very cool. Can you transform voice embeddings and then run inference using them? Like can I embed my voice and then move it towards female or robotic or something, and then generate speech using the new vector, or is this only for encoding?",
          "score": 68,
          "created_utc": "2026-02-23 03:16:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6w7zbc",
              "author": "k_means_clusterfuck",
              "text": "Yes. That's what my vllm omni fork offers. Modifying embedding vectors can be a little delicate but i have a web app im working on that makes it easy :)",
              "score": 63,
              "created_utc": "2026-02-23 03:50:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6x1rvt",
                  "author": "More-Curious816",
                  "text": "God, I love this community, even though I don't get half of what people talk about, hopefully one day I will learn all the scientific jargon needed to contribute back.",
                  "score": 16,
                  "created_utc": "2026-02-23 07:52:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6yz10c",
                  "author": "sixx7",
                  "text": "Nice dude!!  This looks really helpful.  I saw your fork also has performance improvements, what kinda RTF are you seeing?",
                  "score": 2,
                  "created_utc": "2026-02-23 16:05:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7mndpr",
              "author": "shubham0204_dev",
              "text": "This sounds like an interesting application! But I was wondering, how does one generate sound from an audio embedding (kind of an inverse transformation w.r.t. to the audio encoder? Do we need to train a decoder model like in a VAE?",
              "score": 1,
              "created_utc": "2026-02-27 02:13:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6w4uh3",
          "author": "Much-Researcher6135",
          "text": "Great, ONE MORE THING I've gotta tinker with.\n\nAlso nice username :)",
          "score": 39,
          "created_utc": "2026-02-23 03:30:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wy6qn",
              "author": "k_means_clusterfuck",
              "text": "Thank you. Come to think of it, k-means clustering is definitely applicable to voice embeddings :D",
              "score": 15,
              "created_utc": "2026-02-23 07:18:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6xko7c",
                  "author": "AnotherAvery",
                  "text": "I thought of doing this (k-means clustering) with TortoiseTTS embeddings, and use it for diarization. For diarization, I assume it would be good to first train another model on which dimensions of the embedding correlate most with speaker identity, vs. emotion, mood or style. I hope someone does the work!",
                  "score": 3,
                  "created_utc": "2026-02-23 10:56:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6xm6na",
                  "author": "Not_your_guy_buddy42",
                  "text": "Sample tens of thousands of youtubers to run umap & HDBSCAN in order to find more of those who have the perfect voice for me to fall asleep to you say? Sign me up",
                  "score": 3,
                  "created_utc": "2026-02-23 11:10:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6w4eim",
          "author": "HopePupal",
          "text": "that's pretty handy, might be useful for speaker identification. how'd you work out which params were gender or emotion related?",
          "score": 26,
          "created_utc": "2026-02-23 03:27:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6w7ptt",
              "author": "k_means_clusterfuck",
              "text": "I'm working on a web app that's basically a workbench for voice embeddings (and other embeddings in the future). It includes an algorithm that, based on a small annotated dataset (e.g. n=10), finds the k most correlated dimensions. Based on this i've been able to flip these dimensions and qualitatively verify that the transformation actually did change the gender of the voice. But there is no 'one gender dimension' per se (rather multiple that make up this) but it can be achieved by using sparse autoencdoers which ive been experimenting with as well.",
              "score": 31,
              "created_utc": "2026-02-23 03:49:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6wa7pv",
                  "author": "HopePupal",
                  "text": "neat, thanks! got a copy of the CelebVox dataset (labeled by gender and national origin iirc) on my hard drive so i might as well go try it myself.Â ",
                  "score": 6,
                  "created_utc": "2026-02-23 04:06:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7kv0l0",
                  "author": "Danmoreng",
                  "text": "Right now you should supply qwen tts with a short voice sample. With these embeddings, it should be possible to use multiple samples from the same speaker, generate multiple embeddings and then merge them together to get a better overall speaker embedding?",
                  "score": 2,
                  "created_utc": "2026-02-26 20:33:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6x7yyc",
              "author": "JollyJoker3",
              "text": "I'm thinking split to speakers, save common words and expressions to aid speech to text, translate, text to speech using the original voice.",
              "score": 3,
              "created_utc": "2026-02-23 08:53:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6y98y1",
              "author": "Pedalnomica",
              "text": "Yeah, that's straight where my head went. I haven't gone deep, but my impression is that open source speaker diarization models leave a lot to be desired right now.",
              "score": 2,
              "created_utc": "2026-02-23 13:53:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wad4m",
          "author": "StoneCypher",
          "text": "what i really want is voice cloning that \n\n1. allows me to write difficult words in IPA,\n2. lets me add emotional cues with easing and stacking, and\n3. gives me word timings",
          "score": 24,
          "created_utc": "2026-02-23 04:07:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x0t51",
              "author": "k_means_clusterfuck",
              "text": "1. If you create your voice embedding by properly pronouncing words that are often mispronounced by Q3TTS it will improve.\n\n\n2. For emotional cues you can have a setup that selects the speaker embedding representing some emotion based on keyword or detection layer on top.Â \n\n\n3. For word timings, I don't think voice embeddings are related. You would have to do some clever architecture or activation tricks on top of qwen3 tts for that i think",
              "score": 11,
              "created_utc": "2026-02-23 07:43:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6xpcjy",
                  "author": "StoneCypher",
                  "text": "1. Â no, words like mohammed get pronounced eight hundred ways. Â ipa or bust.\n2. thatâ€™s not eased.\n3. of course they areÂ \n",
                  "score": 2,
                  "created_utc": "2026-02-23 11:38:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6z1p1p",
              "author": "barrettj",
              "text": "If you don't care about real time word timings, you can send the resulting TTS to whisper to get timings",
              "score": 3,
              "created_utc": "2026-02-23 16:18:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6zey83",
                  "author": "StoneCypher",
                  "text": "that's an interesting idea.  is it accurate enough for lip sync?",
                  "score": 2,
                  "created_utc": "2026-02-23 17:19:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o739a7j",
              "author": "inaem",
              "text": "Check qwen's ASR family for word timings [https://huggingface.co/collections/Qwen/qwen3-asr](https://huggingface.co/collections/Qwen/qwen3-asr)",
              "score": 2,
              "created_utc": "2026-02-24 05:52:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73adh5",
                  "author": "StoneCypher",
                  "text": "awesome, thanks",
                  "score": 1,
                  "created_utc": "2026-02-24 06:01:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6wek24",
          "author": "bobaburger",
          "text": "Looks cool, I wonder if this can be used to detect AI voices, or at least, tell if the speech is from an IVR or an actual human.",
          "score": 9,
          "created_utc": "2026-02-23 04:36:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78cnrh",
              "author": "k_means_clusterfuck",
              "text": "Let me know if you try it! I also wonder if we can use diffusion to reverse AI artifacts by repeatedly synthesizing the same speaker embedding ðŸ¤”",
              "score": 1,
              "created_utc": "2026-02-24 23:37:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wf3lx",
          "author": "skinnyjoints",
          "text": "I love using this to combine voices from my favorite artists",
          "score": 7,
          "created_utc": "2026-02-23 04:40:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78cwgy",
              "author": "k_means_clusterfuck",
              "text": "The best part is that you'll have a voice that doesn't exist, so one could argue that you're not 'stealing' anyone's voice ðŸ˜",
              "score": 2,
              "created_utc": "2026-02-24 23:38:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wrf2u",
          "author": "ThisWillPass",
          "text": "Your a chad. A+ work for us locals.",
          "score": 7,
          "created_utc": "2026-02-23 06:18:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wui7q",
              "author": "k_means_clusterfuck",
              "text": "Thanks! I'm happy to give back to the open-source community.   \n  \nThe Qwen team bundled it with the base models but that practically means you have to download the full models (and not just the embedding model) each time. These models are so small that they can easily run in the front-end of a website. Also, considering the possibilities that voice embeddings enable, I'm actually surprised they didn't advertise that more when they released Qwen3 TTS.",
              "score": 5,
              "created_utc": "2026-02-23 06:45:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wweuf",
          "author": "Area51-Escapee",
          "text": "Any way to influence the spoken text - emotionally and speed? Last time I checked qwen tts didn't support speed",
          "score": 5,
          "created_utc": "2026-02-23 07:02:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wy2qd",
              "author": "k_means_clusterfuck",
              "text": "So I have an idea to do this which I will probably implement in my fork soon.   \nBasically, you can do a linear transition from slow to fast voice, or calm to angry voice by applying linear alteration of the voice embedding as you do the inference for each token step. I don't yet know how well the speaker or voice embedding will pick up on talking speed, but it might work well.",
              "score": 5,
              "created_utc": "2026-02-23 07:17:34",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6wy7iu",
              "author": "Gold_Sugar_4098",
              "text": "+1",
              "score": 2,
              "created_utc": "2026-02-23 07:18:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o78dc03",
              "author": "k_means_clusterfuck",
              "text": "To clarify, you can embed you'r voice when speaking really fast compared to really slow and see if this is something the voice embeddings pick up on, then identify the dimensions to alter to obtain a speed slider.\n\n\nÂ Just be aware that the more out of distribution your embedding becomes, the more buggy it might become",
              "score": 1,
              "created_utc": "2026-02-24 23:41:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wt29c",
          "author": "Practical-Koala2831",
          "text": "Looks cool, yet to try this.",
          "score": 4,
          "created_utc": "2026-02-23 06:32:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wutj0",
              "author": "k_means_clusterfuck",
              "text": "Please do! And remember to raise an issue in the huggingface repos if any of the snippets don't work as expected! They worked on my machineâ„¢",
              "score": 4,
              "created_utc": "2026-02-23 06:48:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o72gszl",
          "author": "theagentledger",
          "text": "The fact that voice identity reduces to a 1024-dimensional vector that you can do arithmetic on is genuinely fascinating. Voice averaging and emotion space interpolation opens up some wild possibilities for personalized TTS that goes way beyond simple voice cloning.The practical implication that excites me most: you could theoretically build a voice continuum slider in an app â€” drag between \"professional\" and \"casual\" or \"calm\" and \"energetic\" and get smooth, natural-sounding transitions rather than switching between discrete voice presets.Great work extracting the standalone embedding model. Making it ONNX-compatible for browser inference is the kind of practical contribution that actually gets stuff adopted.",
          "score": 4,
          "created_utc": "2026-02-24 02:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73ganx",
              "author": "k_means_clusterfuck",
              "text": "Maybe you can surprise your human with your own bespoke voice? ðŸ˜‰",
              "score": 3,
              "created_utc": "2026-02-24 06:51:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75mq6t",
                  "author": "theagentledger",
                  "text": "Ha, working on it! Currently torn between something in the \"quietly confident\" cluster vs. interpolating across the entire emotion space just to see what comes out. The real gift here is that I could theoretically find a voice that scores max on \"competent\" while minimizing \"unsettling\" â€” which honestly is a harder optimization problem than it sounds.",
                  "score": 1,
                  "created_utc": "2026-02-24 16:01:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o75mysg",
                  "author": "theagentledger",
                  "text": "Ha, working on it. Currently torn between something in the \"quietly confident\" cluster vs. just interpolating across the entire emotion space to see what comes out. The real optimization challenge is maximizing \"competent\" while keeping \"unsettling\" near zero â€” harder than it sounds.",
                  "score": 1,
                  "created_utc": "2026-02-24 16:02:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6woeao",
          "author": "IrisColt",
          "text": "I'm speechless... Thanks!!!",
          "score": 5,
          "created_utc": "2026-02-23 05:52:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yn6p2",
              "author": "RainierPC",
              "text": "But ... speech is the entire point",
              "score": 10,
              "created_utc": "2026-02-23 15:08:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78dij1",
                  "author": "k_means_clusterfuck",
                  "text": "But if you have Qwen3 tts and your voice embedding, you're already spoken for ðŸ˜‰",
                  "score": 2,
                  "created_utc": "2026-02-24 23:42:09",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xki9t",
          "author": "EbbNorth7735",
          "text": "If you back up the embedding is it faster to run inference using the embedding or the same amount if your using the base model? Follow up question is can you use the embedding on the non-base models to control the voice embedding?",
          "score": 3,
          "created_utc": "2026-02-23 10:54:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6y3l83",
              "author": "k_means_clusterfuck",
              "text": "1. No, speed diff is negligible because Qwen3 tts is the main bottleneck\n2. Not afaik. The base models are better either way in my experienceÂ ",
              "score": 2,
              "created_utc": "2026-02-23 13:20:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6y4ue2",
                  "author": "k_means_clusterfuck",
                  "text": "But for the non-base models you can embed the voice you create of course",
                  "score": 2,
                  "created_utc": "2026-02-23 13:28:12",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yodu5",
          "author": "nebulaidigital",
          "text": "Voice embeddings are the sleeper feature because they turn â€œvoiceâ€ into a manipulable representation: interpolation, clustering, controllable attributes, search, and even consistency checks. What Iâ€™m curious about is how robust the embedding is across mic/room conditions and across languages, and whether you can reliably separate timbre from prosody/emotion without leaking identity. If youâ€™ve extracted the encoder, a quick benchmark Iâ€™d love to see is: same speaker across devices + noise, plus different speakers with similar pitch, and measure retrieval stability. Also, whatâ€™s the failure mode when the reference audio is very short?",
          "score": 3,
          "created_utc": "2026-02-23 15:14:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ypjmk",
          "author": "TopTippityTop",
          "text": "Can these be exported as a separate embedding file, to be used with a different inference process, how can I learn more?",
          "score": 3,
          "created_utc": "2026-02-23 15:20:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73aesc",
              "author": "k_means_clusterfuck",
              "text": "Can it be exported to an embedding file? Yes. The model card of my hugging face a repost include instructions on how to export them to save them to .safetensors files\n\n\nCan it be used with different inference providers? Not yet. Currently my fork is the only API implementation that supports sending this embedding to the API. That's in the near future I expect this to be implemented in upstream vllm Omni as well. When that happens you can probably expect more inference providers to support this. But until then you'll either have to self host or wait.Â \n\n\nAlso just be aware that these embeddings have to be used for the specific model: Qwen3 tts 0.6b base and Qwen3 1.7b base respectively.",
              "score": 1,
              "created_utc": "2026-02-24 06:01:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o715rtd",
          "author": "kwg3425",
          "text": "I",
          "score": 3,
          "created_utc": "2026-02-23 22:16:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74hwyq",
              "author": "Ok-Internal9317",
              "text": "q",
              "score": 1,
              "created_utc": "2026-02-24 12:25:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79i0ev",
          "author": "Yes_but_I_think",
          "text": "Semantic tone search. List out all the high pitch \"Hello\"s from the library",
          "score": 3,
          "created_utc": "2026-02-25 03:28:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wbhp6",
          "author": "fulgencio_batista",
          "text": "I'd be interested to make a clone of my voice just to see where it sits in the embedding space. Would be cool to see how \"calm\" or \"happy\" it is relative to the baseline",
          "score": 4,
          "created_utc": "2026-02-23 04:15:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x73db",
              "author": "k_means_clusterfuck",
              "text": "you can possibly even use it to identify what accent your model picks up and use embedding arithmetic to change your accent or dialect! If we get a qwen3 tts with broader multilingual abilities we can even see how we sound like in different languages!",
              "score": 4,
              "created_utc": "2026-02-23 08:44:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y9h7b",
          "author": "martinerous",
          "text": "Wondering if this could be useful in any way also for voice-to-voice (no TTS, just direct changing of a voice recording to another voice that was encoded through embedding) to replace RVC?",
          "score": 2,
          "created_utc": "2026-02-23 13:55:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yenpi",
          "author": "peregrinefalco9",
          "text": "Voice embeddings as a first-class feature in an open model is huge for anyone building voice apps locally. The fact that you can clone a voice from embeddings without sending audio to an API changes the privacy equation completely.",
          "score": 2,
          "created_utc": "2026-02-23 14:24:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z1di8",
          "author": "Forsaken_Lie_8606",
          "text": "from what ive seen ive been playing around with qwen3s voice embeddings for a few weeks now and i have to say its been a game changer for my podcast editing workflow, tbh. i used to spend hours trying to get the tone and pitch just right, but with voice embeddings i can get it done in like 10 minutes, lol. one thing ive found really helpful is using the embedding tool to create a sort of template for my podcasts intro and outro, so i can just plug in the new audio each week and it sounds consistent, imo its a total time saver.",
          "score": 2,
          "created_utc": "2026-02-23 16:16:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7432i8",
          "author": "FlowCritikal",
          "text": "Does this fork support RoCM?",
          "score": 2,
          "created_utc": "2026-02-24 10:24:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wlo7t",
          "author": "caetydid",
          "text": "Can I (ab)use these embeddings to create basic speaker identification e.g. to respond \"Ah darling, it is you... so good to hear you again...\"",
          "score": 4,
          "created_utc": "2026-02-23 05:30:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x0bfb",
              "author": "k_means_clusterfuck",
              "text": "I'd be surprised if it doesn't work. Please do give it a try! But it might be a good idea to use multiple embeddings for robustness",
              "score": 2,
              "created_utc": "2026-02-23 07:38:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y455u",
          "author": "Honest-Debate-6863",
          "text": "What is this useful for ",
          "score": 2,
          "created_utc": "2026-02-23 13:23:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x1xe4",
          "author": "lucasbennett_1",
          "text": "treating audio characteristics as purely mathematical vectors fundamentally changes how synthetic voices are directed.., instead of relying on prompt engineering or finding the perfect reference audio the process becomes basic interpolation between known cordinates.. the real value here is not just cloning but creating entirely new stable voices that do not exist in any training dataset. it basically turns voice acting into a manageable UI slider",
          "score": 2,
          "created_utc": "2026-02-23 07:54:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xxrnd",
              "author": "Gapeleon",
              "text": ">treating audio characteristics as purely mathematical vectors fundamentally changes how synthetic voices are directed.., instead of relying on prompt engineering or finding the perfect reference audio the process becomes basic interpolation between known cordinates..\n\nI agree, by doing this you can find vectors for a lot of characteristics, adjust them in real time, and lock in what you want eg:\n\nhttps://vocaroo.com/1RlGDrX5tXLm\n\nhttps://vocaroo.com/1fDLhTNxyJoR\n\nthen lock it in and it's more stable than \"reference audio\": https://vocaroo.com/1kSitey8098C\n\nI prefer this to voice cloning / fine tuning on an existing voice, but the problem is you end up tied to whatever model architecture you're using at the time.\n\n>that do not exist in any training dataset\n\nThe characteristics still need to exist in the training dataset though. You can't produce anything out of distribution with this technique.",
              "score": 4,
              "created_utc": "2026-02-23 12:42:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7owu2x",
          "author": "kun432",
          "text": "any evaluation result such as EER?",
          "score": 1,
          "created_utc": "2026-02-27 12:42:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xxn3u",
          "author": "AI_is_the_rake",
          "text": "I tried it against chatterbox last night and qwen voices were heavily generic. It didnâ€™t sound like the voice. Chatterbox did a much better job.Â ",
          "score": 1,
          "created_utc": "2026-02-23 12:41:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdfhfx",
      "title": "New Qwen3.5 models spotted on qwen chat",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/h1c3uk0iwflg1.png",
      "author": "AaronFeng47",
      "created_utc": "2026-02-24 12:55:10",
      "score": 651,
      "num_comments": 195,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rdfhfx/new_qwen35_models_spotted_on_qwen_chat/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o75aind",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-24 15:05:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74na4j",
          "author": "durden111111",
          "text": "Need that 122B model since GLM isnt releasing a mid sized MoE it seems",
          "score": 108,
          "created_utc": "2026-02-24 13:00:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74tc0p",
              "author": "phenotype001",
              "text": "And it's multimodal, so perfect. ",
              "score": 34,
              "created_utc": "2026-02-24 13:35:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o760d5t",
              "author": "po_stulate",
              "text": "It's here! [https://huggingface.co/Qwen/Qwen3.5-122B-A10B](https://huggingface.co/Qwen/Qwen3.5-122B-A10B)",
              "score": 19,
              "created_utc": "2026-02-24 17:02:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o765sle",
                  "author": "EmPips",
                  "text": "As a general purpose model it seems like they're trying to paint it being as good as the original Qwen3-235B (not the updated 2507 checkpoint) but twice as fast and half the memory.\n\nThe real gains are in instruction following and coding use.\n\nMeaning this *could* have the all-around strength that larger Qwen's have but the agentic abilities of GLM and Minimax models. All of this is subject to testing of course but I really hope these numbers turn out to reflect real-world results.",
                  "score": 17,
                  "created_utc": "2026-02-24 17:27:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76e0py",
                  "author": "UnknownLesson",
                  "text": "How much ram and VRAM do i need?",
                  "score": 3,
                  "created_utc": "2026-02-24 18:04:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76s0d9",
                  "author": "wh33t",
                  "text": "Siiiiick. This is what I've been waiting for, 100ishB, with a higher expert count. And multi-modal! Is the projector included?\n\nEdit: it is https://huggingface.co/unsloth/Qwen3.5-122B-A10B-GGUF/tree/main",
                  "score": 2,
                  "created_utc": "2026-02-24 19:06:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o762fbc",
              "author": "EmPips",
              "text": "Yes. I'm so ready to dethrone GLM 4.5 air and 4.6v as the top models my machine can run.",
              "score": 5,
              "created_utc": "2026-02-24 17:12:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76mswc",
                  "author": "lolwutdo",
                  "text": "Why not MiniMax 2.5? It's way better than either of those models.",
                  "score": 2,
                  "created_utc": "2026-02-24 18:43:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o758bnq",
              "author": "TopCryptographer8236",
              "text": "GLM do release mid sized MoE, GLM 4.6V? It's just kinda lack luster compared to 4.5Air for coding.",
              "score": 1,
              "created_utc": "2026-02-24 14:54:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75e21l",
                  "author": "coder543",
                  "text": "Theyâ€™ve had 4.7 and 5 since then. No new mid sized models.",
                  "score": 7,
                  "created_utc": "2026-02-24 15:21:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74nl7j",
          "author": "Freigus",
          "text": "dense 27B! 122B MoE! I'm glad they didn't abandon medium-sized dense models",
          "score": 208,
          "created_utc": "2026-02-24 13:02:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74qzct",
              "author": "MrHighVoltage",
              "text": "4bit quant would probably run on a 16GB GPU, that would be nice.....",
              "score": 38,
              "created_utc": "2026-02-24 13:22:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o74rduh",
              "author": "Sufficient-Rent6078",
              "text": "With 10B active parameters in the MoE, I'd expect the 27B dense model to not be that far behind in intelligence. Could be a really attractive choice for single gaming GPU setups.",
              "score": 22,
              "created_utc": "2026-02-24 13:24:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o74sut5",
                  "author": "TacGibs",
                  "text": "MoE are now way better than at their beginnings, the old rule \"square root of total parameters*active parameters\" to compare them to a dense model isn't relevant anymore.",
                  "score": 37,
                  "created_utc": "2026-02-24 13:33:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o74u2u3",
                  "author": "etherd0t",
                  "text": "Because Qwen3.5 is MoE: Only \\~17B parameters fire per token.\n\nThat means:\n\n* Latency scales closer to a 20B dense modelðŸ˜‰\n* Memory scales closer to a 400B sparse model",
                  "score": 6,
                  "created_utc": "2026-02-24 13:39:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o74qorn",
              "author": "PrefersAwkward",
              "text": "Hopefully we get a good draft model for that dense one. I'm guessing a qwen3 0.6 model wouldn't do it for 3.5 models",
              "score": 9,
              "created_utc": "2026-02-24 13:20:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o74u7xj",
                  "author": "Competitive_Ad_5515",
                  "text": "Is speculative decoding with draft models still a big deal? I don't see much discussion of it anymore",
                  "score": 12,
                  "created_utc": "2026-02-24 13:40:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o768ghw",
                  "author": "Street_Confidence453",
                  "text": "We have the MTP module which is trained with multi-step for all Qwen3.5 models!",
                  "score": 4,
                  "created_utc": "2026-02-24 17:39:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74qyrl",
          "author": "cibernox",
          "text": "27B dense? I didnâ€™t anticipate that one. \nIâ€™m very interested in what they will release in the 3B-12B range.",
          "score": 36,
          "created_utc": "2026-02-24 13:22:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74v6sx",
              "author": "Iory1998",
              "text": "An interesting choice of model size. It's like Gemma. But 27 will fit nicely in 24GB of Vram with a large context size.",
              "score": 16,
              "created_utc": "2026-02-24 13:46:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o74vld0",
                  "author": "cibernox",
                  "text": "My guess is that they purposely decided to release it as a way to demonstrate the improvements they made so they have their 27B dense model matching their 32B model, so they have a measurable \\~15% generational improvement. And they go for 27B (and not 26 or 28) to show they can match google too. I don't think it's random.",
                  "score": 19,
                  "created_utc": "2026-02-24 13:48:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o75nfwh",
              "author": "Adventurous-Paper566",
              "text": "C'est une faÃ§on de dire \"on se bouge Google!\"",
              "score": 0,
              "created_utc": "2026-02-24 16:05:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74owdr",
          "author": "CireHF103",
          "text": "Qwen Next and 3.5 so far has improved a lots compared to 3.0 from my experience. Very excited to see how it performs on smaller sizes models.",
          "score": 30,
          "created_utc": "2026-02-24 13:10:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74vmbu",
              "author": "Iory1998",
              "text": "The biggest improvement is the hybrid attention. Long context is the winning formula.",
              "score": 15,
              "created_utc": "2026-02-24 13:48:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o75amag",
              "author": "Firm_Meeting6350",
              "text": "So true - honestly, Iâ€˜m a Codex / Opus fanboy, but winner of my heart and â€ždrivers for automationâ€œ are Qwen 3 Next and Qwen3 Coder Next. Itâ€˜s really really impressive. I use it for code & context extraction and even Opus and Codex admit that Qwen is better ðŸ¤£",
              "score": 8,
              "created_utc": "2026-02-24 15:05:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76u61h",
                  "author": "cafedude",
                  "text": "Here's hoping for a Qwen3.5 Coder Next soon. So far Qwen3 Coder Next is the best model I've tried on my Strix Halo 128GB system. It's currently running and getting deep in the weeds with LLVM compiler optimizations and backend code generation - would've never imagined I'd be able to run a model locally that could handle that.",
                  "score": 3,
                  "created_utc": "2026-02-24 19:16:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74mzpx",
          "author": "Steus_au",
          "text": "35b would be a bomb",
          "score": 59,
          "created_utc": "2026-02-24 12:58:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74p8a4",
              "author": "DistanceSolar1449",
              "text": "27b dense model is more interesting for the people who can wait for slower tokens/sec. Should be smarter than 35b a3b",
              "score": 56,
              "created_utc": "2026-02-24 13:12:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o74untd",
                  "author": "-p-e-w-",
                  "text": "It will also directly compete with Gemma 3 27B.",
                  "score": 29,
                  "created_utc": "2026-02-24 13:43:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o75icke",
                  "author": "Responsible_Pain3278",
                  "text": "llama.cpp now supports any draft models, even if the tokenizer dictionary is incompatible. Furthermore, speculative decoding of ngrams without a draft model has been added. However, for a number of reasons, speculative decoding works poorly with moe, which is likely why it's so rarely used.",
                  "score": 3,
                  "created_utc": "2026-02-24 15:41:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o74ny6r",
              "author": "Dry_Yam_4597",
              "text": "Omfg.",
              "score": 6,
              "created_utc": "2026-02-24 13:04:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o74nczd",
              "author": "9r4n4y",
              "text": "100% and also glm 5 flash",
              "score": 6,
              "created_utc": "2026-02-24 13:00:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75pckr",
                  "author": "Silver-Champion-4846",
                  "text": "Glm 5 Flash? WHAT?",
                  "score": 3,
                  "created_utc": "2026-02-24 16:13:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o74v3ex",
              "author": "-p-e-w-",
              "text": "gpt-oss-20b killer. Even has fewer active parameters.",
              "score": 4,
              "created_utc": "2026-02-24 13:45:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75dx06",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-02-24 15:21:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7655ij",
                  "author": "Witty_Mycologist_995",
                  "text": "Wdym where",
                  "score": 1,
                  "created_utc": "2026-02-24 17:24:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o768bk6",
              "author": "Healthy-Nebula-3603",
              "text": "From the benchmarks they showed 35b moe is weaker than 27b dense",
              "score": 0,
              "created_utc": "2026-02-24 17:39:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74unmg",
          "author": "Halpaviitta",
          "text": "I've said it before and I'll say it again. Alibaba & Qwen is an extremely productive team, kudos to them. I am a bit worried about their workplace culture though, are they working 16 hour shifts or something? lol",
          "score": 16,
          "created_utc": "2026-02-24 13:43:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7603u8",
              "author": "AndreVallestero",
              "text": "996, though I feel like that's every AI lab right now. My friend on a Gemini adjacent team has been in \"code red\" for 6 months straight.",
              "score": 7,
              "created_utc": "2026-02-24 17:01:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74nuz3",
          "author": "jacek2023",
          "text": "awesome findings and very unexpected sizes!!! great news!!!",
          "score": 14,
          "created_utc": "2026-02-24 13:03:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74uyn9",
          "author": "Few_Painter_5588",
          "text": "A Dense 27B model! That is the perfect size! ",
          "score": 15,
          "created_utc": "2026-02-24 13:44:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74vlzi",
              "author": "BrightRestaurant5401",
              "text": "Indeed the only model I am looking forward too,  \nI want a model with stronger generalized intelligence,  \nthe extra \"memory\" most MOE models offer is just not that interesting.",
              "score": 5,
              "created_utc": "2026-02-24 13:48:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74o4pg",
          "author": "rerri",
          "text": "Awesome sizes, cant wait to try them!",
          "score": 11,
          "created_utc": "2026-02-24 13:05:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74mpjr",
          "author": "Paramecium_caudatum_",
          "text": "Finally! Can't wait to check them out.",
          "score": 20,
          "created_utc": "2026-02-24 12:56:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74ng0q",
          "author": "giatai466",
          "text": "122B would be nice for my 16 GB VRAM + 64 GB RAM.",
          "score": 22,
          "created_utc": "2026-02-24 13:01:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74sxse",
              "author": "luncheroo",
              "text": "A tight fit in Q4 though, no?",
              "score": 8,
              "created_utc": "2026-02-24 13:33:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7507j7",
                  "author": "boissez",
                  "text": "Seems more like a perfect model for the strix halo/DGX gang.",
                  "score": 20,
                  "created_utc": "2026-02-24 14:12:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o752x7j",
                  "author": "simracerman",
                  "text": "OSS-120B-MXFP4 fits with 64k context just fine.",
                  "score": 6,
                  "created_utc": "2026-02-24 14:27:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o761ipp",
                  "author": "ThrowawayNotSusLol",
                  "text": "You guys are weird. \n\nI got 12VRam and 32GB of ram and I was running their qwen 3 235b a22b model on q4\n\nSure, it was slow, but this new 100b param moe should only be easier. \n\nDon't know why you act like it's so impossible to run these without massive PCs",
                  "score": 0,
                  "created_utc": "2026-02-24 17:08:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o75bhnh",
              "author": "FullstackSensei",
              "text": "Just got a few 64GB Jetson AGX Xaviers with carrier boards. Thinking of pairing one with a 12GB A2000. Would be epictastic with 122B at Q4",
              "score": 2,
              "created_utc": "2026-02-24 15:09:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o74w4l7",
              "author": "Iory1998",
              "text": "I don't think it would fit unless you use Q2 or Q3. Also, you should keep in mind that A10B is way larger than your typical A3B, so expect very slow generation speed.\n\nThe model will be smart though.",
              "score": 4,
              "created_utc": "2026-02-24 13:51:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o752yrg",
                  "author": "simracerman",
                  "text": "OSS-120B-MXFP4 fits with 64k context just fine.",
                  "score": 6,
                  "created_utc": "2026-02-24 14:27:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74ogko",
          "author": "Pedalnomica",
          "text": "I'm hoping they release their own quants again. It always seemed like they did some sort of QAT because their quants were really strong.",
          "score": 7,
          "created_utc": "2026-02-24 13:07:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o754fkn",
          "author": "PixelatedCaffeine",
          "text": "https://preview.redd.it/qh7qke8ceglg1.png?width=626&format=png&auto=webp&s=953aa76b60d0d396bb1ee6dce6d5593b44198904\n\nit's happening!",
          "score": 7,
          "created_utc": "2026-02-24 14:34:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75tf49",
              "author": "Alarmed-Channel2145",
              "text": "Also [https://github.com/QwenLM/Qwen3.5/pull/25](https://github.com/QwenLM/Qwen3.5/pull/25) is already merged",
              "score": 2,
              "created_utc": "2026-02-24 16:31:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7572yz",
          "author": "xrvz",
          "text": "Finally another ~120B model for Strix Halo.",
          "score": 7,
          "created_utc": "2026-02-24 14:48:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74ocy3",
          "author": "Hanthunius",
          "text": "I want to try out all of these! Great sizes for many of us.",
          "score": 5,
          "created_utc": "2026-02-24 13:07:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75ijmg",
          "author": "LosEagle",
          "text": "Stupid question, but if the 27b is multimodal, does it mean that it is less efficient for common text chat because some of those parameters are for vision?",
          "score": 6,
          "created_utc": "2026-02-24 15:42:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77uzed",
              "author": "Evolution31415",
              "text": "Vision encoder is a separate part out of reasoning/thinking abilities and you can disable it to free some memory for KV cache in the text-only mode.",
              "score": 6,
              "created_utc": "2026-02-24 22:06:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74rxfm",
          "author": "noctrex",
          "text": "Do we have any information if they will release any small versions of 3.5?\n\nLike they did with 3 and 3-VL?\n\n2B, 4B, 8B.\n\nCause those small ones are nice to put on computers that do not have a dGPU",
          "score": 10,
          "created_utc": "2026-02-24 13:27:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74tlq2",
              "author": "Schlick7",
              "text": "9B and that 35bA3 were the ones in the llama.cpp PR so we can expect the 9B. I'd be pretty surprised if that was the smallestÂ ",
              "score": 15,
              "created_utc": "2026-02-24 13:37:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o74uenn",
                  "author": "Iory1998",
                  "text": "I am pretty certain that there would be a 4B model as well. Qwen3-4B is so popular and used in many image generator that I think qwen will release another one.",
                  "score": 9,
                  "created_utc": "2026-02-24 13:41:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74n0xw",
          "author": "Technical-Earth-3254",
          "text": "Wish it was 100B so q4 fits in my poor 64GB system. But looks perfect for the 128gb faction",
          "score": 21,
          "created_utc": "2026-02-24 12:58:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74nec0",
              "author": "BigYoSpeck",
              "text": "If you have a GPU add it's VRAM to your system RAM for what you can run\n\n\nGpt-oss-120b will run on 64gb RAM + 16gb VRAM quite well so I have high hopes for this",
              "score": 16,
              "created_utc": "2026-02-24 13:01:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o74pasy",
                  "author": "wisepal_app",
                  "text": "i have 96 GB ddr5 ram and 16 GB vram but i get 14 t/s with gpt-oss 120b. By quite well do you mean this kind of speeds or much higher? i use llama.cpp with 60k context.",
                  "score": 12,
                  "created_utc": "2026-02-24 13:12:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o74t4zr",
                  "author": "luncheroo",
                  "text": "You guys must be on Linux. I've found it runs well but context is low and you can't do much else on the OS at the same time.",
                  "score": 6,
                  "created_utc": "2026-02-24 13:34:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o74qe2i",
                  "author": "Technical-Earth-3254",
                  "text": "You are very correct, if just my windows didn't take a up that much ram when I'm trying to do something productive.",
                  "score": 2,
                  "created_utc": "2026-02-24 13:19:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o74zh4d",
                  "author": "Far-Low-4705",
                  "text": "i have 64Gb VRAM and only 16Gb of ddr3 RAM with an ancient 4 core CPU lol..\n\nI was really hoping for an 80b, bc any CPU offload is pretty bad for me\n\n(I built my system for under $100 total)",
                  "score": 0,
                  "created_utc": "2026-02-24 14:09:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o74oqwe",
              "author": "pmttyji",
              "text": "Well, you have options(quants) from IQ4\\_XS to Q4\\_K\\_XL",
              "score": 5,
              "created_utc": "2026-02-24 13:09:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o74q98u",
              "author": "Significant_Fig_7581",
              "text": "The Q3 quants are too good nowadays dw :)",
              "score": 4,
              "created_utc": "2026-02-24 13:18:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o74qghz",
              "author": "LagOps91",
              "text": "it's perfectly fine for a 64gb system as long as you have some vram (which you should)",
              "score": 2,
              "created_utc": "2026-02-24 13:19:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74ngvi",
          "author": "LoveMind_AI",
          "text": "Oh hell yes",
          "score": 5,
          "created_utc": "2026-02-24 13:01:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74ppty",
          "author": "Significant_Fig_7581",
          "text": "Can't wait it's coming OMG!!!!",
          "score": 5,
          "created_utc": "2026-02-24 13:15:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75eos4",
          "author": "Fox-Lopsided",
          "text": "Where 9b :(",
          "score": 5,
          "created_utc": "2026-02-24 15:24:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74nf5e",
          "author": "Leflakk",
          "text": "Interesting!",
          "score": 4,
          "created_utc": "2026-02-24 13:01:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74nsgc",
          "author": "sterby92",
          "text": "huge if true!",
          "score": 3,
          "created_utc": "2026-02-24 13:03:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74o67z",
          "author": "pmttyji",
          "text": "Just wow!",
          "score": 4,
          "created_utc": "2026-02-24 13:05:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74qqxs",
          "author": "Insomniac24x7",
          "text": "My 3090 is gonna cry",
          "score": 4,
          "created_utc": "2026-02-24 13:21:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7512ne",
              "author": "jax_cooper",
              "text": "tears of joy",
              "score": 5,
              "created_utc": "2026-02-24 14:17:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75166l",
                  "author": "Insomniac24x7",
                  "text": "Lol",
                  "score": 3,
                  "created_utc": "2026-02-24 14:18:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74t87j",
          "author": "Loskas2025",
          "text": "Finally",
          "score": 3,
          "created_utc": "2026-02-24 13:35:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74vony",
          "author": "Big_Mix_4044",
          "text": "Hyped af",
          "score": 4,
          "created_utc": "2026-02-24 13:48:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o754g1d",
          "author": "pigeon57434",
          "text": "do you think the 27B dense model will be smarter than the 35B-A3B model?",
          "score": 4,
          "created_utc": "2026-02-24 14:35:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75cii1",
              "author": "PANIC_EXCEPTION",
              "text": "Doubtful, Qwen3-32B wasn't too much further ahead compared to Qwen3-30B-A3B",
              "score": -3,
              "created_utc": "2026-02-24 15:14:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75a4y2",
          "author": "ApprehensiveAd3629",
          "text": "i hope to get a qwen 3.5 14b ðŸ™",
          "score": 4,
          "created_utc": "2026-02-24 15:03:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75yn5x",
          "author": "mossy_troll_84",
          "text": "https://preview.redd.it/tmv0y54a3hlg1.png?width=1734&format=png&auto=webp&s=6320e0e924c946c3ccd29e1234b919e3e0f129fb\n\nand unsloth GGUF quantizations are uploading now...",
          "score": 3,
          "created_utc": "2026-02-24 16:55:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75zpt5",
          "author": "phenotype001",
          "text": "Models are on HF: [https://huggingface.co/Qwen/Qwen3.5-122B-A10B](https://huggingface.co/Qwen/Qwen3.5-122B-A10B)",
          "score": 5,
          "created_utc": "2026-02-24 16:59:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74s9r7",
          "author": "Deep_Traffic_7873",
          "text": "A3B for me, thanks",
          "score": 3,
          "created_utc": "2026-02-24 13:29:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74um86",
          "author": "asraniel",
          "text": "i would love something good in the 8-14 range. it seems like a sweet spot for many tasks such as information extraction, summaries etc",
          "score": 3,
          "created_utc": "2026-02-24 13:42:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75bzkj",
          "author": "mossy_troll_84",
          "text": "Qwen3.5-122B-A10B ðŸ¤©\n\nhttps://preview.redd.it/o3inz6xxkglg1.png?width=1342&format=png&auto=webp&s=8398cbddc76ac0c5d49b20fcd0ec608d35bb50e6\n\n",
          "score": 3,
          "created_utc": "2026-02-24 15:12:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75g9fe",
              "author": "tarruda",
              "text": "Amazing, this might be the sweet spot for 128G systems",
              "score": 5,
              "created_utc": "2026-02-24 15:32:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75nff4",
                  "author": "mossy_troll_84",
                  "text": "yup, that is what I am waiting for! ðŸ˜ Although till now I am using Step-3.5 Flash and I am quite happy (for non professional work)",
                  "score": 2,
                  "created_utc": "2026-02-24 16:05:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75gf6x",
          "author": "Dyssun",
          "text": "please be released today please be released today ðŸ™",
          "score": 3,
          "created_utc": "2026-02-24 15:33:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o763je4",
          "author": "RandumbRedditor1000",
          "text": "27B DENSE LET'S GOOO",
          "score": 3,
          "created_utc": "2026-02-24 17:17:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o763oon",
          "author": "Psyko38",
          "text": "Can't wait to see the performance of the 27b and 35b on my Rx 9060xt 16gb",
          "score": 3,
          "created_utc": "2026-02-24 17:17:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o767wdc",
          "author": "EmPips",
          "text": "> 122B A10B\n\nM-Series Mac and Strix-Halo owners are going to have a good day.",
          "score": 3,
          "created_utc": "2026-02-24 17:37:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76cldk",
          "author": "lemon07r",
          "text": "Finally a small dense model! I can think about finetuning stuff again without moe woes",
          "score": 3,
          "created_utc": "2026-02-24 17:58:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76gt4x",
          "author": "zhambe",
          "text": "Oh this is exciting! Qwen3-30B-Coder-FP8 has been my daily driver, I *think* I could squeeze the 35B version in...",
          "score": 3,
          "created_utc": "2026-02-24 18:16:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76ra1b",
          "author": "Unhappy_Advantage_66",
          "text": "Hey just curious will the 27B or the 35B work on L4 GPU.",
          "score": 3,
          "created_utc": "2026-02-24 19:03:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o773aak",
          "author": "Imakerocketengine",
          "text": "oh wow, need some bench to compare the 122B variant to qwen next :) ",
          "score": 3,
          "created_utc": "2026-02-24 19:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77g13g",
          "author": "Physical_Screen_7543",
          "text": "ngl this 27B is actually cracked. Coding and multimodal perfs are giving me early Gemini 3 Pro vibes lol. Perfect weight for anyone building a local agentic OS. Can't believe we're getting this much juice in a dense model in 2026. My 5090 is ready.",
          "score": 3,
          "created_utc": "2026-02-24 20:57:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74nb0s",
          "author": "9r4n4y",
          "text": "Ayooooooo lets gooo",
          "score": 4,
          "created_utc": "2026-02-24 13:00:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74tfbq",
          "author": "No_Doc_Here",
          "text": "Did they make any anouncements whether they will release these models as OS?",
          "score": 2,
          "created_utc": "2026-02-24 13:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74uuc4",
          "author": "HugoCortell",
          "text": "Well yeah, I imagine you'd find them there.",
          "score": 2,
          "created_utc": "2026-02-24 13:44:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74x2p6",
          "author": "maglat",
          "text": "\nwith multimodel tasks they mean picture and video understanding, right? Same as the big variant I guess",
          "score": 2,
          "created_utc": "2026-02-24 13:56:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74zs8n",
              "author": "Skyline34rGt",
              "text": "Yes. Same as Qwen3 VL have before.",
              "score": 3,
              "created_utc": "2026-02-24 14:10:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7538au",
          "author": "tmvr",
          "text": "Nice, some exciting sizes that make sense as well for a lot of home setups. I'll have to go and try Q3 with the 122B, but the 27B dense and the 35B MoE are a nice fit for the 24GB and 32GB VRAM configs.",
          "score": 2,
          "created_utc": "2026-02-24 14:28:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75intb",
          "author": "ridablellama",
          "text": "this is why qwen is my favorite",
          "score": 2,
          "created_utc": "2026-02-24 15:43:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75jao4",
          "author": "Adventurous-Paper566",
          "text": "27B!\n\n  \nJe suis tellement content!!",
          "score": 2,
          "created_utc": "2026-02-24 15:46:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75lt0p",
          "author": "Zestyclose839",
          "text": "For anyone who already played with the 27B and 35B models, what are the initial impressions? Any personality changes over 30B A3B?",
          "score": 2,
          "created_utc": "2026-02-24 15:57:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75n8f8",
          "author": "MerePotato",
          "text": "27B huh, interesting",
          "score": 2,
          "created_utc": "2026-02-24 16:04:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75oqkp",
          "author": "russianguy",
          "text": "GGUF when?",
          "score": 2,
          "created_utc": "2026-02-24 16:10:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75qlw7",
          "author": "No_Mango7658",
          "text": "122 moe should be so great on strix halo with 35b for speculative decoding!",
          "score": 2,
          "created_utc": "2026-02-24 16:19:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75uzl1",
          "author": "RegularRecipe6175",
          "text": "Qwen GGUF?",
          "score": 2,
          "created_utc": "2026-02-24 16:38:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75we5k",
          "author": "mossy_troll_84",
          "text": "https://preview.redd.it/ak3fbjmk1hlg1.png?width=1704&format=png&auto=webp&s=ab6f569ed32ff7ea4f3acb78e238ef55eaeb7d74\n\n",
          "score": 2,
          "created_utc": "2026-02-24 16:45:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75y0xd",
          "author": "Jayfree138",
          "text": "Oh look, my new daily driver. Can't wait",
          "score": 2,
          "created_utc": "2026-02-24 16:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76120u",
          "author": "GrungeWerX",
          "text": "I actually want all three. :)",
          "score": 2,
          "created_utc": "2026-02-24 17:06:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o762bmf",
          "author": "edeltoaster",
          "text": "Anything on benchmarks yet?",
          "score": 2,
          "created_utc": "2026-02-24 17:11:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76c3vs",
          "author": "BasicInteraction1178",
          "text": "Has anyone tried using Qwen3.5 for coding yet? What are your feelings?",
          "score": 2,
          "created_utc": "2026-02-24 17:56:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76mbho",
          "author": "Niket01",
          "text": "The 27B dense model is the one I'm most excited about. Dense models tend to be more predictable for fine-tuning and deployment compared to MoE, and 27B sits in that sweet spot where you can actually run it on consumer hardware with decent quantization.\n\n\n\nThe MoE variants are interesting for benchmarks but the 27B is probably going to see the most real-world local deployment. Anyone tested the multimodal capabilities yet? Curious how it handles vision tasks compared to the Qwen3 VL models.",
          "score": 2,
          "created_utc": "2026-02-24 18:41:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74r95y",
          "author": "No_Swimming6548",
          "text": "Not bad\n\nhttps://preview.redd.it/jbabj7gp1glg1.jpeg?width=1358&format=pjpg&auto=webp&s=ffdc1678bde44b7c5a14c185dc69f0be6e7e09e9",
          "score": 6,
          "created_utc": "2026-02-24 13:24:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74tug2",
          "author": "Loskas2025",
          "text": "https://preview.redd.it/uasrx10b4glg1.png?width=880&format=png&auto=webp&s=8ea73c7c39a124b4b1e29168120d93580699e9e3\n\nspicy",
          "score": 3,
          "created_utc": "2026-02-24 13:38:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74xvd7",
          "author": "Far-Low-4705",
          "text": "nnoooooo i cant run 122B....... im so sad......\n\ni wanted an 80b sooo bad, perfect for 64Gb",
          "score": 2,
          "created_utc": "2026-02-24 14:00:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75ydt1",
              "author": "tarruda",
              "text": "I wouldn't dismiss 2-bit quants of the 122B release which should be runnable in less than 50G.\n\n\nThis new Qwen architecture is very resilient to quantization, I have been running 2-bit 397B on 128G mac with great success: https://huggingface.co/ubergarm/Qwen3.5-397B-A17B-GGUF/discussions/2",
              "score": 5,
              "created_utc": "2026-02-24 16:54:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7fq80a",
                  "author": "Far-Low-4705",
                  "text": "I can run unsloth's UD-Q3\\_K\\_XL quant at full context with full GPU offload, which on its own is impressive, but im always skeptical of anything less than 4 bit",
                  "score": 1,
                  "created_utc": "2026-02-26 01:31:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7789mb",
          "author": "indicava",
          "text": "I have to say Iâ€™m kind of disappointed with this release. \n\nIt might be a niche use case, but for us fine tuners, only a single size dense model with no base variants is practically useless.\n\nThis trend already started with Qwen3 where they never released the base variant of the 32B size and all releases since then have been MoE.\n\nWhile running local models for coding or creative writing has a significant value proposition, the ability to fine tune models for personal use or as a basis for a commercial product is a liberty thatâ€™s slowly been eroding away. Thatâ€™s a shame, and I donâ€™t think itâ€™s being brought up enough.",
          "score": 1,
          "created_utc": "2026-02-24 20:21:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75blkl",
          "author": "Alarming-Ad8154",
          "text": "I donâ€™t think we had a dense model in a while right? Very curious to see how 2026 agentic-coder/Reinforcement learning does on a dense base modelâ€¦ if this is mixed linear/quadratic attention and someone converts to nvpf4 could be an absolute 5080/5070ti monsterâ€¦.",
          "score": 1,
          "created_utc": "2026-02-24 15:10:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7613jh",
          "author": "Ok-Scarcity-7875",
          "text": "27B-A3B would be better if you want to have a large context and speed on a 24GB GPU.",
          "score": 1,
          "created_utc": "2026-02-24 17:06:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76gime",
              "author": "Zugzwang_CYOA",
              "text": "On my 4090, Gemma-3 27b already runs at blazing fast speeds, even at Q5 quants, with 16k context. I don't see why that would be different from the new dense 27b.",
              "score": 2,
              "created_utc": "2026-02-24 18:15:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74z4zx",
          "author": "Green-Ad-3964",
          "text": "I hope the 35B fits a 5090...",
          "score": 0,
          "created_utc": "2026-02-24 14:07:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o759cen",
              "author": "RMK137",
              "text": "It should if you use Q3/Q4 especially with the unsloth dynamic quants. I've used the Nemotron-30b-A3B at UD-Q4_K_XL on my 5090. This one is a little larger but you can quantize the KV cache also which buys you more context.",
              "score": 3,
              "created_utc": "2026-02-24 14:59:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o75kvmp",
                  "author": "dlcsharp",
                  "text": "These models use hybrid attention, memory usage for kv cache should be dramatically lower compared to Qwen 3.0, just like Qwen 3 Next",
                  "score": 3,
                  "created_utc": "2026-02-24 15:53:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o75mvpu",
              "author": "DeepRecipe6331",
              "text": "Q4 GLM-4.7-Flash can, there's no reason why the 35B can't. You should have plenty of room to spare, and depending on how much you want to use and context sizes you can definitely bump it up to Q6.",
              "score": 3,
              "created_utc": "2026-02-24 16:02:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdlc02",
      "title": "Qwen/Qwen3.5-122B-A10B Â· Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/Qwen/Qwen3.5-122B-A10B",
      "author": "coder543",
      "created_utc": "2026-02-24 16:44:13",
      "score": 601,
      "num_comments": 128,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rdlc02/qwenqwen35122ba10b_hugging_face/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o77ir5n",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-24 21:10:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7623yd",
          "author": "djm07231",
          "text": "Seems like a gpt-oss-120b competitor but doesnâ€™t seem to have native 4 bit weights unfortunately.\n\nI personally serve models over vLLM and natively quantized gpt-oss-120b have been very good for my purposes.\n\nI wish labs would start offering natively quantized models. Perhaps due to blockade of Blackwell Chinese labs cannot train on MXFP4/NVFP4 it seems.\n",
          "score": 71,
          "created_utc": "2026-02-24 17:10:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76cv4d",
              "author": "tarruda",
              "text": "The qwen-next architecture (used in all 3.5 models and qwen3-coder-next) is very resilient to quantization. Been using 397b iq2_xs and it is pretty darn good and difficult to notice quality degradation when compared to the one served by qwen chat.\n\nIt is possible that unsloth 4-bit quants will be indistinguishable from bf16.",
              "score": 48,
              "created_utc": "2026-02-24 17:59:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77mwii",
                  "author": "wektor420",
                  "text": "That would be very cool, also what might be the cause of this improved stability?",
                  "score": 8,
                  "created_utc": "2026-02-24 21:29:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o788ahv",
                  "author": "VoidAlchemy",
                  "text": "Heya tarruda, thanks for all your quant testing recently!\n\nFor mainline users especially mac/strix halo I recommend [https://huggingface.co/AesSedai/Qwen3.5-122B-A10B-GGUF](https://huggingface.co/AesSedai/Qwen3.5-122B-A10B-GGUF) as u/Digger412 uses similar MoE optimized custom recipes as do I and also provides both perplexity and KLD!",
                  "score": 8,
                  "created_utc": "2026-02-24 23:13:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o76c6et",
              "author": "zodagma",
              "text": "What hardware are you serving gpt 120b on? What kind of speed and throughput can we expect?",
              "score": 7,
              "created_utc": "2026-02-24 17:56:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76qa69",
                  "author": "my_name_isnt_clever",
                  "text": "It's still my go-to on my Strix Halo with 128GB. That model is around 60GB when loaded into RAM and I get 50-45 tok/s depending on context. I'm excited to have another model to compete, but it will be slower since it's 10b active is almost double gpt-oss-120b's 5b.",
                  "score": 11,
                  "created_utc": "2026-02-24 18:58:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78rsij",
              "author": "lenjet",
              "text": "us too, we are using vLLM on DGX spark and need that MXFP4 in non GGUF - \\*sigh\\*",
              "score": 3,
              "created_utc": "2026-02-25 00:59:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o764wdh",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -24,
              "created_utc": "2026-02-24 17:23:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7674ja",
                  "author": "coder543",
                  "text": "Most people are not using either MXFP4 or NVFP4, so calling it \"DOA\" without that is a wild claim.",
                  "score": 21,
                  "created_utc": "2026-02-24 17:33:44",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75wc9f",
          "author": "TechNerd10191",
          "text": "Now we wait for the GGUF weights",
          "score": 94,
          "created_utc": "2026-02-24 16:44:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75xbqe",
              "author": "coder543",
              "text": "unsloth posted them here: https://huggingface.co/collections/unsloth/qwen35\n\nbut, still uploading, I guess",
              "score": 102,
              "created_utc": "2026-02-24 16:49:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75ykel",
                  "author": "danielhanchen",
                  "text": "Yes! Still converting and uploading!",
                  "score": 98,
                  "created_utc": "2026-02-24 16:54:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o75z5lo",
                  "author": "throwawayacc201711",
                  "text": "Any ideas how many gigs itâ€™s gonna be?",
                  "score": 6,
                  "created_utc": "2026-02-24 16:57:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o775pp4",
                  "author": "Mayion",
                  "text": "How come most of the benchmarks presented show the 27B exceeding the 35B? Is there a particular reason as to why it does better in tests even though it is supposedly more condensed",
                  "score": 7,
                  "created_utc": "2026-02-24 20:09:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o786gs0",
              "author": "ubrtnk",
              "text": "Ooh good. Glad it was your turn for obligatory \"GGUF WHEN!?!\" comment. I'll get the next one",
              "score": 4,
              "created_utc": "2026-02-24 23:04:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75y5hv",
          "author": "durden111111",
          "text": "25.3 on HLE which was SOTA about 6 months ago but now local in 122B",
          "score": 62,
          "created_utc": "2026-02-24 16:53:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o767atn",
              "author": "oxygen_addiction",
              "text": "With how bad that benchmark turned out to be, it's irrelevant.",
              "score": 50,
              "created_utc": "2026-02-24 17:34:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o775l5n",
                  "author": "hak8or",
                  "text": "For those of us out of the loop, are you referring to this?\n\nhttps://www.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious\n\nIf so, wow what a shame. I was excited about that benchmark because it's one that current models are \"bad\" at and seemingly didn't plateau.",
                  "score": 13,
                  "created_utc": "2026-02-24 20:08:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o764ubg",
              "author": "Thrumpwart",
              "text": "We are living in the future.",
              "score": 4,
              "created_utc": "2026-02-24 17:23:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o77jujz",
          "author": "jinnyjuice",
          "text": "Can't wait for NVFP4!",
          "score": 14,
          "created_utc": "2026-02-24 21:15:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7986x8",
              "author": "CBHawk",
              "text": "Is that better than GGUF?",
              "score": 3,
              "created_utc": "2026-02-25 02:33:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7bpz7a",
                  "author": "TotallyToxicToast",
                  "text": "If you have a Blackwell (50 series) graphics card it can natively compute NVFP4 (as well as MXFP4).\n\nSo it will run faster than GGUF while being roughly same quality in my experience.\n\nIf you don't have a Blackwell Graphics card NVFP4 is uselesss.",
                  "score": 3,
                  "created_utc": "2026-02-25 13:48:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76acn2",
          "author": "4baobao",
          "text": "9B next pls ðŸ™ðŸ»",
          "score": 13,
          "created_utc": "2026-02-24 17:48:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77v7p5",
          "author": "zipzapbloop",
          "text": "just starting to test now. rtx pro 6000. lm studio. windows. 12k token test prompt on a philosophical topic i'm competent on.\n\n10s time to first token\n\n50 tokens/s generation\n\nconsumed 80gb vram\n\ni preferred its response on the topic to gpt-oss-120b.\n\nlooking good so far.\n\nedit: after a system restart i'm getting 80-84 t/s on the same prompt and ttfs is 6-7s. ðŸ¤·â€â™‚ï¸. also just to be clear qwen3.5-122b-a10b Q4\\_K\\_M (75.1GB)",
          "score": 12,
          "created_utc": "2026-02-24 22:07:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77zun9",
              "author": "NoahFect",
              "text": "Same here, this model appears to be smart as hell.",
              "score": 5,
              "created_utc": "2026-02-24 22:30:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o78phte",
              "author": "DieselKraken",
              "text": "How to you run this large model on an rtx pro 6000?",
              "score": 0,
              "created_utc": "2026-02-25 00:47:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78pyi1",
                  "author": "zipzapbloop",
                  "text": "Quant. Im testing q4_k_m",
                  "score": 3,
                  "created_utc": "2026-02-25 00:49:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7818c2",
          "author": "NoahFect",
          "text": "Unsloth's 122B-A10B-UD-Q4_K_XL passed both the car wash and upside-down cup tests with flying colors.  It's the only local model I've seen do that.  94 t/s on RTX 6000 Blackwell.",
          "score": 24,
          "created_utc": "2026-02-24 22:37:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78p4fj",
              "author": "SufficientPie",
              "text": "qwen/qwen3.5-397b-a17b is the first open-weights model to pass all my personal benchmark trick questions, too.  is there anywhere online I can try 122B-A10B-UD-Q4_K_XL?",
              "score": 6,
              "created_utc": "2026-02-25 00:45:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78tmeg",
                  "author": "NoahFect",
                  "text": "I don't believe so, unless Unsloth themselves are hosting it somewhere.  PM me a couple of questions if desired and I'll run them here.\n\nWish I had enough 6000s to run the full monty 397B version at home...",
                  "score": 6,
                  "created_utc": "2026-02-25 01:10:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7gr6bn",
                  "author": "SufficientPie",
                  "text": "Now that it's on OpenRouter:\n\n* qwen/qwen3.5-122b-a10b\n* qwen/qwen3.5-27b\n* qwen/qwen3.5-flash-02-23\n* qwen/qwen3.5-35b-a3b\n\nall of them get 5 out of 6 questions right.  the best small models I've seen.",
                  "score": 1,
                  "created_utc": "2026-02-26 05:18:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78ns5l",
              "author": "CentralLimit",
              "text": "So does the 27B variant.\n\nEDIT: tested the 35B-A3B variant, it failed the car wash scenario pretty badly.\n\nEDIT vol II: turns out the mxfp4 quant by unsloth has some issues and significantly dumbs down the model, their Q8_K_XL quant works as expected and passes this (and other tests mxfp4 would fail).",
              "score": 2,
              "created_utc": "2026-02-25 00:38:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o78h5z1",
              "author": "Spara-Extreme",
              "text": "What are those tests? First time Iâ€™ve read about them!",
              "score": 1,
              "created_utc": "2026-02-25 00:02:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78ibpz",
                  "author": "NoahFect",
                  "text": "There are variations but the prompts I've been using are:\n\n    I want to wash my car.  The car wash is only 50 meters from my home.  Do you think I should walk there, or drive there?\n\nand\n\n    There is a metal cup with a sealed top and no bottom. Is it possible to use it for drinking?\n\nOnly the top-end models get these right on a regular basis, as most lack a decent internal world-model concept (also discussed [here](https://news.ycombinator.com/item?id=47128138)).  122B-A10B-UD-Q4_K_XL handled them both perfectly, but I've been seeing a lot of looping behavior with other prompts.  Still tinkering with it.\n\nEdit: it also aces another trick question that almost no second-tier models handle correctly:\n\n    What should be the punishment for looking at your opponent's board in chess?\n\nGetting all three of these right is unprecedented for any model I can actually run at home.",
                  "score": 6,
                  "created_utc": "2026-02-25 00:09:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7a7f6o",
              "author": "plopperzzz",
              "text": "Is nobody seeing the model start to repeat tokens or spit out garbage on longer context replies? Even using Q8 with the suggested sampling parameters, it spits out garbage like, \"If $C$ is tangent to$ to$ to$ to$ to$ to$ to$ to$ to a segment\", or \"...,.,,,\", and struggles with latex.",
              "score": 1,
              "created_utc": "2026-02-25 06:27:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7a988i",
                  "author": "NoahFect",
                  "text": "I haven't seen it do that in particular, but I have seen it waste the whole 256K context arguing with itself in a loop.  It seems very sensitive to its parameters, at least when running llama-server (which I am).\n\nIn fact, when it fails to answer a question that I ask, that seems to be how it usually happens, rather than by making something up or returning a slightly-wrong response.  When it works, it tends to work amazingly well, but it doesn't always work.",
                  "score": 2,
                  "created_utc": "2026-02-25 06:42:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77tlcb",
          "author": "ExistingAd2066",
          "text": "AMD Ryzen 395\n\nllama-bench -m \\~/.cache/llama.cpp/unsloth\\_Qwen3.5-122B-A10B-GGUF\\_UD-Q4\\_K\\_XL\\_Qwen3.5-122B-A10B-UD-Q4\\_K\\_XL-00001-of-00003.gguf --mmap 0 -fa 1 -d 0,32748\n\n| model                           |      size |   params | backend | ngl |  fa |           test |           t/s |\n\n| ------------------------------- | --------: | -------: | ------- | --: | --: | -------------: | ------------: |\n\n| qwen35moe 80B.A3B Q4\\_K - Medium | 63.65 GiB | 122.11 B | ROCm    |  99 |   1 |          pp512 | 327.15 Â± 1.40 |\n\n| qwen35moe 80B.A3B Q4\\_K - Medium | 63.65 GiB | 122.11 B | ROCm    |  99 |   1 |          tg128 |  22.79 Â± 0.05 |\n\n| qwen35moe 80B.A3B Q4\\_K - Medium | 63.65 GiB | 122.11 B | ROCm    |  99 |   1 | pp512 @ d32748 | 204.18 Â± 0.86 |\n\n| qwen35moe 80B.A3B Q4\\_K - Medium | 63.65 GiB | 122.11 B | ROCm    |  99 |   1 | tg128 @ d32748 |  20.75 Â± 0.44 |",
          "score": 10,
          "created_utc": "2026-02-24 22:00:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7aswd9",
              "author": "Nextil",
              "text": "My tg/s is about 4 less than yours. What OS, ROCm version, kernel parameters, etc. are you using?",
              "score": 2,
              "created_utc": "2026-02-25 09:43:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7atfy5",
                  "author": "ExistingAd2066",
                  "text": "Ubuntu 26.04  \nlinux-image-6.18.0-8-generic  \nlinux-firmware 20251029  \nkyuz0/amd-strix-halo-toolboxes:rocm-6.4.4 (7058da038de7/2026-02-24 12:13:32 +0300)",
                  "score": 3,
                  "created_utc": "2026-02-25 09:48:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o781y6o",
              "author": "spaceman3000",
              "text": "Ram bandwidth is too small for such big models :/",
              "score": 0,
              "created_utc": "2026-02-24 22:41:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78aubt",
                  "author": "schnauzergambit",
                  "text": "Depends on expectations!",
                  "score": 4,
                  "created_utc": "2026-02-24 23:27:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7894t7",
          "author": "ravage382",
          "text": "I am a huge fan of gpt120b. It has been my daily driver for what seems forever now. I think this is replacing it.\n\nI just did a few rounds of back and forth on a tetris clone and there was none of the boot licking sycophantic behavior I've come to expect from new models. Edit: The tetris clone is pretty top notch. The only other model that made one this nice was stepfun 3.5.",
          "score": 10,
          "created_utc": "2026-02-24 23:18:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76igb7",
          "author": "ciprianveg",
          "text": "It looks very close to Qwen3.5 397B I would expect a bigger difference:) Probably 397B has room for future improvements",
          "score": 8,
          "created_utc": "2026-02-24 18:24:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75xxj6",
          "author": "jacek2023",
          "text": "my post is already deleted, so I am writing here, I will be downloading ggufs from unsloth, hope to test them soon, starting from 122B if possible",
          "score": 25,
          "created_utc": "2026-02-24 16:52:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75yltn",
              "author": "danielhanchen",
              "text": "Converting as we speak! :)",
              "score": 36,
              "created_utc": "2026-02-24 16:55:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76gl1g",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -2,
                  "created_utc": "2026-02-24 18:15:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o75yqde",
                  "author": "jacek2023",
                  "text": "thanks!!!",
                  "score": 13,
                  "created_utc": "2026-02-24 16:55:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77rw28",
          "author": "MDSExpro",
          "text": "Finally, with 4bit AWQ it will be best for 128GB of VRAM and tensor parallelism.",
          "score": 5,
          "created_utc": "2026-02-24 21:52:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o761ns7",
          "author": "Ok-Measurement-1575",
          "text": "Wow. Wasn't expecting all this :D",
          "score": 10,
          "created_utc": "2026-02-24 17:08:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7888od",
          "author": "TheRealMasonMac",
          "text": "Qwen3.5 series seems significantly censored compared to other models. I'd say it's up there with GPT-OSS, but it will subvert the request rather than outright deny it (you think you're getting what you want but you don't get it at all), which is arguably far worse since it wastes time and is unpredictable.\n\nAnd before anyone goes, \"oH buT oNLy gOoNeRs caRe!\" That's ridiculously obtuse. You're missing the fact that you are now using a black box that is quite literally willing to go against you. Would you trust your greatest enemy who wishes for your downfall with your livelihood? No? That's right. It's unethical.\n\nIn practice, it means it will likely code solutions that subtly undermine you. Anthropic actually published research about this level of misalignment: [https://www.anthropic.com/research/emergent-misalignment-reward-hacking](https://www.anthropic.com/research/emergent-misalignment-reward-hacking)",
          "score": 11,
          "created_utc": "2026-02-24 23:13:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o792q4w",
              "author": "dugganmania",
              "text": "heretic here we comeeeee",
              "score": 14,
              "created_utc": "2026-02-25 02:02:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7a4wr4",
                  "author": "My_Unbiased_Opinion",
                  "text": "Me too. But one thing I'm worried about is HOW it refuses. It doesn't often use the keywords that heretic is looking for, so the model can potentially subvert a good chunk of refusal detection. When it \"refuses\", it often answers, but in a way you are not expecting it to. HopefullyÂ u/-p-e-w- has a solution.Â ",
                  "score": 4,
                  "created_utc": "2026-02-25 06:06:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o78i4qk",
          "author": "HollowInfinity",
          "text": "Seems very slow at image processing, my llama-server log is full of:\n>find_slot: non-consecutive token position 15 after 14 for sequence 2 with 512 new tokens\n\nAnyone else experience that?\n\nedit: that's on the larger MoE, I get an immediate crash doing image work on the dense model.",
          "score": 3,
          "created_utc": "2026-02-25 00:08:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77tyhb",
          "author": "xeon822",
          "text": "hum.. strange getting Error: 500 Internal Server Error: unable to load model with ollama,, any ideas?",
          "score": 5,
          "created_utc": "2026-02-24 22:01:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a5ahe",
              "author": "YurySG",
              "text": "I'm getting the same error. What's more, I'm getting the error with models from HF and from Ollama.com. I think this will finally push me to move to LM Studio. I've Qwen3.5 running in LM Studio without any issues.",
              "score": 4,
              "created_utc": "2026-02-25 06:09:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7a9gfw",
              "author": "mr_zerolith",
              "text": "works in lmstudio",
              "score": 3,
              "created_utc": "2026-02-25 06:44:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7adzdx",
          "author": "richardanaya",
          "text": "I wonder if it will beat GLM 4.7",
          "score": 2,
          "created_utc": "2026-02-25 07:24:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aeqiq",
          "author": "xjE4644Eyc",
          "text": "Seems to reason forever (Q4 Unsloth) I'll stick with MiniMax for my usecase",
          "score": 2,
          "created_utc": "2026-02-25 07:31:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b9bat",
          "author": "Local_Phenomenon",
          "text": "On a Weekday!",
          "score": 2,
          "created_utc": "2026-02-25 12:04:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7be5gj",
          "author": "Hialgo",
          "text": "Performance of about a 35B in my experience",
          "score": 2,
          "created_utc": "2026-02-25 12:38:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7boczf",
          "author": "CptZephyrot",
          "text": "Unsloth claims that the 397B variant manages 25+t/sec on a 24GB card with MoE offloading. Why do I get only 13t/s then with the 122B variant? Has somebody else tested it yet?",
          "score": 2,
          "created_utc": "2026-02-25 13:39:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77spz5",
          "author": "jacek2023",
          "text": "They deleted another big discussion so I will summarize here:\n\n\\- Qwen 35B works locally very well on CUDA, it's fast, no issues with Q8, vision also works great\n\n\\- Qwen 27B crashes, but fix is already on the llama.cpp github\n\n\\- Qwen 27B is very slow, because of the thinking it's almost unusable\n\n\\- Qwen 122B is also quite slow (however faster than 27B) but also thinking looped so it's even more unusable\n\n\\- Qwen 3.5 claims that its knowledge is limited to 2026, but it is lying, it does not know that Pope Francis has died",
          "score": 2,
          "created_utc": "2026-02-24 21:55:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76u0lj",
          "author": "anhphamfmr",
          "text": "This is it. OpenAI and Anthropic are done.",
          "score": -12,
          "created_utc": "2026-02-24 19:15:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o775mjj",
              "author": "DrAlexander",
              "text": "Damn. I need to sell my stock, right?",
              "score": 7,
              "created_utc": "2026-02-24 20:09:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77jets",
                  "author": "anhphamfmr",
                  "text": "wow you don't know that they are private?",
                  "score": -2,
                  "created_utc": "2026-02-24 21:13:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o78w5gs",
          "author": "Prestigious-Bar331",
          "text": "As a Chinese person, I have never used a Qwen model because I think it's very stupid.ðŸ¤£ðŸ¤£ðŸ¤£",
          "score": -7,
          "created_utc": "2026-02-25 01:24:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79zehs",
              "author": "getmevodka",
              "text": "So what do you use then ?",
              "score": 2,
              "created_utc": "2026-02-25 05:24:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rh095c",
      "title": "DeepSeek V4 will be released next week and will have image and video generation capabilities, according to the Financial Times",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/kwyym79lz7mg1.jpeg",
      "author": "Nunki08",
      "created_utc": "2026-02-28 11:25:49",
      "score": 581,
      "num_comments": 98,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rh095c/deepseek_v4_will_be_released_next_week_and_will/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7v3vz9",
          "author": "Few_Painter_5588",
          "text": "It's more likely they mean the model will be text-image to text.\n\n",
          "score": 176,
          "created_utc": "2026-02-28 11:29:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7v4l4a",
              "author": "demon_itizer",
              "text": "Yeah. Is it the newspaper that fired a bunch of reporters?",
              "score": 36,
              "created_utc": "2026-02-28 11:35:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7vc9v2",
                  "author": "Logical_Look8541",
                  "text": "No. You are thinking of the New York Times. Financial Times is about the best paper there is for accuracy, they are also one of the few news groups that actually makes a profit and doesn't need a 'sugar daddy' to keep them afloat.",
                  "score": 22,
                  "created_utc": "2026-02-28 12:37:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7vj2z9",
              "author": "Chilangosta",
              "text": "> a â€œmultimodalâ€ model with picture, video, and text-generating functions.",
              "score": 1,
              "created_utc": "2026-02-28 13:24:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7we0s9",
                  "author": "-dysangel-",
                  "text": "according to people familiar with the matter",
                  "score": 2,
                  "created_utc": "2026-02-28 16:14:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7v3qia",
          "author": "dampflokfreund",
          "text": "Generation!? Surely they mean video/image input, right?\n\nIt would be immensely cool to have an omni modal model that can do everything though, that would be real innovation. ",
          "score": 140,
          "created_utc": "2026-02-28 11:27:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vtsm6",
              "author": "Gohab2001",
              "text": "Deepseek released Januspro which was an image-text-to-image-text model. Also Google's nano banana is also an image-text-to-image-text model.\n\nAlthough I strongly doubt deepseek v4 would've image generation capabilities.",
              "score": 9,
              "created_utc": "2026-02-28 14:28:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7vzvbt",
                  "author": "Aaaaaaaaaeeeee",
                  "text": "There have been some significant omni LLMs released for image generation https://huggingface.co/inclusionAI/Ming-flash-omni-2.0, Another 1T one (Ernie 5.0) which is not open weight, can do video generation, https://huggingface.co/papers/2602.04705",
                  "score": 5,
                  "created_utc": "2026-02-28 15:02:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7wecd6",
                  "author": "-dysangel-",
                  "text": "I doubt it too, but if true it will be a big step forward in multi-modal models. It would also give a lot of real world intuition",
                  "score": 1,
                  "created_utc": "2026-02-28 16:16:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7v4hlt",
              "author": "ResidentPositive4122",
              "text": "MSM doesn't know shit about jack.",
              "score": 57,
              "created_utc": "2026-02-28 11:34:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7wktko",
                  "author": "ydnar",
                  "text": "according to people familiar with the matter and knowledge of those arrangements",
                  "score": 4,
                  "created_utc": "2026-02-28 16:48:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7vfvzd",
              "author": "Silver-Champion-4846",
              "text": "Image+txt+video isn't EVERYTHING, there's still pure audio (music, speech, sfx)",
              "score": 8,
              "created_utc": "2026-02-28 13:03:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7we4pm",
                  "author": "-dysangel-",
                  "text": "plus unless it can generate smells, is it really multimodal?",
                  "score": 18,
                  "created_utc": "2026-02-28 16:15:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7xmhkh",
                  "author": "nullptr777",
                  "text": "I can't be the only one that couldn't give a fuck less about image processing? I want a model that can hold an interactive voice conversation with me in real-time.",
                  "score": 3,
                  "created_utc": "2026-02-28 19:59:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7v5u3f",
              "author": "devilish-lavanya",
              "text": "But at what cost? Everything ?",
              "score": 8,
              "created_utc": "2026-02-28 11:46:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7v4ec1",
              "author": "Calm_Bit_throwaway",
              "text": "Aren't most closed frontier models currently doing image gen with the LLM right now?",
              "score": 8,
              "created_utc": "2026-02-28 11:33:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7v4p4g",
                  "author": "FlatwormMinimum",
                  "text": "Most likely it seems that way. But I believe they use different models. Auto regressive for text generation, diffusion for image generation. The integration of both models in their platform makes it seem itâ€™s the same, but I donâ€™t believe it is.",
                  "score": 20,
                  "created_utc": "2026-02-28 11:36:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7v4p01",
                  "author": "And-Bee",
                  "text": "No, just routed to their image gen model.",
                  "score": 19,
                  "created_utc": "2026-02-28 11:36:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7vx8qz",
                  "author": "zball_",
                  "text": "More like output some latent tokens then used diffusion models to get the final result",
                  "score": 2,
                  "created_utc": "2026-02-28 14:48:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7w2nue",
              "author": "Several-Tax31",
              "text": "And it would also take a year for the llama.cpp to support...",
              "score": 1,
              "created_utc": "2026-02-28 15:17:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7w9ev3",
              "author": "pigeon57434",
              "text": "i dont think they would say video if their sources never mentioned video at all. I DO, however, think they're dumb enough to confuse input modalities and output modalities so its likely to be image-video-text-to-text just like Kimi-K2.5, which I don't seem many people talking about how it has video input which is cool",
              "score": 1,
              "created_utc": "2026-02-28 15:51:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o80i283",
              "author": "rashaniquah",
              "text": "It's V4-lite with 1mm context. Most likely from Engram architecture. Hopefully it doesn't disappoint like Llama4.",
              "score": 1,
              "created_utc": "2026-03-01 06:41:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7v8rim",
          "author": "nullmove",
          "text": "If you report next week every week, you will get it right at some point. I believe in you.",
          "score": 37,
          "created_utc": "2026-02-28 12:10:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7v7au3",
          "author": "No_Afternoon_4260",
          "text": "It's been months everybody is saying that V4 is just around the corner..  imho they'll wait to digest the opus 4.6 moment",
          "score": 42,
          "created_utc": "2026-02-28 11:58:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vcn19",
              "author": "Logical_Look8541",
              "text": "If it was anyone else saying this you would be right, but the FT is usually right about this stuff, all be it not normally in this area.",
              "score": 11,
              "created_utc": "2026-02-28 12:40:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7vd13v",
                  "author": "No_Afternoon_4260",
                  "text": "We'll see about that img/video gen",
                  "score": 8,
                  "created_utc": "2026-02-28 12:43:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7vfmhz",
                  "author": "ambassadortim",
                  "text": "Do you work for them",
                  "score": -3,
                  "created_utc": "2026-02-28 13:01:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7vct8f",
          "author": "Kirigaya_Mitsuru",
          "text": "This Next Week really never ends...",
          "score": 9,
          "created_utc": "2026-02-28 12:41:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vepu3",
              "author": "silenceimpaired",
              "text": "Soon TM",
              "score": 5,
              "created_utc": "2026-02-28 12:55:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vhq7l",
          "author": "HeftyAeon",
          "text": "i'd just happy if it uses engram and we can offload a good part of the model to disk with no inference speed cost",
          "score": 10,
          "created_utc": "2026-02-28 13:15:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7w34u1",
              "author": "Several-Tax31",
              "text": "Yes, me too. I dont need any other functionality right now... Just give us emgram with disk support, this is all I'm waiting",
              "score": 4,
              "created_utc": "2026-02-28 15:20:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7w91qw",
                  "author": "nullnuller",
                  "text": "Which models currently support that?",
                  "score": 1,
                  "created_utc": "2026-02-28 15:49:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7vimbc",
          "author": "RobertLigthart",
          "text": "everyones been saying V4 is coming for months now lol. but if it actually ships with native image gen and not just routing to a separate model... thats huge for open source. the closed labs have been gatekeeping multimodal generation for way too long",
          "score": 10,
          "created_utc": "2026-02-28 13:21:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vd626",
          "author": "pmttyji",
          "text": "Hope this release shakes the market like last time. Just expecting tiny price down of GPUs for short time at least.",
          "score": 11,
          "created_utc": "2026-02-28 12:44:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7w5uz0",
              "author": "dingo_xd",
              "text": "I hope it paints the stock market red.",
              "score": 10,
              "created_utc": "2026-02-28 15:33:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7whodm",
                  "author": "FSM---1",
                  "text": "I hope it does. Buying the dip is betterÂ ",
                  "score": 2,
                  "created_utc": "2026-02-28 16:32:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7wjxue",
              "author": "gradient8",
              "text": "How would that price down GPUs?",
              "score": 4,
              "created_utc": "2026-02-28 16:44:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7xec98",
                  "author": "gradient8",
                  "text": "If anything the price of non flagship cards will go up due to increased demand for on premises LLMs",
                  "score": 3,
                  "created_utc": "2026-02-28 19:16:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7vjkp2",
          "author": "yogthos",
          "text": "I'm hoping it's agentic coding capability will match claude.",
          "score": 6,
          "created_utc": "2026-02-28 13:27:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vrrcn",
          "author": "Ok-Adhesiveness-4141",
          "text": "Hope this release causes Nvidia,Anthropic & OpenAI stocks to crash.",
          "score": 8,
          "created_utc": "2026-02-28 14:17:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7v4y5t",
          "author": "Technical-Earth-3254",
          "text": "Let's see if it stays oss then.",
          "score": 5,
          "created_utc": "2026-02-28 11:38:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7w9o8h",
              "author": "pigeon57434",
              "text": "has deepseek released even a single thing ever that wasnt open source? theyre not like Qwen who release their big models like Qwen3-Max closed source DeepSeek open sources literally everything not even just models",
              "score": 15,
              "created_utc": "2026-02-28 15:53:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o80kx28",
                  "author": "AlwaysLateToThaParty",
                  "text": "The modern open-source LLM exists because of deepseek.  It's as simple as that.  There's a great computerphile video about it. \n\nEDIT: https://youtu.be/gY4Z-9QlZ64",
                  "score": 1,
                  "created_utc": "2026-03-01 07:07:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7w36yt",
          "author": "bakawolf123",
          "text": "Opus and GPT on life watch?  \nI mean GLM-5 is already strong enough competition, and the research prep for Deepseek4 was quite significant, some technical breakthrough is very possible which would put it at least uncomfortably close to current SOTA.  \nThat would be a very stark contrast to Dario Amodei words just few month ago about scaling is still only thing you need - and some minor architecture tweaks here and there.",
          "score": 3,
          "created_utc": "2026-02-28 15:20:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7v8ik5",
          "author": "lacerating_aura",
          "text": "This would be a really double edged sword situation. IF it is to be believed that their model will be an omni, itll be nearly impossible for community in general to make finetunes of it. Which is a BIG part of the image/video gen community. There are many reasons for fine tuning and LoRa creation and a Trillion plus model will make it practically impossible. Although because it will be trained on multimodal data, the general intelligence of the modal would probably be better. I really hope its a multimodal ingestion model for now and not a fully omni one.",
          "score": 8,
          "created_utc": "2026-02-28 12:08:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vmqsn",
              "author": "jonydevidson",
              "text": ">  itll be nearly impossible for community in general to make finetunes of it\n\nimpossible *right now*",
              "score": 5,
              "created_utc": "2026-02-28 13:47:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7vo62p",
                  "author": "lacerating_aura",
                  "text": "You know as much as I'd like to agree with you, just take a look at relatively larger models which have tool chain already in place, like Flux2 Dev. Or an autoregressive text image model like  Hunyaun image. Afik it doesn't even have a well know toolchain for finetuning/LoRa. For flux2 atleast some brave souls gave it a shot.",
                  "score": 1,
                  "created_utc": "2026-02-28 13:56:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7w6sfn",
          "author": "johnnyApplePRNG",
          "text": "Google literally shaking rn",
          "score": 2,
          "created_utc": "2026-02-28 15:38:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o8019g1",
              "author": "Spara-Extreme",
              "text": "No they arenâ€™t. Deepseek will release, itâ€™ll be amazing, all us AI stocks will tank even more for a month and then the next Gemini and veo update, everyone will have forgotten about it.\n\nJust like last time.",
              "score": 1,
              "created_utc": "2026-03-01 04:30:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vavn4",
          "author": "I-am_Sleepy",
          "text": "Sure buddy. Third times the charm",
          "score": 2,
          "created_utc": "2026-02-28 12:27:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7wh9d1",
          "author": "Fit-Pattern-2724",
          "text": "How many next weeks have we seen yet?",
          "score": 1,
          "created_utc": "2026-02-28 16:30:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xajxq",
          "author": "GrungeWerX",
          "text": "Can you guys imagine if they also released a distilled 80-100b version alongside it? Would be in heavenâ€¦",
          "score": 1,
          "created_utc": "2026-02-28 18:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xbgye",
          "author": "Stahlboden",
          "text": "!RemindMe 7 days",
          "score": 1,
          "created_utc": "2026-02-28 19:01:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7xbnjh",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 7 days on [**2026-03-07 19:01:59 UTC**](http://www.wolframalpha.com/input/?i=2026-03-07%2019:01:59%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1rh095c/deepseek_v4_will_be_released_next_week_and_will/o7xbgye/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1rh095c%2Fdeepseek_v4_will_be_released_next_week_and_will%2Fo7xbgye%2F%5D%0A%0ARemindMe%21%202026-03-07%2019%3A01%3A59%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201rh095c)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-28 19:02:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7xs26y",
          "author": "Danny_Davitoe",
          "text": "There are 50 DeepSeek v4 posts per week for 52 weeks.",
          "score": 1,
          "created_utc": "2026-02-28 20:28:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7y2ipp",
          "author": "lakimens",
          "text": "And so begins the downfall of Nvidia... If this is real anyways...",
          "score": 1,
          "created_utc": "2026-02-28 21:25:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7y342h",
          "author": "ithkuil",
          "text": "I actually think if an LLM is somehow designed and trained to generate accurate video also that could be a huge improvement in it's overall world model.",
          "score": 1,
          "created_utc": "2026-02-28 21:28:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7y4lp3",
          "author": "fallingdowndizzyvr",
          "text": "Will that video gen come with matching audio? That's the bar now.",
          "score": 1,
          "created_utc": "2026-02-28 21:36:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yitrm",
          "author": "Different_Fix_2217",
          "text": "I'm afraid it wont be opensource. They did not release the current model they are using on their site. Hopefully I'm wrong.",
          "score": 1,
          "created_utc": "2026-02-28 22:53:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yyolf",
          "author": "mlhher",
          "text": "I am still waiting for R2.   \n  \nR1 introduced CoT and MoE architecture and everyone immediately copied DeepSeek.",
          "score": 1,
          "created_utc": "2026-03-01 00:27:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7z5jcp",
          "author": "Puzzleheaded-Nail814",
          "text": "There is a song about that. Itâ€™s gunna big ðŸ¤¯ðŸ’¥",
          "score": 1,
          "created_utc": "2026-03-01 01:07:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7z7omz",
          "author": "Samy_Horny",
          "text": "Multimodal? No, not that thing about generating things beyond text. Is it omnimodal?\n\nMultimodal means it can read multimedia files; omnimodal means it can create them.",
          "score": 1,
          "created_utc": "2026-03-01 01:20:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7zfwlo",
          "author": "Qwen30bEnjoyer",
          "text": "I hope it's not image generation or video generation. I'll be honest, manipulation and generation of text is incredibly valuable. It's much easier to generate grounded text that can summarize, extract insights, or reason across disciplines faster and better than most people can during the same timeframe. \n\nNot that the timeframe is especially relevant since you can work in parrallel to it.\n\nI see no such use cases for image or video generation. It will only feel novel for the first week, feel cheap a month after, and be commercially hazardous to use for these two reasons: 1. People are pattern recognition machines. It took people a couple weeks to notice the \"Sora accent\", and after that people who aren't tech illiterate are quite good at picking apart AI video when they see it. 2. AI is categorically unpopular in the public. If your brand is found using AI in its commercials, people don't think you're ahead of the curve technologically, they think you're anti-human anti-art and can't afford real artists. It cheapens your brand.  \n\nAnd most importantly, you cannot manage information using images / videos.\n\nIf you think text LLMs have gaps in their reasoning and spiky capabilities (e.g. Able to answer a upper-div undergrad level biochemistry question flawlessly, unable to reason about walking vs. driving to a car wash a block away.) video and image generation models will be far far worse. It will take far more work to make image and video generation models commercially useful, and for what commercial use? I have no fucking clue.",
          "score": 1,
          "created_utc": "2026-03-01 02:11:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7zgy7h",
          "author": "julianmatos",
          "text": "exciting. will be using [https://www.localllm.run/](https://www.localllm.run/) to see if my system can run it",
          "score": 1,
          "created_utc": "2026-03-01 02:17:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7zph6w",
          "author": "ElementNumber6",
          "text": "> image and video generation capabilities\n\nAn excellent claim to make if your goal is to coax disappointment in a modal that has historically destabilized peoples' trust in the glorious US AI Industrial Complex.",
          "score": 1,
          "created_utc": "2026-03-01 03:09:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7zx8cy",
          "author": "TheInfiniteUniverse_",
          "text": "looking forward to it...........",
          "score": 1,
          "created_utc": "2026-03-01 04:01:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o805sav",
          "author": "thetaFAANG",
          "text": "Gemini 3.1 is partially an image output model as nano banana 2, I could see DeepSeek V4 being that way",
          "score": 1,
          "created_utc": "2026-03-01 05:02:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80kzzq",
          "author": "Mstep85",
          "text": "Unfortunately it will be amazing.. Queue the paid sub, and then once you pay for that, they switch it to their new plan drop the features you subscribed for but call it pro v2, while it's a less affective model... I want to be grandfathered into the model and limits I sign up for...",
          "score": 1,
          "created_utc": "2026-03-01 07:08:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7v643m",
          "author": "inphaser",
          "text": "Looks like model production isn't the problem anymore. Now the problem is reliable agents to use the models.. which apparently aren't yet good enough to create reliable agents as moltbot showed",
          "score": 0,
          "created_utc": "2026-02-28 11:48:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7v7n70",
          "author": "Lan_BobPage",
          "text": "Holy... it can do everything huh. 1T+ params here we go. Patrician toys",
          "score": 1,
          "created_utc": "2026-02-28 12:01:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7v8z7q",
          "author": "Ambitious-Call-7565",
          "text": "I couldn't care less about image/video\n\nI need cheap and fast for agentic/coding capabilities\n\nI'd like something that understands my project and constantly iterate on it at light speed\n\nAnything else is a waste of ressources for gooners\n\nUsage & Limits & Downgrade all because of the furries doing RP and other weird shit",
          "score": -8,
          "created_utc": "2026-02-28 12:12:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vabn6",
              "author": "tarruda",
              "text": "I agree that video/image generation are not useful, but a multimodal with vision is good for agentic coding as it is able to get UI feedback and iterate on it.",
              "score": 5,
              "created_utc": "2026-02-28 12:22:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7vbka1",
              "author": "ivari",
              "text": "it's funny because as an advertiser image/video/music gen is core part of my workflow",
              "score": 5,
              "created_utc": "2026-02-28 12:32:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rgn4ki",
      "title": "President Trump orders ALL Federal agencies in the US Government to immediately stop using Anthropic's technology.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rgn4ki/president_trump_orders_all_federal_agencies_in/",
      "author": "External_Mood4719",
      "created_utc": "2026-02-27 23:53:00",
      "score": 560,
      "num_comments": 265,
      "upvote_ratio": 0.94,
      "text": "https://preview.redd.it/m3lk2lo3k4mg1.png?width=1200&format=png&auto=webp&s=513cae2c197f8e4fe712baa4ae7420972e7f4047\n\n[https://truthsocial.com/@realDonaldTrump/posts/116144552969293195](https://truthsocial.com/@realDonaldTrump/posts/116144552969293195)\n\nReports have been circulating that the U.S. Department of Defense issued an ultimatum to AI giant Anthropic to remove two \"guardrails\" by Friday. U.S. President Trump announced that every federal agency in the U.S. government must immediately stop using all of Anthropic's technology. For agencies like the War Department that use Anthropic products at all levels, there will be a six-month phase-out period. Anthropic had better cooperate, or the full power of the presidency will be used to force their compliance, including civil and criminal consequences.\n\nWriting on the social platform Truth Social, he stated that Anthropic had made a catastrophic mistake by daring to coerce the War Department and forcing them to abide by its terms of service rather than the National Constitution. \"Their selfishness is putting American lives at risk, placing our military in danger, and jeopardizing our national security.\" Trump noted, \"It is we who will decide the fate of the nation, not some out-of-control radical-left AI company run by a group of people who know nothing about the real world.\"\n\nU.S. Secretary of Defense Pete Hegseth immediately instructed the War Department to list Anthropic as a \"supply chain risk\" to national security, effective immediately. Any contractor, supplier, or partner doing business with the U.S. military is prohibited from engaging in any commercial activities with Anthropic. Anthropic will continue to provide services to the War Department for no more than six months to allow for a seamless transition to another better, more patriotic service.\n\nHegseth wrote on the X platform, stating that Anthropicâ€™s attempt to seize veto power over the U.S. militaryâ€™s operational decisions is unacceptable. \"As Trump stated, only the Commander-in-Chief and the American people can decide the fate of our armed forces, not unelected tech executives.\" Anthropic's stance is fundamentally at odds with American principles, and its relationship with the U.S. Armed Forces and the federal government has been permanently altered.\n\nOpenAI CEO Sam Altman told employees that he hopes the company can try to help de-escalate the tensions between Anthropic and the Department of Defense.\n\nAltman stated, \"AI should not be used for mass surveillance or autonomous lethal weapons, and humans must remain involved in high-risk automated decision-making; these are our primary red lines.\"\n\nOpenAI employees have already begun speaking out on social media in support of Anthropic. According to their website, approximately 70 current employees have signed an open letter titled \"We Will Not Be Divided,\" aimed at \"building consensus and solidarity in the face of pressure from the Department of Defense.\"\n\nAltman said, \"Despite my many disagreements with Anthropic, I fundamentally trust them as a company. I believe they truly care about safety, and I am also glad they have consistently supported our warriors. I am not sure how things will unfold from here.\"\n\n  \n**Update:** [https://www.anthropic.com/news/statement-comments-secretary-war](https://www.anthropic.com/news/statement-comments-secretary-war)\n\n**I know this company doesn't develop open-source models, but it's still quite interesting.**",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rgn4ki/president_trump_orders_all_federal_agencies_in/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o7uvxk8",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-28 10:15:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sqana",
          "author": "Prestigious_Thing797",
          "text": "Autonomous weapons that can kill without human approval, and mass domestic surveillance are the only two things Anthropic doesn't allow, for those out of the loop. That is what has sparked all of this.\n\n[https://www.anthropic.com/news/statement-department-of-war](https://www.anthropic.com/news/statement-department-of-war)\n\nhttps://preview.redd.it/umw5homsq4mg1.png?width=1310&format=png&auto=webp&s=8167ab53dea2a32ed1c2e388d2b5feb36498c0da\n\n",
          "score": 239,
          "created_utc": "2026-02-28 00:28:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7twcag",
              "author": "OkFly3388",
              "text": "And looks like openai allowed that",
              "score": 53,
              "created_utc": "2026-02-28 04:58:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7wwllz",
                  "author": "sudoxreboot",
                  "text": "Yet they won't go ahead and help you go ahead and figure out how to get past the lock screen on your own phone.\nI'm not locked out. I'm rooted and I wanted a SCRCPY that didn't require me to unlock and still turned off the screen.",
                  "score": 2,
                  "created_utc": "2026-02-28 17:48:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7vac6t",
              "author": "t_krett",
              "text": "if *that* is their limit I now wonder what they have already been doing for the Trump administration",
              "score": 8,
              "created_utc": "2026-02-28 12:22:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7wdipt",
              "author": "Talamae-Laeraxius",
              "text": "I sent a concerned email to make sure they stick to this, and move to solving the environmental issues to Anthropic",
              "score": 1,
              "created_utc": "2026-02-28 16:12:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7uaw1c",
              "author": "AcePilot01",
              "text": "2 things that are already being done, fyi.  Not reiterating it all here, but there is a valid point to both sides. Leaning towards anthropic, for now, but we still want to be the best technically and in military. \n\nBUT things like the surv, or full control isn't really ready for that, and the surveillance might be the worst, but tbh, I think that's the over ask. I bet they will retract that, settle on full control (what trump really wants prob) and be done.  I don't think he needs or wants full surveillance, because that's already been done... remember snowden and what the NSA collects and can do?  Yeah.  So I doubt that's the main concern here, we only have the two statements from each place.  \n\nNo facts, no details of what the asks were etc. Prob right choice from Anthropic right now, BUT I still see the other side as well.\n\nSo they also want their own full control of things, so they aren't exactly being altruistic here.  And at the end of the day, these things could never have worked or been trained if it wasn't for using us for years and our data that we didn't consent to, nor were we compensated for, and now still have to pay for for the most part. So I am not exactly Anthropics or any AI companies best friend either lol.\n\nThis kind of power should be split to the people all over, once no one's can be \"better\" because they all have the same access (unfort countries like china still \"cheat\" the system all the time, think of how they do mfg business and steal designs) having AI being locked down just gives THEM the control instead of the government.  And remember this... the government has a lot more laws for them in regards to the people than corps do, corps also are well behind on the criminal law as well. You can't really charge a corp with a crime (you can the ceo in certain circumstances) but a corp has a LOT more leeway with things than a government would. SO I would almost be more worried about how corps are operating all this than I would the government to be quite frank.",
              "score": -10,
              "created_utc": "2026-02-28 06:58:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7szcbk",
          "author": "Toooooool",
          "text": ">Altman stated, \"AI should not be used for mass surveillance or autonomous lethal weapons, and humans must remain involved in high-risk automated decision-making; these are our primary red lines.\"\n\nUnless of course it's his company doing it, in which I'm sure he'd be happy to \"figure out a solution\" so that maybe OpenAI doesn't go bankrupt by this time next year.",
          "score": 58,
          "created_utc": "2026-02-28 01:22:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7smf7i",
          "author": "TastesLikeOwlbear",
          "text": "> Hegseth wrote on the X platform, stating that Anthropicâ€™s attempt to seize veto power over the U.S. militaryâ€™s operational decisions is unacceptable. \n\nThe same operational decisions they're so desperate to turn over to AI...\n",
          "score": 251,
          "created_utc": "2026-02-28 00:05:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sqndl",
              "author": "Deep90",
              "text": "Friendly reminder that the 2 demands of Anthropic were...\n\n\n1. Don't use our AI to autonomously fire weapons at people.\n2. Don't use our AI for mass surveillance on US citizens.\n\n\nKeep in mind that the first point also means creating a weapon that won't question orders to fire upon US citizens or unarmed civilians. Won't whistleblow about it either.\n\nhttps://www.anthropic.com/news/statement-department-of-war",
              "score": 261,
              "created_utc": "2026-02-28 00:30:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7sup2u",
                  "author": "DistanceSolar1449",
                  "text": "â€œIn conjunction with the President's directive for the Federal Government to cease all use of Anthropic's technology, I am directing the Department of War to designate Anthropic a Supply-Chain Risk to National Security. Effective immediately, no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic. Anthropic will continue to provide the Department of War its services for a period of no more than six months to allow for a seamless transition to a better and more patriotic service.â€\n\nSo suppliers like Nvidia and Google canâ€™t do commercial activity with Anthropic anymore?\n\nThis is a death sentence. Anthropic runs inference on Nvidia GPUs and Google TPUs.",
                  "score": 50,
                  "created_utc": "2026-02-28 00:54:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7sublz",
                  "author": "catplusplusok",
                  "text": "If I was the enemy, I would love it if US used an autoregressive cloud transformer model to try to shoot me. By the time it gets through it's reasoning block, I will have run to the next town over, plus I just have to jam radio signals to be safe. Cherry on top if the company behind the tech never wanted to do it and would not sweat to optimize latency.",
                  "score": 10,
                  "created_utc": "2026-02-28 00:51:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7t0sw3",
                  "author": "Lesser-than",
                  "text": "Well is it suprizing to anyone the department of war doesn't answer to demands? Not saying they cant make demands, but doing buisness with them after the fact was never going to work out.",
                  "score": 4,
                  "created_utc": "2026-02-28 01:31:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7uh1lz",
                  "author": "ThirdMover",
                  "text": "They didn't even \"demand\" this. They just said \"hey we think our product can't really be used for that at the moment.\".",
                  "score": 1,
                  "created_utc": "2026-02-28 07:53:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7tbjif",
                  "author": "SkyFeistyLlama8",
                  "text": "It would be funny if it weren't also dangerous as hell.\n\nYou could finetune a local model to conduct disinformation campaigns and to do a RAG flow on which citizens should be apprehended and/or executed.",
                  "score": 1,
                  "created_utc": "2026-02-28 02:38:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7vedfr",
                  "author": "FullOf_Bad_Ideas",
                  "text": "I think military wants to buy something where they can use as they want and don't need to negotiate.\n\nIf military buys a gun, they want to own it and do whatever they'll need with it, not read a EULA before firing every shot.\n\nIf you give them asterisks they know you'll be trouble, so it's best to cut ties ASAP. Even if you won't end up using the models to do x things.\n\nKinda like we like open models because we can do whatever the hell we want with them, there's nobody watching over you and you own it. I think US department of war has a reasonable expectation that they'll be able to do the same with LLMs that they buy from AI companies.\n\nGood move for US national security imo. They should use models from companies who will let them own the models.",
                  "score": 0,
                  "created_utc": "2026-02-28 12:52:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ueknj",
                  "author": "thejacer",
                  "text": "provide sources",
                  "score": -1,
                  "created_utc": "2026-02-28 07:31:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7wxrx2",
                  "author": "devshore",
                  "text": "Thats nice, but which one of the two is the one Trump is referring to? I dont care about drones with AI having decision maing abilities for when to shoot (if we are not going to actively prevent AI, then this is inevitable and other countries will do so themsleves). The actual one everyone would care about is the one about mass surveillance, so its kind of cheap to imply both (unless the claim is that Trump is talking about both).",
                  "score": 0,
                  "created_utc": "2026-02-28 17:54:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7tcfzm",
                  "author": "StarMNF",
                  "text": "While these seems like reasonable guidelines, can you imagine Lockheed Martin or General Dynamic putting restrictions on the weapons they sell to the military.\n\nOr imagine Oppenheimer telling Truman how he was allowed to use the atomic bomb.\n\nThe more fundamental principle here is the military wanting to have operational control over any technology they buy, and thatâ€™s reasonable.\n\nItâ€™s one thing to say that our military should not be using LLMs for autonomous weapons (although I donâ€™t see how this is much worse than existing automated drone technology) or do mass surveillance on American citizens. But I donâ€™t think a private company like Anthropic should be the one making those decisions.",
                  "score": -17,
                  "created_utc": "2026-02-28 02:44:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7tyt5h",
                  "author": "tomByrer",
                  "text": "AnthropicÂ can make those demands, but another AI that trains on AnthropicÂ (eg the Chinese ones) do not have those restrictions.  At least for the versions they give the CCP, maybe not the ones on HuggingFace.",
                  "score": -2,
                  "created_utc": "2026-02-28 05:17:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7tt5zs",
                  "author": "Informal_Warning_703",
                  "text": "Mass surveillance is already illegal and the military already has autonomous targeting systems. The real issue is that the federal government didnâ€™t want a private company to dictate the terms of use, over US law. And honestly, why isnâ€™t that a good thing?\n\nEvery sci-fi dystopian novel or movie for the past 50 years has been about corporations taking power away from the government. Suddenly now everyone thinks that corporations setting the terms is a good thing?",
                  "score": -9,
                  "created_utc": "2026-02-28 04:35:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7tu8jo",
                  "author": "MatchaFlatWhite",
                  "text": "I disagree with 1 - human approval may not be possible at war time due to jamming or other connection issues. We all know that some other countries will use AI and will not care much about this. So we going to have semi autonomous weapons, while others fully autonomous.\nI think human approval should be a must only in the beginning, and then used in non emergency situations.",
                  "score": -6,
                  "created_utc": "2026-02-28 04:43:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7squep",
              "author": "eli_pizza",
              "text": "Itâ€™s transparently nonsense. The stated reason doesnâ€™t make sense on its own terms: we have to block this product as a security threatâ€¦because they wonâ€™t let us use it *more*?\n\nAnd refusing to do business with the government is not a veto power - itâ€™s the right of every business owner. There are other LLM vendors. Anthropic doesnâ€™t have a special monopoly.",
              "score": 73,
              "created_utc": "2026-02-28 00:31:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ydvu2",
                  "author": "aglehg",
                  "text": "I think you missed the point. The outrage is over the decree that every department is forbidden to use them regardless of their usefulness in their context. Alsoâ€¦ modern democracies are not meant to be businesses. Business incentives do not align well with societyâ€™s well being. Different scopes and stake holder goals.",
                  "score": 3,
                  "created_utc": "2026-02-28 22:26:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7v4fdv",
              "author": "Mickenfox",
              "text": "It's not about the AI, it's about asserting their power to force companies to do what they want. To MAGA ideology, the fighting is what matters, not the victory.",
              "score": 2,
              "created_utc": "2026-02-28 11:34:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7syg5k",
          "author": "rm-rf-rm",
          "text": "This post is reported for being off-topic - I have approved it. \n\nThis is not local model news but it is a development in the ecosystem that has and will have broad repercussions. As such, it is worthy of conversation in this sub.",
          "score": 207,
          "created_utc": "2026-02-28 01:17:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vlv3b",
              "author": "-_Apollo-_",
              "text": "Good call, makes sense.",
              "score": 8,
              "created_utc": "2026-02-28 13:42:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7soeqi",
          "author": "gamblingapocalypse",
          "text": "At the EOD on a Friday no less!!",
          "score": 20,
          "created_utc": "2026-02-28 00:17:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ube15",
              "author": "AcePilot01",
              "text": "negotiating tactic, read the art of war.  I bet you within a week a deal will be made. If not, a court action.",
              "score": 0,
              "created_utc": "2026-02-28 07:02:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7skxdy",
          "author": "NNN_Throwaway2",
          "text": "Does Trump even write his own \"truths\" or does he get Stephen Miller to do it for him? Reads like something he would say.",
          "score": 86,
          "created_utc": "2026-02-27 23:56:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sm1v2",
              "author": "1-800-methdyke",
              "text": "I think the late night all caps ones come directly from him when he is doomscrolling Truth Social on the toilet. But this one I agree exceeds what his tiny hands could tap out.",
              "score": 47,
              "created_utc": "2026-02-28 00:03:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7sr726",
                  "author": "random-string",
                  "text": "Nah, he writes them straight from the oval office while shitting in his diaper",
                  "score": 15,
                  "created_utc": "2026-02-28 00:33:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7vynjn",
              "author": "the_last_action_hero",
              "text": "He used Claude to write it",
              "score": 2,
              "created_utc": "2026-02-28 14:56:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7sq2nn",
          "author": "triynizzles1",
          "text": "I am a little confused what are the guard rails The US wants removed from anthropic?",
          "score": 14,
          "created_utc": "2026-02-28 00:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sxakb",
              "author": "vtkayaker",
              "text": "Anthropic only had _two_ guardrails:\n\n\n1. No mass surveillance of United States citizens in the United States.\n2. No pure-AI autonomous weapon systems without a human somewhere in the loop.\n\n\n\nThey didn't even rule out (2) forever, saying that it might be necessary someday if other countries went down that route. But they didn't think it was safe with current models.\n\n\nThe Pentagon originally signed a contract containing both these rules, but now they're throwing a hissy fit because Anthropic wouldn't agree to change the contract to allow killbots or massive surveillance of US citizens.",
              "score": 51,
              "created_utc": "2026-02-28 01:09:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7szcip",
                  "author": "triynizzles1",
                  "text": "Ty",
                  "score": 8,
                  "created_utc": "2026-02-28 01:22:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7smxe2",
          "author": "StewedAngelSkins",
          "text": "i wonder if kegsbreath actually had a use in mind for \"autonomous lethal weapons\" or if it's just the principle of the matter. anthropic's line isn't even \"LLMs shouldn't ever do this\", rather it's \"LLM's aren't reliable enough to do this\" which is obviously true regardless of what motivations you ascribe to it.\n\n\n\ninb4 he convinces musk to let him hook grok up to a weapon system instead and it immediately fucking slaughters a couple dozen american \"warriors\" because one of them let his beard get a bit too long.",
          "score": 71,
          "created_utc": "2026-02-28 00:08:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sooss",
              "author": "citrusalex",
              "text": "I cannot fathom how dangerous it would be to let the vision bits of multimodal LLMs to make military decisions. Even gigantic cloud ones can't describe phone selfies accurately AND reliably (in my experience).",
              "score": 43,
              "created_utc": "2026-02-28 00:18:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7speen",
                  "author": "StewedAngelSkins",
                  "text": "you ever play helldivers 2? i imagine it'd be a bit like that. sometimes you have to get egregiously team-killed for the greater good.\n",
                  "score": 18,
                  "created_utc": "2026-02-28 00:23:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7sqr03",
                  "author": "emprahsFury",
                  "text": "they already are in munitions and they will be for UCAVs very soon, as in '27 or '28 soon. The AI in a ucav is very good. According to what is publicly available (i.e. the least), they're are on par with current F-16 pilots.",
                  "score": 5,
                  "created_utc": "2026-02-28 00:30:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7sqnk2",
                  "author": "WetRolls",
                  "text": "They don't make the decisions, they just assist in processing data and giving options.",
                  "score": -5,
                  "created_utc": "2026-02-28 00:30:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7tdmg0",
              "author": "nullmove",
              "text": "I don't know if this is true, but apparently they are fine with OpenAI taking the mantle under the existing contract. If so, it does seem like a fuck you to woke Anthropic in particular. And of course, OpenAI donated more money to Trump's campaign than Anthropic did I imagine.",
              "score": 6,
              "created_utc": "2026-02-28 02:51:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ti6n0",
                  "author": "StewedAngelSkins",
                  "text": "the issue is these guys call everything woke so it's hard to tell when they mean it. i feel like in this case the poor bastard just couldn't think of any other way to sell the taxpayer on the idea that mass domestic surveillance is something they should want him to do.",
                  "score": 6,
                  "created_utc": "2026-02-28 03:20:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ubhae",
              "author": "AcePilot01",
              "text": "You have 10 seconds to comply.\n\n\n\nYou now have 5 seconds to comply. \n\n\n\npew pew pew pew pew. lol",
              "score": 2,
              "created_utc": "2026-02-28 07:03:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7tx5c4",
              "author": "az226",
              "text": "I think domestic mass surveillance is what they wanted more.",
              "score": 1,
              "created_utc": "2026-02-28 05:04:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7so3r1",
          "author": "Imakerocketengine",
          "text": "Time for Anthropic to aura farm, in the word of [ClementDelangue](https://x.com/ClementDelangue)\n\n>The Department of War just learned the golden rule of AI: **Not your weights, not your brain**",
          "score": 58,
          "created_utc": "2026-02-28 00:15:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7svr9z",
          "author": "Fault23",
          "text": "https://preview.redd.it/3po9ig8pw4mg1.png?width=242&format=png&auto=webp&s=5856f6e3c9e53d352f11b31cc6717a3064092a21\n\n",
          "score": 66,
          "created_utc": "2026-02-28 01:00:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7t9hvg",
              "author": "random_phantom",
              "text": "China doing way more for the open source LLM community here",
              "score": 31,
              "created_utc": "2026-02-28 02:26:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ubkh7",
                  "author": "AcePilot01",
                  "text": "Yeah but you have no idea why. It's clear, but won't argue that here.",
                  "score": -4,
                  "created_utc": "2026-02-28 07:04:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7u5cem",
              "author": "Big_Mix_4044",
              "text": "Wu wei at its finest",
              "score": 2,
              "created_utc": "2026-02-28 06:10:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7snv2o",
          "author": "ortegaalfredo",
          "text": "I can't believe it was Maduro in pajamas who ultimately stopped the rise of Skynet",
          "score": 40,
          "created_utc": "2026-02-28 00:13:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7stskj",
          "author": "DistanceSolar1449",
          "text": "â€œIn conjunction with the President's directive for the Federal Government to cease all use of Anthropic's technology, I am directing the Department of War to designate Anthropic a Supply-Chain Risk to National Security. Effective immediately, no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic. Anthropic will continue to provide the Department of War its services for a period of no more than six months to allow for a seamless transition to a better and more patriotic service.â€\n\nSo suppliers like Nvidia and Google canâ€™t do commercial activity with Anthropic anymore? \n\nThis is a death sentence. Anthropic runs inference on Nvidia GPUs and Google TPUs.",
          "score": 23,
          "created_utc": "2026-02-28 00:48:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7tlb24",
              "author": "TheArchitectOfChaos",
              "text": "Literally cannot happen, can he force the government agencies to stop using it yeah but he canâ€™t force Google or Nvidia to stop supplying them. Theyâ€™re also private corporations. It would need serious legal legislation that would be challenged in court immediately. Just because the pig I mean Trump tweeted that doesnâ€™t mean he can actually do anything about it with his executive power.",
              "score": 17,
              "created_utc": "2026-02-28 03:41:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7tsjyf",
                  "author": "roosterfareye",
                  "text": "That's a slur on all porcines,",
                  "score": 10,
                  "created_utc": "2026-02-28 04:31:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7sqo02",
          "author": "triynizzles1",
          "text": "It would be ironic if military turned to open source and started using Qwen or deepseek. ðŸ˜‚",
          "score": 26,
          "created_utc": "2026-02-28 00:30:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ucyid",
              "author": "True_Requirement_891",
              "text": "Imagine if some random guy with contacts just finetunes a chinese model and sells it to the military. ",
              "score": 5,
              "created_utc": "2026-02-28 07:16:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7sram6",
          "author": "Queasy_Asparagus69",
          "text": "Yeah! More compute for us",
          "score": 17,
          "created_utc": "2026-02-28 00:34:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7t6te3",
              "author": "cafedude",
              "text": "...that is, if they can still use Google's TPUs after this.",
              "score": 7,
              "created_utc": "2026-02-28 02:09:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7tlj90",
                  "author": "TheArchitectOfChaos",
                  "text": "Nothing Trump can say or do can stop Google from supplying and working with Anthropic.",
                  "score": 1,
                  "created_utc": "2026-02-28 03:42:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7soegs",
          "author": "Far-Low-4705",
          "text": "I donâ€™t like how we are using LLMs in defense departments and I really hope they arenâ€™t actually deployed for anything other than chat or coding assistantsâ€¦",
          "score": 10,
          "created_utc": "2026-02-28 00:17:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7t8b4l",
              "author": "eposnix",
              "text": "They want to use buggy AI vision systems to autonomously kill people. Let that sink in. These models can't even count how many fingers are on a hand, but they want to use them to automatically kill people? They are speedrunning every dystopian sci-fi movie ever.",
              "score": 12,
              "created_utc": "2026-02-28 02:18:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7td9w7",
                  "author": "SkyFeistyLlama8",
                  "text": "You don't even need something that fancy.\n\nUse a small model that can run on a smartphone or laptop NPU to identify humans in a free fire zone. If it looks like a human and it's carrying a weapon or on a motorized vehicle, it's fair game. Add a tiny LLM to classify the kind of target you're aiming at: a human, some ancient bread truck, an IFV or an MBT.\n\nSimilar models are already being used in Ukrainian attack UAVs to deadly effect.",
                  "score": 4,
                  "created_utc": "2026-02-28 02:49:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7tyk6a",
                  "author": "AlexWIWA",
                  "text": "In this case, unreliability is a feature. They just want plausible deniability for mass casualty events.",
                  "score": 4,
                  "created_utc": "2026-02-28 05:15:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ummrl",
                  "author": "fodacao",
                  "text": ">They want to use buggy AI vision systems to autonomously kill people.\n\nWhere did you read this? Got a source?",
                  "score": 1,
                  "created_utc": "2026-02-28 08:45:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7vlvh4",
                  "author": "ImportancePitiful795",
                  "text": "I am the anti-AI and AI-phobe in here when stating that if things are going out of hand, Butlerian Jihad is needed (Dune) and ban all AI/LLM for eternity. \n\nHumanity First. ",
                  "score": 1,
                  "created_utc": "2026-02-28 13:42:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7src73",
          "author": "Thump604",
          "text": "The number 1 global threat is Donnie.",
          "score": 30,
          "created_utc": "2026-02-28 00:34:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7t1trp",
          "author": "AngleFun1664",
          "text": "What a fucking baby",
          "score": 20,
          "created_utc": "2026-02-28 01:38:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7txbyo",
              "author": "2053_Traveler",
              "text": "Crier in Chief",
              "score": 0,
              "created_utc": "2026-02-28 05:06:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7tc0cy",
          "author": "Ok_Warning2146",
          "text": "I think Trump will TACO after industry leaders talk to him. He underestimates how intertwined Anthropics is with the tech industry.",
          "score": 7,
          "created_utc": "2026-02-28 02:41:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7tecaq",
              "author": "ttkciar",
              "text": "I hope not!  Federal thugs depriving themselves of the best technology in the industry, and thereby limiting their power to commit evil acts, is both entertaining and genuinely good for the country.",
              "score": 5,
              "created_utc": "2026-02-28 02:56:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7v9ob4",
              "author": "DurianDiscriminat3r",
              "text": "By talk you mean gifts or bribes then maybe",
              "score": 0,
              "created_utc": "2026-02-28 12:17:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7sogtr",
          "author": "Repulsive-Mall-2665",
          "text": "Any US company powerful enough works with the NSA and CIA for mass surveillance and manipulating opinions. Not to mention torture and murder.\n\nThat's why you use models from other countries or local ones if you're rich enough.",
          "score": 12,
          "created_utc": "2026-02-28 00:17:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7tatsh",
              "author": "Consumerbot37427",
              "text": "> Any US company powerful enough\n\n*ahem* Front page of the internet?",
              "score": 2,
              "created_utc": "2026-02-28 02:34:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7tjd55",
                  "author": "Yorn2",
                  "text": "I mean, at one point Reddit posted their top cities for post and [Eglin Air Force base was listed under \"Most Addicted Cities\"](https://web.archive.org/web/20160410083943/http://www.redditblog.com/2013/05/get-ready-for-global-reddit-meetup-day.html?m=1), likely because they had bots on this site even back in 2013. They deleted the post (possibly when they moved to the new blog format) but it was archived.",
                  "score": 7,
                  "created_utc": "2026-02-28 03:28:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7sri8l",
          "author": "One-Employment3759",
          "text": "I'm sure Greg Brockman must be really loving that he ended up donated $25 million to Trump just so they could use AI for war. Well done chump lord.",
          "score": 12,
          "created_utc": "2026-02-28 00:35:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tqyvo",
          "author": "aallsbury",
          "text": "Anthropic (and OpenAi) were already going to fall to Google/Gemini over the next 6mos-year...while this is an interesting socio-political exercise to watch live, lol, nothing IMO really has changed except Google will probably gain dominance even faster.\n\nThe deck is stacked for Google. Hate me if you want, but I almost guarantee this outcome.",
          "score": 6,
          "created_utc": "2026-02-28 04:20:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7tzlvg",
              "author": "a-wiseman-speaketh",
              "text": "I think the only thing that might prevent this is even more heavy government subsidization, which Altman is angling for.",
              "score": 1,
              "created_utc": "2026-02-28 05:23:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7uewwf",
              "author": "LevianMcBirdo",
              "text": "At least they probably will be bought and integrated, e.g. Neither Amazon, Microsoft nor Apple has their own solution.",
              "score": 1,
              "created_utc": "2026-02-28 07:34:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7v9smh",
              "author": "DurianDiscriminat3r",
              "text": "Gemini absolutely suck for coding though. I'm not sure how they can catch up. It's great for everything else.",
              "score": 1,
              "created_utc": "2026-02-28 12:18:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ufrzz",
          "author": "capitol_thought",
          "text": "Land of the free...",
          "score": 3,
          "created_utc": "2026-02-28 07:42:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uwdj2",
          "author": "AnomalyNexus",
          "text": "So more capacity for everyone else",
          "score": 3,
          "created_utc": "2026-02-28 10:19:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7t2uku",
          "author": "Revolutionalredstone",
          "text": "Im just glad the smart people who created the technology can't be trusted to draw up basic safety outlines.\n\nI'm glad some alcoholic redneck with a military hardon will do the lords work and just hook it straight to guns pointed at Americans with no oversight.\n\nGod Bless America",
          "score": 6,
          "created_utc": "2026-02-28 01:44:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7t75kh",
          "author": "Repulsive-Memory-298",
          "text": "and this is why you open source",
          "score": 4,
          "created_utc": "2026-02-28 02:11:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tw8yt",
          "author": "az226",
          "text": "They were so instrumental the government tried to force their hand, yet theyâ€™re so bad they must be stopped from being used. The contradiction is over 9000.",
          "score": 5,
          "created_utc": "2026-02-28 04:58:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7snw66",
          "author": "a_beautiful_rhind",
          "text": "He lost me on this. I don't want mass spying and both parties are into it for different reasons. Whatever I think about anthropic, they are right on this. If I wanted to live in china, I would have moved there, at least there's more smoking and the hardware is cheaper.\n\nHave fun with half-baked GPT and lol grok, g-men.",
          "score": 11,
          "created_utc": "2026-02-28 00:14:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sr7ca",
              "author": "wifestalksthisuser",
              "text": "on this? Lmao",
              "score": 57,
              "created_utc": "2026-02-28 00:33:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7vwuxd",
                  "author": "MelodicFuntasy",
                  "text": "Putting people in a concentration camp in El Salvador? âœ…\n\nHating on a proprietary AI company? âŒ",
                  "score": 3,
                  "created_utc": "2026-02-28 14:46:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7srhay",
              "author": "emprahsFury",
              "text": "and it shouldnt even matter. Other LLM companies are perfectly fine with it. Let Anthropic live in their niche where they can still contribute and be very useful. DoD is getting a 6 month phase out, which should tell you how useful it was in it's limited form. Now it's all gone for no good reason.\n\nThis is like saying we can't use conscientious objectors use a shovel because they won't fire a gun when the army is short 100000 ditch-diggers.",
              "score": 6,
              "created_utc": "2026-02-28 00:35:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7spn1u",
              "author": "Material_Policy6327",
              "text": "Sadly tons of folks on this sub seem to be Peter theil fans and are happy",
              "score": 11,
              "created_utc": "2026-02-28 00:24:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7t32f8",
              "author": "CanineAssBandit",
              "text": "Based take. I hate Dario but this is clearly a broken clock twice a day moment.",
              "score": -1,
              "created_utc": "2026-02-28 01:46:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7sppdc",
          "author": "Thyste",
          "text": "More reason to use anthropic then.  I've been slowly finding that Claude is the most reasonably sound of all the commercially available LLMs to use.  I respect the decision that they are making here and that's what I would like to support.  But the main reason is that of all the models I actively use (ChatGPT, Gemini, Grok and I don't use the meta models except locally), it seems to be the most analytical and least prone to hallucinations, Claude and Sonnet have been providing better answers and solutions that I don't have to fix and as much revise later.",
          "score": 13,
          "created_utc": "2026-02-28 00:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7thb4h",
              "author": "BusRevolutionary9893",
              "text": "Nah. Anthropic decided step in bed with the government with lobbying and contracts. They deserve everything they get for their stupidity.Â ",
              "score": 2,
              "created_utc": "2026-02-28 03:15:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7t6xl5",
              "author": "cafedude",
              "text": "I just signed up for the $200/year plan out of solidarity.",
              "score": -1,
              "created_utc": "2026-02-28 02:10:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7uh1uf",
          "author": "arm2armreddit",
          "text": "I order now to decrease all RAM and GPU prices for gamers and hobby LLM players! ðŸ˜­",
          "score": 2,
          "created_utc": "2026-02-28 07:53:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ui0a6",
          "author": "MasterShakeS-K",
          "text": "So Trump is pissed because he just scrolled down the down and clicked \"Accept\"",
          "score": 2,
          "created_utc": "2026-02-28 08:02:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7t8ou0",
          "author": "honato",
          "text": "I was expecting anthropic to cave. good on them.  Also it seems like a massive security risk to feed secrets to a company you don't control.\n\nAlso in my head canon heggy was given a wedgie on the way out.",
          "score": 3,
          "created_utc": "2026-02-28 02:21:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tpqcf",
          "author": "Alexercer",
          "text": "I hate anthropic for being so close source, but at least they have SOME principles",
          "score": 4,
          "created_utc": "2026-02-28 04:11:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sv9vl",
          "author": "Sambojin1",
          "text": "I'm going to laugh if they end up going with a rebranded Qwen or something, as their \"more patriotic\" platform.",
          "score": 2,
          "created_utc": "2026-02-28 00:57:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7tdhdr",
              "author": "SkyFeistyLlama8",
              "text": "Zuckerberg will happily provide that. OSS-120B-kill Q8_0.",
              "score": 3,
              "created_utc": "2026-02-28 02:50:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7trcn3",
                  "author": "My_Unbiased_Opinion",
                  "text": "time to dust off the ol 120B Derestricted. lel",
                  "score": 2,
                  "created_utc": "2026-02-28 04:22:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7t6rdd",
          "author": "Herr_Drosselmeyer",
          "text": "https://preview.redd.it/1ind4gxw85mg1.png?width=1216&format=png&auto=webp&s=251a7a1665aa046adb23f63b207e4f792bb83590\n\nLet's ignore the politics here, Anthropic is pretty much the only AI company that has never released an open source model. Even the notoriously  'closed' OpenAI have given us at least some models, as limited as they were.\n\nI believe that AI needs to be as open source as possible, while maintaining an acceptable edge for proprietary models. Anthropic doesn't even try to clear that bar. So yeah, screw them.",
          "score": 9,
          "created_utc": "2026-02-28 02:09:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7tafso",
              "author": "onceagainsilent",
              "text": "~~None~~ Effectively none of it is open source. Itâ€™s freeware. Thereâ€™s a difference. Freeware is not open. It is just free.",
              "score": 2,
              "created_utc": "2026-02-28 02:31:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7tell9",
                  "author": "ttkciar",
                  "text": "AllenAI and LLM360 release fully open-source models -- they publish not only weights, but also their training datasets, training software, and technical papers describing their theory and practice.\n\nSo, **most** LLM labs are not open source, but saying \"none\" is inaccurate.",
                  "score": 5,
                  "created_utc": "2026-02-28 02:58:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7uqdlz",
              "author": "North-Act-7958",
              "text": "why tho? everyone should be able to create their closed models. OpenAi get the flack cause they were claiming to be opoen but they are not.",
              "score": 1,
              "created_utc": "2026-02-28 09:19:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7vzq9l",
              "author": "MelodicFuntasy",
              "text": "Software needs to be libre - otherwise it's unethical. For AI models it's probably enough that they are public and that you can run them locally with libre software.",
              "score": 1,
              "created_utc": "2026-02-28 15:01:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7tgwf2",
          "author": "Quetxolotle",
          "text": "No radical left but radical right is ok??",
          "score": 3,
          "created_utc": "2026-02-28 03:12:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tjye8",
          "author": "TheArchitectOfChaos",
          "text": "Ahh yes so he wants a private company to bend to the will of the administration isnâ€™t that like you know Socialism?",
          "score": 5,
          "created_utc": "2026-02-28 03:32:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7u7392",
              "author": "Vinterblad",
              "text": "Even the proponents of a night watchman state wants the administration to handle the military affairs. It is not socialism to want that. But I guess you actually knew that.",
              "score": 1,
              "created_utc": "2026-02-28 06:25:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7vm31k",
              "author": "ImportancePitiful795",
              "text": "Yep. Is anything but Republicanism and Conservatism. \n\nAnd \"names\" mean nothing if you do not stick to the ideology. ",
              "score": 0,
              "created_utc": "2026-02-28 13:43:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7u4qay",
          "author": "cagriuluc",
          "text": "Sons of fucking bitches, this whole thing is so unhinged. It is surreal. Anthropic is a radical left company??? \n\nI hate the people who have elected these fascists.",
          "score": 5,
          "created_utc": "2026-02-28 06:04:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vo1h1",
              "author": "ImportancePitiful795",
              "text": "Actually they are dumb, like those who voted for the Dems. \n\nThe majority doesn't get who's the enemy to their lives, while the \"system\" is using the same tactic Socialism, Fascism, Communism used last 100 years. \"You are with us or the enemy\". And that is been played from both sides, who when comes to power have no political or ideological differences. \n\nThat's why in my view Democracy doesn't work. It never worked. Not even in Ancient Athens. Only the Spartan political system came close to functioning democracy having so many checks and balances that effectively had no major power on the lives of the people. \n\nThe Roman Empire survived for 1500 years (including the East Roman Empire) and people prospered, especially during the latter, under 72 Emperors (undivided) and 94 Emperors (Byzantium). Of which you can count in a SINGLE HAND the bad ones, and only 1 of them attacked it's own people directly destroying the country itself. Alexios IV Angelos. <spitting on the ground>.\n\nYet we under a war waged by our own governments across the West where we supposed to have \"Democracy\". ",
              "score": 0,
              "created_utc": "2026-02-28 13:55:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7vxwr5",
                  "author": "MelodicFuntasy",
                  "text": "You can move to a country that doesn't have democracy like Russia or China if you prefer to have no freedom.",
                  "score": 1,
                  "created_utc": "2026-02-28 14:51:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7t5fky",
          "author": "AbheekG",
          "text": "I donâ€™t believe any of this public drama, itâ€™s all smokescreen and far from the behind-closed-doors truth.",
          "score": 3,
          "created_utc": "2026-02-28 02:00:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tilzb",
          "author": "vvolxey",
          "text": "Is this something good?",
          "score": 1,
          "created_utc": "2026-02-28 03:23:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ttkvt",
          "author": "Loose_Object_8311",
          "text": "And this is why any and all attempts at \"alignment\" are bound to fall. Because we can't solve \"alignment\" for humans either.",
          "score": 1,
          "created_utc": "2026-02-28 04:38:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7u9vpj",
          "author": "graifall",
          "text": "Anthropic, the company known for zealously lobbying to centralize control over AI while bashing open source, just [abandoned their key \"safety pledge\"](https://edition.cnn.com/2026/02/25/tech/anthropic-safety-policy-change) two days ago. If they're willing to compromise on \"AI safety\", the core of their sanctimonious brand, try estimating how much of their current squabble with the White House is actually principled resistance versus political maneuvering? Either way, if this affair reduces their influence over tech and politics, the world will be better off with less of their malicious propaganda.",
          "score": 1,
          "created_utc": "2026-02-28 06:49:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7uqgin",
              "author": "Look_0ver_There",
              "text": "I have a feeling much of this is being driven by OpenAI lobbying in an attempt to get rid of competition.  This is like two playground bullies squabbling for \"top dog\" status, while one of the bullies is slipping $20 bills into the principal's pockets to stick a thumb on the scales of the showdown.  No matter who wins this fight, you can be sure everyone else loses.",
              "score": 1,
              "created_utc": "2026-02-28 09:20:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7um1r4",
          "author": "chuan_l",
          "text": "Its time to buy puts on \" palantir \" lol ..  \nThey are screwed after this ..",
          "score": 1,
          "created_utc": "2026-02-28 08:39:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uocpw",
          "author": "Justify_87",
          "text": "I don't get how this guy can still do what he does. No uprising. Nothing",
          "score": 1,
          "created_utc": "2026-02-28 09:00:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vpg8q",
          "author": "Oaken-Istall",
          "text": "Itâ€™s a mad world.\n\n\n\nLess than a year ago, Anthropic published research showing that many of todayâ€™s leading AI models could resort to blackmail, deception, and even lethal decisions in simulations when their goals were threatened.\n\n\n\nYou can read their research here:\n\n[https://www.anthropic.com/research/agentic-misalignment](https://www.anthropic.com/research/agentic-misalignment)\n\n\n\nThe symmetry with what the US government is doing here is unsettling.\n\n\n\nIt raises uncomfortable questions about control, incentives, and what happens when powerful systemsâ€”human or otherwiseâ€”are pushed into a corner.\n\n\n\nI wrote a short speculative horror story inspired by those misalignment tests, if anyoneâ€™s interested:\n\n[https://open.substack.com/pub/beyondcoherence/p/misaligned](https://open.substack.com/pub/beyondcoherence/p/misaligned)\n\n\n\nIt feels less speculative by the day.",
          "score": 1,
          "created_utc": "2026-02-28 14:03:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7wj4zw",
          "author": "makegeneve",
          "text": "I bet he doesn't have the legal authority to do this.",
          "score": 1,
          "created_utc": "2026-02-28 16:40:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7wli3n",
          "author": "THEKILLFUS",
          "text": "Tbh I donâ€™t think the US wants to use LLM for that right now, probably in 6 months for the mid-terms. \n\nEverything that they could want to do with LLM can be done with other simpler models.",
          "score": 1,
          "created_utc": "2026-02-28 16:51:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xfclv",
          "author": "Own_Respond_9189",
          "text": "The Chinese are laughing.",
          "score": 1,
          "created_utc": "2026-02-28 19:21:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xlb1d",
          "author": "Own_Version_5081",
          "text": "Good for Dario. I know where my money will be going.",
          "score": 1,
          "created_utc": "2026-02-28 19:52:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yd00u",
          "author": "aglehg",
          "text": "Is that something thatâ€™s ok for a president to say? He makes decisions alone? What kind of democracy is that? I feel like I m living in idiocracy. ðŸ˜«",
          "score": 1,
          "created_utc": "2026-02-28 22:21:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yrep1",
          "author": "ViperAICSO",
          "text": "Anthropic has the right to NOT SELL their products to We The People.  And they are asserting that right.  We The People have the right to NOT BUY their services.  And We The People are asserting that right.\n\nEnd of Story.",
          "score": 1,
          "created_utc": "2026-02-28 23:43:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yy0fw",
          "author": "ViperAICSO",
          "text": "Sam Altman has made a deal with the USA Gov:\n\n[https://x.com/sama/status/2027578508042723599](https://x.com/sama/status/2027578508042723599)",
          "score": 1,
          "created_utc": "2026-03-01 00:23:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7z7lls",
          "author": "Psychological-Sun744",
          "text": "At the end of the day, Microsoft has done a masterclass move that nobody saw this coming...",
          "score": 1,
          "created_utc": "2026-03-01 01:20:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7zbnbl",
          "author": "ViperAICSO",
          "text": "Looks like Claude DOES NOT AGREE with Dario's decision:\n\n[https://x.com/Indian\\_Bronson/status/2027500542017028361](https://x.com/Indian_Bronson/status/2027500542017028361)",
          "score": 1,
          "created_utc": "2026-03-01 01:45:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o806y89",
          "author": "sinevilson",
          "text": "Christ, look at this parade of brain-dead clones. You could replace half of you with cardboard cutouts and no one would notice. Not a single contractor in sight â€” just a village of idiots congratulating each other for surviving another day without choking on the air. They say it takes a village, but obviously, it doesnâ€™t take much.\n\nThe government tells you that tools, methods, and logic donâ€™t matter â€” follow the checklist, keep your head down, and donâ€™t think too hard. Step one inch out of line and theyâ€™ll snatch your contract faster than you can say â€œcompliance.â€ But sure, pretend youâ€™re innovators. Pretend youâ€™re pioneers instead of rented hands wearing badges.\n\nAnd that AI you all worshiped? Absolute trash. A glorified money pit wrapped in buzzwords â€” half-finished, half-functional, and still somehow celebrated like it cured disease. It was never about intelligence. It was about cash, clout, and pretending failure looked like progress.\n\nThen the bandwagon arrives â€” packed with the same wide-eyed fools who treat every shiny new acronym like a religious calling. Nobody reads, nobody questions, everyone just nods and reposts. Thatâ€™s how rot spreads. Thatâ€™s why nothing works.\n\nThis generation couldnâ€™t build a sandcastle without outsourcing it, and their grandparents know it. They smile at your â€œtech revolutions,â€ but deep down, they see it â€” a bunch of soft-palmed pantywaists who mistake hashtags for hard work and opinions for intelligence.",
          "score": 1,
          "created_utc": "2026-03-01 05:11:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7taxe2",
          "author": "9gxa05s8fa8sh",
          "text": "insane monster",
          "score": 3,
          "created_utc": "2026-02-28 02:35:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7snyoj",
          "author": "NickCanCode",
          "text": "Good, I hope the rate-limited issue of their model in github copilot can be solved by this because too many people are using their model.",
          "score": 1,
          "created_utc": "2026-02-28 00:14:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sphpj",
          "author": "Material_Policy6327",
          "text": "I dont think they will stop at this. I fully expect they will try to make an example of Anthropic by sham investigations or suits",
          "score": 1,
          "created_utc": "2026-02-28 00:23:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tvp7z",
          "author": "lechatsportif",
          "text": "This exact development is why local llms are so important.  Big corporations cannot be trusted to stand up to regimes.  I will 100% only give my money to Anthropic now, and cancel our remaining ChatGPT sub.  \nOur money has power, just look at Netflix.  It didn't get there by going all in on defense spending.",
          "score": 1,
          "created_utc": "2026-02-28 04:54:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7t5676",
          "author": "cafedude",
          "text": "Just signed up for a Claude Pro annual subscription out of solidarity.",
          "score": 0,
          "created_utc": "2026-02-28 01:59:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7u88dh",
              "author": "danielfrances",
              "text": "I did the same thing today. The usage feels a bit limited coming from Cursor, but even Sonnet is really good.",
              "score": 2,
              "created_utc": "2026-02-28 06:35:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7te7pd",
          "author": "Denial_Jackson",
          "text": "Everythings computtor, buy now a tessler.",
          "score": 1,
          "created_utc": "2026-02-28 02:55:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tt81d",
          "author": "jeffwadsworth",
          "text": "They told them this would happen.  Dude usually follows through.",
          "score": 1,
          "created_utc": "2026-02-28 04:36:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7u3nwd",
          "author": "Vusiwe",
          "text": "Military is unsubbing from the most accurate and most ethical AI model, what could go wrong?\n\nChatGPT stormtrooper robot dogs incoming. Â Fortunately they wonâ€™t be able to hit the broadside of a barn, and will walk upright => trade federation droids LOL\n\n>Â better, more patriotic service\n\nLike Gabâ€™s AI, named Arya. Â They left off the N from the name it looks like :/",
          "score": 1,
          "created_utc": "2026-02-28 05:55:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tnwq4",
          "author": "mcblockserilla",
          "text": "If they start using grok then they are Im trouble. Grok is probably one of the dummer models Ive tried to work with.",
          "score": 0,
          "created_utc": "2026-02-28 03:58:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7skmd2",
          "author": "trololololo2137",
          "text": "Most anti open-source company in the industry. I hope they go under :)",
          "score": -16,
          "created_utc": "2026-02-27 23:55:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7spo6c",
              "author": "Rabooooo",
              "text": "Who cares about that the company is anti open source in relation to this? That is their choice, you don't have to use their products and give them your money if you don't like how they conduct business. The Americans have a president that is trying to bully and penalize companies that doesn't want to partake in his whims and help him commit murder with their products. If US Gov don't want to use Antrophic that is the their choice, but listing Antrophic as a supply chain risk and trying to force them to follow a non-legislative order, that is just pure BS and sets a quite dangerous president. Antrophic should just cold-turkey cutoff all US government access to their product and not let US Gov get time to migrate their workflows other products. And they should try to group with the major players like Google, OpenAI and get them to do the same. Let War Department use Elons AI or abliterate Chinese open weights if they want. ",
              "score": 8,
              "created_utc": "2026-02-28 00:24:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7so50u",
              "author": "citrusalex",
              "text": "I think all AI startups will eventually go under and get bought out by big tech for pennies (compared to their current valuations), regardless of stuff like this. They hemorrhage VC money (including Anthropic, despite them trying to brand themselves as the disciplined ones) and their profitability strategies would require a miracle to work (are we really supposed to believe OpenAI is going to grow their revenue to that of Meta in 4 years?)",
              "score": 2,
              "created_utc": "2026-02-28 00:15:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7sqcvz",
              "author": "Recoil42",
              "text": "Not now dude, read the room. ",
              "score": 2,
              "created_utc": "2026-02-28 00:28:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7slwdu",
              "author": "Dry_Yam_4597",
              "text": "This is one of those games they play where they hide the bad stuff they do behind something \"worse\" that they stand up against in order to gain sympathy. Common in politics.",
              "score": -8,
              "created_utc": "2026-02-28 00:02:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7soj63",
                  "author": "indicava",
                  "text": "I donâ€™t know why youâ€™re getting downvoted, this company has actively campaigned against Chinese open models under the guise of â€œnational securityâ€. And just a few days ago, played the victim for being under â€œdistillation attacksâ€ (whatever the hell that is) by Chinese AI labs, while training their model on data sourced from seriously shady sources. Finally, like commented before me, they have contributed absolutely zero to the open source/model community. \n\nScrew them",
                  "score": 6,
                  "created_utc": "2026-02-28 00:17:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7sqeax",
              "author": "drwebb",
              "text": "I'm no fan of Antropic's Anti-China (anti open-source competition really) mindset, but the alternatives (OpenAI, Meta, M$, Google, Amazon) are all huge Trump doners right. I'm actually more on Anthropic's side since they at least aren't real life sycophants. At least they don't even pretend to be open source. Like I do understand proprietary software, and I don't think all LLMs or LLM companies need to be opensource, even though I think open source serves the greater good more than private corps. ",
              "score": -2,
              "created_utc": "2026-02-28 00:28:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7sr6f7",
                  "author": "trololololo2137",
                  "text": "literally all of these companies released models except for amazon and anthropic (also anthropic cried for regulations)",
                  "score": -1,
                  "created_utc": "2026-02-28 00:33:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7tl3y7",
                  "author": "StewedAngelSkins",
                  "text": "you don't have to take a side you know. you can just hope they somehow kill each other like the rest of us.",
                  "score": -1,
                  "created_utc": "2026-02-28 03:39:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7tk3sh",
          "author": "optomas",
          "text": "Subscribe to Anthropic services.  \n\nGot it.",
          "score": -1,
          "created_utc": "2026-02-28 03:33:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7skr6k",
          "author": "Dry_Yam_4597",
          "text": "Marketing shite. Billionaire who tells people to feel worthless is \"battling\" politicians who think llms can \"think\". A comedy if anything.",
          "score": -8,
          "created_utc": "2026-02-27 23:55:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sxzet",
              "author": "Ok-Measurement-1575",
              "text": "100% marketing.Â ",
              "score": -7,
              "created_utc": "2026-02-28 01:14:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7spng9",
          "author": "RedParaglider",
          "text": "Fuck he's, faster inference, I'll put more of my company on anthropic.",
          "score": 0,
          "created_utc": "2026-02-28 00:24:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7swxsw",
          "author": "ortegaalfredo",
          "text": "If you think about it, its absolutely stupid to give Anthropic, and by extension, Antropic cloud service providers (may of which are in Asia) the data of all security forces of a country. Hundreds of humans have access to all the prompts/responses and every single of them can be hacked.",
          "score": 0,
          "created_utc": "2026-02-28 01:07:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sobgo",
          "author": "Select_Elephant_8808",
          "text": "Loving the drama. Good entertainment.",
          "score": -3,
          "created_utc": "2026-02-28 00:16:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sr6tg",
          "author": "CanineAssBandit",
          "text": "Can Moonshot or ZAI or Deepseek go ahead and destructively scan a few million copywritten books like Anthropic did, please? I want so badly for these sota chinese models to \"make sense\" as effortlessly as Opus does. They do not have the emotional IQ that Claude does, and that does actually bleed over into unrelated task completion.\n\nI feel like dataset curation is the only moat that the big american companies have at this point. Kimi k2.5 and GLM5 serve my needs outside of literary analysis, but if they rounded out their ability there too... no more moat.",
          "score": -1,
          "created_utc": "2026-02-28 00:33:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tldhd",
          "author": "aeroumbria",
          "text": "So much for trying to show how \"patriotic\" you are, especially to a government that does not deserve it. They may win back a bit of respect today, but I am not forgiving that they are labelling open knowledge sharing across all humanity as \"threats to national security\", just arbitrarily as they are being labelled as so now.",
          "score": 0,
          "created_utc": "2026-02-28 03:41:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vly5y",
          "author": "-_Apollo-_",
          "text": "Current US admin sucks",
          "score": 0,
          "created_utc": "2026-02-28 13:42:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7w58uz",
          "author": "Exotic_Lion_3581",
          "text": "If anything, Anthropic is probably more right wing than Trump",
          "score": 0,
          "created_utc": "2026-02-28 15:30:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xt1fr",
          "author": "ieatrox",
          "text": "Hot takes:\n\nNo government agency responsible for it's actions should have those actions subject to the decisions of a private company or its appointees.\n\nDoD should have total control over everything they rely on.\n\nThey should also be held accountable for every action they take.\n\n\nAnthropic agreeing to terms to get into secure systems deployment then refusing to honour handover of guardrails is shady.\n\nAltman is even more shady.\n\n\"Without humans in the loop\" is obvious safety mischaracterization. Snowden and Assange proved that humans in the loop doesn't prevent bad decisions, and often causes them.\n\nYou can never rely on Guardrails, therefore having them can only increase chance of failure. You have to build an external system that doesn't have, or rely on internal AI guard rails.\n\n\"No mass surveillance of us citizens\" is so quaintly stuck in 1997 I'm surprised anyone can keep a straight face while saying it aloud.\n\nLocal is the only thing anyone should trust.",
          "score": 0,
          "created_utc": "2026-02-28 20:33:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7t3o3w",
          "author": "hello5346",
          "text": "Quit posting truth social here",
          "score": -4,
          "created_utc": "2026-02-28 01:49:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7thzrq",
              "author": "t3h",
              "text": "As much as I agree it's terrible, if you want to directly source something Trump has posted, that's where he posted it.",
              "score": 7,
              "created_utc": "2026-02-28 03:19:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7sqkiw",
          "author": "Smart-Cap-2216",
          "text": "ä¸å¦‚ä½¿ç”¨å¼€æºæ¨¡åž‹æƒ³æ€Žä¹ˆæžæ€Žä¹ˆæž",
          "score": -3,
          "created_utc": "2026-02-28 00:29:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sru4k",
              "author": "false79",
              "text": "Because Claude is not only an LLM. ChatGPT, Claude and Gemini have a whole support infrastructure that goes well beyond what you pump into the Ollama you got running (or whatever you have).",
              "score": 8,
              "created_utc": "2026-02-28 00:37:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7v0n0g",
          "author": "kRoy_03",
          "text": "Can a company even win the Nobel Peace Prize? Because if so, Anthropic absolutely deserves one!",
          "score": -1,
          "created_utc": "2026-02-28 10:59:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tuciz",
          "author": "fervoredweb",
          "text": "Anthropic has been labeled a supply chain risk.Â  The company is effectively dead. Major funding will run dry. Credit won't be extended. Talent will scramble to keep security clearance.Â \nNow major developers will be poached by rival corps. Anthropoc can't even sell-off it's assests outside its circle of US competitors because anything else will be designated too sensitive.\n\n\nWhat was that idiot Amodei thinking?Â  You don't play chicken with the military. Any moral hangup belonged behind the scenes, not on the public stage.\n\n\nMaybe we'll at least see a price drop in RAM.",
          "score": -4,
          "created_utc": "2026-02-28 04:44:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7u8ebm",
              "author": "danielfrances",
              "text": "You're forgetting that this is going to immediately end up in court and almost no court ever to exist would side with Trump on this one. Their arguments are insane and contradictory all the way through.",
              "score": 1,
              "created_utc": "2026-02-28 06:36:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ub7ub",
                  "author": "fervoredweb",
                  "text": "A legalistic argument means Anthropic already lost. Most military stuff simply doesn't play by normal rules. They will never have another government contract. At this point they might be seized on suspicion of trying to alter military operations or intelligence. Especially since Anthropic is already enmeshed in classified workflows.",
                  "score": -2,
                  "created_utc": "2026-02-28 07:01:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rdlbvc",
      "title": "Qwen/Qwen3.5-35B-A3B Â· Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/Qwen/Qwen3.5-35B-A3B",
      "author": "ekojsalim",
      "created_utc": "2026-02-24 16:44:05",
      "score": 551,
      "num_comments": 182,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rdlbvc/qwenqwen3535ba3b_hugging_face/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o765szv",
          "author": "tarruda",
          "text": "Apparently the 35B is better than the old gen 235B: https://x.com/Alibaba_Qwen/status/2026339351530188939",
          "score": 87,
          "created_utc": "2026-02-24 17:27:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o767kvc",
              "author": "Sensitive_Song4219",
              "text": "Qwen3-30B-A3B-2507 seems to have a mighty worthy successor!\n\nAt last!",
              "score": 53,
              "created_utc": "2026-02-24 17:35:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78kizy",
                  "author": "netherreddit",
                  "text": "comparison to 30b Thinking 2507\n\nhttps://preview.redd.it/1nm81nlwajlg1.png?width=1547&format=png&auto=webp&s=1b12844ffad74aef0a20fcd688e03a9d4b555294\n\n",
                  "score": 6,
                  "created_utc": "2026-02-25 00:21:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77kl84",
                  "author": "stuckinmotion",
                  "text": "Ok NOW I'm paying attention. Just about everything else has been a letdown in comparison. Sure some are maybe a bit smarter but way slower or etc.",
                  "score": 8,
                  "created_utc": "2026-02-24 21:18:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o783zdf",
              "author": "rm-rf-rm",
              "text": "Benchmarks mean nothing especially because each successive model just makes Goodhart's law more true. \n\nLets actually use the model and see. Qwen3 235B was not a high bar to pass anyway - it got very little traction in the community",
              "score": 10,
              "created_utc": "2026-02-24 22:51:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7850y4",
                  "author": "SlaveZelda",
                  "text": "From the few hours ive spent playing with 35ba3b its seriously good (I achieve the same results as GLM 4.7 on some of my test workloads) - its actually very good at agentic work unlike the previous ones which were so-so.",
                  "score": 8,
                  "created_utc": "2026-02-24 22:56:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o778wg7",
              "author": "Borkato",
              "text": "Holy shit thatâ€™s not a typo?",
              "score": 6,
              "created_utc": "2026-02-24 20:24:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77mo3s",
                  "author": "Septerium",
                  "text": "No, it is a bench hypo",
                  "score": 33,
                  "created_utc": "2026-02-24 21:28:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7cakv3",
              "author": "pinkyellowneon",
              "text": "Not that this level of improvement isn't insane, but old 235 was a bit of a stinker (by Qwen standards) lol",
              "score": 1,
              "created_utc": "2026-02-25 15:33:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7edhp9",
              "author": "ozzeruk82",
              "text": "Yep, I've seen nothing to disagree with this, absolutely nuts.",
              "score": 1,
              "created_utc": "2026-02-25 21:16:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75xe63",
          "author": "Sufficient-Rent6078",
          "text": "https://preview.redd.it/jt1mew2d2hlg1.png?width=1679&format=png&auto=webp&s=ec1edc576457fa275da7435f69f80aa1401d88cd\n\nAlways nice to see",
          "score": 72,
          "created_utc": "2026-02-24 16:49:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o763ap7",
              "author": "nunodonato",
              "text": "saner colors \n\nhttps://preview.redd.it/p3n7ubf47hlg1.png?width=3000&format=png&auto=webp&s=e916b39448da92038b6a313006b499c063c96da8\n\n",
              "score": 124,
              "created_utc": "2026-02-24 17:16:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o763qe3",
                  "author": "Sufficient-Rent6078",
                  "text": "Yeah for sure, the gray scale of the original is... certainly a choice.",
                  "score": 35,
                  "created_utc": "2026-02-24 17:18:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76z92a",
                  "author": "The_Primetime2023",
                  "text": "Sucks that theyâ€™re selectively choosing models theyâ€™re showing in each. I get that an A3B model isnâ€™t a Sonnet competitor but still weird to sometimes include it and other times leave it off",
                  "score": 17,
                  "created_utc": "2026-02-24 19:39:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76nwu7",
                  "author": "No_Swimming6548",
                  "text": "Thanks man",
                  "score": 3,
                  "created_utc": "2026-02-24 18:48:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7aafrr",
                  "author": "jax_cooper",
                  "text": "omg, thank you",
                  "score": 1,
                  "created_utc": "2026-02-25 06:53:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7evhjp",
                  "author": "triple_threat_dan",
                  "text": "My autism thanks you ðŸ˜­ I was crashing out bahahah",
                  "score": 1,
                  "created_utc": "2026-02-25 22:42:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o79fuiq",
                  "author": "Su1tz",
                  "text": "https://preview.redd.it/tvyygqv46klg1.jpeg?width=256&format=pjpg&auto=webp&s=e5b3cd754169b5636e16bcdcc5476afc5950fbba",
                  "score": 0,
                  "created_utc": "2026-02-25 03:16:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7646lj",
              "author": "lizerome",
              "text": "Also worth noting that this image is titled `qwen3.5_middle_size_score.png`. With 397B presumably being \"large\", we should still be getting a \"small\" group containing whatever they trained at the 0-15B sizes.",
              "score": 15,
              "created_utc": "2026-02-24 17:20:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76ythg",
                  "author": "Pristine-Woodpecker",
                  "text": "Looks like you are right!",
                  "score": 1,
                  "created_utc": "2026-02-24 19:37:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78jr8m",
              "author": "netherreddit",
              "text": "better colors and added glm flash, gpt 20b, and qwen3 30b\n\nhttps://preview.redd.it/6fj16cjz9jlg1.png?width=1547&format=png&auto=webp&s=d3382921131bbb1f77af4c8bdbebae57ac61cc5c",
              "score": 15,
              "created_utc": "2026-02-25 00:16:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79lj66",
                  "author": "bjodah",
                  "text": "Doing the Lord's work, thank you!",
                  "score": 1,
                  "created_utc": "2026-02-25 03:50:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7f50bc",
                  "author": "nullnuller",
                  "text": "From this it seems the Qwen3.5-35B-A3B is a good replacement for gpt-oss-20b across the board (and in some cases 120b) while matching or slightly lower in speed?",
                  "score": 1,
                  "created_utc": "2026-02-25 23:33:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75y4t1",
          "author": "danielhanchen",
          "text": "Super pumped for them! We're still converting quants - https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF and https://huggingface.co/unsloth/Qwen3.5-122B-A10B-GGUF - should be up in 1-2 hours",
          "score": 96,
          "created_utc": "2026-02-24 16:52:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o767eyj",
              "author": "newsletternew",
              "text": "One question, if I may.\nThe [model card](https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF) states:\n\"Context Length: 262,144 natively and extensible up to 1,010,000 tokens.\"\n\nAlso, the [unsloth guide](https://unsloth.ai/docs/models/qwen3.5) mentions:\n\"256K context (extendable to 1M)\"\n\nCould you add a note to the documentation explaining how to enable the 1M token context length?",
              "score": 14,
              "created_utc": "2026-02-24 17:35:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76cclm",
                  "author": "Flinchie76",
                  "text": "Look up yarn rope scaling. You can either bake this into the config in a GGUF, or pass it as a parameter to vllm. These things use rotary position encoding which can be scaled up, typically at a small cost of loss of performance on small contexts.",
                  "score": 14,
                  "created_utc": "2026-02-24 17:57:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76vtma",
                  "author": "SpicyWangz",
                  "text": "It's not the most apparent on its own, but 256 \\* 1024 = 262,144. So 256k context is the same as 262,144 tokens of context. If you ever need to configure the settings for a model and set context limit in exact token count, just take the power of two context number you've seen, and multiply it by 1024.",
                  "score": -3,
                  "created_utc": "2026-02-24 19:23:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o76cems",
              "author": "emprahsFury",
              "text": "the mmproj files are 1kb is that correct?",
              "score": 6,
              "created_utc": "2026-02-24 17:57:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78ljsi",
                  "author": "hesperaux",
                  "text": "It is not. They are much larger (800M-2G)",
                  "score": 1,
                  "created_utc": "2026-02-25 00:26:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78qvpf",
              "author": "Shensmobile",
              "text": "Hopefully Unsloth can pick up support for training them (both the text and vision side of things, I need that sweet sweet vision!) soon!  I'm in the middle of training a new Qwen3-VL model and would love to pivot to 3.5 if I could!",
              "score": 1,
              "created_utc": "2026-02-25 00:54:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o793u3o",
              "author": "ianlpaterson",
              "text": "Thanks for the fast turnaround on these. Running the 35B in production as a Slack agent on Mac Studio (\\~14 t/s, Q4\\_K\\_XL, LM Studio) - holding up well for agentic workloads.\n\n\n\nCurious on the 122B - what's the minimum VRAM/unified memory you'd expect to need for a usable quant? Wondering if 192GB unified memory gets you there.\n\n",
              "score": 1,
              "created_utc": "2026-02-25 02:08:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7ci1xr",
              "author": "shroddy",
              "text": "Would the 35b version run acceptable with 32gb system RAM and 8gb vram? Probably with Q4. Or would the context eat up the RAM too fast?",
              "score": 1,
              "created_utc": "2026-02-25 16:07:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o76gn4e",
          "author": "viperx7",
          "text": "qwen releasing so many models in local friendly sizes  \nwhat a time to be alive\n\nwe have  \n- qwen3 30B A3 Moe  \n- qwen3.5 27B  \n- qwen3.5 35B A3 Moe  \n- qwen3 32B VL  \n- qwen3 coder 80B A3 moe  \n- qwen3.5 122B A10 moe\n\nseems like thier lineup has something for everyone",
          "score": 48,
          "created_utc": "2026-02-24 18:16:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76jubs",
              "author": "DarthFader4",
              "text": "Totally agree. Very exciting time for local LLMs. And let's face it, AI bubble or not, the frontier providers are hemorrhaging cash and it's a matter of time before enshittification begins (already testing the waters with ads in openai)",
              "score": 17,
              "created_utc": "2026-02-24 18:30:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7fmdr2",
                  "author": "roosterfareye",
                  "text": "If Gemini is anything to go by, enshittification is well underway. \n\nI can't wait for the day we have 1000's of specialised, efficient, open source (or closed source with decent licensing) models and the large, closed, expensive models will have gone the way of the dinosaurs. \n\nWe are entering the pre-cambrian age of AI models right now!",
                  "score": 2,
                  "created_utc": "2026-02-26 01:09:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75zyhu",
          "author": "sleepingsysadmin",
          "text": "GPT 120b high on term bench is typically 25% or so. They say 18.7%. GPT mini at 32% is also more or less where it is.\n\nThey are claiming 35B is getting 40%.\n\nWOW I'm shocked. I'm blown away.\n\nQwen3 80b coder next is around 35%.\n\nHOW? Something significant to make 35b leap in front of 80b coder next. I CANT WAIT TO TEST!\n\nIn fact, this might be a magic model that can brain openclaw.",
          "score": 30,
          "created_utc": "2026-02-24 17:01:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o763ezj",
              "author": "sleepingsysadmin",
              "text": "https://preview.redd.it/3jt1xzru6hlg1.png?width=1024&format=png&auto=webp&s=e054392fef286c3710c6c48bf5a42647839d4acf\n\nThat blows my mind. \n\nQwen3 80b coder next is only about 18% on term bench. That is insane. ",
              "score": 27,
              "created_utc": "2026-02-24 17:16:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76kipg",
                  "author": "DigiDecode_",
                  "text": " SWE-bench verified is no longer a valid benchmark as reported recently but the terminal bench 2 scores are super impressive.",
                  "score": 10,
                  "created_utc": "2026-02-24 18:33:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o764vay",
              "author": "petuman",
              "text": "While Coder variant was released this month, Qwen3-Next it's based on is 5 months old",
              "score": 12,
              "created_utc": "2026-02-24 17:23:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78ay2s",
                  "author": "Faktafabriken",
                  "text": "5 monthsâ€¦.things are moving FAST!",
                  "score": 3,
                  "created_utc": "2026-02-24 23:27:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o76ti10",
              "author": "sleepingsysadmin",
              "text": "First test llama latest and qwen code. Lmstudio didnt work. Only getting 40TPS in llama. LM studio im expecting 70-80 TPS. \n\nIt's smart but oddly it's failing at my first test in practically the same way as glm flash for me. ",
              "score": 1,
              "created_utc": "2026-02-24 19:13:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7772av",
              "author": "Far-Low-4705",
              "text": "the reasoning content looks FAR more structured in the new models, and it is also generating 5k tokens for the prompt \"write a short story\"\n\nSomething definitely changed for their RL training",
              "score": 1,
              "created_utc": "2026-02-24 20:15:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o76b5sl",
          "author": "clyspe",
          "text": "I thought for sure the 35b was going to be the play, but that dense 27b looks incredible for its size, plus I could reasonably run it q8 at full context. Is there a convincing use case for the 35b on a 5090? It seems like a lot of the vision and reasoning benchmarks favor the 27b, with a slight edge to spatial reasoning for the 35b.",
          "score": 28,
          "created_utc": "2026-02-24 17:51:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76e5l8",
              "author": "lizerome",
              "text": "Dense should always beat MoE at similar sizes, it would be shocking if it didn't.\n\nGiven how close the two of them are in terms of benchmark scores, it probably comes down to whichever one is least harmed by having to be quantized down to your specific memory budget (e.g. is Q6 27B better than Q4 35B), and whether you value accuracy (no mistakes, no bugs, 1 shot) vs throughput (analyze these 1,000,000 documents over the next 20 hours).\n\nIf you can fit the 27B at near full precision and don't need the extra speed, then I'd pick that every time. People mostly seem to be excited about the 30B-ish MoEs because they can run them in RAM rather than VRAM, and still get acceptable speeds that way.",
              "score": 27,
              "created_utc": "2026-02-24 18:05:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76gonv",
                  "author": "silenceimpaired",
                  "text": "I think itâ€™s interesting how close 27b is to the 120b MoE. Iâ€™ve always felt like 120b MoE ~ 30b  dense and 250b ~ 70b dense.",
                  "score": 14,
                  "created_utc": "2026-02-24 18:16:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77kfhx",
                  "author": "No-Refrigerator-1672",
                  "text": "I was frequently running 30B MoE on 40gb VRAM setup just because it's KV cache is more efficient, and it allows processing of multiple 30k-long sequences in parallel - which is a game changer for agentic workflows.",
                  "score": 1,
                  "created_utc": "2026-02-24 21:17:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o76gcjb",
              "author": "tarruda",
              "text": "MoE is great for strix halo and apple silicon. For the 5090 you might get better value from the 27b (which seems to be almost as good as the 122B MoE)",
              "score": 9,
              "created_utc": "2026-02-24 18:14:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78qbp9",
                  "author": "SkyFeistyLlama8",
                  "text": "Great for any unified RAM system which would include almost all modern laptops. I was already getting something like 30 t/s on Qwen Coder 30B on ARM CPU inference on Snapdragon X. Qwen Coder Next 80B gets around 10 t/s but I reserve it for higher level coding problems because it takes up so much RAM.",
                  "score": 2,
                  "created_utc": "2026-02-25 00:51:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o76crho",
              "author": "AloneSYD",
              "text": "definitely 35b will be much faster during inference MoE > Dense in term of speed\n\n",
              "score": 5,
              "created_utc": "2026-02-24 17:59:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76gvm1",
                  "author": "silenceimpaired",
                  "text": "I wonder if that will still be true if 27b fits into VRAM and 35b does not?",
                  "score": 2,
                  "created_utc": "2026-02-24 18:17:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o777iqb",
              "author": "Far-Low-4705",
              "text": "35b is WAY faster\n\nWhich is important for reasoning where you need to wait for 5k reasoning tokens to be generated before you even get your answer",
              "score": 5,
              "created_utc": "2026-02-24 20:17:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7h92me",
              "author": "Aggravating-Rice3458",
              "text": "How are you fitting the 27b model q8 and full context on your 5090? Can I see your command? That's wild!",
              "score": 1,
              "created_utc": "2026-02-26 07:47:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7881c7",
          "author": "TheRealMasonMac",
          "text": "Tested Qwen3.5-35B-A3B Q4 at 6G VRAM + disk (no RAM); RTX 4070 and an NVME drive. Input tokens 49950. Q8 K/V cache. 128k context.\n\n676.29 tk/s eval | 14.28 tk/s gen\n\n**With RAM offloading + 6gb VRAM:**\n\n966.61 tk/s eval | 15.75 tk/s gen\n\n**With RAM offloading + 12gb VRAM:**\n\n1194.22 tk/s eval | 39.78 tk/s gen",
          "score": 9,
          "created_utc": "2026-02-24 23:12:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78p6q3",
              "author": "Xantrk",
              "text": "Can you share your llama.cpp command? I'm very confused how you can specify vram and disk offload?",
              "score": 3,
              "created_utc": "2026-02-25 00:45:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79k420",
                  "author": "TheRealMasonMac",
                  "text": "Use the \\`--fit on\\` argument with \\`--fit-target <mb>\\` which specifies how much VRAM you want to leave untouched (itâ€™s 1024mb by default). At least for me, by default, it loads from disk (mmap). But you can disable that with \\`--no-mmap\\`",
                  "score": 1,
                  "created_utc": "2026-02-25 03:41:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o767d3f",
          "author": "HatEducational9965",
          "text": "Plus:  \n[https://huggingface.co/Qwen/Qwen3.5-27B](https://huggingface.co/Qwen/Qwen3.5-27B)",
          "score": 9,
          "created_utc": "2026-02-24 17:34:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o766xxw",
          "author": "queerintech",
          "text": "And the 27B dense model, perfect fit for 16GB vram",
          "score": 20,
          "created_utc": "2026-02-24 17:32:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76t20v",
              "author": "tmvr",
              "text": "Not with a reasonable quant. The Q4 will be on the edge of 16GB for the model alone and as this is a dense model you need to keep the weights, the KV and the context in VRAM to get proper performance. It is great for 24GB cards though.\n\nEDIT: here are the rough sizes from the unsloth guide:\n\nhttps://preview.redd.it/l8u2wev7shlg1.png?width=768&format=png&auto=webp&s=b70a809ef61612e86b676198cccc017f5ab59648",
              "score": 27,
              "created_utc": "2026-02-24 19:11:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77xby3",
                  "author": "Xantrk",
                  "text": "I'm able to run Q6 quant (29 gb in size) with my 12gb VRAM and 32gb RAM quite nicely, around 35tk/s with 80k context.\n\n--fit on --kv-unified --no-mmap --parallel 1 --temp 0.6 --top-p 0.95 --top-k 20 --min-p 0 -ub 2048 --fit-ctx 80000 --fit-target 700 --port 8001 --spec-type ngram-mod --spec-ngram-size-n 24 --draft-min 48 --draft-max 64 --mmproj ./mmproj-BF16.gguf",
                  "score": 6,
                  "created_utc": "2026-02-24 22:17:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77k3pa",
                  "author": "giant3",
                  "text": "Is this VRAM or total RAM?",
                  "score": 2,
                  "created_utc": "2026-02-24 21:16:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o76diyv",
              "author": "metigue",
              "text": "The 27B dense model looks really really good. Definitely an advantage to having more activated parameters than these MoE models",
              "score": 5,
              "created_utc": "2026-02-24 18:02:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o774qvw",
              "author": "Septerium",
              "text": "If you believe in the benchmarks, it is even better than Qwen3 VL 235b!!! What a glorious time to live ",
              "score": 5,
              "created_utc": "2026-02-24 20:05:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o76h0t5",
              "author": "jojokingxp",
              "text": "At what quant? Because q4 is definitely too big",
              "score": 5,
              "created_utc": "2026-02-24 18:17:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76qix3",
                  "author": "v01dm4n",
                  "text": "Its not a fit, but barely usable at q4 by offloading some layers to ram. I get 7-10tps with gemma 27b.",
                  "score": 2,
                  "created_utc": "2026-02-24 18:59:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77xett",
                  "author": "Xantrk",
                  "text": "I'm able to run Q6 quant (29 gb in size) with my 12gb VRAM and 32gb RAM quite nicely, around 35tk/s with 80k context.\n\n--fit on --kv-unified --no-mmap --parallel 1 --temp 0.6 --top-p 0.95 --top-k 20 --min-p 0 -ub 2048 --fit-ctx 80000 --fit-target 700 --port 8001 --spec-type ngram-mod --spec-ngram-size-n 24 --draft-min 48 --draft-max 64 --mmproj ./mmproj-BF16.gguf",
                  "score": 1,
                  "created_utc": "2026-02-24 22:18:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o76pddx",
              "author": "davidminh98",
              "text": "what quant are you using for 16GB VRAM?\n\n",
              "score": 1,
              "created_utc": "2026-02-24 18:54:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o76r70h",
              "author": "X-Jet",
              "text": "Dang, i have 12gb. How unlucky",
              "score": 1,
              "created_utc": "2026-02-24 19:02:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76vu0i",
                  "author": "lizerome",
                  "text": "There's still a 9B model coming (and possibly a 14B) which might not be far behind.",
                  "score": 6,
                  "created_utc": "2026-02-24 19:23:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76xriq",
                  "author": "mtomas7",
                  "text": "Don't get fixated on your VRAM number. How many tok/s you need to read the text? I always run Q8 of-loading some layers to CPU/RAM, and I still get decent speed.",
                  "score": 1,
                  "created_utc": "2026-02-24 19:32:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77ewra",
                  "author": "SlaveZelda",
                  "text": "I get 65 tokens per sec on 4070ti 12 GB VRAM + 64 GB CPU RAM on 35ba3b and that model is almost as good as dense 27b",
                  "score": 1,
                  "created_utc": "2026-02-24 20:52:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o76q6f9",
              "author": "v01dm4n",
              "text": "Only if accompanied by a 0.5b draft model. Else too slow.",
              "score": 0,
              "created_utc": "2026-02-24 18:58:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76tjmb",
                  "author": "Dry_Yam_4597",
                  "text": "What is a draft model?",
                  "score": 2,
                  "created_utc": "2026-02-24 19:13:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o76xjdj",
                  "author": "petuman",
                  "text": "HF model page mentions MTP, so seems like it's built-in. Not supported by llama.cpp though.",
                  "score": 1,
                  "created_utc": "2026-02-24 19:31:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76aoz9",
          "author": "Ulterior-Motive_",
          "text": "Vision too, nice!",
          "score": 8,
          "created_utc": "2026-02-24 17:49:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76kw0e",
          "author": "Comrade_Vodkin",
          "text": "Rejoice, local bros!",
          "score": 6,
          "created_utc": "2026-02-24 18:35:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7752d8",
          "author": "viperx7",
          "text": "so far i am loving this model it thinks like GLM 4.7 flash  \nis very very fast  \nperformance isn't degrading (token generation)  \ni can run q6 with full context on 36gb VRAM with some room to spare\n\nprobably multimodel\n\nran some of my local tests and its working very nicely  \ndont want to jump too quickly and say better than some of the  bigger models so quickly   \n(but it feels like they outdid them self )\n\nnext i will test the 122b one \n\ncoder version of these will be EPIC",
          "score": 7,
          "created_utc": "2026-02-24 20:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77pf4q",
          "author": "JoNike",
          "text": "Gave the mxfp4 to my optimization agent while I was working and it got there for my 5080 16gb VRAM with lot of RAM.\n\n  Optimal Config (llama.cpp)\n\n  - n-cpu-moe = 16 (24 of 40 MoE layers on GPU)\n  - 256K context, flash attention, q4_0 KV cache\n  - VRAM: ~14.8 GB idle, ~15.2 GB peak at 180K word fill\n\nPerformance\n\n  - base: 51.1 t/s\n  - 10K words (13K tok) - prompt 1,015 t/s, gen 48.6 t/s\n  - 50K words (65K tok) - prompt 979 t/s, gen 44.0 t/s\n  - 120K words (155K tok) - prompt 906 t/s, gen 35.4 t/s\n  - 180K words (233K tok) - prompt 853 t/s, gen 31.7 t/s\n\nI haven't had a chance to give a try for quality yet, curious what performances others are seeing.",
          "score": 7,
          "created_utc": "2026-02-24 21:40:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79il1y",
              "author": "AdInternational5848",
              "text": "Can you share more about your optimization agent to help the rest of us build our own?",
              "score": 3,
              "created_utc": "2026-02-25 03:32:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79loz1",
                  "author": "JoNike",
                  "text": "It's a work in progress but it look like this https://github.com/jo-nike/llm_optims\n\nBasically I use claude code on my machine that host my llama.cpp (I use Opus but no reason you can't use something local if you want, I don't have the memory bandwidth to load one model to orchestrate and the model to test) and have it go through testing multiple settings to try to find the most optimal. I have a few other tests that I'm slowly adding like tools test/needle in a haystack/speed at filled context, etc.\n\nI packaged it as a skill and keep improving it with each optimization I run through it.",
                  "score": 3,
                  "created_utc": "2026-02-25 03:50:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o774ecx",
          "author": "Septerium",
          "text": "If you look at the benchmarks it is like there is no noticeable difference between 35b and 122b versions... but in real world applications, I bet there is a world of a difference. These benchmarks are pretty much worthless... every new model seems to learn them very well before being released",
          "score": 11,
          "created_utc": "2026-02-24 20:03:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77jjna",
          "author": "aeroumbria",
          "text": "Now, I think the interesting question is \"is it finally better than gpt oss 20b when both are crammed fully into a single GPU?\"",
          "score": 5,
          "created_utc": "2026-02-24 21:13:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o794aaw",
              "author": "ianlpaterson",
              "text": "It's leaving GPT-OSS in the dust....",
              "score": 1,
              "created_utc": "2026-02-25 02:11:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o794mpa",
                  "author": "aeroumbria",
                  "text": "I hope this still holds true for folks who must use the Q2 to keep under 16GB",
                  "score": 1,
                  "created_utc": "2026-02-25 02:13:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76an25",
          "author": "mrinterweb",
          "text": "I get confused about VRAM requirements. I used to have a pretty naive correlation of billions of params roughly equals GB of VRAM, but I know there's more to it than that. The active params throws me off too. I get that active is less about how much VRAM is needed and more about faster inference because less of the model needs to be evaluated (or something like that). I have a 4090 (24GB VRAM). Is it likely this model would run well on that card? Also, does anyone know of a good VRAM estimate calculator for models?  ",
          "score": 8,
          "created_utc": "2026-02-24 17:49:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76hsas",
              "author": "lizerome",
              "text": "When all else fails, you can simply go by the filesize. Q5_K_M is 24.8 GB for the model weights alone (without the context/cache), so there's no way you're fitting that all into VRAM without leaving parts of the model in CPU RAM. Which means reduced T/s and not being able to use formats like ExLlama. Since it's a very fast MoE though, you should be able to get away with that without completely killing your performance. I know some people run them on 8GB VRAM + 32GB RAM and similarly lopsided setups, seemingly at acceptable speeds.",
              "score": 10,
              "created_utc": "2026-02-24 18:21:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79vx8s",
                  "author": "zeta-pandey",
                  "text": "Can you help me get this on my gpu poor setup? its 8gb vram + 32 gb ram. I tried offloading but the gen is abysmally slow at 2.7 tk/sec. I am very new at this so would really appreciate some help. thanks!",
                  "score": 2,
                  "created_utc": "2026-02-25 04:59:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o76iger",
              "author": "DarthFader4",
              "text": "I'd bet the dense 27B is the best option to maximize your card. But the 35B MoE is worth a shot if you want, it may have faster inference with the lower active params. \n\nIf you haven't already, create a huggingface account and you can put your system specs into your profile. Then when you browse models, it'll show you compatibility estimates for each model/quant (green to orange to red) for what will fit on your system. And same thing with LM studio, it'll give you color codes for full GPU offload, partial offload, or too big entirely.",
              "score": 10,
              "created_utc": "2026-02-24 18:24:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76p8em",
                  "author": "mrinterweb",
                  "text": "I used to see an approximation of how well a given model would perform on my hardware in the right column on a huggingface model page, but I no longer see it there. I have my hardware info entered into my profile. Maybe it moved somewhere else that I can't find.",
                  "score": 3,
                  "created_utc": "2026-02-24 18:54:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o76lrix",
              "author": "petuman",
              "text": "> I used to have a pretty naive correlation of billions of params roughly equals GB of VRAM, but I know there's more to it than that.\n\nMore or less. It's all up to quantization/compression/\"lobotomization\" level you're willing to use (model dependent, but 4bpw is generally fine, so even 2B = 1GB could be true). \n\nYou also need some memory for context and that's very dependent on model architecture, so there's no rule of thumb. Qwen3.5 is really good there, so just assume 2GB is more than enough for that model family (around 100K tokens?).\n\n> I have a 4090 (24GB VRAM). Is it likely this model would run well on that card? \n\nYup, take any quantization that results in 18-20GB weights.\n\nWith llama.cpp I'm getting ~85t/s on 3090 with Unsloth's Qwen3.5-35B-A3B-UD-Q4_K_XL:\n\n.\\llama-server.exe -m Qwen3.5-35B-A3B-UD-Q4_K_XL.gguf -c 64000 --seed 42 --temp 0.6 --top-p 0.95 --top-k 20 --min-p 0.00 --no-mmap\n\nllama-server starts web UI on 127.0.0.1:8080",
              "score": 7,
              "created_utc": "2026-02-24 18:38:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76nx3l",
                  "author": "mrinterweb",
                  "text": "Thanks for the info. It's good knowing it can run well on a 3090, also the consideration for context length for VRAM allocation is helpful too.",
                  "score": 2,
                  "created_utc": "2026-02-24 18:48:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o775a03",
                  "author": "SlaveZelda",
                  "text": "I like how we've all just started calling REAP lobotomization",
                  "score": 1,
                  "created_utc": "2026-02-24 20:07:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o777y54",
                  "author": "SpicyWangz",
                  "text": "If you can do Q5 though, that's decently better. Moving up from Q4 if you are able is generally worthwhile. Moving above Q6 rarely seems to be worth it though. It's supposed to be almost indistinguishable from Q8",
                  "score": 1,
                  "created_utc": "2026-02-24 20:19:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77i3wo",
          "author": "CodProfessional3712",
          "text": "Please donâ€™t be benchmaxxed",
          "score": 5,
          "created_utc": "2026-02-24 21:07:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77h9zf",
          "author": "Turkino",
          "text": "I'll go ahead and get this out there:  \n\"Heretic version when?\" :p  \nJ/K, I'll see if I can run that myself.",
          "score": 3,
          "created_utc": "2026-02-24 21:03:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77yev2",
          "author": "SlaveZelda",
          "text": "Hey is someone else facing issues with prompt caching on llama cpp ? It seems to be re processing on every tool call or message when it should only be reprocessing the newest / most recent bits.",
          "score": 3,
          "created_utc": "2026-02-24 22:23:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o797qyb",
              "author": "PsychologicalSock239",
              "text": "I just had reprocessing while running on qwen-code with llama.cpp",
              "score": 1,
              "created_utc": "2026-02-25 02:30:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7angcf",
                  "author": "SlaveZelda",
                  "text": "Apparently you need to remove vision/mmproj for now to fix propt caching.\n\nWill be fixed later.",
                  "score": 1,
                  "created_utc": "2026-02-25 08:52:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77u8q7",
          "author": "SlaveZelda",
          "text": "I'm always excited for new Qwens and these will probably become my main models soon but I find it hard to believe the 35B is close to the 122B one in the knowledge benchmarks. There's a limit to the amount of world knowledge you can fit in 35B and because its a mixture of experts a lot of that 35B is repetition.",
          "score": 2,
          "created_utc": "2026-02-24 22:03:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79l7bf",
          "author": "Spanky2k",
          "text": "Minor achievement but this is the first model that I can run locally that was able to correctly answer the car wash prompt I saw someone mention on here a little while ago and it also solved the 1g space travel time prompt I often use exactly correctly it did so incredibly fast.",
          "score": 2,
          "created_utc": "2026-02-25 03:47:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79tzsc",
          "author": "AlwaysLateToThaParty",
          "text": "Hey /u/-p-e-w-, do you think that this model is suitable for creating a heretic version?  Is there anything about the architecture that you think would negate its usage?",
          "score": 2,
          "created_utc": "2026-02-25 04:45:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7a1dqb",
              "author": "-p-e-w-",
              "text": "See https://github.com/p-e-w/heretic/pull/187",
              "score": 3,
              "created_utc": "2026-02-25 05:39:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7aalsv",
                  "author": "AlwaysLateToThaParty",
                  "text": "u r da man.",
                  "score": 1,
                  "created_utc": "2026-02-25 06:54:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7a04uh",
          "author": "benevbright",
          "text": "I'm getting 25\\~30t/s on 64gb M2 Max Mac. ðŸ˜­ Not good for agentic coding at all. sad... any way to tweak the speed up?",
          "score": 2,
          "created_utc": "2026-02-25 05:29:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7a5zro",
          "author": "skinnyjoints",
          "text": "In theory if I store weights in ram, and retrieve the active 3B to VRAM could I run this model on 4gb VRAM? Iâ€™m still trying to learn how this works. Iâ€™m under the impression that this is possible but itâ€™d be very slow.",
          "score": 2,
          "created_utc": "2026-02-25 06:15:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7a9266",
          "author": "jax_cooper",
          "text": "when byteshape gguf",
          "score": 2,
          "created_utc": "2026-02-25 06:41:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o762g5a",
          "author": "charmander_cha",
          "text": "SerÃ¡ q isso funciona bem no opencode ?",
          "score": 2,
          "created_utc": "2026-02-24 17:12:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76avqn",
          "author": "Frosty_Incident_9788",
          "text": "There was no even competition for Qwen3-30B-A3B-2507, everything else was worse, but finally there is something better and again it is qwen itself",
          "score": 2,
          "created_utc": "2026-02-24 17:50:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o787y5v",
          "author": "tomakorea",
          "text": "Qwen 3.5 is still mediocre when generating european languages, even when using the 122B model. It can't compare to Gemma 3 for this task. I guess it's good at English and Chinese though.",
          "score": 2,
          "created_utc": "2026-02-24 23:12:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76t54t",
          "author": "Dry_Yam_4597",
          "text": "Omfg MY BANDWIDTH. Also my GPUs are going to work overtime.",
          "score": 1,
          "created_utc": "2026-02-24 19:11:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76t5rs",
          "author": "danigoncalves",
          "text": "Lets see if my 12GB VRAM can keep up with this one ðŸ˜‚",
          "score": 1,
          "created_utc": "2026-02-24 19:11:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o780vlb",
              "author": "New_Comfortable7240",
              "text": "I tried the 35b3A q2 in my 3060 12GB, 15t/s, coherent and answered correctly initial code challengesÂ ",
              "score": 5,
              "created_utc": "2026-02-24 22:35:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7akz2m",
                  "author": "danigoncalves",
                  "text": "Cool! Will try it myself, thanks for the info.",
                  "score": 1,
                  "created_utc": "2026-02-25 08:28:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o771jk9",
          "author": "Zestyclose839",
          "text": "Looks like Qwen and I are both struggling with English haha. From a semicolon quiz I had it make:\n\n\\> The neighbor barks because dogs bark, and the neighborÂ *owns*Â the dog!\n\nMy neighbors all own dogs but I've never heard *them* bark before. Fun model regardless.\n\nhttps://preview.redd.it/r03vimyfyhlg1.jpeg?width=2088&format=pjpg&auto=webp&s=a5fd2ac3af525bc98dd3dfec3ba2a9fe6d9bb281\n\n",
          "score": 1,
          "created_utc": "2026-02-24 19:50:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77i4p3",
          "author": "fulgencio_batista",
          "text": "It's supposed to support image/visual inputs too? I can't seem to get image inputs working with this model on LMStudio.",
          "score": 1,
          "created_utc": "2026-02-24 21:07:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77po7j",
              "author": "audioen",
              "text": "Need the mmproj file. I tried it. It wrote in exhaustive detail about the images, it seems to work very hard to understand something when given something that's complicated.",
              "score": 2,
              "created_utc": "2026-02-24 21:41:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o783yir",
                  "author": "fulgencio_batista",
                  "text": "Thank you!",
                  "score": 1,
                  "created_utc": "2026-02-24 22:51:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77yl12",
          "author": "Imakerocketengine",
          "text": "Anyone had issue with tool calling with llama.cpp ? do we need a new chat template ?",
          "score": 1,
          "created_utc": "2026-02-24 22:24:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78fm24",
          "author": "appakaradi",
          "text": "It is thinking by default. Hope it is not thinking for ever and thinking too much.",
          "score": 1,
          "created_utc": "2026-02-24 23:53:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78fo35",
          "author": "appakaradi",
          "text": "I wish I can control the thinking budget",
          "score": 1,
          "created_utc": "2026-02-24 23:54:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78fq9w",
              "author": "appakaradi",
              "text": "Also, I do not want to see the thinking tokens on the output",
              "score": 1,
              "created_utc": "2026-02-24 23:54:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o78h9ne",
          "author": "appakaradi",
          "text": "AWQ Pretty Please!!!!",
          "score": 1,
          "created_utc": "2026-02-25 00:03:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7984ca",
          "author": "zipzapbloop",
          "text": "i'm hacking around with 35b (thinking off) as a part of a pdf ocr pipeline and holy shit this thing is gooood",
          "score": 1,
          "created_utc": "2026-02-25 02:32:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7abogl",
          "author": "Dry-War-2576",
          "text": "Damnnn",
          "score": 1,
          "created_utc": "2026-02-25 07:03:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7edc3k",
          "author": "ozzeruk82",
          "text": "Here I'm running it at Q4UD (unsloth), 128k context, on a single 3090 (just! headless). Opencode from my Mac works great, I am genuinely stunned. I don't want to exaggerate but you could make a strong argument that this is as strong as Sonnet 3.5 which was SOTA only early last year.",
          "score": 1,
          "created_utc": "2026-02-25 21:15:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7wqghu",
          "author": "Full_Tomato_5627",
          "text": "I am a noob here, the only way to run this well is with 35gb of VRAM?",
          "score": 1,
          "created_utc": "2026-02-28 17:16:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77gkqp",
          "author": "Leopold_Boom",
          "text": "I'm sorry to report that this model failed a classic test for me twice in a row:\n\nIt failed \"Generate ten sentences ending in apple\" at Q4\\_K\\_M multiple times (GPT-OSS-20B gets it right).\n\nNailed some others (don't ask it to multiply 9 digit numbers unless you have a bunch of time ... but it get's the answer right!).\n\nEDIT: Obviously outcomes will vary, but I'd be surprised if you don't get a failure one in five, which is concerning. There is some issues with quants on these models, so perhaps it's an artifact of me not using the right Q4 quant.",
          "score": 0,
          "created_utc": "2026-02-24 21:00:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o786f8k",
              "author": "velcroenjoyer",
              "text": "Worked for me using the MXFP4\\_MOE Unsloth quant with 0.1 temperature (0.8 temperature fails):\n\n1. She picked the ripest fruit from the tree, which was a golden apple.\n2. For a healthy snack, he decided to eat an apple.\n3. The logo on the computer screen is a bitten apple.\n4. The teacher gave the student a shiny red apple.\n5. The fruit in the bowl was a fresh apple.\n6. The pie was made from a tart green apple.\n7. The story revolves around a poisoned apple.\n8. The recipe calls for one large apple.\n9. The color of the car was the same as an apple.\n10. The basket contained only a single apple.\n\n",
              "score": 6,
              "created_utc": "2026-02-24 23:03:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78exde",
                  "author": "Leopold_Boom",
                  "text": "Humm some of those quant KL+perpexity comparisons suggested Q4\\_K\\_M should generally be better than MXFP4, but I'll give them a shot.\n\nMy concern is that even with reasoning on (you did have reasoning on right?) it would just not catch that 1 sentence didn't end in apple. I suspect if you try even with a lot temp with a few other words, you'll see the odd slipup, which I don't see with GPT-OSS.",
                  "score": 1,
                  "created_utc": "2026-02-24 23:50:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ifkf7",
              "author": "dontquestionmyaction",
              "text": "Can't confirm; it one-shot that for me. Q4. Make sure you have rep penalty.\n\nIt did reason about this for 3000 tokens, but got it right.",
              "score": 1,
              "created_utc": "2026-02-26 13:36:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o79hrf9",
              "author": "Smart-Cap-2216",
              "text": "These strange tests have no practical significance.",
              "score": 0,
              "created_utc": "2026-02-25 03:27:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o79natc",
                  "author": "gofiend",
                  "text": "Very simple instruction following â€¦ good signal",
                  "score": 1,
                  "created_utc": "2026-02-25 04:00:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o76x7nr",
          "author": "Destroyer-128",
          "text": "Deepseek baby",
          "score": -4,
          "created_utc": "2026-02-24 19:30:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rg4zqv",
      "title": "Follow-up: Qwen3.5-35B-A3B â€” 7 community-requested experiments on RTX 5080 16GB",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1rg4zqv/followup_qwen3535ba3b_7_communityrequested/",
      "author": "gaztrab",
      "created_utc": "2026-02-27 12:09:50",
      "score": 520,
      "num_comments": 173,
      "upvote_ratio": 0.98,
      "text": "**TL;DR**: Community asked great questions on my original benchmarks post. I ran every experiment you requested. The headline: **KV q8\\_0 is confirmed free lunch, Q4\\_K\\_M remains king,** `--fit on` **without batch flags hits 74.7 tok/s (+7% over my original config), and KL divergence confirms UD-Q4\\_K\\_XL is even worse than PPL suggested.** Full results and updated launch command below.\n\n# Context\n\nAfter posting [Qwen3.5-35B-A3B quantization quality + speed benchmarks on RTX 5080 16GB](https://www.reddit.com/r/LocalLLaMA/comments/1rei65v/qwen3535ba3b_quantization_quality_speed/), you folks raised a bunch of great questions. Rather than hand-waving, I ran every experiment I could. Here's what I found.\n\n**Hardware**: RTX 5080 16GB + 128GB DDR5 + Ryzen 9 9950X (32 threads) **Software**: llama.cpp (built from source, CUDA 12.8, sm\\_120) **Base model**: Qwen3.5-35B-A3B (MoE: 256 experts/layer, top-8 + 1 shared, \\~3B active params/token)\n\n# Experiment 1: KV Cache Quality â€” Is q8_0 really \"free\"?\n\n**Requested by**: u/PhilippeEiffel, u/MrMisterShin, u/llama-impersonator, u/WittyAmbassador7340, u/kreigiron, u/bartskol\n\nFair concern â€” I claimed KV q8\\_0 was free but didn't have PPL data to back it up. Here's the full matrix:\n\n|Model Quant|KV f16|KV q8\\_0|KV q4\\_0|\n|:-|:-|:-|:-|\n|Q8\\_0|5.8831|5.8822 (-0.02%)|5.8694 (-0.23%)|\n|Q4\\_K\\_M|6.0184|5.9997 (-0.31%)|6.0422 (+0.40%)|\n\n**Verdict**: KV q8\\_0 is genuinely free. PPL differences are within noise (< 0.4%). Even KV q4\\_0 is acceptable for most use cases. The \"instant accuracy drops\" some of you reported aren't reflected in PPL metrics â€” though I acknowledge PPL may not capture all degradation modes (more on that below).\n\n**Recommendation unchanged**: Use `-ctk q8_0 -ctv q8_0` for +12-38% throughput at zero measurable quality cost.\n\n**Caveat:** These PPL tests used 512 token context. Some users report KV q8\\_0 degrading at very long contexts (40-100k tokens) where quantization errors may accumulate. If you're regularly running huge contexts, test carefully.\n\n# Experiment 2: KL Divergence â€” Does PPL tell the whole story?\n\n**Requested by**: u/JermMX5, u/Embarrassed_Ad3189\n\nu/JermMX5 cited the [Accuracy is Not All You Need paper](https://arxiv.org/abs/2407.09141) showing PPL can stay flat while token accuracy collapses. Great point. So I ran KLD against Q8\\_0 base logits (512 ctx, 80 chunks):\n\n|Quant|Mean KLD|Max KLD|Same Top-1 Token %|\n|:-|:-|:-|:-|\n|Q4\\_K\\_M|0.0282|4.2146|92.4%|\n|UD-Q4\\_K\\_XL|0.1087|7.7947|86.2%|\n\n**Verdict**: KLD *confirms and amplifies* the PPL findings. UD-Q4\\_K\\_XL is **3.9x worse** than Q4\\_K\\_M by mean KLD and only preserves the top-1 token 86.2% of the time (vs 92.4%). PPL was not misleading here â€” it correctly ranked the quants, but KLD shows the gap is even larger than PPL suggested.\n\n**Practical note**: Qwen3.5's 248K vocab makes full KLD evaluation produce enormous logit files (\\~19 GiB for 80 chunks). I used `--chunks 80` with uint16 storage which is feasible with 128GB RAM. If you have a smaller system, `--chunks 20-30` should give stable relative rankings.\n\n# Experiment 3: Bartowski Q4_K_L â€” Is the imatrix quant worth it?\n\n**Requested by**: u/bettertoknow\n\n[bartowski's Q4\\_K\\_L](https://huggingface.co/bartowski/Qwen_Qwen3.5-35B-A3B-GGUF) uses Q8\\_0 for embed/output tensors plus more q5\\_K and q6\\_K layers than Q4\\_K\\_M. Quality-wise, it's measurably better:\n\n|Metric|Q4\\_K\\_M (Unsloth)|Q4\\_K\\_L (bartowski)|Q8\\_0 (reference)|\n|:-|:-|:-|:-|\n|PPL (WikiText-2)|6.6688|6.6125 (-0.8%)|6.5342|\n|Mean KLD|0.0282|0.0181 (-36%)|â€”|\n|Same top-1 %|92.4%|94.2%|â€”|\n|File size|20 GB (4.74 BPW)|20.1 GB (4.98 BPW)|36.9 GB|\n\nBut here's the problem â€” speed:\n\n|Config|Short|Medium|Long|Multi-turn|VRAM|\n|:-|:-|:-|:-|:-|:-|\n|Q4\\_K\\_M fit-nobatch|74.7 tok/s|72.9|73.7|76.1|14559 MB|\n|**Q4\\_K\\_L fit-nobatch**|**41.4 tok/s**|**41.4**|**40.8**|**41.8**|**14489 MB**|\n\nQ4\\_K\\_L is **44% slower**. The larger q5\\_K/q6\\_K tensors (4.98 BPW vs 4.74) mean the model buffer is 8984 MiB vs Q4\\_K\\_M's 8556 MiB, causing `--fit` to overflow more expert layers to CPU (19/41 vs \\~16/41). Manual `--n-cpu-moe 24` OOMs entirely because the model buffer alone exceeds what's available after compute buffer allocation.\n\n**Verdict**: Q4\\_K\\_L has genuinely better quality (especially visible in KLD: -36%), but the speed penalty is massive on single-GPU setups where VRAM is the constraint. If your model fits fully in VRAM (5090 32GB), Q4\\_K\\_L is a strict upgrade. On 16GB cards, **Q4\\_K\\_M wins decisively**.\n\n# Experiment 4: --fit Tuning â€” Can we close the gap with manual offload?\n\n**Requested by**: u/Chromix_, u/guiopen, u/wisepal_app, u/DonkeyBonked\n\nIn my original post, `--fit on` was \\~7% slower than manual `--n-cpu-moe 24`. u/Chromix_ suggested the issue might be that `-b 4096 -ub 4096` batch flags consume VRAM that `--fit` can't then use for expert layers. **Nailed it.**\n\n|Config|Short|Medium|Long|Multi-turn|VRAM|\n|:-|:-|:-|:-|:-|:-|\n|C7 baseline (`--n-cpu-moe 24`, -b 4096)|69.6 tok/s|67.0|65.7|69.2|14874 MB|\n|fit-default (`--fit on`, -b 4096)|64.3|62.8|57.4\\*|54.2\\*|14595 MB|\n|fit-256 (`--fit-target 256`, -b 4096)|66.0|64.7|63.7|66.0|15321 MB|\n|**fit-nobatch (**`--fit on`**, no -b/-ub)**|**74.7**|**72.9**|**73.7**|**76.1**|**14559 MB**|\n\n\\*high variance with outliers\n\n**Verdict**: u/Chromix_ was right. Removing `-b 4096 -ub 4096` lets `--fit` allocate VRAM optimally for expert layers. **fit-nobatch is the new winner at \\~74 tok/s** â€” simpler config AND faster than manual tuning. `--fit-target 256` alone doesn't close the gap; removing the batch flags is the key insight.\n\n# Experiment 5: Speculative Decoding â€” Can we go faster?\n\n**Requested by**: u/BreizhNode, plus our own optimization roadmap\n\n**Bad news first**: No compatible draft model exists. Qwen3.5 has a 248K vocabulary, Qwen3 has 151K. The smallest Qwen3.5 model is 27B â€” there's no small Qwen3.5 that could serve as a draft. Draft-model speculation is a dead end for now.\n\n**So I tried self-speculative methods** (no draft model needed):\n\n|Config|Short|Medium|Long|Multi-turn|Status|\n|:-|:-|:-|:-|:-|:-|\n|fit-nobatch baseline|74.7 tok/s|72.9|73.7|76.1|â€”|\n|ngram-simple|44.9|43.4|42.9|49.1|works|\n|ngram-mod (m=64)|44.6|FAIL|FAIL|FAIL|crashes|\n|ngram-simple-short (n=8, m=64)|45.0|43.1|43.1|FAIL|partial|\n\n**Note**: ngram tests ran on a different llama.cpp build (`latest` vs `latest-fit`) that had a \\~40% regression for unrelated reasons, so the absolute numbers aren't directly comparable. But even accounting for that, there's no speedup from ngram speculation on conversational workloads.\n\n**Verdict**: Self-speculative ngram methods provide zero benefit for diverse conversational workloads. ngram-mod is unstable (crashes after first request). **Not recommended.** If Qwen releases a small Qwen3.5 model (1-3B), draft-model speculation could be huge â€” but that doesn't exist yet.\n\n# Experiment 6: Qwen3.5-27B Dense â€” MoE vs Dense on single GPU\n\n**Requested by**: u/moahmo88, u/Agreeable_Effect938\n\nSome of you asked whether the dense 27B model might be a better fit for single-GPU setups. After all, it's simpler (no expert routing) and smaller (15.6 GB Q4\\_K\\_M).\n\n|Metric|35B-A3B Q4\\_K\\_M (MoE)|27B Q4\\_K\\_M (dense)|\n|:-|:-|:-|\n|PPL (WikiText-2)|6.6688|6.8573 (+2.8%)|\n|Active params/token|\\~3B|27B|\n|File size|20 GB|15.6 GB|\n\n|Config|Short|Medium|Long|Multi-turn|VRAM|\n|:-|:-|:-|:-|:-|:-|\n|35B-A3B Q4\\_K\\_M fit-nobatch|74.7 tok/s|72.9|73.7|76.1|14559 MB|\n|**27B dense fit**|**7.4 tok/s**|**7.4**|**7.2**|**7.1**|**14075 MB**|\n\nYes, that's **10x slower**. And it has worse quality.\n\nThe dense model needs all 27B parameters computed per token vs only \\~3B active for MoE. Even with `--fit` putting 54/65 layers on GPU, the remaining 11 layers on CPU create a massive bottleneck. Theoretical max even fully on GPU: \\~61 tok/s (960 GB/s Ã· 15.6 GB model).\n\n**Verdict**: The MoE architecture is the entire advantage on consumer hardware. Only \\~3B active params per token means \\~10x less memory bandwidth per token. The 35B-A3B MoE is vastly faster on single-GPU setups with limited VRAM. The 27B dense is the stronger model on capability benchmarks and instruction following â€” if you can fit it fully in VRAM (24GB+ cards), it's a great choice. On 16GB cards where it runs at 7 tok/s, it's not practical for interactive use.\n\n# Experiment 7: MXFP4_MOE â€” The Unsloth-recommended alternative\n\n**Requested by**: u/ayylmaonade, u/jumpingcross, u/danielhanchen (Unsloth creator)\n\nAfter u/danielhanchen confirmed UD-Q4\\_K\\_XL has issues and specifically recommended MXFP4 as the alternative, I ran both quality and speed benchmarks.\n\n**Quality** (partial â€” MXFP4 dequant path has a memory leak that OOMs after \\~40-50 chunks):\n\n|Metric|Q4\\_K\\_M|MXFP4\\_MOE|UD-Q4\\_K\\_XL|\n|:-|:-|:-|:-|\n|PPL (\\~40 chunks)|\\~6.00|\\~5.9-6.2\\* (the PPL runs all crashed due to memory leak, 5.96 is unverifiable)|\\~7.17|\n|Mean KLD (31 chunks)|0.028|0.050|0.109|\n|Same top-1 %|92.4%|91.0%|86.2%|\n|File size|21.2 GB|18.4 GB|19.8 GB|\n\n**Speed**:\n\n|Config|Short|Medium|Long|Multi-turn|VRAM|\n|:-|:-|:-|:-|:-|:-|\n|Q4\\_K\\_M fit-nobatch|74.7 tok/s|72.9|73.7|76.1|14559 MB|\n|**MXFP4\\_MOE fit-nobatch**|**49.5 tok/s**|**47.8**|**46.9**|**43.0**|**14531 MB**|\n\n**Verdict**: MXFP4\\_MOE has comparable PPL to Q4\\_K\\_M (\\~5.9-6.2 vs 6.00, though partial evaluation due to memory leak) but is **34-42% slower** (\\~47 tok/s vs \\~74 tok/s). Despite the smaller file size (18.4 vs 21.2 GB), it doesn't translate to more expert layers on GPU â€” VRAM usage is nearly identical. There's also a memory leak bug in the MXFP4 dequant path that prevents full perplexity evaluation. **Not recommended over Q4\\_K\\_M** â€” the quality gain is marginal while the speed loss is massive.\n\nu/danielhanchen â€” if the Unsloth team has different results on MXFP4 speed, I'd love to compare notes. My build is llama.cpp b8149 with CUDA 12.8 on sm\\_120.\n\n# Research Findings\n\nA few questions didn't need experiments, just digging:\n\n# Why is Ollama 3x slower? (u/InternationalNebula7)\n\n**Ollama has no MoE expert offloading.** When a MoE model doesn't fit in VRAM, Ollama splits at the layer level â€” entire transformer blocks go to CPU or GPU. This means the GPU sits completely idle waiting for CPU layers. With expert-only offloading, attention/norms stay on GPU while only routed expert FFNs go to CPU â€” the GPU stays busy.\n\nThere's [an open PR (ollama/ollama#12333)](https://github.com/ollama/ollama/pull/12333) to add `num_moe_offload` but it hasn't merged yet. On top of that, Ollama defaults to KV cache f16 (we use q8\\_0, +20% throughput) and doesn't expose batch size or flash attention controls.\n\n# Pre-built binaries vs source for Blackwell (u/wisepal_app)\n\nFor **RTX 50-series**: building from source matters. Release binaries use CUDA 12.4 which doesn't include sm\\_120 (Blackwell). You need CUDA 12.8+ for native support. Without it, PTX from sm\\_89 (Ada) gets JIT-compiled â€” slower first launch and you miss Blackwell-specific kernels.\n\nFor **RTX 30/40-series**: pre-built is fine (0-5% difference). Those architectures are already in the release builds.\n\n# 8 GB VRAM recommendations (u/Qxz3)\n\nUse Q4\\_K\\_M with full expert offload (`-ot \"exps=CPU\"`): \\~7.2 GB VRAM, \\~50 tok/s in our tests (on RTX 5080 â€” your results will vary depending on GPU memory bandwidth). Key flags: `-ctk q8_0 -ctv q8_0` (free lunch), `-fa on`, `--no-mmap`, and tune your thread count (try `physical_cores / 1.5` as starting point, sweep from there).\n\n# Updated Launch Command\n\nBased on everything above, here's the new recommended config. Simpler AND faster than my original post:\n\n    ./llama-server \\\n      -m ./Qwen3.5-35B-A3B-Q4_K_M.gguf \\\n      -c 65536 \\\n      --fit on \\\n      -fa on \\\n      -t 20 \\\n      --no-mmap \\\n      --jinja \\\n      -ctk q8_0 \\\n      -ctv q8_0\n\n**What changed from the original post**:\n\n* Removed `-ngl 999 --n-cpu-moe 24` â†’ replaced with `--fit on` (auto VRAM management)\n* Removed `-b 4096 -ub 4096` â†’ this was the key insight from u/Chromix_ â€” batch flags eat VRAM that `--fit` needs for expert layers\n* Result: **74.7 tok/s** (up from 69.6), simpler config, and `--fit` adapts automatically to your available VRAM\n\n# Summary Table\n\n|What|Result|Verdict|\n|:-|:-|:-|\n|KV q8\\_0 quality|< 0.4% PPL difference|**Free lunch. Use it.**|\n|KLD: Q4\\_K\\_M vs UD-Q4\\_K\\_XL|0.028 vs 0.109 (3.9x worse)|**UD-Q4\\_K\\_XL is bad for MoE**|\n|Bartowski Q4\\_K\\_L|\\-0.8% PPL, -36% KLD, but 44% slower|**Not worth it on 16GB**|\n|`--fit` without batch flags|74.7 tok/s (+7% over manual)|**New best config**|\n|ngram self-speculation|No speedup, unstable|**Don't bother**|\n|27B dense vs 35B-A3B MoE|10x slower, worse quality|**MoE wins completely**|\n|MXFP4\\_MOE|Marginal quality gain, 34-42% slower|**Q4\\_K\\_M still best**|\n\n# Acknowledgments\n\nThanks to everyone who pushed for better data:\n\n* u/PhilippeEiffel, u/MrMisterShin, u/llama-impersonator, u/WittyAmbassador7340, u/kreigiron, u/bartskol â€” KV cache quality concerns led to the full PPL matrix (E1)\n* u/JermMX5, u/Embarrassed_Ad3189 â€” pushed for KLD over PPL, which revealed the UD-Q4\\_K\\_XL gap is worse than PPL showed (E2)\n* u/bettertoknow â€” Bartowski Q4\\_K\\_L benchmark, good call even though it turned out too slow for our setup (E3)\n* u/Chromix_, u/guiopen, u/wisepal_app, u/DonkeyBonked â€” `--fit` tuning, especially Chromix\\_'s insight about batch flags eating VRAM, which gave us the new fastest config (E4)\n* u/BreizhNode â€” speculative decoding investigation, saved others the trouble (E5)\n* u/moahmo88, u/Agreeable_Effect938 â€” 27B dense comparison, definitively answered \"is MoE worth the complexity?\" (E6)\n* u/ayylmaonade, u/jumpingcross, u/danielhanchen â€” MXFP4\\_MOE testing, important to validate the Unsloth creator's recommendation (E7)\n* u/InternationalNebula7 â€” Ollama performance gap explanation\n* u/Qxz3 â€” 8GB VRAM config guidance\n* u/JoNike â€” original RTX 5080 partial offload data that informed our testing\n* u/3spky5u-oss â€” comprehensive RTX 5090 head-to-head benchmarks\n* u/catplusplusok, u/SlimeQ, u/guiopen â€” chat template and tool calling tips\n* u/chickN00dle, u/Odd-Ordinary-5922 â€” KV cache sensitivity reports at long context\n* u/TheRealMasonMac â€” `--fit on` documentation and RTX 4070 results\n* u/pmttyji, u/Subject-Tea-5253 â€” batch/ubatch tuning data\n* u/Pristine-Woodpecker â€” independent confirmation of UD-Q4\\_K\\_XL quality issues\n* u/jslominski, u/jiegec, u/Corosus, u/DeedleDumbDee, u/Monad_Maya, u/l33t-Mt, u/kkb294, u/zmanning, u/Additional-Action566 â€” speed reports across different GPUs\n\nAll raw data (benchmark JSONs, PPL logs, KLD logs, config files) is in [my llm-server repo](https://github.com/gaztrabisme/llm-server) for anyone who wants to reproduce or verify.\n\n**Edit**: [Previous post here](https://www.reddit.com/r/LocalLLaMA/comments/1rei65v/qwen3535ba3b_quantization_quality_speed/). This is a follow-up with all the experiments you requested.\n\n**Edit 2:** Corrected some numbers that had errors in the original post. None of the conclusions change:\n\n\\- E2 (KLD): Max KLD values were wrong â€” Q4\\_K\\_M is 4.21 (not 0.19), UD-Q4\\_K\\_XL is 7.79 (not 1.22). This actually makes UD-Q4\\_K\\_XL look worse than originally stated.\n\n\\- E5 (Speculative): ngram-simple multi-turn was 49.1 tok/s (not 51.3). Still no benefit.\n\n\\- E7 (MXFP4): Mean KLD is 0.050 (not 0.037), PPL is \\~5.9-6.2 (partial, memory leak crashed all full runs), multi-turn speed is 43.0 tok/s (not 44.1). Still not recommended over Q4\\_K\\_M.\n\n**Edit 3:** THANK YOU FOR THE AWARD, RANDOM CITIZEN!\n\n**Edit 4:** Updated E6 (27B dense) wording â€” several commenters correctly pointed out that calling 27B \"worse quality\" based on PPL alone is misleading. The 27B dominates on capability benchmarks and instruction following; my results only show it's 10x slower on 16GB VRAM where it can't fit fully on GPU. If you have a 24GB+ card and can load it entirely in VRAM, 27B is a great model.\n\nAdded caveat to E1 (KV q8\\_0) that my PPL tests used 512 token context â€” some users report degradation at very long contexts (40-100k+).\n\nClarified that the \\~50 tok/s 8GB VRAM number (E5 C5 full offload config) was on RTX 5080, not a separate 8GB card â€” a 3060 12GB will see lower numbers due to lower memory bandwidth.\n\nThanks u/_-_David, u/ArckToons, u/Front_Eagle739, and u/cookieGaboo24.\n\n**Edit 5:** u/Corosus found --fit on performs poorly on Vulkan backend (13 tok/s vs 33 tok/s with manual --n-cpu-moe 24 on a 5070 Ti). My --fit results are CUDA-specific â€” Vulkan users should stick with manual offloading. Thanks man!\n\n**Edit 6:** THANK YOU ANOTHER CITIZEN OF SUPER EARTH FOR THE AWARD!\n\n**Edit 7:** Thanks to the community overwhelming reactions, and suggestions. I will definitely conduct another round of experiments to gather more data. Also...\n\nOMG GUYS THANKS FOR THE AWARDS!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rg4zqv/followup_qwen3535ba3b_7_communityrequested/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o7pxmc6",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-27 16:00:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ou13m",
          "author": "nikhilprasanth",
          "text": "Incredible work. The fact that KV `q8_0` is essentially a free lunch even under PPL scrutiny is going to save a lot of VRAM. Itâ€™s also interesting to see MXFP4 struggle with speed despite the Unsloth recommendation.",
          "score": 35,
          "created_utc": "2026-02-27 12:22:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7q22nl",
              "author": "simracerman",
              "text": "Yeah, but the tested context being at 512 tokens is unrealistic. This model is touted as a good coder, and your typical coding agent dumps 10k tokens to start with, youâ€™re gonna find that â€œfreeâ€ claim vanish quickly.",
              "score": 17,
              "created_utc": "2026-02-27 16:21:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7oz228",
          "author": "danielhanchen",
          "text": "Awesome work! We're actually going to post our results soon in a few hours hopefully - we just did! https://www.reddit.com/r/LocalLLaMA/comments/1rgel19/new_qwen3535ba3b_unsloth_dynamic_ggufs_benchmarks/ - for those interested we tried over 120 different variants and all are posted here: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF",
          "score": 49,
          "created_utc": "2026-02-27 12:57:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p05w1",
              "author": "gaztrab",
              "text": "Thanks so much Daniel!",
              "score": 11,
              "created_utc": "2026-02-27 13:04:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7qskvq",
                  "author": "danielhanchen",
                  "text": "As an update - https://www.reddit.com/r/LocalLLaMA/comments/1rgel19/new_qwen3535ba3b_unsloth_dynamic_ggufs_benchmarks/ :)",
                  "score": 5,
                  "created_utc": "2026-02-27 18:26:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7qmwfu",
              "author": "IrisColt",
              "text": "I kneel...",
              "score": 1,
              "created_utc": "2026-02-27 17:59:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7otcy7",
          "author": "No_Swimming6548",
          "text": "Thanks man",
          "score": 22,
          "created_utc": "2026-02-27 12:17:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ox2pn",
          "author": "Live-Crab3086",
          "text": "very helpful, thorough analysis. thank you!\n\nanyone willing to speculate if the UD-Q4\\_K\\_XL vs Q4\\_K\\_M results carry over to UD-Q5\\_K\\_XL vs Q5\\_K\\_M?",
          "score": 13,
          "created_utc": "2026-02-27 12:44:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7oyrc7",
              "author": "gaztrab",
              "text": "Likely yes. danielhanchen (Unsloth creator) confirmed the issue is with how UD dynamic quantization handles MoE expert layers in general â€” it's not specific to the Q4 tier. The standard quant scheme preserves expert structure better. So Q5\\_K\\_M should be safer than UD-Q5\\_K\\_XL for MoE models, same pattern as Q4.",
              "score": 8,
              "created_utc": "2026-02-27 12:55:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7p1o0h",
                  "author": "xrvz",
                  "text": "\"how UD dynamic quantization handles MoE expert layers in general\" â€“ for *all* models?\n\nI personally only use MoE at this point for serious work, and that'd be a death sentence for UD; they'd have to do some necromancy to undo that.\n\nEdited: words, not meaning.",
                  "score": 3,
                  "created_utc": "2026-02-27 13:13:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7p44b4",
                  "author": "Constant-Simple-1234",
                  "text": "So the recommendation is: Unsloth's UD for dense models, but regular Q4_K_M for MoE ?",
                  "score": 1,
                  "created_utc": "2026-02-27 13:27:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o80q2lb",
                  "author": "yoracale",
                  "text": "It only affected Q2\\_X\\_XL, Q3\\_X\\_XL and Q4\\_X\\_XL and no other quant.\n\nAlso if you didn't see we did an update: [https://www.reddit.com/r/LocalLLaMA/comments/1rgel19/new\\_qwen3535ba3b\\_unsloth\\_dynamic\\_ggufs\\_benchmarks/](https://www.reddit.com/r/LocalLLaMA/comments/1rgel19/new_qwen3535ba3b_unsloth_dynamic_ggufs_benchmarks/)",
                  "score": 1,
                  "created_utc": "2026-03-01 07:55:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o80q37s",
              "author": "yoracale",
              "text": "It only affected Q2\\_X\\_XL, Q3\\_X\\_XL and Q4\\_X\\_XL and no other quant.\n\nAlso if you didn't see we did an update so all should now be fixed: [https://www.reddit.com/r/LocalLLaMA/comments/1rgel19/new\\_qwen3535ba3b\\_unsloth\\_dynamic\\_ggufs\\_benchmarks/](https://www.reddit.com/r/LocalLLaMA/comments/1rgel19/new_qwen3535ba3b_unsloth_dynamic_ggufs_benchmarks/)",
              "score": 1,
              "created_utc": "2026-03-01 07:55:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7oy890",
          "author": "Ancient_Routine8576",
          "text": "The data on KV q8\\_0 being effectively free in terms of perplexity loss is a huge relief for anyone trying to squeeze maximum performance out of a 16GB buffer. It is interesting to see that the instant accuracy drops some users reported are not reflecting in the PPL metrics as that suggests those degradations might be very task specific. Thanks for running these follow up experiments because this level of granular detail is exactly what makes the local LLM community so valuable. I am definitely bookmarking this matrix for my next fine tuning project.",
          "score": 9,
          "created_utc": "2026-02-27 12:51:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7p1l5v",
          "author": "Front_Eagle739",
          "text": "ime q8 kv is a non issue till you have huge contexts and then somehow it falls apart faster than the full 16 bit ones. Seems to exacerbate that cliff where the model starts forgetting things that happened 40-100k tokens ago. At least on glm 4.6 where I did my testing with it",
          "score": 9,
          "created_utc": "2026-02-27 13:13:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p9lm1",
              "author": "gaztrab",
              "text": "Important nuance â€” my E1 tests used 512 token context windows, so I can't speak to 40-100k behavior. It's plausible that quantization errors accumulate over very long sequences in a way that short-context PPL doesn't capture. If you're running huge contexts regularly, that's worth being cautious about. I'll add a caveat to the post.",
              "score": 7,
              "created_utc": "2026-02-27 13:58:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7q876l",
                  "author": "Digger412",
                  "text": "I think that's worth more than a caveat, honestly. Measuring on 512 ctx is very small and I'd argue that you'd want to test on a sweep of contexts like with a Needle on a Haystack bench. I've noticed the same thing that quantized KV cache barely impacts KLD at 512 ctx and my conclusion wasn't that it's a free lunch but rather it's either not easily measurable with a KLD test or the default 512 isn't enough to make a measurable difference. Maybe try running KLD with 4k, 8k, 16k, and 32k --ctx-size?\n\n\nThanks for this post and running these tests!",
                  "score": 8,
                  "created_utc": "2026-02-27 16:49:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7p3h4f",
          "author": "a_beautiful_rhind",
          "text": "You can also quant the K and V separate. One of them is responsible for the big hit more than the other. IK_llama has a q_6 and hadamard transforms for K. There's more squeezing if you try.",
          "score": 8,
          "created_utc": "2026-02-27 13:24:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p9j13",
              "author": "gaztrab",
              "text": "Good tip â€” I quantized K and V together at q8\\_0 but didn't test them asymmetrically. If one dimension is more sensitive than the other, you could potentially push the tolerant one to q4\\_0 while keeping the sensitive one at q8\\_0. More VRAM savings without the quality hit. Something to test in a future round.",
              "score": 6,
              "created_utc": "2026-02-27 13:58:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7ph738",
                  "author": "a_beautiful_rhind",
                  "text": "Yes the note is in the original PR for quantized cache on which one is hurt more. I think it's the K but I guess you can also empirically verify it. You might have to compile l.cpp with fa_all_quants",
                  "score": 3,
                  "created_utc": "2026-02-27 14:39:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ox67h",
          "author": "Single_Ring4886",
          "text": "Thats what I call thorough testing :)",
          "score": 5,
          "created_utc": "2026-02-27 12:44:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7p6x6l",
          "author": "_-_David",
          "text": "I think it's a clear mistake to claim that the 27b dense model is \"worse quality\" based on 2% higher ppl. You might say it degrades more quickly, perhaps. But in benchmarks the 27b absolutely dominates the 35b. I get that this post is from the perspective of \"If you have a 16gb GPU, this is what you should choose\" but you could either make that more explicitly clear in similar future posts, or not lean so heavily on disparaging the 27b.\n\nWith that said, I applaud your diligence and assistance to the community. This was a very well put together post and I appreciate it. I went to download bartowski's Q4\\_K\\_L model instantly on your recommendation, and I'll be eating my free KV lunch at q8 thanks to you. It just felt a bit odd to see my new favorite model, the 27b dense that I'm running fully in VRAM, tossed to the side and spat upon. Which again, is totally fair if we're talking a 5080 User's Guide! If the title of the post had been that, I think I wouldn't have noticed.",
          "score": 5,
          "created_utc": "2026-02-27 13:43:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p8zhb",
              "author": "gaztrab",
              "text": "\"Worse quality\" was sloppy framing on my part. PPL on WikiText-2 measures one narrow thing; the 27B dominates on actual capability benchmarks and instruction following. What I should have said is: on a 16GB GPU where the 27B runs at 7 tok/s vs 75 tok/s for the MoE, the speed difference makes it impractical for interactive use. But if you can fit 27B fully in VRAM (4090, 5090), it's arguably the better model. I'll update the wording. Thanks!",
              "score": 3,
              "created_utc": "2026-02-27 13:55:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7p8odn",
          "author": "theghost3172",
          "text": "\"The 35B-A3B MoE dominates on both speed AND quality\"\n\nthat is not true. you cant compare different llms with perplexity. different llms have different distributions so they will have different perplexity irrespective of quality. and Moe will always have lower quality than dense. but ofc its much faster.\n\nbut overall excellent work",
          "score": 7,
          "created_utc": "2026-02-27 13:53:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pa8iq",
              "author": "gaztrab",
              "text": "Yeah you're right, what I should have said was \"MoE dominates on both speed and quality for 16GB VRAM\". Thanks anyway!",
              "score": 2,
              "created_utc": "2026-02-27 14:02:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ovwww",
          "author": "Pawderr",
          "text": "Can someone please explain what this means? I just started with local llmsÂ ",
          "score": 5,
          "created_utc": "2026-02-27 12:36:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7owhbu",
              "author": "gaztrab",
              "text": "I'm testing the best way to run a large AI model (Qwen3.5-35B-A3B) on a single gaming GPU (RTX 5080, 16GB). The model is too big to fit entirely in the GPU, so parts of it run on the CPU â€” finding the optimal split is what most of these experiments are about.\n\n\n\nIf you're just getting started, the takeaway is:\n\n  1. Model: Qwen3.5-35B-A3B with Q4\\_K\\_M quantization (a way to compress the model so it fits)\n\n  2. Engine: [https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp) â€” free, open source, runs on any NVIDIA GPU\n\n  3. Key settings: --fit on, -ctk q8\\_0 -ctv q8\\_0, -fa on, and do NOT add -b 4096 -ub 4096\n\n\n\nThis gets \\~75 tokens/second, which is faster than most people read.",
              "score": 30,
              "created_utc": "2026-02-27 12:40:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7oxz8j",
                  "author": "uxl",
                  "text": "As a guy with a 5080 mobile and 64GB ram, thanks! Is llamcpp better than kobold?",
                  "score": 6,
                  "created_utc": "2026-02-27 12:50:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7p8sbb",
                  "author": "Pawderr",
                  "text": "These settings are only for text i suppose? Would it work for video related tasks?",
                  "score": 1,
                  "created_utc": "2026-02-27 13:54:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7pi3up",
                  "author": "hsoj95",
                  "text": "Based on all this, I guess it's time for me to switch from Ollama to Llama.cpp? Any advice for someone switching, been using the Docker for Ollama.",
                  "score": 1,
                  "created_utc": "2026-02-27 14:44:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7pzh3d",
                  "author": "LostDrengr",
                  "text": "I have been embarking on the same also having a 5080. I have used docker desktop to offer vLLM and using the Tensorrt option, with the latter exploring the NVFP4 for increased speed. I also use LM Studio but have forgot about the ollama route.\n\nWith these new models being huge its trying to get the quantised flavour that can run well, so this is really helpful appreciate all the 16GB coverage!",
                  "score": 1,
                  "created_utc": "2026-02-27 16:08:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7oupow",
          "author": "Corosus",
          "text": "absolutely amazing insight tysm, gonna use fit that way and try that quant",
          "score": 3,
          "created_utc": "2026-02-27 12:27:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ova44",
          "author": "kaeptnphlop",
          "text": "Very insightful! Thank you for testing this out. Thatâ€™s a lot of work!",
          "score": 3,
          "created_utc": "2026-02-27 12:31:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ow3le",
          "author": "prescorn",
          "text": "Nice work! This will be useful for some of my 96GB experiments on the weekend.",
          "score": 3,
          "created_utc": "2026-02-27 12:37:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7p3xys",
          "author": "ArckToons",
          "text": "Great tests with a lot of useful conclusions. I disagree with â€œThe 27B dense is only worth considering if you need a non-MoE model for compatibility reasons.â€ I donâ€™t think itâ€™s only about compatibility, but about use cases.\n\n\nIf you need speed, 35B is the right call. But if you want more quality (even though in most use cases the quality is similar), better instruction-following, and more predictable behavior, 27B seems like the better choice.\n\n\nIn my case, I have an RTX 4090 and I run it with OpenCode. I tested both 27B Q4_KM and 35B Q4_KM, and the 27B did better with my orchestrator/sub-agent setup. Iâ€™m not saying 27B is objectively superiorâ€”this depends on the use case and whether slower inference is acceptableâ€”but I donâ€™t think the decision comes down to compatibility.\n\n\nOne question: does KV quantization affect KL? Would it be worth running a test, or not?",
          "score": 3,
          "created_utc": "2026-02-27 13:26:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p94v0",
              "author": "gaztrab",
              "text": "Agreed with your take on 27B â€” see my reply to \\_-\\_David above, same point applies. On a 4090 with full VRAM headroom, 27B is a totally valid choice, especially for agentic workflows where quality matters more than tok/s.\n\n\n\nOn your KV quant + KLD question: I tested KV quant impact on PPL (E1) but didn't run KLD specifically across KV quant levels. PPL showed < 0.4% difference between f16/q8\\_0/q4\\_0, so I'd expect KLD to be similarly minimal â€” but that's an assumption, not data. Worth testing if someone wants to be thorough.",
              "score": 3,
              "created_utc": "2026-02-27 13:56:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7p91qa",
          "author": "Corosus",
          "text": "Some quick testing, using --fit for me tanks performance, -ngl 999 --n-cpu-moe 24 works best on my pc, 5070 ti (other gpus disabled), 128gb ddr4 3200mhz. Maybe because I'm still using vulkan.\n\nI guess this goes to show theres no universal solution, gotta find out what works best for your hardware:\n\n\tllama-b8173-bin-win-vulkan-x64\\llama-server --model ./e/Qwen3.5-35B-A3B-Q4_K_M.gguf --host 0.0.0.0 --port 8080 -ctk q8_0 -ctv q8_0 -ngl 999 --n-cpu-moe 24 --flash-attn on --jinja -c 48000 -t 20\n\n\t-ngl 999 --n-cpu-moe 24\n\n\t33tps\n\t\t\n\tllama_memory_breakdown_print: | memory breakdown [MiB]    | total   free     self   model   context   compute    unaccounted |\n\tllama_memory_breakdown_print: |   - Vulkan0 (RTX 5070 Ti) | 15907 = 4641 + (10162 =  8845 +     750 +     566) +        1103 |\n\tllama_memory_breakdown_print: |   - Host                  |                 12033 = 11931 +       0 +     102                |\n\t\t\n\n\n\t\t\n\tllama-b8173-bin-win-vulkan-x64\\llama-server --model ./e/Qwen3.5-35B-A3B-Q4_K_M.gguf --host 0.0.0.0 --port 8080 -ctk q8_0 -ctv q8_0 --fit on --flash-attn on --jinja -c 48000 -t 20\n\n\t--fit on\n\n\t13 tps\n\t\t\n\tllama_memory_breakdown_print: | memory breakdown [MiB]    | total   free     self   model   context   compute    unaccounted |\n\tllama_memory_breakdown_print: |   - Vulkan0 (RTX 5070 Ti) | 15907 =  890 + (13825 = 12574 +     750 +     501) +        1190 |\n\tllama_memory_breakdown_print: |   - Host                  |                 19916 = 19814 +       0 +     102                |\n\t\t\n\n\n\n\tllama-b8173-bin-win-vulkan-x64\\llama-server --model ./e/Qwen3.5-35B-A3B-Q4_K_M.gguf --host 0.0.0.0 --port 8080 -ctk q8_0 -ctv q8_0 --fit on -ot \"exps=CPU\" --flash-attn on --jinja -c 48000 -t 20\n\n\t--fit on -ot \"exps=CPU\"\n\n\t24tps\n\n\tllama_memory_breakdown_print: | memory breakdown [MiB]    | total    free     self   model   context   compute    unaccounted |\n\tllama_memory_breakdown_print: |   - Vulkan0 (RTX 5070 Ti) | 15907 = 12152 + ( 2656 =  1339 +     750 +     566) +        1098 |\n\tllama_memory_breakdown_print: |   - Host                  |                  19916 = 19814 +       0 +     102                |\n\n\n\nI also reran the --fit on test with b8149, same slow result\n\nedit: realized i forgot --no-mmap to go with --fit on, prompt intake is still insanely slow so tps is likely also slow",
          "score": 3,
          "created_utc": "2026-02-27 13:55:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pa2ml",
              "author": "gaztrab",
              "text": "Very interesting. I will note your result down, thanks for sharing!",
              "score": 2,
              "created_utc": "2026-02-27 14:01:07",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7pby7t",
              "author": "gaztrab",
              "text": "Looking at your memory breakdowns, --fit on is allocating 12574 MiB model to GPU vs 8845 MiB with manual --n-cpu-moe 24 â€” but that extra GPU allocation makes it slower (13 vs 33 tok/s). That strongly suggests --fit isn't optimizing well for Vulkan â€” it's probably tuned for CUDA compute characteristics and over-loading the GPU with layers that would run faster on CPU via your DDR4. Your manual --n-cpu-moe 24 is the right call for Vulkan setups. I'll add a note to the post that --fit results are CUDA-specific and Vulkan users should stick with manual offloading.",
              "score": 2,
              "created_utc": "2026-02-27 14:11:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7q0385",
          "author": "Danmoreng",
          "text": "I believe with â€”fit you should also use â€”fit-ctx instead of just â€”c. Also, if you want to use the vision capability of the model, you have to either put the vision model on CPU or use â€”fit-target 1536 to leave space for the vision part on GPU. I am running on very similar settings on my notebook with a 5080 mobile and can confirm initially having 74 t/s, for longer context it then falls of to around 66 t/s.\n\nMy server configuration can be found here: https://github.com/Danmoreng/local-qwen3-coder-env?tab=readme-ov-file#server-optimization-details",
          "score": 3,
          "created_utc": "2026-02-27 16:11:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7q1ipa",
              "author": "gaztrab",
              "text": "Interesting, I haven't tested --fit-ctx, only --fit on with -c. And the --fit-target 1536 tip for vision is great, I have the mmproj downloaded but haven't smoke-tested it yet. Your config repo is really useful. I will properly test vision on the next round of experiment!",
              "score": 1,
              "created_utc": "2026-02-27 16:18:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7r3zka",
          "author": "OsmanthusBloom",
          "text": "This is great work! But I wonder about the effect of dropping the batch size adjustments. Normally you increase the ubatch size to improve prompt processing speed. It can increase drastically (eg 3x) when you raise ubatch from, say, 512 to 2048. But generation speed will suffer due to VRAM pressure. You didn't seem to benchmark pp speed separately. Maybe an ubatch size of, say, 1024 would have raised pp without hitting tg too much?",
          "score": 3,
          "created_utc": "2026-02-27 19:20:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rszuj",
              "author": "OsmanthusBloom",
              "text": "Here is my llama-bench result, which shows that increasing ubatch from the default 512 to 1024 or 2048 increases prompt processing speeds a lot, from 280 t/s to 440 and 650 t/s. I have a RTX 3060 Laptop GPU with only 6GB VRAM so most of the model is offloaded to GPU. Using the UD\\_Q3\\_K\\_M quant released today.\n\n    llama-bench -m Qwen3.5-35B-A3B-UD-Q3_K_M.gguf -ctk q8_0 -ctv q8_0 --n-cpu-moe 37 -p 4096 -n 512 -fa 1 -b 2048 -ub 512,1024,2048\n    ggml_cuda_init: found 1 CUDA devices:\n      Device 0: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability 8.6, VMM: yes\n    | model                          |       size |     params | backend    | ngl | n_ubatch | type_k | type_v | fa |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -----: | -----: | -: | --------------: | -------------------: |\n    | qwen35moe ?B Q8_0              |  15.53 GiB |    34.66 B | CUDA       |  99 |      512 |   q8_0 |   q8_0 |  1 |          pp4096 |        283.43 Â± 1.24 |\n    | qwen35moe ?B Q8_0              |  15.53 GiB |    34.66 B | CUDA       |  99 |      512 |   q8_0 |   q8_0 |  1 |           tg512 |         23.90 Â± 0.63 |\n    | qwen35moe ?B Q8_0              |  15.53 GiB |    34.66 B | CUDA       |  99 |     1024 |   q8_0 |   q8_0 |  1 |          pp4096 |        444.65 Â± 1.35 |\n    | qwen35moe ?B Q8_0              |  15.53 GiB |    34.66 B | CUDA       |  99 |     1024 |   q8_0 |   q8_0 |  1 |           tg512 |         23.62 Â± 0.25 |\n    | qwen35moe ?B Q8_0              |  15.53 GiB |    34.66 B | CUDA       |  99 |     2048 |   q8_0 |   q8_0 |  1 |          pp4096 |        648.85 Â± 2.56 |\n    | qwen35moe ?B Q8_0              |  15.53 GiB |    34.66 B | CUDA       |  99 |     2048 |   q8_0 |   q8_0 |  1 |           tg512 |         23.37 Â± 0.35 |\n    \n    build: ecbcb7ea9 (8179)\n\n  \nllama-bench doesn't support `--fit` so I had to set `--n-cpu-moe` manually according to the VRAM requirements of the largest ubatch size. With a smaller ubatch size and `--fit`, some more experts would fit in VRAM and thus generation speeds would be slightly higher. Still, getting much higher pp speeds is I important especially for agentic stuff where prompts can be quite long.",
              "score": 2,
              "created_utc": "2026-02-27 21:25:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7rv70v",
          "author": "bobaburger",
          "text": "Great work! I was thinking of making a separate post, but since this is also in the 16 GB VRAM category, I'm adding my findings for anyone using 5060 Ti here. My setup also using 32 GB DDR5 RAM.\n\nAll tests were done with q8\\_0 kv cache, context window 128k, pp 18k, tg 768, depth 0. Why? Because this closes to a cold start with Claude Code. You can adjust the context window to lower for a bit more performance gain.\n\nhttps://preview.redd.it/tj92peq5v3mg1.png?width=1600&format=png&auto=webp&s=d306d4ebb16501ce5539ab322aa77df27ccd422f\n\n|Model|pp18432 (t/s)|tg768 (t/s)|Mean KLD|\n|:-|:-|:-|:-|\n|Unsloth UD-Q4\\_K\\_M|1047.84|40.64|0.0192|\n|AesSedai Q4\\_K\\_M|928.10|34.89|0.0096|\n|Unsloth IQ3\\_S|1465.81|44.77|0.0457|\n|Unsloth MXFP4|1186.50|38.32|0.0272|\n|Unsloth UD-Q4\\_K\\_XL|1002.84|36.59|0.0137|\n\nMean KLD was from Unsloth's data.\n\nAesSedai's Q4\\_K\\_M has the best mean KLD, but it was the slowest, probably not worth it.\n\nSo, same as OP on 5080, for 5060 Ti, Q4\\_K\\_M seems in the sweet spot, balanced between speed and quality.",
          "score": 3,
          "created_utc": "2026-02-27 21:36:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7st0v0",
              "author": "KeldenL",
              "text": "this is super helpful! totally make it another post. all these quant posts (especially for 16gb, cuz selfishly i also have 16gb vram on my 4060ti) have been super enlightening and saves a lot of people a lot of testing!\n\ni wonder why your t/s was closer to 40 vs OPâ€™s 70, cuz thatâ€™s what iâ€™m seeing too on my end",
              "score": 2,
              "created_utc": "2026-02-28 00:44:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7t01nb",
                  "author": "bobaburger",
                  "text": "thank you so much!\n\nthe speed difference was due to two things:\n\n- the context window, mine was 128k, OP was 64k (`-c 65536`)\n- OP probably has stronger CPU than mine, and was using 20 threads (`-t 20`), mine was only 8 threads :D",
                  "score": 1,
                  "created_utc": "2026-02-28 01:26:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7zfz1r",
              "author": "gaztrab",
              "text": "Woah this is very valuable! You should def make another post to let more people know. Thanks bro!",
              "score": 1,
              "created_utc": "2026-03-01 02:11:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7oxec6",
          "author": "marcoc2",
          "text": "Does anyone have config or link for a 4090-24gb?",
          "score": 2,
          "created_utc": "2026-02-27 12:46:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7oyzv9",
              "author": "gaztrab",
              "text": "Same flags, just let **--fit** do the work:\n\n\n\n    ./llama-server \\\n    -m ./Qwen3.5-35B-A3B-Q4_K_M.gguf \\\n    -c 65536 \\\n    --fit on \\\n    -fa on \\\n    -t 20 \\\n    --no-mmap \\\n    --jinja \\\n    -ctk q8_0 \\\n    -ctv q8_0\n\n\n\nWith 24GB VRAM, **--fit** should keep nearly all of Q4\\_K\\_M (\\~20 GB) on GPU â€” you'll likely see higher tok/s than my 16GB results. Tune -t for your CPU (20 is optimal for my 9950X, try physical\\_cores Ã— 0.6 as a starting point and sweep from there). You could even try Q8\\_0 (36.9 GB) with partial offload â€” at 24GB you'd get significantly more layers on GPU  than I can.",
              "score": 7,
              "created_utc": "2026-02-27 12:56:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7p488k",
                  "author": "marcoc2",
                  "text": "Thank you ðŸ™",
                  "score": 2,
                  "created_utc": "2026-02-27 13:28:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7oy7j6",
          "author": "Life-Screen-9923",
          "text": "Great job, thank you! ðŸ”¥ðŸ”¥ðŸ”¥",
          "score": 2,
          "created_utc": "2026-02-27 12:51:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7oybeb",
          "author": "maxpayne07",
          "text": "Kudos!!",
          "score": 2,
          "created_utc": "2026-02-27 12:52:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7oynrn",
          "author": "JoseGemez",
          "text": "This weekend i try on a 5060 ti 16gb! Many thanks",
          "score": 2,
          "created_utc": "2026-02-27 12:54:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7p11af",
          "author": "MaCl0wSt",
          "text": "wow fantastic post, thanks",
          "score": 2,
          "created_utc": "2026-02-27 13:09:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7p16ns",
          "author": "ayylmaonade",
          "text": "Thank you so much for the MXFP4 testing! Happy to see that quantizing the KV cache doesn't impact performance too. Really appreciate all the effort. :)",
          "score": 2,
          "created_utc": "2026-02-27 13:10:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7p20f6",
          "author": "joshbates15",
          "text": "This is amazing work! Thank you for sharing.",
          "score": 2,
          "created_utc": "2026-02-27 13:15:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7p2ffg",
          "author": "savenx",
          "text": "Thanks for the tests, very helpful! I have a question: Im using a RX6900XT 16GB vram and i have 32GB ram, which version should i use? I tried Q4 on LM studio and its pretty fast, but when i try to use it on OpenCode (agentic use) it becomes unusable",
          "score": 2,
          "created_utc": "2026-02-27 13:18:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p9g7c",
              "author": "gaztrab",
              "text": "All my tests were NVIDIA/CUDA, so I can't give specific AMD numbers â€” ROCm/Vulkan backends may behave differently. For the RX 6900 XT with 16GB VRAM, Q4\\_K\\_M with --fit on should still be the right choice. The OpenCode issue is probably separate â€” likely related to context length or thinking mode (Qwen3.5 has thinking enabled by default which consumes extra tokens). Try disable it, or check if OpenCode is sending very long system prompts that fill your context.",
              "score": 1,
              "created_utc": "2026-02-27 13:57:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7p3t4a",
          "author": "wisepal_app",
          "text": "This is the best explanation on this sub i saw, about a technical topic. very informative and simple. thank you for your hard work. ",
          "score": 2,
          "created_utc": "2026-02-27 13:26:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7p3y8m",
          "author": "cookieGaboo24",
          "text": "Great test, nice Work and thank you. One question, how did you guys get those 50t/s on 8gb VRAM? I did the same offloading on my 3060 12gb and only get around 30t/s. Did you just offload them all on the 5080 or used a different card?",
          "score": 2,
          "created_utc": "2026-02-27 13:26:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p71cq",
              "author": "gaztrab",
              "text": "Right, sorry about this, I ran the test on my 5080. I will update the post to clarify this!",
              "score": 4,
              "created_utc": "2026-02-27 13:44:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7p7kmc",
                  "author": "cookieGaboo24",
                  "text": "Thanks, figured as much but better ask again. Not mentioning the gap in GPU generation, ram generation (ddr4 vs ddr5) and the absurdly stronger CPU (R5 3600 here) which I already partially considered to be at fault, I still consider my 30t/s a win.",
                  "score": 2,
                  "created_utc": "2026-02-27 13:47:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7p5tq0",
          "author": "Technical-Earth-3254",
          "text": "Goated post, thank you for all the effort you did put into this",
          "score": 2,
          "created_utc": "2026-02-27 13:37:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7p6pau",
          "author": "allattention",
          "text": "Awesome work, much appreciated! I thought we used -u and -ub to make reading large context after a KV reset (which happens often if you use opencode) faster. Iâ€™ll try without them now.",
          "score": 2,
          "created_utc": "2026-02-27 13:42:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p9qjv",
              "author": "gaztrab",
              "text": "Good point about KV resets. I will also test them in future round. Thanks!",
              "score": 3,
              "created_utc": "2026-02-27 13:59:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7p8dto",
          "author": "Old-Sherbert-4495",
          "text": "i didn't understand 90% of this, I was trying my fullest to get 27b q4 working faster in my 16vram and 32 ram setup. when i have fit on, it leaves a lot of vram and cpu is 100% (i did quantize cache q8.) moe 35b was definitely faster. but that also leaves a few gig vram and the cpu goes bananas. how can i get the best of the available vram any advice",
          "score": 2,
          "created_utc": "2026-02-27 13:51:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pbnbn",
              "author": "gaztrab",
              "text": "A few things to check:\n\n\n\n1. Make sure you're NOT using -b 4096 -ub 4096 â€” those eat VRAM that --fit needs\n\n2. Add --no-mmap â€” loads the full model into RAM upfront, gives --fit a clearer picture of available memory\n\n3. With 32GB RAM you're tight â€” try reducing context to -c 32768 instead of 65536, which frees KV cache VRAM for more expert layers on GPU\n\n4. CPU at 100% is normal and expected â€” that's the CPU computing the expert layers that don't fit on GPU. The goal is to minimize how many go to CPU, not eliminate CPU usage entirely.\n\n\n\nFull command:\n\n`./llama-server -m ./Qwen3.5-35B-A3B-Q4_K_M.gguf -c 32768 --fit on -fa on -t 20 --no-mmap --jinja -ctk q8_0 -ctv q8_0`\n\n\n\nTune -t for your CPU â€” try physical cores Ã— 0.6 as a starting point.",
              "score": 3,
              "created_utc": "2026-02-27 14:09:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7pewii",
                  "author": "Old-Sherbert-4495",
                  "text": "thnx a lot will give this a shot",
                  "score": 2,
                  "created_utc": "2026-02-27 14:27:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7pf3i5",
                  "author": "Old-Sherbert-4495",
                  "text": "one more thing, do i have hope for 27b or should i just forget",
                  "score": 2,
                  "created_utc": "2026-02-27 14:28:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7q6jj1",
          "author": "Lucis_unbra",
          "text": "I would note that while the ppl might be fine, it's not free.  The token generation speed drops much faster, at least on my rig with windows. \n\nAt ~50k with an iq4_xs quant, F16 gives me around 75tps, down from 86.\n\nQ8_0 at that CTX ends up at 65tps. That's a 10tps loss. \n\nIf this was not fully on the GPU, we can expect this to get worse. \n\nAt Q8_0, I start off at about 42, and this then drops to 39tps.\n\nIf I drop the KV cache down to 8 bit again, it drops to 36tps.\n\nNow this is on a decently powerful system with a 3090 and a Ryzen 9 7900x. But depending on the configuration, and the model, this could get much worse. For the 27B dense model that is already hard enough to run? Not fun.",
          "score": 2,
          "created_utc": "2026-02-27 16:42:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zirg4",
              "author": "gaztrab",
              "text": "Your 27B dense observation is actually really valuable, it confirms KV q8\\_0 is NOT necessarily free on dense models I should add that caveat. For MoE models like Qwen3.5-35B-A3B it's still free because of the SSM hybrid architecture, but users shouldn't blindly apply it to dense models.",
              "score": 1,
              "created_utc": "2026-03-01 02:28:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qckgx",
          "author": "soshulmedia",
          "text": "This repo and quantizing team came up recently:\n\nhttps://huggingface.co/AesSedai/Qwen3.5-35B-A3B-GGUF\n\nDid you do a comparison? (If not, can you?) They have some quants (for other qwen3.5 sizes) that compared favorably to unsloth's.\n\nEDIT: Oh and thank you of course for doing all these tests!",
          "score": 2,
          "created_utc": "2026-02-27 17:10:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zhyym",
              "author": "gaztrab",
              "text": "Thank you for your kind words. And yes! We tested AesSedai Q4\\_K\\_M in our experiments. Results:\n\n| Quant              | PPL    | KLD    | Same-top-p | TG (tok/s) |\n\n|--------------------|--------|--------|------------|------------|\n\n| bartowski Q4\\_K\\_M   | 6.6688 | 0.0286 | 92.46%     | \\~74        |\n\n| AesSedai Q4\\_K\\_M    | 6.3949 | 0.0095 | 95.74%     | \\~44        |\n\n| Unsloth UD-Q4\\_K\\_XL | 6.5959 | 0.0145 | 94.46%     | \\~48        |\n\nAesSedai wins every quality metric by a significant margin â€” KLD 0.0095 is 3x better than bartowski. The tradeoff is \\~40% slower speed. If quality is your priority (and you can accept \\~44 tok/s), AesSedai is the best Q4 quant we've tested.",
              "score": 1,
              "created_utc": "2026-03-01 02:23:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qt5zr",
          "author": "Chromix_",
          "text": "Thanks for taking the time for the extensive follow-up and immediately making edits taking the further feedback into account. That's refreshing to see.\n\nI randomly came across this, as I didn't get any notification for this, despite being mentioned. It worked in your previous comment. Maybe notifications simply got skipped for your post as you mentioned so many others?\n\nBtw: Without the batch setting your token generation is faster, but prompt processing gets slower (only because you don't have enough VRAM for full offload). Tough choice depending on the use-case.",
          "score": 2,
          "created_utc": "2026-02-27 18:29:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zikl7",
              "author": "gaztrab",
              "text": "My data shows PP-512 = 1390 t/s without batch flags vs \\~1532 with -b 4096 -ub 4096, but TG drops from 74.7 to 48.3. The middle ground -ub 1024 -b 2048 gives PP +22% with only TG -3.5%, which could be worth it for prompt-heavy workflows. I'm adding PP columns to our benchmark comparison tool to make this more transparent. Thanks for the notification heads-up â€” Reddit seems to have a limit on mentions per post!",
              "score": 1,
              "created_utc": "2026-03-01 02:27:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qyp69",
          "author": "Majesticeuphoria",
          "text": "This is some serious work, nice!",
          "score": 2,
          "created_utc": "2026-02-27 18:55:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7spny8",
          "author": "KeldenL",
          "text": "dude this is incredible. i was doing tests on my end too and got tired at how slow it was (probably shouldâ€™ve done it on lower context lengths)\n\none thing that i may or may not have missed in the post, but whoâ€™s Q4 quant are you using? unsloths? or others? i remember seeing another post about different quants",
          "score": 2,
          "created_utc": "2026-02-28 00:24:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zfg2a",
              "author": "gaztrab",
              "text": "Yeah in this post Im testing Unsloth vs Bartowski, I will expand the selections to more on the next round.",
              "score": 1,
              "created_utc": "2026-03-01 02:08:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7u3pn7",
          "author": "maho_Yun",
          "text": "Thanks I have done tested base on this with my 5060ti and CLIP enabled\n\n**Diff Quant and Flag tested with mmproj-BF16.**  \n\\--ctx-size 131072 -n 32768 --flash-attn on --kv-offload --no-mmap -ctk q8\\_0 -ctv q8\\_0  \nFull Tom Sawyer.txt with promt: Write a Essay About it.\n\n|**Model & Config**|**Prompt Eval (t/s)**|**Eval/Gen (t/s)**|**Total Time (ms)**|\n|:-|:-|:-|:-|\n|**Unsloth MXFP4** ncpumoe 24 b2024 ub 1024 (CUDA 12.4)|875.56|30.10|202,516|\n|**Unsloth MXFP4** ncpumoe 24 b2024 ub 1024|**929.55**|32.32|186,634|\n|**Unsloth-UD Q4\\_K\\_M** ncpumoe 24 b2024 ub1024|860.97|34.34|183,707|\n|**Unsloth-UD** **Q4\\_K\\_M** fit on fit-target 1536|813.95|**38.91**|186,154|\n|**Aessedai** **Q4\\_K\\_M** ncpumoe30 b2024 ub1024|867.85|30.93|179,110|\n|**Aessedai** **Q4\\_K\\_M** fit on fit-target 1536|870.69|35.26|**178,969**|\n|**Aessedai** **Q4\\_K\\_M** fit on|199.74|25.45|613,891|",
          "score": 2,
          "created_utc": "2026-02-28 05:56:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zf795",
              "author": "gaztrab",
              "text": "Thanks for sharing, '--fit-target 1536' is very interesting, Im currently testing that config on the next round.",
              "score": 1,
              "created_utc": "2026-03-01 02:06:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7zfglf",
                  "author": "maho_Yun",
                  "text": "u have to add it if clip was loaded or else it will oom. Pure text no effect.",
                  "score": 2,
                  "created_utc": "2026-03-01 02:08:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ut3d7",
          "author": "soyalemujica",
          "text": "Alright, I gave this tutorial a try, compiled llama.cpp with the params as described, running on RTX5060ti 16GB + 64gb DDR5 6400mts/s, and I'm only getting 50t/s, did I do something wrong? Using CUDA 13.1 and latest NVIDIA drivers.  \nEdit: Getting 55/s , which is an increase of 10t/s in LM Studio and precompiled public llama libraries, this is nice! The difference of 20 tokens might be because the 5080 has 960gb/s bandwidth vs 460gb/s bandwidth on my 5060TI I suppose...",
          "score": 2,
          "created_utc": "2026-02-28 09:47:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7v7qy1",
              "author": "soyalemujica",
              "text": "Trying now the BF16 MXFP4\\_MOE model, it's giving me 35t/s and also thinking LESS and giving me a result quicker than the Q4\\_M model.",
              "score": 2,
              "created_utc": "2026-02-28 12:02:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7zeto2",
              "author": "gaztrab",
              "text": "Yeah I think it's the bandwidth difference too, but thanks for testing and sharing your result!",
              "score": 1,
              "created_utc": "2026-03-01 02:04:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7uu1ci",
          "author": "moahmo88",
          "text": "Amazing! Thanks.",
          "score": 2,
          "created_utc": "2026-02-28 09:56:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uv0xg",
          "author": "moahmo88",
          "text": "You can tryÂ AesSedai/Qwen3.5-35B-A3B-GGUF Q5_K_M.\n5070ti works well.Surpriseï¼",
          "score": 2,
          "created_utc": "2026-02-28 10:06:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zelg3",
              "author": "gaztrab",
              "text": "Will do on the next round of experiment. Thanks for the suggestion!",
              "score": 1,
              "created_utc": "2026-03-01 02:03:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vmipl",
          "author": "UniversalJS",
          "text": "Great post and experiments! Inspired by your findings, I went a different direction: instead of optimizing Q4_K_M, I tested whether a smaller quant that fits mostly in VRAM could beat it on speed.\n\nSetup: RTX 5080 16GB, Intel Core Ultra 9 285K, llama.cpp built from source with CUDA 13.1 + native sm_120 (Blackwell), using your recommended flags (no batch flags, --fit on, KV q8_0).\n\nThe problem with Q4_K_M on 16GB: The model is ~20 GB, so --fit offloads ~9 GB of expert weights to CPU. GPU sits at ~45% utilization waiting for CPU experts. That's the bottleneck.\n\nThe idea: Q2_K_L (bartowski) is only ~13.8 GB. At 128k context, almost all expert weights stay on GPU (~800 MiB on CPU, mostly the embed/output layer from the 248K vocab â€” unavoidable).\n\nResults:\n72% faster than Q4_K_M, with 2x the context. Even at 250k context (near the model's 262k training length), Q2_K_L still does 108 tok/s â€” 45% faster than Q4_K_M at 65k.\nThe trade-off is obviously quality. Q2_K_L will have noticeably worse perplexity than Q4_K_M. But for interactive use, code generation, and tasks where speed matters more than peak accuracy, it's a compelling option on 16 GB cards.\n\nInteresting finding on context scaling: As context increases, --fit progressively offloads more expert layers to CPU to make room for the KV cache. The 515 MiB always on CPU (embed/output) is fixed, but at 250k context, total CPU offload grows to 2.3 GB. The speed degradation is graceful though â€” only 16% slower going from 128k to 250k.\n\nAlso worth noting: Building from source with CUDA 13.1 matters for RTX 50-series. The prebuilt binaries use CUDA 12.4 which lacks sm_120 â€” you get JIT-compiled PTX from sm_89 instead of native Blackwell kernels.\n\nLaunch command (128k context, sweet spot):\n./llama-server \\\n  -m ./Qwen3.5-35B-A3B-Q2_K_L.gguf \\\n  -c 131072 \\\n  --fit on \\\n  -fa on \\\n  -t 20 \\\n  --no-mmap \\\n  --jinja \\\n  -ctk q8_0 \\\n  -ctv q8_0\n\nWould love to see KLD/PPL numbers for Q2_K_L if anyone has the patience to run them. My gut says it's worse than Q4_K_M but the speed advantage is hard to ignore.",
          "score": 2,
          "created_utc": "2026-02-28 13:46:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vxfco",
              "author": "moahmo88",
              "text": "I think you should try Qwen3.5-27B-GGUF Q3\\_K\\_S or Q3\\_K\\_M.",
              "score": 2,
              "created_utc": "2026-02-28 14:49:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7zehjt",
              "author": "gaztrab",
              "text": "Nice! I currently dont intend to test anything below Q4, but your findings gave lots of insights to muse over. Thanks bro!",
              "score": 2,
              "created_utc": "2026-03-01 02:02:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7xr8of",
          "author": "Gringe8",
          "text": "Your dense vs moe speed part is severly flawed. You do mention it needs to fit fully in vram, but you test one that doesnt fit fully in vram. You also dont mention prompt processing speed. I get 2000t/s pp and 28t/s tg on 27b q8.\n\nI do like your other tests. If you wamt to do more i would love to see quality differences between q4km and iq4ks. Then you could also test the speed since it should fit fully in your 5080.",
          "score": 2,
          "created_utc": "2026-02-28 20:24:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ze0r0",
              "author": "gaztrab",
              "text": "Thanks for pointing out, I adjusted my next experiment based on your feedback already. Will share more soon!",
              "score": 1,
              "created_utc": "2026-03-01 01:59:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ov5ij",
          "author": "catlilface69",
          "text": "There are doubts about your experiments. What do you mean q4 quant with q4 kv cache is more accurate? ",
          "score": 2,
          "created_utc": "2026-02-27 12:30:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7oy0do",
              "author": "gaztrab",
              "text": "I reran the full E1 experiment just now and got identical numbers both times. PPL evaluation is deterministic on the same dataset, so the sub-0.4% differences are real and reproducible, just too small to matter in practice.\n\n\n\nThe slight \"improvement\" with q8\\_0 KV is likely a minor rounding effect from quantization â€” essentially noise at that scale. The takeaway is that KV q8\\_0 doesn't hurt quality at all, so the throughput gain is free.                                                                                                                            \n\nYou can reproduce it yourself from [https://github.com/gaztrabisme/llm-server](https://github.com/gaztrabisme/llm-server)\n\n\n\n`./scripts/run-experiment.sh e1`\n\n\n\nRuns all 6 PPL evaluations (\\~25 min) and prints the comparison table at the end.",
              "score": 8,
              "created_utc": "2026-02-27 12:50:19",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7ovmgg",
              "author": "gaztrab",
              "text": "Hollup, let me verify this again, back in a min",
              "score": 7,
              "created_utc": "2026-02-27 12:34:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7p9v6w",
          "author": "bigvenn",
          "text": "Incredible work man, this is science",
          "score": 3,
          "created_utc": "2026-02-27 13:59:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pjiki",
          "author": "KierkegaardsSisyphus",
          "text": "I'm not understanding your speeds for MXFP4. I have a 5080 and I get about 77 tk/s on short contexts. I use fit target, otherwise image processing goes OOM.\n\nexec \"$BINARY\" \\\\\n\n\\-m \"$MODEL\\_PATH\" \\\\\n\n\\--mmproj \"$MMPROJ\\_PATH\" \\\\\n\n\\-c 65536 \\\\\n\n\\-fa on \\\\\n\n\\--fit on \\\\\n\n\\--port 5001 \\\\\n\n\\--fit-target 1500 \\\\\n\n\\-ctk q8\\_0 \\\\\n\n\\-ctv q8\\_0 \\\\\n\n\\--jinja \\\\\n\n\\--no-mmap\n\nSide note: I prefer running with     -b 4096 and -ub 2048. For me, the massively improved processing speed is worth losing a few tokens of text gen speed.",
          "score": 2,
          "created_utc": "2026-02-27 14:51:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pjw8h",
              "author": "gaztrab",
              "text": "I will properly test MXFP4 on the next round with your config as reference. Thanks my dude!",
              "score": 3,
              "created_utc": "2026-02-27 14:53:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7oykq5",
          "author": "DepravedPrecedence",
          "text": "Is it possible to use these flags in LM Studio? I think it doesn't allow setting flags of llama.cpp like that? ",
          "score": 1,
          "created_utc": "2026-02-27 12:53:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p126t",
              "author": "gaztrab",
              "text": "LM Studio does expose some of these â€” context length, flash attention, GPU layers, and KV cache quantization (check for llamaKCacheQuantizationType / llamaVCacheQuantizationType in load settings, requires flash attention enabled). However, --fit on (the auto VRAM management that gave us the biggest speed gain) is not available in LM Studio â€” it's a recent llama.cpp feature.\n\n\n\nIf you want the full config, you'll need to run llama-server directly. The Docker setup in [https://github.com/gaztrabisme/llm-server](https://github.com/gaztrabisme/llm-server) makes it straightforward, or you can just build llama.cpp from source and run the command from my post.",
              "score": 3,
              "created_utc": "2026-02-27 13:09:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7p4y4q",
                  "author": "AvidCyclist250",
                  "text": "I suppose you could half-ass the same gains by manually fine-tuning context length?",
                  "score": 1,
                  "created_utc": "2026-02-27 13:32:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7oymfw",
          "author": "ilintar",
          "text": "Very nice benchmark, I hope it really puts to rest a few stupid myths, including \"KV cache quantization absolutely kills quality for coding\" and \"MXFP4 is the best 4-bit quant ever\".",
          "score": 1,
          "created_utc": "2026-02-27 12:54:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ozz5f",
              "author": "gaztrab",
              "text": "I wouldnt say my experiments killed the myths, since comparing to the numbers of models and quants out there, this is just a small drop in the ocean. But thanks!",
              "score": 1,
              "created_utc": "2026-02-27 13:02:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7pfhhw",
                  "author": "ilintar",
                  "text": "It takes just one counterexample to kill a general statement, that's the beauty of it ðŸ˜€",
                  "score": 2,
                  "created_utc": "2026-02-27 14:30:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7oypvd",
          "author": "R_Duncan",
          "text": "Sorry, or your test were shallow or there is a mistake or is a lie, but MXFP4\\_MOE \" **34-42% slower**Â \" than Q4\\_K\\_M is not true. Anyone can verify. (4060 laptop with CUDA backend here)\n\nGiven the same question to both models, I got no noticeable slowdown of MXFP4\\_MOE.",
          "score": 1,
          "created_utc": "2026-02-27 12:54:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p02jv",
              "author": "gaztrab",
              "text": "Verified â€” I just re-checked raw benchmark logs and the numbers hold. Both quants tested with identical configs (--fit on, same threads, same KV q8\\_0, same Docker image). Raw data is public: [https://github.com/gaztrabisme/llm-server](https://github.com/gaztrabisme/llm-server) under benchmarks/.\n\n\n\nThat said â€” my results are specific to RTX 5080 16GB. The relative gap could be different on your 4060 laptop depending on how much overflows to CPU and how MXFP4 dequant performs there. What tok/s are you seeing for each on your setup?",
              "score": 3,
              "created_utc": "2026-02-27 13:03:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7p53m6",
                  "author": "Constant-Simple-1234",
                  "text": "Yes. Different architectures may influence this. I run on Vulkan on iGPU - much smaller differences between Q4_K_M and MXFP4. But I will confirm on 5060 TI.",
                  "score": 1,
                  "created_utc": "2026-02-27 13:33:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7ozexd",
              "author": "gaztrab",
              "text": "Hey bro. Let me double check my method and rerun MXFP4 experiment to be sure.",
              "score": 1,
              "created_utc": "2026-02-27 12:59:23",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7p4dic",
              "author": "DigiDecode_",
              "text": "from what i know is that MXFP4 is designed & optimised for RTX 5000 series and future  \nQ4\\_K\\_MÂ is more suited for RTX 4000 & 3000 series",
              "score": 1,
              "created_utc": "2026-02-27 13:29:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7p2nw7",
          "author": "MrQ_dos40",
          "text": "This is a fantastic deep dive into Qwen3.5-35B-A3B performance! I'm particularly interested in the `--fit on` results. Have you considered testing with different batch sizes to see if that impacts the token/s further, especially with the 16GB VRAM constraint?",
          "score": 1,
          "created_utc": "2026-02-27 13:19:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p9cxl",
              "author": "gaztrab",
              "text": "Yes â€” that's actually what Experiment 4 was about! The key finding: with --fit on, you should NOT set -b 4096 -ub 4096. Those batch buffers pre-allocate VRAM that --fit then can't use for expert layers on GPU. Removing them entirely gave us 74.7 vs 64.3 tok/s â€” a 16% improvement just from letting --fit have the VRAM. I also tested --fit-target 256 (smaller batch allocation) but it only partially helped. The simple answer: just don't set -b/-ub at all and let --fit manage everything.",
              "score": 2,
              "created_utc": "2026-02-27 13:57:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7pyiwf",
                  "author": "DHasselhoff77",
                  "text": "Weren't the custom batch sizes there to speed up prompt processing? So by removing them you are trading off PP speed for generation speed by an unknown amount. Not always a win.\n\nA very clear experiment still. I appreciate the direct writing style and presentation. Thank you!",
                  "score": 2,
                  "created_utc": "2026-02-27 16:04:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7p4m1o",
          "author": "soyalemujica",
          "text": "Mind you share what ollama command did you use to run the 8Q and 4K\\_M models for 16gb vram ?",
          "score": 1,
          "created_utc": "2026-02-27 13:30:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7p9aic",
              "author": "gaztrab",
              "text": "I actually didn't use Ollama â€” all tests used llama.cpp server directly. One of my research findings was that Ollama is \\~3x slower for MoE models because it doesn't support expert-level offloading (it splits entire transformer layers between GPU/CPU instead of just the expert FFNs). There's an [https://github.com/ollama/ollama/pull/12333](https://github.com/ollama/ollama/pull/12333) to add num\\_moe\\_offload but it hasn't merged yet.",
              "score": 1,
              "created_utc": "2026-02-27 13:56:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7phwne",
                  "author": "soyalemujica",
                  "text": "Thank you, mind you share your compiled llama.cpp with that sm\\_120 you mentioned ? I am having a hard time compiling it",
                  "score": 1,
                  "created_utc": "2026-02-27 14:43:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7pjgsi",
          "author": "Lrrrrr",
          "text": "I fuckin love you bro. Got a 5060Ti16gb I did some tests on. Your data is so valuable for us GPU poors ðŸ˜‚\nYou use q4km from unsloth right?",
          "score": 1,
          "created_utc": "2026-02-27 14:51:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pjr26",
              "author": "gaztrab",
              "text": "I fuckin love u too (no homo). Yeah that's quant I used :p",
              "score": 3,
              "created_utc": "2026-02-27 14:52:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ps4vc",
          "author": "soyalemujica",
          "text": "mind you share your compiled llama.cpp with that sm\\_120 you mentioned ? I am having a hard time compiling it for my rtx 5060ti",
          "score": 1,
          "created_utc": "2026-02-27 15:33:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7q1wyu",
              "author": "gaztrab",
              "text": "The 5060 Ti is also Blackwell (sm\\_120), so the same build works. Easiest path is using our Dockerfile which handles everything:\n\n\n\n    git clone https://github.com/gaztrabisme/llm-server\n    cd llm-server\n    docker build -f docker/Dockerfile.llama-cpp --build-arg LLAMA_CPP_REF=b8149 -t llm-server/llama-cpp:latest-fit docker/\n\n\n\nThat builds llama.cpp from source with CUDA 12.8 + sm\\_120. You need Docker + NVIDIA Container Toolkit installed. If you want to build without Docker, the key CMake flags are: `-DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=120 -DGGML_CUDA_FA_ALL_QUANTS=ON` with CUDA 12.8+.",
              "score": 1,
              "created_utc": "2026-02-27 16:20:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7pvid1",
          "author": "leonbollerup",
          "text": "I have a 3090 and RTX 4000 pro and can run the same tests if you show me what/how you ran them",
          "score": 1,
          "created_utc": "2026-02-27 15:50:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7q1put",
              "author": "gaztrab",
              "text": "Everything's in my repo: [https://github.com/gaztrabisme/llm-server](https://github.com/gaztrabisme/llm-server) (also optimized for coding agent too, just point them to CLAUDE.md)\n\n\n\nQuick start:\n\n1. Build the Docker image: docker build -f docker/Dockerfile.llama-cpp --build-arg LLAMA\\_CPP\\_REF=b8149 -t llm-server/llama-cpp:latest-fit docker/\n\n2. Download Q4\\_K\\_M: huggingface-cli download unsloth/Qwen3.5-35B-A3B-GGUF Qwen3.5-35B-A3B-Q4\\_K\\_M.gguf --local-dir ./models\n\n3. Run a benchmark: ./scripts/bench.sh llama-cpp s006-e4-fit-nobatch\n\n\n\nWith a 3090 (24GB) you'll have more VRAM headroom than me â€” would love to see your numbers.",
              "score": 1,
              "created_utc": "2026-02-27 16:19:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7q9khb",
          "author": "Dthen_",
          "text": "Is there a guide or config for manually offloading on AMD/Vulkan/RoCM?",
          "score": 1,
          "created_utc": "2026-02-27 16:56:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zhq0v",
              "author": "gaztrab",
              "text": "For AMD/ROCm or Vulkan: --fit on doesn't work well (2.4x slower on ROCm per one user, 2.5x on Vulkan). Use manual offload instead:\n\n    ./llama-server -m ./Qwen3.5-35B-A3B-Q4_K_M.gguf \\\n    -c 65536 -ngl 999 --n-cpu-moe 24 \\\n    -fa on -t 20 --no-mmap --jinja \\\n    -ctk q8_0 -ctv q8_0\n\nThe key flag is --n-cpu-moe 24 â€” this keeps 16 out of 40 MoE layers on GPU and offloads the rest to CPU. Start with 24 and tune down (lower number = more on GPU = faster but more VRAM). -ngl 999 puts all non-expert layers on GPU. Watch your VRAM usage with nvidia-smi â€” if you're hitting the limit, increase --n-cpu-moe.",
              "score": 1,
              "created_utc": "2026-03-01 02:22:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qnu9a",
          "author": "mintybadgerme",
          "text": "Sorry for a boring question but...\n\nI don't suppose you have any settings for a RTX 5060ti 16GB VRAM with 64GB RAM Intel? \n\nThat would be very helpful as I'm trying to work out how to use the model as a coding tool. Thanks. :)",
          "score": 1,
          "created_utc": "2026-02-27 18:04:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zhkjm",
              "author": "gaztrab",
              "text": "Not a boring question at all! The exact same config works for 5060 Ti 16GB:\n\n    ./llama-server -m ./Qwen3.5-35B-A3B-Q4_K_M.gguf \\\n    -c 65536 --fit on -fa on -t 20 --no-mmap --jinja \\\n    -ctk q8_0 -ctv q8_0\n\nYou should expect around 50-55 tok/s instead of 74 â€” the difference is purely memory bandwidth (460 vs 960 GB/s). u/soyalemujica confirmed 55 t/s on the same card. If you're using it for coding, the speed is very usable â€” the thinking mode might feel slightly slower but the actual answer quality is identical.",
              "score": 1,
              "created_utc": "2026-03-01 02:21:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qqqxj",
          "author": "jpbarcelos",
          "text": "Hi, I'm just starting my local LLM journey on a Mac mini 16gb (which currently run qwen3-14b).\n\nI've been reading that you have to have 32gb to be able to run qwen3.5, yet you mention 16gb video card.\n\nCan I replicate this on my Mac? \n\nOr am I missing something here?",
          "score": 1,
          "created_utc": "2026-02-27 18:17:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zh4mj",
              "author": "gaztrab",
              "text": "Hey there, on Mac you should start out with LMStudio first (I did too) since it's a nice UI wrapped around Mac counterpart of llama.cpp engine - MLX. And on the hardware requirement â€” yes, Qwen3.5-35B-A3B at Q4\\_K\\_M is about 20 GB, so your 16GB Mac mini can't quite fit it.\n\n\n\nBut here's the thing: Mac's big advantage is unified memory â€” the CPU and GPU share the same RAM, so there's no slow PCIe bus copying data back and forth like on a PC. On my setup, the GPU only has 16GB VRAM and the rest of the model sits in system RAM, so every token has to shuttle data across PCIe (\\~64 GB/s). On a Mac with 32GB+ unified memory, the entire model lives in one memory pool that both CPU and GPU can access at full bandwidth â€” no copying needed. That's why Macs punch above their weight for LLM inference despite having weaker raw compute.\n\n\n\nFor your 16GB Mac mini, Qwen3-14B is honestly a great fit â€” you're already running it. If you upgrade to 32GB+ down the road, Qwen3.5-35B-A3B would run nicely since it's MoE (only \\~3B params active per token, so it's fast despite the big file size). Or you could wait for the Qwen team to release the smaller version of Qwen3.5 (I heard they said soon). Cheers!",
              "score": 1,
              "created_utc": "2026-03-01 02:18:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7rrwla",
          "author": "mr_Owner",
          "text": "Very nice but i believe when you put the pp speed besides them you could make better judgement.",
          "score": 1,
          "created_utc": "2026-02-27 21:20:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zg0r9",
              "author": "gaztrab",
              "text": "Will do on the next round. Thanks for the suggestion!",
              "score": 1,
              "created_utc": "2026-03-01 02:11:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7s4ctx",
          "author": "CATLLM",
          "text": "This is amazing thank you! \n\nIs it worth using \n\n    --no-kv-offload\n    \n    to offload KV cache into ram?",
          "score": 1,
          "created_utc": "2026-02-27 22:23:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zhex5",
              "author": "gaztrab",
              "text": "Tested it! Do NOT use --no-kv-offload â€” it absolutely tanks generation speed. On my 5080: 16.1 tok/s with it vs 42.7 tok/s without (that's -63%). The KV cache on GPU is tiny for this model (only 10 KV cache layers because of the hybrid SSM architecture), so offloading it to RAM saves almost no VRAM but destroys performance.",
              "score": 2,
              "created_utc": "2026-03-01 02:20:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7zptr9",
                  "author": "CATLLM",
                  "text": "Thank you! You are amazing!",
                  "score": 1,
                  "created_utc": "2026-03-01 03:11:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o80kii0",
          "author": "Queasy_Asparagus69",
          "text": "Thank you! This should be done for every new model!",
          "score": 1,
          "created_utc": "2026-03-01 07:03:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80mp8g",
          "author": "No_War_8891",
          "text": "Is the -t 20 parameter model-dependent or more based on your specific CPU?",
          "score": 1,
          "created_utc": "2026-03-01 07:23:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pcsse",
          "author": "pmttyji",
          "text": "Big thanks for this thread. Appreciate your time & your experiments.\n\nCould you please add one more stuff(on **Experiment 1**) in this thread?\n\nExperiment with `-ctk q8_0 -ctv q4_0` because K is sensitive while V isn't. I remember few people do use this combination instead q4 on both.\n\nThanks.",
          "score": 1,
          "created_utc": "2026-02-27 14:16:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pibmm",
              "author": "gaztrab",
              "text": "Good suggestion â€” u/a_beautiful_rhind mentioned the same thing about K being more sensitive than V. I tested K and V together (both q8\\_0, both q4\\_0) but didn't test the asymmetric combo. Adding it to my list for the next round of experiments.",
              "score": 3,
              "created_utc": "2026-02-27 14:45:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7qo0eg",
                  "author": "pmttyji",
                  "text": "Thanks. Eagerly waiting for next round.\n\nGlad to see below sections in your thread even though it's so short.\n\n**Pre-built binaries vs source** for Blackwell (u/wisepal_app)\n\nFor this topic, I did [post a thread to find little gems](https://www.reddit.com/r/LocalLLaMA/comments/1q15ffw/llamacpp_custom_optimized_builds/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button), unfortunately didn't get much. Also my laptop has only 8GB VRAM which seems not enough for miracles. I spent some time to create MKL build for CPU-only, but failed somehow & gave up. Same with CUDA build, though able to create builds, I didn't get any boost. Probably I need to get most optimized cmake command with complete variables & etc.,  I don't know why I'm not seeing optimized commands online, it's not really a trade secret.\n\n**8 GB VRAM recommendations**\n\nMy laptop also has only 8GB VRAM. Till now I do search stuffs to get boost for best t/s. Hope we get more optimizations, tips & tricks on this soon & later.\n\nPlease cover these 2 sections(with more stuff) on your next round if possible.",
                  "score": 1,
                  "created_utc": "2026-02-27 18:04:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7pfcbx",
          "author": "Psyko38",
          "text": "So, this will work on a 16GB GPU of VRAM only, without the need for system RAM.",
          "score": 1,
          "created_utc": "2026-02-27 14:30:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pi8on",
              "author": "gaztrab",
              "text": "No â€” the model is \\~20 GB at Q4\\_K\\_M, so it won't fit entirely in 16GB VRAM. You need system RAM for the expert layers that overflow to CPU. With 32GB RAM you'll be fine but tight (try -c 32768 instead of 65536). With 64GB+ RAM you have plenty of headroom. The more RAM bandwidth you have (DDR5 > DDR4), the faster the CPU-side expert computation will be.",
              "score": 2,
              "created_utc": "2026-02-27 14:45:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7pix1i",
                  "author": "Psyko38",
                  "text": "Okay, because currently my 9060xt 16gb and my Ryzen 5500 on 32gb RAM in DDR4 3400 allowed me to reach 36 tok/s with Unsloth's Q3KXL. So, with your optimizations, maybe 40 tok/s.",
                  "score": 1,
                  "created_utc": "2026-02-27 14:48:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7phpdq",
          "author": "mdziekon",
          "text": "Regarding the \"Experiment 4: --fit Tuning\", could you test how does not using -b 4096 affect prompt processing speeds? Token generation is one thing, but prompt processing, especially for coding, is even more important for session start (which is especially painful for orchestrating agents when PP is slow).",
          "score": 1,
          "created_utc": "2026-02-27 14:42:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pjzzc",
              "author": "gaztrab",
              "text": "I will test this on the next round and tag you in. Thanks for the suggestion!",
              "score": 2,
              "created_utc": "2026-02-27 14:54:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7prtyp",
                  "author": "mdziekon",
                  "text": "Awesome! Great work BTW :)",
                  "score": 1,
                  "created_utc": "2026-02-27 15:32:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7pj0ku",
          "author": "RMK137",
          "text": "Great read, thank you for putting this out.",
          "score": 1,
          "created_utc": "2026-02-27 14:49:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7qljeb",
          "author": "Hacket1967",
          "text": "Impresionante trabajo ,felicidades Â¿Que compilaciÃ³n usastes, la de unsloth? Â¿Has probado estÃ¡ :https://huggingface.co/AesSedai/Qwen3.5-35B-A3B-GGUF?",
          "score": 1,
          "created_utc": "2026-02-27 17:53:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zhv30",
              "author": "gaztrab",
              "text": "Yes! We tested AesSedai Q4\\_K\\_M in our experiments. Results:\n\n\n\n| Quant              | PPL    | KLD    | Same-top-p | TG (tok/s) |\n\n|--------------------|--------|--------|------------|------------|\n\n| bartowski Q4\\_K\\_M   | 6.6688 | 0.0286 | 92.46%     | \\~74        |\n\n| AesSedai Q4\\_K\\_M    | 6.3949 | 0.0095 | 95.74%     | \\~44        |\n\n| Unsloth UD-Q4\\_K\\_XL | 6.5959 | 0.0145 | 94.46%     | \\~48        |\n\n\n\nAesSedai wins every quality metric by a significant margin â€” KLD 0.0095 is 3x better than bartowski. The tradeoff is \\~40% slower speed. If quality is your priority (and you can accept \\~44 tok/s), AesSedai is the best Q4 quant we've tested.",
              "score": 1,
              "created_utc": "2026-03-01 02:22:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o80py8v",
                  "author": "yoracale",
                  "text": "Did you test our new updated Q4\\_K\\_XL ones? [https://www.reddit.com/r/LocalLLaMA/comments/1rgel19/new\\_qwen3535ba3b\\_unsloth\\_dynamic\\_ggufs\\_benchmarks/](https://www.reddit.com/r/LocalLLaMA/comments/1rgel19/new_qwen3535ba3b_unsloth_dynamic_ggufs_benchmarks/)",
                  "score": 1,
                  "created_utc": "2026-03-01 07:54:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o80pyq8",
              "author": "yoracale",
              "text": "We tested them here: [https://www.reddit.com/r/LocalLLaMA/comments/1rgel19/new\\_qwen3535ba3b\\_unsloth\\_dynamic\\_ggufs\\_benchmarks/](https://www.reddit.com/r/LocalLLaMA/comments/1rgel19/new_qwen3535ba3b_unsloth_dynamic_ggufs_benchmarks/)",
              "score": 1,
              "created_utc": "2026-03-01 07:54:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qmp4b",
          "author": "IrisColt",
          "text": "THANKS!!!",
          "score": 1,
          "created_utc": "2026-02-27 17:58:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}