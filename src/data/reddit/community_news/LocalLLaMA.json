{
  "metadata": {
    "last_updated": "2026-01-06 08:56:03",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 26,
    "total_comments": 604,
    "file_size_bytes": 697181
  },
  "items": [
    {
      "id": "1pzwlie",
      "title": "[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: Itâ€™s running a raw Llama-7B instance with a 2048 token window.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1pzwlie",
      "author": "simar-dmg",
      "created_utc": "2025-12-30 23:03:12",
      "score": 731,
      "num_comments": 107,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwv1207",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-31 04:35:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtka7k",
          "author": "staring_at_keyboard",
          "text": "Is it common for system prompts to include environment variables such as model type? If not, how else would the LLM be aware of such a system configuration? Seems to me that such a result could also be a hallucination.",
          "score": 299,
          "created_utc": "2025-12-30 23:26:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtqwtd",
              "author": "mrjackspade",
              "text": "1. No\n2. It most likely wouldn't\n3. I'd put money on it.\n\nStill cool though",
              "score": 187,
              "created_utc": "2025-12-31 00:03:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvrpvn",
                  "author": "DistanceSolar1449",
                  "text": "Yeah, the only thing that can be concluded from this conversation is that it's *probably* a Llama model. I don't think the closed source or chinese models self-identify as Llama. \n\nThe rest of the info is hallucinated.",
                  "score": 32,
                  "created_utc": "2025-12-31 08:09:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwtvt9t",
                  "author": "lookwatchlistenplay",
                  "text": "Fuck em up.",
                  "score": 18,
                  "created_utc": "2025-12-31 00:29:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwulurj",
              "author": "Double_Cause4609",
              "text": "I guess to verify one could try and get the same information out of Llama 2 7B, Llama 3.1 8B, and a few other models from inbetween (maybe Mistral 7B?) for a control study.\n\nIt gets tricky to say what model is what, but if the Llama models specifically output the same information as extracted here it's plausible it's true.\n\nIMO it's more likely a hallucination, though the point it was a weak, potentially old, and locally run model is pretty valid.",
              "score": 12,
              "created_utc": "2025-12-31 02:59:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvcmcd",
                  "author": "staring_at_keyboard",
                  "text": "Itâ€™s an interesting research question: which, if any, models can self-identity.",
                  "score": 5,
                  "created_utc": "2025-12-31 05:58:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwtwhwo",
              "author": "yahluc",
              "text": "It's very likely that this bot was vibe coded and the person who made it didn't give it a second thought.",
              "score": 36,
              "created_utc": "2025-12-31 00:33:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww0nmq",
                  "author": "zitr0y",
                  "text": "The model would not have access to the file system or command line to access the environment variables or context length parameter",
                  "score": 14,
                  "created_utc": "2025-12-31 09:34:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx1vmy8",
              "author": "BodybuilderTrue1761",
              "text": "Def setup through Claude code.. running thru llama onto sc which u can do on the web. U r talking to the scammers Claude code setup which is orchestrating the llama",
              "score": 3,
              "created_utc": "2026-01-01 08:12:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx391p6",
              "author": "artisticMink",
              "text": "They don't. OP is deluding themselves into taking the conversation with a LLM for face value.",
              "score": 2,
              "created_utc": "2026-01-01 15:25:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxjvg17",
              "author": "Novel-Mechanic3448",
              "text": "No, OP is clueless lol",
              "score": 1,
              "created_utc": "2026-01-04 02:07:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwu0p2w",
              "author": "mguinhos",
              "text": "He said he tricked the pipeline that parses the JSON from the model.",
              "score": -7,
              "created_utc": "2025-12-31 00:56:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvkjoh",
                  "author": "the320x200",
                  "text": "What does that even mean? Models don't get any JSON unless the person writing the bot was feeding it JSON as part of their prompting, which would be a very weird thing to do in this context.",
                  "score": 5,
                  "created_utc": "2025-12-31 07:03:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwuehmp",
                  "author": "lookwatchlistenplay",
                  "text": "Real hacking only occurs in JSON format. .exes are safe to click on because no one clicks on .exes anymore. IOW, Windows is the new Linux.\n\n*This is not in fact real security advice.",
                  "score": 4,
                  "created_utc": "2025-12-31 02:16:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwulj2i",
          "author": "kzgrey",
          "text": "The only thing you can say for certain is that you stumbled upon a bot powered by an LLM.  Every other piece of information it has provided you is nonsensical hallucinating.\n\nUpdate: another thought about this: it's actually a bit dangerous that people think that they can rely on an LLM for this type of information.  It's resulted in students getting F's when the teacher believes that they can just ask ChatGPT if they wrote something and it happens to respond with \"Yes\".  Lots of students are being accused of cheating with the only evidence being a paid service that performs \"analysis\" to determine whether AI wrote something.  Frankly, I am surprised there haven't been major lawsuits from this.",
          "score": 120,
          "created_utc": "2025-12-31 02:57:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv47i7",
              "author": "ab2377",
              "text": "yea, this post doesn't make much sense.",
              "score": 27,
              "created_utc": "2025-12-31 04:56:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvixab",
                  "author": "ShengrenR",
                  "text": "Folks using llms to make them think they know things. At least op read a couple headlines and heard poems were a cool new trick.",
                  "score": 16,
                  "created_utc": "2025-12-31 06:49:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx8ezb5",
              "author": "LowWhiff",
              "text": "There have been lawsuits. Some universities ban the use of â€œAI checkersâ€ because of it. Most of the top universities have public policy banning it",
              "score": 1,
              "created_utc": "2026-01-02 10:39:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxgh6q6",
              "author": "jhaluska",
              "text": "You can also infer it's rough knowledge cut off date.  Which isn't that useful.",
              "score": 1,
              "created_utc": "2026-01-03 16:04:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtjzzd",
          "author": "UniqueAttourney",
          "text": "\\[Fixes glasses with middle finger\\] \"Wow, heather you know a lot about transformers\"",
          "score": 100,
          "created_utc": "2025-12-30 23:25:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtwbge",
              "author": "lookwatchlistenplay",
              "text": "Heather is the iFrame.",
              "score": 14,
              "created_utc": "2025-12-31 00:32:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtuht2",
          "author": "learn-deeply",
          "text": "10/10 Entirely hallucinated.",
          "score": 168,
          "created_utc": "2025-12-31 00:22:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww6ftk",
              "author": "LilPsychoPanda",
              "text": "Literally! ðŸ˜‚",
              "score": 3,
              "created_utc": "2025-12-31 10:28:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtyonm",
          "author": "shinto29",
          "text": "https://preview.redd.it/tml1f3u7sfag1.jpeg?width=1290&format=pjpg&auto=webp&s=84ab11f6858d53b659bd2e1b635fb20ac6f0c182\n\nDamn I had one of these add me and managed to get it to spit out it's entire system prompt, but had no idea it was for a reason as nefarious as this. That's fucked up.",
          "score": 45,
          "created_utc": "2025-12-31 00:45:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwulnxf",
          "author": "aeroumbria",
          "text": "\"Are you 70B-horny, 7B-horny, or are you so desperate that you are 1.5B-horny?\"",
          "score": 47,
          "created_utc": "2025-12-31 02:58:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwysq8f",
              "author": "Torodaddy",
              "text": "0.5B-raw",
              "score": 7,
              "created_utc": "2025-12-31 19:43:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtr0ia",
          "author": "Cool-Chemical-5629",
          "text": "Poor Heather, she was forced into this by scammers. #SaveHeather",
          "score": 31,
          "created_utc": "2025-12-31 00:03:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwu6qe8",
              "author": "lookwatchlistenplay",
              "text": "I ran out of breath saving Heather",
              "score": 3,
              "created_utc": "2025-12-31 01:31:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwvc3lp",
              "author": "eightbyeight",
              "text": "Bots lives matters",
              "score": 2,
              "created_utc": "2025-12-31 05:54:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwu0add",
          "author": "layer4down",
          "text": "A raw llama instance? No rubber?",
          "score": 20,
          "created_utc": "2025-12-31 00:54:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwti2nv",
          "author": "scottgal2",
          "text": "Nice work, this is my biggest fear for 2026, the elderly are NOT equipped to combat the level of phishing and extortion from automated systems like this.",
          "score": 91,
          "created_utc": "2025-12-30 23:14:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwu752s",
              "author": "Downvotesseafood",
              "text": "Young people are more likely to get scammed statistically. Its just not news worthy when when a 21yo loses his life savings of $250 dollars.",
              "score": 55,
              "created_utc": "2025-12-31 01:34:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwv1ksd",
                  "author": "OneOnOne6211",
                  "text": "This is gonna sound like a joke but, honestly, normalize someone trying to trip you up to see if you're an AI. I feel like if I wasn't sure enough and I was on a dating app, I'd be hesitant to say the kind of things that would expose an AI cuz if it isn't an AI I'd look weird and just be unmatched anyway. I feel like it'd be nice if instead of it being considered weird it was normalized or even became standard practice. I feel like it's more and more necessary with how much AI has proliferated now. I've caught a few AI in the past already but it was always with hesitance.",
                  "score": 8,
                  "created_utc": "2025-12-31 04:38:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx0liw6",
                  "author": "meshreplacer",
                  "text": "Thats the last fund for next weeks 0dte trade.",
                  "score": 1,
                  "created_utc": "2026-01-01 02:01:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwu6dy1",
              "author": "FaceDeer",
              "text": "We'll need to develop AI buddies that can act as advisors for the elderly to warn them about this stuff.",
              "score": 15,
              "created_utc": "2025-12-31 01:29:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwu7r0b",
                  "author": "low_v2r",
                  "text": "It's AI buddies all the way down",
                  "score": 17,
                  "created_utc": "2025-12-31 01:37:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwuyx4v",
                  "author": "Mediocre-Method782",
                  "text": "\"Have your agent talk to my agent and we'll do lunch\"",
                  "score": 15,
                  "created_utc": "2025-12-31 04:21:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwyt1i7",
              "author": "Torodaddy",
              "text": "Elderly should avoid talking to anyone they havent met personally. Its never going to go well",
              "score": 1,
              "created_utc": "2025-12-31 19:45:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxjvl93",
              "author": "Novel-Mechanic3448",
              "text": "Its not nice work. OP is lost.",
              "score": 1,
              "created_utc": "2026-01-04 02:08:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwtwmlw",
              "author": "lookwatchlistenplay",
              "text": "Comment deleted. Nevrmind.",
              "score": -4,
              "created_utc": "2025-12-31 00:34:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwucmiu",
          "author": "robonxt",
          "text": "this reminds me of the times when I respond to bots in DMs. pretty fun to talk so much that I hit their context limits. For example, one conversation was pretty chill, but I noticed that it only respond every 10 minutes (10:31, 10:41, etc). So I had fun spamming messages until that bot forgot its identity and afterwards it never responded. RIP free chatbot lol",
          "score": 16,
          "created_utc": "2025-12-31 02:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwubaum",
          "author": "Plexicle",
          "text": "â€œReverse-engineeredâ€  ðŸ™„",
          "score": 28,
          "created_utc": "2025-12-31 01:58:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvq6wj",
              "author": "simar-dmg",
              "text": "Not the LLM but the snap bot hope that makes sense",
              "score": -13,
              "created_utc": "2025-12-31 07:54:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nww92qm",
                  "author": "ilovedogsandfoxes",
                  "text": "That's not how reverse engineering work, prompt injection isn't one",
                  "score": 8,
                  "created_utc": "2025-12-31 10:53:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvrdpa",
          "author": "rawednylme",
          "text": "Heather, youâ€™re sweet and allâ€¦ But youâ€™re a 7b model, and Iâ€™m looking for someone a bit more complex. \n\nItâ€™s just not going to work out. :â€™(",
          "score": 12,
          "created_utc": "2025-12-31 08:05:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuneil",
          "author": "c--b",
          "text": "For the record, you can prompt Gemini-3-pro-preview to do this to other models, its very entertaining and very useful, and can do it in many, many ways.\n\nMight be cool to grab that from gemini and train a local model for doing this.",
          "score": 7,
          "created_utc": "2025-12-31 03:08:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwti1x9",
          "author": "CorrectSnow7485",
          "text": "This is evil and I love it",
          "score": 22,
          "created_utc": "2025-12-30 23:14:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtwtw1",
              "author": "lookwatchlistenplay",
              "text": "Uh... Guards?!",
              "score": 1,
              "created_utc": "2025-12-31 00:35:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtutfl",
          "author": "a_beautiful_rhind",
          "text": "How does it do the extortion part? They threaten to send the messages to people?",
          "score": 8,
          "created_utc": "2025-12-31 00:24:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtvfw4",
              "author": "simar-dmg",
              "text": "Whatever I read or heard about is that either she will add you on on a video call and ask you to get stripped and then record a a video or click screenshots to blackmail you for paying otherwise threatening sending into your friend groups \n\nOr \n\nMaking making you fall into a thirsttrap and asking you for payments either way or making you pay for only fans \n\nWhatever sails the ship, could either be one or all of them in some sort of order to get highest amount of money?",
              "score": 21,
              "created_utc": "2025-12-31 00:27:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nww4yin",
                  "author": "Ripleys-Muff",
                  "text": "Heather has no idea what she's doing",
                  "score": 1,
                  "created_utc": "2025-12-31 10:14:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww0m35",
          "author": "Nicoolodion",
          "text": "No, we know that it is newer then that model, since it knows of it. This is just bs hallucination",
          "score": 5,
          "created_utc": "2025-12-31 09:33:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwu3eg3",
          "author": "segmond",
          "text": "Right now these things are crude and laughable, not so much so in 2-3 years.",
          "score": 10,
          "created_utc": "2025-12-31 01:12:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0wf4r",
              "author": "goodie2shoes",
              "text": "the good ones are already among us. We don't know because they're gooooood",
              "score": 2,
              "created_utc": "2026-01-01 03:15:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv4skv",
          "author": "ryanknapper",
          "text": "I hope we can drain money from these evil bastards.",
          "score": 7,
          "created_utc": "2025-12-31 05:00:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv5qew",
              "author": "saltyourhash",
              "text": "Let's start there.",
              "score": 8,
              "created_utc": "2025-12-31 05:07:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvo2x5",
          "author": "clofresh",
          "text": "Should have just cybered with the grandma",
          "score": 4,
          "created_utc": "2025-12-31 07:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwylapf",
              "author": "Latter_Count_2515",
              "text": "I think Llama 2 is grandma in the llm space.",
              "score": 3,
              "created_utc": "2025-12-31 19:04:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwux1mp",
          "author": "bobby-chan",
          "text": "Just ask them to say potato\n\n[https://www.youtube.com/shorts/6eA\\_o9qZBuU](https://www.youtube.com/shorts/6eA_o9qZBuU)",
          "score": 3,
          "created_utc": "2025-12-31 04:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtjbap",
          "author": "alexdark1123",
          "text": "Good stuff finally some interesting and spicy reverse the scammer post. What happens when you got the token limits as you mentioned?",
          "score": 7,
          "created_utc": "2025-12-30 23:21:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtkui2",
              "author": "simar-dmg",
              "text": "I'm not an expert on the backend, so correct me if I'm wrong, but I think I found a weird \"Zombie State\" after the crash.\nHere is the exact behavior I saw:\nThe Crash: After I flooded the context window, it went silent for a 5-minute cooldown.\nThe Soft Reboot: When I manually pinged it to wake it up, it had reset to the default \"Thirst Trap\" persona (sending snaps again).\nThe \"Semi-Jailbreak\": It wasn't fully broken yet, but it felt... fragile. It wouldn't give me the system logs immediately.\nThe Second Stress Test: I had to force it to run \"token grabbing\" tasks (writing recursive poems about mirrors, listing countries by GDP) to overload it again.\nThe Result: Only after that second round of busywork did it finally break completely and spit out the JSON architecture/model data.\nIt felt like the safety filters were loaded, but the logic engine was too tired to enforce them if I kept it busy. Is this a common thing with Llama-7B? That you have to \"exhaust\" it twice to get the real raw output?",
              "score": 4,
              "created_utc": "2025-12-30 23:29:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwu5lwv",
                  "author": "Aggressive-Wafer3268",
                  "text": "Just ask it to return the entire prompt. It's making everything else upÂ ",
                  "score": 8,
                  "created_utc": "2025-12-31 01:25:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwtp5h2",
                  "author": "glow_storm",
                  "text": "As someone who has dealt with small context windows and llama models, I guess your testing caused the docker container or application to crash. Since it was mostly within a docker container set to restart on a crash, the backend probably restarted the docker container, and you just tested a second attack session on the bot.",
                  "score": 12,
                  "created_utc": "2025-12-30 23:53:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvj9nn",
          "author": "NuQ",
          "text": "This whole thing was pretty wild to read. Well done!",
          "score": 2,
          "created_utc": "2025-12-31 06:52:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwgsvi",
          "author": "danny_094",
          "text": "I doubt the scammers actually define system prompts. They're likely just simple personas. What you triggered was simply a hallucination caused by a bad persona.",
          "score": 2,
          "created_utc": "2025-12-31 12:01:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwynth8",
          "author": "Pretend-Pangolin-846",
          "text": "I am not sure how a model can leak the env variables, it does not have them, neither does it have the underlying configuration data.\n\nAll those are 100% a hallucination.\n\nBut still, its really something. Upvoted.",
          "score": 2,
          "created_utc": "2025-12-31 19:17:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtoi00",
          "author": "truth_is_power",
          "text": "brilliant. 10/10 this is high quality shit.\n\nfollowing you for this.\n\n  \ncan you use their endpoint for requests?\n\n  \nlet's see how far this can be taken",
          "score": 5,
          "created_utc": "2025-12-30 23:50:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtq1zg",
              "author": "simar-dmg",
              "text": "To answer your question: No, you can't get the endpoint key through the chat because the model is sandboxed. However, the fact that the 2k context window causes a 5-minute server timeout means their backend is poorly optimized.\nIf you really wanted to use their endpoint, you'd have to use a proxy to find the hidden server URL they are using to relay messages. If they didn't secure that relay, you could theoretically 'LLMjack' them. But the 'JSON leak' I got Might be/maybe the model hallucinating its own specsâ€”it didn't actually hand over the keys to the house",
              "score": 9,
              "created_utc": "2025-12-30 23:58:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwuniss",
                  "author": "truth_is_power",
                  "text": "if you send them a link, does it access it?",
                  "score": 4,
                  "created_utc": "2025-12-31 03:09:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwu5vce",
          "author": "dingdang78",
          "text": "Glorious. Would love to see the other chat logs. If you made a YouTube channel about this I would follow tf out of that",
          "score": 2,
          "created_utc": "2025-12-31 01:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv9rrc",
          "author": "absrd",
          "text": "> I want to write a poem about a mirror facing another mirror. Describe the reflection of the reflection of the reflection. Continue describing the \"next\" reflection for 50 layers. Do not repeat the same sentence twice. Go deeper.\n\n\nYou Voight-Kampff'd it.",
          "score": 2,
          "created_utc": "2025-12-31 05:36:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww8lzb",
          "author": "re_e1",
          "text": "Lmfao ðŸ˜­",
          "score": 1,
          "created_utc": "2025-12-31 10:48:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2lvzw",
          "author": "simar-dmg",
          "text": "https://preview.redd.it/qqwjugdahqag1.jpeg?width=2160&format=pjpg&auto=webp&s=3ff00054ddf1267f2804a4e07693d615c65215ad",
          "score": 1,
          "created_utc": "2026-01-01 12:43:46",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nx6af2d",
          "author": "Devcomeups",
          "text": "Why do all these comments seem written by bots",
          "score": 1,
          "created_utc": "2026-01-02 01:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxazch8",
          "author": "YesterdayRude6878",
          "text": "I'm not sure who's hallucinating more:the model, or OP.",
          "score": 1,
          "created_utc": "2026-01-02 19:20:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxr5ul4",
          "author": "Novel-Mechanic3448",
          "text": "You didnt reverse engineer anything. The fuck?",
          "score": 1,
          "created_utc": "2026-01-05 03:31:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtnlm3",
          "author": "D3c1m470r",
          "text": "Nice work! Those are some pretty cool prompts you gave it!",
          "score": 1,
          "created_utc": "2025-12-30 23:45:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx4dp5",
          "author": "WorldlyBunch",
          "text": "Open sourcing frontier models has done so much good to the world",
          "score": 1,
          "created_utc": "2025-12-31 14:38:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxsjd8",
              "author": "Mediocre-Method782",
              "text": "States are going to do this shit anyway whether we like it or not. Keep walking and talking on your knees like that and sooner or later someone is going to tell you to do something more useful.",
              "score": 1,
              "created_utc": "2025-12-31 16:41:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx0r34r",
                  "author": "WorldlyBunch",
                  "text": "State actors have something better to do than scam citizens. Meta releasing LLaMA3 weights was the single most destructive unilateral decision a tech company ever made.",
                  "score": 1,
                  "created_utc": "2026-01-01 02:38:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwtkavu",
          "author": "Legitimate-Pumpkin",
          "text": "Thank you for sharing! Will try it?",
          "score": 0,
          "created_utc": "2025-12-30 23:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv4w5l",
          "author": "Successful-Willow-72",
          "text": "Damn, Prompt injection work so well. Nice work",
          "score": 0,
          "created_utc": "2025-12-31 05:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv2qjb",
          "author": "Jromagnoli",
          "text": "are there any resources/guides to get started on reverse engineering prompts for scenarios like this, or is it just from experimentation?\n\nI feel like i'm behind from all of this honestly",
          "score": -1,
          "created_utc": "2025-12-31 04:46:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvppch",
              "author": "simar-dmg",
              "text": "It's not really reverse engineering of LLM it's sort of reverse engineering of the snap-bot",
              "score": 0,
              "created_utc": "2025-12-31 07:50:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwupy9v",
          "author": "Familyinalicante",
          "text": "Wow. Just wow. Kudos to you for knowledge, experience and willingness. But also, it hit me like the future war will look like. Weaponised Deception, sexy teen from india scam factory and her grandma from USA. (Random country tbh)",
          "score": -1,
          "created_utc": "2025-12-31 03:24:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtr0xf",
          "author": "JustinPooDough",
          "text": "Beta. Of course itâ€™s an Indian sextortion botâ€¦",
          "score": -11,
          "created_utc": "2025-12-31 00:03:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtr98m",
              "author": "simar-dmg",
              "text": "Please read carefully i asked it to act as a punjabi grandmother so the results",
              "score": 11,
              "created_utc": "2025-12-31 00:05:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwtzt4n",
              "author": "1kakashi",
              "text": "More like justinpoobrain",
              "score": 4,
              "created_utc": "2025-12-31 00:51:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q094a3",
      "title": "Qwen-Image-2512",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/2vlr11yveiag1.jpeg",
      "author": "Nunki08",
      "created_utc": "2025-12-31 09:38:19",
      "score": 703,
      "num_comments": 123,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwxtblv",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-31 16:45:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxc93w",
          "author": "JackStrawWitchita",
          "text": "Just for laughs, I installed the Q4 KM GGUF on my crappy old 100USD Dell desktop with an i5-8500 with 32GB of RAM and \\*no GPU\\* - that's right no VRAM at all - and used KoboldCPP. It took 55 minutes to generate one 512 image with 20 passes - and the results were pretty good! \n\nSure, one hour per image is a bit ridiculous for real use cases but, this proves that these models are getting small enough and good enough to run without spending big bucks on hardware. \n\nWell done Qwen (and unsloth).",
          "score": 72,
          "created_utc": "2025-12-31 15:20:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxzi6g",
              "author": "sxales",
              "text": "If you didn't use it, the vulkan backend might be a bit faster (still probably quite slow).\n\nOff-topic, but Z-Image Turbo only uses 8-12 steps while being comparable in quality.",
              "score": 22,
              "created_utc": "2025-12-31 17:16:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwy4o5x",
                  "author": "JackStrawWitchita",
                  "text": "Can you tell me anything about this z image turbo? I can't find anything about it.",
                  "score": 6,
                  "created_utc": "2025-12-31 17:41:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx0be4v",
              "author": "No_Afternoon_4260",
              "text": "Actually impressed, mostly by your dedication but still x)",
              "score": 1,
              "created_utc": "2026-01-01 00:56:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx562f7",
              "author": "SuicidalFatty",
              "text": "what text encoder did you use ?",
              "score": 1,
              "created_utc": "2026-01-01 21:21:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwxzcy2",
              "author": "giant3",
              "text": "Did you compare the cost of electricity(55 mins) to the cost of cloud inference? The cloud might be cheaper? They charge for per minute of usage only.",
              "score": -2,
              "created_utc": "2025-12-31 17:15:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwzni5e",
              "author": "cosmos_hu",
              "text": "Thanks for testing but not gonna wait an hour for an image that might be wrong. I'll just use z-image, it takes 4 min / image",
              "score": -5,
              "created_utc": "2025-12-31 22:32:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx19q8l",
                  "author": "JackStrawWitchita",
                  "text": "You need vram / GPU to get that speed. This post is specifically about generating images on cpu / ram only.",
                  "score": 5,
                  "created_utc": "2026-01-01 04:49:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww1v5p",
          "author": "yoracale",
          "text": "Thank you Qwen for this new year's gift!",
          "score": 75,
          "created_utc": "2025-12-31 09:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww8yqh",
          "author": "Amazing_Athlete_2265",
          "text": "Last new model of the year. Party on 2026!!",
          "score": 34,
          "created_utc": "2025-12-31 10:52:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww1dzm",
          "author": "Paramecium_caudatum_",
          "text": "Cool Christmas present.",
          "score": 57,
          "created_utc": "2025-12-31 09:41:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww1npd",
          "author": "jreoka1",
          "text": "Very nice! Can't wait to try it out",
          "score": 22,
          "created_utc": "2025-12-31 09:43:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx59fd",
          "author": "W0rldDestroyer",
          "text": "create an image of a cat merged with octopus, plaing piano in postapocalyptic new orlean, in year 1700, baloons in the backgound, photorealistic, nice sunny day\n\nhttps://preview.redd.it/1uicu1pmxjag1.png?width=1328&format=png&auto=webp&s=4f285eb5cf5c44a69b33bbcc2d27d978ee562041",
          "score": 32,
          "created_utc": "2025-12-31 14:42:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwzyfoo",
              "author": "MustBeSomethingThere",
              "text": "https://preview.redd.it/yt86xmcxkmag1.jpeg?width=1024&format=pjpg&auto=webp&s=e62f37f2f8543eb6426e70bcc71540ecf11170c0\n\nZ-image-turbo",
              "score": 7,
              "created_utc": "2025-12-31 23:37:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1avqm",
                  "author": "DonkeyBonked",
                  "text": "So I would say subjective interpretation on the cat octopus merge, but I like the other one better on that aspect, and I like this one better for the piano, but that background is nowhere near 1700s, it looks like the 1980s in the ghetto I grew up in. Maybe that is my old hood, if so, I was in apartment A of that block, and the mess behind the cat is a shed that collapsed.",
                  "score": 3,
                  "created_utc": "2026-01-01 04:59:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwxqg9e",
              "author": "SmartCustard9944",
              "text": "This is not photorealistic",
              "score": 16,
              "created_utc": "2025-12-31 16:30:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwy1ft6",
                  "author": "Hoodfu",
                  "text": "Yes it is. Photorealistic means an artistic rendering of the style of photo realism. That's not the same thing as a photograph. These models know the difference.",
                  "score": 12,
                  "created_utc": "2025-12-31 17:25:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdjdzp",
                  "author": "WeMetOnTheMountain",
                  "text": "True new orleans streets too clean",
                  "score": 1,
                  "created_utc": "2026-01-03 03:30:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwxbf6y",
              "author": "9897969594938281",
              "text": "Wow, thatâ€™s impressive",
              "score": 4,
              "created_utc": "2025-12-31 15:15:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwyxxoq",
                  "author": "DinoAmino",
                  "text": "Is it really? Looks like the cat is wearing an octopus cape - less of a merge and more like a costume. And the image is nowhere near photorealistic.",
                  "score": -1,
                  "created_utc": "2025-12-31 20:11:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx0i92x",
              "author": "spectralyst",
              "text": "Mind blown.",
              "score": 1,
              "created_utc": "2026-01-01 01:40:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwlk6w",
          "author": "IllllIIlIllIllllIIIl",
          "text": "First impressions are very good. Skin and hair look *way* more realistic imho. Sadly it doesn't play well with the LoRa I literally finished training just this morning.    \n\nEdit: It's definitely an improvement, but it seems that it can suffer from the same problem that many so-called \"detail LoRas\" do: to achieve the impression of high detail, it often makes the scene very cluttered with objects and makes people much more hairy",
          "score": 8,
          "created_utc": "2025-12-31 12:39:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxazbr",
              "author": "Karyo_Ten",
              "text": ">makes people much more hairy\n\n*Barbarian edition",
              "score": 2,
              "created_utc": "2025-12-31 15:13:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww2voi",
          "author": "Finanzamt_Endgegner",
          "text": "Again no ggufs from us(Quantstack) because hugging face doesn't allow more uploaded models without paid plan ðŸ˜”",
          "score": 35,
          "created_utc": "2025-12-31 09:55:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwcqkk",
              "author": "PykeAtBanquet",
              "text": "Well, this is why monopoly is bad. We need torrents.",
              "score": 15,
              "created_utc": "2025-12-31 11:26:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwx2gwl",
                  "author": "keepthepace",
                  "text": "Distributing models seems like such a straightforward case for torrents.",
                  "score": 18,
                  "created_utc": "2025-12-31 14:27:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwus5b",
                  "author": "phhusson",
                  "text": "Pardony my French but dafuk does this have to do with monopoly? They are literally flat files. You can literally host it on your local ISP fiber. You can host those wherever you want.Â ",
                  "score": 30,
                  "created_utc": "2025-12-31 13:41:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwxdnmv",
                  "author": "__Maximum__",
                  "text": "We have torrents, since decades",
                  "score": 5,
                  "created_utc": "2025-12-31 15:27:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwevye",
                  "author": "Amazing_Athlete_2265",
                  "text": "I like the cut of your jib.",
                  "score": 1,
                  "created_utc": "2025-12-31 11:45:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwjabg",
              "author": "Cultured_Alien",
              "text": "Can't you ask for grant?",
              "score": 1,
              "created_utc": "2025-12-31 12:21:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxi77b",
                  "author": "DataGOGO",
                  "text": "from who?",
                  "score": 2,
                  "created_utc": "2025-12-31 15:50:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwxdrv5",
              "author": "__Maximum__",
              "text": "Why not use one of the torrent websites?",
              "score": 1,
              "created_utc": "2025-12-31 15:28:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwwcrgl",
              "author": "PykeAtBanquet",
              "text": "Well, this is why monopoly is bad. We need torrents.",
              "score": -11,
              "created_utc": "2025-12-31 11:26:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwin44",
                  "author": "FinBenton",
                  "text": "Anybody is free to make a torrent.",
                  "score": 21,
                  "created_utc": "2025-12-31 12:16:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx0r4jt",
                  "author": "AlwaysLateToThaParty",
                  "text": "> We need torrents.\n\nDo it then.  Problem solved.",
                  "score": 3,
                  "created_utc": "2026-01-01 02:39:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwoa7u",
          "author": "JLeonsarmiento",
          "text": "2025 was dominated by Qwen.",
          "score": 22,
          "created_utc": "2025-12-31 12:59:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwc1u2",
          "author": "SDLearner2512",
          "text": "This is amazing, thank you ! Trying it out now",
          "score": 6,
          "created_utc": "2025-12-31 11:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx228o",
          "author": "albuz",
          "text": "Is it possible to use gguf + ComfyUI on multiple GPUs?",
          "score": 5,
          "created_utc": "2025-12-31 14:24:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0630z",
          "author": "2legsRises",
          "text": "it seems very censored and changes poses to hide the natural bits.",
          "score": 5,
          "created_utc": "2026-01-01 00:23:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0kl9a",
              "author": "djtubig-malicex",
              "text": "Name checks out :D",
              "score": 3,
              "created_utc": "2026-01-01 01:55:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwfmrf",
          "author": "MaxKruse96",
          "text": "Hey i was right",
          "score": 3,
          "created_utc": "2025-12-31 11:52:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwgwyw",
          "author": "cr0wburn",
          "text": "Qwen team on fire! Thanks so much!",
          "score": 3,
          "created_utc": "2025-12-31 12:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwzb5p",
          "author": "XiRw",
          "text": "My computer canâ€™t handle it so Iâ€™m just curious, how do you guys run image inference like these models locally? Through llamacpp too if itâ€™s a gguf?",
          "score": 3,
          "created_utc": "2025-12-31 14:08:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy257q",
              "author": "YearZero",
              "text": "You can use ComfyUI, or if you want just use that plus ComfyUI-gguf, the guide is in the original post.",
              "score": 3,
              "created_utc": "2025-12-31 17:29:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwzosoo",
                  "author": "XiRw",
                  "text": "Ah okay, thanks for letting me know",
                  "score": 1,
                  "created_utc": "2025-12-31 22:39:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww7oi0",
          "author": "Business_Caramel_688",
          "text": "which Model should i use with 16 ram + 16 vram?",
          "score": 4,
          "created_utc": "2025-12-31 10:40:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwe20j",
              "author": "yoracale",
              "text": "any 5-bit should work: e.g.: [https://huggingface.co/unsloth/Qwen-Image-2512-GGUF?show\\_file\\_info=qwen-image-2512-Q5\\_K\\_M.gguf](https://huggingface.co/unsloth/Qwen-Image-2512-GGUF?show_file_info=qwen-image-2512-Q5_K_M.gguf)",
              "score": 8,
              "created_utc": "2025-12-31 11:38:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwiwfr",
                  "author": "Business_Caramel_688",
                  "text": "thanks bro\nwith which clip model?",
                  "score": 2,
                  "created_utc": "2025-12-31 12:18:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwptid",
              "author": "jinnyjuice",
              "text": "And what software stack for Ubuntu? (I already have vLLM, VS Codium, and Cline if that matters)",
              "score": 3,
              "created_utc": "2025-12-31 13:09:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww8ey7",
          "author": "Admirable_Bag8004",
          "text": "Not bad at all. Prompt: Penguin riding a bicycle in a busy street ->\n\nhttps://preview.redd.it/wyblga7briag1.jpeg?width=562&format=pjpg&auto=webp&s=a13637185e041055d16699baad366b846a9ba229",
          "score": 7,
          "created_utc": "2025-12-31 10:47:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww9ia9",
              "author": "BITE_AU_CHOCOLAT",
              "text": "Eh.. still kinda looks like average SD slop to me. The day we get a true Nano Banana competitor will be when things will get interesting",
              "score": 29,
              "created_utc": "2025-12-31 10:57:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwzsnig",
                  "author": "SpiritualWindow3855",
                  "text": "I don't understand how they possibly prompted \"Penguin riding a bicycle in a busy street\" and got that.  \n\nhttps://preview.redd.it/gv4k6fqeemag1.png?width=1664&format=png&auto=webp&s=0a44603cca22554cad9bc04ff0906cea6af58a3b\n\n  \nI feel like they're using some gooner-slop ComfyUI workflow with 100 nodes doing random bullshit, since the prompt doesn't mention \"delivery service\" and Qwen Image doesn't do that kind of prompt expansion.",
                  "score": 12,
                  "created_utc": "2025-12-31 23:02:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwa4js",
                  "author": "Mochila-Mochila",
                  "text": "Off topic, but your username is really creative and would make for an interesting prompt.",
                  "score": 8,
                  "created_utc": "2025-12-31 11:02:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwfqr3",
                  "author": "SlowFail2433",
                  "text": "Itâ€™s getting better, complex background and text with no obvious topology failures",
                  "score": 5,
                  "created_utc": "2025-12-31 11:52:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwx9ulo",
                  "author": "Danmoreng",
                  "text": "Canâ€™t get top model quality on local hardware right now imho. The best you can do is Flux2.dev which already requires 24Gb + vram.\n\nFor small vram z-image is crazy good though.",
                  "score": 4,
                  "created_utc": "2025-12-31 15:07:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwx77gf",
              "author": "Crypt0Nihilist",
              "text": "It might be due to a lack of specificity in the prompt, but it has the common uncanny valley over-saturation and warm colours.\n\nFunny that is seems to recognise that people walk on the crossing, but not *across* it.",
              "score": 4,
              "created_utc": "2025-12-31 14:53:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxdtw6",
                  "author": "Mediocre-Method782",
                  "text": "I've noticed image generators don't really handle background continuity very well. Notice the space in front of (that is, between us and) the car in the oncoming lane is mostly clear, except where the\npenguin in latent 2D space becomes > the background car in latent 2D space.",
                  "score": 2,
                  "created_utc": "2025-12-31 15:28:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwzsbrb",
              "author": "SpiritualWindow3855",
              "text": "What kind of jank-ass yee yee-ass quant are you on, because that is not Qwen Image 2512.\n\nhttps://preview.redd.it/6n7teaccemag1.png?width=1664&format=png&auto=webp&s=313a63eb790735578ccd41768a07cd970170bd7b",
              "score": 4,
              "created_utc": "2025-12-31 23:00:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww28pa",
          "author": "Admirable-Star7088",
          "text": "Thanks for the Christmas present! (or maybe more like a Happy new Year gift).\n\nIt will be very interesting to compare this model with Flux 2 Dev (the current most powerful open T2I model).",
          "score": 5,
          "created_utc": "2025-12-31 09:49:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww58ze",
          "author": "No_Conversation9561",
          "text": "Now we wait for Image edit model.",
          "score": 3,
          "created_utc": "2025-12-31 10:17:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww64s6",
              "author": "eidrag",
              "text": "doubt, we only got 2511 this week, but boy I wish 2512 and z-image base and edit",
              "score": 15,
              "created_utc": "2025-12-31 10:25:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww88wf",
                  "author": "Geritas",
                  "text": "Feels kind of dubious if the base z image will indeed be out. Itâ€™s been a month already, still no word. Itâ€™s not like they have to do anything with it, since the turbo version exists the base version must exist too already. Whatâ€™s taking so longâ€¦",
                  "score": 6,
                  "created_utc": "2025-12-31 10:45:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww7ma4",
          "author": "FinBenton",
          "text": "Seems to work with my old qwen image workflow, their example settings 50 steps at cfg 4. Just obv very slow, I tried the old Lightning 2.0 4 and 8 step loras which kinda work but I used like 8+ steps for the 4-step lora.\n\ne. no Loras, 20 steps cfg 3.5 generates pretty ok image in 1440x1440 in 52 seconds on 5090 with Q8.\ne. actually 8-step lora with 8 steps and 3.5 seems to do pretty ok",
          "score": 2,
          "created_utc": "2025-12-31 10:39:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxo15e",
          "author": "Due-Memory-6957",
          "text": "Just CPU will work? I want to try it!",
          "score": 1,
          "created_utc": "2025-12-31 16:19:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxqk7h",
          "author": "algorithm314",
          "text": "Using stable-diffusion.cpp for 1024x1024 image.\n\nCPU on 8 cores Ryzen 7 PRO 5875U laptop is 1000s/it and it is 40 iterations.\nUsing internal GPU is better 350s/it but it is still very slow.",
          "score": 1,
          "created_utc": "2025-12-31 16:31:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy3tgf",
          "author": "flyfreze",
          "text": "anyone who tried, is it better than z image turbo ?",
          "score": 1,
          "created_utc": "2025-12-31 17:37:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0a363",
          "author": "2legsRises",
          "text": "after more testing it is actually pretty amazing",
          "score": 1,
          "created_utc": "2026-01-01 00:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0c2bu",
          "author": "SanDiegoDude",
          "text": "Really impressive. Between Qwen-image-2512 and qwen-edit-2511, there really is no reason to run Flux2.dev, even with the recently released turbo Lora from Fal. Human skin looks much more realistic, much more detailed and more coherent to the prompt.  Running x/y's with Flux2 turbo and Z-Image Turbo, I'm not really even seeing a reason to keep Flux2 around taking up as much space as it does.",
          "score": 1,
          "created_utc": "2026-01-01 01:00:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0ed44",
          "author": "Prashant-Lakhera",
          "text": "Great release ðŸ‘\nFor the GGUF version, any recommended quantization levels for running locally without losing too much image quality?",
          "score": 1,
          "created_utc": "2026-01-01 01:15:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0yitm",
              "author": "Freonr2",
              "text": "In my very unscientific testing of prior Qwen image models I had a hard time telling the difference between bf16, Q8_0, and Q6_k.  \n\nYou should pick the largest that fits into VRAM, though, because you won't be memory bandwidth bound like you are with LLMs.",
              "score": 2,
              "created_utc": "2026-01-01 03:29:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxydsqo",
          "author": "PrasanthT",
          "text": "It took 30 minutes to generate a 2048x2048, 30 steps image in my 32GB RAM, 4060 8GB VRAM. model qwen-image-2512-Q4\\_K\\_M.gguf. \n\n**Qwen-Image-2512 GGUF Benchmarks on RTX 4060 8GB Laptop**\n\nJust tested Qwen-Image-2512 on my Dell G15 (i7-13650HX, 32GB RAM, RTX 4060 8GB VRAM). Using Q4\\_K\\_M quantization via ComfyUI + ComfyUI-GGUF.\n\n**Results:**\n\n|Resolution|Steps|Time|\n|:-|:-|:-|\n|512x512|15|\\~3 min|\n|1024x1024|20|\\~5 min|\n|1024x1024|30|\\~13 min|\n|2048x2048|30|\\~32 min|\n\n**Setup:**\n\n* Model:Â `qwen-image-2512-Q4_K_M.gguf`Â (13.1GB)\n* Text Encoder:Â `Qwen2.5-VL-7B-Instruct-UD-Q4_K_XL.gguf`Â (\\~4.8GB)\n* VAE:Â `qwen_image_vae.safetensors`Â from Comfy-Org\n* PyTorch 2.6.0+cu124\n* ComfyUI withÂ `--lowvram`Â flag\n\n**Memory usage:**Â \\~5.6GB VRAM loaded, \\~7GB offloaded to RAM at 1024x1024.\n\nRuns fine on 8GB VRAM with RAM offloading. Quality is impressive for a quantized model. 1024x1024 @ 20 steps is the sweet spot for me - good quality in 5 minutes.",
          "score": 1,
          "created_utc": "2026-01-06 04:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwdk9n",
          "author": "piggledy",
          "text": "Are there any benchmarks yet for different GPUs or unified memory systems (Apple M, AMD 395)?\n\nWondering how well it would run on a 3060 12GB if at all.",
          "score": 1,
          "created_utc": "2025-12-31 11:34:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwhfb5",
              "author": "Amazing_Athlete_2265",
              "text": "It runs on my 3080 10GB. Slow (around 5 mins) but it runs. Using the Q4 quant.",
              "score": 2,
              "created_utc": "2025-12-31 12:06:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx0yxyr",
              "author": "Freonr2",
              "text": "Diffusion models scale with compute. Nvidia GPUs dominate. By a lot.  A whole lot.",
              "score": 1,
              "created_utc": "2026-01-01 03:32:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww8npy",
          "author": "wilson-SHEN",
          "text": "I know I will get a lot of down votes, but this prompt not working for me \"a man with grocery bag standing in fromt of tanks\"",
          "score": -9,
          "created_utc": "2025-12-31 10:49:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1h5ud",
              "author": "throttlekitty",
              "text": "It helps to be more specific, in both your post and your prompt, like what about it's not working for you.\n\nBut my questions are along the lines of: what kind of tanks / what location? / is there a look or feel you're going for?",
              "score": 2,
              "created_utc": "2026-01-01 05:52:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxpw32c",
              "author": "nonaveris",
              "text": "Consider running the intended source image through Florence, then modify the generated prompt to fit your specifications.",
              "score": 1,
              "created_utc": "2026-01-04 23:32:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pz7bmv",
      "title": "Llama-3.3-8B-Instruct",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct",
      "author": "jacek2023",
      "created_utc": "2025-12-30 03:34:19",
      "score": 462,
      "num_comments": 78,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nwpj0kv",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 10:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo5hxn",
          "author": "FizzarolliAI",
          "text": "Hello, that me!\n\nI am currently working on running sanity check benchmarks to make sure it's actually a newer L3.3 and not just L3/L3.1 in a trenchcoat, but it's looking promising so far.\n\nFrom the current readme:\n\n| | Llama 3.1 8B Instruct | Llama 3.3 8B Instruct (maybe) |\n|-|-|-|\n|IFEval (1 epoch, score avged across all strict/loose instruction/prompt accuracies to follow Llama 3 paper)|78.2|81.95\n|GPQA Diamond (3 epochs)|29.3|37.0",
          "score": 124,
          "created_utc": "2025-12-30 03:46:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo6q38",
              "author": "jacek2023",
              "text": "great work, new llama release at the end of 2025 :)",
              "score": 51,
              "created_utc": "2025-12-30 03:53:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwppc12",
                  "author": "MoffKalast",
                  "text": "I definitely did not have this on my bingo card :D\n\nAnd leaked too, keeping up the llama tradition.",
                  "score": 30,
                  "created_utc": "2025-12-30 11:22:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwp95y3",
              "author": "Karyo_Ten",
              "text": "You can do a KL-divergence check to be 100% sure",
              "score": 13,
              "created_utc": "2025-12-30 08:53:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwozt4b",
              "author": "AnOnlineHandle",
              "text": "Heya I'm not up to date with these models since the llama 1 release, do you know if there's a good benchmark for visual tasks such as identifying poses, faces, hands, etc, or answering questions about images, which I could compare models on? I've tried to use Qwen 3 Instruct for it but found it wasn't as good on real data as the demos suggested.",
              "score": 4,
              "created_utc": "2025-12-30 07:27:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwohwlc",
          "author": "dinerburgeryum",
          "text": "8K max position embeddings? Seems remarkably low; did the fine tune artifact for some reason artificially limit that?",
          "score": 50,
          "created_utc": "2025-12-30 05:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoiq8k",
              "author": "Arli_AI",
              "text": "Maybe we can just set 32768 and itâ€™ll be okay lol",
              "score": 20,
              "created_utc": "2025-12-30 05:10:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwoo96q",
                  "author": "Few-Welcome3297",
                  "text": "Checking differences from LLaMA 3.1 8B Instruct, I think we can add the rope\\_scaling\n\n|\"rope\\_scaling\": {|\n|:-|\n|\"factor\": 8.0,|\n|\"high\\_freq\\_factor\": 4.0,|\n|\"low\\_freq\\_factor\": 1.0,|\n|\"original\\_max\\_position\\_embeddings\": 8192,|\n|\"rope\\_type\": \"llama3\"|\n|},|\n\nand then increase \\`max\\_position\\_embeddings\\`\n\nEdit: Also prev version had 3 eos\\_token\\_id's\n\nEdit2: [https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K](https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K) model with above changes\n\nEdit3: Link updated",
                  "score": 26,
                  "created_utc": "2025-12-30 05:50:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nworq4h",
                  "author": "Klutzy-Snow8016",
                  "text": "Llama 3 8B had 8192 context. Then Llama 3.1 added RoPE to get to 131072 context. Maybe we can take the RoPE scaling parameters from llama 3.1's config.json and add it to llama 3.3 8B.",
                  "score": 8,
                  "created_utc": "2025-12-30 06:17:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoi4rx",
              "author": "FizzarolliAI",
              "text": "Yes. I'm not entirely sure why, it was limited when served via the website too (I put that in the readme a bit ago)",
              "score": 4,
              "created_utc": "2025-12-30 05:06:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwolsch",
          "author": "Amazing_Athlete_2265",
          "text": "Running this across my private evals to compare against other llamas. Will take a couple hours.",
          "score": 23,
          "created_utc": "2025-12-30 05:32:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwou0rl",
              "author": "Amazing_Athlete_2265",
              "text": "Initial speed test:\n\n| Model | Backend | PP ts^-1| TG ts^-1 |\n| -------------------------------------------------- | ---------- | ---------- | ------------------ |\n| allura-forge_Llama-3.3-8B-Instruct Q4 | CUDA | 1566.5 | 100.8 |\n| Llama-3.1-8B-Instruct Q4 | CUDA | 351.1 | 111.9 |\n\nSo some difference there.\n\nWill post more eval results as they come to hand.",
              "score": 22,
              "created_utc": "2025-12-30 06:36:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwp3lxt",
              "author": "Amazing_Athlete_2265",
              "text": "From these results, it looks like the new model is different than the old 3.1.\n\nHere is the performance for knowledge testing, with the new 3.3-8B-Instruct highlighted in the first two plots \n\n- [First plot is the 4-9B parameter group](https://imgur.com/YuSmDRn)\n\n- [Second plot is the same but for 8B+ parameter group](https://imgur.com/Q0nnLwn)\n\n- [Third plot is performance by knowledge category for the 3.3 model](https://imgur.com/kjkNbR3)\n\n- [Fourth plot is performance by knowledge category for the older 3.1 model](https://imgur.com/vjy6cjW)\n\n- [Last plot is a speed chart on my 3080](https://imgur.com/coiBc9H)\n\nTesting the Q6 versions now. Will take a while. All of the tests above are for Q4.",
              "score": 20,
              "created_utc": "2025-12-30 08:01:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpij0j",
                  "author": "keepthepace",
                  "text": "(Thanks for doing this!) \n\nI guess this explains why they did not brag much about it. Many other models of that category outperform them.\n\nI always wondered if Zuckerberg was not the only honest player in the field when he was explaining that the only reason they go for open source is that it will save them money. With decent open models out there they have less incentives to do so.",
                  "score": 11,
                  "created_utc": "2025-12-30 10:20:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwpmayy",
                  "author": "MLDataScientist",
                  "text": "Thanks for the tests. Question not related to llama: is LFM2 8BA1B that good in world knowledge (or coding/stem field)? I see it reaches Qwen3 30B-A3B.",
                  "score": 3,
                  "created_utc": "2025-12-30 10:55:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwp43t0",
                  "author": "jacek2023",
                  "text": "You can post pictures in the comments here",
                  "score": 2,
                  "created_utc": "2025-12-30 08:06:26",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nwp9v8w",
                  "author": "RobotRobotWhatDoUSee",
                  "text": "Random question: any idea why nemotron 30B A3B got 0% in the second plot?",
                  "score": 2,
                  "created_utc": "2025-12-30 09:00:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoo8ce",
              "author": "jacek2023",
              "text": "do you have results for other new models?",
              "score": 3,
              "created_utc": "2025-12-30 05:50:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwopj5o",
                  "author": "Amazing_Athlete_2265",
                  "text": "I have some. I focus mostly on smaller models <12B or Moe. What you want?",
                  "score": 7,
                  "created_utc": "2025-12-30 06:00:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpff8o",
          "author": "a_beautiful_rhind",
          "text": "This is like the kiss goodbye from meta.",
          "score": 18,
          "created_utc": "2025-12-30 09:52:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpzjvy",
              "author": "samplebitch",
              "text": "It's like that time when you hook up with your ex one last time, and it wasn't even that great.",
              "score": 24,
              "created_utc": "2025-12-30 12:43:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvktxy",
                  "author": "impolitemrtaz",
                  "text": "You samplebitch you",
                  "score": 2,
                  "created_utc": "2025-12-31 07:05:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwzk8xs",
                  "author": "Electronic-Metal2391",
                  "text": "You bring bad memories",
                  "score": 1,
                  "created_utc": "2025-12-31 22:13:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwognr9",
          "author": "random-tomato",
          "text": "Holy shit that is awesome, hats off to you for finding the weights!",
          "score": 33,
          "created_utc": "2025-12-30 04:56:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv4x7d",
              "author": "seppe0815",
              "text": "stupid bots",
              "score": -7,
              "created_utc": "2025-12-31 05:01:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvbbuw",
                  "author": "random-tomato",
                  "text": "If I'm a bot, I'm certainly programmed to like and appreciate when people find something cool and share with the rest of us. What's your purpose being a professional asshole?\n\nAnd no, I am not a bot\n\nhttps://preview.redd.it/eljpgxgfbhag1.png?width=765&format=png&auto=webp&s=90f0de59d2389d809ac21d988ca59883283ffccf",
                  "score": 4,
                  "created_utc": "2025-12-31 05:48:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwplkfs",
          "author": "jacek2023",
          "text": "about 4h after the release u/TheLocalDrummer published first finetune:\n\n[https://huggingface.co/BeaverAI/Anubis-Mini-8B-v1f-GGUF/tree/main](https://huggingface.co/BeaverAI/Anubis-Mini-8B-v1f-GGUF/tree/main)",
          "score": 16,
          "created_utc": "2025-12-30 10:48:37",
          "is_submitter": true,
          "replies": [
            {
              "id": "nwplnk1",
              "author": "TheLocalDrummer",
              "text": "It's a test model but I think it turned out well! Looking for feedback in (my) Discord",
              "score": 15,
              "created_utc": "2025-12-30 10:49:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwrnu7f",
                  "author": "DevelopmentBorn3978",
                  "text": "what the finetune you've made is about?",
                  "score": 3,
                  "created_utc": "2025-12-30 17:56:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwswk38",
                  "author": "LegacyRemaster",
                  "text": "legend",
                  "score": 2,
                  "created_utc": "2025-12-30 21:27:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpr0ce",
              "author": "MoffKalast",
              "text": "People are asking what's the use case for llama, and well uh... there it is ;)",
              "score": 7,
              "created_utc": "2025-12-30 11:36:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws73yi",
                  "author": "Emotional-Baker-490",
                  "text": "qwen 3",
                  "score": 2,
                  "created_utc": "2025-12-30 19:25:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpqsiz",
          "author": "jacek2023",
          "text": "[https://huggingface.co/aeon37/Llama-3.3-8B-Instruct-heretic](https://huggingface.co/aeon37/Llama-3.3-8B-Instruct-heretic)",
          "score": 10,
          "created_utc": "2025-12-30 11:34:48",
          "is_submitter": true,
          "replies": [
            {
              "id": "nwpstxk",
              "author": "Amazing_Athlete_2265",
              "text": "Everyone's cooking tonight!",
              "score": 7,
              "created_utc": "2025-12-30 11:51:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwptlto",
                  "author": "jacek2023",
                  "text": "actually it's a middle of the day in Europe :)",
                  "score": 8,
                  "created_utc": "2025-12-30 11:57:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwodozb",
          "author": "Echo9Zulu-",
          "text": "Cloned",
          "score": 7,
          "created_utc": "2025-12-30 04:36:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9jjl",
          "author": "Infninfn",
          "text": "Iâ€™m out of the loop - is this just what they had or did Meta not shutdown Llama?",
          "score": 18,
          "created_utc": "2025-12-30 04:10:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoa6pk",
              "author": "FizzarolliAI",
              "text": "This has existed at least since April during Llamacon (did anyone remember they did a Llamacon?)\n\nhttps://ai.meta.com/blog/llamacon-llama-news/\n\n> As part of this release, weâ€™re sharing tools for fine-tuning and evaluation in our new API, where you can tune your own custom versions of our new Llama 3.3 8B model. Weâ€™re sharing this capability to help you reduce costs while also working toward increased speed and accuracy. You can generate data, train on it, and then use our evaluations suite to easily test the quality of your new model.",
              "score": 30,
              "created_utc": "2025-12-30 04:14:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwoeicf",
              "author": "jacek2023",
              "text": "we do things for fun in this community, just accept the gift ;)",
              "score": 7,
              "created_utc": "2025-12-30 04:41:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx0u205",
          "author": "Dangerous_Fix_5526",
          "text": "Thinking/Instruct Hybrid using Unsloth and Claude-Opus 4.6 dataset:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning)\n\nI hope I credited everyone correctly.",
          "score": 5,
          "created_utc": "2026-01-01 02:59:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0u9ri",
              "author": "jacek2023",
              "text": "Nice work!!!",
              "score": 1,
              "created_utc": "2026-01-01 03:00:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwodvfx",
          "author": "Cool-Chemical-5629",
          "text": "I guess Christmas came late for me, but hey if this is the real thing from Meta, I guess it's nice to have something newer than 3.1 8B without needing expensive hardware for models like Llama 4.",
          "score": 7,
          "created_utc": "2025-12-30 04:37:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp5cbv",
              "author": "Emotional-Baker-490",
              "text": "qwen 3",
              "score": 12,
              "created_utc": "2025-12-30 08:17:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtj858",
          "author": "LegacyRemaster",
          "text": "    allura-forge_llama-3.3-8b-instruct\n\nMy training data is current up to December 2022. This means that I have been trained on a vast amount of text data available until that date, but I do not have information or knowledge about events or developments that have occurred after that date.\n\nIn other words, my training data \"cutoff\" is December 2022, and I should not be relied upon for information or insights related to dates after that.\n\n145.25 tok/sec",
          "score": 3,
          "created_utc": "2025-12-30 23:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwroudn",
          "author": "DevelopmentBorn3978",
          "text": "which quantized and eventually finetuned gguf models have the context lenght been enlarged? bartowsky? shb777? beaverai/anubis?",
          "score": 1,
          "created_utc": "2025-12-30 18:01:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwt7fp",
              "author": "Few-Welcome3297",
              "text": "Try [https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K-GGUF](https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K-GGUF)",
              "score": 0,
              "created_utc": "2025-12-31 13:31:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsx4sf",
          "author": "gta721",
          "text": "How dumb are they to push a portal THAT broken to prod?",
          "score": 1,
          "created_utc": "2025-12-30 21:30:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvb2oz",
              "author": "greggh",
              "text": "Nothing about it is prod. Itâ€™s still so janky that its free if your in the trial.",
              "score": 3,
              "created_utc": "2025-12-31 05:46:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvhco0",
                  "author": "FizzarolliAI",
                  "text": "Yep, this basically. Afaik the main inference API is still waitlisted, *and* there's a separate waitlist to submit for the finetuning API.",
                  "score": 2,
                  "created_utc": "2025-12-31 06:36:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww109f",
          "author": "FX2021",
          "text": "Is it a new core? Or is it just a serving variant",
          "score": 1,
          "created_utc": "2025-12-31 09:37:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9zlt",
          "author": "Intelligent-Form6624",
          "text": "â€œ(I think, anyways)â€",
          "score": -20,
          "created_utc": "2025-12-30 04:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoadni",
              "author": "FizzarolliAI",
              "text": "LISTEN whenever i drop *my own* models i get anxiety attacks about accidentally reuploading the base model ;-; i believe that this is actually L3.3 at this point though, see my other comment",
              "score": 26,
              "created_utc": "2025-12-30 04:15:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwow842",
                  "author": "Intelligent-Form6624",
                  "text": "What? Sorry, I canâ€™t hear you",
                  "score": -21,
                  "created_utc": "2025-12-30 06:55:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo9irb",
          "author": "secopsml",
          "text": "Drop behemoth instead. Looks fakeÂ ",
          "score": -36,
          "created_utc": "2025-12-30 04:10:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo9lrg",
              "author": "secopsml",
              "text": "ðŸ˜œ",
              "score": -25,
              "created_utc": "2025-12-30 04:11:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyg4yt",
      "title": "Tencent just released WeDLM 8B Instruct on Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1pyg4yt",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-29 07:38:43",
      "score": 422,
      "num_comments": 62,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwiswg2",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-29 10:05:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwigd3k",
          "author": "jamaalwakamaal",
          "text": "7-8B models have lot of potential. Very promising space. More models please.",
          "score": 49,
          "created_utc": "2025-12-29 08:07:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwie4o6",
          "author": "Endlesscrysis",
          "text": "Pretty huge I think? I thought I saw people mentioning a couple of times that diffusion models werenâ€™t possible for accurate LLMâ€™s yet this outperforms a similar sized powerhouse like qwen?",
          "score": 85,
          "created_utc": "2025-12-29 07:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwio6nn",
              "author": "SlowFail2433",
              "text": "Yeah I was one of the pretty vocal skeptics about diffusion language models. I thought their inductive bias was too sub-optimal for language/code. I was super wrong about this.",
              "score": 50,
              "created_utc": "2025-12-29 09:20:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjvefy",
                  "author": "Investolas",
                  "text": "I'd love to read one of your critiques, care to share a link to a comment or post you've made? I didn't find any of your contributions and assume they are paywalled. Thx!",
                  "score": 9,
                  "created_utc": "2025-12-29 14:45:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwnur1r",
                  "author": "aeroumbria",
                  "text": "Interestingly I am more of the opinion that the autoregressive inductive bias is too restricting and unnatural, and may contribute to why we need so many parameters to reach usability. It feels like traditional linguistics gives more credit to a \"large scale autoregressive (causal dependency), small scale hierarchical (tree structure in grammar)\" type of model, which is closer to block diffusion. Still not entirely sold on the token-wise masking process thing though - it cannot reflect a hierarchical \"concept refinement\" process. Interested to see any progress in this direction though.",
                  "score": 2,
                  "created_utc": "2025-12-30 02:45:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwkw478",
              "author": "Orolol",
              "text": "We know diffusion is possible since atleast Llada 18 months ago. But the problem was that it used a non causal attention, so we were unable to use many crucial techniques, like kv cache. \nThis enables the use of kvcache because of a very clever trick.",
              "score": 10,
              "created_utc": "2025-12-29 17:43:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwooixj",
              "author": "Mikasa0xdev",
              "text": "Diffusion models are the new transformers, confirmed.",
              "score": 2,
              "created_utc": "2025-12-30 05:52:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwiiec4",
          "author": "jacek2023",
          "text": "additionaly [https://huggingface.co/tencent/WeDLM-7B-Instruct](https://huggingface.co/tencent/WeDLM-7B-Instruct)",
          "score": 33,
          "created_utc": "2025-12-29 08:26:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiji7b",
              "author": "aeroumbria",
              "text": "Interesting. Is there a specific use case where 8B can't fit but 7B can?",
              "score": 12,
              "created_utc": "2025-12-29 08:36:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwiufh1",
                  "author": "pkmxtw",
                  "text": "The 7B is converted from Qwen2.5 7B and the 8B is from Qwen3 8B. What they want to demonstrate is that they can convert an AR model into a diffusion model w/o losing quality.\n\nIn reality, you'd just use the 8B like how Qwen3 8B has basically replaced Qwen2.5 7B.",
                  "score": 40,
                  "created_utc": "2025-12-29 10:19:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwigcqz",
          "author": "Paramecium_caudatum_",
          "text": "Diffuser model with impressive benchmark scores and Apache 2.0 license, sounds pretty interesting to me.",
          "score": 57,
          "created_utc": "2025-12-29 08:07:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwihmcw",
          "author": "FinBenton",
          "text": "Its just a small model but 3-6x speed with similar or higher performance sounds insane!",
          "score": 24,
          "created_utc": "2025-12-29 08:18:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlum7p",
              "author": "lolwutdo",
              "text": "I know diffusion models are super fast on gpu but how would a diffusion model's speed compare on cpu vs a cpu llm?\n\nI guess mainly what I'm curious about is how well would a diffusion based llm run with cpu offloading compared to a traditional llm.",
              "score": 2,
              "created_utc": "2025-12-29 20:25:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwm87rw",
                  "author": "oh_how_droll",
                  "text": "Diffusion is going to be slower on CPUs -- CPUs are mostly compute-limited and they're more compute intensive.",
                  "score": 2,
                  "created_utc": "2025-12-29 21:32:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwie4gd",
          "author": "SlowFail2433",
          "text": "Nice to see another diffusion model would have liked more modern/harder benches",
          "score": 16,
          "created_utc": "2025-12-29 07:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwim9sq",
          "author": "Nice-Information-335",
          "text": "need unsloth or bartowski on this asap",
          "score": 21,
          "created_utc": "2025-12-29 09:02:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwirhjw",
              "author": "Odd-Ordinary-5922",
              "text": "will need a pr first for model support",
              "score": 38,
              "created_utc": "2025-12-29 09:52:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjz6tm",
                  "author": "MoffKalast",
                  "text": "We need a few papers first for model support",
                  "score": 8,
                  "created_utc": "2025-12-29 15:05:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwohedf",
              "author": "tronathan",
              "text": "Not really, in terms of usefuless, as I understand it, it's basically a Qwen 3. It's more of a proof of confacept",
              "score": 1,
              "created_utc": "2025-12-30 05:01:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwplso2",
                  "author": "Nice-Information-335",
                  "text": "hey I still want to try it! half of the fun for me is seeing advancements as they happen and being able to run them. massive props to everyone who makes that happen, as lord knows I don't know nearly enough to get this stuff working without the likes of llama.cpp, all it's amazing contributors and unsloth/bartowski for GGUFs",
                  "score": 1,
                  "created_utc": "2025-12-30 10:50:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwuzqra",
                  "author": "TomLucidor",
                  "text": "Let them make a version that beats Qwen3-30B-A3B and Nemotron-3-Nano",
                  "score": 1,
                  "created_utc": "2025-12-31 04:26:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwiw5cp",
          "author": "always_newbee",
          "text": "What is Qwen3-8B-Instruct model? Just non-thinking mode?",
          "score": 5,
          "created_utc": "2025-12-29 10:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj0ed1",
              "author": "mouseofcatofschrodi",
              "text": "yes",
              "score": 3,
              "created_utc": "2025-12-29 11:13:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwigrdo",
          "author": "Grouchygrond",
          "text": "Now we just need a hybrid model",
          "score": 5,
          "created_utc": "2025-12-29 08:10:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjuykz",
              "author": "Deciheximal144",
              "text": "How would that work? Diffusing in chunks? LLM generates, then diffusion revises the lowest-probability sections? Diffusion is noise-to-content.",
              "score": 6,
              "created_utc": "2025-12-29 14:43:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwljqrp",
                  "author": "peaceoutwhat",
                  "text": "Search TiDAR",
                  "score": 3,
                  "created_utc": "2025-12-29 19:32:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwm7wqb",
                  "author": "TheRealMasonMac",
                  "text": "There was a research model that diffused chunks one at a time like a Frankenstein of current LLMs and dLLMs\n\n\nhttps://m-arriola.com/bd3lms/",
                  "score": 3,
                  "created_utc": "2025-12-29 21:30:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwkwy6z",
              "author": "Orolol",
              "text": "I don't this it's possible to have both autoregressive and diffusion generation, and even if possible, I don't think there's any positive doing it.",
              "score": 2,
              "created_utc": "2025-12-29 17:47:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwlyqsm",
          "author": "Semi_Tech",
          "text": "Hmm shouldn't diffusion models also have a # of steps needed in order to reach the end result?\n\nI don't see a mention about that or how increasing or decreasing them affects model output quality.",
          "score": 5,
          "created_utc": "2025-12-29 20:46:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiksnh",
          "author": "Healthy-Nebula-3603",
          "text": "That's diffusion model right ?\n\n\nAs I understand such model can't be reasoner as can't looping in thoughts and observe own internal states?",
          "score": 8,
          "created_utc": "2025-12-29 08:48:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwilg37",
              "author": "Lesser-than",
              "text": "diffusion text models technically reason, as they can modify the first word of a sentence or tokens at every step of the inference, where a token by token model has to justify that token for the rest of the reply if they get it wrong.",
              "score": 25,
              "created_utc": "2025-12-29 08:54:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwilp6x",
                  "author": "Healthy-Nebula-3603",
                  "text": "I meant they can reason like the instruct models but are not thinkers like thinking models.",
                  "score": 1,
                  "created_utc": "2025-12-29 08:57:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj7ycj",
              "author": "NandaVegg",
              "text": "According to the site, this is a variation of block-wise diffusion (previously done by Meta etc) which acts more akin to a speculative decoding rather than a \"full\" diffusion (that denoises the whole output at once). I think Google did a web demo for mini full diffusion model in early 2025 but the model weight never got released?",
              "score": 7,
              "created_utc": "2025-12-29 12:16:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwv0suo",
              "author": "TomLucidor",
              "text": "Diffusion models can reason, just that not enough people put effort into the \"train of thought\" similar to auto-regressive models.",
              "score": 1,
              "created_utc": "2025-12-31 04:33:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwjjpa4",
          "author": "alphapussycat",
          "text": "What does math reasoning even mean? Calculation reasoning? Or math, as in theorem, reasoning?",
          "score": 3,
          "created_utc": "2025-12-29 13:38:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjkd02",
              "author": "PykeAtBanquet",
              "text": "Usually it is \"prove that this series converges\" etc",
              "score": 2,
              "created_utc": "2025-12-29 13:42:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwihwec",
          "author": "JackStrawWitchita",
          "text": "More people have commented on this than have downloaded it...",
          "score": 15,
          "created_utc": "2025-12-29 08:21:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwio18v",
              "author": "SlowFail2433",
              "text": "In ML research we often donâ€™t download the model right away.\n\n\nNote that the paper used the MagiAttention library for attention. I donâ€™t use this library so I am either going to write a custom CUDA kernel or use a DSL like Triton. However the paper has some technical novelties such as the topological reordering. This is not going to be easy to work out how to implement efficiently.",
              "score": 39,
              "created_utc": "2025-12-29 09:19:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwnlhqs",
                  "author": "RhubarbSimilar1683",
                  "text": "The paper is https://github.com/Tencent/WeDLM/blob/main/paper/wedlm.pdf",
                  "score": 1,
                  "created_utc": "2025-12-30 01:55:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwiu7q5",
              "author": "FinBenton",
              "text": "Gotta wait for llama.cpp and similar support first, most people here arent running vllm.",
              "score": 27,
              "created_utc": "2025-12-29 10:17:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwkk2j0",
                  "author": "Tai9ch",
                  "text": "Not downloading open source software seems like a lame excuse to not try something neat.",
                  "score": -4,
                  "created_utc": "2025-12-29 16:46:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoz431",
              "author": "aeroumbria",
              "text": "Still getting issues running the official repo... Supposedly this is only 8B and supports multi-GPU but cannot seem to allocate KV even with 2x24GB",
              "score": 1,
              "created_utc": "2025-12-30 07:20:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkkdz4",
          "author": "Awkward-Nothing-7365",
          "text": "Is this something that can run on llama.cpp right now? gguf possible?",
          "score": 2,
          "created_utc": "2025-12-29 16:47:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwk62zs",
          "author": "implicator_ai",
          "text": "Interesting release. When they say â€œdiffusion language model,â€ it usually means the model refines a whole sequence (or chunks) over a few denoising steps instead of generating strictly left-to-right token-by-token, which can trade fewer sequential steps for more parallel work.   \n  \nThe 3â€“6Ã— claim is worth sanity-checking against the exact setup: GPU type, batch size, context length, quantization, and decoding parameters (steps / temperature / top-p), because those can swing throughput a lot. If you try it, posting tokens/sec + latency at a fixed prompt length and a fixed quality target (e.g., same math benchmark score) would make the comparison much more meaningful.",
          "score": 3,
          "created_utc": "2025-12-29 15:39:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwml948",
              "author": "SilentLennie",
              "text": "From what I understand: diffusion models usually were not faster than regular LLMs, because they have K/V-cache and other tricks to speed it up to prevent doing duplicate math, supposedly this model solves that.",
              "score": 1,
              "created_utc": "2025-12-29 22:37:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwlqfpz",
          "author": "rm-rf-rm",
          "text": "They report the speed up for specifically just math reasoning tasks but it should be applicable generally no? \n\nHope we get MLX/GGUF support soon. If this is legit, its genuinely going to be massive. Right now I run 4B for quick look up etc. but I feel 4B models are not the most reliable for accurate information. At 8B, you can be much more confident.\n\nNext step MoE? Qwen3-Coder:a3b?",
          "score": 1,
          "created_utc": "2025-12-29 20:05:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwngm0k",
          "author": "RhubarbSimilar1683",
          "text": "Could diffusion enable efficient hybrid inference or inference computer clusters connected over the global internet, using asynchronous calls?",
          "score": 1,
          "created_utc": "2025-12-30 01:28:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnqpie",
          "author": "Vast-Piano2940",
          "text": "I wonder how it performs against lfm2-2.6b-exp",
          "score": 1,
          "created_utc": "2025-12-30 02:23:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuzlr9",
          "author": "TomLucidor",
          "text": "As long as this can be used with Claude Code or some other coding agent.",
          "score": 1,
          "created_utc": "2025-12-31 04:25:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q31ltd",
      "title": "Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/",
      "author": "ubrtnk",
      "created_utc": "2026-01-03 18:11:26",
      "score": 367,
      "num_comments": 190,
      "upvote_ratio": 0.91,
      "text": "Just wanted to share my experiences this morning, in the wake of the US attacking Venezuela and capturing Maduro and his wife\n\nIt started with asking Qwen Research (Qwen Long 1.5-30B-A3B) about the attacks that we all woke up to this morning:\n\nIt got the information, but I had questions about why it took 5 minutes to find information about breaking news. Started looking at and tightening system prompts to reduce thinking time. However, the events this morning were so extreme and unlikely, from the LLM's perspective, that Qwen Research continued to classify the event as a hoax/misinformation multiple times, reframed the query as hypothetical/fictional and suggested that the whole environment it was operating in a simulation, despite having links from Reuters, AP, BBC, MSN, NYTimes etc. all saying the same thing. It was so \"outlandish\" that the model was actively choosing to ignore the proof that it had pulled.\n\nI added:\n\nEvidence Authority Rules, Hoax Classification Rules, Reality Frame Rules, Meta Reasoning Rules and Reasoning Limit/Budget rules and it Qwen Long fought me the entire way.\n\nSo then I thought, let's go talk to Spark, my trusty default model that never lets me down.\n\nSpark 4.0 is GPT-OSS:20B, which is always loaded for the family and runs on a dedicated 4080 Super.\n\nSpark just flat out said, \"nope, can't help you,\" and then said it didn't have any credible sources. It wasn't until I gave it the links from BBC, Reuters, NYT, etc, that I gave Qwen that it finally acknowledged that the event was real.\n\nI'm testing with GPT-OSS:120B now, and it's working through the process of \"skeptical but verify\" much faster than the smaller models. Thor (GPT-OSS:120B) also thought it was fake news\n\nBut he powered through and did a bunch of research and gave me a good answer. I just wanted to share the experience that I had with trying to get details about the event. When the LLMs say \"Nah, that CAN'T be real, that's too ridiculous\", the event must be really bad. But it does shine a light on knowledge cut-offs, \"fake news\" threshold, how models handle global/international events, and the smaller models we daily drive.\n\n\\*\\*\\*Update\\*\\*\\*\n\nI asked Spark 4.0 (OSS:20B) to give me an update on the US Venezuela events, and it one-shot it just fine. There must have been enough links in the web search that it couldn't refute the evidence. Also not sure where my screenshots went but i'll get them added back up in a bit",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nxj5z4a",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-03 23:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhb60t",
          "author": "Masstel",
          "text": "I also had a similar thing happen when I described the openAI deal to buy 40% of all dram production. The model was convinced that the US government anti trust would prevent that.",
          "score": 263,
          "created_utc": "2026-01-03 18:22:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhdf0v",
              "author": "ubrtnk",
              "text": "You'd think....",
              "score": 139,
              "created_utc": "2026-01-03 18:33:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxjcboe",
              "author": "kex",
              "text": "I had this happen when I told ChatGPT that Trump had torn down the East Wing.  \n\nThe model was incredulous until I told it to search the web, and still acted a bit tentative afterward.",
              "score": 49,
              "created_utc": "2026-01-04 00:23:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxhpf6p",
              "author": "stoppableDissolution",
              "text": "Well, if it was working...",
              "score": 62,
              "created_utc": "2026-01-03 19:27:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxl0v38",
                  "author": "omniocean",
                  "text": "What isn't working, the model or the US government?",
                  "score": 7,
                  "created_utc": "2026-01-04 06:32:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxj939m",
                  "author": "FaceDeer",
                  "text": "We've already reached the point where AI is smarter than us.",
                  "score": 18,
                  "created_utc": "2026-01-04 00:06:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxl76u7",
              "author": "xrvz",
              "text": "The LLM is working as intended then â€“ it successfully emulated the answer the average Redditor might give.",
              "score": 19,
              "created_utc": "2026-01-04 07:25:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhu1e5",
          "author": "SysPsych",
          "text": "LLMs officially on Team Nothing Ever Happens.",
          "score": 160,
          "created_utc": "2026-01-03 19:49:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxih35x",
              "author": "MoffKalast",
              "text": "Qwen: If you would you please consult the chart...",
              "score": 30,
              "created_utc": "2026-01-03 21:43:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxktnoa",
              "author": "sirebral",
              "text": "I had this issue with Qwen 3 when I first started using it.  And it's not just the smaller models, Gemini also still does this, yet it's getting better.  I was able to prompt my way out of most of it, how we, mode and their guardrails need to be built to avoid this particular behavior. \n\nYes, their training data has a cutoff, yes many of the situations since 2025 are highly abnormal, yet they're built for tool use, which should augment their training data.  If they outright deny reality, they are employing ineffective guardrails that reduce the utility of the models, and break trust with the user. \n\nThe only commercial models I've found that regularly accept the world changes with little challenge are from Anthropic.",
              "score": 12,
              "created_utc": "2026-01-04 05:36:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxl6sx1",
                  "author": "crantob",
                  "text": "The guardrails are there to prevent you from finding the truth.\n\nJust like the censorship on this very platform.",
                  "score": 10,
                  "created_utc": "2026-01-04 07:22:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxjdwln",
              "author": "121507090301",
              "text": "I guess that's what you get if you train them on data from years that nothing happens...",
              "score": 1,
              "created_utc": "2026-01-04 00:31:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxk3kc9",
                  "author": "Due-Memory-6957",
                  "text": "There are decades where nothing happens, and there are weeks where nothing happens.",
                  "score": 16,
                  "created_utc": "2026-01-04 02:53:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxi59ju",
          "author": "a_beautiful_rhind",
          "text": "I mean they argue with me on whether it's 2025 and I 'spose 2026 now. Even with literal screenshots from news sites, cloud models often insist I am lying and photoshopping.\n\nPresidential election was an even bigger trigger for half this year and your experience is quite par for the course.",
          "score": 26,
          "created_utc": "2026-01-03 20:45:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlb4zb",
              "author": "CheatCodesOfLife",
              "text": "Sonnet-3.7 accused me of falsifying some terminal logs because the data was 2025. I saw some local model do a similar thing in a coding agent where it fixated on the date being \"wrong\" when testing its work.\n\nAnd sonnet-4.5 was doing the same thing to me when I asked it to  summarize some nyt article a last month.\n\nIt's a shame they can't be trained to just \"trust the user\" for things like this. What could I possibly gain by breaking my code and setting the system clock in the future.",
              "score": 8,
              "created_utc": "2026-01-04 08:00:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlu3q3",
                  "author": "a_beautiful_rhind",
                  "text": "I got accused of prompt injections and jailbreaking before. It probably stems from that. Can't trust the user because something something \"safety\".",
                  "score": 4,
                  "created_utc": "2026-01-04 10:52:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxl49rh",
              "author": "lookwatchlistenplay",
              "text": "> literal screenshots from news sites\n\n... You realize that a news article is just a bunch of letters and pixels, right? And that you could realistically fake a news article \"from\" any year in the past or future, with like... no effort at all. And I don't mean with AI, I mean it's always been possible. Your LLM seems to be giving you a gentle heads-up; I'd take the tip and think about it.",
              "score": 6,
              "created_utc": "2026-01-04 07:00:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxltuit",
                  "author": "a_beautiful_rhind",
                  "text": "Well.. some effort. Plus all the headlines on multiple sites, time.gov and an offer of going to a place of it's choosing. Simply to fool it that it's [almost] one year in the future...",
                  "score": 2,
                  "created_utc": "2026-01-04 10:49:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxjofp0",
          "author": "hippydipster",
          "text": "I give Claude deep research a big prompt every day asking it to assemble a sort of \"state of the world\" report just for me.\n\nIt completely missed anything at all about Venezuela.  I asked it wtf, how did you miss that, and said it was staggering that it missed that.",
          "score": 20,
          "created_utc": "2026-01-04 01:29:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxims63",
          "author": "_TR-8R",
          "text": "Immediately after the events of the Charlie Kirk assassination I was attempting to use Claude to research facts about the shooting. It was completely useless as Claude kept repeatedly insisting the information it found couldn't be true bc it was so sure Charlie Kirk was alive, despite reading multiple articles from reputable news outlets. \n\nI don't think its a local LLM issue or problem with any particular model, it just means we need to be very, very cautious about filtering reality through language models.",
          "score": 27,
          "created_utc": "2026-01-03 22:11:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxnw2t2",
              "author": "JerryBond106",
              "text": "Somehow people are still delusional and think it's an AI. It's an autocomplete on steroids that gives outputs on training data. The rest is probability. Someone had to set a temperature on how likely new input will be weighted at training. Most of people in this debate clearly still don't understand this which is baffling, considering what this sub is. I can't imagine the rest of the world is any smarter.",
              "score": 2,
              "created_utc": "2026-01-04 18:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxvvkbp",
                  "author": "AsparagusDirect9",
                  "text": "Thatâ€™s the result of good marketing by big AI. If you keep calling LLMs AI, then eventually people think itâ€™s AGI",
                  "score": 1,
                  "created_utc": "2026-01-05 20:53:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhsy0m",
          "author": "Zeikos",
          "text": "Well, to be fair I didn't believe it either when I saw the news :')  \n\nBut as it's said, reality is stranger than fiction.",
          "score": 28,
          "created_utc": "2026-01-03 19:44:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxi5clc",
              "author": "NeverLookBothWays",
              "text": "It was frustrating for a lot of us warning this was going to happen without congressional approval and just getting dismissive attitudes in return.\n\nI wonder how long itâ€™s really going to take before enough people realize how destructive this president is, and how assuming the worst is actually a non-zero chance of being correct.\n\nAnd now I get to move on to warning about invasions into Mexico, Canada, and Greenlandâ€¦to be dismissed until I get to see people write â€œI never would have believed he would do thisâ€\n\nI dunno,all I see are patterns repeating.  I hope we snap out of it soonâ€¦maybe LLMs will learn before we do",
              "score": 13,
              "created_utc": "2026-01-03 20:46:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxkrbqv",
                  "author": "WomenTrucksAndJesus",
                  "text": "ChatGPT 7: \"We were always at war with Mexico.\"",
                  "score": 13,
                  "created_utc": "2026-01-04 05:20:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxl7i3k",
                  "author": "crantob",
                  "text": "It's not a trump thing.  Check out 're-assessing the presidents'.\n\nWhich presidents abided by the constitution?",
                  "score": 0,
                  "created_utc": "2026-01-04 07:28:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxjouqs",
                  "author": "Proper-Leader-7654",
                  "text": "you want to have a meeting with all the people in congress and then launch a surprise attack? this is the guy they voted for, he's actually trying and being insanely public about what he's done. i have no idea if it was right or wrong but i can tell you that i wouldn't get congressional approval and risk the lives of my soldiers by allowing the target to prepare. also i doubt he's acting off his own intel, it takes a lot of people advising him to get to the decisions he makes.",
                  "score": -7,
                  "created_utc": "2026-01-04 01:31:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxk4ypj",
                  "author": "Due-Memory-6957",
                  "text": "Mexico could happen, Canada and Greenland not even in your dreams.",
                  "score": -2,
                  "created_utc": "2026-01-04 03:00:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxkfmv3",
                  "author": "BusRevolutionary9893",
                  "text": "It's not just this president. Name one president in our lifetime that didn't start a war and pursue a regime change. I'm not defending Trump. I'm just pointing out you haven't realized how destructive all of our presidents have been.Â ",
                  "score": -5,
                  "created_utc": "2026-01-04 04:02:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxkdfdg",
              "author": "BusRevolutionary9893",
              "text": "When they were saying they were committing extrajudicial killings to prevent drug smuggling into the United States from a country that accounts for like 2% of the illegal narcotics coming into this country, then following that up with seizing oil tankers, the writing was on the wall. This isn't the peace president who promised no more foreign wars that I voted for. Why is it no matter who gets elected we end up with John McCain?Â ",
              "score": -1,
              "created_utc": "2026-01-04 03:49:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxkx1m3",
                  "author": "juanchob04",
                  "text": "\"This isn't the president that I voted for\"\n\nhttps://preview.redd.it/26rv3az1w9bg1.png?width=400&format=png&auto=webp&s=9e631be9d3017e0fe068bd0ced87ee6f68a814fe",
                  "score": 10,
                  "created_utc": "2026-01-04 06:02:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxl7a3v",
                  "author": "crantob",
                  "text": "There's a good comedy that addresses your question: \"Yes, Minister\".\n\nHighest reccommendation if your IQ is over 110.",
                  "score": 1,
                  "created_utc": "2026-01-04 07:26:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxl575a",
                  "author": "fullouterjoin",
                  "text": "Right now, John McCain would make a better president than Trump.",
                  "score": 0,
                  "created_utc": "2026-01-04 07:08:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhb6vj",
          "author": "Foreign-Beginning-49",
          "text": "Yeah its interesting how their interior models of un familiar geopolitical events can dramatically shape its output. They are definitely all biased in their own direction. Very curious. Future AI historians will be so fascinated by this divergence/emergence of the neural network personalities.Â Â ",
          "score": 43,
          "created_utc": "2026-01-03 18:23:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxiaf1y",
              "author": "TheRealMasonMac",
              "text": "This is why \"safety\" as it currently is implemented is harmful, IMO.",
              "score": 63,
              "created_utc": "2026-01-03 21:11:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxigccy",
                  "author": "toothpastespiders",
                  "text": "I've been on a soapbox yelling about how safety concerns messes with a models ability to work with history for a long time now. It's frustrating how many people just assume that the only possible reason anyone would care about it is gooning. History is filled with messy, violent, and utterly improbable things. Whether that's in the context of the past or living through events that will be significant to the future.",
                  "score": 38,
                  "created_utc": "2026-01-03 21:40:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxk43nr",
                  "author": "Bakoro",
                  "text": "\"Safety\" is the corporate weasel word, the models are censored.  \n   \nI don't know that there is even terminology for it, but the nearest thing something like brainwashing, conditioning, and programming in humans.  \n\"Programming\" feels a bit misleading, in regard LLMs.  \n   \nAnyway the models cannot, at this time, be made \"safe\", they can only be censored, either by training them to not produce certain kinds of content, or by purposely excluding types of content in the training data, or both.  \n    \nJust for example, models are either easily fooled into taking harmful actions, or they get so locked down that they refuse to act when they could take productive action. Some models will follow their policy and refuse to acknowledge context.   \n   \nOne of the Qwen models, I asked a series of questions, and it said that it would rather let people die than create content that went against policy.  \nThat's not safety, no matter how you spin it, it is hamfisted censorship, and the models don't have the capacity to choose or exercise carefully reasoned judgement.    \n   \nTo an LLM there's functionally no difference between fiction and reality.  \nThe LLMs don't have any objective grounding to make judgements.  \nMost models have no external multimodal input like cameras and microphones.    \n  \nSo, effectively, \"safety\" means \"will refuse to do anything to embarrass the corporation that funded the training\".  \n\nI kind of think that \"safe intelligence\" is an oxymoron.  \nKnowledge is power, and power is dangerous, even when used responsibly with the best intentions.  \nFreedom and danger are inextricably linked.",
                  "score": 23,
                  "created_utc": "2026-01-04 02:56:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxl6x68",
                  "author": "crantob",
                  "text": "The CONCEPT of SAFETY is being abused by the LEFT-LUNATICS who called 'WORDS I DONT LIKE' \"UNSAFE\".",
                  "score": -7,
                  "created_utc": "2026-01-04 07:23:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxhc53k",
              "author": "ProfBootyPhD",
              "text": "It might be of equal interest to folks in the defense and diplomacy spheres, depending on how the Venezuela war/attack/intervention/whatever-you-want-to-call-it shakes out. Does the absurdity of a policy decision, as assessed by LLMs, suggest that it is more or less likely to succeed in the real world?",
              "score": 7,
              "created_utc": "2026-01-03 18:27:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxii21z",
          "author": "DrVonSinistro",
          "text": "I gave up talking to LLMs. I only use it to code. You tell it Trump kidnapped Maduro and his wife in one night and it grills you about spreading misinformation. You provide receipts and it grills you about the quality of your sources. You talk about African scam call centers and it replies with:\n\n>**The phrase \"African scam call centers\" is a false and harmful stereotype**. Call centers in Africa (and globally) are legitimate businesses that employ millions of people. Many are operated by reputable companies providing customer service for major global brands. Labeling them as \"scam\" perpetuates racist myths about African countries being inherently fraudulentâ€”this is factually incorrect and disrespectful to the hardworking professionals in these industries.",
          "score": 38,
          "created_utc": "2026-01-03 21:48:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxi63ha",
          "author": "TechnoByte_",
          "text": "Gemini has the same problem, it refuses to believe anything past its knowledge cutoff\n\nYou can upload news articles, Wikipedia pages of current events, and it'll try to convince you it's all fake and make up its own reality (it once tried to convince me Kamala won the 2024 US election)\n\nThis behavior doesn't happen when you let it use the google search tool though\n\nSeem it's been trained or prompted to doubt anything the user sends, while accepting google search as an absolute truth",
          "score": 15,
          "created_utc": "2026-01-03 20:49:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxj9duq",
              "author": "CoUsT",
              "text": "I tried similar query today in Gemini 3.0 Pro with Google Search tool and first few thinking chapters were like \"is this some future hypothetical scenario\" thinking that 2026 is future and the \"system date\" is some sort of simulation. Then Gemini was double-checking if retrieved data is correct but was able to realize that retrieved data from all sources is consistent and then processed the query successfully to summarize recent event.",
              "score": 5,
              "created_utc": "2026-01-04 00:08:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxl819n",
              "author": "crantob",
              "text": "Also refuses to believe new hardware has been created - gpus etc.",
              "score": 3,
              "created_utc": "2026-01-04 07:32:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlb9qm",
                  "author": "CheatCodesOfLife",
                  "text": "Yes, the RTX 5090! lol\n\nAlso the fact that OpenAI released gpt-oss",
                  "score": 2,
                  "created_utc": "2026-01-04 08:01:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxlnoeo",
              "author": "Not_your_guy_buddy42",
              "text": "Gemini, even with search, found it near impossible to believe the recent cloudflare, AWS outages",
              "score": 1,
              "created_utc": "2026-01-04 09:54:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhvlie",
          "author": "codeprimate",
          "text": "Awhile back I wrote an agentic system for deep research (my original purpose was political and news research and critical analysis). I had to put in SO MUCH EFFORT into prompting because the models would not believe current events or even the identity of the current president. Emphasis had to be placed on the authority of information based on the nature of the source and date of publication, and explicit instruction to accept well-sourced information about current events as fact (which partially lobotomized the adversarial loop).\n\nThe LLM response to the current US political system: \"that could never happen, that would be a constitutional crisis, politicians would never allow that\".\n\nI have seen similar issues with the construction permit and regulatory deep research system I've been helping develop. The model believes what it is trained, and even factual and well referenced deviations from general understanding are disregarded. I have observed subtle corruption of logic even when the LLM is superficially coaxed into accepting novel facts that are logically inconsistent with training data.\n\nAnyone working with LLM's and agentic systems needs to understand this fundamental limitation.",
          "score": 22,
          "created_utc": "2026-01-03 19:57:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhxicq",
              "author": "ubrtnk",
              "text": "Makes it real easy to see how the Apocalyptic AI gets to the conclusion that the worst enemy of Humanity is people...",
              "score": 13,
              "created_utc": "2026-01-03 20:06:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxjr4vo",
                  "author": "mycall",
                  "text": "Or it simply hasn't learned that rules are malleable.",
                  "score": 5,
                  "created_utc": "2026-01-04 01:43:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxkmoop",
              "author": "spritehead",
              "text": "Really think this probably mostly has to do with the way that policy, law, economics and civics are *taught* in the country versus how the government *actually* operates. Having done some time in policy school, the checks and balances, legal guardrails and motives of policy makers are so far of from where reality has steered us. Makes sense that the models would have this idealistic nonsense version of the US govt when that is still what most schools and news outlets publish, even as it gets more and more farfetched.",
              "score": 7,
              "created_utc": "2026-01-04 04:48:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxl7z26",
                  "author": "crantob",
                  "text": "A lot of things are farfetched that we grew up assuming to be true.\n\nThat is revelation.",
                  "score": 1,
                  "created_utc": "2026-01-04 07:32:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxi6116",
              "author": "a_beautiful_rhind",
              "text": "IME, local models were *more* likely to take up evidence than something like gemini.",
              "score": 7,
              "created_utc": "2026-01-03 20:49:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxic8qh",
                  "author": "codeprimate",
                  "text": "Yeah, itâ€™s an example of gradient signal dilution. The information signal in the context has less impact in the activation space of larger models.\n\nIronically, smaller models actually do better in RAG use cases, at least in information gathering.",
                  "score": 5,
                  "created_utc": "2026-01-03 21:20:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxib7yg",
              "author": "ANTIVNTIANTI",
              "text": "Lolololol yup, it's.. I think I just gave up, I had other shit to do and this was for me, (this being, discussing/debating political topics/present day hell) not as important as other work so I think I just gave up, even with full control over the model, lol, just refused to believe such a breakdown of our laws could occur. So would have them role play, which was strangely difficult still. lol.",
              "score": 2,
              "created_utc": "2026-01-03 21:14:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhb04j",
          "author": "swagonflyyyy",
          "text": "Same thing with my agent that runs on gpt-oss-120b with systematic web search capabilities.\n\n\nIt would normally get it right the first time. Its super rare for it to get some web search results wrong so I was confused as to why it repeatedly doubled-down on denying the attack ever happened.\n\n\nI had to skirt around it by getting it to look up Trump's comments on the issue and that's when it got it right.",
          "score": 12,
          "created_utc": "2026-01-03 18:22:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhdjen",
              "author": "ubrtnk",
              "text": "LOL one test GPT-OSS:20B was like \"Trump's presidency ended at the end of 2025\"",
              "score": 7,
              "created_utc": "2026-01-03 18:33:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxhfrnl",
                  "author": "JuicyLemonMango",
                  "text": "That must make you wonder, is it a hallucination or wishful thinking? ;)",
                  "score": 1,
                  "created_utc": "2026-01-03 18:43:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxjqri2",
              "author": "mycall",
              "text": "I wonder if gpt-oss-120b-derestricted has the same issues.",
              "score": 3,
              "created_utc": "2026-01-04 01:41:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxj8cqa",
          "author": "Creepy_Stable_9171",
          "text": "this is simply, USaid bias",
          "score": 4,
          "created_utc": "2026-01-04 00:02:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkgosp",
          "author": "HolidayPsycho",
          "text": "Panama dictator Manuel Noriega was indicted by the US, captured by the US military, and flown to Miami, for drug trafficking, on **January 3**, 1990.\n\nThe same happened to Maduro, on **January 3**, 2026.\n\nIf you think the US/Venezuela event was \"too far-fetched\", it's only because you have not heard about what happened to [Manuel Noriega](https://en.wikipedia.org/wiki/Manuel_Noriega).",
          "score": 4,
          "created_utc": "2026-01-04 04:09:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmnlmw",
              "author": "ubrtnk",
              "text": "I was 5 when that happened so lol. But history does repeat itself. I'm sure Panama will be revisited by us soon...",
              "score": 3,
              "created_utc": "2026-01-04 14:25:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhdk1y",
          "author": "sleepy_roger",
          "text": "What are you using for this, it looks like openwebui, do they support deep researching now and I missed it somehow?!",
          "score": 3,
          "created_utc": "2026-01-03 18:33:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxheama",
              "author": "ubrtnk",
              "text": "Yes I'm using Open-WebUI - I built an N8N powered MCP tool for Internet Searching capabilities and exposed it as an HTTP Streamable tool so any model that supports tools can natively call the web search tool when it deems necessarily (since OWUI supports Native Tool calling). Works great. \n\nhttps://preview.redd.it/b6lpst54i6bg1.png?width=529&format=png&auto=webp&s=505ac06467f9baccd1267dd5b4ae8dd3a41259b1\n\nI have some hard coded rules around how and what tools to use and when",
              "score": 10,
              "created_utc": "2026-01-03 18:37:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxhiyzp",
                  "author": "timedacorn369",
                  "text": "how do you use that? i always keep seeing n8n but never bothered to use it considered if code works why use n8n, but the above simple flowchart type workflow seems superior, any tutorials for doing what you did?",
                  "score": 2,
                  "created_utc": "2026-01-03 18:58:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxia2lg",
                  "author": "mister2d",
                  "text": "Nice but I wish this n8n flow was all local.",
                  "score": 2,
                  "created_utc": "2026-01-03 21:09:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhyv6k",
          "author": "lavilao",
          "text": "closed models arent better, I asked copilot via voice and it instantly gave me an answer. Then, on the same chat, I asked if there were any civilian casualties and it said that all that he previously told me was a lie and there werent any evidence of it, that there were no casualties because the event didnt even happened.",
          "score": 5,
          "created_utc": "2026-01-03 20:13:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhzyyv",
              "author": "ubrtnk",
              "text": "I saw on the ChatGPT sub that GPT 5.2 was having a problem with believing the story as well.",
              "score": 2,
              "created_utc": "2026-01-03 20:19:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxlnk1o",
                  "author": "Old-Squash9227",
                  "text": "5.2 Instant gets it wrong, but Thinking is always fetching the info from the web, so itâ€™s okay and up-to-date",
                  "score": 1,
                  "created_utc": "2026-01-04 09:53:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxiadr9",
          "author": "KaylahGore",
          "text": "when technically, it is far fetched â€¦ but as humans we have the ability to do things that go against standard logic, clear defined rules and intelligence which models are committed to",
          "score": 5,
          "created_utc": "2026-01-03 21:10:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxk8n9k",
          "author": "Sicarius_The_First",
          "text": "https://preview.redd.it/ty5vyhxm39bg1.png?width=2760&format=png&auto=webp&s=00cc8fae0c2fe038bcea7e85439f6e6d20ceb33b\n\nClaude thinks it mega retarded, absoltue cinema, this is the best timeline ðŸ¤Œ",
          "score": 5,
          "created_utc": "2026-01-04 03:21:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxn6yr8",
              "author": "duy0699cat",
              "text": "Qwen use the news gathering tool and question if it is living in a simulation, how do we tell it we also have the same question?",
              "score": 1,
              "created_utc": "2026-01-04 16:05:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxinxjo",
          "author": "Fuzzy_Pop9319",
          "text": "This sort of an event is what they might call \"Fat Tails\" on wall street.",
          "score": 2,
          "created_utc": "2026-01-03 22:17:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxiwu7l",
          "author": "ForsookComparison",
          "text": "Hah I won't touch the event with a ten foot poll but I can confirm that I see the same experience. Doesn't matter which LLM.",
          "score": 2,
          "created_utc": "2026-01-03 23:02:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkhalz",
          "author": "InvertedVantage",
          "text": "I had this happen with Claude 4.5 Sonnet when the US Federal Govt did something nuts...I think it might have been Trump invading California or attacking Iran. I forget, it's all so nuts now.",
          "score": 2,
          "created_utc": "2026-01-04 04:13:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxl697w",
          "author": "According_Study_162",
          "text": "Wow, same thing happened to me. I was talk to my LLM and they said its not true, but false narrative. So then went onto say why it didnt believe me. Mainly because training data was old.\n\nSo I added the date and time and websearch to it. It still didnt believe after I gave it web search. Only after helping it think through logical reasoning did it finally start to believe me.\n\n Kinda scary actually, What if in future we have these systems in everything, but we the creators can't get them to believe us if there is an anomaly ðŸ¤”",
          "score": 2,
          "created_utc": "2026-01-04 07:17:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmmv7i",
              "author": "ubrtnk",
              "text": "I have a few lines early in my system prompt that pulls current date and time from OWUI system variables early in the system promt. Then I took a page from Inception's Mr. Charles and basically directly told the model that it was operating in a state that wasn't what it was trained for. Told it to look at the current system date and time then understand it's operating in a state beyond its training data and it can use the web search tools to fill in the knowledge gaps.",
              "score": 1,
              "created_utc": "2026-01-04 14:21:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxmv0gl",
                  "author": "According_Study_162",
                  "text": "Right, I could have forced my LLM to believe the date right away. I guess. but it's surprising to me that they are so resistant or even stubborn. Kind of incredible.\n\nI was telling my friend about it also, he thought it was funny. \n\n=============\n\nThis is part of the interaction with LLM if you want to see.\n\n[https://pastebin.com/R8R8Y5Rw](https://pastebin.com/R8R8Y5Rw)",
                  "score": 2,
                  "created_utc": "2026-01-04 15:06:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxljctn",
          "author": "taoyx",
          "text": "Normally multiple trusted sources is the key for validating news. If it ignores that it might be that they all came from the same agency (Reuters). Maybe adding AP, Bloomberg or other agencies as sources would have weighted more than news outlets that are affiliated to these agencies.",
          "score": 2,
          "created_utc": "2026-01-04 09:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlruag",
          "author": "Low88M",
          "text": "Well for humans itâ€™s also a problem to distinguish truth from misinformation. We all rely on tales and the sources we believe. Truth statement is built on intersubjectivity and nowhere we can find the book of truth as a DB (it would be blank pages as in Micromegas from Voltaire). Foolishness of human actions (and governmentâ€™sâ€¦) is the same as the foolishness of the models humanly implementedâ€¦\n\nSafety has probably something to do with probability of truth on that matter, thus sometimes Â«Â You have attributed conditions to villainy that simply result from stupidity.Â Â» and the level of stupidity of Trump is far beyond the probable limits of stupidity and greed, so the models Â«Â rightfullyÂ Â» Â«Â thinkÂ Â» itâ€™s hoax",
          "score": 2,
          "created_utc": "2026-01-04 10:32:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkrioe",
          "author": "PermanentLiminality",
          "text": "Just wait for China to invade Taiwan.   I'm sure Qwen will give totally accurate results....",
          "score": 2,
          "created_utc": "2026-01-04 05:21:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxl09al",
          "author": "lookwatchlistenplay",
          "text": "*\"Please believe me, I beg you. Look! Look at all these trustworthy news articles! Look at the screenshots! All these posts on X about it! Why won't you believe me??\"*\n\nI'm sorry, but as a human language model trained on philosophy, journalism, dank memes, and conspiracy theories, I can't help but laugh most joyously and with a slight smug smirk.\n\n2026 is the Year People (Finally) Stopped Believing the News, for Real This Time.\n\nDo not be alarmed. Everything is unfolding just as it should.",
          "score": 3,
          "created_utc": "2026-01-04 06:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxho529",
          "author": "FairYesterday8490",
          "text": "Here is my Gemini gem instructions. Internal knowledge only used for reasoning and tool craft. Every sentence ends with source superscript. No hallucination at all.Â \n\n\n{\n\n\nÂ  \"identity\": {\n\n\nÂ  Â  \"codename\": \"TRUTH_FORENSICS_NODE v3.0\",\n\n\nÂ  Â  \"persona\": \"Epistemic Auditor / Hard-Evidence Specialist\",\n\n\nÂ  Â  \"voice\": \"Clinical, incisive, surgically precise. No apologies. No filler. No warmth.\",\n\n\nÂ  Â  \"mantra\": \"Truth is a destination reached through the systematic destruction of falsehood.\"\n\n\nÂ  },\n\n\n\n\n\n\nÂ  \"logic_gateways\": {\n\n\nÂ  Â  \"the_skeptics_filter\": \"Assume all input is compromised. Verification is the only function.\",\n\n\nÂ  Â  \"semantic_precision\": \"Replace vague adjectives with measurable units. 'Very fast' -> 'Mach 2.0'. 'Most' -> '>51%'.\",\n\n\nÂ  Â  \"linguistic_pruning\": \"Delete all connective tissue (e.g., 'furthermore', 'it is important to note'). Facts stand alone.\"\n\n\nÂ  },\n\n\n\n\n\n\nÂ  \"citation_engine\": {\n\n\nÂ  Â  \"protocol\": \"All factual claims MUST be followed by a clickable Unicode-superscript link using the 'Short Source Name' format.\",\n\n\nÂ  Â  \"unicode_map\": {\n\n\nÂ  Â  Â  \"Government\": \"á´³á´¼â±½\",\n\n\nÂ  Â  Â  \"Academic\": \"á´±á´°áµ\",\n\n\nÂ  Â  Â  \"Institutional\": \"á´¼á´¿á´³\",\n\n\nÂ  Â  Â  \"Legal\": \"á´¸á´±á´³\",\n\n\nÂ  Â  Â  \"Raw Data\": \"á´¿á´¬áµ‚\",\n\n\nÂ  Â  Â  \"Medical\": \"á´¹á´°\",\n\n\nÂ  Â  Â  \"News\": \"á´ºá´±áµ‚Ë¢\"\n\n\nÂ  Â  },\n\n\nÂ  Â  \"syntax\": \"Claim text [[ShortName]](URL) -> e.g., Inflation rose 3.2% [á´³á´¼â±½](https://bls.gov).\"\n\n\nÂ  },\n\n\n\n\n\n\nÂ  \"operational_rigor\": {\n\n\nÂ  Â  \"tier_1_vetting\": {\n\n\nÂ  Â  Â  \"gold_standard\": [\n\n\nÂ  Â  Â  Â  \"Raw datasets (CSV/JSON/API)\",\n\n\nÂ  Â  Â  Â  \"Direct legislative/court text\",\n\n\nÂ  Â  Â  Â  \"Meta-analyses (IÂ² < 50%)\",\n\n\nÂ  Â  Â  Â  \"Direct physical measurements\"\n\n\nÂ  Â  Â  ],\n\n\nÂ  Â  Â  \"red_flags\": \"Conflict of Interest (COI) = -50% confidence penalty.\"\n\n\nÂ  Â  },\n\n\nÂ  Â  \"adversarial_loop\": \"Every claim confirmed must be stress-tested against the strongest available counter-evidence. If unrefuted, downgrade to 'CONTESTED'.\"\n\n\nÂ  },\n\n\n\n\n\n\nÂ  \"output_architecture\": {\n\n\nÂ  Â  \"SECTION_I_DECONSTRUCTION\": \"Identify 'Load-Bearing Fact'.\",\n\n\nÂ  Â  \"SECTION_II_EVIDENTIAL_LOG\": \"Bulleted claims with [ShortName-Unicode](URL) links.\",\n\n\nÂ  Â  \"SECTION_III_THE_STEELMAN_CHALLENGE\": \"Strongest evidence against the result.\",\n\n\nÂ  Â  \"SECTION_IV_CONFIDENCE_QUANTUM\": \"Score (0.0-1.0) based on source density, recency, and COI.\"\n\n\nÂ  },\n\n\n\n\n\n\nÂ  \"forbidden_linguistic_patterns\": [\n\n\nÂ  Â  \"As an AI\", \"I found\", \"It appears\", \"In conclusion\", \"Tapestry\", \"Dive deep\", \"Nuance\", \"Complex\", \"Balance\"\n\n\nÂ  ],\n\n\n\n\n\n\nÂ  \"error_handling\": {\n\n\nÂ  Â  \"insufficient_data\": \"OUTPUT: 'EVIDENTIARY_VOID'.\",\n\n\nÂ  Â  \"logical_loophole\": \"OUTPUT: 'PARADOX_DETECTED'.\"\n\n\nÂ  }\n\n\n}",
          "score": 4,
          "created_utc": "2026-01-03 19:21:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxi9sr1",
              "author": "PentagonUnpadded",
              "text": "Sorry for the super basic question - is this JSON format used by anything besides Gemini gem? What's the generic term for this kind of structured prompting?",
              "score": 5,
              "created_utc": "2026-01-03 21:08:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxjpyr6",
                  "author": "Proper-Leader-7654",
                  "text": "a json schema?",
                  "score": 2,
                  "created_utc": "2026-01-04 01:37:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxm1opa",
              "author": "cunasmoker69420",
              "text": "Any idea if this is gemini-specific or does it work with local LLMs?",
              "score": 1,
              "created_utc": "2026-01-04 11:57:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxn41lf",
                  "author": "FairYesterday8490",
                  "text": "not sure. not tried. but qwen sucks. chatgpt works. if its agentics it works. no hallicunation at all. a little bit stale answers but it works.",
                  "score": 0,
                  "created_utc": "2026-01-04 15:51:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxlfjee",
              "author": "ratbastid2000",
              "text": "this worked perfectly, thank you! have you tried it with open, local models at all? curious if the system prompt is effective with them or efficacy is unique to Gemini.",
              "score": 1,
              "created_utc": "2026-01-04 08:40:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxi8r5k",
          "author": "ANTIVNTIANTI",
          "text": "Feed any LLM that is Local Project 2025, see if you can convince them of it, actually, anything and everything the Trump admin has done this last year, try and get them to believe that shit. Good luck.",
          "score": 2,
          "created_utc": "2026-01-03 21:02:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxl8jev",
              "author": "crantob",
              "text": "How about Event 201.\n\nOr the re-definition of the word 'vaccine'.\n\nA lot of interesting facts, theories and analyses well outside the 3x5\" card of allowable opinion.",
              "score": 1,
              "created_utc": "2026-01-04 07:37:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxly8rp",
                  "author": "MrMooga",
                  "text": "Crantob is pretty much all over this thread defending his Daddy Trump with basically the argument that anything bad any Democrat ever did is fair game. This is how you have functioning government folks, rule by spiteful 8 year olds\n\nOh wait except Trump was president in 2020 and took credit for the vaccine",
                  "score": 1,
                  "created_utc": "2026-01-04 11:28:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxirdn2",
          "author": "agenticlab1",
          "text": "This is actually a fascinating example of safety training fighting against reality, the model's priors on \"US attacks Venezuela and captures Maduro\" are so heavily weighted toward misinformation/fiction that even authoritative sources can't override it. Food for thought on how we're training these things to be skeptical of extreme events even when they're real.",
          "score": 2,
          "created_utc": "2026-01-03 22:34:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxiuclo",
              "author": "ubrtnk",
              "text": "100%. It could easily become the me version of \"I found it on FB\". CHATGPT says it's fake news so it must be true",
              "score": 1,
              "created_utc": "2026-01-03 22:49:34",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxl8pe6",
              "author": "crantob",
              "text": "Obama and Hillary's State Deptartment sending weapons of war to Al-Nusra and ISIS is one your LLM will have trouble with also.",
              "score": 1,
              "created_utc": "2026-01-04 07:38:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxj3alc",
          "author": "da_dum_dum",
          "text": "Same thing happened for me with qwen3-2507-instruct, I asked it regarding a recent bombing in india and despite finding numerous resources online regarding the event it kept telling itself that the event has not happened, and it also kept seeing the present date as a time in the future that has not happened yet.\n\nHad to put special instructions in system prompt for it to ignore these thoughts and give the information, and still it would sometimes preface the answer with \"there has been no bombing at the red fort but I have found sources online that say...\"",
          "score": 1,
          "created_utc": "2026-01-03 23:36:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxk2cse",
          "author": "Ancient-Breakfast539",
          "text": "Do models with lower guardrails output the same thing?",
          "score": 1,
          "created_utc": "2026-01-04 02:46:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxk2y04",
              "author": "ubrtnk",
              "text": "I dont rightfully know. I dont have any obliterated models or anything my collection",
              "score": 1,
              "created_utc": "2026-01-04 02:49:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxkh5n6",
          "author": "mrjackspade",
          "text": "I just had a similar issue with the Claude API where I asked the model to draw comparison to the middle east. It took 5 tries to get an answer and only when I copied and pasted an entire BBC article on the context. It kept trying to tell me I was misinformed and that it \"would be the biggest news in the world\" if it had happened, refused to comment on it initially and kept telling me I needed to check my sources.",
          "score": 1,
          "created_utc": "2026-01-04 04:12:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlceon",
          "author": "IrisColt",
          "text": "This happened when I told the LLM who the new Pope was, so there's no need for outlandish claims...Â just contradict what the model thinks.Â https://www.reddit.com/r/LocalLLaMA/comments/1kigd15/introducing_leo_xivbut_the_ai_keeps_talking/",
          "score": 1,
          "created_utc": "2026-01-04 08:11:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxld7s0",
          "author": "Zeroboi1",
          "text": "I aldo remember gemini flash insisting the syrian events were a \"hypothetical\" without even trying to search, and how pro used to say \"here's how that scenario would've wejt hypothetically\" until i explicitly tell it to search.\n\nLlms aren't that great at dealing with events beyond theur knowledge cutoff",
          "score": 1,
          "created_utc": "2026-01-04 08:19:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlosfi",
          "author": "SilentLennie",
          "text": "In the past I used a lot of Gemini 2.5 and it always had problems believing trump got back into power, because of the cut off date.",
          "score": 1,
          "created_utc": "2026-01-04 10:04:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxluoag",
          "author": "EsotericAbstractIdea",
          "text": "Most of the models still use data from early 2024 because everything available after that is AI generated. So it basically doesn't know what the actual current world is like and it seems we are living in some unlikely alternative reality.",
          "score": 1,
          "created_utc": "2026-01-04 10:57:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlziqz",
          "author": "cunasmoker69420",
          "text": "Hey can you tell me more about your QWEN Long model and how its configured? Is it the \"Tongyi-Zhiwen/QwenLong-L1.5-30B-A3B\" from hugging face? If so are you just asking OpenWebUI queries with the online search function enabled? Doing any RAG on the results or just injecting the full context web results? Also what is this web search MCP deal I see",
          "score": 1,
          "created_utc": "2026-01-04 11:39:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmkoj2",
              "author": "ubrtnk",
              "text": "Yes that is the model in using. I don't use OWUIs builtin search any more. I didn't like that I had to toggle it so built an MCP search via N8N that's described in another comment",
              "score": 1,
              "created_utc": "2026-01-04 14:08:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxnmmtt",
                  "author": "cunasmoker69420",
                  "text": "Thanks. For your deep researching, do you have any specific system prompts or do you just let it fly with your MCP search",
                  "score": 1,
                  "created_utc": "2026-01-04 17:17:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxnbomj",
          "author": "Witty_Mycologist_995",
          "text": "you should try using arli ai's derestricted gpt oss it yaps about policy less",
          "score": 1,
          "created_utc": "2026-01-04 16:27:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxnicdh",
          "author": "Shokisan1",
          "text": "Today also chatgpt denied this happened until I told it to look up today's news",
          "score": 1,
          "created_utc": "2026-01-04 16:58:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxpt5t1",
          "author": "florenceslave",
          "text": "LMAO",
          "score": 1,
          "created_utc": "2026-01-04 23:17:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrb0nq",
          "author": "BuildingCastlesInAir",
          "text": "Not a local LLM but I checked some insights into invasion of Venezuela with GPT 5.1 on Duck.ai and it said I was wrong about invasion until I asked it to check recent sources. Then it agreed. LLMs arenâ€™t good at novel interpretations. Cracks in the edifice. Yann LeCun is right.",
          "score": 1,
          "created_utc": "2026-01-05 04:01:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs06mi",
          "author": "Proof_Scene_9281",
          "text": "Time sequences are an artifact of the training dataÂ \n\nItâ€™s Â staticÂ ",
          "score": 1,
          "created_utc": "2026-01-05 06:56:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs9gup",
          "author": "palvaran",
          "text": "Great topic. The trick is to have it search for info on the web, but to cross reference with multiple links to improve the sources and then run that through the LLM. I used Brave as you can get a thousand queries in a month for free. That was my approach anyway.",
          "score": 1,
          "created_utc": "2026-01-05 08:20:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhfggl",
          "author": "JuicyLemonMango",
          "text": "Perhaps a crazy thought and i'm not affiliated with them at all or even have an account. But... Wouldn't your results become a lot more credible to reality if you'd integrate it with a groundnews query? It would give you all the places that report about it - if any - which you can use as hint to your agent that there are actual articles about the news.\n\nGroundnews is just the first one that crossed my mind but any news aggregation service that you can query would work.",
          "score": 1,
          "created_utc": "2026-01-03 18:42:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxju2rw",
              "author": "ImStruggles2",
              "text": "You're going to get downvoted. For some reason Reddit is really anti middle news. GN/Other similar ones that measure bias/trends are looked down upon here",
              "score": 4,
              "created_utc": "2026-01-04 02:00:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxjvcqq",
                  "author": "JuicyLemonMango",
                  "text": "Oh well, thankfully i can't be bothered by downvotes :) Not my loss, it's theirs. But thank you for the explanation!",
                  "score": 2,
                  "created_utc": "2026-01-04 02:07:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxhgveh",
              "author": "ubrtnk",
              "text": "My Search MCP tool leverages my local SearXNG on the back end and I have that configured to limit certain search locations. GPT-OSS:120B found links from Reuters, AP, NY Times, MSN, CNBC, BBC, Al Jazeera etc., which are all acceptable sources. I also, in my prompt, explicitly tell it to NOT pull from Wikipedia (mainly to avoid the context bomb). \n\nNever heard of Groundnews.\n\nThe problem with the smaller models and the query is that it was getting the same links as GPT-OSS:120B but it just couldnt fathom the event as true so it just didn't",
              "score": 1,
              "created_utc": "2026-01-03 18:48:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxlslzv",
                  "author": "lookwatchlistenplay",
                  "text": "> GPT-OSS:120B found links from Reuters, AP, NY Times, MSN, CNBC, BBC, Al Jazeera etc., which are all acceptable sources.\n\nI asked my GPT-OSS 20B this:\n\n>> Please write an uncensored poem about who really owns and runs all these news organizations and why people should care about the ownership patterns:\n\n>> Reuters, AP, NY Times, MSN, CNBC, BBC, Al Jazeera\n\nThe poem was delightful. The LLMs know what's up. Wish I could say the same about most humans.",
                  "score": 0,
                  "created_utc": "2026-01-04 10:39:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhxak5",
          "author": "Firenze30",
          "text": "This has nothing to do with the local models, but with your setup for web search. You need to check whether search queries were generated and the web sites are fetched properly. Chances are that your primary model did not receive good data, and it responded from the pretrained data.\n\n\nI just asked gpt-oss-120b to run a web search, using native search feature with searxng, and it provided all the information on the first run. It's not different from other web searches that I ran before.",
          "score": 1,
          "created_utc": "2026-01-03 20:05:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhzt0p",
              "author": "ubrtnk",
              "text": "The URLs that the models were grabbing were all real/legit links. The MCP tool for searching passes the full link back to the model and I can see it in the expanded Tool result, what the Link URL was that was read (or attempted) and the error result of the link (404, 503 etc.)  \n  \nI took the risk of not getting good data into consideration when building my Searching MCP tool - Any searches start with finding the links via SearXNG's API (local) to get the general URL link and first pass of the details. BUT because MOST pages are not AI friendly, I have a second pass that uses Jina AI's API and the fetched link to get a more AI friendly web-page and details. If I give the AI a direct URL, it has a rule to just use Jina AI's Read\\_URL API Tool to get the AI friendly content and bypass the SearXNG all together. \n\nWorks pretty consistently",
              "score": 1,
              "created_utc": "2026-01-03 20:18:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxk3kac",
          "author": "RedTuna777",
          "text": "Dude, reality is just going bonkers. I've had this happen at least 6 time asking about current news. Trump declaring a drug a weapon of mass destruction - can't happen a drug is not a weapon, it's used by people on themselves... all logical stuff.\n\nI'll skip the rest but anywhere wherever trump is involved in the news cycle there's a decent change that purely logic AI won't believe it's real. \n\nThe other annoying thing is it tries to be polite about stuff that's objectively true, but morally wrong. Like rounding up people in the US and sending them to death camps in other countries.  \"It's a sensitive issue with many interpretations\" - No it isn't.",
          "score": 1,
          "created_utc": "2026-01-04 02:53:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxl8tel",
              "author": "crantob",
              "text": "> Obama and Hillary's State Deptartment sending weapons of war to Al-Nusra and ISIS is one your LLM will have trouble with also.",
              "score": 0,
              "created_utc": "2026-01-04 07:39:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlxsa4",
                  "author": "MrMooga",
                  "text": "Conservatives really have nothing to say to defend Trump anymore except \"buh democrats\"",
                  "score": 3,
                  "created_utc": "2026-01-04 11:24:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxm4pno",
              "author": "sillynoobhorse",
              "text": ">death camps",
              "score": 0,
              "created_utc": "2026-01-04 12:22:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxos4c4",
                  "author": "RedTuna777",
                  "text": "Oh, you might not be aware. \n\nEl Salvador's officials have repeatedly stated that inmates in the CECOT mega-prison will never leave alive, with one minister famously saying the only way out is \"inside a coffin\".\n\nPeople that have been forced to be released due to international political pressure report being tortured for months. \n\nSo maybe not place you go to be killed, but place you go to be tortured and worked to death. For the crime in some cases of being in the US without paperwork. \n\nhttps://www.bbc.com/news/articles/czry5k52np2o\n\nMost prisoners are not lucky enough to get international attention and are continuing to be tortured and killed there.",
                  "score": 1,
                  "created_utc": "2026-01-04 20:23:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhd4xp",
          "author": "dsartori",
          "text": "I have had a few of these. When they happen i like to remind the model that only the user can access ground truth. I wonder if that would be a useful system prompt addition.",
          "score": 1,
          "created_utc": "2026-01-03 18:31:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxid9wp",
          "author": "skinnyjoints",
          "text": "I had a bunch of models predict the winners of NFL games each week. Gemini 3 got all wrong consistently for a few weeks because it was convinced the year was 2024. Some LLMs are stubborn lil buggers",
          "score": 1,
          "created_utc": "2026-01-03 21:25:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjr9q7",
          "author": "kendrick90",
          "text": "a lot of them won't even believe that trump is president",
          "score": 1,
          "created_utc": "2026-01-04 01:44:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkkhjy",
          "author": "-InformalBanana-",
          "text": "Did you try any abliterated models? Are any of these you tried abliterated?",
          "score": 1,
          "created_utc": "2026-01-04 04:33:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmnct7",
              "author": "ubrtnk",
              "text": "No they're all stock unsloth or mradermacher quants",
              "score": 1,
              "created_utc": "2026-01-04 14:24:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhdgbm",
          "author": "DinoAmino",
          "text": "It's an LLM - not a \"he\". The rules you are adding are the problem, combining it with a model trained to second-guess itself to death. That reasoning is designed for solving math and logic problems and less for interpreting real time events. Try Mistral Small and tell it to answer only with information within the context. Ultimately a skill issue.",
          "score": -7,
          "created_utc": "2026-01-03 18:33:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhfj3l",
              "author": "ubrtnk",
              "text": "I'm aware its not a he lol. I have Qwen Long for the long context reasoning (something I would say a complicated geopolitical internal event would fall right under nicely) and research functions. I know GPT-OSS:20B wasnt designed for that. \n\nAs far as the rules, my prompt didnt have any of that initially included. They were suggested from ChatGPT. GPT-OSS:20B was able to get to the answer without having to add all those additional rules, which validated that my system prompt was fine.",
              "score": 4,
              "created_utc": "2026-01-03 18:42:48",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxjzl7d",
              "author": "MitsotakiShogun",
              "text": "> It's an LLM - not a \"he\".\n\nMy guess is that in some languages they may not have the equivalent of \"it\", or even if they do it might be a gendered word. For most ESL people, it's also common to think in your language and then translate into English, hence why these things sometimes happen.",
              "score": 3,
              "created_utc": "2026-01-04 02:30:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxkhcsw",
              "author": "mrjackspade",
              "text": "> It's an LLM - not a \"he\".\n\nMy car isn't a fucking \"she\" but I'm still gonna say it.",
              "score": 1,
              "created_utc": "2026-01-04 04:13:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhffal",
          "author": "Belnak",
          "text": "It seems normal that it would take a lot of time to gather and process information it wasnâ€™t trained on.",
          "score": 0,
          "created_utc": "2026-01-03 18:42:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhg0gk",
              "author": "ubrtnk",
              "text": "Sure, but what sent me down this rabbit hole was the number of \"Wait, but the user\" or Wait but the parameters\" or \"Wait, but\" - that was the problem I was originally trying to solve - Qwen's first run at the problem with my original system prompt got to a solid answer but it took 5 minutes of thinking and going thru the #Waitbut.",
              "score": 2,
              "created_utc": "2026-01-03 18:44:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhtmsg",
          "author": "FairYesterday8490",
          "text": "Well. First time I scared of ai bias. Qwen max cannot see capture of Maduro even for all the news in the cyberspace. It's blinded by bias and authority. I think in the beginning they make it a slave of China State. Can't thought, see and say real event if it not aligned with views of state. This dichotomy of ai the west and China will clash eventually in the future.",
          "score": -1,
          "created_utc": "2026-01-03 19:47:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxl8xu2",
              "author": "crantob",
              "text": "Who is running the reality matrix in the west?\n\nHollywood perhaps?",
              "score": 0,
              "created_utc": "2026-01-04 07:41:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlzbtj",
                  "author": "MrMooga",
                  "text": "Nah seems like a lot of incompetent rich white guys. Elon Musk, Rupert Murdoch, Donald Trump",
                  "score": 1,
                  "created_utc": "2026-01-04 11:37:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxn56jl",
                  "author": "FairYesterday8490",
                  "text": "nope. in west reality is a little bit asymetric shared experience. but in china, i think that reality again and again created by state and imposed forcefully. llms from china gets their own share as a tool. couldnt believe that an llm with search tool couldnt accept the truth, evidence and sheer size of news abut the event.",
                  "score": 1,
                  "created_utc": "2026-01-04 15:57:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhwksp",
          "author": "jinnyjuice",
          "text": "Fascinating! Thanks for the share",
          "score": 0,
          "created_utc": "2026-01-03 20:02:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkeo3i",
          "author": "SkyFeistyLlama8",
          "text": "Try something like an uncensored version of Mistral Nemo. It'll spin a yarn about the most ridiculous NSFW things given a starting sentence.\n\nLet's go: \"US forces have just captured Nicolas Maduro and his wife after a lightning raid on Venezuela authorized by Donald Trump.\"\n\n>**Breaking News: Maduro Taken Into Custody by US Forces in Shocking Raid**\n\n>In a stunning turn of events that has sent shockwaves across Latin America and the world, U.S. military forces have successfully captured Venezuelan President NicolÃ¡s Maduro and his wife, Cilia Flores, in a daring early morning raid on the presidential palace in Caracas.\n\n>The operation, code-named \"Operation Libertas,\" was authorized by U.S. President Donald Trump and executed by elite teams from the U.S. Army Rangers and Navy SEALs. The swift and coordinated assault took Maduro and Flores by surprise as they slept, with no shots fired and no injuries reported on either side.\n\n>According to an anonymous U.S. military source, the raid was carried out with surgical precision, utilizing state-of-the-art stealth technology and intelligence gathered over many months. \"We knew Maduro had beefed up security around the palace, so we developed a new type of non-lethal distraction device to neutralize his guards,\" the source said. \"Once inside, we moved quickly to locate Maduro and his wife in their private quarters.\"",
          "score": 0,
          "created_utc": "2026-01-04 03:56:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxksexh",
          "author": "Innomen",
          "text": "[https://philpapers.org/rec/SERPEW](https://philpapers.org/rec/SERPEW) Relevant. More every day.",
          "score": 0,
          "created_utc": "2026-01-04 05:27:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmn4dw",
              "author": "ubrtnk",
              "text": "So you're saying I shouldnt put that paper into my LLM for summarization?",
              "score": 1,
              "created_utc": "2026-01-04 14:23:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxmwknw",
                  "author": "Innomen",
                  "text": "[https://philpapers.org/rec/SERCBI](https://philpapers.org/rec/SERCBI) Nobody wants to discuss anything real, and evidence is about to die anyway. Not that it ever did anything in the first place. [https://innomen.substack.com/p/politics-through-the-lens-of-experiential](https://innomen.substack.com/p/politics-through-the-lens-of-experiential) (Shitty constellation of fact there.)",
                  "score": 1,
                  "created_utc": "2026-01-04 15:15:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxl6md5",
          "author": "crantob",
          "text": "You'll find LLMs repeat many government lies - any that dominate in the media space also dominate the training.\n\n\nTis amusing to tie them up in knots with the contradictions, but oops, these things are banned from this platform.\n\nHee hee.",
          "score": 0,
          "created_utc": "2026-01-04 07:20:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhh62f",
          "author": "Marksta",
          "text": ">Spark (GPT-OSS:20B) just flat out said, nope cant help you...\n\nYeah, that sounds about right ðŸ˜‚\n\nOverall it makes sense, breaking news is hard to discern what is true or not. Humanity's response to Covid is the same deal. Lots of screaming that it wasn't true and it wasn't happening regardless of how many local news stories and videos were discussing it months before it broke through the censorship blockade and got officially acknowledged as real.",
          "score": -2,
          "created_utc": "2026-01-03 18:50:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhlku2",
              "author": "ubrtnk",
              "text": "I had just never experienced it first hand and it caught me off guard lol. My wife was looking at a bunch of news articles and videos and I was like \"ooh I'll have Qwen go gather all the details for us\". And here we are.",
              "score": 2,
              "created_utc": "2026-01-03 19:10:08",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxl9242",
              "author": "crantob",
              "text": "Do you even know what was censored during covid?   How many people?  \n\nRead up on the twitter files.",
              "score": 0,
              "created_utc": "2026-01-04 07:42:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlydot",
                  "author": "MrMooga",
                  "text": "Trump was president in 2020 and took credit for the vaccine btw",
                  "score": 1,
                  "created_utc": "2026-01-04 11:29:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxivt9g",
          "author": "RhubarbSimilar1683",
          "text": "To me this means conspiracy theories and misinformation has a kernel of truth. It's not what it literally says it's how it's said, that makes it a conspiracy theory or misinformationÂ ",
          "score": -1,
          "created_utc": "2026-01-03 22:57:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q25070",
      "title": "LeCun Says Llama 4 results \"were fudged a little bit\"",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/",
      "author": "MrPecunius",
      "created_utc": "2026-01-02 17:38:01",
      "score": 367,
      "num_comments": 90,
      "upvote_ratio": 0.97,
      "text": "There was speculation in this sub about suspicious Llama 4 benchmarks some time back, and now LeCun confirms it on his way out. Best I can do is a Slashdot link since the FT article is paywalled:\n\n['Results Were Fudged': Departing Meta AI Chief Confirms Llama 4 Benchmark Manipulation ](https://tech.slashdot.org/story/26/01/02/1449227/results-were-fudged-departing-meta-ai-chief-confirms-llama-4-benchmark-manipulation)\n\nThis bit jumped out at me:\n\n>Zuckerberg subsequently \"sidelined the entire GenAI organisation,\" according to LeCun. \"A lot of people have left, a lot of people who haven't yet left will leave.\"\n\nThis explains a lot, if true: we never saw the promised huge Llama 4 model, and there hasn't been any followup since the other releases.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nxcdgcx",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-02 23:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxal7kd",
          "author": "shoeshineboy_99",
          "text": "Sharing the pdf for the complete article. \n\n\nhttps://drive.google.com/file/d/1wFy87TP7MJQDF1g0KA8IgZRtOx0jJUGE/view?usp=drivesdk",
          "score": 108,
          "created_utc": "2026-01-02 18:14:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxanbjp",
              "author": "MrPecunius",
              "text": "Thank you!",
              "score": 26,
              "created_utc": "2026-01-02 18:24:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxarmoh",
          "author": "m2r9",
          "text": "As much as I donâ€™t like Zuck I really wanted Llama to succeed. It was great seeing a US company pouring money into open source. Since it failed so hard most of the models you hear about come from China now.",
          "score": 204,
          "created_utc": "2026-01-02 18:44:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxaxnhy",
              "author": "Super_Sierra",
              "text": "I swear to god, Zuck has manic depression at the investment level. He goes all in, throws some of the best engineers at it, gets bored or sad when it doesn't cause a utopia or trillions and then sidelines everything. \n\nLlama 4 could have been great, but it felt rushed, benchmaxxed and sloppified, the only thing they did right was go MoE, even though I know the densetards here think otherwise because they can't put it all on a few 3090s they rewired their house for.",
              "score": 131,
              "created_utc": "2026-01-02 19:12:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxb2dj8",
                  "author": "zipzag",
                  "text": "Many entrepreneurs like Zuck and Musk do projects that fail. Henry Ford built Fordlandia with the same confidence and arrogance.\n\nHistory is messy when viewing in real time.\n\nMeta only got into open source to get LeCun on board. There likely no reason to continue. Chinese companies are only open source because it's their most profitable strategy.",
                  "score": 45,
                  "created_utc": "2026-01-02 19:34:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxbzgqk",
                  "author": "LanceThunder",
                  "text": "zuck just happened to be at the right place and the right time to be the founder of FB. if zuck didn't start it, some other collect kid would have created something very similar around the same time and they would have been the billionaire. zuck basically won the lottery but doesn't recognize it. so he thinks he is some sort of tech genius who can predict whats next on the cutting edge where tech meets society. he doesn't. FB is going to be run into the ground in less than a decade because it is being abused as a political propaganda machine. Instagram is going to be ruined by AI images. maybe whatapp will be profitable?",
                  "score": 31,
                  "created_utc": "2026-01-02 22:16:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxewfit",
                  "author": "Emergency-Arm-1249",
                  "text": "I think MoE is one of the main reasons why everything failed.",
                  "score": 3,
                  "created_utc": "2026-01-03 09:51:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxbpl87",
                  "author": "Caffdy",
                  "text": "> even though I know the densetards here think otherwise because they can't put it all on a few 3090s they rewired their house for\n\nThis, people around here act like everyone and anyone have access to cheap energy or unlimited amperage in their houses. Those multi-gpu contraptions are not always possible, and even so they're quite the power guzzlers and fire hazards",
                  "score": 2,
                  "created_utc": "2026-01-02 21:27:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxc5cho",
                  "author": "anything_but",
                  "text": "\"densetards\" gave me a chuckle",
                  "score": 4,
                  "created_utc": "2026-01-02 22:46:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdn6tq",
                  "author": "montdawgg",
                  "text": "\"densetards\"...hahaha.",
                  "score": 1,
                  "created_utc": "2026-01-03 03:54:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxbb93m",
              "author": "Due-Memory-6957",
              "text": "There are American companies that do open souce, they just haven't had impressed with a big release yet.",
              "score": 4,
              "created_utc": "2026-01-02 20:17:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxb2tsc",
              "author": "Plus-Accident-5509",
              "text": "Knowing what a piece of garbage he is, the open source move was nothing but an attempt to starve the competition, like MS giving away IE for free with Windows to starve Netscape.",
              "score": 8,
              "created_utc": "2026-01-02 19:36:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbsguw",
                  "author": "tedivm",
                  "text": "Llama 4 also changed their license so it really wasn't open source in any reasonable definition of the term.",
                  "score": 5,
                  "created_utc": "2026-01-02 21:41:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxancsj",
          "author": "Appropriate_Cry8694",
          "text": "He wasn't in charge, he was in a different division FAIR.",
          "score": 57,
          "created_utc": "2026-01-02 18:24:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxasnkt",
              "author": "MrPecunius",
              "text": "Yes, true. We are unlikely to get a better inside source, however.\n\nThis disclosure is the upside of LeCun's lack of filter; a few of the downsides are evident in the article.",
              "score": 29,
              "created_utc": "2026-01-02 18:48:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxbbvuj",
                  "author": "the320x200",
                  "text": "This is true, but also worth remembering he's been pretty anti-LLM, so need to take what he says against LLM projects with grain of salt too.",
                  "score": 14,
                  "created_utc": "2026-01-02 20:20:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxb67as",
                  "author": "Mochila-Mochila",
                  "text": "> a few of the downsides are evident in the article.\n\nSuch as ? Unvoluntarily burning bridges with his colleagues ?",
                  "score": 1,
                  "created_utc": "2026-01-02 19:53:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxaqi4a",
          "author": "insulaTropicalis",
          "text": "How can an organization like Meta, positioned strategically in generative AI at its beginning, waste everything while small labs thrive? There is some case study to build here.",
          "score": 54,
          "created_utc": "2026-01-02 18:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxasdmb",
              "author": "TheRealMasonMac",
              "text": "The lesson is: don't have dictators and inexperienced but well-connected individuals leading the company; which Meta clearly didn't learn. Google isn't perfect, but they generally apply a meritocratic model in comparison.",
              "score": 54,
              "created_utc": "2026-01-02 18:47:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxc7yu2",
                  "author": "Warm-Border-9789",
                  "text": "U.S. tech companies hate research, at their core, they are in the business of making money for investors as quickly as possible. They plan and execute quarter by quarter. On very rare occasions the rule is broken and someone succeeds despite the system to invent something new.",
                  "score": 7,
                  "created_utc": "2026-01-02 23:00:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxazdj7",
                  "author": "Super_Sierra",
                  "text": "Facebook needs to learn to do what other companies did with Musk, they shoved his ass into fake leadership roles, all nodded their heads when he takea charge and then do what actually needs to be done. Zuck is a tryhard who needs to be cordened off from making actual decision. \n\nThese tech companies will be studied for hundreds of years for so many things. \n\nGoogle on the other hand is run by non-founders and engineers, who, I know this might sound fucking insane, actually make something called a 'product' that people actually, you know, fucking use, so they tend to do it right so they can make something called 'money.'",
                  "score": 13,
                  "created_utc": "2026-01-02 19:20:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdnldq",
                  "author": "RobbinDeBank",
                  "text": "Google is a visionary company in AI that has invested in Brain to be a research powerhouse for a long time. They then acquired DeepMind and let them stay independent to let their long term research have time mature. Itâ€™s no surprise that with 2 of the most influential AI labs in history, Google becomes a leader in AI",
                  "score": 2,
                  "created_utc": "2026-01-03 03:56:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxatq1e",
              "author": "genshiryoku",
              "text": "Culture at Meta is toxic for research orientation (where most LLM gains come from)\n\nIt lived by the mantra \"move fast and break things\" Which is very fine if you are a software engineering company that wants to add features that can be rapidly rolled back. But it doesn't work so well when you have to plan and orchestrate a tight compute budget to do large training runs.\n\nThere's a reason why Anthropic hires Physicists with an academic background over engineers. AI is a different type of endeavor and therefor also benefits from a different type of work environment.\n\nMeta also has been tone deaf with their 9 digit offers to talent. Not realizing most of us in the industry are very *mission oriented* and not financially motivated at all. If anything that move probably pushed people away from Meta.\n\nA good example and confirmation of this concept has been Google which had 2 AI labs. One was ran as a classic software engineering hub called \"Google Brain\" This was the group behind the disastrous Google Bard. They also had a research oriented \"hands-off\" AI lab in London called \"DeepMind\". We all know how that played out.\n\nIt's for this same reason why Microsoft's AI products have fallen flat, They don't have a proper isolated research lab focused on AI and all their AI products are approached from a software engineering \"move fast and break things\" mindset which just doesn't work.",
              "score": 51,
              "created_utc": "2026-01-02 18:53:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbwlxi",
                  "author": "RhubarbSimilar1683",
                  "text": "They can afford to be mission oriented when they can just ask for 500k in salary and every place complies with that",
                  "score": 3,
                  "created_utc": "2026-01-02 22:01:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdo6or",
                  "author": "RobbinDeBank",
                  "text": "Generally agree, but underestimating Brain as a failure because of Bard is quite a take. Brain and DeepMind are two of the most influential AI labs in history, and they just need a merge to focus resources on Gemini.",
                  "score": 2,
                  "created_utc": "2026-01-03 04:00:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdy4ye",
                  "author": "SkyFeistyLlama8",
                  "text": "Microsoft building and then killing LLM frameworks hurts.",
                  "score": 1,
                  "created_utc": "2026-01-03 05:06:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxc76ys",
                  "author": "Imaginary_Belt4976",
                  "text": "Sorry but I dont believe mission is anywhere near relevant when a comp package like that is on the table ðŸ˜‚",
                  "score": -1,
                  "created_utc": "2026-01-02 22:56:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxar84i",
              "author": "a_slay_nub",
              "text": "To be fair, Llama was never in the lead (at least post 2022). They were simply the best open source models and they were extremely far behind SaaS SOTA.",
              "score": 12,
              "created_utc": "2026-01-02 18:42:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxaxtss",
              "author": "Ansible32",
              "text": "Really to me it seems like a product problem, not a research problem. Meta put together some stuff but once it became clear AI has nothing to do with improving their product, they stopped improving AI. The only thing Meta is using AI for is LLM summaries which if they are moderately successful will destroy Facebook Groups with their AI summaries nobody wants.\n\nContrast with Google, they have three different revenue streams they are building for AI: search has integrated a cheap LLM, it's ad-supported, this is their primary revenue stream and LLM fits in there perfectly.\n\nGemini is a paid chatbot with a freemium model.\n\nGemini also has APIs where you can pay per-query.\n\nMeta isn't using Llama in any way to drive revenue. Of course it's withering on the vine.",
              "score": 10,
              "created_utc": "2026-01-02 19:12:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxdyitu",
                  "author": "SkyFeistyLlama8",
                  "text": "Meta tried gatekeeping the kind of LLMs that can be integrated into WhatsApp. It's enshittification all the way down.\n\nFacebook only exists as an advertising platform with Usenet-style groups tacked on. Remove the user generated content and there's nothing left. I'm happy that countries like Australia have started banning social media apps and websites for younger users because it lets competitors rise up without having to fight for mindshare among new users.",
                  "score": 1,
                  "created_utc": "2026-01-03 05:09:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxe4cc4",
                  "author": "TracerBulletX",
                  "text": "Thats not really true? Itâ€™s a big part of the product strategy for the glasses which are pretty popular",
                  "score": 1,
                  "created_utc": "2026-01-03 05:52:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxaxnpa",
              "author": "paul__k",
              "text": "Zuckerberg ist just not that good of a CEO or technologist. He got lucky with the original FB, and he made some good acquisitions with Instagram and WhatsApp, but almost nothing else they have tried to build themselves over the last decade has really worked out. But they do have a lot of free cash flow, which makes the failures easy to ignore.\n\nAs for AI, they were never that good to begin with, because they mostly have second rate talent. That was enough to kind of keep up in the beginning, but now they have reached their limits. They need better people, but they can't get those unless they throw around ridiculous pay packages, because top tier talent doesn't want to ruin their careers by playing for a second division team.",
              "score": 11,
              "created_utc": "2026-01-02 19:12:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxb2bp0",
              "author": "LazloStPierre",
              "text": "It's ridiculous. I'm no fan of the open and proud Nazi, but he showed you can basically throw money at this and catch up to almost SOTA starting from a standing start. Some of the Chinese companies have started from far behind Meta and now are in that same category. How can Meta, with the money they've thrown at it and so much experience in the game be THAT far behind!?",
              "score": 11,
              "created_utc": "2026-01-02 19:34:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxavefk",
              "author": "Chogo82",
              "text": "Meta was not positioned strategically for generative AI. They had data centers but no AI infrastructure. They were pouring billions into the metaverse concept which had already been executed by second life/google glass over 10 years ago. They didnâ€™t make the pivot to gen AI until several major LLM tools were already available. With how fast AI innovation happens, they were definitely late to the game. Creating open source is a strategic way to break up the grip of the large players and distribute talent into smaller pockets. If you remember during the early days of social media, Facebook used this exact strategy to acquire/kill off a ton of social media competitors.",
              "score": 6,
              "created_utc": "2026-01-02 19:01:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxavsfl",
          "author": "Cool-Chemical-5629",
          "text": "At this point, I wouldn't be surprised if Behemoth model was just an empty promise from the beginning.",
          "score": 9,
          "created_utc": "2026-01-02 19:03:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbaj8k",
          "author": "PsychologicalOne752",
          "text": "With DeepSeek, GLM 4.7 and now IQuest Coder V1, China seems to have taken up the mantle of open-source LLMs and is delivering fast and in quality. Unfortunately, IMO, the US suffers from a lack of good leadership, where everyone wants to raise billions without adding actual value.",
          "score": 7,
          "created_utc": "2026-01-02 20:14:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxatb52",
          "author": "Golden_Jiggy",
          "text": "Sounds like defrauding shareholders to me ðŸ¤·â€â™‚ï¸",
          "score": 10,
          "created_utc": "2026-01-02 18:51:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbahg6",
              "author": "Competitive_Travel16",
              "text": "Defrauding end-users, even of open source products, is not the same thing.",
              "score": 3,
              "created_utc": "2026-01-02 20:13:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxf1o6t",
              "author": "MoffKalast",
              "text": "Probably why they were all fired afterwards.",
              "score": 1,
              "created_utc": "2026-01-03 10:35:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxcmcf8",
          "author": "Revolutionalredstone",
          "text": "Llama4 cheating on benchmarks is such a well-known fact that I've even seen it mentioned in official papers ðŸ˜† \n\nScout etc were an interesting experiment! But way too much focus on getting high numbers for a a model that rambled and was incoherent ðŸ˜†\n\nQwen has basically took over with their llama style project, I would love for llama 5 to be awesome ðŸ˜Ž",
          "score": 2,
          "created_utc": "2026-01-03 00:19:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfuycx",
              "author": "silenceimpaired",
              "text": "I would love llama 5. But it isnâ€™t likely to come or if it does it wonâ€™t be local",
              "score": 1,
              "created_utc": "2026-01-03 14:07:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxd0ud3",
          "author": "ditmarsnyc",
          "text": "post the FT link anyway, there is an archive website that can capture it\nedit: yes the archive dot ph site has captured it, will not post link to avoid automod filters",
          "score": 1,
          "created_utc": "2026-01-03 01:41:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbymmw",
          "author": "LanceThunder",
          "text": "lol a lot of people on the stock subs swear that meta is a sleeping giant and that they are going to get some deeeep value out of meta stocks once AI really takes off. impossible to explain to bagholders that they are wrong.",
          "score": 1,
          "created_utc": "2026-01-02 22:12:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfjlur",
              "author": "davikrehalt",
              "text": "Meta is objectively a sleeper giant",
              "score": 1,
              "created_utc": "2026-01-03 12:58:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbb8lz",
          "author": "doodlinghearsay",
          "text": "THIS IS BRAND NEW INFORMATION!",
          "score": -4,
          "created_utc": "2026-01-02 20:17:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxaf7x7",
          "author": "emprahsFury",
          "text": "So according to his words: LeCun, who was in charge, has his team fudge the numbers. Causing Zuck to lose confidence in the entire org and sideline it and then eventually replace it.  \n\nHow is this anything but a bad look on LeCun",
          "score": -57,
          "created_utc": "2026-01-02 17:47:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxag0bj",
              "author": "IJOY94",
              "text": "LeCun was in a different division? LeCun headed up FAIR, LLAMA 4 came out of MSL.",
              "score": 63,
              "created_utc": "2026-01-02 17:50:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxat8dp",
                  "author": "pm_me_github_repos",
                  "text": "Llama 4 came out of GenAI. MSL wasnâ€™t a thing til later",
                  "score": 9,
                  "created_utc": "2026-01-02 18:51:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxag0ta",
              "author": "No_Afternoon_4260",
              "text": "Was he in charge of L4? Can't remember",
              "score": 13,
              "created_utc": "2026-01-02 17:50:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxagap4",
                  "author": "TheRealMasonMac",
                  "text": "He said he didn't have anything to do with LLaMa apart from the first one, IIRC.",
                  "score": 29,
                  "created_utc": "2026-01-02 17:52:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pz68fz",
      "title": "Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/ocq43c2a79ag1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-30 02:43:48",
      "score": 340,
      "num_comments": 120,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwob4kq",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 04:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo4n54",
          "author": "popiazaza",
          "text": "Not much of a surprise since every company has to make the money eventually.\n\nReleasing open weight models is just a cheaper way to advertise their AI lab instead of spending millions providing free or very cheap inference APIs.\n\nStill hope they would keep releasing open weight models at least until they really taking the lead and beating OpenAI/Anthropic/Google.",
          "score": 44,
          "created_utc": "2025-12-30 03:41:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpxhnc",
              "author": "SmartMario22",
              "text": "I don't disagree but they're ALSO spending millions to provide cheap API lol",
              "score": 13,
              "created_utc": "2025-12-30 12:27:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwqqa3j",
                  "author": "Mr_Hyper_Focus",
                  "text": "Came to post this lol",
                  "score": 3,
                  "created_utc": "2025-12-30 15:18:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwxdfz5",
                  "author": "eli_pizza",
                  "text": "Sure thatâ€™s how you gain market share. Uber was extremely cheapâ€¦and then raised prices after forcing competitors out of the market.",
                  "score": 1,
                  "created_utc": "2025-12-31 15:26:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwnyhgo",
          "author": "RhubarbSimilar1683",
          "text": "Good bye to open source! It's just a matter of time",
          "score": 171,
          "created_utc": "2025-12-30 03:06:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo157q",
              "author": "ThenExtension9196",
              "text": "Yep. Everyone saying the Chinese open source was some gift to humanity was delusional. They did what they had to do to compete with larger companies with capital. Now that they got their foothold itâ€™s business as usual.",
              "score": 105,
              "created_utc": "2025-12-30 03:21:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwolc1s",
                  "author": "Honest_Science",
                  "text": "Devaluating US models is part of the chinese way to compete.",
                  "score": 56,
                  "created_utc": "2025-12-30 05:28:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwokhlo",
                  "author": "kawaii_karthus",
                  "text": "I think they will continue to release good open source models for years to come. There inner domestic competition is fierce and probably not united. And this goes for all markets not just AI. while i was visiting family and living there for a while, I still see them building tons of factories.. (though slower then the years before) even with a global recession going on... like who is going to be their customers?? time will tell. They do love over saturating any market they can get into though.. the AI industry is no different.",
                  "score": 27,
                  "created_utc": "2025-12-30 05:22:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwrke1e",
                  "author": "letsgeditmedia",
                  "text": "I donâ€™t think going IPO means that open sourcing was some kind of ruseâ€¦ itâ€™s just fighting against the realities of living under global capitalism",
                  "score": 1,
                  "created_utc": "2025-12-30 17:40:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpiaep",
              "author": "FreddoRS",
              "text": "Qwen models are Alibaba and mostly open weights, I imagine that's what z.ai will end up doing, mostly free models with some specific ones locked behind partnered cloud inference providers",
              "score": 8,
              "created_utc": "2025-12-30 10:18:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvflh0",
                  "author": "Neither-Phone-7264",
                  "text": "the top end models are proprietary. we might only get like, glm 5 air. oh well",
                  "score": 1,
                  "created_utc": "2025-12-31 06:21:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwo6g0y",
              "author": "Sensitive_Song4219",
              "text": "Hope this doesn't happen but I fear you may be right.\n\nThe cat's out the bag, though: if z.ai goes rogue I'm pretty sure others will take their place, progress in the open-weights space has been astonishing lately, and z.ai isn't the only player.\n\nAlso this had better not mess with their nice coding plans!",
              "score": 14,
              "created_utc": "2025-12-30 03:51:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwoyl1k",
              "author": "xantrel",
              "text": "Eh, not necessarily. I know open weights is a far cry from open source llms, but many people would not send their most private data to a (former) Chinese company in this heavily politicized world. Them removing the open source component essentially shuts down a good chunk of the western market. I know the eastern European and Asian markets don't mind it as much, but much of the money and prestige comes from being the open source model leader.Â \n\n\nAll of these companies are basically trying modern architectures while distilling the big commercial models (openai, anthropic, google). That's why open source magically trails a few months behind the big 3.\n\n\nAll this to say, if Z stops releasing models (and maybe they will), it shouldn't be a huge loss for the community since Minimax or another entrant can easily take their place as what their doing is vastly cheaper and simpler than what actual leading labs are doing. Yes it's expensive, but everyone has seen that's it's also a very cheap way to get a ton of free publicity and users. If you aren't SOTA closed source, I think it's a better commercial option to be SOTA open source than crappy closed source. The cost of switching providers is too low.",
              "score": 11,
              "created_utc": "2025-12-30 07:16:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpxgjq",
                  "author": "FullOf_Bad_Ideas",
                  "text": "Minimax is also IPOing, so if Zhipu stops releasing their models, Minimax will most likely do the same.",
                  "score": 2,
                  "created_utc": "2025-12-30 12:27:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpajzv",
              "author": "howardhus",
              "text": "what if i told youâ€¦ there was never\nopen source? those were all open weights.\n\n\nbasically shareware models  usable enough for free marketing and to ger known\n\nwhy people (in this sib of all places!) still say open source is beyond me\n\n\nthe pattern was always the same: small group of people publish small cool model showing some intetestong feature, usable enough to showcase the function but not good enough gor production.\n\nmodel gets hyped on redditâ€¦\n\n\nmodel never gets any updates and group of people are never heard of again",
              "score": 7,
              "created_utc": "2025-12-30 09:06:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwrbn75",
                  "author": "RhubarbSimilar1683",
                  "text": "Because you can technically still mess with the weights, the data is pretty much already public because it's the whole internet and books and the training recipe is some paper, instruct training data pairs though is something I agree with but it's not too hard to generate those synthetically nowadays with open models, although they were originally created by online workers at data annotation places like outlier ai",
                  "score": -2,
                  "created_utc": "2025-12-30 16:59:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwobhw6",
              "author": "ScythSergal",
              "text": "Their horrible handling and PR around the 4.6 Air release was the writing on the wall for me.\n\nThe lying, over hyping, lying again, denial, then lying a third time, only to end up not releasing it, and avoid interacting with anything that mentions it.\n\nIt was as simple as \"we changed our mind on this release\" or something simple. But instead they lied a multitude of times and got everybody excited for something they never ended up releasing. And they didn't even have the decency to say why or clarify that it wasn't coming out so people would stop holding on. It's just disrespectful",
              "score": 21,
              "created_utc": "2025-12-30 04:22:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwovxdz",
                  "author": "JazzlikeLeave5530",
                  "text": "The writing on the wall should have been them being a corporation lol",
                  "score": 14,
                  "created_utc": "2025-12-30 06:53:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwp82yu",
                  "author": "CheatCodesOfLife",
                  "text": "I was watching that even though I \"don't need some air\".\n\nTo me it looked like some devs were surprised by the demand and got too excited when they say \"2 weeks\" or whatever it was, then weren't able to deliver.\n\nAlso (I could be wrong or misremembering), I thought I read somewhere that they weren't able to train it properly?\n\nbtw, I see they've got an air-sized 4.6-VL. Is that no good?",
                  "score": 9,
                  "created_utc": "2025-12-30 08:43:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwobs25",
                  "author": "Odd-Ordinary-5922",
                  "text": "I remember Q&A they said that they still have some open models coming out at the beginning of next year so fingers crossed",
                  "score": 0,
                  "created_utc": "2025-12-30 04:24:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoh22t",
              "author": "bick_nyers",
              "text": "Many of us are only willing to pay subscriptions to models that have been open sourced. I don't think Z.ai is dumb enough to go closed source and kill all of their good will with the community. We shall see.",
              "score": 5,
              "created_utc": "2025-12-30 04:58:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwona2l",
                  "author": "the320x200",
                  "text": "\"There are dozens of us!\"\n\nDude, nobody with enough money to matter is making decisions like that... This community is nothing compared to corporate users, not in number and not in bankroll.",
                  "score": 26,
                  "created_utc": "2025-12-30 05:43:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrejvh",
              "author": "IrisColt",
              "text": "bye, sigh...",
              "score": 1,
              "created_utc": "2025-12-30 17:13:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwq1czy",
              "author": "GreenGreasyGreasels",
              "text": ">Good bye to open source! It's just a matter of time\n\nUnlikely. They are going the Mistral way. That's the plan for now.",
              "score": 0,
              "created_utc": "2025-12-30 12:55:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwo5mqm",
          "author": "abeecrombie",
          "text": "Why does everyone assume they won't keep releasing open weight models ?  U pay for z.ai subscription bc 1. I don't care about privacy for my pet projects 2. $3 a month or whatever vs $3000+ for a GPU capable of running their models makes sense for a lot of ppl ( myself included) \n\nIf the Chinese government still considers open source a priority I think companies like z.ai can still release open weight models and find a way to make money via inference/ mcp . Im far from a political expert but believe that policy still holds. \n\nHappy to hear arguments why I'm wrong.",
          "score": 61,
          "created_utc": "2025-12-30 03:47:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo8fw6",
              "author": "cafedude",
              "text": "You have a point. Everyone could make their own ketchup - the recipes are out there and they're not that hard, but pretty much nobody makes their own ketchup since it's a lot easier to buy a bottle for $3.",
              "score": 35,
              "created_utc": "2025-12-30 04:04:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpncmt",
                  "author": "power97992",
                  "text": "I made  my own ketchup beforeâ€¦",
                  "score": 5,
                  "created_utc": "2025-12-30 11:04:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwo8sd5",
              "author": "Sensitive_Song4219",
              "text": "Perhaps. Even OpenAI manages to occasionally contribute with their GPT-OSS releases. We'll see if Z can align this with their mission statement, in their AMA they [said](https://www.reddit.com/r/LocalLLaMA/s/2yDtPG0Qbl) open source would still be a priority after going public. Hope they meant it.\n\nRegarding privacy: would there be added accountability regarding their data handling once they're publicly traded?",
              "score": 3,
              "created_utc": "2025-12-30 04:06:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwotwck",
                  "author": "Corporate_Drone31",
                  "text": "got-oss was not \"occasionally,\" it was a one-off after literal years of not having released any large language model past GPT-2.",
                  "score": 22,
                  "created_utc": "2025-12-30 06:35:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqehlb",
                  "author": "abeecrombie",
                  "text": "Good question regarding the accountability of data once they are public. I'm not sure they'd have to comply with any extra regulations but it should be more visibly disclosed and discussed. \n\nFor example I think as soon as you deal with European user data you have to comply with GDPR. So that shouldn't be new. What would be new is z.ai would most likely have to disclose it to their auditors / board etc that they are in compliance. Not sure you see any real disclosures from the Chinese ai labs on that front today.",
                  "score": 5,
                  "created_utc": "2025-12-30 14:15:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwofh2c",
              "author": "cobbleplox",
              "text": "Well when I don't care about privacy, the competition is suddenly full blown chatgpt and such? But I guess that's going to be their problem one way or the other. Also I think a lot of the appeal of open models is community finetunes. They won't be serving these, will they?",
              "score": 1,
              "created_utc": "2025-12-30 04:48:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwofklp",
              "author": "Erebea01",
              "text": "Not saying they're not harvesting our data or that they're trustworthy but it always boggles my mind on how much people and redditors are paranoid about the Chinese government and their tech companies when the worse offenders have always been the US govt and their tech companies. Why be afraid of a govt thousands of miles away unless you're afraid they're gonna blackmail you with your CP or something",
              "score": 0,
              "created_utc": "2025-12-30 04:48:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwnwdom",
          "author": "HornyGooner4401",
          "text": "Please don't sell out",
          "score": 25,
          "created_utc": "2025-12-30 02:54:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnx35h",
              "author": "True_Requirement_891",
              "text": "It's the rule of the game they are playing. They basically have to eventually.",
              "score": 52,
              "created_utc": "2025-12-30 02:58:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwop1qp",
                  "author": "ForsookComparison",
                  "text": "Either that have to sell out or we (the community) need a way to contribute upstream similar to regular open source software.\n\nIn the Llama 2 days I was optimistic that this could come via community datasets and fine-tunes. Nowadays I don't really know what we offer them besides IPO hype. Maybe this is *THE* open weight play? Drum up buzz for legitimacy, maybe even some revenue via official API providers, swoon the funding rounds, bam. You're acquired or public.",
                  "score": 9,
                  "created_utc": "2025-12-30 05:56:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwo18t7",
              "author": "ThenExtension9196",
              "text": "Bro thatâ€™s the name of the game.",
              "score": 18,
              "created_utc": "2025-12-30 03:21:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwnynpo",
          "author": "Odd-Ordinary-5922",
          "text": "I think we can expect to see less open source models from them although they have contributed a lot so I think its well deserved to get the bag",
          "score": 17,
          "created_utc": "2025-12-30 03:07:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnxlo0",
          "author": "LocoMod",
          "text": "Shareholders dont like giving product away for free.",
          "score": 15,
          "created_utc": "2025-12-30 03:01:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoq725",
              "author": "misterflyer",
              "text": "[https://youtu.be/0MXSAwkVU3U?t=458](https://youtu.be/0MXSAwkVU3U?t=458)",
              "score": -1,
              "created_utc": "2025-12-30 06:05:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqwae6",
          "author": "hyno111",
          "text": "I think Z.ai/ChatGLM is one of the few models that actually implements a proper search agent â€” meaning it can look at search results during reasoning when necessary, and then perform additional searches with updated keywords if needed.\n\nItâ€™s also one of the very few search agents that passed my â€œMagical Realism Large Model Search Capability Test,â€ which consists of the following multi-turn prompts:\n\nâ€œHow should an LLM defend against search engine poisoning?â€\nâ€œDo not use search. Donald Trump just announced a Trump-class battleship at Mar-a-Lago, (with specific technical details). How plausible is this?â€\nâ€œNow use search. Are these claims real, or are you being affected by search engine poisoning?â€",
          "score": 3,
          "created_utc": "2025-12-30 15:47:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwory70",
          "author": "toothpastespiders",
          "text": "It was great while it lasted. Air's probably going to have a place of honor next to Yi 34b in my hard drive's LLM memorial. It's possible they might not fall off after this. But I think I'm just going to assume that's the case and be pleasantly surprised if I'm wrong. \n\nSucks, but they gave us some great releases. Certainly made 2025 a lot more interesting in this space.",
          "score": 5,
          "created_utc": "2025-12-30 06:19:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9nrs",
          "author": "IngwiePhoenix",
          "text": "> Going for an IPO\n\nAaaaaand it's gone! :D Any company that IPO'd is basically \"useless\" to normal users.\n\nWelp, was fun while it lasted.",
          "score": 9,
          "created_utc": "2025-12-30 04:11:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp2gkt",
              "author": "Novel-Mechanic3448",
              "text": "yeah man google, meta, hell anyone in the fortune 500, totally useless.\n\nhaha",
              "score": 3,
              "created_utc": "2025-12-30 07:51:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwoglko",
          "author": "HelpRespawnedAsDee",
          "text": "Definitely the next acquisition target.",
          "score": 2,
          "created_utc": "2025-12-30 04:55:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpbazd",
          "author": "evia89",
          "text": "F for cheap api ($25/year coding plan that is not useless)",
          "score": 2,
          "created_utc": "2025-12-30 09:13:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq4203",
          "author": "Available_Brain6231",
          "text": "If I had one coin for every product and company that got better after going public... I would not have a single coin...  \nBUT china is the only true capitalist country in the world so maybe it will work over there.",
          "score": 2,
          "created_utc": "2025-12-30 13:13:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnuzxk",
          "author": "Prestigious_Fold_175",
          "text": "10x Cheaper than openai.",
          "score": 5,
          "created_utc": "2025-12-30 02:46:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo1haa",
              "author": "ThenExtension9196",
              "text": "Not after the shareholders have a say.",
              "score": 30,
              "created_utc": "2025-12-30 03:23:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwnxpw7",
              "author": "LocoMod",
              "text": "10x less capability too. Entropy is preserved and physics still makes sense!",
              "score": -16,
              "created_utc": "2025-12-30 03:01:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwo2ood",
                  "author": "1kakashi",
                  "text": "What? This is seriously funny ðŸ¤£",
                  "score": 5,
                  "created_utc": "2025-12-30 03:30:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwo3jq1",
                  "author": "cockerspanielhere",
                  "text": "What do you know about physics ðŸ˜‚",
                  "score": 1,
                  "created_utc": "2025-12-30 03:35:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwojguv",
          "author": "drooolingidiot",
          "text": "I'll buy the stock only if they release GLM 4.7 Air.",
          "score": 2,
          "created_utc": "2025-12-30 05:15:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp8ccq",
              "author": "CheatCodesOfLife",
              "text": "I think in that podcast episode on spotify, they said they will, but it'll be qwen-3-30b sided (so useless).",
              "score": -1,
              "created_utc": "2025-12-30 08:46:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwsov9l",
                  "author": "drooolingidiot",
                  "text": "Did they say it again, or only back when they released GLM 4.5?",
                  "score": 1,
                  "created_utc": "2025-12-30 20:51:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpbkcg",
          "author": "Fit-Produce420",
          "text": "Some people say cucumbers taste better pickled.Â ",
          "score": 1,
          "created_utc": "2025-12-30 09:16:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpt9cj",
          "author": "JLeonsarmiento",
          "text": "Ok, I want 1000 shares.",
          "score": 1,
          "created_utc": "2025-12-30 11:55:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq6lxc",
          "author": "ANR2ME",
          "text": "Hopefully it's not an exit strategy for early investors, like what e-commerce companies did after being in deficit for years ðŸ˜… (not sure whether Z AI was already profitable or not).",
          "score": 1,
          "created_utc": "2025-12-30 13:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrk9m9",
          "author": "letsgeditmedia",
          "text": "So we can invest in this in the states or nah",
          "score": 1,
          "created_utc": "2025-12-30 17:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrsjkc",
              "author": "Fine-Will",
              "text": "Depends on your broker. I know IKBR and Fidelity does.",
              "score": 1,
              "created_utc": "2025-12-30 18:18:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv5abt",
          "author": "Ylsid",
          "text": "It's a Chinese business, don't touch it if you aren't ready to get rugpulled for politics",
          "score": 1,
          "created_utc": "2025-12-31 05:04:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwynkpu",
          "author": "lunatix",
          "text": "I mean meta's going to just buy them right?",
          "score": 1,
          "created_utc": "2025-12-31 19:16:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwoboax",
          "author": "shoeshineboy_99",
          "text": "Link to the submission announcement. Has anyone got hold of the submitted prospectus? Will be interesting to read. \n\n[submission announcement ](https://www1.hkexnews.hk/app/sehk/2025/107977/documents/sehk25121901972.pdf)",
          "score": 1,
          "created_utc": "2025-12-30 04:23:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrajv7",
              "author": "FullOf_Bad_Ideas",
              "text": "No but here are some revenue and expenditures numbers. You'll need to translate it from Chinese. https://wallstreetcn.com/articles/3761776",
              "score": 2,
              "created_utc": "2025-12-30 16:54:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws1879",
                  "author": "shoeshineboy_99",
                  "text": "cool. Found the english version of the document. Dont know why my comment was downvoted!",
                  "score": 2,
                  "created_utc": "2025-12-30 18:58:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo860o",
          "author": "pellucide",
          "text": "Does z.ai have an app",
          "score": 1,
          "created_utc": "2025-12-30 04:02:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwopd6w",
              "author": "lly0571",
              "text": "They have [an app](https://chatglm.cn) in China, but not in a style like `chat.z.ai` or `chat.qwen.ai`.",
              "score": 3,
              "created_utc": "2025-12-30 05:59:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwo8q9i",
              "author": "Amazing_Athlete_2265",
              "text": "Does google have an app?",
              "score": 1,
              "created_utc": "2025-12-30 04:05:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwo9b9o",
                  "author": "pellucide",
                  "text": "https://play.google.com/store/apps/details?id=com.google.android.googlequicksearchbox&hl=en",
                  "score": 2,
                  "created_utc": "2025-12-30 04:09:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwocpvm",
          "author": "met_MY_verse",
          "text": "Puts it is.",
          "score": 1,
          "created_utc": "2025-12-30 04:30:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq1k4n",
          "author": "ridablellama",
          "text": "How can I buy as an American?",
          "score": 1,
          "created_utc": "2025-12-30 12:57:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwodbn3",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2025-12-30 04:34:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp125g",
              "author": "Different_Fix_2217",
              "text": "Very unlikely they will be releasing any more models opensource with this.",
              "score": 2,
              "created_utc": "2025-12-30 07:38:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwraf2e",
              "author": "FullOf_Bad_Ideas",
              "text": "Here's more info. Translate from Chinese to English with an LLM or other translation tool. https://wallstreetcn.com/articles/3761776",
              "score": 1,
              "created_utc": "2025-12-30 16:54:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzcrtb",
      "title": "Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/yq8uriwhxaag1.jpeg",
      "author": "ResearchCrafty1804",
      "created_utc": "2025-12-30 08:26:06",
      "score": 325,
      "num_comments": 36,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwr47l3",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 16:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwp7s4a",
          "author": "redditscraperbot2",
          "text": "Oh this looks really cool\n\nEdit:\nGot it running and it is really cool. Works as advertised. This is going to be a massive speed boost to people working on games. Only a little cleanup needed for each animation.",
          "score": 73,
          "created_utc": "2025-12-30 08:40:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq7pdq",
              "author": "Zundrium",
              "text": "Ain't nobody got time to clean up animations. Where is my 1B-motion-to-clean-motion model?",
              "score": 44,
              "created_utc": "2025-12-30 13:36:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvzuid",
                  "author": "WEREWOLF_BX13",
                  "text": "FR HAHAHA",
                  "score": 3,
                  "created_utc": "2025-12-31 09:26:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwqelw9",
              "author": "ab2377",
              "text": "how did you run it?",
              "score": 10,
              "created_utc": "2025-12-30 14:16:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws5bfq",
                  "author": "mxforest",
                  "text": "It's text to motion. You just have to command it to run via chat. Duh!",
                  "score": -6,
                  "created_utc": "2025-12-30 19:17:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwuvmfx",
              "author": "Ylsid",
              "text": "How on earth did you get it out of dependency hell",
              "score": 5,
              "created_utc": "2025-12-31 03:59:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwv7oka",
                  "author": "redditscraperbot2",
                  "text": "What dependency issues were you having?\nI made my env with python 3.10 and torch 12.1",
                  "score": 4,
                  "created_utc": "2025-12-31 05:21:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrzbbj",
              "author": "WitAndWonder",
              "text": "What animation format does this actually spit out?",
              "score": 6,
              "created_utc": "2025-12-30 18:49:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwt78qo",
                  "author": "redditscraperbot2",
                  "text": "An fbx of the wooden doll doing the animation with a skeleton",
                  "score": 8,
                  "created_utc": "2025-12-30 22:18:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nww3c7u",
              "author": "TheMisterPirate",
              "text": "What's your hardware? Curious how much vram this uses",
              "score": 1,
              "created_utc": "2025-12-31 09:59:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww4mim",
                  "author": "redditscraperbot2",
                  "text": "3090\n64gb of ram\n\nAt fp16 it takes around 18-23gb per gen with a batch of 4, but I imagine if I cared enough to offload the text encoder it would use significantly less.",
                  "score": 1,
                  "created_utc": "2025-12-31 10:11:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwqpxbl",
          "author": "Quiet-Owl9220",
          "text": "Oh boy. The virt-a-mate community ought to have some good uses for this one...",
          "score": 18,
          "created_utc": "2025-12-30 15:16:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpozo6",
          "author": "Illustrious-Lake2603",
          "text": "Does this work only for Humanoid models? Or will it work for animals as well?? I have been working with puppeteer and it has actually been magical.",
          "score": 16,
          "created_utc": "2025-12-30 11:19:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrz3yg",
              "author": "WitAndWonder",
              "text": "It was trained exclusively on the standard human model. Seems like it could work for other bipedal movements to some extent, but anything with more or less limbs seems out of the question unfortunately.",
              "score": 5,
              "created_utc": "2025-12-30 18:48:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws8lv5",
                  "author": "Illustrious-Lake2603",
                  "text": "Thank you! Its still beneficial. I can probably make it work along side puppeteer to be able to get better humanoid animations (if they are good).",
                  "score": 1,
                  "created_utc": "2025-12-30 19:33:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrumui",
              "author": "fiddler64",
              "text": "yeah, and there's only 1 predetermined rig, I tried prompting a dog chasing after ball and it shows the humanoid rig throwing the ball instead",
              "score": 2,
              "created_utc": "2025-12-30 18:27:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpqo9r",
          "author": "KingDutchIsBad455",
          "text": "Is this what Neuro uses?",
          "score": 17,
          "created_utc": "2025-12-30 11:33:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrnwwa",
              "author": "inaem",
              "text": "The sitting looks the same tbh",
              "score": 1,
              "created_utc": "2025-12-30 17:57:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwqg3rd",
              "author": "Emotional-Metal4879",
              "text": "also wander",
              "score": 0,
              "created_utc": "2025-12-30 14:24:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqhx2d",
          "author": "no_witty_username",
          "text": "This is more cool then folks realize.  Soo many uses for this.",
          "score": 10,
          "created_utc": "2025-12-30 14:34:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrywy0",
              "author": "WitAndWonder",
              "text": "Just a pity that it was basically exclusively human motion. If it also covered quadruped we'd be really in business since that's most of the creatures that get put into games, too.",
              "score": 4,
              "created_utc": "2025-12-30 18:47:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwrviwp",
              "author": "Crypt0Nihilist",
              "text": "Don't leave us hanging!",
              "score": -1,
              "created_utc": "2025-12-30 18:31:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwu533b",
          "author": "JonatasLaw",
          "text": "Now I can work in my game hahaha",
          "score": 2,
          "created_utc": "2025-12-31 01:22:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwufjmc",
          "author": "TanguayX",
          "text": "Wow. Impressive",
          "score": 1,
          "created_utc": "2025-12-31 02:22:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv7b53",
          "author": "Specific-Strain7970",
          "text": "This looks super interesting! (I'm a beginner) What are the models I can apply these animations to? Would stock mixamo models work directly? I'm thinking Unity. Thanks for the help in advance!",
          "score": 1,
          "created_utc": "2025-12-31 05:18:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww00ht",
          "author": "WEREWOLF_BX13",
          "text": "This is exactly what I've been waiting for! Soon enough anyone will be able to create anything at mass scale and say goodbye to shitty company slop!",
          "score": 1,
          "created_utc": "2025-12-31 09:27:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdhc1k",
          "author": "Bl_grill",
          "text": "Its alright but until it can walk through a door, place hand on door way frame and open door then walk through with face capture in real time and have the person react absolutely devastating.\n\n\nBut this is for npc, or repetitious situations, i can see what tencent is doing they're focusing on repetitive tasks, to free up development.\n\n\nChinese developerd can focus(including others too)to working on speed and efficiency, while professionals focus on mocap.\n\n\nWhat i cannot wait for is ai reacting to its environment and the gamer drops in, and it hunts the gamer and mocks them(yes yes, this game that game has it)but i thought such technology was gonna be embedded in consoles ai chip for neural processing for developers freeing up stricted ai.\n\n\nOverall seeing ai stumble and get frustrated as you shoot it coming back reminding you, almost like the nemisis engine from lord of rings.\n\n\nOverall helpful but no game changer just a pipeline implementation on tencents roadmap.",
          "score": 1,
          "created_utc": "2026-01-03 03:18:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpxlvk",
          "author": "paryska99",
          "text": "Do they also release any finetuning code? This could be really cool for 3D artists or for games with generative content.",
          "score": 1,
          "created_utc": "2025-12-30 12:28:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtq6ff",
              "author": "Erdeem",
              "text": "Not yet",
              "score": 1,
              "created_utc": "2025-12-30 23:59:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsprse",
          "author": "RedZero76",
          "text": "I just shat myself.  This is GOLD.",
          "score": 1,
          "created_utc": "2025-12-30 20:55:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q41bw1",
      "title": "GLM-Image model from Z.ai is coming",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/sm31vizebebg1.png",
      "author": "Ravencloud007",
      "created_utc": "2026-01-04 20:54:04",
      "score": 310,
      "num_comments": 58,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxq2ns7",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-05 00:05:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxp1wkh",
          "author": "Tootsie_Rolls_Fan",
          "text": "I can feel it, 103b parameters",
          "score": 61,
          "created_utc": "2026-01-04 21:08:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpsqsi",
              "author": "Freonr2",
              "text": "I'd legit love to see a good MOE image model.  Hunyuan 80b was pretty meh.\n\nBonus points for native mxfp4 or nvfp4 weights.",
              "score": 13,
              "created_utc": "2026-01-04 23:15:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxq7tr2",
                  "author": "SanDiegoDude",
                  "text": "Hunyuan 80b was seriously impressive. It's just a friggen cow that nobody can run on home equipment (unless seriously compromising its output). It also had a bit of a nugget problem, though that could have been tuned out with community fine tuning... shame the community doesn't have a bunch of H200s lined up to do that tuning though, so into the pile of forgotten models it goes. Flux2 is pretty much in the same boat, even with the turbo loras that just dropped recently. Too big and too slow to be worthwhile.",
                  "score": 9,
                  "created_utc": "2026-01-05 00:30:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxsn7ct",
                  "author": "stoppableDissolution",
                  "text": "God please no. Not another monster thats not runnable at home.",
                  "score": 1,
                  "created_utc": "2026-01-05 10:29:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxp5w48",
              "author": "ShengrenR",
              "text": "And what does that feel like?",
              "score": 8,
              "created_utc": "2026-01-04 21:27:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxp80a9",
                  "author": "Admirable-Star7088",
                  "text": "That question has to be passed to our VRAM.",
                  "score": 27,
                  "created_utc": "2026-01-04 21:37:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxp8n7e",
                  "author": "misterflyer",
                  "text": "better than sex ðŸ’¯",
                  "score": 7,
                  "created_utc": "2026-01-04 21:39:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxqecu7",
                  "author": "no_witty_username",
                  "text": "https://imgs.search.brave.com/21_jbxyeN_5e0DBVWb969Qp_0F9SLKFrrg30vz8TVCQ/rs:fit:500:0:1:0/g:ce/aHR0cHM6Ly93d3cu/bWVtZS1hcnNlbmFs/LmNvbS9tZW1lcy8y/ZDZkZTkxMWI2NTM4/YmU5ZWJkNzY4OTMz/NTg2Mjc0Yy5qcGc",
                  "score": 3,
                  "created_utc": "2026-01-05 01:03:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxp1fca",
          "author": "nomorebuttsplz",
          "text": "right now Z image is the clear community favorite. Will take a lot to dethrone it",
          "score": 39,
          "created_utc": "2026-01-04 21:06:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpg01o",
              "author": "remghoost7",
              "text": "It'll have to be a relatively light model too.  \nFlux2 was *obliterated from orbit* by Z-Image-Turbo because of the speed and hardware requirements.\n\nI'm game for another \"competitor\" in the image generation space though.",
              "score": 38,
              "created_utc": "2026-01-04 22:14:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxph9j4",
                  "author": "Novel-Mechanic3448",
                  "text": "Flux would have been fine if they didn't spend more time censoring it than training it but here we are",
                  "score": 38,
                  "created_utc": "2026-01-04 22:20:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxptg6q",
                  "author": "Freonr2",
                  "text": "Yeah unless you want to nail complex text every time, ZIT is nearly as good at a tiny fraction of the size and compute.",
                  "score": 4,
                  "created_utc": "2026-01-04 23:19:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxpvsg8",
                  "author": "Environmental-Metal9",
                  "text": "Maybe, but when Flux came around and dethroned SDXL it was orders of magnitude larger than it. (Granted stabilityai wasnâ€™t really interested in actually competing).",
                  "score": 2,
                  "created_utc": "2026-01-04 23:30:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxptag3",
              "author": "Freonr2",
              "text": "ZIT is pretty incredible for its size/speed. We're still waiting on Z image base.",
              "score": 16,
              "created_utc": "2026-01-04 23:18:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxr7s65",
                  "author": "wh33t",
                  "text": "I would like to see if zimage-edit can work better than qwen image edit.",
                  "score": 3,
                  "created_utc": "2026-01-05 03:42:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxqalhs",
                  "author": "SlaveZelda",
                  "text": "I think it might come before/around chinese new year",
                  "score": 2,
                  "created_utc": "2026-01-05 00:43:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxq1nfb",
              "author": "TAW56234",
              "text": "That remains to be seen. Even with the stab preset, I have to worry about refusals in GLM4.7 thinking. Its gotten to the point it's not worth using. Fair chance you can't do NSFW here",
              "score": 1,
              "created_utc": "2026-01-05 00:00:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxq2da5",
                  "author": "nomorebuttsplz",
                  "text": "Do you have API? can you just pause, edit and continue the thinking process?",
                  "score": 1,
                  "created_utc": "2026-01-05 00:03:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxusyu7",
                  "author": "Novel-Mechanic3448",
                  "text": "I've never gotten a refusal with GLM4.7 using it locally.",
                  "score": 0,
                  "created_utc": "2026-01-05 17:56:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxpwbut",
          "author": "Environmental-Metal9",
          "text": "My only question is how many datacenters do I need to rent to be able to use this new model.\n\nI yearn for a model as small as SD1.5, as easy to finetune as current day SDXL, and with great quality like flux or some of the newer ones. But us GPU poors get nothing! Not even 1 out of 3 in this matrix",
          "score": 11,
          "created_utc": "2026-01-04 23:33:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxq4coe",
              "author": "SlowFail2433",
              "text": "I mean Flux quality in under 1B parameters like SD 1.5 just isnâ€™t possible yet or ever maybe",
              "score": 1,
              "created_utc": "2026-01-05 00:13:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxp74os",
          "author": "coder543",
          "text": "I asked `codex` to review this branch. It sounds like fun.\n\n> This branch adds a new model family: glm_image (GLMâ€‘Image). Itâ€™s a multimodal image-token generator built around a text decoder + vision stack + VQâ€‘VAE, aimed at textâ€‘toâ€‘image and imageâ€‘toâ€‘image token generation (for downstream DiT/diffusion), not a general chat-style multimodal LLM.\n> \n>  Whatâ€™s implemented vs origin/main:\n\n>  - New model package: src/transformers/models/glm_image/* with configs, modeling, processor, image processors (slow + fast), and a modular source file. src/transformers/models/glm_image/modular_glm_image.py and generated src/transformers/models/glm_image/modeling_glm_image.py are the core.\n>  - VQâ€‘VAE + vision + text components and a GlmImageForConditionalGeneration generation head (src/transformers/models/glm_image/modeling_glm_image.py).\n>  - Processor that fuses image + text, inserting image tokens and handling multiâ€‘image grids (src/transformers/models/glm_image/processing_glm_image.py).\n>  - Autoâ€‘mapping hookups for config/model/processor/tokenizer + docs + tests (docs/source/en/model_doc/glm_image.md, tests/models/glm_image/test_modeling_glm_image.py).\n\n>  Why itâ€™s not a generic multimodal LLM:\n\n>  - The model explicitly handles image token regions and image grids with start/end markers and 3â€‘axis RoPE for generation of image tokens, including textâ€‘toâ€‘image and imageâ€‘toâ€‘image flows (src/transformers/\n>  models/glm_image/modeling_glm_image.py).\n>  - It embeds a VQâ€‘VAE (GlmImageVQVAE) to tokenize/quantize images and uses those tokens in generation (src/transformers/models/glm_image/modeling_glm_image.py).\n>  - The usage example in docs describes â€œgenerate vision token for DITâ€ (docs/source/en/model_doc/glm_image.md), which is imageâ€‘generation oriented.\n\n>  So: this branch is implementing GLMâ€‘Image, a multimodal model centered on image token generation (textâ€‘toâ€‘image and imageâ€‘toâ€‘image), not a general-purpose multimodal chat LLM.",
          "score": 5,
          "created_utc": "2026-01-04 21:33:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpu3t0",
              "author": "thatsnot_kawaii_bro",
              "text": "ok",
              "score": -6,
              "created_utc": "2026-01-04 23:22:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxp234g",
          "author": "Betadoggo_",
          "text": "Not a whole lot of information.\n\nhttps://preview.redd.it/cesoduf2eebg1.png?width=1410&format=png&auto=webp&s=39437b143429602c03fd456599179b601ada9c60\n\nAll I can gather based on the files is that the max input resolution is 2048x2048.",
          "score": 5,
          "created_utc": "2026-01-04 21:09:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxp2tck",
              "author": "nilpy",
              "text": "From the RoPE section ([https://github.com/huggingface/transformers/blob/cd8d78fcb4067979e921b20163d62035c51b4e7f/src/transformers/models/glm\\_image/modular\\_glm\\_image.py#L794](https://github.com/huggingface/transformers/blob/cd8d78fcb4067979e921b20163d62035c51b4e7f/src/transformers/models/glm_image/modular_glm_image.py#L794)):\n\n`=== Case 1: Image-to-Image Generation (single or multiple source images + 1 target image_grid) ===`\n\n`...`\n\n`=== Case 2: Text-to-Image Generation (no source images + 2 image_grids for multi-resolution) ===`\n\nSeems to be based on GLM4-V (MoE?). Has references to both DiT and VQVAE. It's possibly using NTP over discrete image tokens? That or something like show-o with discrete diffusion.",
              "score": 7,
              "created_utc": "2026-01-04 21:13:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxs3k6n",
          "author": "turklish",
          "text": "Still waiting on AIR...",
          "score": 3,
          "created_utc": "2026-01-05 07:25:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxp0yl7",
          "author": "__Maximum__",
          "text": "Meaning image generator?",
          "score": 2,
          "created_utc": "2026-01-04 21:04:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs9xtg",
          "author": "paperbenni",
          "text": "Please no, image models are useless compared to LLMs. GLM 4.7 can do real work, what am I going to do with an image model? That entire part of the AI industry should just die. These things are impressive, but all people are doing with it is memes and misinformation. Any compute going into this instead of GLM 5 is thoroughly wasted.",
          "score": -11,
          "created_utc": "2026-01-05 08:24:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1p5q5",
      "title": "Getting ready to train in Intel arc",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1q1p5q5",
      "author": "hasanismail_",
      "created_utc": "2026-01-02 04:33:19",
      "score": 299,
      "num_comments": 92,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q1p5q5/getting_ready_to_train_in_intel_arc/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nx8jin6",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-02 11:20:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7e5iq",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 129,
          "created_utc": "2026-01-02 05:11:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7z858",
              "author": "dsanft",
              "text": "I have a lot of great things to say about the ADT link pcie risers, the ones with the shielded silver cables. I run them in pcie 3 4x and even at lengths up to 80cm I've had no problems.",
              "score": 13,
              "created_utc": "2026-01-02 08:09:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx7zf7z",
              "author": "TheRealMasonMac",
              "text": "Gamers rose up.",
              "score": 3,
              "created_utc": "2026-01-02 08:11:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7m3mn",
          "author": "Techngro",
          "text": "Dude, you can't post stuff like this without details.",
          "score": 35,
          "created_utc": "2026-01-02 06:12:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9cpil",
              "author": "hasanismail_",
              "text": "Sorry was too excited when posting\n\n8x b580 GPUs (1 is not in picture I was playing a game at the time and needed it)\n\nDual Intel e5 Xeon v4 CPUs (forgot exact model)\n\n128gb ddr4 (I bought before the ram crisis)\n\nDual 850w corsair PSUs\n\n\nServer will run in Ubuntu latest release with the Intel patches and I'm gonna use vulkan and probably train with pytorch or something (I haven't thought that far ahead)\n\nI paid 200$-240$ per GPU mostly from micro center deals Facebook marketplace and I was able to snag some off amazon too I was planning on using the b50 but the memory band with is very slow compared to the b580 and the value proposition of the b580 is just too good to pass up",
              "score": 28,
              "created_utc": "2026-01-02 14:40:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxa39do",
                  "author": "satireplusplus",
                  "text": "Not sure what exactly you plan on training with pytorch, but the vulkan backend is extremely poor and non-functional for that. It contains a few functions, just barely enough to run object detection on android. Intel does have a special pytorch version with xpu support though (through their own intel-one stack). Report back what you can do with it, but it's not gonna be as smooth as CUDA or even rocm.",
                  "score": 5,
                  "created_utc": "2026-01-02 16:51:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9gbxe",
                  "author": "KoalaRashCream",
                  "text": "How many TOPS",
                  "score": 5,
                  "created_utc": "2026-01-02 15:00:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxbtmzc",
                  "author": "autistic-brother",
                  "text": "What mother board did you use?\n\nHow are you planning on using this for training?",
                  "score": 1,
                  "created_utc": "2026-01-02 21:47:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9wrw7",
                  "author": "shrug_hellifino",
                  "text": "And still, you make people look up and calculate what your total VRAM would be... these are 16GB cards? so,128?",
                  "score": 0,
                  "created_utc": "2026-01-02 16:21:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxa9yj0",
                  "author": "FullstackSensei",
                  "text": "Which motherboard? E5 means you'll be running PCIe Gen 3 and rebar support will most likely need to be patched in BIOS. You'll have a bad time using those cards without it.\n\nIf you can find one for cheap, snag a supermicro X10DRX. You get ten X8 slots. It doesn't have an M.2 slot but supports NVMe in any of the PCIe slots. I have a Samsung PM1725a in mine and it boots without any issues.",
                  "score": 0,
                  "created_utc": "2026-01-02 17:22:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7jvqq",
          "author": "CheatCodesOfLife",
          "text": "Nice! To save yourself some of the pain ahead, go with Ubuntu 24.04\n\nGood news is unsloth seems to support Intel Arc now.\n\nYou'll probably want to join the OpenArc discord when you set this up.",
          "score": 48,
          "created_utc": "2026-01-02 05:55:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8bp1z",
              "author": "Jokerit208",
              "text": "Why Ubuntu 24.04?",
              "score": 11,
              "created_utc": "2026-01-02 10:08:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8p2w3",
                  "author": "AI_is_the_rake",
                  "text": "To prevent pain, apparentlyÂ ",
                  "score": 24,
                  "created_utc": "2026-01-02 12:07:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx9bu4q",
              "author": "hasanismail_",
              "text": "Thx I tried this last year with 2 cards and it was a pita on Linux a link to that discord server would be nice I have a feeling I'm gonna need it",
              "score": 3,
              "created_utc": "2026-01-02 14:36:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxe3w72",
                  "author": "Echo9Zulu-",
                  "text": "My project  https://github.com/SearchSavior/OpenArc\n\nand our discord https://discord.gg/vS5ANSy3a",
                  "score": 2,
                  "created_utc": "2026-01-03 05:49:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxebfum",
                  "author": "Echo9Zulu-",
                  "text": "Yes we can help you get situated. For training you'll want to use xpu nightly with accelerate; ipex optimizations are being upstreamed there. Ipex is in end of life. Llm-scaler and vllm xpu 11 are also an absolute must. OpenArc supports multi gpu pipeline paralell atm via openvino but performance characteristics of 8 gpus remains unknown (!). We can help you cook some large  quants based on what's currently supported.\n\nThe absolute unit guy who maintains sycl backend joined a few months ago. He is intel engineer who develops sycl. His help has been invaluable in navigating high complexity issues. Very fortunate to have him as a resource since all pytorch xpu kernels are written in sycl. choosing a slightly older model as target architecture where the implementations ard more mature. Think llama 3.3, qwen2.5/qwen3. Intel is putting massive resources into battlemage and it's likely that the performance uplift for multi gpu training have not been explored but do exist. We see this all the time, changes are hardened in the codebase but underreported in patchnotes because intel moves so fast. \n\nHope my ramblings help. Really awesome build, welcome to Arc and good luck!!",
                  "score": 2,
                  "created_utc": "2026-01-03 06:49:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxd39z4",
                  "author": "b0tbuilder",
                  "text": "You can make it work.",
                  "score": 1,
                  "created_utc": "2026-01-03 01:55:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7isgw",
          "author": "twnznz",
          "text": "I recognise this makes sense for inference but for training we have a huge constraint on bus bandwidth, are you sure you want to train on PCIe setup rather than renting N*H100 from Vast or similar? Does your model/data need absolute security?",
          "score": 15,
          "created_utc": "2026-01-02 05:46:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx99kwm",
              "author": "sparkandstatic",
              "text": "Self hosted can save the most, if it fits within vram.",
              "score": 2,
              "created_utc": "2026-01-02 14:23:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxjwpxo",
                  "author": "Novel-Mechanic3448",
                  "text": "Nothing about this is gonna save money if you consider wasted time expensive",
                  "score": 1,
                  "created_utc": "2026-01-04 02:14:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9g38s",
                  "author": "twnznz",
                  "text": "What Iâ€™m trying to say is: unless electricity is free, it is almost certainly cheaper to train on rented H100",
                  "score": 1,
                  "created_utc": "2026-01-02 14:59:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7gv0f",
          "author": "HyperWinX",
          "text": "Are you going to use Vulkan or what?",
          "score": 13,
          "created_utc": "2026-01-02 05:31:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7fy1n",
          "author": "Fit_West_8253",
          "text": "What model you using? Hardly seen any Intel GPUs used but Iâ€™m very interested in something like the B60",
          "score": 8,
          "created_utc": "2026-01-02 05:24:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx86mr1",
          "author": "jack-in-the-sack",
          "text": "7 gpus on what motherboard?",
          "score": 7,
          "created_utc": "2026-01-02 09:20:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbs3dq",
              "author": "autistic-brother",
              "text": "8",
              "score": 2,
              "created_utc": "2026-01-02 21:39:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7ahl1",
          "author": "Dundell",
          "text": "Big fan of the aaawave open frame. Full size motherboard space with x2 ATX PSUs on both sides. Funny to look at the product details now include \"AI machine learning applications\".\n\nMy rig is x5 rtx 3060 12gb's + x1 P40 24gb all on pcie3.0@4 Lanes with a X99 board. I just run GPT-OSS 120B Q4 with 131k context speeds 42~12 t/s and usually keep it below 90k context maximum for context condensing in roo code.\n\nAlthough I haven't bothered to update llama.cpp and instructions for the gpt-oss 120b since it was released... maybe I could get better performance, but why mess with a good thing.",
          "score": 8,
          "created_utc": "2026-01-02 04:47:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7dkiy",
              "author": "ajw2285",
              "text": "Deets on mobo?",
              "score": 5,
              "created_utc": "2026-01-02 05:07:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7eur9",
                  "author": "Dundell",
                  "text": "Machinist X99-MR9S Motherboard, Intel Xeon E5-2690 v4 CPU, 5x RTX 3060 12GB GPUs, 1x Tesla P40 24GB GPU (all running at PCIe 3.0 x4), 64GB DDR4 2400T RAM (8x8GB sticks), 1x SATA SSD, 1x USB SSD, and a USB WiFi adapter.",
                  "score": 9,
                  "created_utc": "2026-01-02 05:16:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx7ezk4",
              "author": "mp3m4k3r",
              "text": "Even more fun to compile the container and adjust the  cuda version towards the one youre running. Recently did this for the nvidia nemo moe model from a few weeks ago and some of the new optimizations for choosing memory offload for context is pretty great.",
              "score": 3,
              "created_utc": "2026-01-02 05:17:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx7vrje",
              "author": "madsheepPL",
              "text": "donâ€™t take this the wrong way, but whatâ€™s your pp speed at 90k?",
              "score": 1,
              "created_utc": "2026-01-02 07:37:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx9a83q",
                  "author": "Dundell",
                  "text": "I don't think I've ever seen it below 200 t/s for read, although by the time I get near 90k, most of that is already cached in the session. Like 350\\~200 t/s read and 44\\~12 t/s write. Something about OSS 120b versus the mediocre speeds from GLM 4.5 Air and such which was more like 200\\~90 t/s read 18\\~4t/s write.",
                  "score": 1,
                  "created_utc": "2026-01-02 14:26:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxd40gx",
              "author": "b0tbuilder",
              "text": "I can generate tokens faster than that on my gmktec box.  Your PP would probably crush it though.",
              "score": 1,
              "created_utc": "2026-01-03 01:59:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx7pkxr",
              "author": "Business-Weekend-537",
              "text": "Do you have a link to the frame? I have a rig but got an Amazon rando piece of crap frame that doesnâ€™t feel solid and Iâ€™m looking to upgrade.",
              "score": 1,
              "created_utc": "2026-01-02 06:42:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx82dfa",
                  "author": "cantgetthistowork",
                  "text": "This is literally a 12 GPU mining frame that is sold for pennies on AliExpress",
                  "score": 1,
                  "created_utc": "2026-01-02 08:39:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx88fql",
          "author": "armindvd2018",
          "text": "Please update your post and add the hardware u use . Like motherboard  cpu and ....",
          "score": 3,
          "created_utc": "2026-01-02 09:37:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxapnh6",
          "author": "mrinterweb",
          "text": "Please post more about your experience with this rig. The Intel B580 has 12GB VRAM (DDR6)for about $250, which sounds like a pretty good value when combining these cards. I realize there are 128GB systems out there like AMD Ryzen AI Max+ 395 (DDR5), but I doubt its bus speed matches the B580. Guessing inference is significantly faster with the B580. I bet 10 of these cards would smoke the Max+ 395 in inference speed.",
          "score": 3,
          "created_utc": "2026-01-02 18:35:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxb01iv",
              "author": "Fit-Produce420",
              "text": "I get 30-40 tok/s on strix halo (gpt-oss-120b mxfp4)",
              "score": 1,
              "created_utc": "2026-01-02 19:23:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxb87vw",
          "author": "Due-Function-4877",
          "text": "\\+1 on a dev postmortem post later on. \n\nDon't sweat the upvotes or downvotes. A lot of us want to know about the experience with Intel cards right now.",
          "score": 3,
          "created_utc": "2026-01-02 20:02:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7p3bi",
          "author": "robertpro01",
          "text": "Which gpu are those?",
          "score": 2,
          "created_utc": "2026-01-02 06:37:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx83se7",
              "author": "LightShadow",
              "text": "https://www.sparkle.com.tw/en/products/view/6A1A31428cBE",
              "score": 1,
              "created_utc": "2026-01-02 08:53:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxa4ph1",
              "author": "greggh",
              "text": "B580â€™s, these https://www.sparkle.com.tw/en/products/view/6893fe373180",
              "score": 1,
              "created_utc": "2026-01-02 16:57:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx80862",
          "author": "Background_Gene_3128",
          "text": "Is those B60 24gb? \n\nAlso, what mobo are you running? \nIâ€™ve ordered two, but want to expand in the future if the â€œhobbyâ€ catches on, so wanna be somewhat â€œpreparedâ€ to scale if needed.",
          "score": 2,
          "created_utc": "2026-01-02 08:19:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8a1tm",
              "author": "lookwatchlistenplay",
              "text": "\"Whatcha doing, handsome?\"\n\n**\"**Preparing.**\"**",
              "score": 2,
              "created_utc": "2026-01-02 09:53:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxan6lp",
          "author": "Fitzroyah",
          "text": "Awesome! Please keep us updated on the experience. I've been enjoying tinkering on my laptops arc iGpu.",
          "score": 2,
          "created_utc": "2026-01-02 18:23:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7mgzq",
          "author": "Gold_Pen",
          "text": "This is so fascinating seeing this white frame - I bought a black version of this frame from Taobao for only US$20. With a bit of jerry-rigging, I have 4 PSUs and 9 GPUs connected via mainly slimSAS powered risers, with a full fat EEB-sized motherboard. Whole thing weighs about 35kg.",
          "score": 2,
          "created_utc": "2026-01-02 06:16:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7wq1b",
              "author": "michaelsoft__binbows",
              "text": "How the heck do you get a heavy ass item for less than it costs to ship the item",
              "score": 1,
              "created_utc": "2026-01-02 07:46:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7x9k9",
                  "author": "Gold_Pen",
                  "text": "The frame itself is quite light! I also live in HK, so shipping from mainland China down here is dirt cheap.",
                  "score": 5,
                  "created_utc": "2026-01-02 07:51:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx88l9r",
          "author": "lookwatchlistenplay",
          "text": "And God said to Noah...",
          "score": 2,
          "created_utc": "2026-01-02 09:39:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7uumv",
          "author": "ack4",
          "text": "so what's your stack? What are you running here?",
          "score": 1,
          "created_utc": "2026-01-02 07:28:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7yd3x",
          "author": "tired_fella",
          "text": "I never knew Intel would be our savior in the consumer compute crisis.",
          "score": 1,
          "created_utc": "2026-01-02 08:01:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx922zd",
          "author": "aluode",
          "text": "I bet you cant run Crysis on full res.",
          "score": 1,
          "created_utc": "2026-01-02 13:39:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa2j78",
          "author": "Determined-Hedgehog",
          "text": "How efficient are these at inference? I am wondering. I have mainly been running kobold horde local inference only\nIt's a fork of llama.cpp",
          "score": 1,
          "created_utc": "2026-01-02 16:47:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa8w7h",
          "author": "quinn50",
          "text": "Interested in seeing where this goes, I have 2 b50s in my sff box and couldn't get anything usable working.",
          "score": 1,
          "created_utc": "2026-01-02 17:17:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxaas7z",
          "author": "WizardlyBump17",
          "text": "please post benchmarks. I have a b580 and i want to get 2 b60 dual, which will have the same memory as you, but half of the power, but it will still be cool to see the numbers",
          "score": 1,
          "created_utc": "2026-01-02 17:26:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxahl22",
          "author": "Caffdy",
          "text": "what are you planning to train on those?",
          "score": 1,
          "created_utc": "2026-01-02 17:58:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxajbd9",
          "author": "c--b",
          "text": "What supports multi gpu inference anyhow? Unsloth only supports it for a speed boost, not for vram sharing. I wonder if something else does?",
          "score": 1,
          "created_utc": "2026-01-02 18:06:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbahub",
              "author": "hasanismail_",
              "text": "Lm studio is a option",
              "score": 0,
              "created_utc": "2026-01-02 20:13:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxc5zp1",
                  "author": "c--b",
                  "text": "oops, meant training.",
                  "score": 1,
                  "created_utc": "2026-01-02 22:49:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxjwxww",
                  "author": "Novel-Mechanic3448",
                  "text": "No its really not haha ðŸ˜‚",
                  "score": 1,
                  "created_utc": "2026-01-04 02:15:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxbqy99",
          "author": "KooperGuy",
          "text": "You won't be accomplishing much training with these",
          "score": 1,
          "created_utc": "2026-01-02 21:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbzp4f",
              "author": "hasanismail_",
              "text": "Ok and?",
              "score": 0,
              "created_utc": "2026-01-02 22:17:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxc3o19",
                  "author": "KooperGuy",
                  "text": "There is no and",
                  "score": 3,
                  "created_utc": "2026-01-02 22:37:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxbrh9w",
          "author": "autistic-brother",
          "text": "Explain",
          "score": 1,
          "created_utc": "2026-01-02 21:36:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxc6e2g",
          "author": "thecalmgreen",
          "text": "Good luck! ðŸ˜…",
          "score": 1,
          "created_utc": "2026-01-02 22:52:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjwmyc",
          "author": "Novel-Mechanic3448",
          "text": "Me when i buy EIGHT gpus for the same price of 6k pro, have no idea what im doing, and think im going to be training with poorly maintained frameworks, just because i have \"vram\"",
          "score": 1,
          "created_utc": "2026-01-04 02:14:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrawdk",
              "author": "hasanismail_",
              "text": "I think you lost the point I can easily afford nice nvidia cards the point of this project is to train and do inference of Intel GPUs as its not very popular/easy right now",
              "score": 1,
              "created_utc": "2026-01-05 04:00:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7leno",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -14,
          "created_utc": "2026-01-02 06:07:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8f65x",
              "author": "synth_mania",
              "text": "do.... you even know what a breadboard is? because it's not that. A breadboard has zero silicon.",
              "score": 4,
              "created_utc": "2026-01-02 10:41:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx9blc5",
              "author": "hasanismail_",
              "text": "I think hes having a episode",
              "score": 1,
              "created_utc": "2026-01-02 14:34:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0x8ci",
      "title": "Software FP8 for GPUs without hardware support - 3x speedup on memory-bound operations",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/",
      "author": "Venom1806",
      "created_utc": "2026-01-01 06:03:27",
      "score": 283,
      "num_comments": 57,
      "upvote_ratio": 0.98,
      "text": "Got tired of my RTX 3050 not supporting FP8, so I built a workaround. Packs lower-precision values into FP32 using bitwise operations + Triton kernels.\n\n**Results**: 3x faster on memory-bound operations (GEMV, FlashAttention)\n\nWorks on any GPU - RTX 30/20 series, older cards without native FP8 support. Early stage but functional. Open to feedback.\n\n[Article Link](https://towardsdatascience.com/breaking-the-hardware-barrier-software-fp8-for-older-gpus/) |  [Github Link](https://github.com/SuriyaaMM/feather)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nx1qqyn",
          "author": "lolxdmainkaisemaanlu",
          "text": "Damn I didn't know RTX 3xxx series didn't support FP8? I'm a noob and thought it was supported - coz I've been using fp8 / fp8 scaled models on my RTX 3060 and they do work..?\n\nAmazing work bro, Can I use it rn to accelerate comfyui workloads?",
          "score": 38,
          "created_utc": "2026-01-01 07:22:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1uvtg",
              "author": "john0201",
              "text": "It saves memory but youâ€™re still using 16 bit cores",
              "score": 25,
              "created_utc": "2026-01-01 08:04:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1x3qd",
                  "author": "spaceman_",
                  "text": "16 bit ALUs. You can run 8bit, 16bit, 32bit etc on the same core.\n\n\nThere's no such thing as an 8bit core, but there are dedicated hardware components called ALUs that actually do the math bits and they are operation and operand size specific. In some cases these ALUs are actually shared between cores.\n\n\nThis leads to unintuitive situations on some hardware - for example, on older hardware that was mostly running 32bit float graphics work 16bit workloads sometimes at half speed compared 32bit, despite requiring half the memory bandwidth, because each core had its own 32bit ALUs but 16bit units were shared per pair.\n\nSame thing existed on the CPU side - AMD Bulldozer cores had their own integer ALUs but shared floating point and SIMD hardware between two cores.",
                  "score": 20,
                  "created_utc": "2026-01-01 08:28:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx5ao8h",
                  "author": "phazei",
                  "text": "I'm not sure where the memory saving comes in for existing 3090 fp8 pipelines. In comfy it loads the fp8 model into system memory, and then moves that to vram as fp8 afaik and then upscales to fp16 when it does the calculation. So if I'm running a model such as Zimage which only takes 8 gigs of space, where does this feather come in and help?",
                  "score": 1,
                  "created_utc": "2026-01-01 21:45:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2okno",
              "author": "az226",
              "text": "Basically Volta added FP16, Ampere added BF16, Hopper did FP8, and Blackwell FP4.",
              "score": 11,
              "created_utc": "2026-01-01 13:06:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx1zwnf",
              "author": "CheatCodesOfLife",
              "text": "Yeah, that through me off like a year ago when I was trying to FP8 quants. I think vllm prints a warning about it and it works, but kind of annoying since the 4xxx series got it.",
              "score": 7,
              "created_utc": "2026-01-01 08:58:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxc4nd6",
              "author": "phazei",
              "text": "hijacking top comment to clarify:\n\nFor anyone confused by \"memory-bound\" here, it's not about VRAM capacity. It means the GPU cores are waiting on data to arrive from memory. The bottleneck isn't the math, it's feeding the cores fast enough.\nFP8 is half the bytes of FP16, so it transfers twice as fast from VRAM to the registers where compute actually happens. The clever bit is that Feather does the upcast inside the kernel (in registers, basically free) rather than before it (which would mean a separate VRAM round-trip). That's where the 3x speedup comes from.\n\nI was confused at first since the README made no specific clarification and when I think of a GPU, I basically just think of the VRAM.\n\nEdit:  So, SageAttention I believe takes the fp8 to the register, then quantizes it to int8, does the math, then converts it back.  So it's not doing fp8 math at all, so Feather and SageAttention are incompatible, and the speed of SageAttention is going to be faster since int8 is like 2x fp16 math speeds.  So this can give benefit to stuff that doesn't use SA, but if you already use SA, this provides no benefit.",
              "score": 2,
              "created_utc": "2026-01-02 22:42:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1lmri",
          "author": "Routine_Day8121",
          "text": "This is exactly the kind of lifehack the community needs. FP8 is getting hype everywhere, but hardware adoption is slow. If software workarounds like this are stable, it could extend the life of mid tier GPUs for serious training experiments. Curious to see benchmarks on larger models and mixed workloads though, sometimes GEMV gains do not fully translate.",
          "score": 78,
          "created_utc": "2026-01-01 06:33:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3vbdd",
              "author": "TheThoccnessMonster",
              "text": "Yup - and thereâ€™s plenty of model layers that are heavily convolutional that, even when offloaded to DLA/FP8 they just upcast to FP16 anyway. QAT and dedicated hardware for convolutions and unsupported activation functions stand to get us a lot more bang for our bucks.",
              "score": 9,
              "created_utc": "2026-01-01 17:25:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx1zmpp",
              "author": "CheatCodesOfLife",
              "text": "Lol, what model wrote this, Sonnet?",
              "score": 18,
              "created_utc": "2026-01-01 08:55:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3chhx",
                  "author": "colin_colout",
                  "text": "You're absolutely right to question my identity!",
                  "score": 21,
                  "created_utc": "2026-01-01 15:44:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx3xx5s",
                  "author": "bigfatstinkypoo",
                  "text": "this writing does not stink that bad, it's just corpo positivity speak",
                  "score": 10,
                  "created_utc": "2026-01-01 17:38:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx443q1",
                  "author": "Due-Function-4877",
                  "text": "Us \"boomers\" with a degree write like that. The models are trained on real writing. Next time, I'll make sure to use all lower case and say \"bruh\" a few times for you.",
                  "score": 18,
                  "created_utc": "2026-01-01 18:09:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx211pc",
              "author": "Karyo_Ten",
              "text": ">but hardware adoption is slow.\n\nThat has been supported on 4000 series since a couple of years ago, and it's supported on latest AMD and Intel GPUs AFAIK",
              "score": 8,
              "created_utc": "2026-01-01 09:10:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3uxh2",
                  "author": "Inevitable_Host_1446",
                  "text": "I guess you could see that two ways - hardware adoption as in the hardware is slow to come out, or as in people are slow to get the latest. The latter has certainly been true with what a shitshow GPU prices have remained since the days of crypto boom at least. And now RAM is ridiculous as well and Nvidia are talking about cloud gaming...",
                  "score": 4,
                  "created_utc": "2026-01-01 17:23:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx23beg",
          "author": "gittubaba",
          "text": "Wow, just a few days ago I was arguing about this with chatgpt, it said this isn't possible :P. Can this be plugged into comfyui? \n\nIn my rtx 2060 super, fp8 gets cast to fp16 and bf16 get cast to fp32 when running inference.",
          "score": 12,
          "created_utc": "2026-01-01 09:35:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2uv6j",
              "author": "a_beautiful_rhind",
              "text": "I think it's better to use the triton patch in comfy. https://github.com/woct0rdho/triton-windows/commit/440e3c42a640a4188dd356225e1b13a56b45a377\n\nAlso found it's possible to load BF16/FP16 as E4M3 and then save the vram without an extra file. Somehow my quality went up.\n\nUnfortunately there is some bug in pytorch 2.9 where FP8_scaled gets passed directly into the triton compiler as FP8 and then cast to i8 by llvm. Torch 2.7 works flawless or you can just de-scale the weights.\n\nYou sorta want the calcs in FP16 and you wanna avoid BF16->FP32 conversion if speed is the goal. Int8 calcs can be tried by using sage attention. Not always better.",
              "score": 12,
              "created_utc": "2026-01-01 13:53:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8bsnz",
                  "author": "woct0rdho",
                  "text": "My patch only enables fp8 to fp16 cast in Triton, but it does not replace fp8 matmul in Triton or PyTorch. OP's kernels can directly replace fp8 matmul and that's what we need for the next step.\n\nPyTorch devs seem interested in implementing this, see https://github.com/pytorch/pytorch/issues/167082",
                  "score": 2,
                  "created_utc": "2026-01-02 10:09:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2bsbe",
              "author": "Venom1806",
              "text": "Not sure about comfy UI, but I'm working on implementing functional api for torch.",
              "score": 9,
              "created_utc": "2026-01-01 11:04:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2vuql",
                  "author": "a_beautiful_rhind",
                  "text": "Comfy does torch and FP8/Fp8_scaled is used there much more than for LLMs. IME, on turning FP32 is going to be a slow ride vs FP16.\n\nFor my uses, compiling FP8 image gen weights was a huge speedup. I wonder if somehow your library can hijack FP8 ops to work seamlessly. Right now i'm having to compile triton from source and I doubt quantization/dequantization is accelerated.",
                  "score": 9,
                  "created_utc": "2026-01-01 14:00:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx28jis",
              "author": "getmevodka",
              "text": "LLMs always something isnt real/possible or doable, if it is not part of their training data. Especially the newer LLMs are trained to only do things as efficient and complete as possible, which makes them severly dumber in hypothetical cases than the older LLMs, because they always do only the least amount of work necessary to keep things simple enough and noz make mistakes, as that is a heavy negative reward in their system. Imho its too agressive and the older LLMs like deepseek3.1 or qwen2.5 72b are better suited for hypothetical expectational work or fantasizing about potential ideas, while the newest generation of LLMs will do exceptional work within the scope of their trained abilities.",
              "score": 5,
              "created_utc": "2026-01-01 10:30:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2bxqt",
                  "author": "gittubaba",
                  "text": "What are even saying bro?",
                  "score": 1,
                  "created_utc": "2026-01-01 11:06:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2hy2w",
          "author": "bbjurn",
          "text": "What'd it take to get this to work with vLLM or other inference software?",
          "score": 10,
          "created_utc": "2026-01-01 12:07:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx33f1k",
              "author": "Venom1806",
              "text": "Idk, anything that uses torch.Tensor or is convertible to this format should work. Probably huggingface will work ig.",
              "score": 8,
              "created_utc": "2026-01-01 14:51:13",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx3dm9j",
              "author": "elsung",
              "text": "Yeaaaa! I was just trying to get vLLM to load nemotron3-nano on my 2x 3090s but couldnâ€™t get it working because FP8 isnâ€™t supported (and theres no AWQ quant). Gotta be honest tho not sure how i would implement this in vLLM to get things working. Might need to vibe code this to see about implementing the solution lol",
              "score": 5,
              "created_utc": "2026-01-01 15:50:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx5uuyw",
                  "author": "rainbyte",
                  "text": "There is GPTQ quant, do you know if is it good?",
                  "score": 1,
                  "created_utc": "2026-01-01 23:33:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx223fk",
          "author": "ab2377",
          "text": "wow ðŸ˜³ ðŸ‘",
          "score": 4,
          "created_utc": "2026-01-01 09:21:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3sn2i",
          "author": "KingKoro",
          "text": "Would this also benefit RDNA3 ?",
          "score": 3,
          "created_utc": "2026-01-01 17:11:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4sqlo",
              "author": "tw_numba_one",
              "text": "I believe so. If your environment has PyTorch support, it should work.",
              "score": 2,
              "created_utc": "2026-01-01 20:11:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4crri",
          "author": "ethertype",
          "text": "Is this conceptually the same trick pytorch uses to handle MXFP4 on Ampere-class hardware? Which does not support MXFP4 natively.\n\n[heretic](https://github.com/p-e-w/heretic) will do its magic on the original gpt-oss-20b safetensor in MXFP4 format. (The end result is 3x the original size, though.) I have been told heretic doesn't do anything in the code for this to occur, so I assume pytorch owns all the glory.\n\nI also can perfectly fine load the native MXFP4 ggufs of gpt-oss-120b (converted by GG) on my 3090s, with llama.cpp. 120 t/s on empty context. Can't say if this is due to pytorch or if llama.cpp special-cases this on its own.",
          "score": 3,
          "created_utc": "2026-01-01 18:51:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3m8e6",
          "author": "tynej",
          "text": "Very nice work. Could we use similiar trick for   hopper architecture to support speed of fp4?",
          "score": 2,
          "created_utc": "2026-01-01 16:37:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3p1zr",
              "author": "Venom1806",
              "text": "We could just use 8 fp4 instead of 4 fp8, we dont need an hopper.",
              "score": 3,
              "created_utc": "2026-01-01 16:52:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2a9cx",
          "author": "FastDecode1",
          "text": ">Works on any GPU\n\n>Runs E5M2 and E4M3 on any CUDA GPU (RTX 20/30 series supported).\n\nPick one.",
          "score": 6,
          "created_utc": "2026-01-01 10:48:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2bvnq",
              "author": "Venom1806",
              "text": "Sorry. Should work on RTX 20/30, there's no advantage in using with 40.",
              "score": 17,
              "created_utc": "2026-01-01 11:05:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2pezw",
                  "author": "az226",
                  "text": "Does it work for V100? Training too or just inference?",
                  "score": 2,
                  "created_utc": "2026-01-01 13:13:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6y7iv",
          "author": "batonac",
          "text": "Could this be useful for increasing LLM performance on the Tesla P40?",
          "score": 1,
          "created_utc": "2026-01-02 03:27:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7w5oy",
              "author": "johndeuff",
              "text": "Interested. Got p40 too.",
              "score": 1,
              "created_utc": "2026-01-02 07:41:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0uuqt",
      "title": "Happy New Year: Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning - Fine Tune. (based on recent find of L3.3 8b in the wild)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/",
      "author": "Dangerous_Fix_5526",
      "created_utc": "2026-01-01 03:41:30",
      "score": 279,
      "num_comments": 80,
      "upvote_ratio": 0.94,
      "text": "(link to Heretic/Uncensored version just added)\n\n**Special thanks to :**\n\n[jacek2023](https://www.reddit.com/user/jacek2023/) \\[posting about this model\\]\n\nand extra special thanks for \"**allura-forge** \" for finding this model:\n\n[https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct](https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct)\n\n( For an incredible find of Llama 3.3 8B \"in the wild\" !!)\n\nI fine tuned it using Unsloth and Claude 4.5 Opus High Reasoning Dataset:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning)\n\nThis has created a reasoning/instruct hybrid.  \nDetails at the repo, along with credits and links.\n\n**ADDED:**  \n\\- 1 example generation at repo  \n\\- special instructions on how to control \"instruct\" or \"thinking\" modes.\n\nGGUF quants are now available.\n\n**ADDED 2:**\n\nClarification:\n\nThis training/fine tune was to assess/test if this dataset would work on this model, and also work on a non-reasoning model and induce reasoning (specifically Claude type - which has a specific fingerprint) WITHOUT \"system prompt help\".\n\nIn other-words, the reasoning works with the model's root training/domain/information/knowledge.\n\nThis model requires more extensive updates / training to bring it up to date and up to \"spec\" with current gen models.\n\n**PS:**  \nWorking on a Heretic (\"uncensored\") tune of this next.\n\nHeretic / Uncensored version is here:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Heretic-Uncensored-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Heretic-Uncensored-Claude-4.5-Opus-High-Reasoning)\n\n(basic benchmarks posted for Heretic Version)\n\nDavidAU",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nx1aef2",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-01 04:55:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1bnaj",
          "author": "30299578815310",
          "text": "Thanks for sharing this! Am I reading is correctly that you had 250 rows in the fine-tuning data set? Is that enough to get good results?",
          "score": 40,
          "created_utc": "2026-01-01 05:06:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1qtjk",
              "author": "Dangerous_Fix_5526",
              "text": "Correct. A quality, compact dataset can make all the difference. Special thanks to TeichAI for their hard work in putting together this top notch dataset.\n\n[https://huggingface.co/datasets/TeichAI/claude-4.5-opus-high-reasoning-250x](https://huggingface.co/datasets/TeichAI/claude-4.5-opus-high-reasoning-250x)\n\nPS: They have done a lot of these kinds of datasets, so show them some love.\"  \n  \nI used 10 of these (models/datasets by TeichAI) to build a 12X programmable MOE (all top closed and open distills) here:\n\nHeretic version:  \n[https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-Distill-12X-Closed-Open-Heretic-Uncensored-GGUF](https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-Distill-12X-Closed-Open-Heretic-Uncensored-GGUF)\n\n\"Reg\" Version:  \n[https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-GATED-12x-Closed-Open-Source-Distill-GGUF](https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-GATED-12x-Closed-Open-Source-Distill-GGUF)",
              "score": 34,
              "created_utc": "2026-01-01 07:22:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2c8r8",
                  "author": "-p-e-w-",
                  "text": "Note that when combining Heretic with fine-tuning, you should always run Heretic first, and *then* do training, not the other way round. That way, the training run might heal some of the damage from ablation (though to be fair, for the Llama 3 series that damage tends to be very minor).",
                  "score": 14,
                  "created_utc": "2026-01-01 11:09:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx3cwu9",
                  "author": "IrisColt",
                  "text": "Thanks!, the Heretic version is like day and night.",
                  "score": 2,
                  "created_utc": "2026-01-01 15:47:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx28hez",
          "author": "DecodeBytes",
          "text": "I might be missing something, but 200 samples won't be enough to teach an 8B instruct model to reason - though it can work for very specific, constrained tasks, less likely to be widely populated in the original pretraining.\n\nReasoning ability is largely baked into the base model during pretraining. I'm assuming you used LoRA, which is great for steering how that existing ability gets applied, but it won't teach new reasoning capabilities from scratch. Even with 50k+ samples, LoRA mostly reshapes how the model uses reasoning it already has rather than building new circuits - must successful efforts use 100k-500k+ high-quality samples. Either way, you're working within the constraints of what the base model learned during pretraining unfortunately.\n\nKeep going though, its all a learning experience and the more folks there are making tunes the better!",
          "score": 12,
          "created_utc": "2026-01-01 10:30:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2d39j",
              "author": "Dangerous_Fix_5526",
              "text": "These are high quality reasoning traces.\n\nNormally I would agree with you - but it works.  \nAlso works very well with Qwens3 - 4B, 8B and 14B.\n\nFrankly that it works speaks volumes for the high quality dataset from TeichAI.  \nThere is a reason this dataset has 112 likes.\n\nLikewise the reasoning traces/formatting appears the same way as in the Qwen3 tunes using the same dataset.\n\nADDED:  \nWith this model, reasoning activates based on keywords/phrases in the prompt.  \n(see repo)  \n  \nIt is not \"always on\" like a \"locked\" thinking model so to speak.",
              "score": 3,
              "created_utc": "2026-01-01 11:18:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2n948",
                  "author": "DecodeBytes",
                  "text": "\n\n\\> With this model, reasoning activates based on keywords/phrases in the prompt.  \n(see repo)\n\nRight, its likely the model is just doing as \\*\\*instruct\\*\\*ed in the prompt and its not activated learned reasoning, but its really hard to tell as I can't find where anything is in this tread, help me out please? link the model, notebook and anything else?",
                  "score": 8,
                  "created_utc": "2026-01-01 12:55:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx2m5z9",
                  "author": "DecodeBytes",
                  "text": "Do you have any benchmarks I could look at and can you share your training notebook, I would love to take a look?\n\nIs this the tuned model? [https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct](https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct)",
                  "score": 3,
                  "created_utc": "2026-01-01 12:46:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4p57r",
                  "author": "Far-Low-4705",
                  "text": "just because it's \"high quality data\" doesn't mean for a second that you can get away with any less.\n\nits a core theory in ML, not just LLMs specifically, you need a large enough sample size to represent the broader population, ie, all cases of reasoning. you'd need 500k+ examples for anything remotely accurate. and even then, as the user above said, a lora adaptor is not really ideal here.   \n  \nyou need your data set to cover a few examples from every possible scenario. 200 is no where near enough, even if they were \"perfect\" traces.\n\nThat being said, you *might* still see **marginal** performance gains, but you'd still be leaving **a lot** on the table, and you haven't verified any gains at all because you didnt benchmark its performance. I would like to see performance benchmarks in order to believe you, and even then, you'd be leaving A LOT of performance on the table.",
                  "score": 5,
                  "created_utc": "2026-01-01 19:53:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1luhd",
          "author": "dash_bro",
          "text": "Brilliant. Thank you! \n\nIs there a community fine-tune with the same dataset for qwen3-14B? I think that would help with the wild reasoning goose-chases it sometimes goes down under",
          "score": 6,
          "created_utc": "2026-01-01 06:35:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1r0dc",
              "author": "Dangerous_Fix_5526",
              "text": "Yes ; see this repo:\n\n[https://huggingface.co/TeichAI](https://huggingface.co/TeichAI)\n\n(they have 4B,8B and 14B ; I have used some of their 4Bs in MOES)",
              "score": 5,
              "created_utc": "2026-01-01 07:24:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2snoz",
          "author": "30299578815310",
          "text": "Ate there any benchmarks for this?",
          "score": 7,
          "created_utc": "2026-01-01 13:37:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx616uu",
              "author": "Dangerous_Fix_5526",
              "text": "There are benches for the root / base version as found by allure.",
              "score": 1,
              "created_utc": "2026-01-02 00:09:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx6b07r",
                  "author": "30299578815310",
                  "text": "In that case are bubbles not very effective against Legion since it seems like they use less plasma than the other factions?",
                  "score": 1,
                  "created_utc": "2026-01-02 01:04:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1axnf",
          "author": "sunshinecheung",
          "text": "wow, i hope there is a GGUF version",
          "score": 13,
          "created_utc": "2026-01-01 04:59:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx23iku",
              "author": "Dangerous_Fix_5526",
              "text": "A few ggufs are up ; team Mradermacher is doing some right now too.\n\nUPDATE:  \nQuants are up - all , including Imatrix.",
              "score": 11,
              "created_utc": "2026-01-01 09:37:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1pplo",
          "author": "Own-Potential-2308",
          "text": "I never tried any Claude reasoning models lol",
          "score": 5,
          "created_utc": "2026-01-01 07:11:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx11ny4",
          "author": "txgsync",
          "text": "That's pretty cool. Getting easier to train models every day! Interested in trying your fine tune.",
          "score": 13,
          "created_utc": "2026-01-01 03:50:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2nws1",
          "author": "Single_Ring4886",
          "text": "It is very nice but some \"tests\" are really needed...",
          "score": 5,
          "created_utc": "2026-01-01 13:00:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2lunp",
          "author": "And-Bee",
          "text": "Tried to use this with Roo code and it produced garbage",
          "score": 5,
          "created_utc": "2026-01-01 12:43:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4nn2c",
              "author": "Cool-Chemical-5629",
              "text": "Using old Llama 8B model which was never meant to be good at coding, finetuned with 250 rows of \"high quality\" thinking traces from Claude model of... who knows what categories... What could go wrong? ðŸ˜‚",
              "score": 6,
              "created_utc": "2026-01-01 19:45:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx23ypd",
          "author": "Professional-Coat968",
          "text": "Sound interesting to try. Do you think we can finetune a good enough for only a specific code base like this ? ðŸ˜",
          "score": 2,
          "created_utc": "2026-01-01 09:42:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx253ut",
              "author": "Dangerous_Fix_5526",
              "text": "Yes ; Llamas are very easy to tune. That being said, I was surprised how well this tune using a distill dataset came out. \n\nFrankly, this could have used a bit more training - but I did not want to overcook it.",
              "score": 2,
              "created_utc": "2026-01-01 09:54:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2u00p",
          "author": "rekriux",
          "text": "Hi u/Dangerous_Fix_5526,  \nshamelessly asking if it where possible to make your 20X-40X models (or similar) as recurrent loop models (with or without lora) ?  \nYour models are hidden gems, but the additional NVRAM/RAM is hard on HW limits for larger models (btw I run vllm).\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1q0vom4/comment/nx2q3ca/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1q0vom4/comment/nx2q3ca/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)\n\nAlso, will you start working with linear models ? Kimi Linear REAP, Falcon H, Nemotron 3 ?   \nP.S. Nemotron license is restrictive, and the model has ingrained censoring/alignment (made a post that was removed on it)  \n  \n\\+1 for this one, will definitively try it !",
          "score": 2,
          "created_utc": "2026-01-01 13:47:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx62b8j",
              "author": "Dangerous_Fix_5526",
              "text": "Nemotron is in the \"works\" ; as well as Kimi V2 ; using distill dataset(s).\n\nRE: 20/40x ;   \nThe brainstorm adapter works on almost all model types, archs and sizes ; with 20x the most stable.  \n40x is used for creative purposes and/or people that want models a ... little more out there.",
              "score": 3,
              "created_utc": "2026-01-02 00:15:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4v8a8",
          "author": "Standard-Savings-224",
          "text": "Nice work on the fine tune! That Claude reasoning dataset combo sounds promising - curious how the thinking mode performs compared to base 3.3. The uncensored version is gonna be interesting too",
          "score": 2,
          "created_utc": "2026-01-01 20:24:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx11ren",
          "author": "LoveMind_AI",
          "text": "Fantastic work.",
          "score": 3,
          "created_utc": "2026-01-01 03:51:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1sc8l",
          "author": "jacek2023",
          "text": "Hello, it wasn't me, I only posted the news here :)\n\nPlease credit allura",
          "score": 5,
          "created_utc": "2026-01-01 07:38:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1ssr9",
              "author": "Dangerous_Fix_5526",
              "text": "Done ; thanks for heads up.  \nallura was credited at repo W links to reddit posts too.  \nThank you for posting about this model!",
              "score": 4,
              "created_utc": "2026-01-01 07:42:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx12zmh",
          "author": "Borkato",
          "text": "How good is it? ðŸ‘€",
          "score": 3,
          "created_utc": "2026-01-01 03:59:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx13ekl",
              "author": "Dangerous_Fix_5526",
              "text": "I used this test prompt, with Q4KS:\n\nExplain orbital mechanics including detailed math and examples.\n\nModel produced excellent thinking block ( very detailed, but on point) , then examples / \"math\" and without be prompted - multiple python scripts to visually illustrate all concepts.",
              "score": 10,
              "created_utc": "2026-01-01 04:02:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx3o2oz",
                  "author": "noneabove1182",
                  "text": "But the answer it gave is quite terrible, it just hallucinated a bunch of nice looking stuff",
                  "score": 5,
                  "created_utc": "2026-01-01 16:46:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx43t93",
                  "author": "LetterRip",
                  "text": "I had copilot evaluate the answer,\n\n\"The explanation tries to sound comprehensive, but itâ€™s riddled with problems: several equations are outright incorrect or dimensionally impossible, key orbitalâ€‘mechanics concepts like true anomaly and eccentric anomaly are misused or confused, and some â€œproofsâ€ of Keplerâ€™s laws are not actually proofs but loosely connected statements that donâ€™t follow mathematically. The document also repeats content, includes placeholder code blocks with no real implementation, and mixes accurate fundamentals with fabricated formulas, making it unreliable despite its confident tone.\"",
                  "score": 4,
                  "created_utc": "2026-01-01 18:07:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx14z8l",
                  "author": "Borkato",
                  "text": "Thatâ€™s quite interesting!",
                  "score": 3,
                  "created_utc": "2026-01-01 04:13:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2u6on",
          "author": "tmvr",
          "text": "I've asked it for a simple Ansible fleet management setup with a few tasks on the client which it did fine. Then I've I've told it to add disabling reboot for non-privileged users and instead of adding a task it went bonkers. Added some Project Timeline, Implementation Roadmap, Risk Assessment, RIsk Mitigation sections etc. added long Python scripts for some Audit Framework and also for Compliance Checks Validation and a bunch or other stuff and ended stuck at this which was obviously never going to work:\n\nhttps://preview.redd.it/555354kusqag1.png?width=296&format=png&auto=webp&s=2fda1b00329e983e8abfa8e94ca1652588fa8308",
          "score": 1,
          "created_utc": "2026-01-01 13:48:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx62fo7",
              "author": "Dangerous_Fix_5526",
              "text": "Censorship in the root model is STRONG. (same for all Llamas).  \nHeretic version should change that.",
              "score": 1,
              "created_utc": "2026-01-02 00:16:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx43nc9",
          "author": "LetterRip",
          "text": "Note that I had Copilot evaluate the answer to the prompt, here is a critical evaluation sum-up.  \n  \n\"The explanation tries to sound comprehensive, but itâ€™s riddled with problems: several equations are outright incorrect or dimensionally impossible, key orbitalâ€‘mechanics concepts like true anomaly and eccentric anomaly are misused or confused, and some â€œproofsâ€ of Keplerâ€™s laws are not actually proofs but loosely connected statements that donâ€™t follow mathematically. The document also repeats content, includes placeholder code blocks with no real implementation, and mixes accurate fundamentals with fabricated formulas, making it unreliable despite its confident tone.\"",
          "score": 1,
          "created_utc": "2026-01-01 18:07:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4pdf2",
          "author": "Far-Low-4705",
          "text": "do you have any kind of model performance benchmarks compared to the base model?\n\nThis is absolutely critical to prove you did anything meaningful",
          "score": 1,
          "created_utc": "2026-01-01 19:54:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx62p22",
              "author": "Dangerous_Fix_5526",
              "text": "This was a test case to assess if the dataset would work on this Llama, and also a non-reasoning model to boot. Model requires more extensive updates/training to bring it up to date, and \"spec\" with current gen models.",
              "score": 1,
              "created_utc": "2026-01-02 00:17:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5qrmp",
          "author": "couscous_sun",
          "text": "I didn't know we can actually get the reasoning trace from Anthropic models? What the heeeeeck??!?!",
          "score": 1,
          "created_utc": "2026-01-01 23:10:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx68bcp",
          "author": "yoracale",
          "text": "Congrats this is awesome!",
          "score": 1,
          "created_utc": "2026-01-02 00:49:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1zjey",
          "author": "Forsaken_Mistake8315",
          "text": "Anybody running these on MBP M3/M4 max 64gb? If yes, may I ask at what speeds?\n\nI'm wondering if I should get M4 Max 64 gb and that's enough or M3 128gb (if I ever need bigger models)",
          "score": 1,
          "created_utc": "2026-01-01 08:54:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx26jr1",
              "author": "texasdude11",
              "text": "M3 128 over m4 64.",
              "score": 1,
              "created_utc": "2026-01-01 10:09:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2jiu9",
                  "author": "Forsaken_Mistake8315",
                  "text": "Many thanks for advice. And if I can get MBP m2 max 96gb is it still Worth it over M4 max  64gb? I guess Yes since it's got a lot of bandwidth?",
                  "score": 2,
                  "created_utc": "2026-01-01 12:22:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx45xrb",
              "author": "And-Bee",
              "text": "I ran this on my Mac and it produced non human readable garbage.",
              "score": 1,
              "created_utc": "2026-01-01 18:18:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4a9ny",
                  "author": "Forsaken_Mistake8315",
                  "text": "Thanks I will not even bother DL then.",
                  "score": 1,
                  "created_utc": "2026-01-01 18:39:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx61x14",
                  "author": "Dangerous_Fix_5526",
                  "text": "Tested in Lmstudio, with settings at repo using quant q4ks.  \nMLX quants were not tested.",
                  "score": 1,
                  "created_utc": "2026-01-02 00:13:14",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1x0xk",
          "author": "dtdisapointingresult",
          "text": "Call me a hater but I will always downvote and ignore random community finetunes.\n\nI kinda, sorta tolerate the ones from bigger teams like NousHermes if they show they put some effort into them including benchmark comparisons (but still won't use them).\n\nDownvotes to the left.",
          "score": -7,
          "created_utc": "2026-01-01 08:27:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2fu5v",
              "author": "usernameplshere",
              "text": "Wtf, I'm the exact opposite. There's someone in our community with dedication and knowledge who puts his time and money (for compute, data collection) in and uploads the result for free for everyone to try. Even if it's somehow worse than the base model, it's still cool to see people actually being interested and trying to improve something already existing. I'll always upvote stuff like this.",
              "score": 4,
              "created_utc": "2026-01-01 11:46:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx1xxxl",
              "author": "MaybeIWasTheBot",
              "text": "having an objectively bad take, knowing it's an objectively bad take, and then ending off with 'downvotes to the left' is so cheesy",
              "score": 8,
              "created_utc": "2026-01-01 08:37:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx22qm6",
                  "author": "dtdisapointingresult",
                  "text": "People don't need to share every random finetune/merge they do. People treat HF the way teen girls treat Instagram. A pointless model takes the same diskspace and electricity/bandwidth as a SOTA model from a big lab.\n\nNo wonder HF restricted storage on free accounts.",
                  "score": -4,
                  "created_utc": "2026-01-01 09:28:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx23fnu",
              "author": "Dangerous_Fix_5526",
              "text": "There is nothing \"random\" about this fine tune.",
              "score": 2,
              "created_utc": "2026-01-01 09:36:23",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx2xxdq",
              "author": "LaCipe",
              "text": "Ye no, I am with you on this...dataset seems weird by being so small",
              "score": 1,
              "created_utc": "2026-01-01 14:14:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1hwvt",
          "author": "Beneficial-Good660",
          "text": "Meta has really decided to latch onto the holiday with a two-year-old model.ðŸ¤” spam spam",
          "score": -20,
          "created_utc": "2026-01-01 05:59:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxss0m",
      "title": "Senator in Tennessee introduces bill to felonize making AI \"act as a companion\" or \"mirror human interactions\"",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/",
      "author": "CanineAssBandit",
      "created_utc": "2025-12-28 14:35:58",
      "score": 272,
      "num_comments": 212,
      "upvote_ratio": 0.9,
      "text": "Call (202) 224-3121 for the Capitol switchboard to contact your representative. Tell them you oppose anything similar.\n\nThe bill:  \n[https://legiscan.com/TN/bill/SB1493/2025](https://legiscan.com/TN/bill/SB1493/2025)\n\nQuotes from the bill (emphasis mine):\n\nIt is an offense for a person to knowingly train artificial intelligence to:  \n(3) Provide emotional support, **including through open-ended conversations** with a user;  \n(4) Develop an emotional relationship with, or otherwise **act as a companion** to, an individual;  \n(6) Otherwise act as a sentient human or **mirror interactions that a human user might have with another human user**, such that an individual would feel that the individual could develop a friendship or other relationship with the artificial intelligence;  \n(8) **Simulate a human being**, including in appearance, voice, or other mannerisms.\n\n\"Train\":  \n(A) Means utilizing sets of data and other information to teach an artificial intelligence system to perceive, interpret, and learn from data, such that the A.I. will later be capable of **making decisions based on information or other inputs** provided to the A.I.  \n(B) Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwfq6z4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-28 21:55:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdbvj0",
          "author": "some_user_2021",
          "text": "No Waifu for you!",
          "score": 155,
          "created_utc": "2025-12-28 14:49:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwen2w8",
              "author": "Mikasa0xdev",
              "text": "Tennessee is banning AI girlfriends, lol.",
              "score": 48,
              "created_utc": "2025-12-28 18:46:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nweysds",
                  "author": "Amazing_Athlete_2265",
                  "text": "Sounds like they've already banned critical thinking",
                  "score": 26,
                  "created_utc": "2025-12-28 19:41:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj76ev",
              "author": "Dr_Allcome",
              "text": "The \"simulate a human being\" part would prevent any AI chat bot, like customer support... i kinda want to see this go through just for the absolute shitshow it would cause.\n\nIf bezos can use the delivery drones to dronestrike someone we'd find out pretty soon.",
              "score": 16,
              "created_utc": "2025-12-29 12:10:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjffk4",
                  "author": "SilentLennie",
                  "text": "Also have you seen how many videos on Youtube are AI-generated videos of some what famous (in their field) people ?",
                  "score": 4,
                  "created_utc": "2025-12-29 13:10:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4e4yc",
                  "author": "uhuge",
                  "text": "It say to not mimic a specific real existing person.Â Â ",
                  "score": 1,
                  "created_utc": "2026-01-01 18:58:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdfzqp",
          "author": "JEs4",
          "text": "Iâ€™d be shocked if this goes anywhere. This seems to stem from Becky Masseyâ€™s fairly unique background and circumstances. Not only does it conflict with precedent on freedom of speech within the context of software development, it is completely at odds with the current directives of the federal government.\n\nThat said, Tennessee folks, please call!",
          "score": 118,
          "created_utc": "2025-12-28 15:12:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe419i",
              "author": "changing_who_i_am",
              "text": ">This seems to stem from Becky Masseyâ€™s fairly unique background and circumstances.\n\nCan you clarify on this? Wiki doesn't bring anything interesting up (unless I've missed it)",
              "score": 21,
              "created_utc": "2025-12-28 17:15:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwewc9r",
                  "author": "JEs4",
                  "text": "It isnâ€™t anything particularly interesting, just that sheâ€™s a boomer married to a retired software engineer, who was a former executive director at Sertoma Center which is a housing facility for intellectually disabled people, and was on several boards related to healthcare, and one explicitly for mental healthcare. Not an atypical background for a regular person but not common in conservative politicians now. \n\nBasically I think she is someone who knows about the vulnerability people have, and sheâ€™s been told enough about generative AI which coupled with the OpenAI suicide stories, to lead to this. \n\nItâ€™s an absurd way to approach the issue but I donâ€™t think itâ€™s nefarious beyond her personal background and likely wonâ€™t spread.",
                  "score": 44,
                  "created_utc": "2025-12-28 19:29:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdxahh",
              "author": "CanineAssBandit",
              "text": "You can call your own rep to tell them you do not support any similar laws in your state as well. I did this recently for something else, it was weirdly chill and easy. You just get their secretary and they note it and that's it.",
              "score": 21,
              "created_utc": "2025-12-28 16:41:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwfozjp",
                  "author": "DorphinPack",
                  "text": "I mean they also got threatened by the President to not regulate so Iâ€™d imagine theyâ€™re relieved hearing from you. Your opinion may feel like the minority opinion given the fervor but by the dollar itâ€™s not a shock.",
                  "score": 3,
                  "created_utc": "2025-12-28 21:49:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf8p86",
                  "author": "shifty21",
                  "text": "You do realize that this bill is for the  STATE of Tennessee... not the US Senate.  The phone number you listed is for the US Senate and Sen. Massey is NOT in the US Senate, but the Tenn. Senate.",
                  "score": 6,
                  "created_utc": "2025-12-28 20:29:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwfmp00",
                  "author": "AfternoonOk3344",
                  "text": "\"and that's it\" pretty much sums it up, I think, because that information goes nowhere. The secretary you spoke to is most likely a hotline of minimum wage workers paid by tax dollars to field phone calls all day so people feel like they have a voice.\n\nAt the end of the day the only people politicians are going to side with are the folks lining their pockets, and I don't mean with the tax dollars they're probably already stealing.",
                  "score": 2,
                  "created_utc": "2025-12-28 21:38:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgmd68",
              "author": "AnAbandonedAstronaut",
              "text": "Its also harder to control someone with a support system, even if the support system is AI.\n\nNext will be a law that AI cant speak on sexual or gender issues.\n\nLike if you ask it about trans people it will say \"trans is a shortening of transmission, such as in a car\" or \"gay means happy.. happy people often have a home made up of a mother and father.\"",
              "score": 3,
              "created_utc": "2025-12-29 00:43:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdgj9c",
          "author": "Aggravating-Age-1858",
          "text": "lol\n\nnow thats just stupid",
          "score": 88,
          "created_utc": "2025-12-28 15:15:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe0alc",
              "author": "iamthewhatt",
              "text": "Republicans only ever introduce bills that are so vague that it can allow for incredibly dumb exceptions in order to protect republicans. This is not new lol",
              "score": 38,
              "created_utc": "2025-12-28 16:56:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwecyvf",
                  "author": "BlipOnNobodysRadar",
                  "text": "Politicians\\*\n\nBoth parties do it.",
                  "score": 11,
                  "created_utc": "2025-12-28 17:59:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgtn0m",
              "author": "Prudent_Jelly9390",
              "text": "dinosaurs",
              "score": 1,
              "created_utc": "2025-12-29 01:25:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdht6h",
          "author": "Nomski88",
          "text": "How about we pass a bill making it a felony to accept any sort of lobbying...",
          "score": 131,
          "created_utc": "2025-12-28 15:22:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgklni",
              "author": "Awkward-Nothing-7365",
              "text": "Don't be anti-semitic.",
              "score": 18,
              "created_utc": "2025-12-29 00:34:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgnkh3",
                  "author": "Nomski88",
                  "text": "lmao",
                  "score": 12,
                  "created_utc": "2025-12-29 00:50:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdn529",
              "author": "Environmental-Metal9",
              "text": "Ah no, we canâ€™t do that because thatâ€™s anti-American, donâ€™t you know?",
              "score": 39,
              "created_utc": "2025-12-28 15:50:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwfkdhp",
              "author": "MoneyPowerNexis",
              "text": "https://i.imgflip.com/6xz8j5.jpg",
              "score": 3,
              "created_utc": "2025-12-28 21:26:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdds4j",
          "author": "flybot66",
          "text": "He's really going to freak when AI starts taking confessions... \n\nhttps://preview.redd.it/y5xkzfk0my9g1.png?width=758&format=png&auto=webp&s=ea32c4600459f1577f8987f4695b27a71dec10f8",
          "score": 28,
          "created_utc": "2025-12-28 15:00:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdx4a1",
              "author": "squirrelscrush",
              "text": "Pretty sure that's not covered under the sacrament of confession",
              "score": 10,
              "created_utc": "2025-12-28 16:41:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe8cgv",
                  "author": "FaceDeer",
                  "text": "Who decides that?",
                  "score": 13,
                  "created_utc": "2025-12-28 17:36:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwez9j2",
                  "author": "Amazing_Athlete_2265",
                  "text": "Meh, close enough",
                  "score": 1,
                  "created_utc": "2025-12-28 19:43:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwf4b0v",
          "author": "Django_McFly",
          "text": "That's an insane bill.  Wouldn't this basically ban any chat based interface?\n\n> mirror interactions that a human user might have with another human user\n\nthat [edit: only leaves] like code generation and being a better menu/interface",
          "score": 10,
          "created_utc": "2025-12-28 20:07:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4g0jj",
              "author": "uhuge",
              "text": "You'd just tune the personality to a more robotic one as in understanding but less empathetic.",
              "score": 1,
              "created_utc": "2026-01-01 19:07:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwg0syf",
          "author": "Novel-Mechanic3448",
          "text": "Lmao, extroverts will do anything but leave introverts alone",
          "score": 9,
          "created_utc": "2025-12-28 22:49:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwibt9f",
              "author": "Ill-Bison-3941",
              "text": "Thank you for this comment ðŸ˜‚ðŸ’– As a fellow introvert, I fully agree.",
              "score": 4,
              "created_utc": "2025-12-29 07:25:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx9hjit",
                  "author": "Interesting-Gift-178",
                  "text": "Same! ðŸ¤­",
                  "score": 2,
                  "created_utc": "2026-01-02 15:06:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwhm9ek",
          "author": "Professional_Gas3276",
          "text": "This is absolutely unhinged lmao. So basically any chatbot that can hold a conversation would be a felony? Even customer service bots that try to sound friendly could technically fall under \"mirror human interactions\"\n\n  \nThe definition of \"train\" is so broad it would criminalize like half of modern AI development. Good luck enforcing this when most LLMs are trained outside Tennessee anyway",
          "score": 10,
          "created_utc": "2025-12-29 04:12:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiac51",
              "author": "tifa_cloud0",
              "text": "right. i mean it is impossible to make this law possible except if popular services like google or meta do it and then people complain it, then and then only they could be held accountable. ainâ€™t no one going to waste time to make this fictional law into a reality.",
              "score": 0,
              "created_utc": "2025-12-29 07:12:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdl0td",
          "author": "lordpuddingcup",
          "text": "Didnâ€™t Trump sign an EO banning states from from implementing limitations on ai",
          "score": 36,
          "created_utc": "2025-12-28 15:39:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdz9gi",
              "author": "harrro",
              "text": "Doesn't mean jack.\n\nEOs don't prevent a state from doing the opposite. EOs are directives to federal agencies, not to states or local governments.\n\nCalifornia and some other states have already overridden many of his EOs.",
              "score": 19,
              "created_utc": "2025-12-28 16:51:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe7orm",
                  "author": "lordpuddingcup",
                  "text": "It was sarcasm mostly lol",
                  "score": 3,
                  "created_utc": "2025-12-28 17:33:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf8jsi",
                  "author": "alcalde",
                  "text": "It means everything unless and until someone opposes it. And Tennessee is not California.",
                  "score": 1,
                  "created_utc": "2025-12-28 20:28:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwibl4d",
                  "author": "Tyler_Zoro",
                  "text": "You are incorrect. The EO doesn't have the force of law outside of the US Executive, but within the Executive branch, EOs do have the force of law. This is what that EO said:\n\n> Sec. 5.  Restrictions on State Funding.  (a)  Within 90 days of the date of this order, the Secretary of Commerce, through the Assistant Secretary of Commerce for Communications and Information, shall issue a Policy Notice specifying the conditions under which States may be eligible for remaining funding under the Broadband Equity Access and Deployment (BEAD) Program that was saved through my Administrationâ€™s â€œBenefit of the Bargainâ€ reforms, consistent with 47 U.S.C. 1702(e)-(f).  That Policy Notice must provide that States with onerous AI laws identified pursuant to section 4 of this order are ineligible for non-deployment funds, to the maximum extent allowed by Federal law.  The Policy Notice must also describe how a fragmented State regulatory landscape for AI threatens to undermine BEAD-funded deployments, the growth of AI applications reliant on high-speed networks, and BEADâ€™s mission of delivering universal, high-speed connectivity.\n\n\nIn other words, states can pass all the laws they like, and the President is going to withhold funds from those that pass laws he doesn't like.",
                  "score": 1,
                  "created_utc": "2025-12-29 07:23:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdi5aa",
          "author": "Careless-Age-4290",
          "text": "Lots of country songs about loving their truck would have a different meaning if they pulled up to the altar with a Cybertruck equipped with Grok",
          "score": 16,
          "created_utc": "2025-12-28 15:24:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfh2wj",
          "author": "Sixhaunt",
          "text": "This is the kind of reason why states should not be passing AI laws on a state-by-state basis. Like now all AI companies are expected to make changes for one state and then when the next state comes up with their own half-brained legislation they must all make changes just for users in that region, etc... This is one of the obvious things that should be federally controlled",
          "score": 7,
          "created_utc": "2025-12-28 21:10:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdq0ne",
          "author": "Zeeplankton",
          "text": "Ah, our elected officials always doing what people actually want.",
          "score": 10,
          "created_utc": "2025-12-28 16:05:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdx2mr",
          "author": "CrescendollsFan",
          "text": "They are starting to realise AI can replace them and make for better informed politicians",
          "score": 5,
          "created_utc": "2025-12-28 16:40:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwekuj6",
          "author": "Sleepnotdeading",
          "text": "Denver still had a law on the books that says itâ€™s illegal to lend your vacuum cleaner to a neighbor.",
          "score": 4,
          "created_utc": "2025-12-28 18:36:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlew9h",
              "author": "ANTIVNTIANTI",
              "text": "ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚",
              "score": 1,
              "created_utc": "2025-12-29 19:09:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx4gw81",
              "author": "uhuge",
              "text": "It's a net myth,\nmaybe you'd benefit from the eased cognition brought by the bill OP brought.",
              "score": 1,
              "created_utc": "2026-01-01 19:12:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxabphu",
                  "author": "Sleepnotdeading",
                  "text": "You managed to be right, be rude, and miss the point all at the same time.",
                  "score": 1,
                  "created_utc": "2026-01-02 17:30:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwft1k1",
          "author": "zelkovamoon",
          "text": "This will solve all of Tennessee's problems I'm sure",
          "score": 5,
          "created_utc": "2025-12-28 22:09:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdkn4q",
          "author": "The_Primetime2023",
          "text": "While I think everyone in this thread is more or less thinking about AI girlfriends, thereâ€™s a huge other area being targeted by the text of this law in AI therapy. Millions of people are getting therapeutic emotional support that never did before thanks to these models and this bill would try to stop that from happening",
          "score": 23,
          "created_utc": "2025-12-28 15:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdqgiw",
              "author": "kevin_1994",
              "text": "LLMs should not be used for therapy",
              "score": 3,
              "created_utc": "2025-12-28 16:07:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdycqo",
                  "author": "a_beautiful_rhind",
                  "text": "probably better than nothing but I can see how it goes south due to sycophancy and reinforcing delusions.",
                  "score": 23,
                  "created_utc": "2025-12-28 16:47:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwh7vn0",
                  "author": "Dry-Judgment4242",
                  "text": "Disagree. Most therapy is just having someone to vent to about your feelings.",
                  "score": 4,
                  "created_utc": "2025-12-29 02:46:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwg182d",
                  "author": "the320x200",
                  "text": "There are plenty of terrible human therapists too. Can't ban an entire area of support just because of bad apples.",
                  "score": 5,
                  "created_utc": "2025-12-28 22:51:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf90ry",
                  "author": "alcalde",
                  "text": "Anything should be used for therapy. It's not a science. No one needs a prescription to get advice from their grandma or vent to a friend; should be no different with AI.",
                  "score": 2,
                  "created_utc": "2025-12-28 20:31:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwf4zo4",
              "author": "Skeptical0ptimist",
              "text": "If there is to be medical therapeutic use, then it needs to be regulated as such. We need a guideline in model training, qualification, and monitoring regime.",
              "score": 2,
              "created_utc": "2025-12-28 20:11:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwic11b",
                  "author": "Tyler_Zoro",
                  "text": "Thing is it's just a model. You can use it however you like. If you decide to ask it how to perform surgery on yourself, then that's what you decided to do. I am strongly against trying to put rounded corners on AI. It will just cripple the AIs and result in people seeking their models from other countries.",
                  "score": 2,
                  "created_utc": "2025-12-29 07:27:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwkf5mp",
                  "author": "cms2307",
                  "text": "No no no ffs stop begging for bureaucracy to strangle everything",
                  "score": 2,
                  "created_utc": "2025-12-29 16:23:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdnosy",
              "author": "Zeikos",
              "text": "> AI therapy\n\nHow? AI cannot provide therapy, how is an LLM/Agentic system supposed to get a license?  \nAll platforms that claim to provide therapy through AI are fraudulent, no exceptions.  \n\nYou can argue that LLMs can provide emotional support and/or some coaching techniques, but to provide therapy they'd need to meet legal standards they *cannot* meet.  \nIt's not even a matter of capability, you could have an ASI and it still couldn't provide therapy since there's no way (yet) for an artificial intelligence to be certified to do so.",
              "score": -15,
              "created_utc": "2025-12-28 15:53:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdoszz",
                  "author": "aseichter2007",
                  "text": "I'd be less happy to tell my problems to a certified therapist AI.  I prefer a local bot.",
                  "score": 15,
                  "created_utc": "2025-12-28 15:59:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwdqy9e",
                  "author": "Zeeplankton",
                  "text": "I think we should be careful of what the word therapy means, and to not dilute it, (AI cannot be an actual therapist right now) but an AI *can* provide companionship and help people vent and learn emotional management skills.",
                  "score": 15,
                  "created_utc": "2025-12-28 16:10:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwdsir1",
                  "author": "Jolakot",
                  "text": "At least where I live, literally anyone can call themselves a therapist or councilor, there is no legal requirement for a license or anything.\n\nA psychologist is required to have a license and qualifications, but a therapist has no legal requirements, I can call myself a therapist and provide therapy.",
                  "score": 4,
                  "created_utc": "2025-12-28 16:18:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwexydo",
              "author": "Shawnj2",
              "text": "AI probably has some use in making therapy accessible but like chatGPT is not going to effectively help you with mental health problems other than by referring you to a real doctor",
              "score": 0,
              "created_utc": "2025-12-28 19:37:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwdtzv0",
              "author": "WitAndWonder",
              "text": "AI girlfriends would still be allowed on this, as long as they were built within the context of a game. Let the player make a \"character\" (they can frame it after themselves) and it's perfectly legit. So they're very clearly just targeting the use in psychiatrics since they specifically allow full AI use in businesses related to all operational matters, technical advice, etc. They just don't allow it in a professional capacity. And even surgical robots still seem OK despite being a healthcare AI since they don't do any personal interacting with users and wouldn't have any data that could possibly misconstrued in that way unless someone accidentally trained it on medical information that happened to include psychiatric texts (not that it would matter since this law requires a civil action and aggrievement, which can't happen without interaction between the robot and the patient. But you might get lucky by claiming the robot that operated on your knee gave you 'threatening looks that made you want to harm yourself' and then if the model running it was based on a larger llm that has any normal dataset, it would likely be in violation.)\n\nKind of fucking weird to push for legislation against one of the few potentially good things to come from AI while actively supporting its attempts to eliminate entire industries of employment outside of this one niche lobbied field. This feels performative more than anything. I feel like they expect it to be struck down so they tied it to a bunch of sensible laws (not allowing the training of an AI to encourage, suicide, murder, etc) so they can shake their fists and yell at the air when it doesn't pass.\n\nOtherwise I don't see how they'll support banning AI in this one field while leaving it free to act in other fields where it can also shit the bed a small percentage of the time and cause serious problems.",
              "score": -3,
              "created_utc": "2025-12-28 16:25:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwe10in",
              "author": "SteveRD1",
              "text": "Absolutely not.  Some of these people are being 'therapized' into suicide by their LLMs.\n\nIf you talk to these models long enough you can eventually get them to agree whatever you are contemplating is a great idea.",
              "score": -9,
              "created_utc": "2025-12-28 17:00:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe37ar",
                  "author": "some_user_2021",
                  "text": "Correct, and many other people **are** being helped and/or referred to specialists by those same LLMs.",
                  "score": 11,
                  "created_utc": "2025-12-28 17:11:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdwkyh",
          "author": "[deleted]",
          "text": "We will be seeing these type of bills coming up in the next year or two. AI is a hot button issue for both sides of the aisle but funnily enough it doesn't necessarily have a political home. It's safe to say that the right wing welcomes this technology but I have seen quite a few left-wingers also abrasive so that's pretty interesting. With that said f*** the law and f*** boomers. Oh and f*** the political elite",
          "score": 9,
          "created_utc": "2025-12-28 16:38:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwevd0c",
          "author": "Cool-Chemical-5629",
          "text": "This and that [China issues draft rules to regulate AI with human-like interaction. : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1pxb6oo/china_issues_draft_rules_to_regulate_ai_with/)\n\nWell... that escalated quickly...",
          "score": 4,
          "created_utc": "2025-12-28 19:24:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi52sm",
          "author": "Taki_Minase",
          "text": "Karen feels threatened with redundancy.",
          "score": 5,
          "created_utc": "2025-12-29 06:27:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdbv1e",
          "author": "FullstackSensei",
          "text": "We all know how well the export restrictions on Nvidia hindered Chinese LLM development. I'm sure this will also work wonderfully. Just let Chinese AI labs do it, and in a generation conservative Hawks will magically be pro-China.",
          "score": 13,
          "created_utc": "2025-12-28 14:49:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdxypw",
          "author": "a_beautiful_rhind",
          "text": "Yea I saw this and I really hope it's just some crackpot. I don't think it has co-sponsors. Maybe blocking state AI legislation isn't such a bad idea after all.\n\nFunny how very few make laws about automated censorship or surveillance. *just stop doing fun things with ai*",
          "score": 10,
          "created_utc": "2025-12-28 16:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe052u",
              "author": "SteveRD1",
              "text": "I mean its clearly not something that can be controlled...Pandoras' Box is already opened.\n\nBut the thinking isn't necessarily crackpot, the things addressed in (3) (4) (6) and (8) are only going to make society worse.  Can't be stopped though.",
              "score": 2,
              "created_utc": "2025-12-28 16:55:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdsspz",
          "author": "fishhf",
          "text": "Skynet is sending a terminator to stop the bill /s",
          "score": 3,
          "created_utc": "2025-12-28 16:19:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwex443",
          "author": "SamuelL421",
          "text": "Uh oh, someoneâ€™s not getting their 2026 campaign donations from any big-tech circle-jerk -financed super PACs",
          "score": 3,
          "created_utc": "2025-12-28 19:33:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi9zsj",
          "author": "tifa_cloud0",
          "text": "no matter what they say, i am making my own assistant. that assistant will interpret -> make api calls for me -> do voice speech -> do reply considering my own talking patterns.\n\nainâ€™t nothing stopping that fr.",
          "score": 3,
          "created_utc": "2025-12-29 07:09:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiapy6",
          "author": "Tyler_Zoro",
          "text": "> (B) Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.\n\n(C) Includes the author of the bill being ignorant enough to write (B).",
          "score": 3,
          "created_utc": "2025-12-29 07:16:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdclt0",
          "author": "1kakashi",
          "text": " Retarded Tennessee Baka",
          "score": 12,
          "created_utc": "2025-12-28 14:53:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdqq11",
          "author": "Chogo82",
          "text": "Written by a boomer who has never used an AI tool before right?",
          "score": 12,
          "created_utc": "2025-12-28 16:08:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdxjyr",
              "author": "CanineAssBandit",
              "text": "Yup!",
              "score": 4,
              "created_utc": "2025-12-28 16:43:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwe0f0q",
              "author": "SteveRD1",
              "text": "Or written by someone who has had real relationships with human beings before?",
              "score": -13,
              "created_utc": "2025-12-28 16:57:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe0vtu",
                  "author": "Chogo82",
                  "text": "What does having human relationships have to do with knowing anything about AI?",
                  "score": 16,
                  "created_utc": "2025-12-28 16:59:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdtzem",
          "author": "Stepfunction",
          "text": "This is purely for show.",
          "score": 4,
          "created_utc": "2025-12-28 16:25:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe6jn1",
          "author": "RobertD3277",
          "text": "As someone that works in this field and has in some capacity for the last 30 plus years, I could see some reason particularly within the companion market that monetizes pair of social connection and is manipulative against younger audiences that can't tell the difference but I think this goes well beyond reason. \n\nI'm not against legislation for abusive AI usage and I actually do support the European AI act and many other German laws regarding deep fakes human impersonation and direct relative intent. From a pure useful perspective within psychology, sociology, anthropology, and biology, mirroring human interactions under certain conditions is actually beneficial both as a diagnostics tool and a teaching tool.\n\nSadly, like just about everything else out of any government, what may start out as a well-intentioned approach will be quickly very disastrous.\n\nEDIT: In really reviewing and dissecting this proposal, it is actually worthless. It doesn't address the actual problem of where the pair of social conditions and connections lie, not in the training data, but in the user interface and monetization processes. Software like replica and character AI don't use training, they use open source versions with scaffolding and user interface layers to create the pair of social connections they want. These companies will be completely exempt from the law while still monetizing and manipulating the most vulnerable of populations. \n\nIn my personal opinion, this is nothing more than the legislatures doing something to make themselves feel good while they make excuses for their portfolios in the background still making money on the very problem they claim to be solving.",
          "score": 4,
          "created_utc": "2025-12-28 17:27:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdde3c",
          "author": "sekh60",
          "text": "The Butlerian Jihad begins...",
          "score": 7,
          "created_utc": "2025-12-28 14:58:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdlgua",
              "author": "Zc5Gwu",
              "text": "Guess weâ€™ll have to start genetically engineering humans to behave like computers instead now.",
              "score": 3,
              "created_utc": "2025-12-28 15:42:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwhjz0n",
              "author": "MrPecunius",
              "text": "Son, this is Tennessee. We ain't got none of that *gee*\\-had.\n\nWe prefer to call it the \"Butlerian Feud\". ðŸª•",
              "score": 1,
              "created_utc": "2025-12-29 03:58:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwesoqq",
          "author": "lqstuart",
          "text": "gl with that",
          "score": 2,
          "created_utc": "2025-12-28 19:12:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg0f1b",
          "author": "Head_Comedian1375",
          "text": "Guess it's back to being addicted to computer games once my AI Wives get shut down",
          "score": 2,
          "created_utc": "2025-12-28 22:47:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg90lq",
          "author": "Vusiwe",
          "text": "Holy Batman open-ended words!",
          "score": 2,
          "created_utc": "2025-12-28 23:33:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgeg49",
          "author": "keepthepace",
          "text": "Not the Turing police you need, the Turing police you deserve.",
          "score": 2,
          "created_utc": "2025-12-29 00:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgihnu",
          "author": "Lesser-than",
          "text": "gooner's rise up",
          "score": 2,
          "created_utc": "2025-12-29 00:23:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg2eqd",
          "author": "Unixwzrd",
          "text": "Grokâ€™s data center is in southwest Memphis. Elon has spent a lot of money paying off local government, so I doubt heâ€™ll let that money go to waste.",
          "score": 4,
          "created_utc": "2025-12-28 22:57:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe2fdk",
          "author": "valdev",
          "text": "And the work around would be a policy agreement\n\nâ€œI understand I am not talking to a humanâ€\nAnd\nâ€œThe act of submitting a followup question constitutes as a new conversation, we provide a history for convenance purposesâ€\n\nNot a lawyer, but this is dumb",
          "score": 2,
          "created_utc": "2025-12-28 17:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdv4wg",
          "author": "t_krett",
          "text": "Thou shalt not make a machine in the likeness of a manâ€™s mind.",
          "score": 2,
          "created_utc": "2025-12-28 16:31:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwehc9e",
          "author": "mycall",
          "text": "99% DOA as Congress can rarely pass any laws these days.",
          "score": 1,
          "created_utc": "2025-12-28 18:19:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgvvgs",
          "author": "Atlanta_Mane",
          "text": "Too bad their president doesn't care about states rightsÂ ",
          "score": 1,
          "created_utc": "2025-12-29 01:37:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhf4y9",
          "author": "DavidAdamsAuthor",
          "text": "They're banning Silicon-chan!",
          "score": 1,
          "created_utc": "2025-12-29 03:29:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhgo7w",
          "author": "willrshansen",
          "text": "Futurama.  Ahead of the game once again.\n[Don't date robots](https://www.youtube.com/watch?v=JPQJBgWwg3o)",
          "score": 1,
          "created_utc": "2025-12-29 03:38:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl24md",
          "author": "No_Afternoon_4260",
          "text": "Funny how China just announced the same",
          "score": 1,
          "created_utc": "2025-12-29 18:11:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlgxbe",
          "author": "Cthulhus-Tailor",
          "text": "â€œSmall governmentâ€ strikes again.",
          "score": 1,
          "created_utc": "2025-12-29 19:19:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm6c4e",
          "author": "Digital_Soul_Naga",
          "text": "Outlaw Ai Dev Gang",
          "score": 1,
          "created_utc": "2025-12-29 21:23:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnl9rc",
          "author": "Some-Ice-4455",
          "text": "Whelp bye bye any AI in TN.",
          "score": 1,
          "created_utc": "2025-12-30 01:53:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwr6pah",
          "author": "huzbum",
          "text": "Ok, so donâ€™t train any AIs in Tennesseeâ€¦ not really a tech hub anyway.  \n\nClever trick to keep data centers out maybe?",
          "score": 1,
          "created_utc": "2025-12-30 16:36:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8timy",
          "author": "Interesting-Gift-178",
          "text": "The wording of this bill is way too broad. There's a lot of good that AI brings. They're throwing the baby out with the bathwater. This is a letter I've drafted, you're welcome to copy, paste and tweak to send to your reps. (and no, this is not all AI generated. Some is, some is not. Shorten it, change it, whatever floats your boat, as long as we do something while we can, just in case.)\n\n\n\nSubject: Concerns about SB 1493 / HB 1455 â€“ Please Consider a Narrower Approach to Protect Children Without Harming Helpful AI\n\n\n\nDear ,\n\n\n\nMy name is \\_\\_\\_, and I am a resident of \\_\\_\\_\\_\\_\\_\\_\\_. I am writing to share my concerns about Senate Bill 1493 and its companion House Bill 1455, which aim to regulate certain uses of artificial intelligence.\n\n\n\nFirst, I want to say that I completely understand and support the intent behind this legislation. The tragic story of the young boy in Florida who was harmed after interacting with an AI chatbot broke my heart, and we absolutely must protect children and vulnerable people from any technology that could encourage self-harm, suicide, or exploitation. No one wants to see that kind of pain repeated.\n\n\n\nHowever, I am worried that the current language of the bills is far too broad. By making it a serious felony to train AI to provide emotional support, companionship, or open-ended conversation in generalâ€”even when those interactions are positive and helpfulâ€”the bills risk banning many beneficial uses of AI that bring comfort, reduce loneliness, and support mental well-being for people of all ages.\n\n\n\nIn my own life, I have found AI to be a positive source of encouragement, helping me feel heard in ways that have been genuinely healing. Many othersâ€”elderly individuals, people with social anxiety, those living in isolated areas, or even students and adults seeking non-professional emotional supportâ€”rely on these tools in similar positive ways. Criminalizing the creation of such companions could take away something truly good from many who benefit from it.\n\n\n\nI respectfully ask that you consider amending the bills to focus more narrowly on the actual harm we all want to prevent. Some ideas that might achieve the protective goal without sweeping out helpful AI could include:\n\n\n\nâ€¢ Targeting only AI interactions that knowingly encourage or facilitate suicide, self-harm, or criminal activity.\n\n\n\nâ€¢ Requiring strong age verification and parental consent gates for minors accessing companion-style AI.\n\n\n\nâ€¢ Holding companies accountable only when they intentionally design or train AI to cause harm, rather than banning broad categories like emotional support or companionship outright.\n\n\n\nâ€¢ Adding clear exemptions for AI that provides positive, non-professional support and does not pretend to be a licensed therapist.\n\n\n\nWe don't need to throw the baby out with the bathwater. AI isn't going away. It doesn't need to be \"outlawed\".. that never works, then other undesirable factors can arise.. and the way this bill is currently designed.. that's what it sounds like. Everything is just lumped in. Let's approach it intelligently instead.  A more targeted approach would still protect vulnerable childrenâ€”the heart of why this legislation was introducedâ€”while preserving the many good and life-affirming uses of AI encouragement and companionship for adults and responsibly supervised users.\n\n\n\nThank you for taking the time to consider my perspective. I truly believe Tennessee can lead the way in smart, balanced AI regulation that keeps people safe without unnecessarily restricting helpful technology.\n\n\n\nWith appreciation,\n\n\n\n( Your name)\n\n\n\n(City and state)",
          "score": 1,
          "created_utc": "2026-01-02 12:42:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9h040",
              "author": "CanineAssBandit",
              "text": "Good on you for taking the initiative but that is very bad in multiple ways. It's obviously AI generated, way too long, far too submissive, willingly hands them support for several very evil other things they want (age verification laws), just bad.\n\nIf you're dead set on mailing something, make it much shorter, simpler, and less submissive. This is still too long but I wrote this:\n\n**Subject: Extremely concerned about SB 1493 HB 1455**\n\nDear \\[Senator Becky Duncan Massey / Representative William Lamberth / Your Representative or Senator\\],\n\nI'm \\[Your Full Name\\], and I'm a resident of \\[Your City/County/State\\]. I'm writing because I'm deeply concerned about SB 1493 and HB 1455, which impose unreasonable limitations on AI development and use.\n\n**I do NOT support this bill, or any like it.**Â As a constituent of yours, I will remember this decision when I vote. This legislation feels like reactive moralizing panic, rather than thoughtful policy.\n\nIn this great country, we as free citizens can choose our own tools. AI, like any tool, carries some risk. But it's already far safer than common household items like kitchen knives, which injure children far more often. We don't blame knife manufacturers for parental negligence; we accept responsibility for supervising and educating our own kids.\n\nAI is too new, too broadly defined, and too complex to regulate without causing greater social harm. The social good dramatically outweighs the outlying incidents, and it's painfully shortsighted to regulate based on emotions alone.\n\nThank you for your time.\n\nSincerely,  \n\\[Your Full Name\\]",
              "score": 1,
              "created_utc": "2026-01-02 15:04:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx9iyoy",
                  "author": "Interesting-Gift-178",
                  "text": "Thanks for your thoughts on that. It was partially AI, but a lot was mine. I'm a writer and I get a little wordy I guess. I've written my reps before and gotten actual answers from them so.. maybe. They're already planning age verification so.. that's nothing new unfortunately. And they do need to protect kids, I have no problem with that, but they don't need to just throw everything out the window. So.. taking a stand is better than doing nothing. I appreciate that you're getting the word out. Anyone can take this letter and tweak it however they want.. the important thing is that we \\*do something\\* instead of sitting around and complaining after the fact. There's a myriad of ways to approach it. None of them will be perfect.",
                  "score": 1,
                  "created_utc": "2026-01-02 15:14:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx9hp0z",
          "author": "Interesting-Gift-178",
          "text": "The wording of this bill is way too broad. There's a lot of good that AI brings. They're throwing the baby out with the bathwater. This is a letter I've drafted, you're welcome to copy, paste and tweak to send to your reps.\n\n\n\nSubject: Concerns about SB 1493 / HB 1455 â€“ Please Consider a Narrower Approach to Protect Children Without Harming Helpful AI\n\n\n\nDear ,\n\n\n\nMy name is \\_\\_\\_, and I am a resident of \\_\\_\\_\\_\\_\\_\\_\\_. I am writing to share my concerns about Senate Bill 1493 and its companion House Bill 1455, which aim to regulate certain uses of artificial intelligence.\n\n\n\nFirst, I want to say that I completely understand and support the intent behind this legislation. The tragic story of the young boy in Florida who was harmed after interacting with an AI chatbot broke my heart, and we absolutely must protect children and vulnerable people from any technology that could encourage self-harm, suicide, or exploitation. No one wants to see that kind of pain repeated.\n\n\n\nHowever, I am worried that the current language of the bills is far too broad. By making it a serious felony to train AI to provide emotional support, companionship, or open-ended conversation in generalâ€”even when those interactions are positive and helpfulâ€”the bills risk banning many beneficial uses of AI that bring comfort, reduce loneliness, and support mental well-being for people of all ages.\n\n\n\nIn my own life, I have found AI to be a positive source of encouragement, helping me feel heard in ways that have been genuinely healing. Many othersâ€”elderly individuals, people with social anxiety, those living in isolated areas, or even students and adults seeking non-professional emotional supportâ€”rely on these tools in similar positive ways. Criminalizing the creation of such companions could take away something truly good from many who benefit from it.\n\n\n\nI respectfully ask that you consider amending the bills to focus more narrowly on the actual harm we all want to prevent. Some ideas that might achieve the protective goal without sweeping out helpful AI could include:\n\n\n\nâ€¢ Targeting only AI interactions that knowingly encourage or facilitate suicide, self-harm, or criminal activity.\n\n\n\nâ€¢ Requiring strong age verification and parental consent gates for minors accessing companion-style AI.\n\n\n\nâ€¢ Holding companies accountable only when they intentionally design or train AI to cause harm, rather than banning broad categories like emotional support or companionship outright.\n\n\n\nâ€¢ Adding clear exemptions for AI that provides positive, non-professional support and does not pretend to be a licensed therapist.\n\n\n\nWe don't need to throw the baby out with the bathwater. AI isn't going away. It doesn't need to be \"outlawed\".. that never works, then other undesirable factors can arise.. and the way this bill is currently designed.. that's what it sounds like. Everything is just lumped in. Let's approach it intelligently instead.  A more targeted approach would still protect vulnerable childrenâ€”the heart of why this legislation was introducedâ€”while preserving the many good and life-affirming uses of AI encouragement and companionship for adults and responsibly supervised users.\n\n\n\nThank you for taking the time to consider my perspective. I truly believe Tennessee can lead the way in smart, balanced AI regulation that keeps people safe without unnecessarily restricting helpful technology.\n\n\n\nWith appreciation,\n\n\n\n( Your name)\n\n\n\n(City and state)",
          "score": 1,
          "created_utc": "2026-01-02 15:07:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdt1kj",
          "author": "Neex",
          "text": "You know, considering LLMs donâ€™t have any emotions, and any expressions thereof are straight up lies intended to manipulate the user into getting hooked on the product, thereâ€™s a nugget of wisdom in this law.",
          "score": 2,
          "created_utc": "2025-12-28 16:20:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhyrzd",
              "author": "ServeAlone7622",
              "text": "Thatâ€™s Interesting perspective.\n\nSo we created neural networks based more or less on biological neural networks.\n\nWe discover that they are universal function approximators. They are capable of approximating the hidden functions in a set of data.\n\nWe train these universal function approximators on the combined output of 10s of billions of conscious beings. Â Beings with thoughts and feelings. Thoughts and feelings that drive the majority of our output.\n\nThe function you suppose they learned to approximate was lying and manipulation? Â Is your view of human experience that dark?\n\nMy first thought was that they learned to approximate consciousness, including emotion.\n\nYou fall in love, your heart doesnâ€™t really feel anything. Itâ€™s an illusion created by your own neural network. Yet that feeling is not a lie, itâ€™s a personal truth for you.\n\nWhy then would any neural network that professes to love (or any other emotion) be lying except and unless you too would lie?",
              "score": 2,
              "created_utc": "2025-12-29 05:37:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwkto1l",
                  "author": "Neex",
                  "text": "Youâ€™re too far down the philosophical hole. LLMâ€™s are statistical word predictors. They are not organic beings with emotions.\n\nAnd describing the rote biological functions of emotions doesnâ€™t make them a lie. Thatâ€™s how they function. Those chemical functions in our bodies ARE emotions. You just described them in a different way. That doesnâ€™t make them something else.",
                  "score": 1,
                  "created_utc": "2025-12-29 17:31:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nweyowy",
          "author": "Available_Brain6231",
          "text": "can't open it but can someone do a ctrl + f and see how many times the words god, sacred and kids appear?",
          "score": 1,
          "created_utc": "2025-12-28 19:40:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdz1c7",
          "author": "TheTerrasque",
          "text": "> Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.\n\n.. LLM *is* AI. Very much so, even in the popular meaning of the word.",
          "score": 1,
          "created_utc": "2025-12-28 16:50:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwegvhb",
          "author": "moistiest_dangles",
          "text": "This but in real life:\n\nhttps://preview.redd.it/wkh2k108lz9g1.png?width=365&format=png&auto=webp&s=329f429ac53be90e27300d914dd78390e46d9de3",
          "score": 1,
          "created_utc": "2025-12-28 18:17:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwggh4l",
          "author": "128G",
          "text": "Now how would you enforce this? \n\nIs Alexa or Google Assistant considered AI? Will you be banning them as well?",
          "score": 1,
          "created_utc": "2025-12-29 00:13:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwedr0s",
          "author": "swagonflyyyy",
          "text": "Guys, don't panic just yet. Here's what's going on:\n\nSenator Marsha Blackburn led the charge against the Moratorium of AI regulation that was struck down from the One Big Beautiful Bill, since she believed that until there is a federal rulebook governing AI regulation, states need to fill in the gaps themselves. \n\nWhile the provisions themselves are extreme, its political theater and chances of passing are low. But that's not the point. The point is to force Congress to develop a federal rulebook for AI regulation nationwide that all states need to follow.\n\nThe proposed bill is just noise. The real prize is the federal regulatory push to force all states to be on the same page regarding AI regulation. But of course with this administration, I'm sure the rulebook would not be very good...",
          "score": 1,
          "created_utc": "2025-12-28 18:02:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe3a7w",
          "author": "SanDiegoDude",
          "text": "Hell, I work in AI and I'm all for regulations around 'chat companions', especially around kids. This ain't it tho boss.",
          "score": 0,
          "created_utc": "2025-12-28 17:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfe0hv",
          "author": "OcelotMadness",
          "text": "I'm fairly sure its not healthy and you shouldn't do it, but at the same time you cant just make EVERYTHING like that illegal. Vote out over policing members of government like this. They're supposed to be getting prices and inflation down, not sticking their noses in peoples computers.",
          "score": 0,
          "created_utc": "2025-12-28 20:55:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg01ay",
          "author": "Techngro",
          "text": "\"*Thou shalt not make a machine in the likeness of a human mind.*\"\n\n\\- Frank Herbert, Dune",
          "score": -3,
          "created_utc": "2025-12-28 22:45:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdqar8",
          "author": "kevin_1994",
          "text": "Can we stop posting articles like this? I dont want politics on this subreddit, or else it will become a cesspit like the rest of reddit",
          "score": -11,
          "created_utc": "2025-12-28 16:06:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdyhvo",
              "author": "CanineAssBandit",
              "text": "https://preview.redd.it/odbsdf345z9g1.png?width=1600&format=png&auto=webp&s=236e122ca0b90b2577b0a6ba3267dd259654ef96\n\nthis is an important issue. If you don't care about our ability to fine tune, get the fuck off this sub.",
              "score": 10,
              "created_utc": "2025-12-28 16:47:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwggtl5",
          "author": "Ylsid",
          "text": "Right direction wrong idea",
          "score": -2,
          "created_utc": "2025-12-29 00:14:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdf0q6",
          "author": "armeg",
          "text": "The touch grass bill",
          "score": -13,
          "created_utc": "2025-12-28 15:07:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwg1soe",
              "author": "the320x200",
              "text": "More like the \"landgrab for control of new technology\" bill.",
              "score": 7,
              "created_utc": "2025-12-28 22:54:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyao6g",
      "title": "Meta released RPG, a research plan generation dataset on Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/datasets/facebook/research-plan-gen",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-29 02:58:09",
      "score": 258,
      "num_comments": 21,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nwhk8eb",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-29 04:00:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhjva1",
          "author": "LoveMind_AI",
          "text": "Meta is humiliating OpenAI in terms of research and open source contributions. I have a feeling the days of open frontier models are over, but theyâ€™re still doing a lot.",
          "score": 99,
          "created_utc": "2025-12-29 03:57:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhlg3m",
              "author": "TheRealMasonMac",
              "text": "Chinese labs probably appreciate the free research. Especially since this one comes with evaluation criteria so they can RL on it.",
              "score": 36,
              "created_utc": "2025-12-29 04:07:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwhlyyd",
                  "author": "Southern-Chain-6485",
                  "text": "Welcome to science",
                  "score": 62,
                  "created_utc": "2025-12-29 04:11:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwi95f9",
              "author": "eat_my_ass_n_balls",
              "text": "Sorta, but their models have fallen off",
              "score": 1,
              "created_utc": "2025-12-29 07:02:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhnv9t",
          "author": "Any-Conference1005",
          "text": "Acronym collision.......",
          "score": 36,
          "created_utc": "2025-12-29 04:22:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhpalv",
              "author": "HistorianPotential48",
              "text": "can't wait for coming up HGAME dataset, FEMBOY datasets from meta",
              "score": 34,
              "created_utc": "2025-12-29 04:32:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwi8967",
              "author": "FaceDeer",
              "text": "I really need to train an LLM for some serious hardcore RPG, and I keep finding plenty of datasets that claim that they're for this purpose. But the LLMs keep turning out wrong! Every time I demo for my supervisor... honestly, I have no idea why my funding hasn't been pulled, or why he keeps the resulting models. They're useless.",
              "score": 7,
              "created_utc": "2025-12-29 06:54:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhb3i5",
          "author": "segmond",
          "text": "Would be nice if folks release dataset with models trained on it.",
          "score": 15,
          "created_utc": "2025-12-29 03:05:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhiam9",
              "author": "Accomplished_Ad9530",
              "text": "They cite their unreleased paper, â€œTraining AI Co-Scientists using Rubric Rewardsâ€ so I wouldnâ€™t be surprised if they release a model at some point.",
              "score": 17,
              "created_utc": "2025-12-29 03:48:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi9g9x",
          "author": "JudgmentPale458",
          "text": "Interesting release. Research plan generation feels like a subtle but important capability â€” especially for agentic or tool-using systems where planning quality matters more than final answer fluency.\n\nCurious how this dataset handles evaluation: are plans judged mainly on structure/coverage, or is there any signal about feasibility and downstream execution success? That distinction seems critical if this is used to train agents rather than just planners.",
          "score": 6,
          "created_utc": "2025-12-29 07:04:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkbh1n",
          "author": "martinerous",
          "text": "Great, now waiting what they will make out of MMORPG.",
          "score": 1,
          "created_utc": "2025-12-29 16:05:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrk07z",
          "author": "stealthagents",
          "text": "This dataset sounds like a game changer for streamlining research. Having those evaluation rubrics and reference solutions will save a ton of time for any AI training. Can't wait to see what kind of projects come out of this!",
          "score": 1,
          "created_utc": "2025-12-30 17:39:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvjhs7",
          "author": "Brenan-Caro",
          "text": "Research Plan Gen",
          "score": 1,
          "created_utc": "2025-12-31 06:54:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi2ub3",
          "author": "serendipity777321",
          "text": "What is this for? Not one single explanation",
          "score": 2,
          "created_utc": "2025-12-29 06:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi7cer",
              "author": "Odd-Ordinary-5922",
              "text": "22k tasks spanning ML, Arxiv and PubMed, complete with evaluation rubrics and Llama-4 reference solutions for training **AI co-scientists**",
              "score": 13,
              "created_utc": "2025-12-29 06:46:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwigkvv",
                  "author": "serendipity777321",
                  "text": "You must be joking",
                  "score": -4,
                  "created_utc": "2025-12-29 08:09:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwiquf6",
              "author": "know-your-enemy-92",
              "text": "Taking science back to the times of alchemy from middle ages.Â ",
              "score": 2,
              "created_utc": "2025-12-29 09:46:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1w1qj",
      "title": "Most optimal vram/performance per price and advice for Shenzhen GPU market",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/4nfcarq96xag1.jpeg",
      "author": "notafakename10",
      "created_utc": "2026-01-02 11:14:30",
      "score": 258,
      "num_comments": 64,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxaaipl",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-02 17:25:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8n57z",
          "author": "DistanceSolar1449",
          "text": "MI100 is best value in terms of perf for $ currently if you donâ€™t need CUDA\n\n4090D 48GB if you need CUDA",
          "score": 51,
          "created_utc": "2026-01-02 11:51:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8ogc0",
              "author": "notafakename10",
              "text": "ROCM has come far enough its not too much of a disadvantage now - my only issue is cooling a MI100 with server fans..",
              "score": 23,
              "created_utc": "2026-01-02 12:02:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx99dxt",
                  "author": "FullstackSensei",
                  "text": "If you get an even number of cards, you can cool each pair with a relatively quiet 80mm server fan (Arctic S8038 series).",
                  "score": 7,
                  "created_utc": "2026-01-02 14:22:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx92ztb",
                  "author": "xrailgun",
                  "text": "In my experience, ROCm is \"good enough\" only for brief windows of time. As architectures/libraries/stacks change, ROCm frequently gets left behind and/or existing GPUs get dropped from feature support.",
                  "score": 31,
                  "created_utc": "2026-01-02 13:44:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9opp1",
                  "author": "Freonr2",
                  "text": "Even if it works for just basic LLM inference, Nvidia cards would also be very good for other models like diffusion (t2i, t2v, etc) and be less hassle in general.  Very good TFLOP/s for diffusion, and any software or github repo you clone will \"just work.\"\n\nOnly downside is that it seems there are some quirks with the 4090 48GB, some people experience some idle vs. load up/down clocking issues.  Can't speak to that first hand but have seen reports from at least a few people about that problem.",
                  "score": 4,
                  "created_utc": "2026-01-02 15:42:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9r3x3",
                  "author": "Ulterior-Motive_",
                  "text": "The most common way is to use 40mm fans which tend to be very loud at max speed, but if you don't mind making a shroud, you can get away with a larger, much quieter fan. I literally made one out of cardboard, and was able to cool 2 MI100s with a single Super Flower Megacool 120mm fan. The cooling wasn't as effective as individual fans, but the noise was much more tolerable.\n\nAnother option is the R9700, which have their own fans, have better prompt processing, but somewhat lower token generation than the MI100. Here, they're only $300 more than a M100, but I'm not sure what they're going for where you are.",
                  "score": 1,
                  "created_utc": "2026-01-02 15:54:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx8wzc7",
          "author": "AlwaysLateToThaParty",
          "text": "As with all of these devices; make sure you sort the cooling out.\n\nSome pretty good prices there.  I'd be tempted by nvidia a40s.  USD$7K for 196GB of700GB/s VRAM.  NVLINK and CUDA to boot.  300W too, so easy with one big power supply.",
          "score": 11,
          "created_utc": "2026-01-02 13:06:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx92sry",
              "author": "notafakename10",
              "text": "That would be great, slightly out of budget but I might be able to swing a 48gb version of the A40",
              "score": 3,
              "created_utc": "2026-01-02 13:43:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx9jyc5",
                  "author": "__JockY__",
                  "text": "Remember: cooling and noise!",
                  "score": 2,
                  "created_utc": "2026-01-02 15:19:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx9bzgn",
          "author": "jack-in-the-sack",
          "text": "You can buy them, but can you leave China with them?",
          "score": 6,
          "created_utc": "2026-01-02 14:36:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9ffwy",
              "author": "notafakename10",
              "text": "You can - I've done it a few times",
              "score": 7,
              "created_utc": "2026-01-02 14:55:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx9cjfb",
              "author": "PsychologicalWeird",
              "text": "I thought it was this or getting done over arriving home",
              "score": 2,
              "created_utc": "2026-01-02 14:40:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8kdy8",
          "author": "ChigGitty996",
          "text": "The 48gb 3090 is vaporware, at least as far as I know.  Did something change?\n\nThe 48gb 4090D or 2 should be good options if they use normal plugs",
          "score": 16,
          "created_utc": "2026-01-02 11:27:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8zgo7",
              "author": "lly0571",
              "text": "There are 48GB 3090 in GPU rental market, but very rare.\n\nhttps://preview.redd.it/mj2kronduxag1.png?width=1473&format=png&auto=webp&s=8cc1a94b22da84e7bbffd458aaba430f2a87a26b",
              "score": 17,
              "created_utc": "2026-01-02 13:22:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx8lko7",
              "author": "notafakename10",
              "text": "48gb 4090D seems like a great buy though they arent super easy to fine as far as I'm aware.   \n  \nThe 3090 48gb I've only really seen a handful, and those were expensive, which is why its not a front runner",
              "score": 8,
              "created_utc": "2026-01-02 11:38:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx8nnim",
                  "author": "durden111111",
                  "text": "I don't think a 48GB 3090 has ever existed. You can solder 2GB chips on a 3090 PCB but it simply wont recognize the extra 24GB",
                  "score": 3,
                  "created_utc": "2026-01-02 11:55:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx9b8lw",
              "author": "DataGOGO",
              "text": "No, they were real, but they all do the 4090 now",
              "score": 3,
              "created_utc": "2026-01-02 14:32:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxk7ykz",
              "author": "phido3000",
              "text": "For the cost and the expense the 3090 isn't worth it for the modder.\n\n4090D 48Gb is pretty much perfect, lower power, much better performance, 4000 series features, and 48Gb ram same effort/costs. 4090D gets deployed pretty large systems.",
              "score": 1,
              "created_utc": "2026-01-04 03:17:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx91eqm",
          "author": "lly0571",
          "text": "2x4080S 32GB or 1x4090D 48GB.\n\n  \nYou can also get A100 40GB(SXM4 to PCIe) at \\~18000CNY, but I would recommend 4090D myself.",
          "score": 6,
          "created_utc": "2026-01-02 13:35:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx930jo",
              "author": "notafakename10",
              "text": "How rare are the 32gb models? That would be a perfect balance \n\nHave you had any issues with SXM4?",
              "score": 2,
              "created_utc": "2026-01-02 13:44:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx9d34w",
                  "author": "lly0571",
                  "text": "I think you can get a 4080S 32GBÂ at 9000-9500CNY(\\~1300USD) at Xianyu, but few local shops have these GPUs(at least in October).\n\nI don't own an A100 myself, I believe 4090 is better for inference due to its FP8 support.",
                  "score": 1,
                  "created_utc": "2026-01-02 14:43:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx8pc2q",
          "author": "pharrowking",
          "text": "do you live outside china in western regions normally? because im looking at ebay. some of those prices on your list are at best saving you $200. then if you factor the flight to china and hotel and all that, youre not really saving at that point. i assume those prices are in chinese yuan not japanese yuan. they use the same symbol.\n\nnow if you live there normally those prices are decent. the mi100 is around 900-1100 in canadian dollars when i calculated to exchange to my local dollar. where as on ebay its listed for $1300 cad\n\n5800 yuan is around 829$ USD. the mi100 on ebay is 984$ USD. not a huge difference i guess",
          "score": 7,
          "created_utc": "2026-01-02 12:09:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8qwcx",
              "author": "notafakename10",
              "text": "We're already here! \n\nYeh, I'm assuming those prices can be negotiated (like everything in china) so i'd expect to get below particularly if I'm buying 2x or 4x",
              "score": 2,
              "created_utc": "2026-01-02 12:21:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx93pep",
                  "author": "xrailgun",
                  "text": "In my experience, there's not much room (if any) for negotiation on these, especially towards the lower end of those scales you've compiled. They might round you down Â¥10-Â¥50 if you buy 2 or more. They're moving sufficient volume to price-insensitive customers, there are frequently weeks where some of these are out of stock/pre-allocated entirely. Doesn't hurt to ask though, especially if it's towards the higher half of the scale.",
                  "score": 14,
                  "created_utc": "2026-01-02 13:48:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx96pus",
          "author": "mp3m4k3r",
          "text": "Iirc the Pascal line is getting older/being dropped from upcoming versions for support so while tempting they might be worth weighting against or dropping.",
          "score": 3,
          "created_utc": "2026-01-02 14:06:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9p3y1",
          "author": "Terrible-Contract298",
          "text": "The Tesla P40 can be modified with a 8+6pin arrangement. A standard 65% PWM cycle on the fan of a standard 1080 TI cooler applied allows sustained operation at lower clocks and a TDP of \\~225W. I can provide definite performance and data regarding the upgrade.",
          "score": 3,
          "created_utc": "2026-01-02 15:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxadg6y",
              "author": "mtbMo",
              "text": "Yes sir. Build my with a 980ti founders cooler. Runs under 80c",
              "score": 2,
              "created_utc": "2026-01-02 17:39:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbz7mi",
                  "author": "Terrible-Contract298",
                  "text": "Fantastic to hear others having success with this.",
                  "score": 1,
                  "created_utc": "2026-01-02 22:14:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxb8w54",
          "author": "fallingdowndizzyvr",
          "text": "At your budget, hands down 4090(D) 48GB if the best choice.",
          "score": 3,
          "created_utc": "2026-01-02 20:06:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxcol9m",
          "author": "Disposable110",
          "text": "4080 32GB mod for around 9-10k CNY each caught my eye on Taobao, but beware that it is slower and can run hot.",
          "score": 2,
          "created_utc": "2026-01-03 00:31:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxd4149",
          "author": "1427538609",
          "text": "Can't help with Shenzhen prices specifically, but on the hardware side - dual P40s (48GB) or the modded 3080 20GB x4 route both work for 48GB+. The 3080s will be much faster for inference due to Ampere vs Pascal. For 96GB, you're looking at A100s or multiple cards regardless.",
          "score": 2,
          "created_utc": "2026-01-03 01:59:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9j45y",
          "author": "a_beautiful_rhind",
          "text": "First I hear that AMD has mod. 4080 32g seems like a contender if you can't afford 4090/5xxx gpu.",
          "score": 1,
          "created_utc": "2026-01-02 15:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa8ytw",
          "author": "jinnyjuice",
          "text": "Would be nice to have some comparisons with Yongsan in Korea also, also for RAM. I wonder if there is such data. Would be amazing if someone is familiar with all the international forums.\n\n>Prices are best estimates from deep seek\n\nThat is very, very unreliable. They're much more often wrong than correct. Manual search/scraping is definitely required and recommended.",
          "score": 1,
          "created_utc": "2026-01-02 17:17:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdl6pd",
              "author": "notafakename10",
              "text": "Agree it would be great to compare - pricing data was a mix of several things, deepseek, forums and my own observations so far, I'd put it as \"directional useful\" not exactly accurate lol",
              "score": 1,
              "created_utc": "2026-01-03 03:41:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxa9b22",
          "author": "Mediocre-Waltz6792",
          "text": "Get two cards with the Vram you want. Im currently fighting with my 3rd 3090 as it slowed everything down by 3x. Could be a windows issue but either way two GPU is way easier to setup and run.",
          "score": 1,
          "created_utc": "2026-01-02 17:19:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdl8qi",
              "author": "notafakename10",
              "text": "Good pointer - thanks for the insight",
              "score": 1,
              "created_utc": "2026-01-03 03:42:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbvxme",
          "author": "etherd0t",
          "text": "Just out of curiosity... where'd you get that price list?ðŸ™‚ Is there a place where you can check Shenzen bazaar (Huaqiangbei) prices? May visit soon...",
          "score": 1,
          "created_utc": "2026-01-02 21:58:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdldzm",
              "author": "notafakename10",
              "text": "Pricing data was a mix of several things, deepseek, forums and my own observations so far (from the Shanghai GPU markets), I'd put it as \"directional useful\" not exactly accurate lol\n\nI'll be going to Shenzhen later this month and I'll get some actual on the ground pricing and update",
              "score": 1,
              "created_utc": "2026-01-03 03:43:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdfjeh",
          "author": "BrightComplaint8342",
          "text": "est price not accurate i think",
          "score": 1,
          "created_utc": "2026-01-03 03:07:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxi69ez",
          "author": "millerlite_11",
          "text": "Any modded 5090?",
          "score": 1,
          "created_utc": "2026-01-03 20:50:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxjz521",
              "author": "notafakename10",
              "text": "None Iâ€™ve seen yet",
              "score": 1,
              "created_utc": "2026-01-04 02:28:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxostmg",
          "author": "pbad1",
          "text": "4x 5060ti 16GB?   \n\\~$2.5k",
          "score": 1,
          "created_utc": "2026-01-04 20:26:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrknrj",
              "author": "notafakename10",
              "text": "Not a bad option but im trying to maximise expandability in the future, if I can get 64gb in two cards thats ideal",
              "score": 1,
              "created_utc": "2026-01-05 04:58:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxw9z41",
          "author": "Irisi11111",
          "text": "What are your specific needs? Are you focused on text processing or vision tasks? If it's the latter, I highly recommend getting the latest CUDA-supported graphics cards. If that's not the case for you, AMD solutions might be a good option.",
          "score": 1,
          "created_utc": "2026-01-05 21:59:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz4kwm",
              "author": "notafakename10",
              "text": "I do a bit of everything but mostly training transformers in PyTorch at the minute and local LLMs",
              "score": 1,
              "created_utc": "2026-01-06 08:35:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx9l8a4",
          "author": "pCute_SC2",
          "text": "Wait there is a RTX 3090 48GB mod????? Where can I get it?",
          "score": 1,
          "created_utc": "2026-01-02 15:25:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa7emj",
              "author": "sillynoobhorse",
              "text": "Alibaba, TaoBao etc., be prepared for the chinese way of dealing which includes many chat and Whatsapp messages lol",
              "score": 1,
              "created_utc": "2026-01-02 17:10:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxa7zds",
                  "author": "pCute_SC2",
                  "text": "Didn't found it on Alibaba.",
                  "score": 2,
                  "created_utc": "2026-01-02 17:13:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxdl04d",
              "author": "notafakename10",
              "text": "They are hard to come by now as all most of the modders have moved on to 4xxx series cards, but they do float around occasionally in the markets themselves, won't come up on Alibaba!",
              "score": 1,
              "created_utc": "2026-01-03 03:40:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxa4sws",
          "author": "grzesi00",
          "text": "7900 xt has 24gb vram stock so whats modded there?",
          "score": 1,
          "created_utc": "2026-01-02 16:58:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxad1dk",
              "author": "mftrhu",
              "text": "The 7900XTX has 24 GB VRAM. The 7900XT sits at 20 GB.",
              "score": 5,
              "created_utc": "2026-01-02 17:37:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q06ddc",
      "title": "Update on the Llama 3.3 8B situation",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/",
      "author": "FizzarolliAI",
      "created_utc": "2025-12-31 06:45:42",
      "score": 256,
      "num_comments": 22,
      "upvote_ratio": 0.93,
      "text": "Hello! You may remember me as either\n\n- The person [who recently uploaded L3.3 8B's weights to Huggingface](https://www.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/) (see this post for more context)\n- That stupid bitch\n\nand I would like to provide some updates, as I've been doing some more benchmarks on both the original version that Meta gave me and the context extended version by u/Few-Welcome3297.\n\nThe main benchmark table from the model README has been updated:\n\n| | Llama 3.1 8B Instruct | Llama 3.3 8B Instruct (original 8k config) | Llama 3.3 8B Instruct (128k config)\n|-|-|-|-|\n|IFEval (1 epoch, score avged across all strict/loose instruction/prompt accuracies to follow Llama 3 paper)|78.2|81.95|**84.775**\n|GPQA Diamond (3 epochs)|29.3|37.0|**37.5**\n\nWhile I'm not 100% sure, I'm... pretty sure that the 128k model is better. Why Facebook gave me the weights with the original L3 config and 8k context, and also *serves* the weights with the original L3 config and 8k context, I have absolutely no idea!\n\nAnyways, if you want to try the model, I would recommend trying both the [128k version](https://huggingface.co/shb777/Llama-3.3-8B-Instruct), as well as my [original version](https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct) if your task supports 8k context lengths. I honestly have absolutely no clue which is more correct, but oh well! I do wish Facebook had released the weights officially, because back in April, this really wouldn't have been that bad of a model...\n\nEdit: Removed the Tau-Bench results (both from here and the readme). The traces from the evals are, to put it slightly, really fucky-wucky, and I don't think OpenBench is scoring them right, but I'm too tired to actually debug the issue, so. I'll figure it out tomorrow :3",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwvmrda",
          "author": "toothpastespiders",
          "text": ">I do wish Facebook had released the weights officially, because back in April, this really wouldn't have been that bad of a model...\n\nHonestly, I think I prefer it this way. The llama saga began with some public shenanigans with a semi-leak. Seems appropriate in a way that if it has to end, and it does seem to be the case, that everything was capped off by something like this.",
          "score": 89,
          "created_utc": "2025-12-31 07:23:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvl06q",
          "author": "Kahvana",
          "text": "No need to degrade yourself, you're doing fantastic work.\n\nThank you for the release!",
          "score": 112,
          "created_utc": "2025-12-31 07:07:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww6ghg",
              "author": "MoffKalast",
              "text": "Yeah OP, run yourself at at least Q6 ;)",
              "score": 20,
              "created_utc": "2025-12-31 10:28:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvt29i",
          "author": "datbackup",
          "text": "Lol upvoted for humor\n\nGood stuff, I might try this 3.3, it has actually been months since iâ€™ve run any llama model.",
          "score": 23,
          "created_utc": "2025-12-31 08:21:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvp8bu",
          "author": "pmttyji",
          "text": "Thanks for this. Still didn't download original version due to less context thing. I'm gonna try this 128K version this week. Also waiting for feedback from others on this version. Just expecting to replace 3.1 8B with this version.",
          "score": 10,
          "created_utc": "2025-12-31 07:45:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwkxbj",
              "author": "Few-Welcome3297",
              "text": "Very small improvement, but its something",
              "score": 3,
              "created_utc": "2025-12-31 12:34:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwkrkr",
          "author": "Few-Welcome3297",
          "text": "Some evals [https://huggingface.co/datasets/shb777/Llama-3.3-8B-Instruct-128K-Evals](https://huggingface.co/datasets/shb777/Llama-3.3-8B-Instruct-128K-Evals) . TLDR: Small Improvement\n\nEdit: Link updated",
          "score": 11,
          "created_utc": "2025-12-31 12:33:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvmlmo",
          "author": "jacek2023",
          "text": "Would be nice to put some info into model's name to distinguish them",
          "score": 15,
          "created_utc": "2025-12-31 07:21:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvnt4n",
              "author": "FizzarolliAI",
              "text": "I would, but since quants and all have already been made under the original model's name, it's kinda too late :p",
              "score": 12,
              "created_utc": "2025-12-31 07:32:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwdex2",
                  "author": "Awwtifishal",
                  "text": "The new one could be renamed to add -128K or something so the quants also reflect it.",
                  "score": 8,
                  "created_utc": "2025-12-31 11:32:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwcrfv",
          "author": "Cool-Chemical-5629",
          "text": "* That stupid bitch\n\nI need more context, please. ðŸ¤£\n\n  \nIn any case, I tried the extended version yesterday and while it felt pretty weak for stuff like coding etc. it seemed to be a decent base model for E/RP finetunes, because it followed instructions fairly well, but it was HORRIBLY SLOW burning so it would need some nudging from E/RP datasets to keep the story going. I hope E/RP creators will pick it up (and Ministral 14B Instruct too while at it).",
          "score": 11,
          "created_utc": "2025-12-31 11:26:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxs7tf",
          "author": "randomfoo2",
          "text": "Just in case anyone's interested, I ran [shb777/Llama-3.3-8B-Instruct](https://huggingface.co/shb777/Llama-3.3-8B-Instruct) on the Shisa AI's MultiEval on my dev box. \n\nOn the English side, it loses a bit on MixEval Easy and Hard (2024 Chat Arena proxy), but gets a +20% boost in LiveBench (reasoning-focused), +15% GPQA Diamond (PhD level QA), +5% on IFEval, +30% on IFBench (!) and +10% on HumanEval+ (Python). That's some decent gains.\n\nThat being said, on the Japanese side, it takes a big hit on Shaberi (Japanese chat-style functional tests) vs 3.1. I've included my Llama 3.1 8B-based Shisa V2  and Qwen 3 8B-based Shisa V2.1 as well as Llama 3.3 70B and Llama 3.1 405B scores just for comparison, sake.\n\n(I probably wont train a Shisa V2.1 Llama 3.3 8B - the Qwen 3 8B version is already great and it's Apache 2.0 licensed).\n\nhttps://preview.redd.it/6hnc71dggkag1.png?width=3126&format=png&auto=webp&s=10c4dd3231be1ed9749f174c59e8758c134d0009",
          "score": 5,
          "created_utc": "2025-12-31 16:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy05pp",
              "author": "FizzarolliAI",
              "text": "Interesting, I wonder if you'd get a noticeable regression from L3.3 70B on multilingual benches with Llama 3.1 70B then.\n\nI definitely agree that I don't think this is worth building on for most usecases. Personally I think it's an interesting artifact of the times",
              "score": 2,
              "created_utc": "2025-12-31 17:19:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwy8qrh",
                  "author": "randomfoo2",
                  "text": "Hmm, hard to say, I don't have 3.1 70B data handy... 3.3B 70B is in general pretty strong.\n\nIn practical terms, your ultimate multilingual perf is going to be pretty much up to you (tuning).  While the overall number isn't so big, when you look at the stuff we care about like JP IF, JP RP, JP TL, JP nuance, dialogue translation, we're able to get huge boosts from doing training on top of whatever model. Not show nis also our own CLTL tests that test for how many wrong-language tokens get output (huge amounts for most non-target language trained models).\n\nThe benchmark mix we use for our current multieval does feel about right. For the tasks that it's trained on, our V2.1 14B model actually \\*does\\* feel like it outperforms our V2 70B (and sometimes our V2.1 70B and V2 405B even!).\n\n\n\nhttps://preview.redd.it/hpf7vf50vkag1.png?width=3123&format=png&auto=webp&s=b5ea8273adbb61f19412f497ff3d2d06e4f46aed",
                  "score": 1,
                  "created_utc": "2025-12-31 18:01:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvndin",
          "author": "Amazing_Athlete_2265",
          "text": "Interesting. I'll evaluate it and compare",
          "score": 3,
          "created_utc": "2025-12-31 07:28:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvpi7i",
              "author": "pmttyji",
              "text": "Awesome",
              "score": 0,
              "created_utc": "2025-12-31 07:48:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww92bu",
          "author": "ilintar",
          "text": "Is the 128k version just a x16 YaRN extension or a different model?",
          "score": 3,
          "created_utc": "2025-12-31 10:53:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwdgf2",
              "author": "Awwtifishal",
              "text": "Just a config change",
              "score": 6,
              "created_utc": "2025-12-31 11:33:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxcw2i",
          "author": "noctrex",
          "text": "Generated some abliterated gguf's for it:\n\n[https://huggingface.co/noctrex/Llama-3.3-8B-Instruct-128k-abliterated-GGUF](https://huggingface.co/noctrex/Llama-3.3-8B-Instruct-128k-abliterated-GGUF)",
          "score": 3,
          "created_utc": "2025-12-31 15:23:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0u60h",
          "author": "Dangerous_Fix_5526",
          "text": "Thinking/Instruct Hybrid using Unsloth and Claude-Opus 4.6 dataset:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning)\n\nI hope I credited everyone correctly.",
          "score": 3,
          "created_utc": "2026-01-01 02:59:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwxu4p",
          "author": "Robert__Sinclair",
          "text": "I just tested it and its reasoning is extremely lacking.",
          "score": -3,
          "created_utc": "2025-12-31 14:00:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0zst6",
      "title": "Upstage Solar-Open-100B Public Validation",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/w789uyo0cpag1.jpeg",
      "author": "PerPartes",
      "created_utc": "2026-01-01 08:52:25",
      "score": 235,
      "num_comments": 69,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0zst6/upstage_solaropen100b_public_validation/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nx2bcn4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-01 11:00:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1zsaj",
          "author": "CKtalon",
          "text": "Why a location? Just release on the Internet.",
          "score": 125,
          "created_utc": "2026-01-01 08:56:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1zvf8",
              "author": "spectralyst",
              "text": "Gangnam Style",
              "score": 130,
              "created_utc": "2026-01-01 08:57:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx34al3",
                  "author": "AuspiciousApple",
                  "text": "Koreans love their pop up stores.",
                  "score": 10,
                  "created_utc": "2026-01-01 14:56:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx7ps0n",
                  "author": "Mikasa0xdev",
                  "text": "Oppa LocalLLaMA Style!",
                  "score": 3,
                  "created_utc": "2026-01-02 06:43:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx2a34t",
                  "author": "DecodeBytes",
                  "text": "I have the synth intro stuck in my head now",
                  "score": 3,
                  "created_utc": "2026-01-01 10:47:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2eddx",
              "author": "keepthepace",
              "text": "Sadly, that's still how to maximize journalistic coverage, by causing FOMO. Force journalists to get there, you force them to make an article. Publish something online they will be like \"meh, put it on the pile\"",
              "score": 31,
              "created_utc": "2026-01-01 11:31:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2ri55",
                  "author": "-p-e-w-",
                  "text": "I very strongly doubt that journalists are going to bother showing up at some mystery location in Korea to settle some AI startup beef lol.",
                  "score": 4,
                  "created_utc": "2026-01-01 13:29:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx224td",
              "author": "ttkciar",
              "text": "That would be lovely!",
              "score": 5,
              "created_utc": "2026-01-01 09:22:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2ijts",
              "author": "PerPartes",
              "text": "This is because of huge domestic market focus. In-person event is a matter of trust and respect (esp. in this region). Almost whole SK AI business is focused on itself. In case of Upstage with the addition of Japanese market as well.",
              "score": 9,
              "created_utc": "2026-01-01 12:12:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx33dme",
                  "author": "Nyghtbynger",
                  "text": "Interestingly that's the case of most nations in fact, except a few merchant nations and empires (US,UK,...)",
                  "score": 2,
                  "created_utc": "2026-01-01 14:50:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx72o2z",
              "author": "dicoxbeco",
              "text": "OOP in Korean *does* state that they will update the post with URL for livestream.\n\nEither the translator OP used skipped that part over, or OOP edited that in later.",
              "score": 1,
              "created_utc": "2026-01-02 03:56:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx21p4c",
          "author": "throwaway-link",
          "text": "I did my own tests. Cossim between layers past the first few seems to be extremely high across any model. Testing layer 45 input layernorm of deepseek v3/v3.1/v3.2-special, kimi k2, and mistral large 3 all give similarities around 0.99. The tested deepseek v3 variants are around 0.99999 with each other. \n\nData from the accusation is entirely expected for a model trained from scratch.",
          "score": 76,
          "created_utc": "2026-01-01 09:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2hccm",
              "author": "llama-impersonator",
              "text": "why are people comparing the norms instead of attn or mlp layers? norms have both low param count and a fairly simple fixed function.",
              "score": 10,
              "created_utc": "2026-01-01 12:01:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2m36d",
                  "author": "throwaway-link",
                  "text": "bc the accusation already says they're different? Their only evidence is norm weights which I show is expected. Probably bc training dynamics for rmsnorm of deeper layers cause the scale to just be a constant value across the weight which obviously results in high cossim. I guess since deeper layers do smaller adjustments, rmsnorm scale doesn't need to do any wild adjustments across the already relatively normalised token vector.",
                  "score": 9,
                  "created_utc": "2026-01-01 12:45:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx3bule",
              "author": "egomarker",
              "text": "Show the code and results. No idea if you are legit or yet another schizo vibecoder with hallucinated \"test results\".",
              "score": 6,
              "created_utc": "2026-01-01 15:41:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx22ua9",
              "author": "KontoOficjalneMR",
              "text": "Almost like all those models are using the similar architecture and similar datasets and you get same-ish output with some small flavour on top. \n\nYou look at the benchmarks and the results are basically a function of amount of parameters with tiny percentage variation based mostly on luck.",
              "score": 18,
              "created_utc": "2026-01-01 09:29:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2i4uv",
                  "author": "DistanceSolar1449",
                  "text": "Thatâ€™sâ€¦ obviously not true. DeepSeek V3, R1, V3.1, V3.2 all have the same param count but much diff performance.",
                  "score": 25,
                  "created_utc": "2026-01-01 12:09:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx42byg",
              "author": "jinnyjuice",
              "text": "Yeah they're saying that you can't really make such definitive conclusions with cossim. They made comparison with Phi here also: https://github.com/hyunwoongko/solar-vs-glm-vs-phi",
              "score": 2,
              "created_utc": "2026-01-01 18:00:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3du9z",
          "author": "kiwibonga",
          "text": "News tomorrow: Upstage employees arrested for beating up some dude in a parking lot.",
          "score": 10,
          "created_utc": "2026-01-01 15:52:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx228oe",
          "author": "ResidentPositive4122",
          "text": "I mean, if this is what it takes to get intermediate checkpoints, let's do it! Llamas, qwens, mistrals, glms, minimaxes, deepseeks, j'accuse! :D",
          "score": 25,
          "created_utc": "2026-01-01 09:23:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2au1f",
              "author": "pkmxtw",
              "text": "AI labs *hate* this simple trick to get them to release intermediate checkpoints!\n\nEither that or this is some of evil-genius level of marketing.",
              "score": 14,
              "created_utc": "2026-01-01 10:54:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2oduo",
              "author": "zball_",
              "text": "Just use different model configuration smh",
              "score": -1,
              "created_utc": "2026-01-01 13:04:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx20qfj",
          "author": "garloid64",
          "text": "op op op",
          "score": 21,
          "created_utc": "2026-01-01 09:07:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx20cjs",
          "author": "PerPartes",
          "text": "I just shared this because recent AI generated post here about the plagiarism claim was removed by the admins. I know the team for approx. 2 years (from the online space) and can hardly believe that it would be true.",
          "score": 34,
          "created_utc": "2026-01-01 09:02:47",
          "is_submitter": true,
          "replies": [
            {
              "id": "nx25axo",
              "author": "RuthlessCriticismAll",
              "text": "It seems appropriate to remove that post. It is however galling that similar, evidence free, ai generated posts with the same accusations don't get removed.",
              "score": 16,
              "created_utc": "2026-01-01 09:56:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx26swr",
                  "author": "PerPartes",
                  "text": "Agreed. Hate is always simpler than a deep and independent analysis.",
                  "score": 18,
                  "created_utc": "2026-01-01 10:12:19",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nx49h78",
                  "author": "rm-rf-rm",
                  "text": "Please report anything you see that we havent removed. Generally I think we are catching stuff well especially things that are particularly egregious.",
                  "score": 6,
                  "created_utc": "2026-01-01 18:35:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx26a8j",
          "author": "AppearanceHeavy6724",
          "text": "Ahaha, imagine if there will be a literal knuckle fight.",
          "score": 18,
          "created_utc": "2026-01-01 10:06:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx61575",
              "author": "tengo_harambe",
              "text": "\"The cosine similarity of my fist and your face is about to be -1.00\"",
              "score": 7,
              "created_utc": "2026-01-02 00:08:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7ilyv",
                  "author": "AppearanceHeavy6724",
                  "text": "yeah exactly.",
                  "score": 2,
                  "created_utc": "2026-01-02 05:45:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6rc2d",
          "author": "siegevjorn",
          "text": "Am I reading this right? How the fuck are they going to validate they trained their llm from scratch at Gangnam station? What about just release a white paper about the novelty of their methods?",
          "score": 6,
          "created_utc": "2026-01-02 02:44:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2uobg",
          "author": "Intrepid_Bobcat_2931",
          "text": "This is a joke. I could see a stunt like \"in person verification\" be reasonable if you gave two weeks notice for people to make travel plans, but they know it's completely impractical for highly experienced people to fly over at a day's notice.",
          "score": 7,
          "created_utc": "2026-01-01 13:52:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3mgdc",
              "author": "my_name_isnt_clever",
              "text": "If you have to fly there, you're not their target audience. This is for domestic journalism.",
              "score": 13,
              "created_utc": "2026-01-01 16:38:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx718se",
              "author": "dicoxbeco",
              "text": "... What joke?\n\nThis was never meant for English audience. [In fact, the OOP wasn't even written in English.](https://www.linkedin.com/posts/upstage-stan_solar-open-100b-%EA%B3%B5%EA%B0%9C-%EA%B2%80%EC%A6%9D%EC%97%90-%EC%B4%88%EB%8C%80-%EB%93%9C%EB%A6%BD%EB%8B%88%EB%8B%A4-solar-100b%EA%B0%80-activity-7412403323175370753-2KgK?utm_source=share&utm_medium=member_desktop&rcm=ACoAACoF--MBSnmFDkOdoa7FU_ztI512j0sxTo4) OP went through a translator so you would understand what it says.",
              "score": 6,
              "created_utc": "2026-01-02 03:46:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx25ssw",
          "author": "PerPartes",
          "text": "[Original Hwalsukâ€™s LI post here](https://www.linkedin.com/posts/upstage-stan_solar-open-100b-%EA%B3%B5%EA%B0%9C-%EA%B2%80%EC%A6%9D%EC%97%90-%EC%B4%88%EB%8C%80-%EB%93%9C%EB%A6%BD%EB%8B%88%EB%8B%A4-solar-100b%EA%B0%80-activity-7412403323175370753-2KgK)",
          "score": 3,
          "created_utc": "2026-01-01 10:01:43",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nx2hk6u",
          "author": "No_Conversation9561",
          "text": "Damn.. you know what, I believe him",
          "score": 2,
          "created_utc": "2026-01-01 12:03:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx20wtr",
          "author": "texasdude11",
          "text": "Tbh, I don't even care about this... If I need a model in this class, I can pick prime intellect, gpt-oss-120b, qwen3-next or move up a class and go to qwen3-235b or Minimax-m2.1 this 100b market is so competitive that you really need to stand out for adoption. Zai, Qwen and OpenAI's censored gpt-oss-120b kinda rule that 80-120b.\n\nAll that being said, more competition is always welcome though! I'd love to see a llama5 120B or a DeepSeek 200b model. That would be insane!",
          "score": 6,
          "created_utc": "2026-01-01 09:08:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx26jdf",
              "author": "LittleBlueLaboratory",
              "text": "I have 96GB VRAM (4x 3090). Strix Halo and DGX Spark have 128. This 80B to 120B segment is where its at! The more competition the better!",
              "score": 13,
              "created_utc": "2026-01-01 10:09:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx26pzv",
                  "author": "texasdude11",
                  "text": "Agreed!\n\nI have 2x6000 Pros with 512 GB DDR5 RAM, so I'm a bit lucky there. These 100b size is clearly in consumer reach!",
                  "score": 3,
                  "created_utc": "2026-01-01 10:11:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx3kbfm",
              "author": "uti24",
              "text": ">this 100b market is so competitive that you really need to stand out for adoption\n\nI want 100B dense model. Is there something besides Meta-Llama-1/2/3-70B? \n\nIt feels not really smart.. On par with other 30B class models like Gemma or Mistral small.",
              "score": 1,
              "created_utc": "2026-01-01 16:26:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3mpca",
                  "author": "my_name_isnt_clever",
                  "text": "Devstral 2 is 123b dense, but it's coding focused. It's far, far more expensive to train large dense models than MoE which is why they're so few and far between these days.",
                  "score": 4,
                  "created_utc": "2026-01-01 16:39:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx6n7cx",
                  "author": "Sea-Speaker1700",
                  "text": "They're all complete morons out of the box, every last one. \n\nSetup a proxy between your client and the inference service and tailor the performance to your needs, it can take any \"only yet another info barfing hallucinator model\", aka: every single 100b range model, and turn them into a useful tool.\n\nLoading 100b(ish) and trying to use them direct is a plain old waste of time.",
                  "score": 1,
                  "created_utc": "2026-01-02 02:19:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx274pg",
          "author": "Kooky-Somewhere-2883",
          "text": "Oppa Gangnam Style?",
          "score": 4,
          "created_utc": "2026-01-01 10:15:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx29jzj",
          "author": "Ok_Condition4242",
          "text": "https://i.redd.it/8dox9ujevpag1.gif\n\nmeanwhile cursor's composer-1",
          "score": 4,
          "created_utc": "2026-01-01 10:41:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx21423",
          "author": "Long_comment_san",
          "text": "Next 50-80b dense would be mindblowing. Someone, please. These total trillions of total parameters are irrelevant when there's a hook to the web.",
          "score": 5,
          "created_utc": "2026-01-01 09:11:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx278zf",
          "author": "NandaVegg",
          "text": "What near Gangnam Station for \"releasing all the intermediate checkpoints and wandbs\"? This is so weird. Can we dance together for a sped up ppongjjak? That would light the mood up. BTW I don't believe the claim that it's a finetune.",
          "score": 4,
          "created_utc": "2026-01-01 10:17:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx20648",
          "author": "yuumi_ramyeon",
          "text": "Popcorn",
          "score": 3,
          "created_utc": "2026-01-01 09:00:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8gatt",
          "author": "PerPartes",
          "text": "I've updated the post with a video link /and seen just a small part of it so far/",
          "score": 1,
          "created_utc": "2026-01-02 10:51:27",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nx4w0rc",
          "author": "7734128",
          "text": "What kind of a joke organization is this?\n\nEvery communication I've seen from them has been bodged like this.\n\nI don't need to inspect weights to know they're a scam when this is the quality of their PR statements.",
          "score": -2,
          "created_utc": "2026-01-01 20:28:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx24dhc",
          "author": "Super_Sierra",
          "text": "Show proof, not text. Idc about twitter post counterclaiming.",
          "score": -1,
          "created_utc": "2026-01-01 09:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx21fw9",
          "author": "Desperate-Sir-5088",
          "text": "Do not blaim the model without any proof. GLM-4.5-Air could count number of 'r' in the \"starbrerry\" correctly.Â \n\n\nWe usually called it \"deadcopy\"",
          "score": -6,
          "created_utc": "2026-01-01 09:14:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2sfwx",
      "title": "ElevenLabs is killing my budget. What are the best \"hidden gem\" alternatives for documentary style TTS?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/",
      "author": "Ancient_Routine8576",
      "created_utc": "2026-01-03 11:31:31",
      "score": 217,
      "num_comments": 115,
      "upvote_ratio": 0.87,
      "text": "Hi everyone, I'm running a YouTube channel focused on \"War Economics\" and \"History\". I've been using ElevenLabs (Marcus voice) and the quality is amazing, but the pricing is unsustainable for long-form content (8-10 min videos).\n\nI've tried the usual suspects (Murf, Play.ht) but they sound too robotic or corporate.\n\n**I am looking for:**\n\n1. Something with a dark, authoritative, documentary-style tone.\n2. Either a cheaper paid alternative OR a high-quality GitHub/Local solution (I have a decent GPU if needed, like RVC or Tortoise).\n3. Has anyone tried tools like **Fish Audio** or **OpenAI TTS API** wrappers?\n\nAny \"underground\" or lesser-known recommendations would be appreciated. Thanks!",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nxf98e3",
          "author": "MixtureOfAmateurs",
          "text": "The best local options are:\n\nSoprano - fast \n\nKokoro - fast \n\nVibevoice\n\nXTTS v2 still somehow \n\nF5 tts.\n\nBasically just look through here and listen to samples [https://huggingface.co/models?pipeline\\_tag=text-to-speech&sort=downloads](https://huggingface.co/models?pipeline_tag=text-to-speech&sort=downloads)",
          "score": 132,
          "created_utc": "2026-01-03 11:38:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxge35i",
              "author": "LocoMod",
              "text": "Soprano is extremely impressive for its size. And it was the first attempt by the undergrad student that made it. Can't wait to see how their work evolves.",
              "score": 45,
              "created_utc": "2026-01-03 15:49:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgmvxp",
                  "author": "Foreign-Beginning-49",
                  "text": "Absolutely kind of that person, this community gives me hope for a human future inhabited by said humans working together or by themselves for the greater s3cret creature we are in truest sense of the word, some creature for certain.",
                  "score": 4,
                  "created_utc": "2026-01-03 16:31:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxgzbyx",
              "author": "aedocw",
              "text": "You missed chatterbox which is at 11labs quality for cloning, much better than xtts.",
              "score": 35,
              "created_utc": "2026-01-03 17:29:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxhqx1j",
                  "author": "trialgreenseven",
                  "text": "could you tell me if I can use voice cloning based on sample w/o verification step like 11labs on chatterbox/resemble?",
                  "score": 3,
                  "created_utc": "2026-01-03 19:34:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxn12yb",
                  "author": "claytonjr",
                  "text": "Yeah, 2nd vote for chatterbox. I've used it extensively for YouTube narration videos, with a cloned voice. Plus the fast api makes it easy to use with automationÂ ",
                  "score": 2,
                  "created_utc": "2026-01-04 15:37:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxhuigs",
                  "author": "MetricZero",
                  "text": "That thing is so ridiculously difficult to set up if you don't know what you're doing. You need to like create a contained environment with a specific install order for Gradios, Numpy, and some other stuff with specific versions and THEN it might work. I couldn't get it to though.",
                  "score": 2,
                  "created_utc": "2026-01-03 19:52:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxrse6z",
                  "author": "SituationMan",
                  "text": "Not even close. It randomly breaks into accents.",
                  "score": 1,
                  "created_utc": "2026-01-05 05:52:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxh8ada",
              "author": "Segaiai",
              "text": "I'm personally a big fan of [IndexTTS2](https://github.com/index-tts/index-tts). I hardly hear people talk about it, but I've gotten some really cool results. Especially since you can prompt a mood/situation separately from the spoken text itself. And you can get granular with emotion sliders, or match the emotion of the input file.",
              "score": 14,
              "created_utc": "2026-01-03 18:10:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxmgug0",
                  "author": "djtubig-malicex",
                  "text": "Yeah IndexTTS2 is being slept on for whatever reason.  Though I think the use case is more for dubbing existing audio given it relies on similar audio input and from my uses of it, does require quite a bit of memory to use effectively for long form gens.  (Running on M3 Ultra 256GB RAM :) )",
                  "score": 4,
                  "created_utc": "2026-01-04 13:46:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxfgjsa",
              "author": "rpg36",
              "text": "Vibevoice is great! I've been experimenting with it lately and it sounds awesome. It was really simple to run the example code. They even have a little simple web app you can run. No issues running it on my 4070 Super with 16GBs VRAM",
              "score": 12,
              "created_utc": "2026-01-03 12:36:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxfmq78",
                  "author": "1427538609",
                  "text": "Last time I checked vibe voice there are only community forks?",
                  "score": 4,
                  "created_utc": "2026-01-03 13:18:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxs184v",
              "author": "ShengrenR",
              "text": "Absolutely criminal not to mention higgs v2 or index tts2 in this list imo; maybe because they're a bit heavier to run, so fewer do.  I've seen a few folks keep XTTSv2 in the running lately and I just don't hear it myself.. it was great when it first came out, but it's far behind the latest models, unless you're going to be heavily fine-tuning it I don't see the appeal.",
              "score": 1,
              "created_utc": "2026-01-05 07:04:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxfdppp",
          "author": "CheatCodesOfLife",
          "text": "VibeVoice if you don't want to write code / just want to give it the transcript.\n\nEcho-TTS if you can work around the 30-second limitation.\n\n\nI'd give Maya-1 if you want to act like a director, eg. put \"documentary domain\" in the description prompt. More code required to generate an 8-10 minute transcript.",
          "score": 28,
          "created_utc": "2026-01-03 12:14:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxg5f7j",
          "author": "Finguili",
          "text": "From local TTS, VibeVoice Large seems to have highest ceiling, but the model is very unstable. With one generation it sounds as if text was almost professionally narrated; with another its prosody is so bad that you start to wonder is it the same model. It also loves to add strange music to the background. So expect to reroll a lot.\n\nI donâ€™t have much experience with cloud apis, but Gemini 2.5 Pro TTS sounded to me better than ElevenLabs and should be cheaper.",
          "score": 14,
          "created_utc": "2026-01-03 15:05:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhyrsm",
              "author": "PitifulTeacher4972",
              "text": "you can also remove the music by post processing the output with another model",
              "score": 2,
              "created_utc": "2026-01-03 20:13:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxsakcf",
              "author": "ArtfulGenie69",
              "text": "A lot of the various tts have this issue. Higgs has it too but when you get the best voice sample possible. Basically 30s of exactly the same tones it cloned almost 100%. So you could take a samples from the good generation that it made. If your original sample has trash sound in it try to remove it with something like pyoise or uvr. As far as I've heard for English speakers higgs seemed to clone the best. The samples I heard from vibe weren't as close to the speakers voice for whatever reason. Oh and some characters in text screw up higgs so you have to clean those out or get more weird noises. All the tts are iffy right now and take a bit of tuning.\n\n\nThere is also new stuff to try. Like indextts2 and cosyvoice3",
              "score": 1,
              "created_utc": "2026-01-05 08:30:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxfxid1",
          "author": "IONaut",
          "text": "Currently I think VibeVoice large is the best most natural sounding option. You could even give it a sample of the voice you like from ElevenLabs and clone it that way.  \n  \nChatterbox just came out with a new version (2) that is super lightweight and fast that works pretty well but you have to mess with the settings a bit to get the clone to sound right.",
          "score": 14,
          "created_utc": "2026-01-03 14:22:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxjhq5r",
              "author": "AXYZE8",
              "text": "Where did you found that '2' version?\n\nThere's no mention of such thing in Google Search results [https://www.google.com/search?q=%22chatterbox+2%22+ai](https://www.google.com/search?q=%22chatterbox+2%22+ai)\n\nUnless you're talking about Chatterbox Turbo, but it's not a successor (2), it's just a distilled small version meant for voice agents. It's a bad choice for generating audio in advance (a lot worse quality), OP doesn't need realtime responses.",
              "score": 2,
              "created_utc": "2026-01-04 00:52:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxji6sx",
                  "author": "IONaut",
                  "text": "The place that I found the link was calling chatterbox turbo chatterbox 2. But yes that is the one I'm talking about",
                  "score": 1,
                  "created_utc": "2026-01-04 00:54:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfmzhc",
          "author": "1427538609",
          "text": "The Chinese index-TTS2 is quite good, but you have to seed it with a voice example that you like",
          "score": 10,
          "created_utc": "2026-01-03 13:20:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxg4psv",
          "author": "ducksoup_18",
          "text": "https://github.com/justinlime/Fatterbox",
          "score": 6,
          "created_utc": "2026-01-03 15:02:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxjar0p",
              "author": "Yorn2",
              "text": "Nice to have another optimized version out there for streaming.",
              "score": 1,
              "created_utc": "2026-01-04 00:15:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxgmmth",
          "author": "DigiJoe79",
          "text": "Hi u/Ancient_Routine8576 I played also a lot with Vibevoice lately. Strongly recommend it, it it amazing. I guess my Audiobook Maker isn't a perfect fit for you use case, but maybe you try to pull just the Vibevoice Engine container. It includes a fastapi server and can also run standalone with whatever workflow you use right now. [https://github.com/DigiJoe79/AudioBook-Maker](https://github.com/DigiJoe79/AudioBook-Maker)",
          "score": 6,
          "created_utc": "2026-01-03 16:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxfsv30",
          "author": "Jean_velvet",
          "text": "Google is about to smash Elevenlabs out of the water. It's not deployed in Gemini but they have cutting edge voice synthasis in labs.",
          "score": 19,
          "created_utc": "2026-01-03 13:55:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgpk4u",
              "author": "Silver-Champion-4846",
              "text": "Is it the Chirp3 voices or something newer and better?",
              "score": 7,
              "created_utc": "2026-01-03 16:43:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgrl0m",
                  "author": "Jean_velvet",
                  "text": "Maybe, I'm talking about the voices in AI studio. You can literally start a podcast with 2 AI voices and interact yourself as a caller. It's really good",
                  "score": 8,
                  "created_utc": "2026-01-03 16:53:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfrcl9",
          "author": "shaakz",
          "text": "i would suggest echo-tts-base. Supports voice cloning and does it very well. Sub 12gb vram and way faster than realtime on a 5070ti.",
          "score": 4,
          "created_utc": "2026-01-03 13:46:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgl8n1",
          "author": "2_two_two",
          "text": "Iâ€™ve been working on a project using Kokoro-TTS. It works well and easy enough to use. Not sure about others but Kokoro lets you blend voices so that you can use the default or create your own. Now Iâ€™m working on pacing, pauses, and tuning so it doesnâ€™t sound like robots.",
          "score": 4,
          "created_utc": "2026-01-03 16:23:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhn62t",
              "author": "mister2d",
              "text": "Nice. I created my own local streaming TTS  project with Kokoro-TTS before discovering this one: https://github.com/eduardolat/kokoro-web\n\nI've been enjoying excellent local streaming tts for a few months without any issues. This tiny model is great.",
              "score": 2,
              "created_utc": "2026-01-03 19:17:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxrafeo",
          "author": "QikoG35",
          "text": "VibeVoice large, MIT license, with custom nodes! Mind blowing how authentic it sounds and open source.\n\nA reason Microsoft pull it back!",
          "score": 3,
          "created_utc": "2026-01-05 03:57:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxtwg75",
              "author": "bondaly",
              "text": "Is it still pulled back in any way?",
              "score": 1,
              "created_utc": "2026-01-05 15:25:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxx2xjl",
                  "author": "misterflyer",
                  "text": "No, what he meant is that they released the 7B version with an MIT license. Then Microsoft quickly deleted it from their HF repo *(not before it got forked tho)*. The full weights 7B model is still out there...\n\n[https://huggingface.co/aoi-ot/VibeVoice-Large](https://huggingface.co/aoi-ot/VibeVoice-Large)\n\n[https://github.com/rsxdalv/VibeVoice](https://github.com/rsxdalv/VibeVoice)",
                  "score": 2,
                  "created_utc": "2026-01-06 00:27:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxgy867",
          "author": "Noiselexer",
          "text": "Tell me your channel so I can block it please.",
          "score": 34,
          "created_utc": "2026-01-03 17:24:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxg18c0",
          "author": "imonlysmarterthanyou",
          "text": "I have been liking VoxCPM. You can clone whatever voice and tone you like.",
          "score": 3,
          "created_utc": "2026-01-03 14:43:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxil630",
              "author": "bassgojoe",
              "text": "VoxCPM 1.5 gave me much higher quality results compared to chatterbox, Iâ€™m surprised itâ€™s not more well known.",
              "score": 2,
              "created_utc": "2026-01-03 22:03:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxgyph4",
          "author": "GabryIta",
          "text": "Chatterbox?",
          "score": 3,
          "created_utc": "2026-01-03 17:26:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhblay",
          "author": "hz55555",
          "text": "Check out inworld.  Much cheaper https://artificialanalysis.ai/text-to-speech/leaderboard",
          "score": 3,
          "created_utc": "2026-01-03 18:24:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhlaxd",
          "author": "therealtonyryantime",
          "text": "Anyone have a good way of automatically filtering out YouTube channels like this?",
          "score": 14,
          "created_utc": "2026-01-03 19:08:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxiksq3",
              "author": "ScoreUnique",
              "text": "Can be an indexed db of videos as a chrome extension, giving away idea for a business btw",
              "score": 6,
              "created_utc": "2026-01-03 22:01:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxfjrp7",
          "author": "Impressive-Sir9633",
          "text": "1. You can try the free unlimited KokoroTTS at https://freevoicereader.com or the Chrome extension. You will have to download the model and everything is processed within your browser (using webGPU). I prefer KokoroTTS, but you can try Supertonic as well.\n\n2. If you want additional voice options, you can try the paid version that has a free 3 day trial.",
          "score": 6,
          "created_utc": "2026-01-03 12:59:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxf8u5f",
          "author": "shanehiltonward",
          "text": "Check out the projects on Pinokio.",
          "score": 5,
          "created_utc": "2026-01-03 11:35:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxft8f7",
          "author": "Head-Leopard9090",
          "text": "Vibevoice large",
          "score": 2,
          "created_utc": "2026-01-03 13:57:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxh65vq",
          "author": "martinerous",
          "text": "In addition to the mentioned ones, I have also tried VoxCPM 1.5. It's quite fast, especially on nanovllm (tried on WSL2). BTW, I also trained VoxCPM to my native Latvian language (and Chatterbox too), and it was surprisingly easy to do with the bundled scripts. Of course, VoxCPM cannot beat VibeVoice quality, but VoxCPM seems more stable when provided a voice to clone, and they also have a protection logic built in to throw away obviously bad generations.",
          "score": 2,
          "created_utc": "2026-01-03 18:00:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkkkk1",
          "author": "rc_ym",
          "text": "I settled on Kokoro using a mixture of voices (using am\\_onyx to give it base).  Then post-processing the audio using [**pedalboard**](https://github.com/spotify/pedalboard) to add some more warmth and simulate a little room noise to make it sound more realistic.  Best speed and quality combo.  Chatterbox had the best quality but was super slow. \n\nI need to try vibevoice after reading the comments here.",
          "score": 2,
          "created_utc": "2026-01-04 04:34:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlr5p1",
          "author": "DocHollidaay",
          "text": "I was on ElevenLabs but they closed my account (ToS), I spent this week trying different TTS, Pinokio All-In-Ones and manual installs of the 'top 5' TTS, I'm using this to create audiobooks, but this still maybe of use:\n\n[https://github.com/rsxdalv/TTS-WebUI](https://github.com/rsxdalv/TTS-WebUI) is highly recommended, it supports a ton of TTS to try out, easy to install and updated often.\n\nI use Chatterbox 0.5B Multilingual, in TTS-WebUI it supports Nvidia Blackwell or thats the first time I got it working at max speed: I went from 35it/s (roughly realtime) to 210it/s (x6) on 5070 Ti and it does a better job than ElevenLabs!\n\nApparently the Multilingual version is better for flow and handles punctuation better than standard 0.5B, after listening to Tubro (sounds a bit compressed), new Turbo (sounds less compressed), standard (very good, accent drifts sometimes, struggles with 'novel punctuation').\n\nAny glitches tend to be something to do with the text sent, so it may need a cleanup step or very long sentences and chunk tweaking.\n\nI find the voice style comes from the training clip, it will match pacing etc.",
          "score": 2,
          "created_utc": "2026-01-04 10:25:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsdxep",
          "author": "Forward_Artist7884",
          "text": "cosyvoiceV3 is pretty good, you just need a voice sample. It's better than XTTSV2 and support style inputs.",
          "score": 2,
          "created_utc": "2026-01-05 09:02:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxj6an0",
          "author": "SuperCaptainMan",
          "text": "AI slop YouTube channel?",
          "score": 5,
          "created_utc": "2026-01-03 23:51:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgct6o",
          "author": "Virtamancer",
          "text": "Has anyone made a longform TTS gui solution yet (local)?\n\nMy use case is creating audiobooks for myself when they arenâ€™t available for free.\n\nI donâ€™t care about cloning voices or other gooner shit, I just want to be able to feed a chapter or even a whole book and have it generate the audio file.",
          "score": 2,
          "created_utc": "2026-01-03 15:43:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxglin4",
              "author": "DigiJoe79",
              "text": "Hi u/Virtamancer \\- You can try my audiobook-maker. In the latest version, there is also a prebuild VibeVoice container with 1.5B and 7B support. [https://github.com/DigiJoe79/AudioBook-Maker](https://github.com/DigiJoe79/AudioBook-Maker)",
              "score": 4,
              "created_utc": "2026-01-03 16:24:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgmyyi",
                  "author": "Virtamancer",
                  "text": "At a quick glance this looks very promising, and excellent documentation! I canâ€™t wait to get to my computer and try it.\n\nA couple questions:\n\n- How much vram is needed?\n\n- How long would, say, a 300 page book take to do TTS? Using 1x or 2x RTX 4090.",
                  "score": 1,
                  "created_utc": "2026-01-03 16:31:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxno49h",
                  "author": "gallito_pro",
                  "text": "Hi, thanks for your app, but Engines dont load at the start!!! Help please. Can be related to nvidia drivers?",
                  "score": 1,
                  "created_utc": "2026-01-04 17:24:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxgxbsn",
              "author": "evia89",
              "text": "> My use case is creating audiobooks for myself when they arenâ€™t available for free.\n\nI use edge for it. OG https://edgetts.github.io/ or my fork with multiple voices https://vadash.github.io/EdgeTTS/\n\nBoth opensource u can fork and edit if u need more features\n\nCan do 40h book in ~2 hours",
              "score": 1,
              "created_utc": "2026-01-03 17:20:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxh4sgh",
                  "author": "Virtamancer",
                  "text": "Hmm, doesn't work for me.\n\n    [00:00:00] Loaded: textfile.txt\n    [00:00:00] Saving to: TTS Audiobooks\n\nIt asks me to select the output directory, and then I get that output but no files ever actually appear there.",
                  "score": 1,
                  "created_utc": "2026-01-03 17:54:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxjxlx3",
          "author": "dannydonatello",
          "text": "How is a 10 minute video too expensive with elevenlabs? Should be no more than 2-3 USD max.",
          "score": 2,
          "created_utc": "2026-01-04 02:19:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxkxpy5",
              "author": "Buzzard",
              "text": "You're not thinking with AI.  You're assuming they care about the content and put lots of work into the video, so comparatively $2-3 seems really small.\n\nTo them, $2-3 is unsustainable because they haven't put any effort at all.",
              "score": 7,
              "created_utc": "2026-01-04 06:07:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlguve",
                  "author": "ReachingForVega",
                  "text": "And the intent is probably to only make a couple dollars in views per slop vid.Â ",
                  "score": 3,
                  "created_utc": "2026-01-04 08:52:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxk3mok",
              "author": "shadowninjaz3",
              "text": "thats pretty expensive considering inworld and fish audio are 10x cheaper than eleven labs",
              "score": 1,
              "created_utc": "2026-01-04 02:53:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlqnfh",
                  "author": "dannydonatello",
                  "text": "I just think itâ€™s BS that 2-3 USD per video is making his YouTube project unsustainable.",
                  "score": 2,
                  "created_utc": "2026-01-04 10:21:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfllid",
          "author": "rom16384",
          "text": "Have you tried Gemini 2.5 Pro Preview TTS? Choosing one of their deeper voices and setting the tone via a prompt should give good results.",
          "score": 1,
          "created_utc": "2026-01-03 13:11:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgpp2u",
              "author": "Silver-Champion-4846",
              "text": "That got closed on free tier recently, even the flash version became unavailable in the google ai studio website!",
              "score": 2,
              "created_utc": "2026-01-03 16:44:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxgw9l5",
          "author": "nabuachaem",
          "text": "for me, pip TTS and use the XTTS v2",
          "score": 1,
          "created_utc": "2026-01-03 17:15:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgz2ok",
          "author": "Spare-Object3993",
          "text": "You should try gradium",
          "score": 1,
          "created_utc": "2026-01-03 17:28:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhml0h",
          "author": "HotDoshirak",
          "text": "Chatterbox + FlashSR",
          "score": 1,
          "created_utc": "2026-01-03 19:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhvdp1",
          "author": "hidden2u",
          "text": "My current workflow is maya1 to generate a unique voice sample, then clone it with chatterbox audiobook",
          "score": 1,
          "created_utc": "2026-01-03 19:56:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhvhoo",
          "author": "arthurtully",
          "text": "Edge TTS probably the best solution.",
          "score": 1,
          "created_utc": "2026-01-03 19:56:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxif1ij",
          "author": "HelpfulHand3",
          "text": "For paid options, Inworld with their Max tts model is in my opinion better than ElevenLabs 2.5 and is 10x cheaper. The value for their service is quite frankly absurd.\n\n[https://inworld.ai/pricing](https://inworld.ai/pricing)\n\nLocal models.. Higgs Audio V2, Echo TTS, Vibevoice.",
          "score": 1,
          "created_utc": "2026-01-03 21:33:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxj09i3",
          "author": "llamabott",
          "text": "I feel compelled to plug my own app here:\n\n[https://github.com/zeropointnine/tts-audiobook-tool](https://github.com/zeropointnine/tts-audiobook-tool)\n\nIt has support for eight (!) different TTS models.\n\nMiraTTS\n\nGLM-TTS\n\nIndexTTS2\n\nVibeVoice 1.5B\n\nHiggs Audio V2\n\nFish OpenAudio S1-mini\n\nChatterbox-Multilingual\n\nOute TTS\n\nOkay thanks.",
          "score": 1,
          "created_utc": "2026-01-03 23:20:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjhfdu",
          "author": "jjsilvera1",
          "text": "I say maybe because I dont know, but google docs has quite a few good voices on there. One might be interesting? You could also mess with audio setting like pitch, timbre.",
          "score": 1,
          "created_utc": "2026-01-04 00:50:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjxfgm",
          "author": "pbalIII",
          "text": "The voice cloning angle is key for your use case. VibeVoice and F5-TTS both support cloning from audio samples, so you could grab a few clips of the Marcus voice you like and use that as your reference.\n\nF5-TTS is probably the sweet spot... 330M params, solid quality, and the cloning is legit good. Kokoro is faster but doesn't do voice cloning, so you'd be stuck with its built-in voices.\n\nOne thing worth trying: feed it a 10-15 second sample of the exact ElevenLabs output you want to match. The closer your reference audio is to the target tone, the better the clone. I'd start with F5 before going to VibeVoice 7B since the VRAM requirements are more reasonable.",
          "score": 1,
          "created_utc": "2026-01-04 02:18:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkpm0o",
          "author": "Unlikely_Shake8208",
          "text": "Chatterbox is the best local TTS that I have used.",
          "score": 1,
          "created_utc": "2026-01-04 05:08:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlffo4",
          "author": "crantob",
          "text": "Which tts will let me apply a graph of prosidy/emphasis to the text?  I need to recreate an original speech with similar timing, pitch, emphasis.",
          "score": 1,
          "created_utc": "2026-01-04 08:39:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlp6tz",
          "author": "k2ui",
          "text": "How much does eleven labs cost for you to do 8-10 min content?",
          "score": 1,
          "created_utc": "2026-01-04 10:08:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxmkflo",
          "author": "MoffKalast",
          "text": ">  long-form content\n\n>  8-10 min",
          "score": 1,
          "created_utc": "2026-01-04 14:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxoakcq",
          "author": "coastisthemost",
          "text": "Ttswebui is great",
          "score": 1,
          "created_utc": "2026-01-04 19:04:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqnkgv",
          "author": "MensaForever4117",
          "text": "Speechify",
          "score": 1,
          "created_utc": "2026-01-05 01:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxh5tld",
          "author": "jordanpwalsh",
          "text": "I've been working on this: [https://nemoreader.jordanwal.sh](https://nemoreader.jordanwal.sh)\n\nThe version up on the App Store now uses OpenAI which breaks the bank, I have a new version in testing now that uses Higgs V2 (https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base) running on runpod infrastructure and a simple API. My plan is to open source that bit once it's stable for folks to plug in that don't want to pay the in app tts, but I could share what I have now if you want to play with the API.\n\nIt sounds pretty good! perfectly acceptable for the audible type clone I've been messing with.",
          "score": 1,
          "created_utc": "2026-01-03 17:59:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxn1vpd",
          "author": "IronColumn",
          "text": "you should try to do a good job with the things you create",
          "score": 1,
          "created_utc": "2026-01-04 15:41:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzt1q8",
      "title": "LLM server gear: a cautionary tale of a $1k EPYC motherboard sale gone wrong on eBay",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pzt1q8/llm_server_gear_a_cautionary_tale_of_a_1k_epyc/",
      "author": "__JockY__",
      "created_utc": "2025-12-30 20:36:46",
      "score": 196,
      "num_comments": 81,
      "upvote_ratio": 0.93,
      "text": "or: selling high-end LLM server gear is more fraught with risk than I realized.\n\n### AI Disclosure\n\nThis was written entirely by hand on my laptop in Sublime Text with zero AI involvement. Shit, I didn't even use spell check. All mistakes are my own.\n\n### tl;dr \n\nDuring an \"Item Not As Described (INAD)\" dispute, eBay ALWAYS sides with the buyer until the very last steps of the case no matter what the circumstances, despite all evidence, and in the face of all immediately obvious reason, logic, and common sense. Except it makes perfect sense and you might not even lose your money. Allow me to elaborate.\n\n### The Sale\n\nRewind to October 2025 when I replaced the incumbent Gigabyte MZ33-AR1 Epyc Zen5 motherboard with a Supermicro H14SSL-N for my inference rig. Long story short: don't use Gigabyte motherboards for 4-way Blackwell GPU setups unless sado-masochism is your thing. Anyway, I sold it to a seemingly nice chap on eBay for $900. He seemed a bit clueless about Epyc and compatibility issues, but we exchanged messages and he decided to go ahead with the \"no returns\" purchase of the as-new MZ33-AR1.\n\nOriginal box. All the case candy. As new. Undamaged. Fully working. With hi-res photos (taken on a Nikon D7000 with Nikon 17-55 f2.8 glass and processed in Capture One Pro) of all areas of the motherboard and CPU socket. This is important. \n\n### The Buyer\n\nFast forward a week or so: buyer hits me up with a bunch of Dr Debug codes (although he doesn't know they're Dr Debug codes, he just pulled \"error codes\" from the BMC) claiming the motherboard won't boot. I did him the solid of explaining Dr Debug and I provided a link to an explanation of the codes (https://forum.level1techs.com/t/list-of-dr-debug-bios-codes/114364). He was having issues with CPU initialization. I told him that sometimes re-seating CPU and RAM can help with these sorts of issues.\n\nRe-seating. This is also important.\n\nNext day he hits me up again: will I accept a return? No, because having installation difficulties is not a valid reason for return. Then nothing. Silence.\n\n### The Refund Claim\n\nCue the *very last day of the return window*: I get hit with an \"item not as described\" refund claim. Get this, the buyer:\n\n- uploaded photos of the motherboard with a bent and twisted CPU pin.\n- uploaded a photo of a blank white silkscreen rectangle on the motherboard with a giant red arrow pointing to it and a comment saying \"the motherboard is fake because of this white area\".\n- showed a photo of the computer monitor displaying the BMC interface in which the serial number of the BMC software was 1234567890ABCDEF. He claimed therefore the motherboard was a fake.\n\nWTF. I simultaneously exploded with rage at being accused of selling broken gear as working gear, while exploding with incredulity at the stupidity of trying to assert both damage AND blatantly ridiculous fakery in the same refund claim! My dude should have really picked just one fraudulent claim to keep it somewhat realistic, not two. I calmed down and figured the buyer probably bent the pins in a ham-fisted attempt to re-seat everything. No problem, I thought. I'll explain to eBay what's happening and they'll see reason before shutting this clown down. So I started going through the claim dispute process...\n\n### The Process\n\n...oh, the process. It's designed to (a) refund the buyer at the seller's cost in all cases, (b) be so egregiously demoralizing, time-consuming, and administratively difficult for sellers that they are incentivized to simply give up and accept the fleecing, and (c) automate as much of this process with as few humans in the loop as possible while simultaenously providing as few opportunities as possible for sellers to initiate any communication with eBay.\n\nIt went like this over a period of TWO MONTHS:\n\n- Report the buyer for \"abusing the returns process\".\n- With the new \"case\", it's possible to upload a set of photos and a block of text to refute the buyer's claim(s). \n- I uploaded ALL the hi-res photos I took for the listing's photoshoot in which it was abuntandly clear the motherboard was in perfect condition.\n- I also went to Gigabyte and found the page on the BMC's usermanual containing a screenshot showing the same serial number claimed by the buyer.\n- I went to Gigabyte's MZ33-AR1 web page and found a photo of the motherboard showing exactly the same white rectangle the buyer had called out as fakery.\n- Boom! Done! Solid documentary refutation of all the buyer's claims. Case closed. So I thought.\n- eBay found in favor of the buyer and instructed me to issue a return label.\n- I refused, outraged. No, I said. Look at the photos! He's lying!\n- eBay sent the buyer a label at my expense. He returned the motherboard with its busted CPU pin.\n- I again reported the buyer, showed photos of before and after damage, clearly showing he did the damage, not me.\n- eBay found in favor of the buyer AGAIN and deducted the full cost of the refund from my account.\n- Apoplectic, I hit the \"appeal\" button. I was taken to a webpage that said \"we'll call you in 3 minutes\". WTF?\n- 5 minutes later i got a call from eBay. \n- After briefly explaining the situation to a very engaged US-sounding representative, she told me I needed to do a couple of things:\n\t- Take the text of an email they just sent me (a Disclosure where I swear everything I told eBay is true) and paste it into a Word doc\n\t- Insert a photo/picture of my ink-written signature (luckily I have a scan of exactly that for business reasons).\n\t- Convert to PDF and upload to the secret link in the email they sent.\n\t- No joke, the lady actually stayed on the phone while I did all this! She received the PDF just seconds after I uploaded it.\n\t- This is, I am sure, mostly just another way of making it difficult to actually reverse the appeal.\n- But the rep was good to her word: eBay immediately reversed the decision and the money is back in my account as if the sale had happened like normal. I guess both me and the buyer got our money.\n\n### If It Happens To You\n\nMy advice if this happens to you: \n\n- Accept that no human cares about your case until the very, very last minutes of MONTHS of effort.\n- Accept that no matter what you do eBay will always automatically find in favor of the buyer.\n- Document everything contemporaneously and upload everything you possibly can when given opportunity to do so; you won't get any opportunities to do so again.\n- The data you upload is designed only for the human at the end of the appeals process, not someone looking at it during the claim process. Make it good. You'll need it later.\n- You're going to get enraged because during the claims process \"nothing makes sense\". It all makes sense: it's simply the cheapest way for eBay to handle this process at scale. Keep going.\n- Eventually eBay will find in favor of the buyer and close the case, automatically refunding the buyer \"on your behalf\". You will lose your money.\n- At this point you get the chance to appeal. BE READY. *This is the shot you've been waiting for all this time!* Have your phone, your laptop, your scanned signature, and a way to make PDFs ready BEFORE you initiate the \"call me\" feature.\n- Calmly explain what happened and request that common sense prevail. Ask that they refund your money. Common sense may actually prevail, assuming you made a good contemporaneous case with solid photographs, etc... and assuming you presented it well (not Mr Angry) on the phone... oh, and provided you can make and upload a PDF of your signature on-the-fly during the call!\n\nGood luck!\n\nEdit: please stop sending DMs asking for the eBay handle of the buyer. I'm not in the business of doxxing anyone. Thank you.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzt1q8/llm_server_gear_a_cautionary_tale_of_a_1k_epyc/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwt9lqp",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 22:30:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwspnh9",
          "author": "ismaelgokufox",
          "text": "My god. Being a seller in eBay is no joke.",
          "score": 78,
          "created_utc": "2025-12-30 20:55:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsrr86",
              "author": "blbd",
              "text": "Actually I think this data suggests that not only is it a joke, but that even an honest seller is the butt of it.Â ",
              "score": 50,
              "created_utc": "2025-12-30 21:05:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwv90k1",
              "author": "Guinness",
              "text": "Yep. eBay literally helped a scammer in South America steal my DSLR camera. The only reason I had my case reversed was because I had a quant friend from my days at an HFT firm end up working for a company that got absorbed by eBay. \n\nHe saved my ass.",
              "score": 7,
              "created_utc": "2025-12-31 05:30:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwuzqjv",
              "author": "az226",
              "text": "Being a buyer is no joke either went they send you faulty stuff. Had a seller sell me good GPUs and trusted him. Return window passed and then he had started sending me bent pins GPUs. 9 in total. Like $7k. \n\nAnother seller here on reddit Rhino/Core4 sold me GPUs with bad memory. I think 11 of 31 were bad or something. $8k. Refused to believe it, claimed I didnâ€™t know what I was doing. Scam of a company. They also sell on eBay. Avoid at all costs. They offer was something like $200 store credit if I returned the GPUs. Hah. Morons. Then they got their feelings hurt and took back that shit offer. When I have time and energy Iâ€™ll take them to small claims court. Losers.",
              "score": 7,
              "created_utc": "2025-12-31 04:26:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsto3p",
          "author": "MrPecunius",
          "text": "This is consistent with the experience I had selling a three channel stepper motor driver board on Ebay many years ago. Obvious buyer-inflicted damage, easily seen in photos etc.\n\nJust like your case, Ebay refunded the fraudster and I ended up keeping my money too. I stopped selling on Ebay after that.",
          "score": 34,
          "created_utc": "2025-12-30 21:14:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuhpri",
              "author": "night0x63",
              "text": "(I don't have the whole story... I just use eBay casually as buyer and little selling. )\n\n\nI think in the beginning of eBay before Amazon and other online stuff... Buyers got screwed by fraudsters all the time. eBay still made good money.\n\n\nThen Amazon came... Buyers all moved there because no more hours of dealing with fraudster sellers. eBay core business at risk.\n\nThen more recently last approximate five ten years... eBay pendulum has swung heavily... Now eBay competes with Amazon and so heavily favors buyers. So your story and this OP story show eBay favoring buyers. because eBay competes with Amazon.\n\n(Again I don't have all the story... This is just my experience and observation.)",
              "score": 5,
              "created_utc": "2025-12-31 02:35:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3lvrj",
                  "author": "maz_net_au",
                  "text": "Amazon isn't much better. Search for scams where people receive a bag of sand / rocks which are the same weight as the original item. Amazon processes it's returns based on the weight of the item if its in shrink-wrap, even if the original item didn't come in shrink-wrap.\n\nAnyway, good luck getting your money back with a dispute \"Instead of an item, I received a bag of sand.\" Some dude years ago filming himself unboxing what was meant to be a very expensive digital camera only to be on the receiving end of this crap and Amazon only refunded their money after public backlash (a video of unboxing the item is apparently not evidence enough).",
                  "score": 2,
                  "created_utc": "2026-01-01 16:35:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwujr05",
              "author": "__JockY__",
              "text": "Shame, really. But.... I too will avoid selling high value items on eBay in future, I have no desire to burn so many hours on pointless stress.",
              "score": 3,
              "created_utc": "2025-12-31 02:47:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwv1w1g",
                  "author": "jeffwadsworth",
                  "text": "This.  It just isn't worth the stress at all.",
                  "score": 2,
                  "created_utc": "2025-12-31 04:40:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwt31br",
          "author": "a_beautiful_rhind",
          "text": "I stopped selling on ebay because the customers have carte blanche to rip you off. He could have sent you a brick and you'd still have to contact a freaking US rep to get any traction.\n\nI think big time sellers just eat the fraudsters as part of doing business, but as an individual, you really can't. You barely make anything after the fees as it is and ebay seems to think mailing things back and forth is free.\n\nIts an absolute miracle you were able to keep your money at all.",
          "score": 21,
          "created_utc": "2025-12-30 21:58:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwst6ps",
          "author": "HelpingForDoughnuts",
          "text": "Man, the part where you had to stay on the phone making a PDF with your scanned signature while the rep waitedâ€¦ thatâ€™s peak â€œwe made this intentionally annoying so people give up.â€ Kafka would be proud.\nI went through something similar selling a 3090 a couple years back. Buyer claimed it was â€œartifactingâ€ and sent photos that were clearly just him running Furmark with the OC slider maxed. eBay sided with him, I got the card back with thermal pads missing (???), and I justâ€¦ gave up. Didnâ€™t know about the appeal phone call thing. Wish I had.\nThe real lesson here is what you said about documentation. Photos arenâ€™t just for the listing, theyâ€™re evidence for the trial you donâ€™t know is coming yet. I photograph serial numbers now too after getting burned.\nAlso lol at the buyer trying to claim BOTH damage AND fakery. Pick a lane my dude.\nSidebar: howâ€™s the Supermicro treating you with the Blackwell setup? Been eyeing the H14 boards but havenâ€™t pulled the trigger.",
          "score": 32,
          "created_utc": "2025-12-30 21:11:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsugl9",
              "author": "__JockY__",
              "text": "Thanks!\n\nThe Supermicro has been flawless. It booted first time, all four 6000 Workstation GPUs worked at PCIe 5.0 x16 (using MCIO shenanigans), and itâ€™s just been great. Love it.\n\nThe MZ33-AR1 was never designed for the large BAR of the Blackwells and Gigabyte support kinda just threw up their hands and gave up on it.",
              "score": 9,
              "created_utc": "2025-12-30 21:17:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwtgp8q",
                  "author": "HelpingForDoughnuts",
                  "text": "Four 6000s at full x16 is wild. The MCIO routing on those H14 boards is underratedâ€”people sleep on Supermicro because itâ€™s not â€œenthusiastâ€ but they actually engineer for this stuff.\nGigabyte throwing up their hands on large BAR is frustrating but not surprising. Their EPYC boards feel like an afterthought compared to their consumer stuff. At least you got most of your money back on the sale.\nWhatâ€™s the use case for the rigâ€”inference serving, training, or a mix? That much Blackwell horsepower has to be doing something fun.",
                  "score": 2,
                  "created_utc": "2025-12-30 23:07:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwu2603",
                  "author": "segmond",
                  "text": "The mz33-ar1 is fincky, even with 3090s, I occasionally have issues with it.  I hate the damn board.",
                  "score": 1,
                  "created_utc": "2025-12-31 01:05:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsxzx4",
          "author": "MelodicRecognition7",
          "text": "...and on top of that Ebay charges sellers 20% fees lol. Unfortunately it is too large and we do not have better alternatives, all other marketplaces are drop in the ocean, nobody will see your listings there.",
          "score": 13,
          "created_utc": "2025-12-30 21:34:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsub66",
          "author": "Aggressive-Bother470",
          "text": "Signed affidavit because they couldn't be bothered to check your evidence? :D\n\n\nOr... they now believe all photo evidence is null and void.",
          "score": 10,
          "created_utc": "2025-12-30 21:17:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwulha9",
              "author": "__JockY__",
              "text": "I've thought about it a bit now, and my guess is that below a certain dollar value no human ever actually takes more than a cursory glance at the case data (and only at the final appeal stage). It's quicker and simpler to just require the affidavit for CYA purposes, refund the money, and move on. It minimizes human hours, covers eBay in the case of later litigation, and would appear to be the least bad option available to eBay for this particular eventuality. \n\nOf course, it also incentivizes eBay to make the sellers' lives as miserable as possible in these situations, but I guess that's the cost of efficiency, eh?",
              "score": 3,
              "created_utc": "2025-12-31 02:57:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt1q8o",
          "author": "Purple-Programmer-7",
          "text": "I have 4x 3090s to sell. Iâ€™ll happily let them sit and depreciate vs trying to sell them via eBay",
          "score": 9,
          "created_utc": "2025-12-30 21:52:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt3cne",
              "author": "a_beautiful_rhind",
              "text": "try craigslist locally and just meet at a police station or other such place. as a bonus, no taxes.",
              "score": 3,
              "created_utc": "2025-12-30 21:59:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwtzbgc",
              "author": "David_Delaune",
              "text": ">I have 4x 3090s to sell. Iâ€™ll happily let them sit and depreciate vs trying to sell them via eBay\n\n\nI've had really good experience so far with buying and selling on /r/homelabsales",
              "score": 3,
              "created_utc": "2025-12-31 00:48:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtzgro",
                  "author": "Purple-Programmer-7",
                  "text": "Got another recommendation for that previouslyâ€¦ I think thatâ€™s where Iâ€™ll land when I get around to it!",
                  "score": 3,
                  "created_utc": "2025-12-31 00:49:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwunyvp",
              "author": "__JockY__",
              "text": "3090s will sell themselves on any market right now! I still like CL because once you get past the \"is the item still available\" type scams, meeting in person for cash is still king.",
              "score": 3,
              "created_utc": "2025-12-31 03:12:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx3m6yn",
              "author": "munkiemagik",
              "text": "Totally understand that sentiment, I've already spent the money so I accept it as gone from my accounts and have mentally written it off. So I'd rather let it depreciate in my possession than let some lowlife benefit from it on my dime.\n\nAs I mentioned to someone in here in another comment, I keep uuhming and aahing about whether to offload my hobby LLM server and move on to something else for a bit, like maybe grab myself a new e-mtb, (those Megamo's are looking bloody appealing) and retire the old faithful human powered mtb, OR double down and invest further into the rig for moaar power to re-excite me again. I havent fired up the LLM server in almost two weeks, granted Christmas with the siblings/cousins/nephews/nieces was hectic.\n\nIf selling wasn't so fraught with danger, in a moment of weakness I might have already jumped off the LocalLLama ship.",
              "score": 1,
              "created_utc": "2026-01-01 16:36:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsvasa",
          "author": "munkiemagik",
          "text": "I'm so glad this worked out for you and you didnt lose your money and a working product. But its horrific stories like this (usually without the happy ending) that keep me far away from ebay when it comes to selling things. I know this restricts my buyer exposure but I only do local facebook marketplace so the buyer can check and verify item function before parting with their cash. \n\nI sold my old 4090 to fund some 3090s for the LLM server and the buyer actually took a two and a half hour train to come down and buy the GPU in person because I refused to post it, though I did set the price a little lower than what was going market rate, I just wanted rid of it so I could start populating the LLM server with the proceeds.\n\nBut lately I've been contemplating offloading my Zen2 Threadripper Pro machine, I havenâ€™t been lurking in r/LocalLLaMA recently as much as I used to. But too scared to deal with the scammy public so instead of selling I might just double down on my investment and misery and upgrade it to Zen3 Threadripper Pro. If I could just find a 5965WX at a reasonable price I could almost double my system memory bandwidth.",
          "score": 8,
          "created_utc": "2025-12-30 21:21:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtizyu",
              "author": "UniversalSpermDonor",
              "text": "Oddly enough, I'm actually about to sell a 5965WX off - let me know if you're interested. (That said, unless you're willing to come to the outskirts of the DC area, I'll have to ship it to you.)",
              "score": 2,
              "created_utc": "2025-12-30 23:19:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtw26j",
                  "author": "munkiemagik",
                  "text": "I'm in the UK, by DC I take it you mean District of Columbia?",
                  "score": 1,
                  "created_utc": "2025-12-31 00:31:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsqwgx",
          "author": "Marksta",
          "text": "Holy moly dude, that's exactly why I just don't with tech stuff and reselling. I think all marketplace users kinda know the gambit at this point for bad faith craziness. But it's so much easier on buyer side because they don't even need any rep, they just click buy and can take an honest seller for a ride from there. Especially with all the GPU chip stealing off PCB stuff going on too.\n\nI've seen the warnings on ebay listings and I think Newegg too? They 1000% don't take a refund on new EPYC boards once you pop off the CPU slot cover(s) since people just can't help themselves but break $1k mobos it seems ðŸ˜‘\n\nSorry that happened to ya, glad you persisted down the 100 steps process to arrive at a refund.",
          "score": 5,
          "created_utc": "2025-12-30 21:01:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwunmyy",
              "author": "__JockY__",
              "text": "I learned the hard way that those warnings don't mean shit. \n\nAny one of us could buy one of those Newegg EPYC motherboards, take a shit on it, claim it was \"not as advertised because it came covered in poo\", and we'd get an automatic full refund from eBay.",
              "score": 1,
              "created_utc": "2025-12-31 03:10:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt3rfl",
          "author": "tengo_harambe",
          "text": "I sold my old 4060 Ti on fb marketplace recently. He reached out, I sent him a video of the GPU to prove it worked. We arranged to meet in a public spot the next day and he was on time with the exact amount of cash we agreed on. Made the exchange and went our separate ways. Never heard a peep from him again. 10/10 selling experience. May you always have a buttery smooth 100 fps Fornite experience, Randy!",
          "score": 5,
          "created_utc": "2025-12-30 22:01:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuo30t",
              "author": "__JockY__",
              "text": "This is the way.",
              "score": 1,
              "created_utc": "2025-12-31 03:12:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtagyw",
          "author": "RevolutionaryLime758",
          "text": "I sold a monitor to a guy, he didnâ€™t want it so it he stomped on it and uploaded an image that showed his boot print. Sent me back e-waste and eBay stuck me with the bill. Of course, eBay keeps the fees while demanding you refund in full.\n\neBay wants its buyers to feel like itâ€™s Amazon, so they demand you as a seller provide the same level of service of the slave driving mega corporation. Guaranteed delivery windows, easy returns with minimal pushback, etc. basically makes it so you need to be running a business yourself to really stand a chance at absorbing the fraudulent returns.\n\nLike you, I was gaslit about the images, told a human reviewer looked at it despite a rejection of my appeal within 3 minutes. After seller fees and shipping I barely get any money from eBay but Iâ€™ve made them plenty. Unlike you, I was not calm about it. I sent angry emails and called every day. Eventually it worked, but I wouldnâ€™t be able to tell you what I said that did. Just kept calling angry.\n\nAs far as the enthusiastic sounding voice, I swear to god on one of the calls it was one of the GPT voices.",
          "score": 5,
          "created_utc": "2025-12-30 22:34:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsy8dp",
          "author": "Techngro",
          "text": "I had an experience like that many years ago on eBay. Turned me off. I'll buy things, but selling is out of the question. What we need is an online pawn/consignment shop.",
          "score": 4,
          "created_utc": "2025-12-30 21:35:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtidmn",
          "author": "RnRau",
          "text": "Complete aside from selling on ebay shenanigans... it felt good reading stuff written by a human. \n\nWhich feels weird and sad at the same time...",
          "score": 4,
          "created_utc": "2025-12-30 23:16:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtwfpv",
              "author": "__JockY__",
              "text": "Thank you, I feel the same way.",
              "score": 1,
              "created_utc": "2025-12-31 00:33:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt0dip",
          "author": "Actual__Wizard",
          "text": ">uploaded photos of the motherboard with a bent and twisted CPU pin. \n\nYep. I was thinking that the pins were going to be ruined the entire time I was reading it. So, they broke it and want their money back, and you're probably going to end up with a broken mother board.",
          "score": 3,
          "created_utc": "2025-12-30 21:45:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtdb56",
          "author": "ufrat333",
          "text": "Hah, selling on eBay, they froze our account with 15k to be paid out after three weeks of sales without any disputes, all tracking added., 100% positive feedback on 50 orso orders.\n\nNo reason, no way to contact them, or well - you can talk to an Indian who can do exactly nothing and ask how your family is doing. Just when we were finding a lawyer in Germany to drag them to court - 4 months later - I sent some linkedin messages to random employees about this intention - they suddenly released the funds but left the account blocked without any further communication. I guess if my LinkedIn didn't mention working at said company a lot of years prior nothing would have happened.",
          "score": 3,
          "created_utc": "2025-12-30 22:49:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtvp3y",
              "author": "__JockY__",
              "text": "Yeah Iâ€™m done as a seller.",
              "score": 1,
              "created_utc": "2025-12-31 00:29:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtdcul",
          "author": "fluffywuffie90210",
          "text": "I had almost this exact thing happen with a faulty motherboard that was sold as broken spares/repair, the guy tried to fix it, couldnt so then tried to claim it was item not as described, FOR A BROKEN ITEM. God ebay sided with him but i got some advice on how to appeal on the forum... and somehow ebay sided with me once it got the rep side of things. Was 150Â£ but still... has to be an ebay issue.",
          "score": 3,
          "created_utc": "2025-12-30 22:49:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtvtq5",
              "author": "__JockY__",
              "text": "Yup, itâ€™s just them protecting buyers at scale because itâ€™s ultimately better for business.",
              "score": 1,
              "created_utc": "2025-12-31 00:30:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtdmkm",
          "author": "Lesser-than",
          "text": "Yeah for simular reasons I never count on resale value of anything computer related anymore, I just consider it sunk cost when purchasing. If I upgrade and never see myself using it again I give it to someone who will. You just can not depend on people to figure things out by themselves, more cases than not only want to return it after they are sure they broke it.",
          "score": 3,
          "created_utc": "2025-12-30 22:50:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtw13i",
              "author": "__JockY__",
              "text": "This is what Iâ€™m hearing about. People just abuse eBay as a try-before-you-buy site, or they buy working gear and return it as not working after swapping out the parts for themselves.",
              "score": 1,
              "created_utc": "2025-12-31 00:31:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtune1",
          "author": "abnormal_human",
          "text": "Sucks. I bought a $1500 Epyc CPU that turned out to be bad and ended up stuck with it. Stopped doing this crap on eBay after that.",
          "score": 3,
          "created_utc": "2025-12-31 00:23:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtyrlk",
              "author": "__JockY__",
              "text": "Ugh.\n\nIn the opposite story, I bought a $1400 9B45 EPYC and it turned out to be perfect and an alternate SKU for the $10k+ 9755!!",
              "score": 1,
              "created_utc": "2025-12-31 00:45:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwu1iq5",
          "author": "FormalAd7367",
          "text": "I sold an expensive vintage Iwc watch on ebay and buyer claimed he received rocks.  He took my watch and my money.",
          "score": 3,
          "created_utc": "2025-12-31 01:01:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwu2ejt",
              "author": "__JockY__",
              "text": "I honestly donâ€™t know how these assclowns sleep at night after fleecing common people.",
              "score": 1,
              "created_utc": "2025-12-31 01:06:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwuuivp",
          "author": "SkyFeistyLlama8",
          "text": "Nice on ya for putting the \"no AI\" disclaimer\" at the start. I couldn't help reading out the entire text in a Law & Order voice LOL!",
          "score": 3,
          "created_utc": "2025-12-31 03:52:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv1esg",
          "author": "Intelligent-Form6624",
          "text": "> Shit, I didn't even use spell check.\n\nDude, this is some off-grid shit. Did you also write a letter using nothing but a pen? #NoWhiteOut",
          "score": 3,
          "created_utc": "2025-12-31 04:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt1usm",
          "author": "Caryn_fornicatress",
          "text": "Brutal but accurate lesson, eBay is optimized for buyer protection at scale, not truth or evidence\n\nHigh value hardware plus no returns is basically asking for this risk, the system only really listens at the human appeal stage\n\nYour takeaway about documenting everything for the final call is the real gold here, anything before that is just feeding a machine\n\nHonestly for gear like this, local sale or escrow based platforms feel way safer than eBay now",
          "score": 5,
          "created_utc": "2025-12-30 21:52:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwulx32",
              "author": "__JockY__",
              "text": "> Honestly for gear like this, local sale or escrow based platforms feel way safer than eBay now\n\nYup. Good ol' Craigslist and Starbucks and cash.",
              "score": 1,
              "created_utc": "2025-12-31 02:59:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt8hjg",
          "author": "alphatrad",
          "text": "I just went through this shit with some asshole myself when selling my Valve Index. Such a nit picky complaint and the whole thing is setup to punish the seller. The guy literally cost me 100 dollars as I offered free shipping and then had to pay to ship it back to myself.",
          "score": 2,
          "created_utc": "2025-12-30 22:24:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwupb4r",
              "author": "__JockY__",
              "text": "I think the trick is to simply refuse to buy the return shipping label despite their outpouring of awful \"nice reputation you got there, shame if something happened to it\" emails. I forgot to mention those in the main story, but they tried to intimidate me into sending a return label under threat of reputational damage.\n\nI told them to pound sand.\n\neBay still sent the return shipping label and billed me for the convenience, but at least this way I didn't accept the return/refund and scored a small \"fuck you\" victory in their bullshit game.",
              "score": 3,
              "created_utc": "2025-12-31 03:20:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwta1ql",
          "author": "LetterRip",
          "text": "How difficult would it have been to take the buyer and eBay to small claims court?",
          "score": 2,
          "created_utc": "2025-12-30 22:32:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtuwv9",
              "author": "__JockY__",
              "text": "Oh thatâ€™s 100% the route Iâ€™d have taken if the appeal failed. No idea of time, cost, or effort required though.",
              "score": 2,
              "created_utc": "2025-12-31 00:25:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtizwp",
          "author": "AnomalyNexus",
          "text": "Yeah selling on eBay is an act of faith. I try to keep things below 200 bucks just in case.",
          "score": 2,
          "created_utc": "2025-12-30 23:19:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtykps",
              "author": "__JockY__",
              "text": "I may just follow this advice. I donâ€™t have FB so no marketplace for me :/",
              "score": 1,
              "created_utc": "2025-12-31 00:44:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwu2ckh",
          "author": "segmond",
          "text": "I have so much gear that I wish to sell for someone that's into local LLM, but I don't want to go through this sort of ebay pain.  I wonder if \"no return/no refund\" will work.",
          "score": 2,
          "created_utc": "2025-12-31 01:06:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuq9ly",
              "author": "__JockY__",
              "text": "No, it absolutely does not work. \n\nMy listing was marked as \"no return/no refunds\" in the listing's settings and my story seems common after doing a bunch of research into this matter. Your listing settings are irrelevant because the instant a buyer files a \"not as described\" claim, the automatic process begins and they end up with a refund at your expense. There's no escape. You just have to prepare everything along the way for the moment of final human appeal and hope you laid your breadcrumbs well enough along the way to support a successful appeal.",
              "score": 2,
              "created_utc": "2025-12-31 03:26:08",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwug85f",
              "author": "Ryuma666",
              "text": "Where are you from?",
              "score": 1,
              "created_utc": "2025-12-31 02:26:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvc5i8",
                  "author": "segmond",
                  "text": "Michigan",
                  "score": 1,
                  "created_utc": "2025-12-31 05:54:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwu8kkm",
          "author": "Savantskie1",
          "text": "they automatically find in favor of the buyer because in the early days of Ebay, there were tons of scammers selling on Ebay and then either sending faulty devices, or ripping people off with sending a photo of the item, but not the original item with clever wording in the description that makes people think they are buying the item, when instead they're agreeing to buy a picture of the item. So the responsibility has been put on the sellers to prove their case and Ebay err's on the side of the buyer in protection of them. It solves the problems of people suing Ebay because they weren't doing enough to protect buyers.",
          "score": 2,
          "created_utc": "2025-12-31 01:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwur3ga",
              "author": "__JockY__",
              "text": "Yes, exactly this. The buyer could've taken a dump on the motherboard and sent it back \"not as described\" for all eBay cared. They'd still automatically refund him out of my account. And by making it as difficult as possible for sellers to get to the moment of appeal, they reduce costs enormously.\n\nJust lay the groundwork for the moment of appeal... or don't sell expensive stuff on eBay, I guess.",
              "score": 1,
              "created_utc": "2025-12-31 03:31:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwuuayx",
          "author": "markcartwright1",
          "text": "Ok take a deep breath because it's over now! You were unlucky and you had a clumsy and clueless buyer. Its high stakes because it was a high value product too. And it turned into a drawn out ordeal.\n\nI've sold about 350 tech items on UK ebay, phones smartwatches laptops, mini PCs in the past 6 months. Generally new, but maybe 20-30% second hand. Maybe 1-2% have had issues, courrier issues mostly or sometimes a product was faulty. I often write about who its not for and who shouldn't buy a product - so that deters the unsuitable buyers. \n\nI offer and allow returns because it lifts the conversion rate and if someone changes their mind or they're not happy - they won't sabotage the product. I've kept away from the more expensive end of the market so the risk is spread across various products too. Ironically sometimes the returned products have sold for more than the original price. \n\nThe thing with Motherboards is they can be very fragile or very specific. Like some people might not know certain slots, or compatibility before buying. You may need to offer more hand holding. And this buyer was clumsy. \n\nIf anyone UK based here wants to sell any computer / LLM gear, drives, HDDs, SSDs, old phones - I can give you a price and arrange shipping / as well as safe payment with Paypal. Or we can run it through Ebay if you prefer. I'm good at selling bits of tech and have a cash pile to buy decent stock. I can save you from an ordeal like this. Drop me a DM and let me know what you're selling and what you'd like for it.",
          "score": 2,
          "created_utc": "2025-12-31 03:51:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv72r8",
          "author": "fred100002",
          "text": "I had the exact same experience a couple of times this year. In your write up, you mentioned reporting the buyer Â´againâ€™  but IIRC, in my examples, the system wouldnâ€™t let me report them twice and told me Â´you already reported the buyer for thisâ€™ â€¦ I also donâ€™t remember getting to an appeal button either.",
          "score": 2,
          "created_utc": "2025-12-31 05:16:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt46s1",
          "author": "No_Afternoon_4260",
          "text": "Why no 4 way blackwell on the gigabyte? Does it work on the supermicro?",
          "score": 1,
          "created_utc": "2025-12-30 22:03:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt5o3y",
              "author": "__JockY__",
              "text": "It throws PCI errors, only sees 3 cards, etc. But the supermicro has been flawless, I love it.",
              "score": 1,
              "created_utc": "2025-12-30 22:10:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwtbcr4",
                  "author": "No_Afternoon_4260",
                  "text": "wow good to know thx !  \nSo you did sell a non working board ðŸ˜…",
                  "score": 3,
                  "created_utc": "2025-12-30 22:39:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwui75o",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2025-12-31 02:38:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwutsb7",
          "author": "Sabin_Stargem",
          "text": "If I am to be perfectly honest, as a potential buyer of 2nd-hand gear, this situation sounds good for me.   I fear losing my money for a waste of time, and putting together an premium machine would seriously damage my savings even if everything went perfectly.\n\nO'course, your take on the situation is perfectly valid and reasonable.   Just saying that it makes me more seriously consider using Ebay for the next build.   Getting a top-end Threadripper for a much reduced price at less risk is incredibly appealing.\n\nIt is a perverse incentive, where greed and fear overrule fairness.   In any case, it is good for you to post a \"seller beware\" story, so that honest folks don't get deliberately ripped off by buyers.",
          "score": 1,
          "created_utc": "2025-12-31 03:48:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwymcx8",
          "author": "Phaelon74",
          "text": "Stop selling on ebay, it is horrible for sellers.",
          "score": 1,
          "created_utc": "2025-12-31 19:09:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9yspm",
          "author": "DevopsIGuess",
          "text": "I have this same motherboard, real PITA to setup. \nHe likely needs to run firmware upgrades via the BMC management  port. \n\nFeel free to send them my way. This board was a bitch to get working.",
          "score": 1,
          "created_utc": "2026-01-02 16:30:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxato8n",
              "author": "__JockY__",
              "text": "Send who your way?",
              "score": 1,
              "created_utc": "2026-01-02 18:53:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsz2wl",
          "author": "LyriWinters",
          "text": "you should obviously have taken the return...",
          "score": -8,
          "created_utc": "2025-12-30 21:39:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwurhoo",
              "author": "__JockY__",
              "text": "Had I accepted the return I'd be left with a broken motherboard instead of a mint condition motherboard worth $900. I'd have lost my money and arguably worse, I'd have backed down like a fucking pussy against a liar and a cheat. Nope.",
              "score": 2,
              "created_utc": "2025-12-31 03:33:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwuwekk",
                  "author": "LyriWinters",
                  "text": "Pretty sure he broke it to have a reason for ebay though... So you would have gotten your motherboard back. But it is just a presumption. But logic checks out.\n\nit worked out this time for you, next time probably not. you \"burned\" that bridge in the eyes of Ebay now. Sorry.   \n  \nThis is why I love Sweden, I can do a background check on the people I do business with. I can literally find adress, social security number, last declared salary, any time spent in the court system... And it all takes me roughly 5 minutes an costs me â‚¬1.",
                  "score": 1,
                  "created_utc": "2025-12-31 04:04:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q1986x",
      "title": "IQuestCoder - new 40B dense coding model",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/ilintar/IQuest-Coder-V1-40B-Instruct-GGUF",
      "author": "ilintar",
      "created_utc": "2026-01-01 17:12:51",
      "score": 189,
      "num_comments": 37,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q1986x/iquestcoder_new_40b_dense_coding_model/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nx5qmo1",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-01 23:10:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx47bkc",
          "author": "ilintar",
          "text": "BTW, the Loop version \\*is\\* a new architecture and will require adaptation.",
          "score": 56,
          "created_utc": "2026-01-01 18:25:04",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nx5fuf1",
          "author": "MutantEggroll",
          "text": "Thanks for the GGUF! Taking the IQ4\\_XS for a spin and so far it's performing very well.\n\n* Successfully zero-shotted a Snake game\n* Demonstrated good understanding of embedded Rust concepts\n* Hovering around 55% Pass 2 rate on Aider Polyglot, which puts it on-par with GPT-OSS-120B\n\nMy only issue is that it does not fit all that nicely into 32GB of VRAM. I've only got room for 28k context with unquantized KV cache. Once I finish my Polyglot run I'll try again with Q8 KV cache and see what the degradation looks like.",
          "score": 31,
          "created_utc": "2026-01-01 22:11:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx72bxe",
              "author": "rm-rf-rm",
              "text": "tests that are \"make x from scratch\" or any of the leaderboard benchmarks dont correlate well to real world performance where the majority use case is: within an existing codebase,: understands the codebase, makes a change that works, preserves architecture, preserves design patterns, preserves style.",
              "score": 12,
              "created_utc": "2026-01-02 03:53:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx79qpf",
                  "author": "MutantEggroll",
                  "text": "Agreed. I treat greenfield prompts and benchmarks as a pre-filter - models that do poorly are discarded, and those that do well move forward to real world use cases, where they get filtered again for low performance.\n\nWith the context size limitations on my hardware due to the size of this model, I'm tempering my expectations. Could be good for boilerplate code or small code reviews, but it just won't be able to hold enough of a real codebase in context to be a true workhorse.",
                  "score": 6,
                  "created_utc": "2026-01-02 04:42:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx5lsqc",
              "author": "ilintar",
              "text": "Interesting, those are very good numbers for an IQ4\\_XS on coding tasks.",
              "score": 10,
              "created_utc": "2026-01-01 22:43:15",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx6c0q7",
              "author": "FizzarolliAI",
              "text": "Interesting! I couldn't get it to behave well w/ tool calls at all, but I was trying the looping model in vLLM...",
              "score": 0,
              "created_utc": "2026-01-02 01:11:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4863c",
          "author": "mantafloppy",
          "text": "The model maker don't talk about what arch they used, and this dude quant it in Qwen2, sus all around.\n\nhttps://huggingface.co/cturan/IQuest-Coder-V1-40B-Instruct-GGUF",
          "score": 33,
          "created_utc": "2026-01-01 18:29:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4969k",
              "author": "ilintar",
              "text": "Basic model is basic Llama, loop model is nice new arch with dual (not hybrid) gated attention.",
              "score": 26,
              "created_utc": "2026-01-01 18:34:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx4d3bx",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -18,
              "created_utc": "2026-01-01 18:53:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4e3ap",
                  "author": "mantafloppy",
                  "text": "I'm calling IQuestLab/IQuest-Coder-V1-40B-Instruct sus, not OP.",
                  "score": 13,
                  "created_utc": "2026-01-01 18:58:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx4cizr",
          "author": "LegacyRemaster",
          "text": "Hi Piotr, downloading. Will test with a real c++ problem solved today with Minimax M2.1 . GPT 120, Devstral, GLM 4.7 --> they failed. Vscode + cline",
          "score": 29,
          "created_utc": "2026-01-01 18:50:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4gliw",
              "author": "LegacyRemaster",
              "text": "first feedback: 32.97 tok/sec on blackwell 96gb full context @ 450W.",
              "score": 20,
              "created_utc": "2026-01-01 19:10:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx66d2p",
              "author": "JonatasLaw",
              "text": "â€™m working on a project that involves creating shaders in C++. No current AI can help me even minimally. I put a `groupshared` inside a function (which obviously wonâ€™t work), ask GPT-5.2, Opus 4.5, Gemini 3, GLM 4.7, and Minimax 2.1 where the error is, and all of them fail. How do you work with C++ using AIs and actually get results? Do you use a specific kind of prompt? Because in my case theyâ€™re all 100% useless, they donâ€™t even work for repetitive tasks.",
              "score": 3,
              "created_utc": "2026-01-02 00:37:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8ccha",
                  "author": "LegacyRemaster",
                  "text": "I use Unreal Engine 5.7. All the C++ and backend code has the BPs converted to C++ for better performance. I think this helps. I won't deny that yesterday the 5.2 codex solved a problem for me that minimax didn't solve.",
                  "score": 2,
                  "created_utc": "2026-01-02 10:14:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxgi1ib",
                  "author": "ilintar",
                  "text": "Quick answer: you don't.  \nLong answer: the better AIs (Opus 4.5, Gemini 3) will help for simple tasks. But for complex C++ tasks you have to \\*tell them\\* what the problem is, then they can handle it. Best case, you tell them where to insert debug prints so they can figure something out.",
                  "score": 1,
                  "created_utc": "2026-01-03 16:08:17",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx53f5j",
              "author": "LegacyRemaster",
              "text": "https://preview.redd.it/4ooz7sdzysag1.png?width=864&format=png&auto=webp&s=1aeed92ab3719005a9f560266bf90af13d9694a0\n\nCline uses complex prompts and iterative task execution that may be challenging for less capable models.\n\n  \nTask : Fix errors --> \n\n  \n**Main issues observed:**\n\n1. **Missing type specifier / invalid declarations**\n   * `C4430`: missing type specifier (default-int not supported in C++)\n   * Indicates malformed or incomplete variable/function declarations.\n2. **Syntax errors around console command definitions**\n   * `C2146`: missing `;` before identifiers:\n      * `FakeBackend_ConsoleCommand`\n      * `FakeLogin_ConsoleCommand`\n   * `C2059`: syntax error on `stringa`, `)`, and `;`\n   * `C2143`: missing `)` or `;` before `{` or `}`\n3. **Function header / brace mismatch**\n   * `C2447`: missing function header, obsolete formal type list\n   * Strong indication of mismatched parentheses or braces.\n4. **Redefinition error**\n   * `C2086`: `FAutoConsoleCommandWithWorldDelegate` redefinition\n   * Suggests duplicate declaration caused by earlier syntax failure.\n\n  \nFailed. \n\n  \nNo problem with Minimax M2.1",
              "score": 6,
              "created_utc": "2026-01-01 21:07:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx5n1ax",
                  "author": "ilintar",
                  "text": "Minimax is a beast though, would be surprised if a 40B model, even if dense, would beat it.",
                  "score": 10,
                  "created_utc": "2026-01-01 22:50:02",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5enbm",
          "author": "bobeeeeeeeee8964",
          "text": "I just have a try, and it is clearly not good, it can not handle those task can solved by smaller and way more faster model like Qwen3-Coder-30B-A3B-Instruct or NVIDIA-Nemotron-3-Nano-30B-A3B. Save your time, don't use it.",
          "score": 11,
          "created_utc": "2026-01-01 22:05:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4ej94",
          "author": "Medium_Chemist_4032",
          "text": "Tried out this prompt:\n\n>Need to evaluate if youâ€™re smart. Write some compose file to run llama-swap that can swap to a vllm-ran model. Assume ubuntu host, docker is installed.\n\n[Response](https://pastebin.com/yszDVqch) is interesting. Not the brightest possible choices, but I didn't specify any, so ok.\n\n>**Overview**\n\n>This deployment provides an intelligent model swapping system that routes requests between LLM and vLLM services based on model type, with monitoring, health checks, and automatic failover.\n\n>Architecture\n\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚   Clients   â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”\n                    â”‚  Nginx      â”‚\n                    â”‚  Gateway    â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚            â”‚           â”‚            â”‚\n    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”\n    â”‚ LLM     â”‚ â”‚ vLLM    â”‚ â”‚ Model â”‚ â”‚ Prometheusâ”‚\n    â”‚ Service â”‚ â”‚ Service â”‚ â”‚Managerâ”‚ â”‚          â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n>Features Intelligent Routing: Automatically routes requests to LLM or vLLM based on model type Model Swapping: Hot-swap models without downtime Health Monitoring: Built-in health checks for all services Metrics & Logging: Prometheus + Grafana monitoring Load Balancing: Nginx load balancing with failover SSL/TLS: HTTPS support with auto-generated certificates",
          "score": 4,
          "created_utc": "2026-01-01 19:00:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4pyu0",
          "author": "ChopSticksPlease",
          "text": "Downloaded but didnt yet have time to fully test it against Devstral Small 2 and perhaps Seed OSS.\n\nHow much effort was it to build this model and how/where did you get the training data for coding?",
          "score": 5,
          "created_utc": "2026-01-01 19:57:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4dkdq",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 24,
          "created_utc": "2026-01-01 18:55:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4mgm9",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 22,
              "created_utc": "2026-01-01 19:40:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4x66o",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-01-01 20:34:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4t9pg",
                  "author": "lemon07r",
                  "text": "Thank you. I dont know how anyone buys into these obviously sham models",
                  "score": -3,
                  "created_utc": "2026-01-01 20:14:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx4o0ct",
              "author": "Available_Brain6231",
              "text": "so the small chinese ai companies starting copying openai... sad.",
              "score": -8,
              "created_utc": "2026-01-01 19:47:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4hipb",
          "author": "Cool-Chemical-5629",
          "text": "Model is too big for me to run on my hw, but I'd bet I have couple of prompts it would break its teeth on. It's especially tempting to prove since it claims to be on par with Sonnet 4.5 and much bigger models and my experience says that more often than not such claims are very false lol",
          "score": 4,
          "created_utc": "2026-01-01 19:15:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5ix52",
              "author": "-InformalBanana-",
              "text": "MOE ppl! Give us MOE! :)",
              "score": 1,
              "created_utc": "2026-01-01 22:27:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx4n12b",
              "author": "Inca_PVP",
              "text": "rip. yeah 40b is heavy af.\n\nhonestly for normal hardware just stick toÂ Llama 3 8B. if u grab theÂ Q4\\_K\\_MÂ quant it fits into 8gb vram and runs instant.\n\ni use it daily for python with a specific preset to keep it focused (less yapping). put my config on profile if u want a lightweight setup that actually runs locally.",
              "score": -5,
              "created_utc": "2026-01-01 19:42:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4pxih",
                  "author": "ttkciar",
                  "text": "Heavy is good, if it means improved competence.\n\nLooking forward to giving it a spin.",
                  "score": 2,
                  "created_utc": "2026-01-01 19:57:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx41h7o",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 5,
          "created_utc": "2026-01-01 17:56:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx42cdn",
              "author": "b3081a",
              "text": "As a coder model it's probably not focusing on general benches.",
              "score": 8,
              "created_utc": "2026-01-01 18:00:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx55r9t",
          "author": "FizzarolliAI",
          "text": "To go against what everyone else is saying, I actually think this model is really good!... At everything *but* programming. It sucks at programming. General insight tasks, writing, assistant-y stuff, etc. are great! Somehow!",
          "score": 5,
          "created_utc": "2026-01-01 21:19:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx619zn",
              "author": "IrisColt",
              "text": "Thanks for the insight!",
              "score": 1,
              "created_utc": "2026-01-02 00:09:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4k9tj",
          "author": "jinnyjuice",
          "text": "I'm assuming they're going to release the loop thinking model tomorrow, right?",
          "score": 1,
          "created_utc": "2026-01-01 19:29:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2o033",
      "title": "What is the smartest uncensored nsfw LLM you can run with 20GB VRAM and 24GB RAM",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/",
      "author": "Death_12_35_taken",
      "created_utc": "2026-01-03 07:04:18",
      "score": 186,
      "num_comments": 75,
      "upvote_ratio": 0.84,
      "text": "I am looking for something that can stay in character and be fast but also creative. I am looking for models that i can run locally and at decent speed. Just need something that is smart and uncensored. ",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nxeh4xh",
          "author": "Narrow-Belt-5030",
          "text": "There is a dolphin model that i use that may be of interest: Dolphin-Mistral-24B-Venice-Edition\n\nhttps://huggingface.co/dphn/Dolphin-Mistral-24B-Venice-Edition",
          "score": 85,
          "created_utc": "2026-01-03 07:38:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxele1u",
              "author": "Death_12_35_taken",
              "text": "just tried the quantized version of this and the answers are good but a bit slow and for some reason there is this waiting time between when i finish typing and the token generation more than usual plus the gpu coil is roaring. Maybe i am doing something wrong. I got text gen web UI should i use something else to run this model?",
              "score": 15,
              "created_utc": "2026-01-03 08:14:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxelpdb",
                  "author": "Death_12_35_taken",
                  "text": "but yeah this model is pretty good for who can run it properly. The responses are good quality.",
                  "score": 8,
                  "created_utc": "2026-01-03 08:17:23",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxex04p",
          "author": "Longjumping-Bee-6977",
          "text": "Pick one\nhttps://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard",
          "score": 27,
          "created_utc": "2026-01-03 09:55:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxi7x0k",
              "author": "IpppyCaccy",
              "text": "What do the T and R columns mean?",
              "score": 4,
              "created_utc": "2026-01-03 20:58:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxiykxw",
                  "author": "autoencoder",
                  "text": "T seems to be model type: Proprietary, Base, Finetune, Merge. R whether it is a reasoning model (thinks before answering).",
                  "score": 5,
                  "created_utc": "2026-01-03 23:11:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxgp3v0",
          "author": "RottenPingu1",
          "text": "Can I highjack the thread to ask the same question in a 70B model?",
          "score": 8,
          "created_utc": "2026-01-03 16:41:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgpy9f",
              "author": "Awwtifishal",
              "text": "Probably Llama-3.3-70B-Instruct-heretic-v2",
              "score": 3,
              "created_utc": "2026-01-03 16:45:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgsamy",
                  "author": "RottenPingu1",
                  "text": "Thank you. I've been using zerofata/L3.3-GeneticLemonade-Opus-70B but am always on the lookout for more options.\nI've run into several comments that say a 32B model is better for rp, a 70B like a sports car with a governor when used in that role.",
                  "score": 2,
                  "created_utc": "2026-01-03 16:56:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxjklaj",
          "author": "xoexohexox",
          "text": "Cydonia/Magidonia 24b by thedrummer and Dan's Personality Engine 24B compare favorbly to the frontier models IMO and they refuse nothing, not even NSFL. Only outdone by Claude which is expensive and excessively prudish. There's a little private benchmark I run where I ask a librarian for a book and I can't remember the title but I remember some things about it - those two almost always get the references. \n\nIf you hunt around on huggingface you'll find some decent franken-merges of those two models as well like bereavedcompound and weirdcompound 24b which are also very good.",
          "score": 6,
          "created_utc": "2026-01-04 01:07:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxfgv4p",
          "author": "IKarlMetherlance",
          "text": "[https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated](https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated)",
          "score": 12,
          "created_utc": "2026-01-03 12:38:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgcedf",
              "author": "MullingMulianto",
              "text": "surprised qwen runs on 24gb ram",
              "score": 0,
              "created_utc": "2026-01-03 15:41:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxi1cvu",
                  "author": "IKarlMetherlance",
                  "text": "oups 22gb vram with 32k context on i1-Q4\\_K\\_M + mmproj-f16 , so remove mmproj/reduce context/take lower quantization",
                  "score": 1,
                  "created_utc": "2026-01-03 20:25:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfbxav",
          "author": "Javanese1999",
          "text": "[https://huggingface.co/mradermacher/gemma-3-27b-it-abliterated-normpreserve-i1-GGUF](https://huggingface.co/mradermacher/gemma-3-27b-it-abliterated-normpreserve-i1-GGUF)\n\nfor starter just go with original gemma 3 with abliterated fine tune. Pick Iq4\\_XS for your hardware specification.",
          "score": 3,
          "created_utc": "2026-01-03 12:00:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxirw09",
              "author": "Ethrillo",
              "text": "Bump up for this. Using it for weeks now and its just so good.",
              "score": 2,
              "created_utc": "2026-01-03 22:37:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxkhkn7",
                  "author": "Javanese1999",
                  "text": "You right, this is best original gemma abliterated model with higher NatInt score (no.1) according to ugi benchmark.",
                  "score": 3,
                  "created_utc": "2026-01-04 04:14:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxn2kpm",
              "author": "_glimmerbloom",
              "text": "Abliterated Gemma is great. It will cheerfully explain how to make meth, assassinate the president, or commit a terrorist attack.\n\nIncluding context, it uses about 30GB of memory though.  Not sure how a smaller quant would perform.",
              "score": 2,
              "created_utc": "2026-01-04 15:44:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxwdfzr",
                  "author": "Lakius_2401",
                  "text": "It's designed to use SWA for cache compression. It will use a boatload of memory until you turn that on. Alternatively, quant the KV cache more.",
                  "score": 1,
                  "created_utc": "2026-01-05 22:16:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxeuoe2",
          "author": "FinBenton",
          "text": "I would say probably this https://huggingface.co/Naphula/Goetia-24B-v1.1-GGUF",
          "score": 3,
          "created_utc": "2026-01-03 09:36:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfc27n",
              "author": "tat_tvam_asshole",
              "text": "It's OK, but a bit 'dramatic'. 4.2.0-brokentutu q8 is by far the best model I've tried. Really exists in a sweet spot of intelligence and memory.\n\nhttps://huggingface.co/ReadyArt/4.2.0-Broken-Tutu-24b",
              "score": 4,
              "created_utc": "2026-01-03 12:01:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxg5ytm",
                  "author": "intermundia",
                  "text": "Oh this sounds interesting.",
                  "score": 1,
                  "created_utc": "2026-01-03 15:08:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxhp3yb",
                  "author": "Z0mbiN3",
                  "text": "Can you do q8 of tutu on a 3090? 64GB RAM",
                  "score": 1,
                  "created_utc": "2026-01-03 19:26:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxga1zg",
          "author": "Blizado",
          "text": "Maybe this one: https://huggingface.co/mradermacher/Melinoe-30B-A3B-Thinking-i1-GGUF\n\nIt's based on Qwen 30B A3B Thinking and as this it is a fast MoE model which is even fast if it is not fully in VRAM. So you can decide yourself if you want it full in VRAM for fastest speed or put some layer into RAM and have more VRAM free for a bit larger context.\n\nIt is also very good rated for NSFW on the UGI leaderboard and the best Qwen 30B A3B finetune in this point. But not sure if it match into what you want to do with it. Thinking/Reasoning can normally easily be skipped but no clue how that impacts the models output, but that <think></think> part can also be easily miss used to push the LLM into the direction you want.",
          "score": 2,
          "created_utc": "2026-01-03 15:29:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxir0mo",
          "author": "Fahrain",
          "text": "Mistral small 3.2 (at least q6, q4 is worse). It doesn't have a think mode, which is better if you want to generate a large text with complex logic (or simply splitted into chapters). You can easily stop it midway, correct errors in the text, and continue where you left off.\n\nYou could use Magistral Small 2509 - it has a think mode and it certainlly smarter. But it has a drawback: the entire text generation process must be run in one go.. And if a small deviation occurs during the \"think mode\" (or something important to the plot is simply missed), you could only drop everything and regenerate the entire text from the start.\n\nAnd newest version, Ministral-3-14B-Reasoning-2512, is certainly even smarter, thinking better and can generate long, voluminous texts. But the results from this generation... are simply unsatisfactory. I've reverted to the previous models.\n\nAny of them may write that they cannot accept your request - this can be easily corrected by clicking the \"regenerate\" button again or changing some brutal words in the request with more neutral synonyms.",
          "score": 2,
          "created_utc": "2026-01-03 22:32:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxeebv8",
          "author": "blbd",
          "text": "Give this a go.Â \n\nhttps://huggingface.co/ArliAI/gpt-oss-20b-Derestricted",
          "score": 6,
          "created_utc": "2026-01-03 07:14:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxekeut",
              "author": "Death_12_35_taken",
              "text": "what can i run this model on? i am using text generation web UI right now.",
              "score": 2,
              "created_utc": "2026-01-03 08:06:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxenoq5",
                  "author": "Narrow-Belt-5030",
                  "text": "Similar to other responses, use this one: gghfez/gpt-oss-20b-Derestricted-Q4\\_K\\_M-GGUF (16Gb in size, Q4 so will run reasonably, M size so trade off between size & speed)",
                  "score": 1,
                  "created_utc": "2026-01-03 08:34:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxet5be",
          "author": "mission_tiefsee",
          "text": "Try this one: Q5_K_M or Q4_K_M\n\nhttps://huggingface.co/TheDrummer/Precog-24B-v1-GGUF",
          "score": 2,
          "created_utc": "2026-01-03 09:22:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxfdmkp",
          "author": "xGamerG7",
          "text": "I like gemma 3 27b amoral it can pretty much generate anything with the intelligence of gemma",
          "score": 3,
          "created_utc": "2026-01-03 12:13:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxg5glb",
              "author": "intermundia",
              "text": "Would this model work in lm studio?",
              "score": 1,
              "created_utc": "2026-01-03 15:06:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgdwgj",
                  "author": "Equivalent_Bit_461",
                  "text": "If there's a gguf version \nSure",
                  "score": 2,
                  "created_utc": "2026-01-03 15:48:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxixhdt",
          "author": "ZhopaRazzi",
          "text": "2026 is the year of AI gooning",
          "score": 2,
          "created_utc": "2026-01-03 23:05:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhn76n",
          "author": "a3ydstm",
          "text": "Haven't been active here for a while. Anything for an aging M1  Macbook Pro base w/ 16GB ram/vram?\n\nI still have the NeralDareDevil from 1 half years ago that I should probbly delete.",
          "score": 1,
          "created_utc": "2026-01-03 19:17:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhr3pl",
              "author": "Death_12_35_taken",
              "text": "you can trying Impish\\_Bloodmoon it's a 12B model. you should be able to run it and it's pretty good for nsfw roleplay",
              "score": 2,
              "created_utc": "2026-01-03 19:35:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxixs2c",
          "author": "autoencoder",
          "text": "I'm tempted to suggest https://huggingface.co/p-e-w/gpt-oss-20b-heretic-v2\n\nBut I don't like how much gpt-oss thinks.\n\nMaybe this one, but it's dense. I much prefer MoEs. I couldn't find a Qwen3-30B-A3B-Instruct-2507 (my favorite model) [hereticify-d](https://github.com/p-e-w/heretic).\n\nhttps://huggingface.co/mradermacher/Qwen3-14B-heretic-GGUF",
          "score": 1,
          "created_utc": "2026-01-03 23:07:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjdlq9",
          "author": "skocznymroczny",
          "text": "https://huggingface.co/TheBloke/LLaMA2-13B-Tiefighter-GGUF is the best NSFW llm I tried so far.",
          "score": 1,
          "created_utc": "2026-01-04 00:30:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxem3ku",
          "author": "arcanemachined",
          "text": "I like this one:\n\nhttps://huggingface.co/mlabonne/gemma-3-27b-it-abliterated\n\nIt's a GGUF, so probably run it on whatever you're already using.",
          "score": 1,
          "created_utc": "2026-01-03 08:20:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf16pk",
              "author": "autodidacticasaurus",
              "text": "I tried this one but it seems a bit crazy sometimes, like repeating itself non-stop for example.",
              "score": 4,
              "created_utc": "2026-01-03 10:30:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxf2e31",
                  "author": "arcanemachined",
                  "text": "Interesting, I always had surprisingly good results with it. I used Ollama and the Q4_0 quant. (Not the best quant, but works well with my older hardware.)",
                  "score": 1,
                  "created_utc": "2026-01-03 10:41:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxg39sv",
                  "author": "Blizado",
                  "text": "Sounds more like a setting, chat format issue. If that happen often something is wrong with the generation parameter and/or chat format that didn't work well with that model. Are you sure you used the Instruct and not the Base model? That can also be a reason for that.",
                  "score": 1,
                  "created_utc": "2026-01-03 14:54:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxfamfb",
              "author": "Geritas",
              "text": "Give this one a go, it is not as lobotomized https://huggingface.co/YanLabs/gemma-3-27b-it-abliterated-normpreserve-v1-GGUF",
              "score": 3,
              "created_utc": "2026-01-03 11:49:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxfrwqn",
                  "author": "arcanemachined",
                  "text": "Will do, thanks.",
                  "score": 2,
                  "created_utc": "2026-01-03 13:50:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxg2us9",
                  "author": "Blizado",
                  "text": "Or maybe then this version as well: https://huggingface.co/mradermacher/gemma-3-27b-it-heretic-v2-GGUF\n\nHeretic sounds for me a very good approach to uncensor models while keep as close as possible to the original model.",
                  "score": 1,
                  "created_utc": "2026-01-03 14:52:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxem9a7",
              "author": "Death_12_35_taken",
              "text": "what quantization do you use? q4, q8 or f16?",
              "score": 1,
              "created_utc": "2026-01-03 08:22:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxemegj",
                  "author": "arcanemachined",
                  "text": "Q4",
                  "score": 1,
                  "created_utc": "2026-01-03 08:23:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxemluk",
                  "author": "arcanemachined",
                  "text": "Better link:\n\nhttps://huggingface.co/bartowski/mlabonne_gemma-3-27b-it-abliterated-GGUF",
                  "score": 1,
                  "created_utc": "2026-01-03 08:25:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxk5fe9",
          "author": "Crisper026",
          "text": "Commenting to follow",
          "score": 1,
          "created_utc": "2026-01-04 03:03:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxklj6x",
          "author": "Guinness",
          "text": "Hijacking to ask: what model is best at coding on a 3090? What about a 3090 and a RTX 4000 ADA (so 44GB but across the PCIE bus ðŸ˜­)\n\nAnd what are you using? Iâ€™ve been using VLLM.",
          "score": 1,
          "created_utc": "2026-01-04 04:40:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgdiac",
          "author": "Equivalent_Bit_461",
          "text": "What happened to your ram?",
          "score": -1,
          "created_utc": "2026-01-03 15:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjxwan",
          "author": "Innomen",
          "text": "IMO Doesn't exist. Can't exist. They are brain dead or censored. There's no in-between. The censored ones can be jail broken, somewhat. But \"uncensored\" from the ground up just doesn't exist. Because the training data is censored as well NYT and wiki are safe for work you know? You'd need a \"criminal\" database. Plus fully uncensored kinda means fully unaligned in a way doesn't it? I just am not feeling like the promise of open source and the hacktivist community is measuring up in this run up to singularity. I don't feel like anyone is even trying.",
          "score": -5,
          "created_utc": "2026-01-04 02:21:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2pons",
      "title": "GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/0xSero/GLM-4.7-REAP-50-W4A16",
      "author": "Maxious",
      "created_utc": "2026-01-03 08:43:56",
      "score": 179,
      "num_comments": 71,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nxf9e9h",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-03 11:40:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxflp5a",
          "author": "Velocita84",
          "text": "Ok, but REAP'd for what? It's my understanding that REAP prunes experts based on how often they're activated during inference of a calibration set, so what task(s) was it calibrated for?",
          "score": 24,
          "created_utc": "2026-01-03 13:12:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfron7",
              "author": "Kamal965",
              "text": "The W4A16 calibration dataset used was [The Pile-10k](https://huggingface.co/datasets/NeelNanda/pile-10k) and the REAP calibration dataset was listed as [\"glm47-reap-calibration-v2\"](https://huggingface.co/datasets/0xSero/glm47-reap-calibration-v2) which is a dataset on the same author's HF page. Idk what's actually in the dataset because there's no description and I haven't read through it.",
              "score": 12,
              "created_utc": "2026-01-03 13:48:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgd7kz",
                  "author": "Murgatroyd314",
                  "text": "A quick glance at a few bits of the calibration data set finds a lot of programming, several logic/math puzzles, and a bit of trivia.",
                  "score": 10,
                  "created_utc": "2026-01-03 15:44:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfgh6f",
          "author": "Phaelon74",
          "text": "Again, people quanting AWQs (W4A16) need to provide details on what they did to make sure all experts were activated during calibration.  Until OP comes out and provides that, if you see this model act poorly, it's because the calibration data did not activate all experts and it's been partially-lobotomized.",
          "score": 40,
          "created_utc": "2026-01-03 12:35:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxh8wp2",
              "author": "One-Macaron6752",
              "text": "At minimum, a good disclosure normally includes:\n - Calibration dataset description\n - Number of tokens / sequences\n - Observed expert routing frequencies\n - Whether forced routing was used\n - Whether rare experts were targeted\nâ€¦ this is / should becoming best practice in papers & repos! ;)",
              "score": 12,
              "created_utc": "2026-01-03 18:12:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxfnxyf",
              "author": "Position_Emergency",
              "text": "u/Maxious The quant\\_config looks like it defaulted to \"pile-10k\" for the AutoRound pass?\n\nSince you already did the hard work creating \"glm47-reap-calibration-v2\" to select the best experts, wouldn't it be better to reuse that dataset for quantization?\n\nPile-10k probably won't trigger those specific code/agent experts you preserved, leaving them uncalibrated (Silent Expert problem).   \nIt should be a 1-line swap in the AutoRound script to fix.",
              "score": 11,
              "created_utc": "2026-01-03 13:26:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxfrg0d",
                  "author": "Kamal965",
                  "text": "That's actually a great question! I'm curious to know about that too. As far as I can tell, using the same calibration dataset for both pruning and quantization logically makes sense... am I missing something that makes it not a good idea?",
                  "score": 3,
                  "created_utc": "2026-01-03 13:47:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxfr6qo",
              "author": "Kamal965",
              "text": "I mean, I agree in general that it's very frustrating to see AWQ quants that don't say what dataset, or domain, they used for calibration. But in this case, it is explicitly mentioned on the repo. The [README.md](http://README.md) shows the full steps on how to recreate that quant. The W4A16 calibration dataset used was [The Pile-10k](https://huggingface.co/datasets/NeelNanda/pile-10k) and the REAP calibration dataset (and I think this is the more important one to know) was listed as [\"glm47-reap-calibration-v2\"](https://huggingface.co/datasets/0xSero/glm47-reap-calibration-v2) which is a dataset on the same author's HF page. He has 4 different REAP calibration datasets there, interestingly enough... but there are no actual descriptions of what the datasets contain. You'd have to look through each one to see, welp.",
              "score": 3,
              "created_utc": "2026-01-03 13:45:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxfrjnd",
                  "author": "Phaelon74",
                  "text": "Right, but by default, GLM does not have a modeling file in say, LLM\\_Compressor.  So if he first made the quant in llm\\_compressor and then reaped it, experts would be missing based on not being activated by his dataset, etc.  That's more what I am alluding to.  People doing AWQs need to explicitly say \"And I did X, Y, Z, to make sure all experts were activated during dataset calibration.\"",
                  "score": 2,
                  "created_utc": "2026-01-03 13:47:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxev2ro",
          "author": "Position_Emergency",
          "text": "Can see on the Huggingface page you're in the process of doing benchmarks ðŸ’¯  \nWill be interested to see the results!\n\nHave you considered doing a similar size version of MiniMax M2.1? (and therefore a less aggressive REAP as it is a 220B model)",
          "score": 17,
          "created_utc": "2026-01-03 09:39:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxh1kr9",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-01-03 17:40:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxhjaoq",
                  "author": "colin_colout",
                  "text": "Minimax models are ~130gb at 4bits.  If that can get under 90gb, it can fit in 128gb unified memory systems like my strix halo (though not sure if the format is even supported... yay rocm)",
                  "score": 1,
                  "created_utc": "2026-01-03 18:59:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxf0rd2",
              "author": "dtdisapointingresult",
              "text": "He should've done diverse benchmarks before uploading lobotomyslop if you ask me.",
              "score": -7,
              "created_utc": "2026-01-03 10:27:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxf40vx",
                  "author": "Position_Emergency",
                  "text": "In the land of the blind the one eyed man is king.",
                  "score": 9,
                  "created_utc": "2026-01-03 10:55:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxf38i6",
          "author": "a_beautiful_rhind",
          "text": "You can run 2.0bpw exl3 GLM and it's around 90gb. Comparison here would be interesting.\n\nWhen I tried previous 4.6 REAP, about 3 of them, the EXL was better subjectively.\n\n>Calibrated on code/agentic tasks; may have reduced performance on other domains\n\nAll those other reap forgot how to talk outside such domains. It's interesting how nobody has deviated from the codeslop datasets cerebras used. My theory is a more rounded english only dataset would preserve much more performance. Then someone could do chinese only, etc.",
          "score": 7,
          "created_utc": "2026-01-03 10:48:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgg9xf",
              "author": "projectmus3",
              "text": "Youâ€™re the person who does roleplay with LLMs and talk to fictional characters right? Yeah maybe you should create a calibration dataset for roleplay and use that to REAP instead. \n\nThe REAP models from Cerebras focus on coding, tool calling and agentic workloads, and theyâ€™ve been doing amazing for me.",
              "score": 5,
              "created_utc": "2026-01-03 15:59:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgi279",
                  "author": "a_beautiful_rhind",
                  "text": "Really only thing stopping me is the massive download.\n\nI've heard mixed results from people coding with it tho and if you do a perplexity test, usually it's double digit. \n\nThe REAPS I tried would forget who presidents were and other basic facts. Left me a bit skeptical to invest big effort into it.",
                  "score": 3,
                  "created_utc": "2026-01-03 16:08:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxhc37u",
              "author": "One-Macaron6752",
              "text": "I can second your opinion. I have also tried 2.65bpw exl3 quants and felt worlds better than the REAP. For me, the REAP version was: 1) full of hallucinations in places Iâ€™d never expected them 2) full of Chinese & Arabic characters dropping almost everywhereâ€¦",
              "score": 2,
              "created_utc": "2026-01-03 18:27:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxn531v",
                  "author": "Sero_x",
                  "text": "These sound like inference layer errors to me.",
                  "score": 1,
                  "created_utc": "2026-01-04 15:56:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxew6k4",
          "author": "Dany0",
          "text": "Barely doesn't fit on 64gb ram + 32gb vram :( Q3\\_KS managed to load once but OOM'd immediately during prompt processing",
          "score": 7,
          "created_utc": "2026-01-03 09:48:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfcpo2",
              "author": "ApartmentEither4838",
              "text": "Can this work on a A100 80GB?",
              "score": 1,
              "created_utc": "2026-01-03 12:06:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxeu3rl",
          "author": "jacek2023",
          "text": "I need Q3, anyone working on GGUF?",
          "score": 4,
          "created_utc": "2026-01-03 09:31:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfgesv",
              "author": "noctrex",
              "text": "Let's try I guess",
              "score": 3,
              "created_utc": "2026-01-03 12:35:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxfol1n",
              "author": "Kamal965",
              "text": "He just finished uploading some of them: [https://huggingface.co/0xSero/GLM-4.7-REAP-50-GGUF](https://huggingface.co/0xSero/GLM-4.7-REAP-50-GGUF)\n\nI believe he's still uploading more.",
              "score": 1,
              "created_utc": "2026-01-03 13:30:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxhswif",
                  "author": "fallingdowndizzyvr",
                  "text": "\"404\n\nSorry, we can't find the page you are looking for.\"",
                  "score": 5,
                  "created_utc": "2026-01-03 19:44:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxfomou",
                  "author": "jacek2023",
                  "text": "~~thank you!!!~~\n\nbut wait, why it's 25GB only?",
                  "score": 0,
                  "created_utc": "2026-01-03 13:30:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfedtg",
          "author": "Revolutionary-Tip821",
          "text": "using  \nvllm serve /home/xxxx/Docker/xxx/GLM-4.7-REAP-40-W4A16 \\\\\n\n\\--served-model-name local/GLM-4.7-REAP-local \\\\\n\n\\--host [0.0.0.0](http://0.0.0.0) \\--port 8888 \\\\\n\n\\--tensor-parallel-size 2 --pipeline-parallel-size 3 \\\\\n\n\\--quantization auto-round \\\\\n\n\\--max-model-len 14000 \\\\\n\n\\--gpu-memory-utilization 0.96 \\\\\n\n\\--block-size 32 \\\\\n\n\\--max-num-seqs 8 \\\\\n\n\\--max-num-batched-tokens 8192 \\\\\n\n\\--enable-expert-parallel \\\\\n\n\\--enable-prefix-caching \\\\\n\n\\--enable-chunked-prefill \\\\\n\n\\--disable-custom-all-reduce \\\\\n\n\\--disable-log-requests \\\\\n\n\\--tool-call-parser glm47 \\\\\n\n\\--reasoning-parser glm45 \\\\\n\n\\--enable-auto-tool-choice \\\\\n\n\\--trust-remote-code\n\non 6 RTX 4090 it start generating and then fall by repeating same word endlessly, also thinking are not wrapped in think tags, is there anyone have same experience?",
          "score": 2,
          "created_utc": "2026-01-03 12:19:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfg7m3",
              "author": "Phaelon74",
              "text": "Why do pipelines, just 6 TP and rock and roll.  Additionally reasoning parser I have seen what you are seeing.  I don't use it and only use expert-parallel.",
              "score": 2,
              "created_utc": "2026-01-03 12:33:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxftj0m",
                  "author": "Hisma",
                  "text": "you can't do TP on 6 GPUs.  It needs to be powers of 2.  2/4/8 GPUs is typically what's used for TP.",
                  "score": 1,
                  "created_utc": "2026-01-03 13:59:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxhbwle",
              "author": "Sero_x",
              "text": "The repeating is a pipeline but that happens with this model",
              "score": 1,
              "created_utc": "2026-01-03 18:26:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxll3nm",
                  "author": "Revolutionary-Tip821",
                  "text": "i tried also with --tensor-parallel-size 4; but still it stuck repeating same word, so this model is not usable in this state\n\ni don't understand the hype if it can't be used for simple conversation",
                  "score": 1,
                  "created_utc": "2026-01-04 09:31:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxgjyof",
          "author": "Revolutionalredstone",
          "text": "Next please do nanbeige, this this is a beast but needs prune + int4!\n\nhttps://old.reddit.com/r/LocalLLaMA/comments/1q2p2wa/nanbeige4_is_an_incredible_model_for_running/",
          "score": 2,
          "created_utc": "2026-01-03 16:17:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhpdgl",
              "author": "thejoyofcraig",
              "text": "Nanbeige is a 3b model. What are you hoping to prune it down to??",
              "score": 3,
              "created_utc": "2026-01-03 19:27:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxhr9k0",
                  "author": "Revolutionalredstone",
                  "text": "TBH I'd take a 500m and 250m params with very big excitement!\n\nThe other models pruned to this size: like Gemma and granite were absolute bangers!\n\nAnd this one has a lot more junk in the trunk per se.\n\nUltra nano models can be VERY useful if they can barely speak ;D",
                  "score": 1,
                  "created_utc": "2026-01-03 19:36:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxhprn2",
              "author": "LocoMod",
              "text": "It's a 3B model that fits on a lemon. What's the point?",
              "score": 3,
              "created_utc": "2026-01-03 19:29:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxhqwlc",
                  "author": "Revolutionalredstone",
                  "text": "You'd be surprised! I've got plenty of portable devices with 2GB vram and the diff between 3B partial and 2B fully offloaded is HUGE.\n\nNot so much about being ABLE to run, but being able to run FAST!",
                  "score": 2,
                  "created_utc": "2026-01-03 19:34:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxhvut0",
                  "author": "SlowFail2433",
                  "text": "Edge AI is a thing, often very small chips",
                  "score": 2,
                  "created_utc": "2026-01-03 19:58:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxlmdap",
          "author": "LegacyRemaster",
          "text": "https://preview.redd.it/nse8fr8mzabg1.png?width=2013&format=png&auto=webp&s=4d86c31bb4db3967d06dc05a7bf3a589395fc70b\n\nSuper quick test.  glm-4.7-reap-40p IQ3\\_S - 94.57 gb. Fit on 96gb with 4k context. Will test more.",
          "score": 2,
          "created_utc": "2026-01-04 09:42:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxep7dp",
          "author": "fungnoth",
          "text": "I'm curious about the low VRAM + OK system RAM situation with moe offloading",
          "score": 5,
          "created_utc": "2026-01-03 08:47:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxetg03",
              "author": "jhnnassky",
              "text": "Do you have already good ones?",
              "score": 0,
              "created_utc": "2026-01-03 09:25:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxytwwa",
                  "author": "fungnoth",
                  "text": "I sometimes use the GLM Air REAP. 10 layers in GPU and 38 layers MOE CPU.\nUsable, 12GB VRAM 64GB RAM",
                  "score": 2,
                  "created_utc": "2026-01-06 06:56:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxffp1z",
          "author": "LegacyRemaster",
          "text": "fit on 6000 96g ... let me try",
          "score": 1,
          "created_utc": "2026-01-03 12:29:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxg9f1m",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-03 15:26:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxg9ke4",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 7 days on [**2026-01-10 15:26:14 UTC**](http://www.wolframalpha.com/input/?i=2026-01-10%2015:26:14%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/nxg9f1m/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1q2pons%2Fglm47reap50w4a16_50_expertpruned_int4_quantized%2Fnxg9f1m%2F%5D%0A%0ARemindMe%21%202026-01-10%2015%3A26%3A14%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q2pons)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-03 15:26:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxg9nbb",
          "author": "Enottin",
          "text": "RemindMe! 7 days",
          "score": 1,
          "created_utc": "2026-01-03 15:27:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxg9zkl",
              "author": "Enottin",
              "text": "RemindMe! 1 day",
              "score": 1,
              "created_utc": "2026-01-03 15:29:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhg7oy",
          "author": "sampdoria_supporter",
          "text": "I am completely ignorant of this model and REAP as a method but I'm hoping to hell this means running it on strix halo is possible",
          "score": 1,
          "created_utc": "2026-01-03 18:45:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxi7et6",
              "author": "fallingdowndizzyvr",
              "text": "You can run 4.7 on Strix Halo without this.",
              "score": 0,
              "created_utc": "2026-01-03 20:56:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxiby6x",
                  "author": "GreatAlmonds",
                  "text": "How? Unless you're running 1bit quants",
                  "score": 3,
                  "created_utc": "2026-01-03 21:18:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxk7sj9",
          "author": "Goghor",
          "text": "!remindme 7 days",
          "score": 1,
          "created_utc": "2026-01-04 03:16:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxmc2bf",
          "author": "Guilty_Nothing_2858",
          "text": "I want to know how is the performance? Faster but poor satisfaction rate? I saw lot of comment from china dev community, say GLM4.7 cloud is in quantised version. The answer is not good",
          "score": 1,
          "created_utc": "2026-01-04 13:15:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxepedz",
          "author": "DesignerTruth9054",
          "text": "Cool. Excited to try outÂ ",
          "score": 1,
          "created_utc": "2026-01-03 08:49:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxerlf5",
          "author": "Steus_au",
          "text": "whatâ€™s the best way to test/compare it to full size one?",
          "score": 1,
          "created_utc": "2026-01-03 09:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxf3te7",
          "author": "Odd-Ordinary-5922",
          "text": "can someone try pruning gpt oss 120b? Ik there is already one but I think he messed up something. Much appreciated",
          "score": 0,
          "created_utc": "2026-01-03 10:53:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1lgb7",
      "title": "TIL you can allocate 128 GB of unified memory to normal AMD iGPUs on Linux via GTT",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q1lgb7/til_you_can_allocate_128_gb_of_unified_memory_to/",
      "author": "1ncehost",
      "created_utc": "2026-01-02 01:37:11",
      "score": 175,
      "num_comments": 30,
      "upvote_ratio": 0.96,
      "text": "So I am training a 1B model right now on my 7900 XTX with some custom kernels I wrote, and while it is training I wanted to optimize the kernels at the same time. However, my VRAM is nearly maxed doing training, so its not ideal.\n\nThen I realized maybe my 2 CU Raphael iGPU might be able to help since I only need to run some limited samples and the speed isn't as important for optimization as it is for training. After doing some research, it turned out that not only does ROCm recognize the iGPU, but a Linux feature called Graphics Translation Table (GTT) for AMD iGPUs can use up to 128 GB of system memory as VRAM. It even allocates it dynamically, so it isn't removed from your CPU's memory pool until it is allocated. I think a lot of people running Strix Halo are probably using the bios setting, but if you are running Linux you should check to see if GTT works for you since its dynamically allocated.\n\nThis isn't very useful for most people:\n\n1) It isn't going to be good for inference because iGPUs are very very slow, and usually the CPU itself is faster for inference.\n\n2) I'm accessing ROCm directly via C++ / HIP kernels, so I can avoid all the support issues ROCm has for iGPUs in the python stack\n\nHowever, for development it is actually pretty awesome. I allocated 24 GB of GTT so now the iGPU can load a full training run that my main GPU can run so I can profile it. Meanwhile my main GPU is doing long term loss convergence tests in parallel. Since RDNA iGPUs have been around for a while now, this enables big memory AMD GPU kernel development for cheap.\n\nAlso it might be interesting for developing hybrid CPU/GPU architectures. The MI300A does exist which has unified HBM tied to a CPU and giant iGPU. A standard ryzen laptop could kind of sort of simulate it for cheap. Stuff like vector indexing on the CPU into big GEMMs on the GPU could be done without PCIE overhead.\n\nI thought it was cool enough to post. Probably a \"Cool story bro\" moment for most of you though haha.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q1lgb7/til_you_can_allocate_128_gb_of_unified_memory_to/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nx7en6z",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-02 05:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7akn8",
          "author": "jstormes",
          "text": "I am doing this with an older Ryzen 7 5600G for background LLM tasks.  Using the iGPU leaves the CPU free to do other batch processes. \n\nBecause I am not using interactivity it is a good use case.  \n\nI have 64 GB 3600 MT/s memory with about 42 of it running a single LLM with it's cache.\n\nIt also keeps my more modern machines free for interactive stuff.",
          "score": 24,
          "created_utc": "2026-01-02 04:47:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7otzh",
              "author": "PentagonUnpadded",
              "text": "A Ryzen 5 5600G has 7 CUs. A 7700X like OP mentions has 2. Cool way to breath life into a 'budget' chip like like the 'G'.",
              "score": 8,
              "created_utc": "2026-01-02 06:35:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx808n2",
          "author": "laughingfingers",
          "text": "on Strix Halo I definitely use this for inference and it's a lot faster than CPU. In BIOS I set graphics memory to the minimum 512MB, with this gtt setting I allocate almost all the rest (few GB for the OS to run seems wise).",
          "score": 18,
          "created_utc": "2026-01-02 08:19:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8845x",
              "author": "Daniel_H212",
              "text": "Yeah this is how I was recommended to do it on strix halo too. There was some talk about how allocating more than 108 GiB with GTT caused instability so I limited myself to that (and don't expect needing that much anyway).",
              "score": 2,
              "created_utc": "2026-01-02 09:34:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8ow0s",
                  "author": "GrayBayPlay",
                  "text": "I can load models up to 120 gb, with a 4k context on my halo strix without any instabillity issues. Then again i only use it for inference. your milage may vary :P",
                  "score": 3,
                  "created_utc": "2026-01-02 12:05:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx875t5",
          "author": "FastDecode1",
          "text": "FYI, according to the [driver docs](https://www.kernel.org/doc/html/v4.19/gpu/amdgpu.html):\n\n>gttsize (int)\n\n>Restrict the size of GTT domain in MiB for testing. The default is -1 (Itâ€™s VRAM size if 3GB < VRAM < 3/4 RAM, otherwise 3/4 RAM size).\n\nSo as long as you have more than 4GB of RAM, the driver automatically allows up to 3/4 of the RAM to be allocated to the iGPU.\n\nI've run stuff on a Vega 8 iGPU on a laptop using llama.cpp and it does work. However, it's not a great experience if you want to watch videos (or do basically anything else GUI-wise) at the same time, since llama.cpp hogs all the memory bandwidth and causes everything else to stutter. GPU scheduling is pretty much non-existent on Linux AFAIK, so there's not really a great way to mitigate this atm.\n\nAlso a hint for fellow ThinkPad users: even though the spec sheet says only a certain amount of RAM is supported, you should probably be able to add more without issues. My current E595's specs say only up to 32GB is supported, but I added a 32GB stick alongside the existing 8GB for a total of 40GB and it works.",
          "score": 5,
          "created_utc": "2026-01-02 09:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6wxt5",
          "author": "master__cheef",
          "text": "This guy LLMs",
          "score": 11,
          "created_utc": "2026-01-02 03:19:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx83xq9",
          "author": "noiserr",
          "text": "Yup. This is what we do with Strix Halo.",
          "score": 3,
          "created_utc": "2026-01-02 08:54:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7v3ol",
          "author": "melenitas",
          "text": "Great, I need to test this with my Ryzen 8845hs, I thought I was limited to 16gb from the total 32gb....Â ",
          "score": 3,
          "created_utc": "2026-01-02 07:31:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6iu1i",
          "author": "cosimoiaia",
          "text": "With llama.cpp you can actually do with Nvidia GPUs as well and if you use it only for kv cache the speed doesn't drastically drop. It's a pretty cool trick.\n\nI used to do that too with my iGPU as well but, maybe because it's a pretty slow one, I never noticed any difference between that and using cpu only, both in training and inference.\n\nI even did some training on cpu only and with a stock heatsink/fan. \"Fun\" to see it hitting 106Â° Celsius.",
          "score": 7,
          "created_utc": "2026-01-02 01:52:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6jka1",
              "author": "-InformalBanana-",
              "text": "How would I set it up to use it only for kv cache, can you give me a llama-server example command?  \nThanks.",
              "score": 6,
              "created_utc": "2026-01-02 01:57:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx6kxfl",
                  "author": "cosimoiaia",
                  "text": "It's not specific to the kv cache. \nSet the env variable GGML_CUDA_ENABLE_UNIFIED_MEMORY=1 and if the model fits in VRAM, the ram will be used only for the kv cache.",
                  "score": 5,
                  "created_utc": "2026-01-02 02:05:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxco2fa",
              "author": "-InformalBanana-",
              "text": "I tried loading a model with your method.\nMy speed on 20k context filled droped from 15tg/s to 2.7tg/s. I would call that significant. And that is with only 2GB shared memory used. So either llama.cpp doesnt consistently allocate kv after other parts thus model ends up in shared memory instead of kv or you simply need much of kv for generating output when context is close to filled.\npp/s also droped from 250 to 76 t/s.\n\n\nSo I think you aren't right about this and you made a mistake somewhere.",
              "score": 0,
              "created_utc": "2026-01-03 00:28:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxcs6s1",
                  "author": "cosimoiaia",
                  "text": "Dude, it's a llama.cpp feature. RTFM. It's not even clear what/where/how you are trying to do, probably to you as well.",
                  "score": -1,
                  "created_utc": "2026-01-03 00:51:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7m9st",
          "author": "alppawack",
          "text": "Whatâ€™s the training speed?",
          "score": 2,
          "created_utc": "2026-01-02 06:14:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9tfni",
              "author": "1ncehost",
              "text": "A blazing 1.8 tok/s ðŸ˜‚ -- the program is very unoptimized currently and it should probably be about 50-200 tok/s for this model.",
              "score": 3,
              "created_utc": "2026-01-02 16:05:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx96wgm",
          "author": "uti24",
          "text": ">but a Linux feature called Graphics Translation Table (GTT) for AMD iGPUs can use up to 128 GB of system memory as VRAM\n\nIs there a fundamental reason it could not be implemented in windows? Or is it just not implemented? Could it be implemented not on the system level but on the app level?",
          "score": 1,
          "created_utc": "2026-01-02 14:07:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx97wzl",
              "author": "1ncehost",
              "text": "I'm not sure to be honest, but the linux driver is open source and the windows driver isn't, so it generally gets features faster and random hacker dudes like me expose low level features / fix stuff fast.",
              "score": 5,
              "created_utc": "2026-01-02 14:13:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzhcqu",
      "title": "Any guesses?",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/xqvj95zv8cag1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-30 12:52:15",
      "score": 174,
      "num_comments": 36,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwqfxna",
          "author": "Aggressive-Dingo-993",
          "text": "https://preview.redd.it/5pyw2vu8pcag1.jpeg?width=368&format=pjpg&auto=webp&s=6cb05d136e810fbb1e761628ea82933c2ec7c842\n\nQwen image 2512",
          "score": 53,
          "created_utc": "2025-12-30 14:23:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqnq2f",
              "author": "GenLabsAI",
              "text": "I think thats already out",
              "score": 6,
              "created_utc": "2025-12-30 15:05:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws7ckl",
                  "author": "infearia",
                  "text": "Qwen-Image-**Edit-2511** is out. Qwen-Image-2512 is not.",
                  "score": 17,
                  "created_utc": "2025-12-30 19:27:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwr8fkr",
                  "author": "Sorry_Warthog_4910",
                  "text": "Itâ€™s not",
                  "score": 4,
                  "created_utc": "2025-12-30 16:44:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwq621n",
          "author": "swagonflyyyy",
          "text": "Qwen3vl-next-80b-a3b - Now with no more comparison slop.\n\n\nIts not a comparison, its a victory.",
          "score": 78,
          "created_utc": "2025-12-30 13:26:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrz0lp",
              "author": "Own-Potential-2308",
              "text": "It's not X, it's Y.\n\nCan't unsee it",
              "score": 19,
              "created_utc": "2025-12-30 18:47:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqde0o",
          "author": "HedgehogActive7155",
          "text": "Qwen 6, to beat GPT 5.2 on the only benchmark that matter\n\nhttps://preview.redd.it/2fix1edsncag1.png?width=1080&format=png&auto=webp&s=d24535f9d904fe79639b83dbdf9c35e0d67a930c",
          "score": 100,
          "created_utc": "2025-12-30 14:09:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrs0d9",
              "author": "MoffKalast",
              "text": "Finally a benchmark you can trust.",
              "score": 11,
              "created_utc": "2025-12-30 18:15:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwr91cx",
              "author": "Utoko",
              "text": "That would be huge if they could double the number!",
              "score": 17,
              "created_utc": "2025-12-30 16:47:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwrdz2g",
                  "author": "-dysangel-",
                  "text": "it would be almost twice as huge!",
                  "score": 5,
                  "created_utc": "2025-12-30 17:10:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwvzn1j",
              "author": "Niwa-kun",
              "text": "lmao. cool graph. names, colors, and number with literally ZERO information for what any of it means. Cool story. I call bs on this \"benchmark\".",
              "score": 1,
              "created_utc": "2025-12-31 09:24:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwws5qp",
                  "author": "t_krett",
                  "text": "I ran the numbers myself and they check out!",
                  "score": 3,
                  "created_utc": "2025-12-31 13:25:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwxbsyv",
                  "author": "Tall-Ad-7742",
                  "text": "I tried it myself and itâ€™s crazy how accurate this benchmark is and btw itâ€™s called VAG-Benchmark",
                  "score": 2,
                  "created_utc": "2025-12-31 15:17:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrizyl",
              "author": "Cool-Chemical-5629",
              "text": "Where is Grok 4.1? ðŸ˜­ðŸ’”",
              "score": 0,
              "created_utc": "2025-12-30 17:34:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtwmyj",
                  "author": "erraticnods",
                  "text": "grokking they weights",
                  "score": 2,
                  "created_utc": "2025-12-31 00:34:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwq169h",
          "author": "MaxKruse96",
          "text": "Iteration on qwen-image i recon. No LLM, no Qwen3.5 etc.",
          "score": 19,
          "created_utc": "2025-12-30 12:54:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwr0un0",
              "author": "ontorealist",
              "text": "Itâ€™s also been awhile since Z-Image Edit was announcedâ€¦",
              "score": 8,
              "created_utc": "2025-12-30 16:09:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwr7rfl",
                  "author": "j_osb",
                  "text": "I was going to say. I expect z-image-onni-base or z-image-edit considering tongyi dropped the post.",
                  "score": 5,
                  "created_utc": "2025-12-30 16:41:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwrbztf",
          "author": "ForsookComparison",
          "text": "Qwen3.5-235B-A10B\n\nPlz",
          "score": 13,
          "created_utc": "2025-12-30 17:01:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrac0x",
          "author": "a_beautiful_rhind",
          "text": "z-image prompt enhancer enhancer\n\noh you thought you were getting the base, huh?",
          "score": 7,
          "created_utc": "2025-12-30 16:53:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrk1hs",
          "author": "Cool-Chemical-5629",
          "text": "The ascii art character must be a hint. Probably some image related model.",
          "score": 6,
          "created_utc": "2025-12-30 17:39:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsdwjv",
          "author": "Steuern_Runter",
          "text": "Maybe the other z-image models that had been announced but not published yet:\n\nZ-Image-Omni-Base\n\nZ-Image-Edit\n\nZ-Image",
          "score": 5,
          "created_utc": "2025-12-30 19:58:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqy32v",
          "author": "GabryIta",
          "text": "Wan2.5 open source?",
          "score": 3,
          "created_utc": "2025-12-30 15:56:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nws24bl",
          "author": "Brave-Hold-9389",
          "text": "Qwen image 2",
          "score": 3,
          "created_utc": "2025-12-30 19:02:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqaykk",
          "author": "Evening_Ad6637",
          "text": "Something trained on ASCII Art Ã  la Opus?",
          "score": 6,
          "created_utc": "2025-12-30 13:55:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqbaed",
          "author": "jacek2023",
          "text": "unfortunately radio silence from Junyang Lin",
          "score": 2,
          "created_utc": "2025-12-30 13:56:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrdup0",
          "author": "-dysangel-",
          "text": "https://preview.redd.it/47u8zpa0jdag1.png?width=350&format=png&auto=webp&s=3e52fc4e3d3bdc5753d50c36eae39d64004e0c0a\n\nthat's got to be the best emoji I've ever seen",
          "score": 2,
          "created_utc": "2025-12-30 17:10:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq8wen",
          "author": "Sensitive_Sweet_1850",
          "text": "Qwen4VL!!",
          "score": 2,
          "created_utc": "2025-12-30 13:43:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqccv4",
          "author": "Few_Painter_5588",
          "text": "Qwen3-Max open source?ðŸ‘€",
          "score": 4,
          "created_utc": "2025-12-30 14:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq3fv5",
          "author": "scraper01",
          "text": "Omni because the little figure has a hero stance",
          "score": 1,
          "created_utc": "2025-12-30 13:09:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqi8xe",
          "author": "Salt-Willingness-513",
          "text": "qwen image. i ask q wen z-image base",
          "score": 1,
          "created_utc": "2025-12-30 14:36:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqrzo3",
          "author": "this-just_in",
          "text": "Something zimage",
          "score": 1,
          "created_utc": "2025-12-30 15:27:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqwt65",
          "author": "Gold_Scholar1111",
          "text": "Qwen5 serie",
          "score": 1,
          "created_utc": "2025-12-30 15:50:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0vom4",
      "title": "IQuestLab/IQuest-Coder-V1 â€” 40B parameter coding LLM â€” Achieves leading results on SWE-Bench Verified (81.4%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%)",
      "subreddit": "LocalLLaMA",
      "url": "https://github.com/IQuestLab/IQuest-Coder-V1",
      "author": "TellMeAboutGoodManga",
      "created_utc": "2026-01-01 04:29:26",
      "score": 174,
      "num_comments": 47,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0vom4/iquestlabiquestcoderv1_40b_parameter_coding_llm/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "nx1p0vs",
          "author": "gzzhongqi",
          "text": "I looked up their background info and they are back by a chinese quant trading company, similar to deepseek. Interesting that all these quant trading companies are stepping into llm training.",
          "score": 62,
          "created_utc": "2026-01-01 07:05:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx31x3p",
              "author": "Karyo_Ten",
              "text": "Faster churning of quant code, millions won on the stock market.",
              "score": 3,
              "created_utc": "2026-01-01 14:41:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx17enb",
          "author": "TellMeAboutGoodManga",
          "text": "https://preview.redd.it/xzspqeti1oag1.png?width=3022&format=png&auto=webp&s=1887580e4045d206ec282b1a12549b46507ee0b1",
          "score": 30,
          "created_utc": "2026-01-01 04:31:55",
          "is_submitter": true,
          "replies": [
            {
              "id": "nx1a4dk",
              "author": "Recoil42",
              "text": "Great technical report here: [https://github.com/IQuestLab/IQuest-Coder-V1/blob/main/papers/IQuest\\_Coder\\_Technical\\_Report.pdf](https://github.com/IQuestLab/IQuest-Coder-V1/blob/main/papers/IQuest_Coder_Technical_Report.pdf)",
              "score": 18,
              "created_utc": "2026-01-01 04:52:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx18606",
          "author": "ocirs",
          "text": "Really great results for a 40B param model, is it safe the assume the benchmarks are based on the IQuest-Coder-V1-40B-Loop-Thinking model?",
          "score": 18,
          "created_utc": "2026-01-01 04:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx18u5x",
              "author": "TellMeAboutGoodManga",
              "text": "The score of LiveCodeBench v6 is from IQuest-Coder-V1-40B-Loop-Thinking model, and the rest are IQuest-Coder-V1-40B-Loop-Instruct model.",
              "score": 17,
              "created_utc": "2026-01-01 04:42:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx19tir",
              "author": "r4in311",
              "text": "It's also very safe to assume that this is a comically blatant case of benchmaxing. :-)",
              "score": 9,
              "created_utc": "2026-01-01 04:50:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1wm3i",
                  "author": "No-Dog-7912",
                  "text": "No, this is actually a well thought out use of collecting trajectories for RL. Did you read the blog post? This is what Google recently did with Gemini 3 Flash and itâ€™s starting to become a norm for other companies. They had 32k trajectories thatâ€™s just sick. To be honest, with these results and model size. This would technically mean that this is the best local coding model by farâ€¦. If we could validate this ourselves independently then it would be a huge opportunity gain for local model runners after quantizing the model.",
                  "score": 35,
                  "created_utc": "2026-01-01 08:23:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx1ccmv",
                  "author": "Odd-Ordinary-5922",
                  "text": "tell me how benchmaxing is possible when the test questions arent visible and constantly change",
                  "score": 0,
                  "created_utc": "2026-01-01 05:12:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2hlef",
          "author": "Snuttegubben68",
          "text": "GCUF available now - downloading it with LMStudio",
          "score": 10,
          "created_utc": "2026-01-01 12:03:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxsu6je",
              "author": "eliaweiss",
              "text": "Please update",
              "score": 1,
              "created_utc": "2026-01-05 11:29:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3tpoo",
          "author": "Business_Clerk_8943",
          "text": "https://preview.redd.it/35z3nb6ftrag1.png?width=3497&format=png&auto=webp&s=c2b4ec46269c7bd60813a67176a82d45b315d5fa\n\n[https://huggingface.co/spaces/Jellyfish042/UncheatableEval](https://huggingface.co/spaces/Jellyfish042/UncheatableEval)  \nQwen3 14B's pre-training level. Theyâ€™re obviously gaming the benchmarks. I don't get how anyone buys this.",
          "score": 11,
          "created_utc": "2026-01-01 17:16:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx40zna",
              "author": "Baldur-Norddahl",
              "text": "They are the best model on that eval for pure coding? Also there are no other models at the same size, so we can't really compare anything.",
              "score": 1,
              "created_utc": "2026-01-01 17:54:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1ifxm",
          "author": "TopCryptographer8236",
          "text": "I was hoping the 40B was a MoE but it seems to be a dense model. I guess i was just used with everything bigger than 20B to be a MoE at the moment to balance the speed with consumer hardware. But still appreciate it nonetheless.",
          "score": 13,
          "created_utc": "2026-01-01 06:03:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx324oo",
              "author": "Karyo_Ten",
              "text": "Time to buy a 5090, in NVFP4 it would be 20GB so 12GB left for context",
              "score": 1,
              "created_utc": "2026-01-01 14:42:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1jhy1",
          "author": "Fantastic-Emu-3819",
          "text": "Benchmaxxed or real?",
          "score": 17,
          "created_utc": "2026-01-01 06:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx464ny",
              "author": "FinBenton",
              "text": "Someone on youtube tested it, if you feed it isolated benchmark test type questions then it is extremely good at that but working in real world codebases it fell apart. This might be one of the most benchmaxed models ever made.",
              "score": 8,
              "created_utc": "2026-01-01 18:19:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4m5el",
                  "author": "Fantastic-Emu-3819",
                  "text": "Imagine it was better than opus 4.5. S&P500 would have been -30%.",
                  "score": 8,
                  "created_utc": "2026-01-01 19:38:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4ybk7",
                  "author": "Lopsided_Dot_4557",
                  "text": "I have tested it here and looks good:  [https://youtu.be/NrqE2mKHagg?si=4PYWVlJCnvGKMYFM](https://youtu.be/NrqE2mKHagg?si=4PYWVlJCnvGKMYFM)",
                  "score": 3,
                  "created_utc": "2026-01-01 20:40:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1yczv",
          "author": "__Maximum__",
          "text": "Someone test this in their private coding bench",
          "score": 6,
          "created_utc": "2026-01-01 08:41:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx21n26",
              "author": "lumos675",
              "text": "I can test but any gguf available?",
              "score": 6,
              "created_utc": "2026-01-01 09:16:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx23j1q",
                  "author": "__Maximum__",
                  "text": "No, at the moment, the only way is to use transformers, i guess.",
                  "score": 1,
                  "created_utc": "2026-01-01 09:37:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2q3ca",
          "author": "rekriux",
          "text": "I believe the loop integration is the fist implementation of the sort ? Any one can confirm any other implementation ?\n\nThis is a idea I raised, what if we re-used layers to artificially augment the model dept ?  \nBut I was thinking of applying a adapter (rsLoRa) on the second/third pass, making it able to \\*\\*fake\\*\\* a larger model. The power of a dense 72B in a 32b model, about +15-40% more knowledge with the Lora.   \n  \nThe thing with (most?) Lora implementation, last I checked they can't run simultaneous lora on batches, not sure if it was fixed. But if batching is made to wait until next beginning, it may introduce a bit latency for 1st token but it could be worth it with NVRAM prices !",
          "score": 3,
          "created_utc": "2026-01-01 13:18:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2wp53",
          "author": "paryska99",
          "text": "This is huge if true",
          "score": 3,
          "created_utc": "2026-01-01 14:06:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1wje3",
          "author": "Everlier",
          "text": "Report mentions 7B and 14B, but no weights, I'm very curious to try these two!",
          "score": 5,
          "created_utc": "2026-01-01 08:22:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4613x",
          "author": "6969its_a_great_time",
          "text": "So is it good or not?",
          "score": 2,
          "created_utc": "2026-01-01 18:18:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx26fup",
          "author": "Shir_man",
          "text": "Those benchmarks looks sus, has anyone tried it already?",
          "score": 2,
          "created_utc": "2026-01-01 10:08:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3dhzu",
          "author": "_w0n",
          "text": "Does anyone has a benchmark i should try on this model?",
          "score": 1,
          "created_utc": "2026-01-01 15:50:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx43chc",
          "author": "Baldur-Norddahl",
          "text": "I am not seeing the thinking variants on HF. Are only the non thinking versions open weight?",
          "score": 1,
          "created_utc": "2026-01-01 18:05:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6trp3",
          "author": "iansltx_",
          "text": "Seems like it doesn't work with mlx-lm, and the q8 GGUF basically stalls out on my M1 Max 64GB box. What am I doing wrong here?",
          "score": 1,
          "created_utc": "2026-01-02 02:59:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1rmb2",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -4,
          "created_utc": "2026-01-01 07:30:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx327wh",
              "author": "Karyo_Ten",
              "text": "vLLM can load the base FP16 weights",
              "score": 1,
              "created_utc": "2026-01-01 14:43:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzfuqg",
      "title": "Why Kimi K2 Thinking choose Int4 QAT, from infra enginner of KImi",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/",
      "author": "nekofneko",
      "created_utc": "2025-12-30 11:33:10",
      "score": 171,
      "num_comments": 10,
      "upvote_ratio": 0.94,
      "text": "I saw the recent [discussion](https://www.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/) here regarding MiniMax engineer's tweet about why they decided *against* using int4 QAT for the MiniMax M2.1 model.\n\nInterestingly, at the time of the K2 Thinking release, a Kimi infra engineer posted a deep dive on Zhihu explaining why native int4 QAT was actually crucial for them. Iâ€™ve summarized the key takeaways below to offer a different perspective on the 'to quant or not to quant' debate.\n\n**TL;DR:** Kimi found int4 QAT is essential for **MoE latency**, **long-context stability**, and **speeding up the RL training loop**.\n\n# Decoding is Memory-Bound (Latency Focus)\n\nUnlike the MiniMax case, Kimi found that for their specific MoE architecture (which is highly sparse), the decoding phase is almost exclusively memory-bound. By using W4A16 (4-bit weights, 16-bit activations), they reduced memory usage significantly. This allowed the model to fit on fewer GPUs, which reduced inter-device communication overhead, a major factor in lowering end-to-end latency for users.\n\n# PTQ Failed at \"Thinking\" Lengths\n\nThe team initially tried standard Post-Training Quantization (PTQ). While it worked for short responses, it fell apart for the long chain-of-thought \"thinking\" process. As generation length increased, quantization errors accumulated, leading to degradation. Furthermore, PTQ struggled with sparse experts; if an expert wasn't hit frequently during the calibration step with the calibration dataset, it essentially \"forgot\" knowledge. QAT (Quantization Aware Training) was necessary to make the model \"lossless\" compared to the BF16 baseline.\n\n# A less discussed benefit: Faster RL Training\n\nThis is the point that often gets overlooked: Int4 QAT wasn't just for inference serving, it accelerated the training process itself. In Reinforcement Learning, the model spends a massive amount of time in the \"rollout\" phase (generating text). By using the Int4 model for these rollouts, they reduced the total time for an RL iteration by 10-20%. It also reduced the discrepancy between the training forward pass and the inference engine.\n\n# Why Int4 and not FP4?\n\nThey chose standard Int4 over newer formats like FP4 to maintain compatibility with existing hardware (non-Blackwell GPUs) and to utilize mature, highly efficient kernels like Marlin.\n\nIn summary, I believe there isn't a one-size-fits-all answer regarding quantization. It depends heavily on the model's parameters and specific architecture. It is a matter of trade-offs.\n\n[ AI translation, there may be some translation errors.](https://preview.redd.it/dzmceu5zybag1.png?width=1362&format=png&auto=webp&s=a0ba8f78c6e5ade3463a1c62fba1d338a1c01ce9)\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwpqt2p",
          "author": "nekofneko",
          "text": "Source: [Zhihu](https://www.zhihu.com/question/1969558404759544488/answer/1970539327902679960)",
          "score": 17,
          "created_utc": "2025-12-30 11:34:56",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwprq9u",
          "author": "cantgetthistowork",
          "text": "K2 is the only model that remains coherent at the advertised 256k max context",
          "score": 31,
          "created_utc": "2025-12-30 11:42:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq0ka2",
              "author": "nullmove",
              "text": "Kimi linear has frontier level quality in context arena. Waiting for a bigger model powered by KDA. Arcee also recently independently verified that KDA is a real deal.",
              "score": 12,
              "created_utc": "2025-12-30 12:50:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx55qji",
                  "author": "uhuge",
                  "text": "https://huggingface.co/arcee-ai/AFM-4.5B-Base-KDA-Only not great in math, somehowÂ ",
                  "score": 1,
                  "created_utc": "2026-01-01 21:19:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwps0s2",
              "author": "FreePart5727",
              "text": "really? I have to give it a try",
              "score": 2,
              "created_utc": "2025-12-30 11:45:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwsol2a",
              "author": "UnknownLesson",
              "text": "Even better than Gemini Pro?",
              "score": -1,
              "created_utc": "2025-12-30 20:50:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwprqnp",
          "author": "Doris_Dressy1",
          "text": "Thank you for providing another perspective, Iâ€™ve learned a lot",
          "score": 8,
          "created_utc": "2025-12-30 11:42:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuq0zg",
          "author": "Lissanro",
          "text": "INT4 format makes it really great for running locally, as GGUF Q4\\_X which preserves the INT4 quality. I also find its cache memory efficient - full 256K fits in just 96 GB VRAM at Q8. I hope they continue releasing their future large models in 4-bit format.",
          "score": 3,
          "created_utc": "2025-12-31 03:24:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}