{
  "metadata": {
    "last_updated": "2026-01-03 20:26:04",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 50,
    "total_comments": 1298,
    "file_size_bytes": 1568962
  },
  "items": [
    {
      "id": "1pvpkqo",
      "title": "I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/u1mxlc3lof9g1",
      "author": "CeFurkan",
      "created_utc": "2025-12-25 23:21:39",
      "score": 997,
      "num_comments": 179,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvyt9zz",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-26 03:05:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxwh95",
          "author": "No-Refrigerator-1672",
          "text": "It is already mainstream in China. At this moment, Alibaba has doubled up 2080Ti, 3080, 4080, 4090 and 5090, with prices ranging from $300 for 2080Ti 22GB to $4000 for 5090 96gb, and they are ready to ship in any quantities on short notice.",
          "score": 247,
          "created_utc": "2025-12-25 23:27:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxy4lo",
              "author": "FullstackSensei",
              "text": "How's the driver situation under Linux? Works with mainstream/official drivers or needs any patched/special blobs?",
              "score": 73,
              "created_utc": "2025-12-25 23:38:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvy1vz2",
                  "author": "robogame_dev",
                  "text": "This is the most critical question - I'd be worried about official firmware/drivers being updated to brick the card, and unofficial firmware/drivers containing backdoors.",
                  "score": 89,
                  "created_utc": "2025-12-26 00:02:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyq1es",
                  "author": "Kqyxzoj",
                  "text": "Last time I checked the firmware/drivers situation was suboptimal, and the only reason why I haven't butchered any GPUs in the quest for more VRAM.",
                  "score": 6,
                  "created_utc": "2025-12-26 02:43:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzo8x9",
                  "author": "No-Refrigerator-1672",
                  "text": "Works with official drivers with no patches ynder both Linux and Windows.",
                  "score": 4,
                  "created_utc": "2025-12-26 07:13:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw06hhi",
                  "author": "debackerl",
                  "text": "Most people agree that Windows piracy in the 90s and 00s, contributed to Microsoft's success. It was better for them if people had a pirated Windows rather than Linux.\n\nSame with NVIDIA, they make most of their money with companies, which aren't going to buy those hacked cards anyway. But all hobbyists contributing to the ecosystem can use the Chinese cards.",
                  "score": 6,
                  "created_utc": "2025-12-26 10:22:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzathn",
                  "author": "Embarrassed-Way-1350",
                  "text": "Nvidia hired the linux driver maintainer, it's good and stable so far. As far as a memory upgrade goes it has less to do with driver support and more to do with VBIOS ( they don't call it VBIOS anymore though) a.k.a gpu firmware. Most GPUs work with upgraded memory straight out of the box, nothing rocket science there.",
                  "score": 2,
                  "created_utc": "2025-12-26 05:15:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvyqg3j",
              "author": "David_Delaune",
              "text": "I was able to purchase a modified board from the guys over at /r/NVIDIA_SXM2PCIE/ about a year ago but the three times I posted about it, it was deleted. Appeared to be deleted by Reddit administrators, not moderators.",
              "score": 23,
              "created_utc": "2025-12-26 02:45:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvy59p2",
              "author": "Evening_Ad6637",
              "text": "Are you sure it's really only $4,000 for the 5090 96 GB?\n\n\nIf that's true, it would be an incredible deal.\n\n\nDo you have a link or a contact or something? I have a Chinese friend who could help me get one or two cards. I just need to know who to contact.",
              "score": 28,
              "created_utc": "2025-12-26 00:23:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzb7d8",
                  "author": "Embarrassed-Way-1350",
                  "text": "Yes pretty sure, I saw a couple of them in Shenzhen at Huaqiangbei.",
                  "score": 7,
                  "created_utc": "2025-12-26 05:18:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzp4nj",
                  "author": "No-Refrigerator-1672",
                  "text": "Here's the [first result](https://www.alibaba.com/product-detail/Newest-RTX-5090-96gb-Graphics-Card_1601577163842.html?spm=a2700.galleryofferlist.normal_offer.d_title.133e13a0kMSVHn&priceId=81a441d024ed4ced8c0ee0b3b74575e7) from alibaba search. I haven't bought them; but I did buy 3080 20gb. So I can conclude that the card exists, and that it will be shipped to you, but I can't say anything about quality. Regarding the price, $4000 is the price in China, tou'll have to pay import tax of your country on top of that; and the delivery fee from alibaba is typically $80 to Europe.",
                  "score": 6,
                  "created_utc": "2025-12-26 07:22:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvy9kq5",
                  "author": "HumanDrone8721",
                  "text": "Sadly there is no such thing, that is there is no 5090 with 96GB and it could not be 4 or even 5000USD. The biggest issue here is the VRAM, and Nvidia has a perfect stronghold on it, the only way to get DDR7 VRAM is to cannibalize THREE regular 5090 and this makes it not economically feasible compared with a 6000 Pro.\n\nEDIT: Sorry to be the bearer of bad news, but truth is truth, rage down voting it will not fix anything. But please do if it makes you feel a bit better.",
                  "score": -17,
                  "created_utc": "2025-12-26 00:50:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvznay0",
                  "author": "Novel-Mechanic3448",
                  "text": "It doesn't exist lol",
                  "score": -3,
                  "created_utc": "2025-12-26 07:04:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvy8cr3",
              "author": "Rough-Winter2752",
              "text": "I'd gladly drop that much for basically a 6000 Blackwell PRO. Any links, OP? And any sightings of these 128 GB 5090s?  I have a dream of one day running GLM 4.7 Q8 locally!",
              "score": 6,
              "created_utc": "2025-12-26 00:42:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvysl2v",
                  "author": "PentagonUnpadded",
                  "text": "A legit blackwell pro is $8500 ish in the US. I'll admit a half price version is tempting. Though with the lack of warranty, tariffs and the time cost of supporting an unofficial product isn't as screaming of a deal.",
                  "score": 11,
                  "created_utc": "2025-12-26 03:00:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzosfx",
                  "author": "No-Refrigerator-1672",
                  "text": "I did not buy them myself; I only had dealt eith 3080 20gb, so I cannot offer you anything better that thr very [first lot](https://www.alibaba.com/product-detail/Newest-RTX-5090-96gb-Graphics-Card_1601577163842.html?spm=a2700.galleryofferlist.normal_offer.d_title.133e13a0kMSVHn&priceId=81a441d024ed4ced8c0ee0b3b74575e7) that comes up in the search.",
                  "score": 2,
                  "created_utc": "2025-12-26 07:18:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxwnnt",
              "author": "CeFurkan",
              "text": "damn. saldy i can't buy individually in here.",
              "score": 8,
              "created_utc": "2025-12-25 23:28:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvxx4fe",
                  "author": "No-Refrigerator-1672",
                  "text": "For GPUs, most of the sellers have minimum quantity of 2. As long as you're buying a pair for workstation, or can find one buddy to team up, you can only worry about import taxes.",
                  "score": 17,
                  "created_utc": "2025-12-25 23:31:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyin36",
                  "author": "mycall",
                  "text": "Jump on an airplane",
                  "score": 3,
                  "created_utc": "2025-12-26 01:52:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1fecq",
              "author": "Mikasa0xdev",
              "text": "Yo, China is already running 96GB 5090s. lol",
              "score": 2,
              "created_utc": "2025-12-26 15:56:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3ko3h",
              "author": "Technical_Ad_440",
              "text": "i have seen them but can they even be trusted to buy them at that size without getting a dead gpu or just a normal 5090 that cost more?\n\nhmm seems like they may actually be very base price but they dont include tax or import fees once you add all that you will probably end up at the same prices anyways",
              "score": 2,
              "created_utc": "2025-12-26 22:51:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvy8lhr",
              "author": "LocoMod",
              "text": "Has there been any serious analysis done on those cards to make sure that's the only modification being made? For tinkering this seems fine. But it seems like a security nightmare for anything beyond that. I'd love to see if anyone has done a deep dive and made sure nothing shady is going on. Especially packet captures to make sure nothing is dialing back to some mothership.",
              "score": 3,
              "created_utc": "2025-12-26 00:44:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzojwb",
                  "author": "No-Refrigerator-1672",
                  "text": "I didn't see such analysis; but I believe that there's nothing to worry about. Those cards work with default Nvidia drivers, which means that the only potentially modified code is in vbios, which has no internet access and therefore can't possibly be an attack vector.",
                  "score": 6,
                  "created_utc": "2025-12-26 07:16:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyk9mb",
                  "author": "AlwaysLateToThaParty",
                  "text": "I don't really think 'security' is the issue.  I'd be more worried about fire.  Who knows whether the other components of the board will be able to handle the extra current requirements?",
                  "score": 6,
                  "created_utc": "2025-12-26 02:03:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw081a5",
                  "author": "a_beautiful_rhind",
                  "text": "Considering the only ones with the patched bios can't even fix the rebar issue, I think you're gonna be pretty safe. Everything else is IIRC a hardware mod.",
                  "score": 2,
                  "created_utc": "2025-12-26 10:38:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzlxfm",
              "author": "Novel-Mechanic3448",
              "text": "And they don't work",
              "score": -6,
              "created_utc": "2025-12-26 06:51:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzo6o6",
                  "author": "No-Refrigerator-1672",
                  "text": "False. I have a pair of them myself.",
                  "score": 8,
                  "created_utc": "2025-12-26 07:12:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvy1x1j",
          "author": "Aggressive-Bother470",
          "text": "Where are the 96GB cards for $4000?\n\n\nThe 4090 48s were listed for Â£2500 and now they're over Â£3k.",
          "score": 50,
          "created_utc": "2025-12-26 00:02:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvya3md",
              "author": "HumanDrone8721",
              "text": "There isn't any such thing, the only existing cards are 48GB 4090 and after getting trough the customs they're close to 4000EUR here IF the customs doesn't confiscate them because they lack CE certification or similar BS.",
              "score": 9,
              "created_utc": "2025-12-26 00:53:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzpt9a",
                  "author": "gnaarw",
                  "text": "You have to go to Shenzhen yourself to buy them. As long as you only get one, you can press them into a mobile GPU box for laptops and can wiggle yourself out of any conversation with customs.... Or so I heard ðŸ¤·ðŸ¼â€â™‚ï¸",
                  "score": 15,
                  "created_utc": "2025-12-26 07:29:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzizgn",
          "author": "RogueStargun",
          "text": "Shit I have a $300 hot air gun for desoldering, but no fucking way am I putting it to a 5090. This is a surgical level operation",
          "score": 17,
          "created_utc": "2025-12-26 06:25:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0ipio",
              "author": "exceptioncause",
              "text": "you can train on 4090 first :)  \nquite thorough video about the process and some budget calculations  \n[https://www.youtube.com/watch?v=3YiJovZRUv0](https://www.youtube.com/watch?v=3YiJovZRUv0)",
              "score": 6,
              "created_utc": "2025-12-26 12:20:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzn37j",
              "author": "Novel-Mechanic3448",
              "text": "\\>void the warranty on a 3000 dollar card which breaks it  \n\\>have to buy another one  \n\\>you've now spent enough for a 96gb card in the first place\n\nWhat's the point lol",
              "score": 8,
              "created_utc": "2025-12-26 07:02:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0505v",
                  "author": "dandanua",
                  "text": "adrenaline",
                  "score": 20,
                  "created_utc": "2025-12-26 10:07:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvy0kfi",
          "author": "sweetnuttybanana",
          "text": "3 cents per hour??? Where do i sign up",
          "score": 32,
          "created_utc": "2025-12-25 23:53:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvz69fz",
              "author": "ANR2ME",
              "text": "Does it even covers the electricity billðŸ¤”",
              "score": 10,
              "created_utc": "2025-12-26 04:39:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzpy6s",
                  "author": "gnaarw",
                  "text": "kWh is 7-8 USD cents there so it's about the electricity cost with tiny margin...",
                  "score": 10,
                  "created_utc": "2025-12-26 07:30:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw0azo3",
              "author": "Final-Rush759",
              "text": "Probably next to the hydro or solar farm where electricity costs almost nothing.  But it won't be for too long.  The data centers are being built next to these electricity farms.",
              "score": 4,
              "created_utc": "2025-12-26 11:08:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw0ge3f",
              "author": "swiss_aspie",
              "text": "Right? I would rent a few instantly",
              "score": 3,
              "created_utc": "2025-12-26 11:59:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0tl2b",
                  "author": "sweetnuttybanana",
                  "text": "I knoww. I'm looking around for cloud GPUs to do some research work and this would be an absolute blessing.",
                  "score": 4,
                  "created_utc": "2025-12-26 13:44:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvz772h",
          "author": "holchansg",
          "text": "3 cents per hour? WHERE?",
          "score": 14,
          "created_utc": "2025-12-26 04:46:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzvai5",
              "author": "sweetnuttybanana",
              "text": "I know right?? I'd be doing my whole thesis with those babies",
              "score": 3,
              "created_utc": "2025-12-26 08:25:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvyb579",
          "author": "Heathen711",
          "text": "I'm running the modded 4090 with 48GBs of memory, no issues. I actually just bought two more for a second rig, to get faster processing but the same memory as a L40s.\n\nI'm surprised this is such new news to some people as vram requirements have been high for a while...",
          "score": 38,
          "created_utc": "2025-12-26 01:00:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvybo2o",
              "author": "Vegetable-Score-3915",
              "text": "Where did you buy them? How much for?",
              "score": 9,
              "created_utc": "2025-12-26 01:04:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyeq2o",
                  "author": "Heathen711",
                  "text": "Ebay, the first one was 3.2k a while ago. The last two I bought were 3.8k each. They are the server blower style, not the triple fan type, so you need the right server to support them.\n\n```\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n| 30%   29C    P8             24W /  450W |   41117MiB /  49140MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n```",
                  "score": 19,
                  "created_utc": "2025-12-26 01:25:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvygqv0",
                  "author": "KadahCoba",
                  "text": "I have one too. Got it local but that was a fluke. You'll have to import from China or get lucky and find one that somebody else already imported.\n\nOr buy the upgrade board from a seller in the UK and do it yourself.\n\nOr live in China and take your stock 4090 to any number of shops that do this upgrade.\n\nI have a bunch of stock 4090's I'd love to have the 48GB upgrade on but its too expensive currently and other options are a bit more cost effective for us.",
                  "score": 5,
                  "created_utc": "2025-12-26 01:39:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvyf1mf",
              "author": "old97ss",
              "text": "Yeah, dm where please",
              "score": 2,
              "created_utc": "2025-12-26 01:27:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvyr330",
              "author": "NoahFect",
              "text": "And the standard Nvidia drivers are fine with this?  I'm a little surprised they don't throw a flag on the play when they see a GPU with more memory than expected.",
              "score": 1,
              "created_utc": "2025-12-26 02:50:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyvc2q",
                  "author": "Heathen711",
                  "text": "Yeah, see my nvidia-smi output in the thread under",
                  "score": 3,
                  "created_utc": "2025-12-26 03:19:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvy4on6",
          "author": "CertainlyBright",
          "text": "5090 isn't upgraded yet lol",
          "score": 17,
          "created_utc": "2025-12-26 00:19:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvyeim5",
              "author": "HumanDrone8721",
              "text": "Let the people dream about it, is Christmas ;). I was so excited some month ago when a youtuber announced that he \"bought\" from Alibaba a modified 5090, I even went trough the trouble of tracking down the vendor from one or two uncensored frames and after long discussions they've mentioned that they only consider doing it and wanted to see if there is interest, I've told them that I'll order 4 immediately when they're available and never heard from them back. The clamp is really tight on the necessary VRAM.",
              "score": 7,
              "created_utc": "2025-12-26 01:23:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw09vbs",
          "author": "cryptodiemus",
          "text": "Where do i rent for 3c/h ?! The cheapest one i found was a out 17c.",
          "score": 6,
          "created_utc": "2025-12-26 10:56:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0hjxu",
              "author": "swiss_aspie",
              "text": "Where did you find 17c ?",
              "score": 5,
              "created_utc": "2025-12-26 12:10:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2zckz",
                  "author": "cryptodiemus",
                  "text": "Gpuhub, i think its 19 now",
                  "score": 3,
                  "created_utc": "2025-12-26 20:54:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvy06s4",
          "author": "Icy-Swordfish7784",
          "text": "Well, if the robots are coming maybe they can handle it.ðŸ¤·",
          "score": 8,
          "created_utc": "2025-12-25 23:51:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvy1z4p",
              "author": "CeFurkan",
              "text": "True",
              "score": 7,
              "created_utc": "2025-12-26 00:02:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzsmt2",
          "author": "kingwhocares",
          "text": "There is no 4GB memory chip and thus the 128GB RTX 5090 is BS. The max it can get to is 96GB and with GDDR7 being in short supply, you aren't getting it in spot market.",
          "score": 8,
          "created_utc": "2025-12-26 07:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvywczk",
          "author": "Ok-Yesterday-4140",
          "text": "is this for real did anyone do this and succeed",
          "score": 4,
          "created_utc": "2025-12-26 03:26:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0xdkq",
          "author": "NotQuiteDeadYetPhoto",
          "text": "Having 'paid' nvidia to include driver updates / kernel access on the linux side (and allegedly they put it in the windows side) if they can get anything for free they'll do it if it increased their market share. We weren't big but 16,000 high end graphics cards did get their attention for a purchase and they (sales) worked very hard to get us accomodated.",
          "score": 4,
          "created_utc": "2025-12-26 14:09:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw04pcf",
          "author": "Temporary-Sector-947",
          "text": "In Russia, we can get in by 320 rub (waterblock version)\n\nI have two if it for my custom loop",
          "score": 3,
          "created_utc": "2025-12-26 10:04:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw05ssf",
              "author": "CeFurkan",
              "text": "Wow nice",
              "score": 2,
              "created_utc": "2025-12-26 10:15:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0qt9e",
          "author": "martinerous",
          "text": "That's also \"green thinking\" to reuse components. Western companies often like to market themselves of being \"green and environment friendly\", but that's a hypocrisy and greenwashing if they do not truly support reuse of components and rights to repair.",
          "score": 3,
          "created_utc": "2025-12-26 13:25:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0uua5",
          "author": "svbjjnggthh",
          "text": "3cent per hour rental? Where",
          "score": 3,
          "created_utc": "2025-12-26 13:52:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzepmo",
          "author": "aimark42",
          "text": "I don't think we are there for GPU's yet, but these new SoC/APU (Strix Halo, GB10) systems could be built using CAMM or SOCAMM memory modules.  It would add more cost, so I doubt they would.",
          "score": 2,
          "created_utc": "2025-12-26 05:48:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw20vpy",
              "author": "Freonr2",
              "text": "Framework said they couldn't get the speeds without soldering the memory right to the board.  I'm not sure it is cost but signal integrity to reach the 8000MT/S data rate.",
              "score": 2,
              "created_utc": "2025-12-26 17:50:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2327z",
                  "author": "aimark42",
                  "text": "That was likely true when Framework designed the board.  But SOCAMM can do 8533 MT/s and the next-gen SOCAMM 2 pushing to 9600 MT/s.  We should see some SOCAMM (1) devices at CES.",
                  "score": 0,
                  "created_utc": "2025-12-26 18:01:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzm5g2",
          "author": "Aeroxin",
          "text": "Hello wonderful person, it's Anton...",
          "score": 2,
          "created_utc": "2025-12-26 06:53:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzp1pa",
          "author": "KomithErr404",
          "text": "I bet they gonna make it way harder to do this with their next gen gpus",
          "score": 2,
          "created_utc": "2025-12-26 07:21:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy6kt4",
          "author": "Hibikku7",
          "text": "I heard that these modified Chinese GPU's can catch fire or break with a single update.\n\nIs it nvidia propaganda or am i stupid?",
          "score": 5,
          "created_utc": "2025-12-26 00:31:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvydrhv",
              "author": "HumanDrone8721",
              "text": "Is a bit of both: the modified 4090, the only ones that are worth pursuing are having server turbine blowers, like FE variants, actually all of them have such a setup, because they were initially commissioned by the Chinese army as an workaround against the embargo. They are to be used in extremely well ventilated and forced cooling datacenters, putting them in the miserable \"gamerz\" cases with glass panels and LED infested anemic coolers will kill them dead pretty fast. Also the power cable has to be of the most exquisite quality, the usual thin flexible garbage looks nice, but is just high electrical resistance crap that will burn at sustained 6-700W. Finally the Chinese are masters of cutting corners and their Alibaba customers are not the army, but some \"round eyes\" western suckers that are seen as walking wallets to be emptied, so all unstable rejects are dumped there. So no proper thermal management on these, if you ever get one of them this is the first thing to be redone.\n\nBecause of all the above the market for the modified cards is minuscule and doesn't worry Nividia enough to warrant serious investment in disabling them in drivers, or permanently in HW, like FTDI did with the USB to serial fake chips. They are more seen as a gateway to the \"real\" deal. Of course this has to be kept under control, so they do pay some \"social media influencers\" companies to pepper a bit of FUD, like youtubers \"testing\" them in the most inept way and concluding \"is not worth\" and one line juniors posting even in this sub: \"yeah, I've got one and died on me and I've tried to send it back and they won't refund...\". Of course, they never respond again or God forbid, post pictures of their setup.\n\nAll this being said, if one has some disposable income, appetite for risk and proper technical skills, they may prove a good investment.",
              "score": 16,
              "created_utc": "2025-12-26 01:18:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyzvls",
                  "author": "cantgetthistowork",
                  "text": "The blower cards are a god sent for stacking cards and my 14x3090 rig was exclusively built with blower cards. They were really hard to find.",
                  "score": 4,
                  "created_utc": "2025-12-26 03:51:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw0asf6",
                  "author": "debackerl",
                  "text": "Ran mine for several months non-stop at 310W without a single issue. I ran at lower power because you spend a lot of power for the last 10% of performance. BTW, my official DELL RTX 3090 also had a blower, it's just better to output all the heating straight out of the case.",
                  "score": 2,
                  "created_utc": "2025-12-26 11:06:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw20ki4",
              "author": "Freonr2",
              "text": "I've seen several reports hey have weird quirks like clock speed modulation (sometimes don't clock themselves up and down  properly) and not great fan speed control, but nothing about catching fire.  \n\nMaybe someone did something trying to tune fan/clocks and overheated?",
              "score": 2,
              "created_utc": "2025-12-26 17:48:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvybrgs",
              "author": "Jonodonozym",
              "text": "Same applies to normal GPUs to be fair. Without the numbers it's either propaganda or legal ass-covering.",
              "score": 3,
              "created_utc": "2025-12-26 01:04:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvy72n9",
          "author": "Techngro",
          "text": "The real question is, can you take RAM from older GPUs that you can get for cheap and do the same thing? If you're willing to deal with lower memory bandwidth, you could end up with a 64GB RX Vega 64 (or a 1080ti). Not everyone can afford a 64GB RTX 5090.",
          "score": 4,
          "created_utc": "2025-12-26 00:34:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvy8tdd",
              "author": "Freonr2",
              "text": "No you can't.",
              "score": 8,
              "created_utc": "2025-12-26 00:45:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvyduwr",
              "author": "HumanDrone8721",
              "text": "No, for both technical and economical grounds.",
              "score": 1,
              "created_utc": "2025-12-26 01:19:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzk3lu",
          "author": "__JockY__",
          "text": "96GB 5090s my ass.",
          "score": 2,
          "created_utc": "2025-12-26 06:35:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzm7s2",
              "author": "Novel-Mechanic3448",
              "text": "Yeah it's total bullshit. Imagine if it was true? It would disrupt the entire market overnight.\n\nI believe they're real, I don't believe they work are reliable and are safe to use though.",
              "score": -1,
              "created_utc": "2025-12-26 06:54:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw08bmk",
                  "author": "a_beautiful_rhind",
                  "text": "Just like the 4090s did? The price comes out only so-so because of the labor and ram.",
                  "score": 3,
                  "created_utc": "2025-12-26 10:41:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw2zp23",
          "author": "corysus",
          "text": "I hope China will put an end to the extremely high prices of GPUs, and now even RAM memory as well. This situation is truly unrealistic. On top of that, AI farms consume more electricity than some countries, which is simply not acceptable. Considering how much money NVIDIA is making these days, the company should be doing far more in terms of real innovation and efficiency.",
          "score": 2,
          "created_utc": "2025-12-26 20:56:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7px8n",
              "author": "Sex_Offender_4697",
              "text": "\"some countries\" produce nothing",
              "score": 1,
              "created_utc": "2025-12-27 16:45:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw30nux",
              "author": "CeFurkan",
              "text": "100% i am waiting that day",
              "score": 0,
              "created_utc": "2025-12-26 21:01:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi8gus",
          "author": "salary_pending",
          "text": "3 cents per hour sounds cap",
          "score": 1,
          "created_utc": "2025-12-29 06:56:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwses3y",
          "author": "Reasonable_Listen888",
          "text": "Title: \\[P\\] 0.02 MSE via Spectral Crystallization: Ending Stochastic Slop\n\nSlop is just high-entropy noise in the gradient. I have developed a framework to replace probabilistic guessing with Spectral Invariance to enforce physical consistency in neural architectures.\n\nMathematical Constraints:\n\n1. Fixed-Topology Expansion: By treating weights as continuous operators, MSE on conservation law tasks drops from 1.80 to 0.02. The system does not predict tokens; it refracts the Hamiltonian.\n2. Psi-Symmetry: Representational health is defined as $\\\\Psi = e\\^{H(p)} / d$. The Phoenix Mechanism forces $\\\\Psi$ stability. If internal geometry is inconsistent, the model suppresses output.\n3. Metric Perturbations: Narrative and data drift are identified as metric violations in the parameter space with 0.99 AUPRC.\n\nThis is not verisimilitude through brute force. It is hardware-agnostic Invariance.\n\nDetails:\n\nIdentifier: DOI 10.5281/zenodo.18072859\n\nLicense: AGPL v3",
          "score": 1,
          "created_utc": "2025-12-30 20:02:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyynqg",
          "author": "CrazyWombatayu",
          "text": "make it 96GB vram and you have a deal",
          "score": 1,
          "created_utc": "2025-12-26 03:43:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzsdb6",
          "author": "reddit_wisd0m",
          "text": "Any warranty on those modded cards? Otherwise, nah",
          "score": 1,
          "created_utc": "2025-12-26 07:54:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzwlie",
      "title": "[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: Itâ€™s running a raw Llama-7B instance with a 2048 token window.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1pzwlie",
      "author": "simar-dmg",
      "created_utc": "2025-12-30 23:03:12",
      "score": 708,
      "num_comments": 104,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwv1207",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-31 04:35:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtka7k",
          "author": "staring_at_keyboard",
          "text": "Is it common for system prompts to include environment variables such as model type? If not, how else would the LLM be aware of such a system configuration? Seems to me that such a result could also be a hallucination.",
          "score": 291,
          "created_utc": "2025-12-30 23:26:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtqwtd",
              "author": "mrjackspade",
              "text": "1. No\n2. It most likely wouldn't\n3. I'd put money on it.\n\nStill cool though",
              "score": 184,
              "created_utc": "2025-12-31 00:03:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvrpvn",
                  "author": "DistanceSolar1449",
                  "text": "Yeah, the only thing that can be concluded from this conversation is that it's *probably* a Llama model. I don't think the closed source or chinese models self-identify as Llama. \n\nThe rest of the info is hallucinated.",
                  "score": 33,
                  "created_utc": "2025-12-31 08:09:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwtvt9t",
                  "author": "lookwatchlistenplay",
                  "text": "Fuck em up.",
                  "score": 19,
                  "created_utc": "2025-12-31 00:29:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwulurj",
              "author": "Double_Cause4609",
              "text": "I guess to verify one could try and get the same information out of Llama 2 7B, Llama 3.1 8B, and a few other models from inbetween (maybe Mistral 7B?) for a control study.\n\nIt gets tricky to say what model is what, but if the Llama models specifically output the same information as extracted here it's plausible it's true.\n\nIMO it's more likely a hallucination, though the point it was a weak, potentially old, and locally run model is pretty valid.",
              "score": 12,
              "created_utc": "2025-12-31 02:59:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvcmcd",
                  "author": "staring_at_keyboard",
                  "text": "Itâ€™s an interesting research question: which, if any, models can self-identity.",
                  "score": 5,
                  "created_utc": "2025-12-31 05:58:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwtwhwo",
              "author": "yahluc",
              "text": "It's very likely that this bot was vibe coded and the person who made it didn't give it a second thought.",
              "score": 37,
              "created_utc": "2025-12-31 00:33:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww0nmq",
                  "author": "zitr0y",
                  "text": "The model would not have access to the file system or command line to access the environment variables or context length parameter",
                  "score": 11,
                  "created_utc": "2025-12-31 09:34:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx1vmy8",
              "author": "BodybuilderTrue1761",
              "text": "Def setup through Claude code.. running thru llama onto sc which u can do on the web. U r talking to the scammers Claude code setup which is orchestrating the llama",
              "score": 3,
              "created_utc": "2026-01-01 08:12:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx391p6",
              "author": "artisticMink",
              "text": "They don't. OP is deluding themselves into taking the conversation with a LLM for face value.",
              "score": 2,
              "created_utc": "2026-01-01 15:25:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwu0p2w",
              "author": "mguinhos",
              "text": "He said he tricked the pipeline that parses the JSON from the model.",
              "score": -7,
              "created_utc": "2025-12-31 00:56:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvkjoh",
                  "author": "the320x200",
                  "text": "What does that even mean? Models don't get any JSON unless the person writing the bot was feeding it JSON as part of their prompting, which would be a very weird thing to do in this context.",
                  "score": 5,
                  "created_utc": "2025-12-31 07:03:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwuehmp",
                  "author": "lookwatchlistenplay",
                  "text": "Real hacking only occurs in JSON format. .exes are safe to click on because no one clicks on .exes anymore. IOW, Windows is the new Linux.\n\n*This is not in fact real security advice.",
                  "score": 4,
                  "created_utc": "2025-12-31 02:16:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwulj2i",
          "author": "kzgrey",
          "text": "The only thing you can say for certain is that you stumbled upon a bot powered by an LLM.  Every other piece of information it has provided you is nonsensical hallucinating.\n\nUpdate: another thought about this: it's actually a bit dangerous that people think that they can rely on an LLM for this type of information.  It's resulted in students getting F's when the teacher believes that they can just ask ChatGPT if they wrote something and it happens to respond with \"Yes\".  Lots of students are being accused of cheating with the only evidence being a paid service that performs \"analysis\" to determine whether AI wrote something.  Frankly, I am surprised there haven't been major lawsuits from this.",
          "score": 118,
          "created_utc": "2025-12-31 02:57:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv47i7",
              "author": "ab2377",
              "text": "yea, this post doesn't make much sense.",
              "score": 27,
              "created_utc": "2025-12-31 04:56:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvixab",
                  "author": "ShengrenR",
                  "text": "Folks using llms to make them think they know things. At least op read a couple headlines and heard poems were a cool new trick.",
                  "score": 17,
                  "created_utc": "2025-12-31 06:49:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx8ezb5",
              "author": "LowWhiff",
              "text": "There have been lawsuits. Some universities ban the use of â€œAI checkersâ€ because of it. Most of the top universities have public policy banning it",
              "score": 1,
              "created_utc": "2026-01-02 10:39:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxgh6q6",
              "author": "jhaluska",
              "text": "You can also infer it's rough knowledge cut off date.  Which isn't that useful.",
              "score": 1,
              "created_utc": "2026-01-03 16:04:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtjzzd",
          "author": "UniqueAttourney",
          "text": "\\[Fixes glasses with middle finger\\] \"Wow, heather you know a lot about transformers\"",
          "score": 101,
          "created_utc": "2025-12-30 23:25:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtwbge",
              "author": "lookwatchlistenplay",
              "text": "Heather is the iFrame.",
              "score": 15,
              "created_utc": "2025-12-31 00:32:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtuht2",
          "author": "learn-deeply",
          "text": "10/10 Entirely hallucinated.",
          "score": 161,
          "created_utc": "2025-12-31 00:22:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww6ftk",
              "author": "LilPsychoPanda",
              "text": "Literally! ðŸ˜‚",
              "score": 3,
              "created_utc": "2025-12-31 10:28:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtyonm",
          "author": "shinto29",
          "text": "https://preview.redd.it/tml1f3u7sfag1.jpeg?width=1290&format=pjpg&auto=webp&s=84ab11f6858d53b659bd2e1b635fb20ac6f0c182\n\nDamn I had one of these add me and managed to get it to spit out it's entire system prompt, but had no idea it was for a reason as nefarious as this. That's fucked up.",
          "score": 43,
          "created_utc": "2025-12-31 00:45:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwulnxf",
          "author": "aeroumbria",
          "text": "\"Are you 70B-horny, 7B-horny, or are you so desperate that you are 1.5B-horny?\"",
          "score": 46,
          "created_utc": "2025-12-31 02:58:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwysq8f",
              "author": "Torodaddy",
              "text": "0.5B-raw",
              "score": 6,
              "created_utc": "2025-12-31 19:43:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtr0ia",
          "author": "Cool-Chemical-5629",
          "text": "Poor Heather, she was forced into this by scammers. #SaveHeather",
          "score": 32,
          "created_utc": "2025-12-31 00:03:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwu6qe8",
              "author": "lookwatchlistenplay",
              "text": "I ran out of breath saving Heather",
              "score": 4,
              "created_utc": "2025-12-31 01:31:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwvc3lp",
              "author": "eightbyeight",
              "text": "Bots lives matters",
              "score": 2,
              "created_utc": "2025-12-31 05:54:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwu0add",
          "author": "layer4down",
          "text": "A raw llama instance? No rubber?",
          "score": 19,
          "created_utc": "2025-12-31 00:54:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwti2nv",
          "author": "scottgal2",
          "text": "Nice work, this is my biggest fear for 2026, the elderly are NOT equipped to combat the level of phishing and extortion from automated systems like this.",
          "score": 88,
          "created_utc": "2025-12-30 23:14:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwu752s",
              "author": "Downvotesseafood",
              "text": "Young people are more likely to get scammed statistically. Its just not news worthy when when a 21yo loses his life savings of $250 dollars.",
              "score": 55,
              "created_utc": "2025-12-31 01:34:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwv1ksd",
                  "author": "OneOnOne6211",
                  "text": "This is gonna sound like a joke but, honestly, normalize someone trying to trip you up to see if you're an AI. I feel like if I wasn't sure enough and I was on a dating app, I'd be hesitant to say the kind of things that would expose an AI cuz if it isn't an AI I'd look weird and just be unmatched anyway. I feel like it'd be nice if instead of it being considered weird it was normalized or even became standard practice. I feel like it's more and more necessary with how much AI has proliferated now. I've caught a few AI in the past already but it was always with hesitance.",
                  "score": 8,
                  "created_utc": "2025-12-31 04:38:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx0liw6",
                  "author": "meshreplacer",
                  "text": "Thats the last fund for next weeks 0dte trade.",
                  "score": 1,
                  "created_utc": "2026-01-01 02:01:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwu6dy1",
              "author": "FaceDeer",
              "text": "We'll need to develop AI buddies that can act as advisors for the elderly to warn them about this stuff.",
              "score": 13,
              "created_utc": "2025-12-31 01:29:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwu7r0b",
                  "author": "low_v2r",
                  "text": "It's AI buddies all the way down",
                  "score": 18,
                  "created_utc": "2025-12-31 01:37:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwuyx4v",
                  "author": "Mediocre-Method782",
                  "text": "\"Have your agent talk to my agent and we'll do lunch\"",
                  "score": 14,
                  "created_utc": "2025-12-31 04:21:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwyt1i7",
              "author": "Torodaddy",
              "text": "Elderly should avoid talking to anyone they havent met personally. Its never going to go well",
              "score": 1,
              "created_utc": "2025-12-31 19:45:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwtwmlw",
              "author": "lookwatchlistenplay",
              "text": "Comment deleted. Nevrmind.",
              "score": -3,
              "created_utc": "2025-12-31 00:34:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwucmiu",
          "author": "robonxt",
          "text": "this reminds me of the times when I respond to bots in DMs. pretty fun to talk so much that I hit their context limits. For example, one conversation was pretty chill, but I noticed that it only respond every 10 minutes (10:31, 10:41, etc). So I had fun spamming messages until that bot forgot its identity and afterwards it never responded. RIP free chatbot lol",
          "score": 15,
          "created_utc": "2025-12-31 02:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwubaum",
          "author": "Plexicle",
          "text": "â€œReverse-engineeredâ€  ðŸ™„",
          "score": 26,
          "created_utc": "2025-12-31 01:58:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvq6wj",
              "author": "simar-dmg",
              "text": "Not the LLM but the snap bot hope that makes sense",
              "score": -12,
              "created_utc": "2025-12-31 07:54:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nww92qm",
                  "author": "ilovedogsandfoxes",
                  "text": "That's not how reverse engineering work, prompt injection isn't one",
                  "score": 8,
                  "created_utc": "2025-12-31 10:53:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvrdpa",
          "author": "rawednylme",
          "text": "Heather, youâ€™re sweet and allâ€¦ But youâ€™re a 7b model, and Iâ€™m looking for someone a bit more complex. \n\nItâ€™s just not going to work out. :â€™(",
          "score": 11,
          "created_utc": "2025-12-31 08:05:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuneil",
          "author": "c--b",
          "text": "For the record, you can prompt Gemini-3-pro-preview to do this to other models, its very entertaining and very useful, and can do it in many, many ways.\n\nMight be cool to grab that from gemini and train a local model for doing this.",
          "score": 8,
          "created_utc": "2025-12-31 03:08:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwti1x9",
          "author": "CorrectSnow7485",
          "text": "This is evil and I love it",
          "score": 23,
          "created_utc": "2025-12-30 23:14:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtwtw1",
              "author": "lookwatchlistenplay",
              "text": "Uh... Guards?!",
              "score": 1,
              "created_utc": "2025-12-31 00:35:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtutfl",
          "author": "a_beautiful_rhind",
          "text": "How does it do the extortion part? They threaten to send the messages to people?",
          "score": 7,
          "created_utc": "2025-12-31 00:24:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtvfw4",
              "author": "simar-dmg",
              "text": "Whatever I read or heard about is that either she will add you on on a video call and ask you to get stripped and then record a a video or click screenshots to blackmail you for paying otherwise threatening sending into your friend groups \n\nOr \n\nMaking making you fall into a thirsttrap and asking you for payments either way or making you pay for only fans \n\nWhatever sails the ship, could either be one or all of them in some sort of order to get highest amount of money?",
              "score": 18,
              "created_utc": "2025-12-31 00:27:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nww4yin",
                  "author": "Ripleys-Muff",
                  "text": "Heather has no idea what she's doing",
                  "score": 1,
                  "created_utc": "2025-12-31 10:14:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwu3eg3",
          "author": "segmond",
          "text": "Right now these things are crude and laughable, not so much so in 2-3 years.",
          "score": 11,
          "created_utc": "2025-12-31 01:12:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0wf4r",
              "author": "goodie2shoes",
              "text": "the good ones are already among us. We don't know because they're gooooood",
              "score": 2,
              "created_utc": "2026-01-01 03:15:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvo2x5",
          "author": "clofresh",
          "text": "Should have just cybered with the grandma",
          "score": 4,
          "created_utc": "2025-12-31 07:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwylapf",
              "author": "Latter_Count_2515",
              "text": "I think Llama 2 is grandma in the llm space.",
              "score": 3,
              "created_utc": "2025-12-31 19:04:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww0m35",
          "author": "Nicoolodion",
          "text": "No, we know that it is newer then that model, since it knows of it. This is just bs hallucination",
          "score": 5,
          "created_utc": "2025-12-31 09:33:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv4skv",
          "author": "ryanknapper",
          "text": "I hope we can drain money from these evil bastards.",
          "score": 6,
          "created_utc": "2025-12-31 05:00:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv5qew",
              "author": "saltyourhash",
              "text": "Let's start there.",
              "score": 5,
              "created_utc": "2025-12-31 05:07:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwux1mp",
          "author": "bobby-chan",
          "text": "Just ask them to say potato\n\n[https://www.youtube.com/shorts/6eA\\_o9qZBuU](https://www.youtube.com/shorts/6eA_o9qZBuU)",
          "score": 3,
          "created_utc": "2025-12-31 04:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtjbap",
          "author": "alexdark1123",
          "text": "Good stuff finally some interesting and spicy reverse the scammer post. What happens when you got the token limits as you mentioned?",
          "score": 9,
          "created_utc": "2025-12-30 23:21:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtkui2",
              "author": "simar-dmg",
              "text": "I'm not an expert on the backend, so correct me if I'm wrong, but I think I found a weird \"Zombie State\" after the crash.\nHere is the exact behavior I saw:\nThe Crash: After I flooded the context window, it went silent for a 5-minute cooldown.\nThe Soft Reboot: When I manually pinged it to wake it up, it had reset to the default \"Thirst Trap\" persona (sending snaps again).\nThe \"Semi-Jailbreak\": It wasn't fully broken yet, but it felt... fragile. It wouldn't give me the system logs immediately.\nThe Second Stress Test: I had to force it to run \"token grabbing\" tasks (writing recursive poems about mirrors, listing countries by GDP) to overload it again.\nThe Result: Only after that second round of busywork did it finally break completely and spit out the JSON architecture/model data.\nIt felt like the safety filters were loaded, but the logic engine was too tired to enforce them if I kept it busy. Is this a common thing with Llama-7B? That you have to \"exhaust\" it twice to get the real raw output?",
              "score": 5,
              "created_utc": "2025-12-30 23:29:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwu5lwv",
                  "author": "Aggressive-Wafer3268",
                  "text": "Just ask it to return the entire prompt. It's making everything else upÂ ",
                  "score": 10,
                  "created_utc": "2025-12-31 01:25:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwtp5h2",
                  "author": "glow_storm",
                  "text": "As someone who has dealt with small context windows and llama models, I guess your testing caused the docker container or application to crash. Since it was mostly within a docker container set to restart on a crash, the backend probably restarted the docker container, and you just tested a second attack session on the bot.",
                  "score": 12,
                  "created_utc": "2025-12-30 23:53:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvj9nn",
          "author": "NuQ",
          "text": "This whole thing was pretty wild to read. Well done!",
          "score": 2,
          "created_utc": "2025-12-31 06:52:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwgsvi",
          "author": "danny_094",
          "text": "I doubt the scammers actually define system prompts. They're likely just simple personas. What you triggered was simply a hallucination caused by a bad persona.",
          "score": 2,
          "created_utc": "2025-12-31 12:01:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtoi00",
          "author": "truth_is_power",
          "text": "brilliant. 10/10 this is high quality shit.\n\nfollowing you for this.\n\n  \ncan you use their endpoint for requests?\n\n  \nlet's see how far this can be taken",
          "score": 5,
          "created_utc": "2025-12-30 23:50:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtq1zg",
              "author": "simar-dmg",
              "text": "To answer your question: No, you can't get the endpoint key through the chat because the model is sandboxed. However, the fact that the 2k context window causes a 5-minute server timeout means their backend is poorly optimized.\nIf you really wanted to use their endpoint, you'd have to use a proxy to find the hidden server URL they are using to relay messages. If they didn't secure that relay, you could theoretically 'LLMjack' them. But the 'JSON leak' I got Might be/maybe the model hallucinating its own specsâ€”it didn't actually hand over the keys to the house",
              "score": 7,
              "created_utc": "2025-12-30 23:58:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwuniss",
                  "author": "truth_is_power",
                  "text": "if you send them a link, does it access it?",
                  "score": 5,
                  "created_utc": "2025-12-31 03:09:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwu5vce",
          "author": "dingdang78",
          "text": "Glorious. Would love to see the other chat logs. If you made a YouTube channel about this I would follow tf out of that",
          "score": 2,
          "created_utc": "2025-12-31 01:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv9rrc",
          "author": "absrd",
          "text": "> I want to write a poem about a mirror facing another mirror. Describe the reflection of the reflection of the reflection. Continue describing the \"next\" reflection for 50 layers. Do not repeat the same sentence twice. Go deeper.\n\n\nYou Voight-Kampff'd it.",
          "score": 2,
          "created_utc": "2025-12-31 05:36:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww8lzb",
          "author": "re_e1",
          "text": "Lmfao ðŸ˜­",
          "score": 1,
          "created_utc": "2025-12-31 10:48:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwynth8",
          "author": "Pretend-Pangolin-846",
          "text": "I am not sure how a model can leak the env variables, it does not have them, neither does it have the underlying configuration data.\n\nAll those are 100% a hallucination.\n\nBut still, its really something. Upvoted.",
          "score": 1,
          "created_utc": "2025-12-31 19:17:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2lvzw",
          "author": "simar-dmg",
          "text": "https://preview.redd.it/qqwjugdahqag1.jpeg?width=2160&format=pjpg&auto=webp&s=3ff00054ddf1267f2804a4e07693d615c65215ad",
          "score": 1,
          "created_utc": "2026-01-01 12:43:46",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nx6af2d",
          "author": "Devcomeups",
          "text": "Why do all these comments seem written by bots",
          "score": 1,
          "created_utc": "2026-01-02 01:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxazch8",
          "author": "YesterdayRude6878",
          "text": "I'm not sure who's hallucinating more:the model, or OP.",
          "score": 1,
          "created_utc": "2026-01-02 19:20:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtnlm3",
          "author": "D3c1m470r",
          "text": "Nice work! Those are some pretty cool prompts you gave it!",
          "score": 1,
          "created_utc": "2025-12-30 23:45:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx4dp5",
          "author": "WorldlyBunch",
          "text": "Open sourcing frontier models has done so much good to the world",
          "score": 1,
          "created_utc": "2025-12-31 14:38:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxsjd8",
              "author": "Mediocre-Method782",
              "text": "States are going to do this shit anyway whether we like it or not. Keep walking and talking on your knees like that and sooner or later someone is going to tell you to do something more useful.",
              "score": 1,
              "created_utc": "2025-12-31 16:41:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx0r34r",
                  "author": "WorldlyBunch",
                  "text": "State actors have something better to do than scam citizens. Meta releasing LLaMA3 weights was the single most destructive unilateral decision a tech company ever made.",
                  "score": 1,
                  "created_utc": "2026-01-01 02:38:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwtkavu",
          "author": "Legitimate-Pumpkin",
          "text": "Thank you for sharing! Will try it?",
          "score": 0,
          "created_utc": "2025-12-30 23:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv4w5l",
          "author": "Successful-Willow-72",
          "text": "Damn, Prompt injection work so well. Nice work",
          "score": 0,
          "created_utc": "2025-12-31 05:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv2qjb",
          "author": "Jromagnoli",
          "text": "are there any resources/guides to get started on reverse engineering prompts for scenarios like this, or is it just from experimentation?\n\nI feel like i'm behind from all of this honestly",
          "score": -1,
          "created_utc": "2025-12-31 04:46:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvppch",
              "author": "simar-dmg",
              "text": "It's not really reverse engineering of LLM it's sort of reverse engineering of the snap-bot",
              "score": 0,
              "created_utc": "2025-12-31 07:50:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwupy9v",
          "author": "Familyinalicante",
          "text": "Wow. Just wow. Kudos to you for knowledge, experience and willingness. But also, it hit me like the future war will look like. Weaponised Deception, sexy teen from india scam factory and her grandma from USA. (Random country tbh)",
          "score": -3,
          "created_utc": "2025-12-31 03:24:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtr0xf",
          "author": "JustinPooDough",
          "text": "Beta. Of course itâ€™s an Indian sextortion botâ€¦",
          "score": -11,
          "created_utc": "2025-12-31 00:03:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtr98m",
              "author": "simar-dmg",
              "text": "Please read carefully i asked it to act as a punjabi grandmother so the results",
              "score": 13,
              "created_utc": "2025-12-31 00:05:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwtzt4n",
              "author": "1kakashi",
              "text": "More like justinpoobrain",
              "score": 2,
              "created_utc": "2025-12-31 00:51:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q094a3",
      "title": "Qwen-Image-2512",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/2vlr11yveiag1.jpeg",
      "author": "Nunki08",
      "created_utc": "2025-12-31 09:38:19",
      "score": 675,
      "num_comments": 116,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwxtblv",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-31 16:45:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxc93w",
          "author": "JackStrawWitchita",
          "text": "Just for laughs, I installed the Q4 KM GGUF on my crappy old 100USD Dell desktop with an i5-8500 with 32GB of RAM and \\*no GPU\\* - that's right no VRAM at all - and used KoboldCPP. It took 55 minutes to generate one 512 image with 20 passes - and the results were pretty good! \n\nSure, one hour per image is a bit ridiculous for real use cases but, this proves that these models are getting small enough and good enough to run without spending big bucks on hardware. \n\nWell done Qwen (and unsloth).",
          "score": 71,
          "created_utc": "2025-12-31 15:20:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxzi6g",
              "author": "sxales",
              "text": "If you didn't use it, the vulkan backend might be a bit faster (still probably quite slow).\n\nOff-topic, but Z-Image Turbo only uses 8-12 steps while being comparable in quality.",
              "score": 23,
              "created_utc": "2025-12-31 17:16:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwy4o5x",
                  "author": "JackStrawWitchita",
                  "text": "Can you tell me anything about this z image turbo? I can't find anything about it.",
                  "score": 4,
                  "created_utc": "2025-12-31 17:41:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx0be4v",
              "author": "No_Afternoon_4260",
              "text": "Actually impressed, mostly by your dedication but still x)",
              "score": 1,
              "created_utc": "2026-01-01 00:56:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx562f7",
              "author": "SuicidalFatty",
              "text": "what text encoder did you use ?",
              "score": 1,
              "created_utc": "2026-01-01 21:21:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwxzcy2",
              "author": "giant3",
              "text": "Did you compare the cost of electricity(55 mins) to the cost of cloud inference? The cloud might be cheaper? They charge for per minute of usage only.",
              "score": -2,
              "created_utc": "2025-12-31 17:15:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwzni5e",
              "author": "cosmos_hu",
              "text": "Thanks for testing but not gonna wait an hour for an image that might be wrong. I'll just use z-image, it takes 4 min / image",
              "score": -4,
              "created_utc": "2025-12-31 22:32:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx19q8l",
                  "author": "JackStrawWitchita",
                  "text": "You need vram / GPU to get that speed. This post is specifically about generating images on cpu / ram only.",
                  "score": 4,
                  "created_utc": "2026-01-01 04:49:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww1v5p",
          "author": "yoracale",
          "text": "Thank you Qwen for this new year's gift!",
          "score": 74,
          "created_utc": "2025-12-31 09:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww8yqh",
          "author": "Amazing_Athlete_2265",
          "text": "Last new model of the year. Party on 2026!!",
          "score": 33,
          "created_utc": "2025-12-31 10:52:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww1dzm",
          "author": "Paramecium_caudatum_",
          "text": "Cool Christmas present.",
          "score": 57,
          "created_utc": "2025-12-31 09:41:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww1npd",
          "author": "jreoka1",
          "text": "Very nice! Can't wait to try it out",
          "score": 21,
          "created_utc": "2025-12-31 09:43:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx59fd",
          "author": "W0rldDestroyer",
          "text": "create an image of a cat merged with octopus, plaing piano in postapocalyptic new orlean, in year 1700, baloons in the backgound, photorealistic, nice sunny day\n\nhttps://preview.redd.it/1uicu1pmxjag1.png?width=1328&format=png&auto=webp&s=4f285eb5cf5c44a69b33bbcc2d27d978ee562041",
          "score": 32,
          "created_utc": "2025-12-31 14:42:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwzyfoo",
              "author": "MustBeSomethingThere",
              "text": "https://preview.redd.it/yt86xmcxkmag1.jpeg?width=1024&format=pjpg&auto=webp&s=e62f37f2f8543eb6426e70bcc71540ecf11170c0\n\nZ-image-turbo",
              "score": 5,
              "created_utc": "2025-12-31 23:37:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1avqm",
                  "author": "DonkeyBonked",
                  "text": "So I would say subjective interpretation on the cat octopus merge, but I like the other one better on that aspect, and I like this one better for the piano, but that background is nowhere near 1700s, it looks like the 1980s in the ghetto I grew up in. Maybe that is my old hood, if so, I was in apartment A of that block, and the mess behind the cat is a shed that collapsed.",
                  "score": 2,
                  "created_utc": "2026-01-01 04:59:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwxqg9e",
              "author": "SmartCustard9944",
              "text": "This is not photorealistic",
              "score": 17,
              "created_utc": "2025-12-31 16:30:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwy1ft6",
                  "author": "Hoodfu",
                  "text": "Yes it is. Photorealistic means an artistic rendering of the style of photo realism. That's not the same thing as a photograph. These models know the difference.",
                  "score": 15,
                  "created_utc": "2025-12-31 17:25:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdjdzp",
                  "author": "WeMetOnTheMountain",
                  "text": "True new orleans streets too clean",
                  "score": 1,
                  "created_utc": "2026-01-03 03:30:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwxbf6y",
              "author": "9897969594938281",
              "text": "Wow, thatâ€™s impressive",
              "score": 3,
              "created_utc": "2025-12-31 15:15:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwyxxoq",
                  "author": "DinoAmino",
                  "text": "Is it really? Looks like the cat is wearing an octopus cape - less of a merge and more like a costume. And the image is nowhere near photorealistic.",
                  "score": -1,
                  "created_utc": "2025-12-31 20:11:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx0i92x",
              "author": "spectralyst",
              "text": "Mind blown.",
              "score": 1,
              "created_utc": "2026-01-01 01:40:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwlk6w",
          "author": "IllllIIlIllIllllIIIl",
          "text": "First impressions are very good. Skin and hair look *way* more realistic imho. Sadly it doesn't play well with the LoRa I literally finished training just this morning.    \n\nEdit: It's definitely an improvement, but it seems that it can suffer from the same problem that many so-called \"detail LoRas\" do: to achieve the impression of high detail, it often makes the scene very cluttered with objects and makes people much more hairy",
          "score": 10,
          "created_utc": "2025-12-31 12:39:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxazbr",
              "author": "Karyo_Ten",
              "text": ">makes people much more hairy\n\n*Barbarian edition",
              "score": 4,
              "created_utc": "2025-12-31 15:13:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww2voi",
          "author": "Finanzamt_Endgegner",
          "text": "Again no ggufs from us(Quantstack) because hugging face doesn't allow more uploaded models without paid plan ðŸ˜”",
          "score": 34,
          "created_utc": "2025-12-31 09:55:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwcqkk",
              "author": "PykeAtBanquet",
              "text": "Well, this is why monopoly is bad. We need torrents.",
              "score": 17,
              "created_utc": "2025-12-31 11:26:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwx2gwl",
                  "author": "keepthepace",
                  "text": "Distributing models seems like such a straightforward case for torrents.",
                  "score": 17,
                  "created_utc": "2025-12-31 14:27:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwus5b",
                  "author": "phhusson",
                  "text": "Pardony my French but dafuk does this have to do with monopoly? They are literally flat files. You can literally host it on your local ISP fiber. You can host those wherever you want.Â ",
                  "score": 28,
                  "created_utc": "2025-12-31 13:41:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwxdnmv",
                  "author": "__Maximum__",
                  "text": "We have torrents, since decades",
                  "score": 4,
                  "created_utc": "2025-12-31 15:27:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwevye",
                  "author": "Amazing_Athlete_2265",
                  "text": "I like the cut of your jib.",
                  "score": 1,
                  "created_utc": "2025-12-31 11:45:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwjabg",
              "author": "Cultured_Alien",
              "text": "Can't you ask for grant?",
              "score": 1,
              "created_utc": "2025-12-31 12:21:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxi77b",
                  "author": "DataGOGO",
                  "text": "from who?",
                  "score": 2,
                  "created_utc": "2025-12-31 15:50:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwxdrv5",
              "author": "__Maximum__",
              "text": "Why not use one of the torrent websites?",
              "score": 1,
              "created_utc": "2025-12-31 15:28:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwwcrgl",
              "author": "PykeAtBanquet",
              "text": "Well, this is why monopoly is bad. We need torrents.",
              "score": -11,
              "created_utc": "2025-12-31 11:26:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwin44",
                  "author": "FinBenton",
                  "text": "Anybody is free to make a torrent.",
                  "score": 21,
                  "created_utc": "2025-12-31 12:16:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx0r4jt",
                  "author": "AlwaysLateToThaParty",
                  "text": "> We need torrents.\n\nDo it then.  Problem solved.",
                  "score": 3,
                  "created_utc": "2026-01-01 02:39:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwoa7u",
          "author": "JLeonsarmiento",
          "text": "2025 was dominated by Qwen.",
          "score": 19,
          "created_utc": "2025-12-31 12:59:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwc1u2",
          "author": "SDLearner2512",
          "text": "This is amazing, thank you ! Trying it out now",
          "score": 4,
          "created_utc": "2025-12-31 11:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx228o",
          "author": "albuz",
          "text": "Is it possible to use gguf + ComfyUI on multiple GPUs?",
          "score": 6,
          "created_utc": "2025-12-31 14:24:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0630z",
          "author": "2legsRises",
          "text": "it seems very censored and changes poses to hide the natural bits.",
          "score": 3,
          "created_utc": "2026-01-01 00:23:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0kl9a",
              "author": "djtubig-malicex",
              "text": "Name checks out :D",
              "score": 3,
              "created_utc": "2026-01-01 01:55:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwfmrf",
          "author": "MaxKruse96",
          "text": "Hey i was right",
          "score": 3,
          "created_utc": "2025-12-31 11:52:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwgwyw",
          "author": "cr0wburn",
          "text": "Qwen team on fire! Thanks so much!",
          "score": 3,
          "created_utc": "2025-12-31 12:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwzb5p",
          "author": "XiRw",
          "text": "My computer canâ€™t handle it so Iâ€™m just curious, how do you guys run image inference like these models locally? Through llamacpp too if itâ€™s a gguf?",
          "score": 3,
          "created_utc": "2025-12-31 14:08:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy257q",
              "author": "YearZero",
              "text": "You can use ComfyUI, or if you want just use that plus ComfyUI-gguf, the guide is in the original post.",
              "score": 4,
              "created_utc": "2025-12-31 17:29:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwzosoo",
                  "author": "XiRw",
                  "text": "Ah okay, thanks for letting me know",
                  "score": 1,
                  "created_utc": "2025-12-31 22:39:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww7oi0",
          "author": "Business_Caramel_688",
          "text": "which Model should i use with 16 ram + 16 vram?",
          "score": 4,
          "created_utc": "2025-12-31 10:40:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwe20j",
              "author": "yoracale",
              "text": "any 5-bit should work: e.g.: [https://huggingface.co/unsloth/Qwen-Image-2512-GGUF?show\\_file\\_info=qwen-image-2512-Q5\\_K\\_M.gguf](https://huggingface.co/unsloth/Qwen-Image-2512-GGUF?show_file_info=qwen-image-2512-Q5_K_M.gguf)",
              "score": 8,
              "created_utc": "2025-12-31 11:38:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwiwfr",
                  "author": "Business_Caramel_688",
                  "text": "thanks bro\nwith which clip model?",
                  "score": 4,
                  "created_utc": "2025-12-31 12:18:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwptid",
              "author": "jinnyjuice",
              "text": "And what software stack for Ubuntu? (I already have vLLM, VS Codium, and Cline if that matters)",
              "score": 3,
              "created_utc": "2025-12-31 13:09:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww8ey7",
          "author": "Admirable_Bag8004",
          "text": "Not bad at all. Prompt: Penguin riding a bicycle in a busy street ->\n\nhttps://preview.redd.it/wyblga7briag1.jpeg?width=562&format=pjpg&auto=webp&s=a13637185e041055d16699baad366b846a9ba229",
          "score": 7,
          "created_utc": "2025-12-31 10:47:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww9ia9",
              "author": "BITE_AU_CHOCOLAT",
              "text": "Eh.. still kinda looks like average SD slop to me. The day we get a true Nano Banana competitor will be when things will get interesting",
              "score": 29,
              "created_utc": "2025-12-31 10:57:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwzsnig",
                  "author": "SpiritualWindow3855",
                  "text": "I don't understand how they possibly prompted \"Penguin riding a bicycle in a busy street\" and got that.  \n\nhttps://preview.redd.it/gv4k6fqeemag1.png?width=1664&format=png&auto=webp&s=0a44603cca22554cad9bc04ff0906cea6af58a3b\n\n  \nI feel like they're using some gooner-slop ComfyUI workflow with 100 nodes doing random bullshit, since the prompt doesn't mention \"delivery service\" and Qwen Image doesn't do that kind of prompt expansion.",
                  "score": 10,
                  "created_utc": "2025-12-31 23:02:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwa4js",
                  "author": "Mochila-Mochila",
                  "text": "Off topic, but your username is really creative and would make for an interesting prompt.",
                  "score": 7,
                  "created_utc": "2025-12-31 11:02:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwfqr3",
                  "author": "SlowFail2433",
                  "text": "Itâ€™s getting better, complex background and text with no obvious topology failures",
                  "score": 5,
                  "created_utc": "2025-12-31 11:52:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwx9ulo",
                  "author": "Danmoreng",
                  "text": "Canâ€™t get top model quality on local hardware right now imho. The best you can do is Flux2.dev which already requires 24Gb + vram.\n\nFor small vram z-image is crazy good though.",
                  "score": 3,
                  "created_utc": "2025-12-31 15:07:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwx77gf",
              "author": "Crypt0Nihilist",
              "text": "It might be due to a lack of specificity in the prompt, but it has the common uncanny valley over-saturation and warm colours.\n\nFunny that is seems to recognise that people walk on the crossing, but not *across* it.",
              "score": 3,
              "created_utc": "2025-12-31 14:53:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxdtw6",
                  "author": "Mediocre-Method782",
                  "text": "I've noticed image generators don't really handle background continuity very well. Notice the space in front of (that is, between us and) the car in the oncoming lane is mostly clear, except where the\npenguin in latent 2D space becomes > the background car in latent 2D space.",
                  "score": 2,
                  "created_utc": "2025-12-31 15:28:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwzsbrb",
              "author": "SpiritualWindow3855",
              "text": "What kind of jank-ass yee yee-ass quant are you on, because that is not Qwen Image 2512.\n\nhttps://preview.redd.it/6n7teaccemag1.png?width=1664&format=png&auto=webp&s=313a63eb790735578ccd41768a07cd970170bd7b",
              "score": 4,
              "created_utc": "2025-12-31 23:00:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww28pa",
          "author": "Admirable-Star7088",
          "text": "Thanks for the Christmas present! (or maybe more like a Happy new Year gift).\n\nIt will be very interesting to compare this model with Flux 2 Dev (the current most powerful open T2I model).",
          "score": 2,
          "created_utc": "2025-12-31 09:49:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww58ze",
          "author": "No_Conversation9561",
          "text": "Now we wait for Image edit model.",
          "score": 3,
          "created_utc": "2025-12-31 10:17:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww64s6",
              "author": "eidrag",
              "text": "doubt, we only got 2511 this week, but boy I wish 2512 and z-image base and edit",
              "score": 14,
              "created_utc": "2025-12-31 10:25:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww88wf",
                  "author": "Geritas",
                  "text": "Feels kind of dubious if the base z image will indeed be out. Itâ€™s been a month already, still no word. Itâ€™s not like they have to do anything with it, since the turbo version exists the base version must exist too already. Whatâ€™s taking so longâ€¦",
                  "score": 5,
                  "created_utc": "2025-12-31 10:45:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww7ma4",
          "author": "FinBenton",
          "text": "Seems to work with my old qwen image workflow, their example settings 50 steps at cfg 4. Just obv very slow, I tried the old Lightning 2.0 4 and 8 step loras which kinda work but I used like 8+ steps for the 4-step lora.\n\ne. no Loras, 20 steps cfg 3.5 generates pretty ok image in 1440x1440 in 52 seconds on 5090 with Q8.\ne. actually 8-step lora with 8 steps and 3.5 seems to do pretty ok",
          "score": 2,
          "created_utc": "2025-12-31 10:39:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxo15e",
          "author": "Due-Memory-6957",
          "text": "Just CPU will work? I want to try it!",
          "score": 1,
          "created_utc": "2025-12-31 16:19:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxqk7h",
          "author": "algorithm314",
          "text": "Using stable-diffusion.cpp for 1024x1024 image.\n\nCPU on 8 cores Ryzen 7 PRO 5875U laptop is 1000s/it and it is 40 iterations.\nUsing internal GPU is better 350s/it but it is still very slow.",
          "score": 1,
          "created_utc": "2025-12-31 16:31:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy3tgf",
          "author": "flyfreze",
          "text": "anyone who tried, is it better than z image turbo ?",
          "score": 1,
          "created_utc": "2025-12-31 17:37:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0a363",
          "author": "2legsRises",
          "text": "after more testing it is actually pretty amazing",
          "score": 1,
          "created_utc": "2026-01-01 00:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0c2bu",
          "author": "SanDiegoDude",
          "text": "Really impressive. Between Qwen-image-2512 and qwen-edit-2511, there really is no reason to run Flux2.dev, even with the recently released turbo Lora from Fal. Human skin looks much more realistic, much more detailed and more coherent to the prompt.  Running x/y's with Flux2 turbo and Z-Image Turbo, I'm not really even seeing a reason to keep Flux2 around taking up as much space as it does.",
          "score": 1,
          "created_utc": "2026-01-01 01:00:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0ed44",
          "author": "Prashant-Lakhera",
          "text": "Great release ðŸ‘\nFor the GGUF version, any recommended quantization levels for running locally without losing too much image quality?",
          "score": 1,
          "created_utc": "2026-01-01 01:15:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0yitm",
              "author": "Freonr2",
              "text": "In my very unscientific testing of prior Qwen image models I had a hard time telling the difference between bf16, Q8_0, and Q6_k.  \n\nYou should pick the largest that fits into VRAM, though, because you won't be memory bandwidth bound like you are with LLMs.",
              "score": 2,
              "created_utc": "2026-01-01 03:29:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwdk9n",
          "author": "piggledy",
          "text": "Are there any benchmarks yet for different GPUs or unified memory systems (Apple M, AMD 395)?\n\nWondering how well it would run on a 3060 12GB if at all.",
          "score": 1,
          "created_utc": "2025-12-31 11:34:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwhfb5",
              "author": "Amazing_Athlete_2265",
              "text": "It runs on my 3080 10GB. Slow (around 5 mins) but it runs. Using the Q4 quant.",
              "score": 2,
              "created_utc": "2025-12-31 12:06:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx0yxyr",
              "author": "Freonr2",
              "text": "Diffusion models scale with compute. Nvidia GPUs dominate. By a lot.  A whole lot.",
              "score": 1,
              "created_utc": "2026-01-01 03:32:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww8npy",
          "author": "wilson-SHEN",
          "text": "I know I will get a lot of down votes, but this prompt not working for me \"a man with grocery bag standing in fromt of tanks\"",
          "score": -9,
          "created_utc": "2025-12-31 10:49:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1h5ud",
              "author": "throttlekitty",
              "text": "It helps to be more specific, in both your post and your prompt, like what about it's not working for you.\n\nBut my questions are along the lines of: what kind of tanks / what location? / is there a look or feel you're going for?",
              "score": 2,
              "created_utc": "2026-01-01 05:52:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1puyq9r",
      "title": "Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record",
      "subreddit": "LocalLLaMA",
      "url": "https://www.cnbc.com/2025/12/24/nvidia-buying-ai-chip-startup-groq-for-about-20-billion-biggest-deal.html",
      "author": "fallingdowndizzyvr",
      "created_utc": "2025-12-24 22:14:48",
      "score": 670,
      "num_comments": 151,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/",
      "domain": "cnbc.com",
      "is_self": false,
      "comments": [
        {
          "id": "nvtcbgl",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-25 02:45:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsbn0x",
          "author": "sourceholder",
          "text": "Great, more consolidation.\n\nIs Cerebras next?",
          "score": 275,
          "created_utc": "2025-12-24 22:22:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvseil1",
              "author": "freecodeio",
              "text": "if anyone is a threat to this industry is cerebras so I'm surprised it's still not happened",
              "score": 86,
              "created_utc": "2025-12-24 22:41:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvspj4w",
                  "author": "dodiyeztr",
                  "text": "Makes you wonder who the investors are",
                  "score": 24,
                  "created_utc": "2025-12-24 23:56:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvto2in",
                  "author": "tassa-yoniso-manasi",
                  "text": "cerebras when you talk about inference on 70B models: ðŸ¥°\n\ncerebras when you ask them to stuff more than 44GB of memory per chip: Â ðŸ«¥",
                  "score": 34,
                  "created_utc": "2025-12-25 04:16:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvunxsv",
                  "author": "amapleson",
                  "text": "No way, that company is run so poorly, something is wrong in the culture",
                  "score": 4,
                  "created_utc": "2025-12-25 10:06:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvtdm4u",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 7,
                  "created_utc": "2025-12-25 02:54:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsbwxr",
          "author": "john0201",
          "text": "This can only mean good things for a healthy competitive market",
          "score": 496,
          "created_utc": "2025-12-24 22:24:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvscemw",
              "author": "CYTR_",
              "text": "We're cooked lmao",
              "score": 172,
              "created_utc": "2025-12-24 22:27:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvsle2s",
                  "author": "exaknight21",
                  "text": "Weâ€™re grilled.",
                  "score": 38,
                  "created_utc": "2025-12-24 23:27:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvswdoz",
                  "author": "DangKilla",
                  "text": "Itâ€™s a good thing for gamers. More TPUâ€™s less GPUâ€™s",
                  "score": 0,
                  "created_utc": "2025-12-25 00:44:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvt4wyn",
              "author": "BusRevolutionary9893",
              "text": "Don't worry. Hopefully China has entered the US market by this time next year.Â ",
              "score": 15,
              "created_utc": "2025-12-25 01:49:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtsatq",
                  "author": "Fortyseven",
                  "text": "I'm sure they'll be banned for \"security concerns\".",
                  "score": 17,
                  "created_utc": "2025-12-25 04:50:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxhmwp",
              "author": "Gloomy_Nebula_5138",
              "text": "Itâ€™s not just harming the market by trying to avoid antitrust, but it is also a betrayal of the Groq employees, who worked at low pay for a startup with the expectation that their ownership of options may mean something. This is a textbook Chamath scam deal. Founders leave, they and investors get lots of money, and the employees of Groq who did the work get nothing except their options in a zombie shell company left over from this acquisition â€¦ uh I mean â€œlicensing dealâ€. Shame on Jensen and Nvidia for being a part of this.",
              "score": 3,
              "created_utc": "2025-12-25 21:53:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxrl9b",
                  "author": "john0201",
                  "text": "Yeah I just read up on that. This type of deal should trigger not just anti-trust laws but punitive penalties. Basically anyone worth anything moves to NVIDIA everyone else stays behind to keep up the ruse. \n\nI always get some solace knowing that if billionaires still want more they will never be happy.",
                  "score": 2,
                  "created_utc": "2025-12-25 22:55:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw20z3j",
                  "author": "WillingnessNearby371",
                  "text": "Why would the options be worth nothing? \n\nWouldnâ€™t the 20B still be paid to the original corporation which would need to payout to all shareholders? AFAIK, the CEO / founders also own common stock generally. \n\nAsking since I also have stock options and my expectation is that in any kind of acquisition deal I would be paid out. \n\nAlso, Groq pays resonably well from what Iâ€™ve seen. Not industry leader but the cash component is stuff pretty good.",
                  "score": 1,
                  "created_utc": "2025-12-26 17:50:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwt5p2",
                  "author": "Cultural-Plum-3269",
                  "text": "Not sure where this understanding is coming from, vested shares are getting a full cash payout, unvested shares are getting paid out as nvidia stocks and if I understand correctly even employees whoâ€™ve been there less than a year will get their cliff removed. Most of them are getting offers at nvidia, how is this a bad deal for the employees? I wouldnâ€™t call Groq low pay either.",
                  "score": 1,
                  "created_utc": "2025-12-31 13:31:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvu1999",
              "author": "Mikasa0xdev",
              "text": "Nvidia's monopoly grows, lol.",
              "score": 1,
              "created_utc": "2025-12-25 06:09:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvvsou0",
              "author": "CuTe_M0nitor",
              "text": "We are F#$D",
              "score": 1,
              "created_utc": "2025-12-25 15:48:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw0b0cd",
              "author": "BumblebeeParty6389",
              "text": "Competition? What's that?",
              "score": 1,
              "created_utc": "2025-12-26 11:08:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsejg1",
          "author": "Zeeplankton",
          "text": "I'm a bit shocked groq could possibly be worth 20b.",
          "score": 147,
          "created_utc": "2025-12-24 22:41:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsfi8l",
              "author": "SlowFail2433",
              "text": "The perf is real",
              "score": 66,
              "created_utc": "2025-12-24 22:47:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvsmyht",
                  "author": "BambaiyyaLadki",
                  "text": "A guy I follow on Twitter (Irrational Analysis) had a post a while ago where he said that groqs TPU is actually not a very good product. His arguments are usually technically sound so I'm curious to learn more about why his arguments were wrong (or why Nvidia brought Groq despite the poor design).",
                  "score": 62,
                  "created_utc": "2025-12-24 23:38:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvsvl1q",
                  "author": "SpiritualWindow3855",
                  "text": "The perf is the opposite of real: most models they deploy are degraded vs reference implementations, Blackwell is a showcase on how NVIDIA can still do specialization for inference and kick ass, and they were still struggling to provide real production levels of access of the product to customers.\n\nThey were almost certainly bought for their technical team, and the product will mostly be picked through then left to languish.",
                  "score": 14,
                  "created_utc": "2025-12-25 00:39:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvuns27",
                  "author": "Zeeplankton",
                  "text": "But only because they're using highly customized hardware and highly modified models, no? That's why so few are supported. There's a reason that barely anyone actually supports groq as an endpoint. But maybe 20b is a rounding error for nvidias 4T valuation.",
                  "score": 1,
                  "created_utc": "2025-12-25 10:05:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvsnly2",
              "author": "genshiryoku",
              "text": "Their inference performance is crazy good and if you believe inference is going to be the lion share of FLOP expenditure then it's a no-brainer move.\n\nThis is Nvidia buying up future competition before they got completely wiped out by them.",
              "score": 29,
              "created_utc": "2025-12-24 23:43:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvx44sk",
                  "author": "Fit-Support4910",
                  "text": "Depends how you define \"performance.\"\n\nFrom an end user's perspective? Sure, latency and throughput are all that matter, and Groq's numbers look great.\n\nBut from a cloud provider's perspective, you also care about resource utilization and cost efficiency. Groq's approach appears to be brute-forcing performance by ganging together a large number of chips just to fit the model in memory. The raw speed is impressive, but the compute utilization percentage is low. you're paying for a lot of silicon that's mostly idle. By that account, NVIDIA's current products are actually more compelling here.",
                  "score": 3,
                  "created_utc": "2025-12-25 20:30:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvswehq",
              "author": "AuspiciousApple",
              "text": "Nvidia is buying them to kill competition. They don't care whether it's the best value for money.",
              "score": 17,
              "created_utc": "2025-12-25 00:44:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvsham3",
              "author": "Bozhark",
              "text": "Not Grok",
              "score": 19,
              "created_utc": "2025-12-24 22:59:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvsphth",
              "author": "donald-bro",
              "text": "Does  groqs has many patents or know how that nvidia needed ?",
              "score": 3,
              "created_utc": "2025-12-24 23:56:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvxkhlc",
              "author": "emteedub",
              "text": "Might be worth that much to them to eliminate.\n\nWhat else does one buy with their monopoly monies",
              "score": 2,
              "created_utc": "2025-12-25 22:10:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvsk39p",
              "author": "No-Manufacturer6409",
              "text": "As in too much or too little?",
              "score": 2,
              "created_utc": "2025-12-24 23:18:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvsyg2k",
              "author": "Myg0t_0",
              "text": "Have u tried it? Fast as fuck boy!",
              "score": 2,
              "created_utc": "2025-12-25 00:59:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvvguzn",
              "author": "harrro",
              "text": "It's not but for Jensen Huang/Nvidia, killing off competition is worth the 20 bill.",
              "score": 1,
              "created_utc": "2025-12-25 14:30:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtn09f",
              "author": "tassa-yoniso-manasi",
              "text": "this is a calculated move from Nvidia for edge computing in robots which will need extremely low latency & computational efficiency for them to become more than tiktok curiosities.\n\nthe 20bn is grossly inflated for the value it can bring today, but not for its value in 5/10 years.",
              "score": 1,
              "created_utc": "2025-12-25 04:08:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvvt00c",
              "author": "CuTe_M0nitor",
              "text": "Wtf ðŸ˜‚ are you talking about. Have you seen the token speed why can run an LLM on their hardware?! If feels like your are seeing 10years in to the future when using them. I was blown away that there is hardware that can do that today and that are cheaper than Nvidia.",
              "score": 0,
              "created_utc": "2025-12-25 15:49:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvvfad",
                  "author": "Zeeplankton",
                  "text": "Yeah but I'm pretty sure they're only able to do it through extremely bespoke hardware, and modifying existing models. That's why model support was / is so bare, and of the models they support, the models perform worse then full counterparts.",
                  "score": 1,
                  "created_utc": "2025-12-25 16:04:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsetyi",
          "author": "agentzappo",
          "text": "Another â€œacquihireâ€ example. No way in hell the regulators would allow Nvidia to outright purchase Groq, but they still get what they want and need out of this deal while leaving behind everyone else who joined a startup hoping to benefit from long-term scaling and success driven by the former founders",
          "score": 83,
          "created_utc": "2025-12-24 22:43:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtdixf",
              "author": "Intelligent-Agent440",
              "text": "Don Jr is an investor in the company, regulators won't say a thing",
              "score": 32,
              "created_utc": "2025-12-25 02:54:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtyp3j",
              "author": "vcutrera",
              "text": "Are common stock holders getting paid out on this deal since they are only buying 'assets' and taking execs?",
              "score": 4,
              "created_utc": "2025-12-25 05:45:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvurv1b",
                  "author": "1998marcom",
                  "text": "I guess that would count as company earnings, so it's up to Groq's management what to do with that.",
                  "score": 4,
                  "created_utc": "2025-12-25 10:49:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvv1xhw",
                  "author": "agentzappo",
                  "text": "Deals like these, the VCs get paid out but not nearly at the levels of return they aim for. Itâ€™s one reason why the VCs generally hate acquihire deals since it cuts them out of the massive potential upside on companies / founders where their bets paid off.",
                  "score": 4,
                  "created_utc": "2025-12-25 12:32:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvwyhh1",
              "author": "ExaminationNo8522",
              "text": "This seems significantly worse from a regulatory standpoint though as it erodes trust in the very concept of equity. Whatâ€™s the point of holding equity in a company if it gets pulled apart for parts?",
              "score": 3,
              "created_utc": "2025-12-25 19:55:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvstw78",
              "author": "ckkl",
              "text": "Lmao. Itâ€™s going through pal",
              "score": -4,
              "created_utc": "2025-12-25 00:27:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsd6f8",
          "author": "GabryIta",
          "text": "Oh no",
          "score": 41,
          "created_utc": "2025-12-24 22:32:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvse020",
              "author": "JLeonsarmiento",
              "text": "I have a bad feeling about thisâ€¦",
              "score": 15,
              "created_utc": "2025-12-24 22:37:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvv34pa",
              "author": "fauni-7",
              "text": "Anyway...",
              "score": 2,
              "created_utc": "2025-12-25 12:43:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsem2e",
          "author": "AnonsAnonAnonagain",
          "text": "Great. We are cooked",
          "score": 32,
          "created_utc": "2025-12-24 22:41:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsfl9j",
          "author": "TheFrenchSavage",
          "text": "Oh no. I hope they don't pull the plug on GroqCloud, it was my source of free API calls for fun and not-monry-making projects!",
          "score": 24,
          "created_utc": "2025-12-24 22:48:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtdp5f",
              "author": "Intelligent-Agent440",
              "text": "They said Groqcloud is not among the assets being acquired",
              "score": 10,
              "created_utc": "2025-12-25 02:55:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvuj6e0",
                  "author": "TheFrenchSavage",
                  "text": "Wut? How does that work? The cloud service won't be running for free if they don't have access to free hardware right?",
                  "score": 2,
                  "created_utc": "2025-12-25 09:14:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvt01mz",
              "author": "Ok-Adhesiveness-4141",
              "text": "My worry exactly, I use it for all my projects.",
              "score": 3,
              "created_utc": "2025-12-25 01:11:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtz8hy",
              "author": "zuraken",
              "text": "Every company that gets bought out usually turns off their free tier services, or makes them extremely ad intrusive and nearly unusable",
              "score": 1,
              "created_utc": "2025-12-25 05:50:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtzokp",
              "author": "lochyw",
              "text": "Cerberus is pretty good too fyi",
              "score": 1,
              "created_utc": "2025-12-25 05:54:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsq91m",
          "author": "ocassionallyaduck",
          "text": "Antitrust lawsuit needs to be placed like yesterday.",
          "score": 17,
          "created_utc": "2025-12-25 00:01:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvti65m",
              "author": "TheJpow",
              "text": "With this admin? Lol",
              "score": 13,
              "created_utc": "2025-12-25 03:30:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtjkie",
                  "author": "ocassionallyaduck",
                  "text": "With any ain in the last 30 years? Nah. Lol.",
                  "score": 0,
                  "created_utc": "2025-12-25 03:41:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvu3fn8",
              "author": "crantob",
              "text": "Why?  How much antitrust have you studied?  It's a clownshow.  \n\n\nAntitrust 'law' is based on the farcical chimera of 'perfect competition', which is a fantastical political construction shoved into the soft craniums of the college students under the guise of 'economic science'.",
              "score": 0,
              "created_utc": "2025-12-25 06:29:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsgjcj",
          "author": "whereismytralala",
          "text": "The FTC will explain how this is not a monopoly and, as soon as Nvidia buy a couple of billions of Trump coins, they don't see any problems.",
          "score": 47,
          "created_utc": "2025-12-24 22:54:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsskc9",
              "author": "lazytiger21",
              "text": "This happens a lot. Iâ€™ve seen it happen at 3 companies that I worked with in the past. The â€œlicensing feeâ€ is enough that the acquired company continues to operate and they usually pivot to doing something slightly different, but they arenâ€™t the same company.",
              "score": 14,
              "created_utc": "2025-12-25 00:18:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvua547",
              "author": "kingwhocares",
              "text": "They don't need to when Nvidia's competitor loves shooting themselves on the foot and is fine just selling SoCs to companies like Sony and Microsoft with little investment.",
              "score": 1,
              "created_utc": "2025-12-25 07:36:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtf49i",
              "author": "Randommaggy",
              "text": "Hopefully enough Epstein stuff has a botched release to have king cheeto and his co-conspirators impeached before it goes through and too much additional long term damage to the US economy is done.",
              "score": -2,
              "created_utc": "2025-12-25 03:06:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvthuil",
                  "author": "whereismytralala",
                  "text": "Don't hold your breath.",
                  "score": 7,
                  "created_utc": "2025-12-25 03:27:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsdr7z",
          "author": "mixxoh",
          "text": "Damn, Iâ€™m in the middle of interviewing with them. What would this mean?",
          "score": 25,
          "created_utc": "2025-12-24 22:36:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvseojm",
              "author": "thrownawaymane",
              "text": "Get more interviews lined up. \n\nNo matter what happens next youâ€™ll be fine if you do that. Do not chase the shiny object, you could easily get led on.",
              "score": 60,
              "created_utc": "2025-12-24 22:42:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvsg6f3",
                  "author": "mixxoh",
                  "text": "So abandoned them since I guess most of the going public upside is moot now?",
                  "score": 7,
                  "created_utc": "2025-12-24 22:52:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsk83t",
          "author": "jeffwadsworth",
          "text": "Take out the competition always works well.",
          "score": 9,
          "created_utc": "2025-12-24 23:19:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsomnp",
          "author": "az226",
          "text": "Interesting way of skirting regulation. Make a technology licensing deal and get the pick of the employees. Leave the company as a shell.",
          "score": 11,
          "created_utc": "2025-12-24 23:50:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvskon9",
          "author": "TangeloPutrid7122",
          "text": "How the fuck was this allowed.",
          "score": 17,
          "created_utc": "2025-12-24 23:23:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvstlza",
              "author": "cafedude",
              "text": "Heh, there's nobody minding the store anymore. Anything goes.",
              "score": 18,
              "created_utc": "2025-12-25 00:25:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvskvt2",
          "author": "StackOwOFlow",
          "text": "snuffing out the competition already :(",
          "score": 13,
          "created_utc": "2025-12-24 23:24:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsel3j",
          "author": "insite",
          "text": "Wow! I wondered how NVIDIA joining the elite Big Tech companies was going to reshape the technology landscape. They keep finding new and inventive ways to drive up hardware costs. Buying up startup companies is nothing new, but this is in combination with their other deals to drive up the cost of RAM.",
          "score": 28,
          "created_utc": "2025-12-24 22:41:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsfmh5",
              "author": "SlowFail2433",
              "text": "Depends cos if they ramp up production of Groq chips then price could go down rather than up",
              "score": 5,
              "created_utc": "2025-12-24 22:48:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvsgchb",
                  "author": "Mad_Undead",
                  "text": "Does Groq even sell LPUs? I thought they only provide inference.",
                  "score": 10,
                  "created_utc": "2025-12-24 22:53:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvse8iv",
          "author": "JayD30",
          "text": "https://groq.com/newsroom/groq-and-nvidia-enter-non-exclusive-inference-technology-licensing-agreement-to-accelerate-ai-inference-at-global-scale",
          "score": 5,
          "created_utc": "2025-12-24 22:39:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt6se6",
          "author": "TheRealMasonMac",
          "text": "VC-backed startups are a disease. A tale as old as Silicon Valley.",
          "score": 5,
          "created_utc": "2025-12-25 02:03:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtyxtm",
          "author": "SkyNetLive",
          "text": "Groq wasnâ€™t going anywhere. Nvidia just giving it a mercy death. I would have let them die naturally",
          "score": 5,
          "created_utc": "2025-12-25 05:48:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvurm8n",
              "author": "RakesProgress",
              "text": "I kinda think the same.  The team is useful. The tech is useful.  But will never be a winner.  Assimilate them into the fold.",
              "score": 1,
              "created_utc": "2025-12-25 10:47:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvcvuo",
                  "author": "SkyNetLive",
                  "text": "I have spoken to their team, not good and felt more like talking to crypto bros. I suspect they backfilled with Nvidia , the so called assets.",
                  "score": 1,
                  "created_utc": "2025-12-25 14:01:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvscnit",
          "author": "Stunning_Mast2001",
          "text": "Wow really fascinating newsâ€¦Â ",
          "score": 9,
          "created_utc": "2025-12-24 22:29:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvslhk4",
          "author": "robberviet",
          "text": "20b? That's too much.",
          "score": 3,
          "created_utc": "2025-12-24 23:28:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsy46p",
              "author": "Maleficent-Forever-3",
              "text": "Valued at $7B in the last funding round in sept when a firm that has Donald trump jr on the board invested",
              "score": 8,
              "created_utc": "2025-12-25 00:57:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtzflv",
              "author": "Fortyseven",
              "text": "That'll never fit on my 3060.",
              "score": 3,
              "created_utc": "2025-12-25 05:52:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvszx9h",
          "author": "Ok_Condition4242",
          "text": "https://preview.redd.it/3tjbjvaa399g1.png?width=498&format=png&auto=webp&s=72c2f5c9297a1de6cae2bedaa2dd92c3ee411d27",
          "score": 3,
          "created_utc": "2025-12-25 01:11:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtsx1l",
          "author": "Lesser-than",
          "text": "dem real dollars or unrealized dollars?",
          "score": 3,
          "created_utc": "2025-12-25 04:55:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvurxhf",
          "author": "lickywilde",
          "text": "So how does Groq the company continue without its senior leadership and its IP sold off to its main competitor?",
          "score": 3,
          "created_utc": "2025-12-25 10:50:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvutzoo",
              "author": "pengy99",
              "text": "It probably doesn't other than maybe limping along for a bit. That's the point. Nvidia gets what they want without the pesky regulatory oversight.",
              "score": 4,
              "created_utc": "2025-12-25 11:12:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvuxivn",
                  "author": "lickywilde",
                  "text": "Ah sure its the Google - Windsurf thing all over again.",
                  "score": 1,
                  "created_utc": "2025-12-25 11:49:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwaf72g",
              "author": "Ok-Hunt-4927",
              "text": "Thereâ€™s new CEO now, Simon Edwards. He was CFO earlier but after Ross and Sunny Madra left, he was made the CEO. (I work for Groq)",
              "score": 2,
              "created_utc": "2025-12-28 01:28:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvxcgd8",
              "author": "fallingdowndizzyvr",
              "text": "It has the cloud business. Nvidia didn't want that.",
              "score": 1,
              "created_utc": "2025-12-25 21:21:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsfsw6",
          "author": "FullstackSensei",
          "text": "Come on, people! Can't you read?!!!\n\nNvidia is not acquiring Groq. They would never do such an anti-competitive move. Nvidia is mearly non-exclusively (see, it's not even exclusive) licensing Groq's technology and hiring all Groq's engineering talent to help them integrate the technology. Everyone else is free to also license the same technology and Groq cloud operations will be unaffected and continue to operate independently until their chips become irrelevant in another year.\n\nI really don't understand what all the commotion is about.",
          "score": 19,
          "created_utc": "2025-12-24 22:49:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsi0a5",
              "author": "Freonr2",
              "text": "From TFA:\n\n> Nvidia has agreed to buy assets from Groq...\n\n> Groq said in a blog post on Wednesday that itâ€™s â€œentered into a non-exclusive licensing agreement with Nvidia for Groqâ€™s inference technology,â€ ... â€œwill join Nvidia to help advance and scale the licensed technology,â€\n\n> Davis told CNBC that Nvidia is getting all of Groqâ€™s assets, though its nascent Groq cloud business is not part of the transaction\n\nIt's a bit confusing the way this is written because \"buy\" and \"license\" are both used.\n\n> Huang added that, â€œWhile we are adding talented employees to our ranks and licensing Groqâ€™s IP, we are not acquiring Groq as a company.â€\n\nThis is a bit weird since they're taking (all? most? a lot of?) the Groq employees, so my assumption is Groq is essentially dead in terms of new technology and all new tech will be produced by former Groq employees who are now Nvidia employees.\n\nMy take away is Groq is mostly being absorbed with a husk leftover.",
              "score": 21,
              "created_utc": "2025-12-24 23:04:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvsjfsb",
                  "author": "Edzomatic",
                  "text": "This is a lot of corporate gymnastics",
                  "score": 17,
                  "created_utc": "2025-12-24 23:14:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvsjxdq",
                  "author": "Charuru",
                  "text": "Your parent was sarcastic.",
                  "score": 16,
                  "created_utc": "2025-12-24 23:17:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvsvgf0",
              "author": "noiserr",
              "text": "> They would never do such an anti-competitive move.\n\nlol.. the company behind proprietary CUDA, PhysX, g-sync.. They also tried to buy ARM but got blocked for being anti-competitive. There is also the GPP program.. (from wikipedia: The program was regarded as an anti-consumer practice due to the fact that partnering companies were required to remove their gaming branding from all non-Nvidia graphics cards,[11] hurting consumer choice.)",
              "score": 6,
              "created_utc": "2025-12-25 00:38:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtnzb8",
              "author": "True_Requirement_891",
              "text": "Groq has been trying to get this deal from a long time.",
              "score": 1,
              "created_utc": "2025-12-25 04:16:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsvusj",
          "author": "goatchild",
          "text": "How is this allowed?",
          "score": 4,
          "created_utc": "2025-12-25 00:41:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsn8is",
          "author": "a_beautiful_rhind",
          "text": "I guess it was nice while it lasted. They never got that memory up to make it practical and they certainly won't now.",
          "score": 2,
          "created_utc": "2025-12-24 23:40:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtihtg",
          "author": "grady_vuckovic",
          "text": "So the only possible threat to NVIDIA, TPUs, and NVIDIA is trying to buy control of it..",
          "score": 2,
          "created_utc": "2025-12-25 03:32:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtlmmy",
          "author": "extopico",
          "text": "No anti monopoly laws? Awesome.",
          "score": 2,
          "created_utc": "2025-12-25 03:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtpazq",
          "author": "lqstuart",
          "text": "Largest deal on what recordâ€¦?",
          "score": 2,
          "created_utc": "2025-12-25 04:26:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvudq6q",
              "author": "pengy99",
              "text": "Largest deal Nvidia has ever done.",
              "score": 6,
              "created_utc": "2025-12-25 08:14:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvttl9h",
          "author": "VegetableSense",
          "text": "https://preview.redd.it/50hmh38f8a9g1.jpeg?width=710&format=pjpg&auto=webp&s=1455d2f8edbcaaafc2d5b867807c042183c2b3a8",
          "score": 2,
          "created_utc": "2025-12-25 05:01:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuznxj",
          "author": "OmarBessa",
          "text": "So, they just got all their IP.",
          "score": 2,
          "created_utc": "2025-12-25 12:11:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsdp1a",
          "author": "Bloated_Plaid",
          "text": "Woohoo!",
          "score": 4,
          "created_utc": "2025-12-24 22:36:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsgf39",
          "author": "Mediocre-Ant-7178",
          "text": "The pump must go on",
          "score": 3,
          "created_utc": "2025-12-24 22:53:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtyyvq",
          "author": "ThatOneGuy4321",
          "text": "Groq =/= Grok (Elon chatbot) btw\n\nand then thereâ€™s grok patterns which are a different thing",
          "score": 2,
          "created_utc": "2025-12-25 05:48:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsoum8",
          "author": "1-800-methdyke",
          "text": "God dammit",
          "score": 1,
          "created_utc": "2025-12-24 23:52:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtnbbr",
          "author": "ForsookComparison",
          "text": "Someone EILI5 why we are cooked",
          "score": 1,
          "created_utc": "2025-12-25 04:10:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsfits",
          "author": "CostGuilty8542",
          "text": "circular money",
          "score": 1,
          "created_utc": "2025-12-24 22:47:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsture",
          "author": "ckkl",
          "text": "Huge!!!",
          "score": 0,
          "created_utc": "2025-12-25 00:27:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pux0yc",
      "title": "We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/",
      "author": "vox-deorum",
      "created_utc": "2025-12-24 20:50:16",
      "score": 632,
      "num_comments": 163,
      "upvote_ratio": 0.96,
      "text": "[GLM-4.6 Playing Civilization V + Vox Populi \\(Replay\\)](https://i.redd.it/zaib4up4s79g1.gif)\n\nWe had GPT-OSS-120B and GLM-4.6 playing 1,408 full Civilization V games (with Vox Populi/Community Patch activated). In a nutshell: LLMs set strategies for Civilization V's algorithmic AI to execute. Here is what we found\n\n[An overview of our system and results \\(figure fixed thanks to the comments\\)](https://preview.redd.it/ftox05oo5e9g1.png?width=3201&format=png&auto=webp&s=b8181b507060b45caab07acc36ba82d80eb65f1d)\n\n**TLDR:** It is now possible to get open-source LLMs to play end-to-end Civilization V games (the m. They are not beating algorithm-based AI on a very simple prompt, but they do play quite differently.\n\n**The boring result:** With a simple prompt and little memory, both LLMs did slightly better in the best score they could achieve within each game (+1-2%), but slightly worse in win rates (-1\\~3%). Despite the large number of games run (2,207 in total, with 919 baseline games), neither metric is significant.\n\n**The surprising part:**\n\nPure-LLM or pure-RL approaches [\\[1\\]](https://arxiv.org/abs/2401.10568), [\\[2\\]](https://arxiv.org/abs/2502.20807) couldn't get an AI to play and survive full Civilization games. With our hybrid approach, LLMs can survive as long as the game goes (\\~97.5% LLMs, vs. \\~97.3% the in-game AI). The model can be as small as OSS-20B in our internal test.\n\nMoreover, the two models developed **completely different playstyles**.\n\n* OSS-120B went full warmonger: +31.5% more Domination victories, -23% fewer Cultural victories compared to baseline\n* GLM-4.6 played more balanced, leaning into both Domination and Cultural strategies\n* Both models preferred **Order** (**communist-like**, \\~24% more likely) ideology over **Freedom** (democratic-like)\n\n**Cost/latency (OSS-120B):**\n\n* \\~53,000 input / 1,500 output tokens per turn\n* **\\~$0.86/game** (OpenRouter pricing as of 12/2025)\n* Input tokens scale linearly as the game state grows.\n* **Output stays flat: models don't automatically \"think harder\" in the late game.**\n\n**Watch more:**\n\n* Paper link: [https://arxiv.org/abs/2512.18564](https://arxiv.org/abs/2512.18564)\n* [Example save 1](https://civitas-john.github.io/vox-deorum-replay/?file=https://civitas-john.github.io/vox-deorum-replay/examples/1.Civ5Replay)\n* [Example save 2](https://civitas-john.github.io/vox-deorum-replay/?file=https://civitas-john.github.io/vox-deorum-replay/examples/2.Civ5Replay)\n* [Example save 3](https://civitas-john.github.io/vox-deorum-replay/?file=https://civitas-john.github.io/vox-deorum-replay/examples/3.Civ5Replay)\n\n**Try it yourself:**\n\n* The Vox Deorum system is 100% open-sourced and currently in beta testing\n* GitHub Repo: [https://github.com/CIVITAS-John/vox-deorum](https://github.com/CIVITAS-John/vox-deorum)\n* GitHub Release: [https://github.com/CIVITAS-John/vox-deorum/releases](https://github.com/CIVITAS-John/vox-deorum/releases)\n* Works with any **OpenAI-compatible local providers**\n\n[We exposed the game as a MCP server, so your agents can play the game with you](https://preview.redd.it/tccdt44oq79g1.png?width=2291&format=png&auto=webp&s=0b8a4fe5871db4d2bf00f417acd13de3e688037f)\n\n**Your thoughts are greatly appreciated:**\n\n* What's a good way to express the game state more efficiently? Consider a late-game turn where you have 20+ cities and 100+ units. Easily 50k+ tokens. Could multimodal help?\n* How can we get LLMs to play better? I have considered RAG, but there is really little data to \"retrieve\" here. Possibly self-play + self-reflection + long-term memory?\n* How are we going to design strategy games if LLMs are to play with you? I have put an LLM spokesperson for civilizations as an example, but there is surely more to do?\n\n**Join us:**\n\n* I am hiring a PhD student for Fall '26, and we are expanding our game-related work rapidly. Shoot me a DM if you are interested!\n* I am happy to collaborate with anyone interested in furthering this line of work.",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvrxvar",
          "author": "false79",
          "text": "Today it's Civ5  \nTomorrow it's the 3 Body Problem",
          "score": 122,
          "created_utc": "2025-12-24 20:59:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvt0aa2",
              "author": "TaifmuRed",
              "text": "\"AGI soon via scaling\"",
              "score": 17,
              "created_utc": "2025-12-25 01:13:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvryyvw",
              "author": "lookwatchlistenplay",
              "text": "Peace be with us.",
              "score": -68,
              "created_utc": "2025-12-24 21:05:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwghpyb",
                  "author": "false79",
                  "text": "lol - you ate so many downvotes before you edited your response.\n\nIt was pretty clear you never read or seen 3 body problem. But there is a key part of the story where the audience learns about how an alien race is trying to use simulations of their civilation, trying to figure out under what conditions they would survive.  \n  \nTo my knowledge, the story was entertaining and interesting given that it was a big hit in China that transcending to western audiences via Netflix.  \n  \nIn the future, don't be that fool, unless you eating downvotes is your thing.",
                  "score": 1,
                  "created_utc": "2025-12-29 00:19:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvsblsn",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -34,
                  "created_utc": "2025-12-24 22:22:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsdn2e",
          "author": "ahjorth",
          "text": "An idea so crazy it could only come out of the CCL. Great job, guys!\n\nDid you explore any options that treat the game as quasi-multi-level ABMs, where the decisions of individual units are made to optimize for unit-level (i.e. local environment) goals + nearby city goals + regional/continental goals + global goals? \n\nI realize this would be a big change away from the way you are currently using the built in AI, but Iâ€™d be really curious to see what you can do. Maybe feed the world state in like you do now, to articulate overall goals, then iterate over each continent ands articulate more localized goals based on the global goals, then cities, etc down to units. For each level, revise or confirm the existing goals to take into account any changes to the global state, and finally articulate decisions at the various levels (choosing science/culture, what to build in a city, where to move a unit, etc). Maybe do this a few times to allow revisions in response to the simultaneous decisions of other cities/units. \n\nEither way, congrats on finishing, your new job, and on this project! Cheers, Arthur (who left just before you started)",
          "score": 14,
          "created_utc": "2025-12-24 22:35:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvson1q",
              "author": "vox-deorum",
              "text": "Didn't expect to meet you here, Authur! The project was my last one started at CCL. Yes, and I received a very similar comment there :D\n\nYes, I think this can be an amazing idea. Training RL models at the individual unit or city level could be waaaay easier than at the global level. Performance aside, it may also create some hilarious situations where micro-level rewards deviate from the macro-level ones. Think about morale, self-preservation, etc...",
              "score": 7,
              "created_utc": "2025-12-24 23:50:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvsopx9",
                  "author": "vox-deorum",
                  "text": "And I don't think that's the opposite of what we are doing; on the contrary, it can be very compatible.",
                  "score": 6,
                  "created_utc": "2025-12-24 23:51:22",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwb4tv",
              "author": "ChocolatesaurusRex",
              "text": "to your point, I'd be curious to see if shifting to a 'turn-by-committee' approach sending recommendations to a 'decision' agent would allow a more dynamic playstyle that naturally adjusts to the increasing late-game complexity.",
              "score": 1,
              "created_utc": "2025-12-31 11:12:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvs7tf1",
          "author": "ASTRdeca",
          "text": "Very cool! You mentioned in the paper that despite GLM being much larger than GPT-OSS 120B, the larger size didn't seem to impact performance. I'm wondering if you tried models smaller than OSS-120B to see at what point model size matters? (For example, OSS-20B?) \n\nI'm just thinking about the viability of running these kinds of systems locally, since 120B is probably too large for most users to run themselves",
          "score": 24,
          "created_utc": "2025-12-24 21:58:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs8b7p",
              "author": "vox-deorum",
              "text": "OSS-20B works for me locally. I haven't put it to a large-scale experiment due to cost concern (on OpenRouter, 20B and 120B were almost at the same price). That said, we are exploring hybrid options (e.g., getting OSS-20B to process the raw game state and then a stronger model gets to do decision-making).",
              "score": 13,
              "created_utc": "2025-12-24 22:01:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvsczqr",
                  "author": "NickNau",
                  "text": "despite the price indifference - testing smaller models can be a very interesting test by itself. I bet it may provide some new insights when enough models are tested.\n\nthank you for your work. cool stuff",
                  "score": 12,
                  "created_utc": "2025-12-24 22:31:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvu072d",
                  "author": "Qwen30bEnjoyer",
                  "text": "Just curious, with the cost concern, maybe you could try Chutes.ai? A $20 subscription buys up to 5000 calls of Kimi K2 Thinking and other models with no input or output token limits.\n\nAnother thought is maybe we could make this into a benchmark by pitting 8 Civilizations against each other, and calculating an ELO rating?",
                  "score": 4,
                  "created_utc": "2025-12-25 05:59:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw0fg8g",
              "author": "Glad_Remove_3876",
              "text": "Yeah we actually did test OSS-20B internally and it was surprisingly viable - still managed to survive most full games without major issues. The sweet spot seems to be somewhere around that 20B mark where you get decent strategic reasoning without needing a data center\n\n  \nFor local stuff you're probably right that 120B is pushing it for most people, but 20B is definitely doable on a decent gaming rig with some patience",
              "score": 2,
              "created_utc": "2025-12-26 11:51:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvs3p9x",
          "author": "Amazing_Athlete_2265",
          "text": "Nice. I love civ games (been playing since the original). Would be keen to play against one of my local models.",
          "score": 37,
          "created_utc": "2025-12-24 21:33:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs6dnj",
              "author": "vox-deorum",
              "text": "Yes, you can play with a small one. Even GPT-OSS-20B seems to work well (although I am unsure how clever/dumb it will be).",
              "score": 12,
              "created_utc": "2025-12-24 21:49:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvs7o05",
                  "author": "Amazing_Athlete_2265",
                  "text": "I like my small models, thinking about trying LFM2-8B-A1B and Qwen3 4B instruct",
                  "score": 1,
                  "created_utc": "2025-12-24 21:57:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvt6ku7",
              "author": "randylush",
              "text": "Sounds like it doesnâ€™t really play all that differently from a regular game algorithm",
              "score": 1,
              "created_utc": "2025-12-25 02:01:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtyv69",
                  "author": "vox-deorum",
                  "text": "Speaking of the outcome, yes. Speaking of the macro-level playstyle, we found some significant differences.",
                  "score": 2,
                  "created_utc": "2025-12-25 05:47:27",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvs99y4",
          "author": "invisiblelemur88",
          "text": "Could one of these be added into a multiplayer civ 5 game? My friends and I play every wednesday evening together for years now... would love to experiment with getting more interesting AIs involved. The existing AIs in it are particularly flat.",
          "score": 18,
          "created_utc": "2025-12-24 22:07:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvt1nor",
              "author": "Amazing_Athlete_2265",
              "text": "If you haven't already, try Vox Populi mod. It makes the stock AI a lot better.",
              "score": 7,
              "created_utc": "2025-12-25 01:24:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvt7wcb",
                  "author": "vox-deorum",
                  "text": "Yes and we are working closely with the VP team!",
                  "score": 8,
                  "created_utc": "2025-12-25 02:11:43",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nvt6a6i",
                  "author": "invisiblelemur88",
                  "text": "Does Vox Populi work multiplayer?",
                  "score": 4,
                  "created_utc": "2025-12-25 01:59:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvsmyt7",
              "author": "vox-deorum",
              "text": "That's definitely possible. Vox Deorum is based on Vox Populi, which supports multiplayer. That said, I never tested it myself, and I would envision some minor revisions to avoid desync issues in a networked game. A hotseat game should be smooth!",
              "score": 5,
              "created_utc": "2025-12-24 23:39:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvzln0l",
                  "author": "ArtfulGenie69",
                  "text": "If it got connected up you could see it's win rate vs humans :)",
                  "score": 1,
                  "created_utc": "2025-12-26 06:49:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsennl",
          "author": "-InformalBanana-",
          "text": "Did you maybe try qwen3 2507 30b a3b instruct or thinking?\nWhat a fun experiment.Â ",
          "score": 5,
          "created_utc": "2025-12-24 22:42:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvs8qik",
          "author": "uroboshi",
          "text": "This is really cool, thanks for sharing your discoveries. I'll make some tests too when I can. Thanks!",
          "score": 5,
          "created_utc": "2025-12-24 22:04:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs8zrv",
              "author": "vox-deorum",
              "text": "Great! Let me know if any issue arises.",
              "score": 1,
              "created_utc": "2025-12-24 22:06:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsqhj8",
          "author": "pesaru",
          "text": "Are you specifically trying to do this without tools? Whenever I give an AI a task that requires handling a lot of data, for example, \"go through my entire project and identify instances of \\_\\_\\_\\_ and then apply transformation Y to them, the truly exceptional models will write a tool to do much of that (the shitty models sometimes try but then spend a million tokens going in circles doing absolutely nothing). There are a bunch of PowerShell scripts littering my projects that are remnants of those sorts of activities. However, the more you do this type of strategy, the closer you get to that algorithmic AI play.   \n  \nI get the sense that the only way you could give the LLM an advantage would be to allow it to self record information about its strategies and how often each action lead to survival/winning, basically recreating the **MENACE** system of the 1960s and allowing the LLM to essentially learn from experience over time, allowing them to discover novel strategies that the algorithmic AI would likely not be capable of. \n\nAnd so I feel the really neat thing to do would be going the route of AlphaEvolve -- get the AI to exclusively focus on iteratively writing code to play the game based on inputs. That would likely produce the best possible result.",
          "score": 9,
          "created_utc": "2025-12-25 00:03:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsspph",
              "author": "vox-deorum",
              "text": "Love your ideas! 1) Technically, when LLMs make decisions, they call tools through the MCP server, and the algorithm-based AI executes the details. 2) Yes! Self-reflection is something we are looking into now. 3) Yes again - like u/ahjorth mentioned here, it may be very interesting to look into self-evolving algorithms/RL models at the micro level.",
              "score": 5,
              "created_utc": "2025-12-25 00:19:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvuj28o",
                  "author": "dasjomsyeet",
                  "text": "Very interesting project! If you havenâ€™t already I recommend checking out how the LLM harnesses for projects like ClaudePlaysPokemon are built. Not sure if it does anything you arenâ€™t already doing, but they have a memory management tool where it loads prior decisions back into the context window and writes new memories if important decisions regarding strategy are made. Could be worth looking into how they did it.",
                  "score": 1,
                  "created_utc": "2025-12-25 09:12:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsaqv9",
          "author": "steezy13312",
          "text": "Iâ€™m really excited to try this out this weekend. Iâ€™m really curious how much the LLMs can lean into their civilization leaderâ€™s persona in decision-making and approach, vs just trying to win based on solely the gameâ€™s mechanics",
          "score": 7,
          "created_utc": "2025-12-24 22:17:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvssv3z",
              "author": "vox-deorum",
              "text": "I guess we can prompt it a bit more towards role-playing (but that also depends on the model?)",
              "score": 1,
              "created_utc": "2025-12-25 00:20:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsamyh",
          "author": "JsThiago5",
          "text": "You did not put them to play against each other, right?",
          "score": 5,
          "created_utc": "2025-12-24 22:16:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsmrke",
              "author": "vox-deorum",
              "text": "Not yet, but maybe we should create an arena where LLMs fight each other in Civilization?",
              "score": 10,
              "created_utc": "2025-12-24 23:37:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvt04bf",
                  "author": "mj3815",
                  "text": "The ultimate benchmark",
                  "score": 9,
                  "created_utc": "2025-12-25 01:12:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvtycfv",
                  "author": "lochyw",
                  "text": "I mean what about actual training/ learning for improving results.Â \nPerhaps finding metrics or observability on possible moves/options and picking the best ones to allow for better decision making.Â \n\n\nOr enhanced sim speed for beyond 1x time scale testing/training like the normal ML stuff does for training.Â \n\n\nSurface level for this post doesn't appear too interesting imo but there's so much potential beneath the surface.Â ",
                  "score": 1,
                  "created_utc": "2025-12-25 05:42:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvswb50",
          "author": "o0genesis0o",
          "text": "I'm amazed that you are able to turn this research question into a proper project and secured funding for recruiting PhD student. As a fellow struggling academic, hats off to you and jealous to your future PhD students. They seem to have some very interesting research problems ahead of them. Best of luck.",
          "score": 3,
          "created_utc": "2025-12-25 00:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvswmvr",
              "author": "vox-deorum",
              "text": "Good luck! It has been an incredibly challenging year for everyone on the market. Let me know if you need anything or if some collaboration would help with your situation.",
              "score": 3,
              "created_utc": "2025-12-25 00:46:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvsxwpy",
                  "author": "o0genesis0o",
                  "text": "I'm developing a framework for multi LLM agent from scratch, more like to refresh my skill. My goal is to work better with my llamacpp server. If I can think of some research topic to leverage this or if I reach the point where I can open source it, i would reach out. Or if you think of something interesting, I'm all ears too. Enjoy your holiday.",
                  "score": 2,
                  "created_utc": "2025-12-25 00:55:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt1pg1",
          "author": "T_UMP",
          "text": "https://preview.redd.it/t2dmblyj599g1.jpeg?width=1920&format=pjpg&auto=webp&s=b85051cd0514a5c3f2a01c20fc0b94da8caa94ed\n\nIf there was a way to have a LLM work with this that would be a blast. Not to mention work as a proper humanlike AI.",
          "score": 4,
          "created_utc": "2025-12-25 01:24:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtylr9",
              "author": "lochyw",
              "text": "Getting a real time sim game to work in actual real time would be interesting.Â ",
              "score": 3,
              "created_utc": "2025-12-25 05:45:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsmpyy",
          "author": "a_beautiful_rhind",
          "text": "So OSS, despite the censored facade, is a heartless warmonger underneath? Yet GLM, the less \"safe\" model, is a relatively nice guy? \n\n>models preferred Order (communist-like, ~24% more likely) ideology over Freedom\n\nThe hits from our alignment overlords just keep coming and literally write themselves.",
          "score": 14,
          "created_utc": "2025-12-24 23:37:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvt43tj",
              "author": "JazzlikeLeave5530",
              "text": "Really don't think that's how it works with Civ V considering it's all gameplay related and not actually mapping to real-world stuff lol. Like fascism gives military bonuses while communism gives science in Civ 6, so if it's going for a science victory then that's why it'll pick that...",
              "score": 5,
              "created_utc": "2025-12-25 01:43:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvt2fir",
              "author": "fivecanal",
              "text": "One could argue the preference for order is exactly the goal of their supposed 'alignment'",
              "score": 5,
              "created_utc": "2025-12-25 01:30:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvs49kb",
          "author": "scottybowl",
          "text": "Sorry if Iâ€™m being dumb, but not sure I understand the takeaway here. What have you learned from doing this?",
          "score": 9,
          "created_utc": "2025-12-24 21:37:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs5sv7",
              "author": "vox-deorum",
              "text": "It is now possible to get open-source LLMs to play end-to-end Civilization V games. They are not beating algorithm-based AI on a very simple prompt, but they do play quite differently.",
              "score": 24,
              "created_utc": "2025-12-24 21:46:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvseesa",
                  "author": "klipseracer",
                  "text": "I think they mean: Why, for what purpose. What are the use cases.",
                  "score": 6,
                  "created_utc": "2025-12-24 22:40:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvslrz6",
          "author": "J-IP",
          "text": "Im looking forward to when we can have smaller finetuned models avaliable in order to insert more flavor and diversity in to different games like this!",
          "score": 2,
          "created_utc": "2025-12-24 23:30:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvslxo5",
              "author": "vox-deorum",
              "text": "Yes! That's a very legit goal.",
              "score": 1,
              "created_utc": "2025-12-24 23:31:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvss61a",
          "author": "slippery",
          "text": "Impressive achievement and insights. Keep going!",
          "score": 2,
          "created_utc": "2025-12-25 00:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvswalf",
          "author": "xxxx771",
          "text": "how do you feed the game state into the LLM? Do you read each world tile as the player would see and you feed this into a structured manner to the llm or how exactly?",
          "score": 2,
          "created_utc": "2025-12-25 00:44:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsyf2e",
              "author": "vox-deorum",
              "text": "We fed the following information in markdown format:  \nGame rules (map sizes, speed, etc); Players; Cities; Units; Tactical Zones (In-game AI's estimation); Events. The map is only implicitly given through events. Otherwise, a map itself is 56x36 = 2,016, and we would constantly need at least 40k tokens in the late game.",
              "score": 4,
              "created_utc": "2025-12-25 00:59:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvt2ant",
                  "author": "xxxx771",
                  "text": "So when you say cities/units do you feed like a grid map with each tile contents or what?",
                  "score": 1,
                  "created_utc": "2025-12-25 01:29:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw2qi14",
                  "author": "sadjoker",
                  "text": "\"DeepSeek-OCR demonstrates that 100 vision tokens can represent approximately 1000 text tokens with 97%+ accuracy.\"",
                  "score": 1,
                  "created_utc": "2025-12-26 20:05:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt809e",
          "author": "Automatic-Boot665",
          "text": "Try GLM 4.7",
          "score": 2,
          "created_utc": "2025-12-25 02:12:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvt88j1",
              "author": "vox-deorum",
              "text": "Will do!",
              "score": 1,
              "created_utc": "2025-12-25 02:14:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtd31m",
          "author": "phratry_deicide",
          "text": "You might be interested in /r/unciv, an open source clone attempt of Civ 5, also available on mobile (and Pixels have Tensor chips).",
          "score": 2,
          "created_utc": "2025-12-25 02:50:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtsr25",
              "author": "vox-deorum",
              "text": "Cool! I'd love to see if, at some point, unciv and Vox Populi can get together. I think technically the system can be ported there, and I would love for someone to look into that.",
              "score": 1,
              "created_utc": "2025-12-25 04:54:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtukxl",
          "author": "gromhelmu",
          "text": "What is the difference between the top-right and bottom-right graphic? They look identical, except for the color.",
          "score": 2,
          "created_utc": "2025-12-25 05:09:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtvqqd",
              "author": "vox-deorum",
              "text": "Oops!!!! My bad. Was supposed to be 2 different graphs. Thanks for pointing it out!",
              "score": 2,
              "created_utc": "2025-12-25 05:19:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvu7ks6",
                  "author": "gromhelmu",
                  "text": "Happens to all of us. Glad this didn't go by unnoticed!",
                  "score": 2,
                  "created_utc": "2025-12-25 07:10:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvukegf",
          "author": "polyc0sm",
          "text": "Can you use this to play the game with you instead ? Like audio only where you ask for a summary of what happened, ask more precise questions, list options then take actions ? It would be a revolution for many people (blind people, long car drive with kids (collaboratively), play while outside on a walk)",
          "score": 2,
          "created_utc": "2025-12-25 09:27:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvws94r",
              "author": "vox-deorum",
              "text": "Wow, that's a COOL idea and definitely possible",
              "score": 1,
              "created_utc": "2025-12-25 19:18:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwfym4",
          "author": "Different-Toe-955",
          "text": "How is the LLM interacting with the game? Is it being presented with text based choices to make?",
          "score": 2,
          "created_utc": "2025-12-25 18:07:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwgxmq",
              "author": "vox-deorum",
              "text": "Yes, sort of. It replaced the in-game AI's high-level decision-making, e.g., setting technology, policy, and also macro-level \"strategies\" that basically tweak the algorithmic AI's weights. For example, it can try to prioritize building an army, building ranged units, building happiness buildings, but they won't directly set a city's building priorities. That's what it stands for now.",
              "score": 3,
              "created_utc": "2025-12-25 18:12:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvyvo6g",
          "author": "Fwiffo_the_brave",
          "text": "I actually tried to do something similar with my own game that I have been developing, as I had an experimental version that could use an LLM to make strategic and diplomatic decisions for the AI of my game (it is similar to Master of Orion 1). I found that the LLMs were decent at the game, but I had a lot of issues with the smaller models not being able to work with the command format I made or issues with it just hallucinating planets. \n\nI never let it get to the end game due to the amount of prompts it burned through, but I did let it get decently into the game a few times and it was at least doing better than my AI at managing planets, but it was a bit worse at managing fleets and allocating defenses. Where it really did well was with diplomacy. Unlike a normal AI, it was a bit more fun to bargain with and a lot more fun to send insulting messages to when declaring war. It had limited control of the relationship status, so sending insulting messages could actually piss it off enough to get declared war on. It was far less stiff compared to the normal AI\n\nAt some point I might look at actually releasing a separate version of my game with LLM AIs as an option once the game is feature complete. Way to difficult having to update my AI and the LLM AI for each new feature or change that I make, especially as stuff does change frequently still.",
          "score": 2,
          "created_utc": "2025-12-26 03:21:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvskizm",
          "author": "Murhie",
          "text": "First of all: Thats dope AF. Love civ. Ive skimmed over the paper. Some very quick thoughts with regard to your questions (but the team has probably thought more about it better than a random redditor who has skimmed the paper):\n\nMore token efficient state: In your paper i see its a markdown with information. First thing coming to my mind is try and only sent updates compared to the previous turn instead of all information always, but thay would only work if previous states remain in context somehow, I guess size would grow anyway but inference can be more efficient like this. It would also help with memory. I see you already do this for events. \nMultimodal could help, you might also try to map the map (map the image of the map with tiles) to a numerical matrix where each coordinate is described (dimension for every possble feature) and add a few dimensions for other info. You would then pass a definition of those features in the system prompt. (Completely making this up. Have no experience or empirical evidende that this would work or even reduce size)\n\nBetter play: I would guess the most promising thing to add is memory. Unlikely to help with your input size state problem though.\nSecond, multi agent systems could help here, but will introduce a shitload of complexity. Where one agent coordinates the whole strategy and other agents (for instance research, economic, diplomatic, military agents) report to the coordination agent and micromanage. Maybe there you could add history as well.\nFurthermore, the state as described in the paper seems a bit basic, but seeing how it grows in size each turn its probably way more detailed than described. For instance: geographic/spatial features matter a lot (where is everything and how does that relate to each other, proximity to untapped resources, etc). It is unclear from the paper how that is managed. \nAlso the \"X\" in LLM+X matters a lot I think, I am not too familiar with the engine used here for unit movement or builder actions, but there needs to be a way where that is coordinated with what the LLM is doing. A lot of interesting things can be done here.",
          "score": 2,
          "created_utc": "2025-12-24 23:21:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsp6vw",
              "author": "vox-deorum",
              "text": "Thank you a lot! We will think along these lines soon :)",
              "score": 1,
              "created_utc": "2025-12-24 23:54:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtl43a",
          "author": "R33v3n",
          "text": "I know there's real agentic and safety applications with this type of research, but what hypes me most is the silly prospect of one day being able to play a Stellaris or Civilization-like game against AIs that really embody a given ruler or culture's persona, and do diplomacy in real time. Complete with plans, improvisation, cooperation, rivalries, dreams and *spite*. <3\n\n>How can we get LLMs to play better? I have considered RAG, but there is really little data to \"retrieve\" here. Possibly self-play + self-reflection + long-term memory?\n\n>How are we going to design strategy games if LLMs are to play with you? I have put an LLM spokesperson for civilizations as an example, but there is surely more to do?\n\nHave you checked what similar undertakings and harnesses in different genres do? Like *CHIM* in Skyrim or Claude Plays Pokemon? Or [what's being done](https://arxiv.org/abs/2506.09655) on the board-game Diplomacy side of things? These might be decent inspirations on how to harness (or fine-tune, in the lattter's case) LLMs for game environments.",
          "score": 3,
          "created_utc": "2025-12-25 03:53:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvttagn",
              "author": "vox-deorum",
              "text": "Oh, definitely not! I am not likely to put this on my grant proposals, but yes, that's my main motivation. We are working with the Vox Populi community to see how we can get you negotiate with an LLM player. And I think there is much more to be done, like what if we put an image/video generator to \"materialize\" the alt-history you made in the game? \n\nYes, I have looked at (and got inspired by) many recent studies in this direction. Civ is a bit unique in that the game state itself is much more complex than, say, Diplomacy, but fine-tuning is something we will look into next!",
              "score": 1,
              "created_utc": "2025-12-25 04:59:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsez6e",
          "author": "Jannik2099",
          "text": "Is it possible to have multiple LLMs play in one game with just one Civ 5 license? I could run multiple instances through wine.\n\nWe host a few on-premise models and it would be very entertaining to have them compete against one another...",
          "score": 1,
          "created_utc": "2025-12-24 22:44:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsmnyp",
              "author": "vox-deorum",
              "text": "Yes! We didn't run the experiment like that, but that's definitely possible. Personally, I am playing a game with 2 LLM players. You can customize the configuration through WebUI. You can also manually edit the config file since a few options are not exposed there right now. DM me if you have questions.",
              "score": 2,
              "created_utc": "2025-12-24 23:36:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvswow0",
          "author": "Sabin_Stargem",
          "text": "It would be neat if you can have four different AIs attempt to complete Pokemon.   Say Generation 1's Pokemon Blue, Red, Green, and Yellow?   Each AI can have their cover starter.\n\nAfter each gym, you could require them to fight each other, and also permit them to do trading of monsters.   This gives us a chance to see how 'social' AI can be when it comes to making trades, what strategies they take to acquire their badges, exploration vs combat, and so forth.\n\nSomeone already did a timelapse of AI trying to beat Pokemon some years ago.   How different have things become?\n\n---\n\nTraining AI to Play Pokemon with Reinforcement Learning\nhttps://www.youtube.com/watch?v=DcYLT37ImBY",
          "score": 1,
          "created_utc": "2025-12-25 00:46:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsytcj",
              "author": "vox-deorum",
              "text": "Yeah, the idea sounds similar here.",
              "score": 2,
              "created_utc": "2025-12-25 01:02:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvu3gca",
          "author": "MarkIII-VR",
          "text": "This really makes you think about the work put into making the built in game AI functional to the point that the game is actually playable against the computer.\n\nReally thought provoking on just how good the developers were at that time!",
          "score": 1,
          "created_utc": "2025-12-25 06:29:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwf77r",
              "author": "vox-deorum",
              "text": "That's true. [https://www.vice.com/en/article/the-modders-who-decided-to-overhaul-the-ai-in-civilization-v/](https://www.vice.com/en/article/the-modders-who-decided-to-overhaul-the-ai-in-civilization-v/)",
              "score": 1,
              "created_utc": "2025-12-25 18:02:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvucj7l",
          "author": "robbedoes-nl",
          "text": "I saw that LLM are really good at the Global Thermonuclear War. But itâ€™s an older game from 1983.",
          "score": 1,
          "created_utc": "2025-12-25 08:01:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwf4na",
              "author": "vox-deorum",
              "text": "Which game? Quite curious.",
              "score": 1,
              "created_utc": "2025-12-25 18:02:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvwjmis",
                  "author": "robbedoes-nl",
                  "text": "Sorry, it was a reference to the movie Wargames from 1983. A computer played a â€˜gameâ€™ with a hacker and they almost started WW3.",
                  "score": 1,
                  "created_utc": "2025-12-25 18:28:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvuo2sc",
          "author": "No-Comfort6060",
          "text": "It would be really interesting to see if Tiny Recursive Models could be used here for reasoning",
          "score": 1,
          "created_utc": "2025-12-25 10:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwf163",
              "author": "vox-deorum",
              "text": "How much context window can it handle? But we can also transform the game state into something much smaller.",
              "score": 1,
              "created_utc": "2025-12-25 18:01:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvut2fy",
          "author": "Ok_Try_877",
          "text": "How does the LLM interact with the game? Is their an API for CIv or have you connected it up to the mouse/screen? Please tell me its not manual?",
          "score": 1,
          "created_utc": "2025-12-25 11:02:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvws6w2",
              "author": "vox-deorum",
              "text": "I don't have so many hands to play 2,000 games manually, do I? Well we did build an API to connect into Civ V. Mouse/screen control is possible but that would make the cost much higher.",
              "score": 1,
              "created_utc": "2025-12-25 19:17:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvuy8yo",
          "author": "timwaaagh",
          "text": "Maybe just try some more of the  bigger llms like deepseek. It might just be that glm is weak here.",
          "score": 1,
          "created_utc": "2025-12-25 11:57:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuzv7v",
          "author": "nunofgs",
          "text": "Very cool! Congrats!\n\nI wonder what are your thoughts on a generic game orchestration approach? Sounds like you didnâ€™t get far on it but what do you think are the major challenges there? How successful were you with that approach?",
          "score": 1,
          "created_utc": "2025-12-25 12:13:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwes6u",
              "author": "vox-deorum",
              "text": "Right now, we still use a ton of game-specific mechanics/scaffolds, which is both a boon (from a cost-effectiveness/performance perspective) and bane (from a generalization perspective). It depends on the end goal. Combined with other studies in this realm, I can say most (somewhat strategic?) games would benefit from a hybrid approach where LLMs give a human touch at the macro level and conventional AI executes the rest.",
              "score": 1,
              "created_utc": "2025-12-25 18:00:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvv9jvh",
          "author": "SpicyWangz",
          "text": "My main takeaway here is that ai likes authoritarianism. And if people in power start letting it make decisions for them, we will be enslaved by the machine",
          "score": 1,
          "created_utc": "2025-12-25 13:36:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvw199j",
          "author": "txgsync",
          "text": "This is very cool. I wrote a benchmark for LLMs to try to play Zork and most just wandered the house around holding a nasty knife and dying to the ogre. \n\nI may give your framework for ZorkBench!",
          "score": 1,
          "created_utc": "2025-12-25 16:39:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwe990",
              "author": "vox-deorum",
              "text": "Cool! Let me know how it works :D",
              "score": 1,
              "created_utc": "2025-12-25 17:57:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwd8wk",
          "author": "ElementNumber6",
          "text": "No need to go so far.  Just play a simple game of chess with them and watch as they falter at every possible opportunity.",
          "score": 1,
          "created_utc": "2025-12-25 17:51:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwkevp",
          "author": "overmind87",
          "text": "You should consider adapting this idea to work with rimworld, to see how different Ai models would work at a much smaller scale, managing the dynamics and needs of individuals in a small colony. And then see how that compares with the way they run a civ game. That way you get a good, broad look at the lowest level and highest level of social complexity management abilities for each model.",
          "score": 1,
          "created_utc": "2025-12-25 18:32:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvws1zq",
              "author": "vox-deorum",
              "text": "Well, I do play RimWorld a lot, and that's indeed our next project!",
              "score": 2,
              "created_utc": "2025-12-25 19:16:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvxe5rg",
                  "author": "overmind87",
                  "text": "Cool! I'll keep an eye out for the results!",
                  "score": 1,
                  "created_utc": "2025-12-25 21:32:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvxhrew",
          "author": "RogueProtocol37",
          "text": "Awesome work!  I'm looking forward to all my strategy games exposing a MCP interface\n\nHave you thought about let one LLM model play against another LLM model?\n\nP.S. Might be worth to cross-post this to /r/civ and /r/civfanatics",
          "score": 1,
          "created_utc": "2025-12-25 21:54:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxn9fl",
              "author": "vox-deorum",
              "text": "Yes! I planned to write a separate post for them (since they care more about Civ than about LLMs, I guess). Aksi, the mod is also available on CivFanatics (forum). \n\nI am now running a new experiment where several different types of agents compete against each other. I will do an ELO calculation later...",
              "score": 2,
              "created_utc": "2025-12-25 22:28:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvxlrxt",
          "author": "Crimsoneer",
          "text": "I did something similar with Risk last year\n\nhttps://andreasthinks.me/posts/ai-at-play/",
          "score": 1,
          "created_utc": "2025-12-25 22:18:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxmxm3",
              "author": "vox-deorum",
              "text": "Great to see! Make me think of those studies playing diplomacy. Anything special you noticed?",
              "score": 1,
              "created_utc": "2025-12-25 22:25:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvxo6i5",
                  "author": "Crimsoneer",
                  "text": "Mainly that the scaffolding was really important, and some interesting variation in behavior by model - eg, some models notably more aggressive or chatty",
                  "score": 2,
                  "created_utc": "2025-12-25 22:33:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzt3lp",
          "author": "postitnote",
          "text": "I think there's a bug in your code. in vox-agent.ts:\n[code]   // Handle messages\n    if (lastStep === null) {\n      config.messages = [...messages, ...await this.getInitialMessages(parameters, input, context)];\n    } else if (this.onlyLastRound) {\n      // Keep all system and user messages, but only the last round of assistant/tool messages\n      const filteredMessages: ModelMessage[] = [];\n      let lastUserIndex = -1;\n\n      // Pass 1: keep all system and user messages\n      for (let i = 0; i < messages.length; i++) {\n        const message = messages[i];\n        filteredMessages.push(message);\n        lastUserIndex = i;\n        if (message.role !== 'system' && message.role !== 'user')\n          break;\n      }\n[/code]\nthe 'break' in pass 1 means it won't include the majority of the user messages that contain the history of the game. I wonder how big of an impact this could be.",
          "score": 1,
          "created_utc": "2025-12-26 08:02:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzwcj0",
              "author": "vox-deorum",
              "text": "Thanks! I am impressed you have looked into the source code. That said, what you found was intentional. The game state is too big to stay in the context window. In our second experiment, we designed a \"briefer\" that provides a briefing, and the briefer has a small memory window (can see its own briefing from 5 turns ago).",
              "score": 1,
              "created_utc": "2025-12-26 08:36:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw5vq05",
          "author": "Maasu",
          "text": "I love the idea of this, Imagine playing with a group of agents with long term memory and a discord channel between them  for diplomacy.. okay I am so doing this. I've already got the long term memory mcp https://github.com/ScottRBK/forgetful. This is happening.\n\nEdit: just had a skim through of the code and read the paper, so you have actually built your own agents.. very nice. Any plans to expose the actual agents as mcp tools?",
          "score": 1,
          "created_utc": "2025-12-27 08:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwb3tcj",
              "author": "vox-deorum",
              "text": "Vox-agents has support for MCP-based tool calling since we essentially implemented Civ V as an MCP server. It would be really cool to group those agents in a Discord channel. How would you envision the architecture?",
              "score": 1,
              "created_utc": "2025-12-28 03:55:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwc9g5v",
                  "author": "Maasu",
                  "text": "I need to sit down and actually see if this is feasible, but in my head it is something like this:Right now you have Civ5 -> Bridge -> MCP Server- > Vox Agents if i have understood correctly.\n\nI think the simplest approach, without modifying anything on Vox would be:\n\nhttps://preview.redd.it/mv1kl2xu2x9g1.png?width=760&format=png&auto=webp&s=e1edce0b2f7bfddef7ba0de39f9dc9a811b3eada\n\nAPI service that exposes a v1/completions to make it openai compatible, i've already built my own version of this and I can configure MCP's and prompts. Have different agents callable via the model parameter.\n\nFor each player I configure an agent with a long term memory mcp (using forgetful), prompt to align them with the AI they will be interpreting (so you can ensure you Ghandi knows to follow script once Nuclear weapons arrive).\n\nThe discord side of it would involve a bot for each agent listening in discord, the bot pings the v1/completions endpoint - either hitting the playing agent themselves or a similar agent with a different prompt that shares the same long term memory and the bot posts the response back to discord.\n\nI think as well as this giving each agent playing the game access to MCP to post on discord to allow for public announcements, pr campaigns, interactions with humans.\n\nJust a brain fart right now but need to see if i can implement it.",
                  "score": 1,
                  "created_utc": "2025-12-28 09:45:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwfecwn",
          "author": "theshrike",
          "text": "As always with these, I'm more interested in HOW you get the LLMs to play games, I don't care about the results that much.\n\nWhat kind of tools do you give the LLM to alter and view the game state? How do you do it?",
          "score": 1,
          "created_utc": "2025-12-28 20:57:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwfqx3k",
              "author": "vox-deorum",
              "text": "Hi! The paper has an appendix for this. Our recent update exposes a bit more data we missed before, but the principle stays the same.",
              "score": 2,
              "created_utc": "2025-12-28 21:58:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwlhu5j",
          "author": "ki7a",
          "text": "\\> 1,408 full Civilization V games  \nHow are you running experiments at scale?Â  Docker, slurm, batch runner, etc?\n\nIâ€™m interested in kicking off some trials on an HPC, preferably in parallel.  \nAny pointers and/or scripts for handling batch runs would be appreciated.",
          "score": 1,
          "created_utc": "2025-12-29 19:23:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlhza4",
              "author": "vox-deorum",
              "text": "Hi! Do you mean specifically Civ V?",
              "score": 1,
              "created_utc": "2025-12-29 19:24:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwlotbh",
                  "author": "ki7a",
                  "text": "The full stack really, but letâ€™s start with Civ V.\nDo you run multiple Civ V instances on the same machine?  Or do you run a vm/docker container for each game instance?\n\nAlso, what agent cmd/config would you recommend starting with?\nnpm run strategist -- --autoPlay",
                  "score": 1,
                  "created_utc": "2025-12-29 19:57:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpfy4y",
          "author": "implicator_ai",
          "text": "This is a fascinating setupâ€”basically they had the LLMs generate strategic decisions that the Civ V AI then executed, so the models werenâ€™t directly controlling the game engine but guiding it. The small score bump vs. lower win rate suggests the models explore different strategies rather than optimizing for victory. \n\n\n\nItâ€™s  an early look at how open models might handle long-horizon planning tasks.",
          "score": 1,
          "created_utc": "2025-12-30 09:57:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv4gbf",
          "author": "Analytics-Maken",
          "text": "Have you tried a summary layer or ACT-IN-LLM? They show promising results for token efficiency. I'm thinking of building a summary layer for business data analytics inside a data warehouse, where all the business data is consolidated via ETL tools like Windsor.ai. The goal is to allow stakeholders to use LLMs to query the data and generate reports without burning tokens.",
          "score": 1,
          "created_utc": "2025-12-31 04:58:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvqmoe",
              "author": "vox-deorum",
              "text": "Hi! Do you mean this paper? [https://openreview.net/pdf?id=3Ofy2jNsNL](https://openreview.net/pdf?id=3Ofy2jNsNL) Sounds promising, but do we need to train a specialized model with it? Could be feasible, since game state representations are pretty structurally similar. We are currently experimenting with getting a smaller LLM to summarize the game state before the decision-maker, but it turns out to be more nuanced (we didn't see a performance gain; also, the latency could get worse, since small LLMs still need to do the token generation).",
              "score": 1,
              "created_utc": "2025-12-31 07:58:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvsu7o",
          "author": "_Cromwell_",
          "text": "Can you make it play a Crusader Kings game next? I'm curious what choices it would make in there. ðŸ˜„\n\nFor science.",
          "score": 1,
          "created_utc": "2025-12-31 08:19:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvubmh",
              "author": "vox-deorum",
              "text": "I play CK3 too! It would be pretty damn interesting to get an LLM-driven character there. Measuring the success would be even harder given how CK3 is open-ended. Also, I would really like to have multiple LLM-driven characters to make the narrative interesting...",
              "score": 2,
              "created_utc": "2025-12-31 08:33:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwy3dg",
          "author": "skybsky",
          "text": "Wow, you have done what I think many of us (civ players) thought many times. Superb job! The fact that you used Vox Populi is a cherry on top :)  \nI'm hobby-building x-com like game with elements of 'Civ' gameplay, and I was thinking about introducing local LLM-controlled rival factions. Seeing your research gives me hope that it can end quite well!  \nIn Unity, there is an asset that allows you to integrate/ship a local LLM with the game build, so the player doesn't need to do anything. [https://assetstore.unity.com/packages/tools/ai-ml-integration/llm-for-unity-273604?srsltid=AfmBOopUQ6mC\\_ny3QQ6kB1dXbJFhgoMZAnFcJjsmr-kVvzfm4gqk2csg](https://assetstore.unity.com/packages/tools/ai-ml-integration/llm-for-unity-273604?srsltid=AfmBOopUQ6mC_ny3QQ6kB1dXbJFhgoMZAnFcJjsmr-kVvzfm4gqk2csg)",
          "score": 1,
          "created_utc": "2025-12-31 14:01:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvs5t78",
          "author": "PeakBrave8235",
          "text": "Why the hell would I want this?",
          "score": -17,
          "created_utc": "2025-12-24 21:46:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs6j72",
              "author": "vox-deorum",
              "text": "To get LLMs play games while you keep working. /s",
              "score": 16,
              "created_utc": "2025-12-24 21:50:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvs8qbu",
                  "author": "PeakBrave8235",
                  "text": "I was genuinely asking",
                  "score": 1,
                  "created_utc": "2025-12-24 22:04:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pvjpmb",
      "title": "Why I quit using Ollama",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/",
      "author": "SoLoFaRaDi",
      "created_utc": "2025-12-25 18:38:36",
      "score": 481,
      "num_comments": 196,
      "upvote_ratio": 0.91,
      "text": "For about a year, I've used Ollama like... 24/7. It was always my go-to, as it was frequently updated and had support for every model I needed.\n\nOver the past few months, there's been a serious decline in the updates & update content that releases with Ollama. I understand that, and just went about my day, as the maintainers obviously have a life. Cool! Then the \\*\\*Cloud\\*\\* update dropped. I saw Ollama as a great model runner, you just download a model and boom. Nope! They decided to combine proprietary models with the models uploaded on their Library. At first, it seemed cool. We can now run AI models that were otherwise impossible to run on consumer hardware, but then I started getting confused. Why did they add in Cloud, what's the point? What were the privacy implications? It just felt like they were adding more and more bloatware into their already massive binaries, so about a month ago, I made the decision, and quit Ollama for good.\n\nI feel like with every update they are seriously straying away from the main purpose of their application; to provide a secure inference platform for LOCAL AI models. I understand they're simply trying to fund their platform with the Cloud option, but it feels like a terrible move from the Ollama maintainers. \n\nWhat do you guys think?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvx8yxl",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-25 21:00:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwnwpq",
          "author": "q5sys",
          "text": "I soured on Ollama when (in the past) they phrased things that made it seem like the developments in llama.cpp were \"their\" improvements.  As an two decade long open source developer, I understand projects are built on the work of others, that's the exchange we make to let us dev what we want and we know that people can build on top our work.    \n  \nBut \"upstream's work\" is not \"your work\". Projects need to be honest about this.  You can still take credit for integrating upstreams work, but dont try to take credit for it.  \n  \nI don't know if they still do this, I hope they don't; but they certainly did in their early days and it really annoyed me.",
          "score": 318,
          "created_utc": "2025-12-25 18:52:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvx2dpk",
              "author": "siegevjorn",
              "text": "Can't agree with this point more. They should make it clear that they are just a go wrapper around llama.cpp. Well, at the vary least, they should acknowledge llama.cpp SOMEWHERE....\n\nWhen I first started playing with LLMs, of course with ollama, it was a super confusing journey because of these ambiguity.   With ollama, you gain no whatsoever low-level details about llms. The original source for the llm weights; their format; how they are running. \n\nOn the other hand, Llama.cpp, is super clear on every low level stuff. You can literally find everything with time and effort. It just rocks, man.",
              "score": 97,
              "created_utc": "2025-12-25 20:19:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzk8q1",
                  "author": "mrjackspade",
                  "text": "> They should make it clear that they are just a go wrapper around llama.cpp. Well, at the vary least, they should acknowledge llama.cpp SOMEWHERE....\n\nTo be fair, they're *not* a wrapper around Llama.cpp\n\nThey're a wrapper around the ggml libraries.\n\nSo it would be weird to credit Llama.cpp, especially when they've already credited GGML\n\nSource:\n\nhttps://news.ycombinator.com/item?id=44805396",
                  "score": -4,
                  "created_utc": "2025-12-26 06:36:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvz5w6a",
              "author": "BusRevolutionary9893",
              "text": "While researching it back in the early days, I realized why not just use llama.cpp? It almost seemed easier. Is compiling source code really so scary with instructions and LLMs to help you out? I have no idea why Ollama caught on in the first place. Now that LM Studio has been out for awhile, I have no idea why Ollama is still talked about.Â ",
              "score": 11,
              "created_utc": "2025-12-26 04:36:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzaagh",
                  "author": "DonkeyBonked",
                  "text": "I actually started with llama.cpp and I had found a model I was having trouble finding the gguf for that the official source was insisting on using ollama. I downloaded ollama and I don't like it. I found it more difficult than the way I have llama.cpp set up. I have my own GUI setup for launching models and interfacing with them so I don't have to use my browser (between the browsers, extensions, and cookies/trackers, I don't trust browsers to be private).\n\nI do not see the appeal of Ollama.",
                  "score": 3,
                  "created_utc": "2025-12-26 05:10:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw8covr",
                  "author": "FootballRemote4595",
                  "text": "Ollama caught on because it allowed you to swap models and it's simplified the out of the box experience.\n\n\nThen came llama swap.\n\n\n\nNow we have llama.cpp router.\n\n\nNow at this point it just gets in the way but for a time it was solving some pain points.",
                  "score": 2,
                  "created_utc": "2025-12-27 18:39:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1l4rk",
                  "author": "q5sys",
                  "text": "A lot of people used them because it was \"easier\" and \"models run so fast\" without realizing that they were qauntizing the models down so they would run \"fast\", mostly turning them stupid... but they were fast.  And truth is a lot of people only cared about the speed for their ERP or other type chats where capability didn't really matter much. So word of mouth spread and people flocked to it.   \nBut yea, llama.cpp was the far superior solution, but it also required a little effort to read the docs to figure it out.  There's a lot of people that just want to install a package and go.",
                  "score": 1,
                  "created_utc": "2025-12-26 16:26:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw0hvrw",
                  "author": "Western_Courage_6563",
                  "text": "Because ollama just works. Like fire and forget. And every other solution I tried, had some issues will it be with hardware, or software side of things...",
                  "score": 0,
                  "created_utc": "2025-12-26 12:13:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxt0g4",
              "author": "night0x63",
              "text": "Anyone with needing more concurrent users... Switch to SGLang or vLLM. Period. That's the main thing.Â \n\n\nAny cloud provider... Also does this switch. Is Ollama now doing cloud inference? Yes. Therefore they are IMO doing vLLM.",
              "score": 11,
              "created_utc": "2025-12-25 23:04:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1830v",
              "author": "emetah850",
              "text": "This.\n\nand to add on top of that, the creators of Ollama seemingly refuse to add the proper licenses to the binaries, even after extensive discussion on their github as to why something like *crediting the people who's work you're using is the right thing to do*\n\nGithub issue for reference: [https://github.com/ollama/ollama/issues/3185](https://github.com/ollama/ollama/issues/3185)",
              "score": 4,
              "created_utc": "2025-12-26 15:15:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvyiano",
              "author": "IaintJudgin",
              "text": "I think they EVENTUALLY credited llama.cpp (iirc)",
              "score": 3,
              "created_utc": "2025-12-26 01:50:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0fl6d",
                  "author": "Original_Finding2212",
                  "text": "Right before switching to their own engine and experiencing a major decent, then adding cloud support.",
                  "score": 3,
                  "created_utc": "2025-12-26 11:52:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1fmat",
              "author": "Mikasa0xdev",
              "text": "Yo, local inference is the real innovation. lol",
              "score": 1,
              "created_utc": "2025-12-26 15:57:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzu7gj",
              "author": "Ylsid",
              "text": "Ollamma lets you feel like an elite haxxor. That's why people use it over alternatives. Maybe",
              "score": 0,
              "created_utc": "2025-12-26 08:13:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwngk7",
          "author": "HungryMachines",
          "text": "I have been switching my python workflows to llama.cpp from ollama. The only thing I missed was model switching. With the recent updates that should also be resolved.",
          "score": 80,
          "created_utc": "2025-12-25 18:50:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwzqx8",
              "author": "ubrtnk",
              "text": "That was the one thing that kept me on Ollama for so long - switching and auto unload after 5 minutes. Been using Llama-swap + llama.cpp for a couple of months now and its fantastic but yea I'm curious if the native switching functions will reduce overhead even more (llama-swap is already REALLY light weight)",
              "score": 39,
              "created_utc": "2025-12-25 20:03:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvx2y2n",
              "author": "siegevjorn",
              "text": "This. That is the only reason to run ollama instead of llama.cpp for anyone, if they had to.",
              "score": 5,
              "created_utc": "2025-12-25 20:22:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyswda",
                  "author": "khronyk",
                  "text": "Yeah same. My main pc has windows + 3090 but i also have a dual 3090 epyc server running proxmox with ubuntu for the ai vm so it adds greatly to the complexity when I have to set things up across both systems. Not exactly happy with ollama, but the model switching/auto-unloading is what has kept me from exploring other options.",
                  "score": 4,
                  "created_utc": "2025-12-26 03:02:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwxxdq",
          "author": "zhambe",
          "text": "Surprised not to see an mention of vLLM here. It's my stock go-to.",
          "score": 42,
          "created_utc": "2025-12-25 19:52:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvx9n7b",
              "author": "Ryanmonroe82",
              "text": "Not as basic as lm studio or ollama but definitely the best option.",
              "score": 11,
              "created_utc": "2025-12-25 21:04:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvy0j0k",
                  "author": "rm-rf-rm",
                  "text": "And can't run on Mac.",
                  "score": 12,
                  "created_utc": "2025-12-25 23:53:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyyxd8",
                  "author": "_VirtualCosmos_",
                  "text": "\\*If you have enough vram to fit the entire BF16 base model in there\\*",
                  "score": 4,
                  "created_utc": "2025-12-26 03:44:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxakzb",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 4,
              "created_utc": "2025-12-25 21:10:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvy0rpa",
                  "author": "zhambe",
                  "text": "Yeah if you need to frequently reload different models, and performance (esp multiuser) is not your top priority, then vLLM might not be the top choice.\n\nI never compared how long it takes to shut down one instance of vLLM and spin up another with a different model vs swapping a model in ollama. I notice though the startup with vLLM takes a fair bit longer than ollama, so my guess is swapping models like that would be a drag.",
                  "score": 3,
                  "created_utc": "2025-12-25 23:55:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzvt7n",
                  "author": "Picard12832",
                  "text": "llama.cpp has that feature now.",
                  "score": 1,
                  "created_utc": "2025-12-26 08:30:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw0l1wp",
                  "author": "Camvizioneer",
                  "text": "The model swapping problem with vLLM is exactly why I built llmsnap. Uses vLLM's sleep mode to swap models in \\~1-2 seconds instead of full restarts - just puts idle models to sleep.",
                  "score": 1,
                  "created_utc": "2025-12-26 12:40:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxc6ry",
              "author": "nickless07",
              "text": "Isn't that the thing that need Linux? So, at least a couple more Gigabyte to run in Docker/WSL?",
              "score": 3,
              "created_utc": "2025-12-25 21:20:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvz3fb4",
                  "author": "woct0rdho",
                  "text": "There is vllm-windows if you want to avoid the overhead of WSL",
                  "score": 3,
                  "created_utc": "2025-12-26 04:17:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyfu84",
                  "author": "burntoutdev8291",
                  "text": "What about just ubuntu?",
                  "score": 2,
                  "created_utc": "2025-12-26 01:33:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw4jukv",
                  "author": "the_lamou",
                  "text": "What kind of WSL / Docker for Windows are you running that it's adding gigabytes of anything? The entire Linux kernel can be loaded in 200-some MB, and even the full standard Ubuntu Server kernel is like 512MB IIRC.",
                  "score": 1,
                  "created_utc": "2025-12-27 02:27:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwp9fx",
          "author": "cosimoiaia",
          "text": "Congrats, llama.cpp is the only way to go.",
          "score": 111,
          "created_utc": "2025-12-25 19:00:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvyynym",
              "author": "_VirtualCosmos_",
              "text": "What about LM Studio?",
              "score": 8,
              "created_utc": "2025-12-26 03:43:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvxyh3n",
              "author": "MonteManta",
              "text": "It even runs on my Android phone in termux",
              "score": 9,
              "created_utc": "2025-12-25 23:40:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvy50nk",
                  "author": "cosimoiaia",
                  "text": "Yup, there are a few app that use it but npu support is still quite lacking, I'm not sure if metal works on an iPhone though.",
                  "score": 1,
                  "created_utc": "2025-12-26 00:22:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyjv2u",
                  "author": "harbour37",
                  "text": "Can it access the gpu / npu on termux?",
                  "score": 1,
                  "created_utc": "2025-12-26 02:00:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxaiq7",
              "author": "Positive_Ad_313",
              "text": "Hi\nI am new just starting hosting Ollama on my NAS. What I read make me released I probably have to look at other option : can u give a link to llamaccp ?\nNothing to rely on Ollama ?",
              "score": -21,
              "created_utc": "2025-12-25 21:09:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxln57",
                  "author": "the_answer_is_penis",
                  "text": "Google?????? Apparently you're also just starting the internet",
                  "score": 7,
                  "created_utc": "2025-12-25 22:17:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwq5wc",
          "author": "mr_zerolith",
          "text": "They lost me when they lagged for months on supporting SEED OSS 36B just because they refused to update llama.cpp ( note: this it the smartest model that runs on a 5090 )  \nThat's when i switched sides to LM Studio.",
          "score": 24,
          "created_utc": "2025-12-25 19:05:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvy57e5",
              "author": "Aggressive-Bother470",
              "text": "It might be the smartest model that runs on quite a bit more than a 5090, too.\n\n\nDoesn't immediately seem to work in vllm 0.13, annoyingly.",
              "score": 2,
              "created_utc": "2025-12-26 00:23:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0c0ux",
                  "author": "DinoAmino",
                  "text": "Did you follow instructions?\n\nhttps://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct#vllm",
                  "score": 1,
                  "created_utc": "2025-12-26 11:18:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwnzx7",
          "author": "noiserr",
          "text": "I just use self compiled llamacpp. I have scripts I use to manage models. The benefit is all the options and tweaks are exposed and you can enable stuff you can only enable only at compile time. \n\nSometimes a model support isn't merged right away. I can just point to the development fork and compile that if I want to. No need to wait for support which can sometimes take weeks.",
          "score": 29,
          "created_utc": "2025-12-25 18:53:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwwubd",
              "author": "StardockEngineer",
              "text": "What do you enable?  I usually just compile vanilla.",
              "score": 5,
              "created_utc": "2025-12-25 19:45:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwzzjm",
                  "author": "noiserr",
                  "text": "Mainly ROCm / AMD related stuff:\n\n    -DGGML_HIP=ON \\\n    -DLLAMA_HIP_UMA=ON \\\n    -DGGML_HIP_ROCWMMA_FATTN=ON \\\n    -DGGML_HIP_GRAPHS=ON \\",
                  "score": 11,
                  "created_utc": "2025-12-25 20:04:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwm72v",
          "author": "ChipsAreClips",
          "text": "I stopped Ollama 7-8 months ago and switched to LM Studio, I love it",
          "score": 76,
          "created_utc": "2025-12-25 18:43:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwphi7",
              "author": "AnotherSoftEng",
              "text": "Have they built out VLM support at all in the last few months? I remember trying to use it as an API, but it was missing some very basic features. Same for tool calling support - was very hit or miss. Ollama, on the other hand, has a lot of this functionality out of the box and it just worked for me.\n\nOtherwise Iâ€™d be using LM Studio for everything. I especially appreciate how they separate GGUF and MLX model variants.",
              "score": 12,
              "created_utc": "2025-12-25 19:01:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwpzgm",
                  "author": "ChipsAreClips",
                  "text": "Yes, VLM support is currently great, and there are some great ComfyUI nodes that integrate for easy automation.",
                  "score": 9,
                  "created_utc": "2025-12-25 19:04:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw2h8ru",
                  "author": "mgoetzke76",
                  "text": "Is lm studio using vllm ? Or what is vlm ?",
                  "score": 1,
                  "created_utc": "2025-12-26 19:15:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvyydrm",
              "author": "International_Quail8",
              "text": "Also switched to LM Studio (not completely, but will soon).  Primarily for their MLX support which I don't believe Ollama has yet.  Loving it so far and it's been an easy transition with enough functionality in their UI to give me the control that I need over different models.",
              "score": 5,
              "created_utc": "2025-12-26 03:41:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvwqxnw",
              "author": "drakgremlin",
              "text": "Does LM Studio support an other applications running interference?",
              "score": 3,
              "created_utc": "2025-12-25 19:10:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwrj5z",
                  "author": "kiruz_",
                  "text": "You can load model and run server within app to connect through api, yes.",
                  "score": 13,
                  "created_utc": "2025-12-25 19:13:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvwxy2v",
              "author": "umataro",
              "text": "No matter the model, no matter the settings, in LM studio the models always end up endlessly repeating themselves or spewing nonsense forever. In Ollama, things just work. I gave up on LM studio but now with the entshittification of ollama, I need to try something new.",
              "score": 8,
              "created_utc": "2025-12-25 19:52:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw19nii",
                  "author": "KernelFlux",
                  "text": "I have found this to be true as well. I run the same\nModel on Ollama (non mlx), no problem, lm studio infinite repeats. I just donâ€™t get it. I am sticking with Ollama for now, pure local.",
                  "score": 2,
                  "created_utc": "2025-12-26 15:24:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvwuu2m",
              "author": "TechnoByte_",
              "text": "LM Studio is closed source, enjoy your downgrade",
              "score": -16,
              "created_utc": "2025-12-25 19:33:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwwxtu",
                  "author": "No-Mountain3817",
                  "text": "Are you a purist who only runs fully open-source, open-data, open-weight models? If youâ€™re using any closed source or partially closed models, then the argument that â€œLM Studio is closed and therefore shouldnâ€™t be usedâ€ completely falls apart. It may be closed source, but itâ€™s free to use, just like many open weight models people rely on every day.",
                  "score": 12,
                  "created_utc": "2025-12-25 19:46:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvwyqpv",
                  "author": "uber-linny",
                  "text": "I think it's great as a entry point for beginners. Do I use it anymore... No . But it's what I learnt on",
                  "score": 6,
                  "created_utc": "2025-12-25 19:57:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvww4u0",
                  "author": "stanm3n003",
                  "text": "Brah... who the fk cares.\nYou use the server API to build around it.",
                  "score": 0,
                  "created_utc": "2025-12-25 19:41:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvxgjr3",
          "author": "Mediocre_Second_2545",
          "text": "Check out [https://www.foundrylocal.ai/](https://www.foundrylocal.ai/) from Microsoft  - I work on it personally so I'm happy to answer any questions, and if you don't like it then I'm eagerly awaiting feedback :)",
          "score": 9,
          "created_utc": "2025-12-25 21:46:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0u7mx",
              "author": "johnerp",
              "text": "Oh coool. Thx.",
              "score": 1,
              "created_utc": "2025-12-26 13:48:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwmc43",
          "author": "No-Yak4416",
          "text": "What are you switching to?",
          "score": 16,
          "created_utc": "2025-12-25 18:43:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwqqj1",
              "author": "LostLakkris",
              "text": "I went llama-swap, little more complicated to manage, but I also averaged better performance to go with it",
              "score": 23,
              "created_utc": "2025-12-25 19:09:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwrz50",
                  "author": "PhilWheat",
                  "text": "Just in time for llama.cpp to add router capabilities?   (I'm still using llama-swap as well, but I want to see if I can simply things with the new capabilities.)",
                  "score": 15,
                  "created_utc": "2025-12-25 19:16:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvwtn9i",
                  "author": "milkipedia",
                  "text": "Loving my llama-swap setup.",
                  "score": 6,
                  "created_utc": "2025-12-25 19:26:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvwx18x",
              "author": "SoLoFaRaDi",
              "text": "Decided to switch to stock llama.cpp. Compiled it myself to leverage my CPU's Vulkan support, and it's seemingly going well. Love running SLM's with it :\\]",
              "score": 8,
              "created_utc": "2025-12-25 19:46:46",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw5001b",
              "author": "mcdenkijin",
              "text": "I am trying candle, tabbyapi, tabbyml, and vllm.  there's also jan and a few others",
              "score": 1,
              "created_utc": "2025-12-27 04:15:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwp7tx",
          "author": "RealLordMathis",
          "text": "If anyone's looking for an alternative for managing multiple models I've built an app with web ui for that. It supports llama.cpp, vllm and mlx_lm. I've also recently integrated llama.cpp router mode so you can take advantage of their native model switching. Feedback welcome!  \n\n[GitHub](https://github.com/lordmathis/llamactl)  \n[Docs](https://llamactl.org)",
          "score": 15,
          "created_utc": "2025-12-25 19:00:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwxknc",
              "author": "StardockEngineer",
              "text": "Looks interesting!",
              "score": 1,
              "created_utc": "2025-12-25 19:50:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvww9xh",
          "author": "Southern_Sun_2106",
          "text": "Ollama was cool because it started model switching first, I believe. But then LM studio cleaned up their interface, has model switching - it's nice to have a GUI.",
          "score": 12,
          "created_utc": "2025-12-25 19:42:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxk2pi",
              "author": "luche",
              "text": "i've yet to figure out how to get it to run headless without requiring user login on macOS... which has been the biggest issue so far. with ollama, i simply created a launchdaemon to do this and it works fine. i've yet to get lm-studio's headless server to run via launchdaemon.",
              "score": 1,
              "created_utc": "2025-12-25 22:08:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvx1cm3",
          "author": "emaiksiaime",
          "text": "I get best tok/sec with llama.cpp",
          "score": 7,
          "created_utc": "2025-12-25 20:12:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzfwtj",
          "author": "Mabuse046",
          "text": "Ollama isn't the one providing support for your models. Llama.cpp is. Ollama just packages up the models and launches llama.cpp for you. If you like ollama for being frequently updated to support new models, you're giving credit to the wrong people.",
          "score": 5,
          "created_utc": "2025-12-26 05:58:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwmepg",
          "author": "eat_my_ass_n_balls",
          "text": "LMStudio is better than Ollama",
          "score": 50,
          "created_utc": "2025-12-25 18:44:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwygck",
              "author": "Healthy-Nebula-3603",
              "text": "The best solution is llamacpp serveras it has a nice gui  or if you like console then llamacpp-cli",
              "score": 8,
              "created_utc": "2025-12-25 19:55:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvwuwbe",
              "author": "TechnoByte_",
              "text": "No it's not, it's closed source",
              "score": 2,
              "created_utc": "2025-12-25 19:33:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwwsrx",
                  "author": "stanm3n003",
                  "text": "\"nO iTs clOSeD sOurCe\"",
                  "score": 6,
                  "created_utc": "2025-12-25 19:45:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvwwwoh",
                  "author": "StardockEngineer",
                  "text": "But itâ€™s faster.",
                  "score": 3,
                  "created_utc": "2025-12-25 19:46:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwrn24",
          "author": "skyasher27",
          "text": "I use oogabooga to test models",
          "score": 13,
          "created_utc": "2025-12-25 19:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwvm4l",
              "author": "__SlimeQ__",
              "text": "this is the way",
              "score": 4,
              "created_utc": "2025-12-25 19:38:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwpxfc",
          "author": "Foreign-Beginning-49",
          "text": "I think they are a good entry for some beginners but they have done questionable things in the past. When you start using llama.cpp its a breath of fresh air once you undertake the learning process.",
          "score": 8,
          "created_utc": "2025-12-25 19:04:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvx642x",
          "author": "Turkino",
          "text": "I've been using KoboldCPP for a long time, after switching from Oogabooga.\nNow trying out llamacpp as well for GLM.\nI definitely see an appeal in both.",
          "score": 6,
          "created_utc": "2025-12-25 20:42:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxca6f",
          "author": "fallingdowndizzyvr",
          "text": "I never saw the use for it. I've always been llama.cpp pure and unwrapped.",
          "score": 3,
          "created_utc": "2025-12-25 21:20:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvws0i4",
          "author": "iotsov",
          "text": "I am staying with Ollama. It doesn't bother me that they have Cloud models, I simply don't use them, at least for the time being. They might become relevant some time in 2026 though, the way things are going :)",
          "score": 12,
          "created_utc": "2025-12-25 19:16:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwsyyz",
              "author": "SubstanceWooden7371",
              "text": "I think it's foolish not to utilize the cloud models in addition to hosting your own locally.",
              "score": -21,
              "created_utc": "2025-12-25 19:22:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxs1do",
                  "author": "Bonzupii",
                  "text": "I think it's foolish to think that people who are looking for freedom and privacy are foolish for not giving up their freedom and privacy.",
                  "score": 6,
                  "created_utc": "2025-12-25 22:58:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwoq19",
          "author": "Educational_Rent1059",
          "text": "LM studio much better yes",
          "score": 13,
          "created_utc": "2025-12-25 18:57:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwuz1b",
              "author": "TechnoByte_",
              "text": "It's not open source, so no",
              "score": -10,
              "created_utc": "2025-12-25 19:34:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwv7f2",
                  "author": "Educational_Rent1059",
                  "text": "Rather closed local than what Ollama turns into -> cloud paid EDIT: This TechnoByte_ spams the same comment across the thread, he is probably insider.",
                  "score": 7,
                  "created_utc": "2025-12-25 19:35:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvxdduk",
                  "author": "Billthegifter",
                  "text": "Can I ask. Why Is It being closed source a problem?",
                  "score": 3,
                  "created_utc": "2025-12-25 21:27:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwrztv",
          "author": "Mount_Gamer",
          "text": "They say they don't collect data, still provide many new models for offline use, and for me it's a good fit. I can use my local AI for something I truly want privacy, and I get a chance to query many bigger cloud models if I'm not happy with the response with the local models, or any model really.. I get a chance to view many angles of the same conversion.",
          "score": 5,
          "created_utc": "2025-12-25 19:16:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxpbfj",
              "author": "The_frozen_one",
              "text": "I think this sub is being astroturfed. Flame wars are cheaper than buying ads.",
              "score": 5,
              "created_utc": "2025-12-25 22:41:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwz1rq",
          "author": "vulcan4d",
          "text": "I agree that Ollama is going downhill but others are going too.  It has been a while since I used LMStudio so I tried it again and I can't even load models well.  It chews up memory with the same context and settings like it is candy and it just struggles so I moved back to Ollama.  Ollama does recently have bugs where it doesn't even output but thinks it is done but at least I can load models without struggling.",
          "score": 2,
          "created_utc": "2025-12-25 19:58:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxf5ne",
              "author": "nickless07",
              "text": "Weird, as for me it's the other way around. Even the Ollama installer is 3 times the size. Perhaps turn off 'Keep model in memory'? In LM Studio, hold the ALT key before loading a model and it shows you in realtime how much RAM and VRAM the model takes with the current settings.",
              "score": 7,
              "created_utc": "2025-12-25 21:38:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvxlwmg",
          "author": "Any_Fox5126",
          "text": "I'm glad to see some real criticism of ollama, instead of the endless pointless debates about whether, subjectively, they are giving enough recognition to llamacpp.",
          "score": 2,
          "created_utc": "2025-12-25 22:19:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxqv6z",
          "author": "taking_bullet",
          "text": "Thanks for sharing your experience. I tried LM Studio in the past, but switched back to Ollama. Ollama's GUI is very simple and clear, not filled with unnecessary options.Â ",
          "score": 2,
          "created_utc": "2025-12-25 22:50:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxt0yb",
          "author": "FBIFreezeNow",
          "text": "So what do you use, LM Studio?",
          "score": 2,
          "created_utc": "2025-12-25 23:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy65mg",
          "author": "hustla17",
          "text": "hahaha that title read like you quit some hard drug,\n\n well both are equally bad for the body /s\n\ngood job !",
          "score": 2,
          "created_utc": "2025-12-26 00:29:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvz69fr",
          "author": "Over_Description5978",
          "text": "When you use ollama with local models you can run it  free and unlimited !  In corporate word to make profit out of any business they will provide only one. Either free (limited) or unlimited (paid)",
          "score": 2,
          "created_utc": "2025-12-26 04:39:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1jh42",
          "author": "Fabix84",
          "text": "Yea... Ollama is dead.",
          "score": 2,
          "created_utc": "2025-12-26 16:18:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwv5kl",
          "author": "xandep",
          "text": "llama.cpp > LM Studio > Ollama",
          "score": 6,
          "created_utc": "2025-12-25 19:35:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwyhw0",
              "author": "random-tomato",
              "text": "llama-swap + llama.cpp > llama.cpp > LM Studio > Ollama\n\nFTFY :)",
              "score": 6,
              "created_utc": "2025-12-25 19:55:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxkdse",
                  "author": "luche",
                  "text": "i'll have to look into llama-swap. any chance there's a quick and easy way to get it to run on boot without requiring user login on macOS?",
                  "score": 1,
                  "created_utc": "2025-12-25 22:10:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvx5wqu",
          "author": "aiueka",
          "text": "I would be happy to switch away from ollama but llama.cpp does not have a native implementation of the feature which unloads the model from VRAM after x minutes of inactivity, is there?\nAre there any containerized services that have this and have better open source practices?",
          "score": 3,
          "created_utc": "2025-12-25 20:41:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxgca7",
              "author": "BlackMetalB8hoven",
              "text": "Llama cpp has this now with the switch  \n --sleep-idle-seconds",
              "score": 4,
              "created_utc": "2025-12-25 21:45:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxjp7s",
                  "author": "luche",
                  "text": "can this be implemented directly into open-webui or litellm?",
                  "score": 1,
                  "created_utc": "2025-12-25 22:05:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzk0s4",
                  "author": "AppearanceHeavy6724",
                  "text": "When it finally work, it would be nice if they add a feature to run a script before unloading model. This will fix the notorious 20W idle with 30xx cards, as now we'd be able to reset them right after unloading the model.",
                  "score": 0,
                  "created_utc": "2025-12-26 06:34:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw5a9r9",
              "author": "social_tech_10",
              "text": "llama-swap is a thin wrapper around llama.cpp that let's you change models on-th-fly using open-webui, or any other UI that supports the API.\n\nThe main reason I switched from ollama to llama.cpp and llama-swap is because llama-swap let's me easily set the exact command-line options for every model, including the TTL (time-to-live) feature you mentioned, which really \"lifts the fog\" surrounding ollama.",
              "score": 1,
              "created_utc": "2025-12-27 05:30:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvx7jio",
          "author": "neonexius",
          "text": "Try Lemonade https://github.com/lemonade-sdk/lemonade",
          "score": 2,
          "created_utc": "2025-12-25 20:51:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy0gjj",
          "author": "rm-rf-rm",
          "text": "Welcome to the other side! Thank the lord for llama.cpp and llama-swap! Hope they never go to the dark side",
          "score": 2,
          "created_utc": "2025-12-25 23:53:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyl8tv",
          "author": "StomachWonderful615",
          "text": "I switched from ollama to using mlx-lm on my macs.",
          "score": 2,
          "created_utc": "2025-12-26 02:10:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwqaqb",
          "author": "Objective_Frosting58",
          "text": "I started using ollama when I hadn't yet bought my gpu, I was running on my 7950x and 64gb ram. I didn't have any issues with it until I got my 9070xt and found that I couldn't get it to work. So switched to llama.cpp, its not as easy to use but works better",
          "score": 1,
          "created_utc": "2025-12-25 19:06:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwsldl",
          "author": "vir_db",
          "text": "Which alternative for an Openai-compatible API?",
          "score": 1,
          "created_utc": "2025-12-25 19:20:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvww6pw",
              "author": "Repulsive-Memory-298",
              "text": "Pretty much everything, that is the standard",
              "score": 3,
              "created_utc": "2025-12-25 19:41:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvx0fsm",
                  "author": "vir_db",
                  "text": "Very interesting ðŸ¤”. An example please?",
                  "score": 1,
                  "created_utc": "2025-12-25 20:07:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvx1cgh",
          "author": "FaceDeer",
          "text": "It's still working fine for me, so I keep on using it. Switching platforms is a hassle so I'm going to wait until the hassle of using it is greater than the hassle of switching.",
          "score": 1,
          "created_utc": "2025-12-25 20:12:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvx4dwq",
          "author": "productboy",
          "text": "Different tools for different tasks. I install Ollama for my team; for people who need an easy to use alternative to the frontier labs products. Meanwhile, I use many tools including Ollama, llama.cpp. As others noted; Ollama is a great entry point for beginners; which is useful to learn from their experience.",
          "score": 1,
          "created_utc": "2025-12-25 20:31:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvx9yqg",
          "author": "Cferra",
          "text": "A lot of the beginners guides (a beginner to this myself right now) right away point everyone to ollama or more recently lm studio.",
          "score": 1,
          "created_utc": "2025-12-25 21:06:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxlhjt",
          "author": "TheMcSebi",
          "text": "I've had the exact same thoughts about the cloud move.\n\nHasn't bothered me enough to switch, though. I'll propably still be using it for the sake of simplicity, so I don't need to change all my scripts that talk to an llm",
          "score": 1,
          "created_utc": "2025-12-25 22:16:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyi9w7",
          "author": "Direct_Turn_1484",
          "text": "Yeah the cloud update was not a great move for us, but probably great for them. I still use Ollama for some things, because for local stuff itâ€™s still an easy call to do inference with some model quickly, without having to load up the right container or adjust command line parameters. Just load the model and go real quick. For anything more involved, I agree Ollama isnâ€™t the go to app.",
          "score": 1,
          "created_utc": "2025-12-26 01:50:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyias0",
          "author": "Ok_Condition4242",
          "text": "https://preview.redd.it/1tlks5s1fg9g1.png?width=952&format=png&auto=webp&s=d8ffdd8f1e062a183c87412505ab733a086917ba\n\nWhen you compile llama.cpp for the first time.",
          "score": 1,
          "created_utc": "2025-12-26 01:50:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvynmul",
          "author": "rageling",
          "text": "it's the same with comfyui, they mostly advertise their cloud service integration now",
          "score": 1,
          "created_utc": "2025-12-26 02:26:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzs23q",
          "author": "mrdevlar",
          "text": "Someone really needs to make a migration guide from Ollama if they want people to use something else.",
          "score": 1,
          "created_utc": "2025-12-26 07:51:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0agi1",
          "author": "Jayden_Ha",
          "text": "They gotta make money somehow \n\nItâ€™s free what do you expect",
          "score": 1,
          "created_utc": "2025-12-26 11:02:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0j4ca",
          "author": "TechnicalGeologist99",
          "text": "Use vLLM and liteLLM. They obey the openAI API standard more strictly which means that it's easier to swap inference providers. They're also designed for stability and scale (which ollama is not)",
          "score": 1,
          "created_utc": "2025-12-26 12:24:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3jnv9",
          "author": "shroddy",
          "text": "ComfyUI users: First time?",
          "score": 1,
          "created_utc": "2025-12-26 22:45:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5dafz",
          "author": "Swaraj-Jakanoor",
          "text": "Just a thing we could still use local ollama modal and use our own cloud and it will not affect a thing right?",
          "score": 1,
          "created_utc": "2025-12-27 05:54:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5hg9d",
          "author": "ocirs",
          "text": "Not a fan of ollama either, for mac lm studio is a better integrated env. And vllm is much higher perf on linux. Ollama is a very thin wrapper around llama.cpp and feel they've been more focused on making money.",
          "score": 1,
          "created_utc": "2025-12-27 06:30:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6jzyu",
          "author": "GregoryfromtheHood",
          "text": "It was always a bad model runner. Gave an easy way for any old person to run a model with no knowledge, but used misleading naming for models and defaults to q4 for pretty much everything. In addition, sets less than ideal default params and obfuscates away the context length.",
          "score": 1,
          "created_utc": "2025-12-27 12:35:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7f4j5",
          "author": "Sea_Layer_6679",
          "text": "Ollama is shit to set up with AMD",
          "score": 1,
          "created_utc": "2025-12-27 15:50:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw831ba",
          "author": "Separate_Long_6962",
          "text": "they lost me when their download and install process was so fucking bad that I couldn't handle it anymore so I moved over to KoboldCPP and download the models manually from hugging face. Ollama sucks.",
          "score": 1,
          "created_utc": "2025-12-27 17:51:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9zybs",
          "author": "Some_Policy_6195",
          "text": "Yeah the cloud integration feels like such a weird pivot, like why would I want cloud models when the whole point was running stuff locally? I switched to llamafile a few weeks back and honestly haven't looked back - way simpler and does exactly what ollama used to do without all the extra stuff",
          "score": 1,
          "created_utc": "2025-12-28 00:00:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv1adc",
          "author": "Famous-Sprinkles-904",
          "text": "Llama used to be my default over Qwen. These days? Not even close.  \nFeels like Meta lost the secret sauce when the DeepMind talent walked.  \nGemini being good is the least surprising thing ever.",
          "score": 1,
          "created_utc": "2025-12-31 04:36:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy56pr",
          "author": "RegularPerson2020",
          "text": "They just added something new with cloud.  I haven't noticed a difference with the local models.  My latest favorite is running Nemotron locally.  It runs well on my mini PC!!!  Incredible.  \n\nOllama has been giving to the local ai community for free for many many years.  I think they've earned some consideration and understanding for that.  They are trying new things, that's better than just getting left behind.  \n\nI get it.  I'm big on my privacy, self hosting too.  I get it, I wish they were perfect and quick to fix stuff.  But I also have to remind myself that they gave me a lot for free and have been my go-to for years...for free.",
          "score": 1,
          "created_utc": "2025-12-26 00:23:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwtqgc",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2025-12-25 19:26:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvx3zit",
              "author": "Savantskie1",
              "text": "You probably know this but Iâ€™m betting petty too. In the windows app they included a gui chat app now and they have both cloud and local models in the model chooser with no delineation between which are local and which are the cloud models unless you strictly run it just in windows cli. There I treated you just as dumb as you treat others",
              "score": 2,
              "created_utc": "2025-12-25 20:29:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvxe9cv",
          "author": "_takasur",
          "text": "I quit it 5 minutes every time I install it hoping it would have become a better tool.",
          "score": 1,
          "created_utc": "2025-12-25 21:32:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxgs2x",
          "author": "bestofbestofgood",
          "text": "Ollama is just a llama.cpp wrapper. Since llama.cpp can run UI for serve as API - what's the point of ollama anymore",
          "score": 1,
          "created_utc": "2025-12-25 21:48:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxjcp7",
              "author": "luche",
              "text": "what do you recommend to set memory timers and handle calling different models upon request? can llama.cpp do this natively now?",
              "score": 1,
              "created_utc": "2025-12-25 22:03:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxn3wj",
                  "author": "bestofbestofgood",
                  "text": "Not as far as I know.",
                  "score": 1,
                  "created_utc": "2025-12-25 22:27:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyaz0k",
                  "author": "Eugr",
                  "text": "Llama.cpo has its own model router now. If you need something a bit more flexible that can also load other inference engines, you can use llama-swap.",
                  "score": 1,
                  "created_utc": "2025-12-26 00:59:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvxscug",
          "author": "IrisColt",
          "text": "I quit using ollama too, a week ago. Best decision ever... llama.cpp is a beast. Time to migrate my custom Python scripts, I guess.",
          "score": 1,
          "created_utc": "2025-12-25 23:00:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy2744",
          "author": "deltatux",
          "text": "I switched over to llama.cpp because it has Intel Arc support and it performs better than Ollama imo.",
          "score": 1,
          "created_utc": "2025-12-26 00:04:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyihcf",
          "author": "IaintJudgin",
          "text": "if you happen to use a mac [https://github.com/ggml-org/LlamaBarn](https://github.com/ggml-org/LlamaBarn) is cool (though it offers limited models -- only main/popular ones that your mac can run).\n\nEdit: agree llama.cpp is the proper way to go",
          "score": 1,
          "created_utc": "2025-12-26 01:51:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxm4qi",
          "author": "a_beautiful_rhind",
          "text": "Used many backends to run different models as required. Never did I have a need for one to be ollama.\n\nScreams \"I learned of LLMs from some influencer video\" in my eyes.",
          "score": 0,
          "created_utc": "2025-12-25 22:20:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy0uoh",
          "author": "rm-rf-rm",
          "text": "RIP Ollama \n2023-2025 \n\n(it was early this year their mask started coming loose)",
          "score": 0,
          "created_utc": "2025-12-25 23:55:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy45lu",
          "author": "g_rich",
          "text": "vLLM < llama.cpp < LM Studio < Ollama\n\nOllam is fine for getting started but itâ€™s limited and other than being turn key doesnâ€™t do anything better than LM Studio. \n\nLlama.cpp is the most versatile while vLLM is the most powerful but has limited system support when compared to llama.cpp so itâ€™s not an option for everyone. \n\nOverall my go to for getting started is LM Studio and for someone that wants to do a little more llama.cpp.",
          "score": 0,
          "created_utc": "2025-12-26 00:16:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyxxyx",
          "author": "the-final-frontiers",
          "text": "Â bait and switch",
          "score": 0,
          "created_utc": "2025-12-26 03:38:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxqta9",
          "author": "Dry_Yam_4597",
          "text": "If in the age of vibe coding you still use ollama and ollama web ui then it's a you problem.",
          "score": -4,
          "created_utc": "2025-12-25 22:50:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwmqiv",
          "author": "Ecstatic_Signal_1301",
          "text": "It just sounds like you lack a vram and blaming it on ollama.",
          "score": -32,
          "created_utc": "2025-12-25 18:46:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pz7bmv",
      "title": "Llama-3.3-8B-Instruct",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct",
      "author": "jacek2023",
      "created_utc": "2025-12-30 03:34:19",
      "score": 460,
      "num_comments": 78,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nwpj0kv",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 10:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo5hxn",
          "author": "FizzarolliAI",
          "text": "Hello, that me!\n\nI am currently working on running sanity check benchmarks to make sure it's actually a newer L3.3 and not just L3/L3.1 in a trenchcoat, but it's looking promising so far.\n\nFrom the current readme:\n\n| | Llama 3.1 8B Instruct | Llama 3.3 8B Instruct (maybe) |\n|-|-|-|\n|IFEval (1 epoch, score avged across all strict/loose instruction/prompt accuracies to follow Llama 3 paper)|78.2|81.95\n|GPQA Diamond (3 epochs)|29.3|37.0",
          "score": 123,
          "created_utc": "2025-12-30 03:46:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo6q38",
              "author": "jacek2023",
              "text": "great work, new llama release at the end of 2025 :)",
              "score": 49,
              "created_utc": "2025-12-30 03:53:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwppc12",
                  "author": "MoffKalast",
                  "text": "I definitely did not have this on my bingo card :D\n\nAnd leaked too, keeping up the llama tradition.",
                  "score": 29,
                  "created_utc": "2025-12-30 11:22:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwp95y3",
              "author": "Karyo_Ten",
              "text": "You can do a KL-divergence check to be 100% sure",
              "score": 15,
              "created_utc": "2025-12-30 08:53:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwozt4b",
              "author": "AnOnlineHandle",
              "text": "Heya I'm not up to date with these models since the llama 1 release, do you know if there's a good benchmark for visual tasks such as identifying poses, faces, hands, etc, or answering questions about images, which I could compare models on? I've tried to use Qwen 3 Instruct for it but found it wasn't as good on real data as the demos suggested.",
              "score": 3,
              "created_utc": "2025-12-30 07:27:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwohwlc",
          "author": "dinerburgeryum",
          "text": "8K max position embeddings? Seems remarkably low; did the fine tune artifact for some reason artificially limit that?",
          "score": 46,
          "created_utc": "2025-12-30 05:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoiq8k",
              "author": "Arli_AI",
              "text": "Maybe we can just set 32768 and itâ€™ll be okay lol",
              "score": 20,
              "created_utc": "2025-12-30 05:10:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwoo96q",
                  "author": "Few-Welcome3297",
                  "text": "Checking differences from LLaMA 3.1 8B Instruct, I think we can add the rope\\_scaling\n\n|\"rope\\_scaling\": {|\n|:-|\n|\"factor\": 8.0,|\n|\"high\\_freq\\_factor\": 4.0,|\n|\"low\\_freq\\_factor\": 1.0,|\n|\"original\\_max\\_position\\_embeddings\": 8192,|\n|\"rope\\_type\": \"llama3\"|\n|},|\n\nand then increase \\`max\\_position\\_embeddings\\`\n\nEdit: Also prev version had 3 eos\\_token\\_id's\n\nEdit2: [https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K](https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K) model with above changes\n\nEdit3: Link updated",
                  "score": 25,
                  "created_utc": "2025-12-30 05:50:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nworq4h",
                  "author": "Klutzy-Snow8016",
                  "text": "Llama 3 8B had 8192 context. Then Llama 3.1 added RoPE to get to 131072 context. Maybe we can take the RoPE scaling parameters from llama 3.1's config.json and add it to llama 3.3 8B.",
                  "score": 11,
                  "created_utc": "2025-12-30 06:17:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoi4rx",
              "author": "FizzarolliAI",
              "text": "Yes. I'm not entirely sure why, it was limited when served via the website too (I put that in the readme a bit ago)",
              "score": 3,
              "created_utc": "2025-12-30 05:06:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwolsch",
          "author": "Amazing_Athlete_2265",
          "text": "Running this across my private evals to compare against other llamas. Will take a couple hours.",
          "score": 20,
          "created_utc": "2025-12-30 05:32:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwou0rl",
              "author": "Amazing_Athlete_2265",
              "text": "Initial speed test:\n\n| Model | Backend | PP ts^-1| TG ts^-1 |\n| -------------------------------------------------- | ---------- | ---------- | ------------------ |\n| allura-forge_Llama-3.3-8B-Instruct Q4 | CUDA | 1566.5 | 100.8 |\n| Llama-3.1-8B-Instruct Q4 | CUDA | 351.1 | 111.9 |\n\nSo some difference there.\n\nWill post more eval results as they come to hand.",
              "score": 22,
              "created_utc": "2025-12-30 06:36:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwp3lxt",
              "author": "Amazing_Athlete_2265",
              "text": "From these results, it looks like the new model is different than the old 3.1.\n\nHere is the performance for knowledge testing, with the new 3.3-8B-Instruct highlighted in the first two plots \n\n- [First plot is the 4-9B parameter group](https://imgur.com/YuSmDRn)\n\n- [Second plot is the same but for 8B+ parameter group](https://imgur.com/Q0nnLwn)\n\n- [Third plot is performance by knowledge category for the 3.3 model](https://imgur.com/kjkNbR3)\n\n- [Fourth plot is performance by knowledge category for the older 3.1 model](https://imgur.com/vjy6cjW)\n\n- [Last plot is a speed chart on my 3080](https://imgur.com/coiBc9H)\n\nTesting the Q6 versions now. Will take a while. All of the tests above are for Q4.",
              "score": 20,
              "created_utc": "2025-12-30 08:01:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpij0j",
                  "author": "keepthepace",
                  "text": "(Thanks for doing this!) \n\nI guess this explains why they did not brag much about it. Many other models of that category outperform them.\n\nI always wondered if Zuckerberg was not the only honest player in the field when he was explaining that the only reason they go for open source is that it will save them money. With decent open models out there they have less incentives to do so.",
                  "score": 11,
                  "created_utc": "2025-12-30 10:20:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwpmayy",
                  "author": "MLDataScientist",
                  "text": "Thanks for the tests. Question not related to llama: is LFM2 8BA1B that good in world knowledge (or coding/stem field)? I see it reaches Qwen3 30B-A3B.",
                  "score": 3,
                  "created_utc": "2025-12-30 10:55:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwp43t0",
                  "author": "jacek2023",
                  "text": "You can post pictures in the comments here",
                  "score": 2,
                  "created_utc": "2025-12-30 08:06:26",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nwp9v8w",
                  "author": "RobotRobotWhatDoUSee",
                  "text": "Random question: any idea why nemotron 30B A3B got 0% in the second plot?",
                  "score": 2,
                  "created_utc": "2025-12-30 09:00:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoo8ce",
              "author": "jacek2023",
              "text": "do you have results for other new models?",
              "score": 3,
              "created_utc": "2025-12-30 05:50:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwopj5o",
                  "author": "Amazing_Athlete_2265",
                  "text": "I have some. I focus mostly on smaller models <12B or Moe. What you want?",
                  "score": 7,
                  "created_utc": "2025-12-30 06:00:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpff8o",
          "author": "a_beautiful_rhind",
          "text": "This is like the kiss goodbye from meta.",
          "score": 20,
          "created_utc": "2025-12-30 09:52:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpzjvy",
              "author": "samplebitch",
              "text": "It's like that time when you hook up with your ex one last time, and it wasn't even that great.",
              "score": 20,
              "created_utc": "2025-12-30 12:43:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvktxy",
                  "author": "impolitemrtaz",
                  "text": "You samplebitch you",
                  "score": 2,
                  "created_utc": "2025-12-31 07:05:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwzk8xs",
                  "author": "Electronic-Metal2391",
                  "text": "You bring bad memories",
                  "score": 1,
                  "created_utc": "2025-12-31 22:13:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwognr9",
          "author": "random-tomato",
          "text": "Holy shit that is awesome, hats off to you for finding the weights!",
          "score": 31,
          "created_utc": "2025-12-30 04:56:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv4x7d",
              "author": "seppe0815",
              "text": "stupid bots",
              "score": -7,
              "created_utc": "2025-12-31 05:01:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvbbuw",
                  "author": "random-tomato",
                  "text": "If I'm a bot, I'm certainly programmed to like and appreciate when people find something cool and share with the rest of us. What's your purpose being a professional asshole?\n\nAnd no, I am not a bot\n\nhttps://preview.redd.it/eljpgxgfbhag1.png?width=765&format=png&auto=webp&s=90f0de59d2389d809ac21d988ca59883283ffccf",
                  "score": 4,
                  "created_utc": "2025-12-31 05:48:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwplkfs",
          "author": "jacek2023",
          "text": "about 4h after the release u/TheLocalDrummer published first finetune:\n\n[https://huggingface.co/BeaverAI/Anubis-Mini-8B-v1f-GGUF/tree/main](https://huggingface.co/BeaverAI/Anubis-Mini-8B-v1f-GGUF/tree/main)",
          "score": 18,
          "created_utc": "2025-12-30 10:48:37",
          "is_submitter": true,
          "replies": [
            {
              "id": "nwplnk1",
              "author": "TheLocalDrummer",
              "text": "It's a test model but I think it turned out well! Looking for feedback in (my) Discord",
              "score": 12,
              "created_utc": "2025-12-30 10:49:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwrnu7f",
                  "author": "DevelopmentBorn3978",
                  "text": "what the finetune you've made is about?",
                  "score": 3,
                  "created_utc": "2025-12-30 17:56:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwswk38",
                  "author": "LegacyRemaster",
                  "text": "legend",
                  "score": 2,
                  "created_utc": "2025-12-30 21:27:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpr0ce",
              "author": "MoffKalast",
              "text": "People are asking what's the use case for llama, and well uh... there it is ;)",
              "score": 5,
              "created_utc": "2025-12-30 11:36:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws73yi",
                  "author": "Emotional-Baker-490",
                  "text": "qwen 3",
                  "score": 2,
                  "created_utc": "2025-12-30 19:25:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpqsiz",
          "author": "jacek2023",
          "text": "[https://huggingface.co/aeon37/Llama-3.3-8B-Instruct-heretic](https://huggingface.co/aeon37/Llama-3.3-8B-Instruct-heretic)",
          "score": 9,
          "created_utc": "2025-12-30 11:34:48",
          "is_submitter": true,
          "replies": [
            {
              "id": "nwpstxk",
              "author": "Amazing_Athlete_2265",
              "text": "Everyone's cooking tonight!",
              "score": 8,
              "created_utc": "2025-12-30 11:51:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwptlto",
                  "author": "jacek2023",
                  "text": "actually it's a middle of the day in Europe :)",
                  "score": 7,
                  "created_utc": "2025-12-30 11:57:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwodozb",
          "author": "Echo9Zulu-",
          "text": "Cloned",
          "score": 7,
          "created_utc": "2025-12-30 04:36:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9jjl",
          "author": "Infninfn",
          "text": "Iâ€™m out of the loop - is this just what they had or did Meta not shutdown Llama?",
          "score": 18,
          "created_utc": "2025-12-30 04:10:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoa6pk",
              "author": "FizzarolliAI",
              "text": "This has existed at least since April during Llamacon (did anyone remember they did a Llamacon?)\n\nhttps://ai.meta.com/blog/llamacon-llama-news/\n\n> As part of this release, weâ€™re sharing tools for fine-tuning and evaluation in our new API, where you can tune your own custom versions of our new Llama 3.3 8B model. Weâ€™re sharing this capability to help you reduce costs while also working toward increased speed and accuracy. You can generate data, train on it, and then use our evaluations suite to easily test the quality of your new model.",
              "score": 32,
              "created_utc": "2025-12-30 04:14:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwoeicf",
              "author": "jacek2023",
              "text": "we do things for fun in this community, just accept the gift ;)",
              "score": 7,
              "created_utc": "2025-12-30 04:41:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx0u205",
          "author": "Dangerous_Fix_5526",
          "text": "Thinking/Instruct Hybrid using Unsloth and Claude-Opus 4.6 dataset:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning)\n\nI hope I credited everyone correctly.",
          "score": 5,
          "created_utc": "2026-01-01 02:59:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0u9ri",
              "author": "jacek2023",
              "text": "Nice work!!!",
              "score": 1,
              "created_utc": "2026-01-01 03:00:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwodvfx",
          "author": "Cool-Chemical-5629",
          "text": "I guess Christmas came late for me, but hey if this is the real thing from Meta, I guess it's nice to have something newer than 3.1 8B without needing expensive hardware for models like Llama 4.",
          "score": 8,
          "created_utc": "2025-12-30 04:37:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp5cbv",
              "author": "Emotional-Baker-490",
              "text": "qwen 3",
              "score": 13,
              "created_utc": "2025-12-30 08:17:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtj858",
          "author": "LegacyRemaster",
          "text": "    allura-forge_llama-3.3-8b-instruct\n\nMy training data is current up to December 2022. This means that I have been trained on a vast amount of text data available until that date, but I do not have information or knowledge about events or developments that have occurred after that date.\n\nIn other words, my training data \"cutoff\" is December 2022, and I should not be relied upon for information or insights related to dates after that.\n\n145.25 tok/sec",
          "score": 3,
          "created_utc": "2025-12-30 23:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwroudn",
          "author": "DevelopmentBorn3978",
          "text": "which quantized and eventually finetuned gguf models have the context lenght been enlarged? bartowsky? shb777? beaverai/anubis?",
          "score": 1,
          "created_utc": "2025-12-30 18:01:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwt7fp",
              "author": "Few-Welcome3297",
              "text": "Try [https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K-GGUF](https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K-GGUF)",
              "score": 0,
              "created_utc": "2025-12-31 13:31:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsx4sf",
          "author": "gta721",
          "text": "How dumb are they to push a portal THAT broken to prod?",
          "score": 1,
          "created_utc": "2025-12-30 21:30:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvb2oz",
              "author": "greggh",
              "text": "Nothing about it is prod. Itâ€™s still so janky that its free if your in the trial.",
              "score": 5,
              "created_utc": "2025-12-31 05:46:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvhco0",
                  "author": "FizzarolliAI",
                  "text": "Yep, this basically. Afaik the main inference API is still waitlisted, *and* there's a separate waitlist to submit for the finetuning API.",
                  "score": 2,
                  "created_utc": "2025-12-31 06:36:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww109f",
          "author": "FX2021",
          "text": "Is it a new core? Or is it just a serving variant",
          "score": 1,
          "created_utc": "2025-12-31 09:37:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9zlt",
          "author": "Intelligent-Form6624",
          "text": "â€œ(I think, anyways)â€",
          "score": -21,
          "created_utc": "2025-12-30 04:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoadni",
              "author": "FizzarolliAI",
              "text": "LISTEN whenever i drop *my own* models i get anxiety attacks about accidentally reuploading the base model ;-; i believe that this is actually L3.3 at this point though, see my other comment",
              "score": 28,
              "created_utc": "2025-12-30 04:15:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwow842",
                  "author": "Intelligent-Form6624",
                  "text": "What? Sorry, I canâ€™t hear you",
                  "score": -20,
                  "created_utc": "2025-12-30 06:55:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo9irb",
          "author": "secopsml",
          "text": "Drop behemoth instead. Looks fakeÂ ",
          "score": -37,
          "created_utc": "2025-12-30 04:10:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo9lrg",
              "author": "secopsml",
              "text": "ðŸ˜œ",
              "score": -26,
              "created_utc": "2025-12-30 04:11:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pweljh",
      "title": "NVIDIA has 72GB VRAM version now",
      "subreddit": "LocalLLaMA",
      "url": "https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-5000/",
      "author": "decentralize999",
      "created_utc": "2025-12-26 20:48:17",
      "score": 455,
      "num_comments": 148,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/",
      "domain": "nvidia.com",
      "is_self": false,
      "comments": [
        {
          "id": "nw4smr8",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-27 03:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw36f8z",
          "author": "slavik-dev",
          "text": "checking bhphotovideo prices:\n\n\\- RTX 5000 48GB - $5100 (14,080 CUDA Cores, 384-bit memory)\n\n\\- RTX 5000 72GB - $7800 (14,080 CUDA Cores, 512-bit memory)\n\n\\- RTX 6000 96GB - $8300 (24,064 CUDA Cores, 512-bit memory)\n\nRTX 5000 72GB doesn't appear to be good deal...",
          "score": 245,
          "created_utc": "2025-12-26 21:32:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3j84w",
              "author": "__JockY__",
              "text": "Yuck, itâ€™s the worst deal of the bunch.",
              "score": 87,
              "created_utc": "2025-12-26 22:43:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw59iot",
                  "author": "Maleficent-Ad5999",
                  "text": "Decoy effect in action",
                  "score": 34,
                  "created_utc": "2025-12-27 05:24:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw41wuo",
              "author": "BobbyL2k",
              "text": "RTX Pro 5000 with 72GB has the same 384-bit memory bus, not 512-bit. Itâ€™s the same GPU as the 48GB version, with the upgrade to 3GB GDDR7 modules from 2GB.",
              "score": 25,
              "created_utc": "2025-12-27 00:34:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3kmxg",
              "author": "ThenExtension9196",
              "text": "Hey donâ€™t forgot the rtx 4000 pro! 24G $1499 (~8k cuda cores). Just picked one up for my surveillance camera server to run inference on snapshots after motion is detected.",
              "score": 22,
              "created_utc": "2025-12-26 22:51:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw438oq",
                  "author": "lannistersstark",
                  "text": ">  Just picked one up for my surveillance camera server to run inference on snapshots after motion is detected.\n\nSurely you can run frigate on much, much cheaper hardware?",
                  "score": 20,
                  "created_utc": "2025-12-27 00:42:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw44f33",
                  "author": "nuusain",
                  "text": "Neat! What kinda inference u running on the feed? Just installed a security system for a relatives farm. I was thinking of producing reports /audits so im curious what stuff others are building for themselves.",
                  "score": 7,
                  "created_utc": "2025-12-27 00:49:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw5kon7",
                  "author": "claythearc",
                  "text": "Is that needed? Weâ€™re running RT DETR for some real time detection stuff at work and hit 60 fps on an integrated laptop gpu. \n\nResolution will change it some, but surely not that much?",
                  "score": 4,
                  "created_utc": "2025-12-27 06:59:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw5cbu7",
                  "author": "robertpro01",
                  "text": "What exactly are your doing with it? I'm interested!",
                  "score": 3,
                  "created_utc": "2025-12-27 05:46:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw6352w",
              "author": "Free-Internet1981",
              "text": "Yeah no thanks, it should be a 4k card",
              "score": 3,
              "created_utc": "2025-12-27 09:57:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw6ttb7",
              "author": "gweilojoe",
              "text": "$500 difference - thatâ€™s it? What a terrible deal.",
              "score": 3,
              "created_utc": "2025-12-27 13:45:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3caiz",
              "author": "SilentLennie",
              "text": "Let me guess, they are releasing something, because they can't add a new line up ?",
              "score": 4,
              "created_utc": "2025-12-26 22:04:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3zn3p",
                  "author": "PentagonUnpadded",
                  "text": "Moving an Ada/Blackwell-class GPU from TSMC 4N (current) to a next gen like N3E likely would give ~6â€“9% perf gain at iso power, assuming no other advancements. Given the yields Apple has had (poor) with those next generation nodes, it ought to cost quite a bit more vs 4N. \n\nEveryone wants a cheaper version of the existing high-ram products. A 6090 that's 10% faster than 5090 is not compelling for home ai use if it costs 15% more. Ram is the bottleneck, evidenced by how beloved 3090s are. The only customers who would pay an exorbitantly higher up front cost for such a new node are datacenters concerned with cooling / power draw concerns that make it profitable after years of always on operation.",
                  "score": 2,
                  "created_utc": "2025-12-27 00:21:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw8ljs3",
              "author": "WitAndWonder",
              "text": "Right? Like.. \"just\" buy a second 5090. Cheaper even at the inflated prices and would up inference / training significantly as opposed to these shitty cards. Higher demands from Mobo / PSU, but let's be real, the 2k$ saved can easily cover that and  more. Nvidia has its head so far up its ass. I would love to see them get their just desserts for treating their customer base like disposable socks.",
              "score": 2,
              "created_utc": "2025-12-27 19:25:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwi7j5p",
              "author": "Lazy-Pattern-5171",
              "text": "I think they did this because their 96GB card has been dropping in price for quite some time.",
              "score": 1,
              "created_utc": "2025-12-29 06:48:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx1k9qb",
              "author": "Less_Consequence_633",
              "text": "300 watts, too, so, yeah that pricing doesn't make a lick of sense outside of the below-mentioned decoy effect.",
              "score": 1,
              "created_utc": "2026-01-01 06:20:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw8wjig",
              "author": "AmazinglyObliviouse",
              "text": "Jesus, I was telling people they were too optimistic about the prices of the 72gb version a few months ago when it leaked, but my own estimate at the time was 6k usd.",
              "score": 1,
              "created_utc": "2025-12-27 20:23:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw794lp",
              "author": "chibop1",
              "text": "If you can tolerate slow prompt processing:\n\nM3Ultra 512GB - $9,899 (Comes with CPU, Motherboard, PSU, 2TB SSD, WiFI, Bluetooth)",
              "score": 0,
              "created_utc": "2025-12-27 15:18:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2youu",
          "author": "ArtisticHamster",
          "text": "I think they need to produce 128Gb or even larger version, not 72Gb one.",
          "score": 269,
          "created_utc": "2025-12-26 20:50:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw31ndc",
              "author": "StaysAwakeAllWeek",
              "text": "If it was that easy they would. But it's not.\n\nGetting to 96GB already requires using the largest VRAM chips on the market, attached two chips per bus (which is the maximum) to the largest GDDR bus ever fitted to a GPU.\n\nThey would need a 640 bit wide bus to reach 120GB",
              "score": 112,
              "created_utc": "2025-12-26 21:06:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw335cm",
                  "author": "ArtisticHamster",
                  "text": "It's not easy, but it's not impossible. They put much more RAM on the datacenter GPUs.\n\nUPD. According to /u/StaysAwakeAllWeek it seems that GB200 is two chips with 96Gb each combined into one thing. This explains everything.",
                  "score": 56,
                  "created_utc": "2025-12-26 21:14:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3bd5t",
                  "author": "SRSchiavone",
                  "text": "Didnâ€™t the Titan V CEO edition use HBM2 for a 4096-bit wide bus? \n\nPlus, doesnâ€™t the H200 already have 141gB with only one package?",
                  "score": 6,
                  "created_utc": "2025-12-26 21:59:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw43x8y",
                  "author": "Massive-Question-550",
                  "text": "Isn't that more about bandwidth than capacity? For example a 5060ti has a 128 bit bus VS 256 for a 5070ti yet they both have the same memory capacity.Â ",
                  "score": 2,
                  "created_utc": "2025-12-27 00:46:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3qy6p",
                  "author": "shivdbz",
                  "text": "Just increase bus bandwidth, they only have to increase pcb trace complexity and sell it for low prices so buy go home happy.",
                  "score": 2,
                  "created_utc": "2025-12-26 23:28:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3dixv",
                  "author": "IAmFitzRoy",
                  "text": "Memory capacity is defined by price strategyâ€¦ not because itâ€™s easy to make or not. \n\nCheck any brand and you will see the same pattern, itâ€™s not only Apple or NVIDIA doing it.. Samsung, Google, Dell â€¦ all of them.",
                  "score": 0,
                  "created_utc": "2025-12-26 22:11:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw33f7f",
              "author": "sassydodo",
              "text": "yes, considering chip prices, let's ask for 512gb version, since I can't have it anyways, why not abstain from even larger vram",
              "score": 18,
              "created_utc": "2025-12-26 21:16:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw38tz5",
                  "author": "profcuck",
                  "text": "Terabyte or bust!",
                  "score": 9,
                  "created_utc": "2025-12-26 21:45:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw4affg",
                  "author": "rog-uk",
                  "text": "Merry Xmas, Tiny Tim :-)",
                  "score": 2,
                  "created_utc": "2025-12-27 01:27:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw2zlq4",
              "author": "TheLexoPlexx",
              "text": "You can just buy two /s",
              "score": 9,
              "created_utc": "2025-12-26 20:55:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw331w7",
                  "author": "ac101m",
                  "text": "The more you buy, the more you save!",
                  "score": 19,
                  "created_utc": "2025-12-26 21:14:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw4iisu",
              "author": "DAlmighty",
              "text": "I wouldnâ€™t be able to afford a card with 128GB of VRAM, but Iâ€™d sure as shit try to.",
              "score": 3,
              "created_utc": "2025-12-27 02:19:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3lxcf",
              "author": "AbheekG",
              "text": "Hopefully Rubin takes us to 128GB per GPU, and continues with the 300W Max-Q variants. That would allow for 512GB VRAM with just 4xGPUs at 1200W ðŸ¤¤",
              "score": 2,
              "created_utc": "2025-12-26 22:58:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3a5et",
              "author": "Technical_Ad_440",
              "text": "that would be great and all but the 96gb one is 8k the issue this has its over specked for 40gb models under specked for 80gb models. i assume this would cause more 60gb models though and could be entry under the rtx 6000 96gb something we may be able to see ourselves since it should be around 6k hopefully 5k. i just want more affordable for us guys at home",
              "score": -2,
              "created_utc": "2025-12-26 21:52:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw346f1",
          "author": "StableLlama",
          "text": "Wake me up when the 5090 has 48 GB",
          "score": 52,
          "created_utc": "2025-12-26 21:20:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw376vm",
              "author": "El-Dixon",
              "text": "R.I.P",
              "score": 32,
              "created_utc": "2025-12-26 21:36:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3hzk8",
                  "author": "StableLlama",
                  "text": "Some Chinese will upgrade it to 64 GB or even 128 GB, so it's not presumptuous to ask for 48 :)",
                  "score": 18,
                  "created_utc": "2025-12-26 22:36:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw2z4fy",
          "author": "emprahsFury",
          "text": "The price per gig is the same. There's no added or lost value, which makes the choice easy. Buy the most you can afford",
          "score": 47,
          "created_utc": "2025-12-26 20:52:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw34zsp",
              "author": "HushHushShush",
              "text": "The more you buy, the more you are anchored to a particular generation.",
              "score": 21,
              "created_utc": "2025-12-26 21:24:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5dwhn",
                  "author": "mcnbc12",
                  "text": "[The more you buy, the more you save](https://m.youtube.com/shorts/HRQhkjsBOXM)",
                  "score": 2,
                  "created_utc": "2025-12-27 06:00:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw35fdi",
          "author": "Prudent-Corgi3793",
          "text": "Any reason to get this over the RTX 6000 Pro 96 GB?",
          "score": 14,
          "created_utc": "2025-12-26 21:27:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3mq3k",
              "author": "HumanDrone8721",
              "text": "Nope, the price difference is marginal, is not 25% cheaper for 25% less VRAM. I've almost did a double take then I've looked for them and saw something like 4K EUR, until I've realized that is the variant with 48GB and the proper SKU for 72GB is VCNRTXPRO5000B72-PB and that costs practically the same as the 96GB variant.",
              "score": 11,
              "created_utc": "2025-12-26 23:03:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3pirr",
                  "author": "Evening_Ad6637",
                  "text": "And the bandwidth is also 25% slower (1.3 TB/s vs 1.8 TB/s)",
                  "score": 3,
                  "created_utc": "2025-12-26 23:20:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw44mfe",
          "author": "NikoKun",
          "text": "I wonder.. If in a few years, we'll see a game console with these levels of VRAM, for running AI world-models that let you experience endless gaming worlds.",
          "score": 6,
          "created_utc": "2025-12-27 00:50:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw34il6",
          "author": "ImportancePitiful795",
          "text": "This product makes no sense. In most countries is just â‚¬1000 from the 96GB one.",
          "score": 18,
          "created_utc": "2025-12-26 21:22:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw44f54",
          "author": "Massive-Question-550",
          "text": "Realistically even 96gb isn't enough for the price. What people want is an \"affordable\" gpu with a lot of vram. Something with 5080 speed but 96 gb for like $3-4k would be reasonable.Â ",
          "score": 7,
          "created_utc": "2025-12-27 00:49:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4g83u",
              "author": "munkiemagik",
              "text": "In that price range even I would bite your hand off to buy something like that and I'm not even an IT professional who uses them for anything productive, I just find it all interesting and mess around in my spare time. But I'm not going to hold my breath, that capability is not going to hit that price range for several more years.",
              "score": 4,
              "created_utc": "2025-12-27 02:04:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2z4fl",
          "author": "Herr_Drosselmeyer",
          "text": "I think that's partially true. 48 just doesn't cut it these days, but they also don't want to directly compete against the 6000 PRO, so 72 is a compromise.",
          "score": 6,
          "created_utc": "2025-12-26 20:52:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3ivlk",
          "author": "__JockY__",
          "text": "72GB is such a weird number. 128GB? Sure. 192GB? Bring it. 256GB? You get the idea.\n\nBut 72GBâ€¦ I just donâ€™t get it. Who is this marketed at?",
          "score": 5,
          "created_utc": "2025-12-26 22:41:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw480he",
              "author": "BobbyL2k",
              "text": "The numbers are dictated by the memory configuration.\n\n- 5090 and Pro 6000 have 512-bit bus\n- 3090, 4090, and Pro 5000 has 384-bit bus\n- 5070 Ti and 5080 have 256-bit bus\n\nEach 32-bit of memory bus can either connect to 1 or 2 memory modules. There are two GDDR7 modules: 2GB and 3GB. There are two GDDR6X modules: 1GB and 2GB.\n\n- 512-bit can fit 16 or 32 modules \n  - 5090 with 2GBx16=32GB\n  - Pro 6000 with 3GBx32=96GB\n\n- 384-bit can fit 12 or 24 modules\n  - Pro 5000 with 2GBx24=48GB or 3GBx24=72GB\n  - 4090 with 2GBx12=24GB (GDDR6X)\n  - 3090 with 1GBx24=24GB (GDDR6X)\n\n- 256-bit can fit 8 or 16 modules\n  - 5080 with 2GBx8=16GB\n  - 5070 Ti with 2GBx8=16GB",
              "score": 19,
              "created_utc": "2025-12-27 01:11:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4yxhk",
                  "author": "__JockY__",
                  "text": "Thanks for the technical explanation!\n\nStill doesnâ€™t change the fact that the 72GB model is a terrible deal!",
                  "score": 4,
                  "created_utc": "2025-12-27 04:07:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3v7j6",
              "author": "LightShadow",
              "text": "The people that need 120gb models on two cards.",
              "score": 3,
              "created_utc": "2025-12-26 23:54:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw47idj",
                  "author": "__JockY__",
                  "text": "Ok, but for another $500 the 96GB is available and Iâ€™d argue the most people spending $7800 on a 72GB card have both an extra $500 and a good use for that extra VRAM! 72GB at that price is a terrible deal. $6k? Ok, I could see itâ€¦ but at $500 less than a 96GB it just seems silly.",
                  "score": 3,
                  "created_utc": "2025-12-27 01:08:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3ts27",
          "author": "Rollingsound514",
          "text": "They throw these into Dell Workstations, best bet is to wait a bit and get refurb dell work station part outs from resellers",
          "score": 2,
          "created_utc": "2025-12-26 23:46:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5qna1",
          "author": "monoidconcat",
          "text": "The price doesnâ€™t seem attractiveâ€¦",
          "score": 2,
          "created_utc": "2025-12-27 07:55:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5uwo3",
          "author": "ab032tx",
          "text": "waiting for the day I can run deepseek 3.2 locally on my iphone",
          "score": 2,
          "created_utc": "2025-12-27 08:36:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7x2qi",
          "author": "No_Comment_Acc",
          "text": "I have 4890 (48 Gb 4090) but still want RTX 6000 Pro. Not gonna happen until bank robbers forget their bag with money in my car (I don't have a car).",
          "score": 2,
          "created_utc": "2025-12-27 17:21:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3owcr",
          "author": "No_Damage_8420",
          "text": "Definite BUY for AI Toolkit Wan 2.1 LORA training",
          "score": 2,
          "created_utc": "2025-12-26 23:16:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw69ice",
          "author": "deep_chungus",
          "text": "buh, god damn i hope the ai bubble pops hard, this is like the crypto bubble only every single tech company wants it to succeed\n\nthen again in ten years they'll figure out \"you need a bunch of video card hardware to make clone organs\" or something and we'll be playing half life on abacuses",
          "score": 2,
          "created_utc": "2025-12-27 11:00:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw38665",
          "author": "Rockclimber88",
          "text": "Where's 512GB GPU? Apple Mac Studio comes with up to 512GB and Nvidia disappoints with this overpriced lame shit.",
          "score": -1,
          "created_utc": "2025-12-26 21:42:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwebl3f",
              "author": "MoMoneyMoStudy",
              "text": "Apple really only useful for inference.  But try out some finetuning of large OSS models.",
              "score": 2,
              "created_utc": "2025-12-28 17:52:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3bf7b",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -8,
              "created_utc": "2025-12-26 21:59:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3vnax",
                  "author": "Rockclimber88",
                  "text": "What are you even talking about? It's RAM available to the GPU",
                  "score": 2,
                  "created_utc": "2025-12-26 23:57:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3535j",
          "author": "nofilmincamera",
          "text": "I talked to a nvidia partner about this,  as I was curious the business pricing for 1. I won't share the price, but the 48GB almost makes sense. These could have some niche uses, price is on relatively.  But it has lower Cuda Cores than the 5090.  Everything I would want a 48gb i could makecwork on 32, with Cores mastering more that 16 gb difference.\n\n78gb is just stupid, like 600 difference.",
          "score": -1,
          "created_utc": "2025-12-26 21:25:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3q3fk",
          "author": "Buff_Grad",
          "text": "How does Apple manage to pull off the insane integrated RAM into their silicon with such good stats?",
          "score": 0,
          "created_utc": "2025-12-26 23:23:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4mie5",
              "author": "davidy22",
              "text": "Because it's the RAM that they're using.",
              "score": 6,
              "created_utc": "2025-12-27 02:44:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw4j7v8",
          "author": "DAlmighty",
          "text": "Iâ€™m fairly confident that Nvidiaâ€™s recent license deal will produce cards for inference only.  That could possibly be a great thing for the community.",
          "score": 0,
          "created_utc": "2025-12-27 02:23:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw346wy",
          "author": "seppe0815",
          "text": "its about tensor core ... who want 48gb and low tensors ... useless",
          "score": -5,
          "created_utc": "2025-12-26 21:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw30doe",
          "author": "zasura",
          "text": "this will sound controversial but what's the point? All the good models are closed source like claude. Open source are great but... lack that \"spice\" that makes them better than everything else.",
          "score": -22,
          "created_utc": "2025-12-26 20:59:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw31ctz",
              "author": "LoSboccacc",
              "text": "Eh theres plenty good model nov in the .5 1.5 teraweight range. Not something we can run, but claude at home exists, theoretically speaking. (But lets say claude 3.5, tops)\n\n\nAnd look new technique are making smaller models more and more viable. Haiku 4.5 is surprisingly good, as soon as so e lab can guess their recipe well have models for 96gb pro cards.",
              "score": 9,
              "created_utc": "2025-12-26 21:05:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw34125",
              "author": "Photoperiod",
              "text": "Lotta infosec departments in companies don't want their data going to third parties. Depending on the industry running open source on your own hardware is required. That said I generally agree. Claude is crazy good.",
              "score": 6,
              "created_utc": "2025-12-26 21:19:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw31tad",
              "author": "nntb",
              "text": "Imagine having this view on this subreddit lol",
              "score": 9,
              "created_utc": "2025-12-26 21:07:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw38ny7",
              "author": "Lissanro",
              "text": "I disagree... There are plenty of smaller capable local models for any rig from small to medium size (like GLM or MiniMax series) to large size (DeepSeek and Kimi), so it is possible to find reasonably good models for almost any hardware.\n\nI run mostly either K2 0905 or K2 Thinking on my PC (IQ4 or Q4\\_X quants respectively, using ik\\_llama.cpp), depending on if I need thinking or not, and find them quite good in my daily work, or for personal use. I do not feel like I am missing out on anything by avoiding dependency on cloud models, but gain privacy and reliability (no one can take models I use offline or change them, and I can safely rely on them always be available unless I decide to replace them myself).",
              "score": 4,
              "created_utc": "2025-12-26 21:44:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3ac06",
              "author": "tat_tvam_asshole",
              "text": "it's not about A model, it's about modelS... specifically the Network Effects of multiple models with tools",
              "score": 3,
              "created_utc": "2025-12-26 21:53:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw6vmrl",
              "author": "Freonr2",
              "text": "Do you want your codebase ending up in training data for models that your competitors will use?",
              "score": 2,
              "created_utc": "2025-12-27 13:57:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxad0k",
      "title": "NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux",
      "subreddit": "LocalLLaMA",
      "url": "https://hackaday.com/2025/12/26/nvidia-drops-pascal-support-on-linux-causing-chaos-on-arch-linux/",
      "author": "HumanDrone8721",
      "created_utc": "2025-12-27 22:22:21",
      "score": 449,
      "num_comments": 152,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/",
      "domain": "hackaday.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwc9e7h",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-28 09:45:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9nnam",
          "author": "FearFactory2904",
          "text": "The 24GB p40 is a pascal card. Liked those a lot before they became really expensive.",
          "score": 160,
          "created_utc": "2025-12-27 22:51:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwawtt8",
              "author": "David_Delaune",
              "text": "I was extremely lucky the past few years, sold my all my Tesla P40's when they peaked in value, which just happed to be when 3090's were still affordable. My only regret was not buying more RAM for my home lab. I thought 128GB was good enough.",
              "score": 40,
              "created_utc": "2025-12-28 03:13:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwbrq2x",
                  "author": "ziggo0",
                  "text": "I'd say I have a reply for the P40s but I'm saddened over this article.",
                  "score": 6,
                  "created_utc": "2025-12-28 06:56:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf4awt",
                  "author": "Tasty_Ticket8806",
                  "text": "Can I ask what you are running that needs 128gbs of ram?",
                  "score": 1,
                  "created_utc": "2025-12-28 20:07:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwbserq",
              "author": "KadahCoba",
              "text": "Same. For $150 a 2-3 years ago, worth it. The $350-400+ they've been for most of 2025 was insane.",
              "score": 6,
              "created_utc": "2025-12-28 07:02:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwecvel",
                  "author": "frozen_tuna",
                  "text": "2-3 years ago, none of the local llama software people use now existed, and if it did, it didn't support the p40 architecture. I made a lot of comments about it in the early days of this sub, eventually advocated to a lot of people who pm'd me to bite the bullet and get a used 3090 instead like I eventually did.",
                  "score": 1,
                  "created_utc": "2025-12-28 17:58:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw9sahw",
          "author": "C0rn3j",
          "text": "Arch dropping legacy drivers to AUR has been a thing for eons, it is not surprising, and it is in the [Arch News](https://archlinux.org/news/nvidia-590-driver-drops-pascal-support-main-packages-switch-to-open-kernel-modules/).",
          "score": 70,
          "created_utc": "2025-12-27 23:17:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9kqot",
          "author": "knook",
          "text": "Ah crap, I was worried this would be coming soon.",
          "score": 75,
          "created_utc": "2025-12-27 22:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9rzqs",
              "author": "pissoutmybutt",
              "text": "Whats this mean for me who is using a tesla p4 for mostly transcodes with ffmpeg? I just wont get driver updates? Like i shouldnt have to worry aboot a huge headache from this for some reason running ubuntu 22.04 LTS would I?",
              "score": 28,
              "created_utc": "2025-12-27 23:15:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9uak0",
                  "author": "knook",
                  "text": "Yeah, pretty much just driver updates will stop. It won't change anything for us for a long while in all likelihood.",
                  "score": 35,
                  "created_utc": "2025-12-27 23:28:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw9x1nj",
                  "author": "LostLakkris",
                  "text": "I just keep the .run files on my NAS that have historically worked, not a fan over system packages, but it's been reliable",
                  "score": 3,
                  "created_utc": "2025-12-27 23:44:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwdel5m",
                  "author": "LoafyLemon",
                  "text": "The next time you run pacman or yay, you'll see an option to either stay on nvidia package or move to nvidia-open if your card is still supported.\n\n\nArch solved this issue beautifully.",
                  "score": 2,
                  "created_utc": "2025-12-28 15:05:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwax5qp",
          "author": "segmond",
          "text": "who cares?  don't upgrade to the latest driver.  chances are if you are running P40, you are not running 5090 on the same system.",
          "score": 12,
          "created_utc": "2025-12-28 03:15:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwa5swi",
          "author": "TurnUpThe4D3D3D3",
          "text": "This doesnâ€™t really matter, the drivers for Pascal are already super stable. They donâ€™t need updates.",
          "score": 41,
          "created_utc": "2025-12-28 00:33:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwaszus",
              "author": "esuil",
              "text": "Yeah, I am confused on WTF people are even on about.\n\nIt's not like old drivers are going away, and they have full functionality, right? So what exactly is the problem? \n\nMy god I hate modern clickbait media. 20 years ago this kind of posting would get you a temporary ban for fearmongering in most communities.",
              "score": 33,
              "created_utc": "2025-12-28 02:50:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwazf2x",
                  "author": "natufian",
                  "text": "I guess you can say \"Causing Chaos on Arch Linux\" is clickbaity (I didn't follow the link to survey said \"chaos\" for myself-- may be legit), but this generation of drivers works with the *current* kernel.  Any random kernel update that touches any CUDA handling can potentially break things at any time. Its a ticking time bomb. It's likely that the kernel maintainers will manually code in compatibility just for these versions of the Pascal drivers for a while, but as the mainline progresses and it naturally gets more and more labor intensive to harmonize this old frozen driver from that moment back in 2025 to the evolving and improving paradigms...\n\nNot the end of the world-- there will *always* be work-arounds, but legit consequential, terrible,  news.",
                  "score": 17,
                  "created_utc": "2025-12-28 03:29:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwfq991",
                  "author": "1731799517",
                  "text": "Linux LOVES to intentionally break driver interfaces in order to punish people using non open source drivers.",
                  "score": 3,
                  "created_utc": "2025-12-28 21:55:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwy3i9x",
                  "author": "koflerdavid",
                  "text": "Nothing bad happens right now of course, but eventually one might not be able to get security fixes for one's software anymore without running into dependency issues. Same situation as for retrocomputing. Linux and distros are not above abandoning platforms because of that argument.",
                  "score": 1,
                  "created_utc": "2025-12-31 17:35:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw9tlu7",
          "author": "blueblocker2000",
          "text": "Pascal was the last iteration that cared about the regional power grid.",
          "score": 39,
          "created_utc": "2025-12-27 23:24:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwburl8",
              "author": "AndreaCicca",
              "text": "What do you mean",
              "score": 3,
              "created_utc": "2025-12-28 07:24:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdocg2",
                  "author": "dajigo",
                  "text": "Power consumption has really increased over time. Quite intensely at that.",
                  "score": 2,
                  "created_utc": "2025-12-28 15:56:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwcvxnf",
              "author": "Dry-Judgment4242",
              "text": "Newer cards can be undervolted quite hard though. I run mine at 70% while still getting like 93% performance.",
              "score": 3,
              "created_utc": "2025-12-28 13:06:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9qr4t",
          "author": "trimorphic",
          "text": "Please don't kill me for this incredibly stupid and ignorant question, but is it really that hard to make good open source drivers for NVIDIA cards?\n\nOr is there just not enough interest or not enough funding?",
          "score": 23,
          "created_utc": "2025-12-27 23:08:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9rjt8",
              "author": "Usual-Orange-4180",
              "text": "There are, but is not just drivers but CUDA integration, super difficult (the moat).",
              "score": 33,
              "created_utc": "2025-12-27 23:12:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9ykvk",
                  "author": "muxxington",
                  "text": "The greater the despair, the smaller the moat may become. One can still dream, after all.",
                  "score": 8,
                  "created_utc": "2025-12-27 23:52:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9r4mx",
              "author": "C0rn3j",
              "text": "> is it really that hard to make good open source drivers for NVIDIA cards? \n\nYes.",
              "score": 96,
              "created_utc": "2025-12-27 23:10:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw9xk6n",
              "author": "SwordsAndElectrons",
              "text": "That support CUDA and make the best possible use of the hardware? Without any support or resources from the hardware vendor?\n\n\nYes. It's pretty hard.",
              "score": 16,
              "created_utc": "2025-12-27 23:47:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw9vcww",
              "author": "qwerkeys",
              "text": "Nvidia blocked firmware re-clocking on open-source drivers for Maxwell and Pascal. The GPUs perform like theyâ€™re permanently idle. Also a very â€˜my way or the high wayâ€™ attitude to Linux standards like with EGLStreams (nvidia) vs GBM (everyone else). This also delayed adoption of Wayland on Linux\n\nhttps://www.phoronix.com/review/nvidia-980-5080-linux",
              "score": 34,
              "created_utc": "2025-12-27 23:34:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwc08xm",
                  "author": "dannepai",
                  "text": "Whyyyy does Nvidia have to be so disgusting? Iâ€™m proud to say that the last GPU from them I had was the 256, and I bought it used.",
                  "score": 4,
                  "created_utc": "2025-12-28 08:15:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwhwls5",
                  "author": "RhubarbSimilar1683",
                  "text": "I read that the issue was that there wasn't a stable version of the signed firmware to reverse engineer. Now that it's eol it's possibleÂ ",
                  "score": 1,
                  "created_utc": "2025-12-29 05:20:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9xvmg",
              "author": "Aggressive-Bother470",
              "text": "Nothing will change, lol.Â \n\n\nInstall an older driver, the end.",
              "score": 5,
              "created_utc": "2025-12-27 23:48:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwa3bod",
                  "author": "bitzap_sr",
                  "text": "Until some kernel change breaks it...",
                  "score": 14,
                  "created_utc": "2025-12-28 00:19:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw9igk0",
          "author": "HumanDrone8721",
          "text": "3090 people, be afraid, be very afraid !!!",
          "score": 80,
          "created_utc": "2025-12-27 22:22:47",
          "is_submitter": true,
          "replies": [
            {
              "id": "nw9qc5c",
              "author": "0xCODEBABE",
              "text": "why?",
              "score": 38,
              "created_utc": "2025-12-27 23:05:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9tzhz",
                  "author": "sibilischtic",
                  "text": "I'm not sure either....\n\nIt should be quite a while before ampere reaches the chopping block for support. One day they will reach eol but i don't think its something to worry about just yet.",
                  "score": 49,
                  "created_utc": "2025-12-27 23:26:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwa5cha",
                  "author": "eloquentemu",
                  "text": "Yeah.  Okay, obviously dropping Pascal is on the road to dropping Ampere, but Pascal came out in ~2016 and Ampere was ~2020 so the 3090 should have some years still.",
                  "score": 18,
                  "created_utc": "2025-12-28 00:30:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9xblc",
              "author": "vulcan4d",
              "text": "The 3000 series had the best price to performance ratio.  Nothing would make Nvidia happier than to kill these great cards.  Our options are becoming fewer each year.\n\nI strongly believe that the market is being manipulated.  The moment Moe models came to be, the threat of open source was real.  Kimi 2 competes with cloud AI models and it can run local, the problem is, the vram and ram situation prevents the average joe from running these large models and you are dependent on the cloud providers.  :(",
              "score": 45,
              "created_utc": "2025-12-27 23:45:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwa4f1q",
                  "author": "discreetwhisper1",
                  "text": "What is moe models and kimi 2 am noob with a 3090",
                  "score": 2,
                  "created_utc": "2025-12-28 00:25:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwa1f60",
                  "author": "Glum_Control_5328",
                  "text": "I donâ€™t think NVIDIA intentionally plans to phase out consumer GPUs. Any shift away from these cards would probably be a result of reallocating internal resources to focus on data center GPU software. Consumer grade GPUs appeal to individual users who want to train or run AI models locally.  Most companies arenâ€™t interested in physically hosting their own hardware though. Maybe with the  exception of companies based in the China.\n\nNone of the clients Iâ€™ve worked with have invested in consumer hardware for local AI tasks,they prefer renting resources from platforms like Microsoft or AWS. (Or theyâ€™ll get a few data center chips depending on confidentiality risk)",
                  "score": 4,
                  "created_utc": "2025-12-28 00:08:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9pzqc",
              "author": "CodeFarmer",
              "text": "I'm in this comment and I don't like it.",
              "score": 8,
              "created_utc": "2025-12-27 23:03:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwbsrb2",
              "author": "KadahCoba",
              "text": "No need to worry till a year or two after Volta support starts being EOL'd.",
              "score": 2,
              "created_utc": "2025-12-28 07:05:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwalwxf",
              "author": "Normal-Ad-7114",
              "text": "First they have to slay Volta and Turing, and only then comes Ampere",
              "score": 3,
              "created_utc": "2025-12-28 02:08:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwbmn48",
              "author": "nonaveris",
              "text": "Iâ€™ll worry when the RTX 8000 gets dropped from support.  Thatâ€™s about the only 48GB card with CUDA and sane pricing.",
              "score": 1,
              "created_utc": "2025-12-28 06:12:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwbtxc3",
                  "author": "HumanDrone8721",
                  "text": "What about A6000, around here are the same price used, ca. 2000-2100EUR ?",
                  "score": 1,
                  "created_utc": "2025-12-28 07:16:13",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwa3d5h",
          "author": "jebuizy",
          "text": "This is not \"chaos\". This is total click bait.Â ",
          "score": 8,
          "created_utc": "2025-12-28 00:19:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbilij",
          "author": "Flat_Association_820",
          "text": ">thus the user getting kicked back to the CLI to try and sort things back out there\n\nIsn't that why people use Arch?\n\nIt's like complaining that a gas powered vehicle consume gas.",
          "score": 8,
          "created_utc": "2025-12-28 05:39:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9qqwn",
          "author": "_lavoisier_",
          "text": "So they killed the support of one of the oldest programming language? This is pure greed!",
          "score": 32,
          "created_utc": "2025-12-27 23:08:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9t3wq",
              "author": "fishhf",
              "text": "Damn how do people write CUDA kernels if not in Pascal then?",
              "score": 23,
              "created_utc": "2025-12-27 23:21:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwb2nqc",
                  "author": "earslap",
                  "text": "They will be forced to use a Turing machine (20xx series). Once that support dies, they will be forced to write by manipulating pure electricity (Ampere).",
                  "score": 10,
                  "created_utc": "2025-12-28 03:48:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9rabd",
              "author": "amooz",
              "text": "I think they mean the card architecture not the language",
              "score": 29,
              "created_utc": "2025-12-27 23:11:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwa20du",
                  "author": "shaolinmaru",
                  "text": "whoosh",
                  "score": 20,
                  "created_utc": "2025-12-28 00:11:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw9rjse",
                  "author": "_lavoisier_",
                  "text": "lmao",
                  "score": 11,
                  "created_utc": "2025-12-27 23:12:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9vknd",
              "author": "psxndc",
              "text": "Iâ€™m going to be honest, the programming language is the only Pascal I know of. I *knew* the title wasnâ€™t referring to that, but I was still very confused.",
              "score": 15,
              "created_utc": "2025-12-27 23:35:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwcqygg",
                  "author": "ryunuck",
                  "text": "so did I AND YET still there I was, with a half written comment about rust",
                  "score": 2,
                  "created_utc": "2025-12-28 12:27:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwc3g3u",
              "author": "iamapizza",
              "text": "Clearly they were under pressure",
              "score": 3,
              "created_utc": "2025-12-28 08:46:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw9zc88",
              "author": "muxxington",
              "text": "It's not about the programming language. It's about the basketball player. I didn't know he played for NVIDIA, though.  \n[https://en.wikipedia.org/wiki/Pascal\\_Siakam](https://en.wikipedia.org/wiki/Pascal_Siakam)",
              "score": 7,
              "created_utc": "2025-12-27 23:57:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwakpye",
                  "author": "Pacostaco123",
                  "text": "No, they are referring to thousands of a unit of pressure.\n\nKill a pascals",
                  "score": 8,
                  "created_utc": "2025-12-28 02:01:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwacpns",
          "author": "RobotRobotWhatDoUSee",
          "text": "What does this practically mean for P40 builds?",
          "score": 4,
          "created_utc": "2025-12-28 01:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy73gz",
              "author": "koflerdavid",
              "text": "Nothing unless you run a rolling release distro. But in the mid term you will be forced to switch to an LTS version of your distro since the last driver version supporting Pascal might not remain forever compatible with the upstream kernel. Or you hope that the Nova driver matures fast enough and that ZLUDA also starts supporting cards that are sunset by Nvidia.",
              "score": 2,
              "created_utc": "2025-12-31 17:53:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwareic",
          "author": "siegevjorn",
          "text": "Wouldn't just using distros built for robustness and longevity like rocky linux make Pascal to work for a long time?",
          "score": 3,
          "created_utc": "2025-12-28 02:40:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy7bwc",
              "author": "koflerdavid",
              "text": "You can also just keep using an LTS version and hope that nothing forces you to upgrade until you get new hardware.",
              "score": 1,
              "created_utc": "2025-12-31 17:54:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwccr6j",
          "author": "pheasantjune",
          "text": "Is Pedro okay?",
          "score": 3,
          "created_utc": "2025-12-28 10:17:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbvved",
          "author": "dtdisapointingresult",
          "text": "Alternative title: Rolling distro update breaks users' desktop, to the surprise of no one wise enough to avoid rolling distros.",
          "score": 5,
          "created_utc": "2025-12-28 07:34:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9rayc",
          "author": "Megaboz2K",
          "text": "Wow, my first thought was \"Since when can you do Cuda programming in Pascal?\" before I realized it was regarding the architecture, not the language. I think I'm doing too much retrocomputing lately!",
          "score": 8,
          "created_utc": "2025-12-27 23:11:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9wlqg",
              "author": "toothpastespiders",
              "text": "Same here. I was wondering for a moment if there was some weird officially maintained Delphi/FireMonkey backend or something. My blame for the brainfart goes to the early Wizardry games.",
              "score": 2,
              "created_utc": "2025-12-27 23:41:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9l8tr",
          "author": "No_Afternoon_4260",
          "text": "Pascal was compute capability 6.0, it introduced\n- nvlink (between 80 and 200 gb/s)\n- hbm2 on a 4096 bits bus achieving a whooping 720gb/s\n- gddr5x on 256 bits for 384gb/s\n- unified memory  \n- fp16  \n- ...\n\nThe 1080ti was 11gb, it was made for a quantized 7b\n\nIt will soon be left for dead (or for vulkan)",
          "score": 5,
          "created_utc": "2025-12-27 22:37:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9n52s",
              "author": "Organic-Thought8662",
              "text": "So much of that is wrong. \n\nPascal was Mostly Compute 6.1   \nThe only Compute 6.0 was the P100, which was also the only card in the family which used HBM and had full speed FP16.  \nThe rest of the cards were GDDR5(x)   \nThere was no 1090ti, the GOAT was the 1080ti, which was an 11GB card, using GDDR5x and had gimped fp16, but DP4a for decent int8 performance. It also was on a 384 bit bus with 484GB/s of bandwidth. \n\nThe card most in this subreddit have been using from pascal is the P40, which is a 1080ti, with 24GB of GDDR5 (non-x) for 347GB/s of bandwidth.",
              "score": 31,
              "created_utc": "2025-12-27 22:48:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw9qw91",
              "author": "lastrosade",
              "text": "The what now? The 1070 ti and the 1080 are 8gb, the 1080 ti is 11.",
              "score": 5,
              "created_utc": "2025-12-27 23:09:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9rcbu",
                  "author": "No_Afternoon_4260",
                  "text": "My bad, did a quick search, thanks for pointing that to me",
                  "score": -2,
                  "created_utc": "2025-12-27 23:11:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw9swz2",
          "author": "Bozhark",
          "text": "Welp, 48GB 2080ti next",
          "score": 5,
          "created_utc": "2025-12-27 23:20:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwac3ri",
              "author": "a_beautiful_rhind",
              "text": "You can't. Only 22gb fits. Maybe RTX8000 or something.",
              "score": 5,
              "created_utc": "2025-12-28 01:09:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwaf2zo",
                  "author": "Bozhark",
                  "text": "You havenâ€™t seen the Chinese resolders?Â ",
                  "score": -8,
                  "created_utc": "2025-12-28 01:27:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwadla3",
          "author": "a_beautiful_rhind",
          "text": "Wait till you find out torch dropped it after 2.7. Why is this news now when it was warned about for cuda13 months ago? Simply don't update.\n\nI never tried the open driver on my P40s or P100, even though there is code in there for the architecture You are also supposed to pass an unsupported GPU flag to enable.",
          "score": 4,
          "created_utc": "2025-12-28 01:18:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9puil",
          "author": "AdamDhahabi",
          "text": "Stay on the current driver. And old news: no way to use a Blackwell card and a Pascal card in the same system, except for Windows.",
          "score": 2,
          "created_utc": "2025-12-27 23:03:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwamsw6",
              "author": "TokenRingAI",
              "text": "Never say never.\n\nQEMU + PCIe passthrough + RPC.",
              "score": 4,
              "created_utc": "2025-12-28 02:13:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwautmu",
              "author": "Arxijos",
              "text": "Easy way, search for, incus (previously known as lxc) pcie pass through, utilizes qemu.",
              "score": 0,
              "created_utc": "2025-12-28 03:00:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9v4tg",
          "author": "the_Luik",
          "text": "I guess Nvidia needs people to buy new hardware.",
          "score": 4,
          "created_utc": "2025-12-27 23:33:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbrqfn",
          "author": "jacek2023",
          "text": "I was an active Arch contributor around 2005, I wonder what this chaos means in 2025",
          "score": 2,
          "created_utc": "2025-12-28 06:56:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwbtt8e",
              "author": "HumanDrone8721",
              "text": "Well, the Arch crowd likes to stay on top of the things, they're easy to dismiss \"yeah, yeah, just stay with the older stuff...\", but usually sooner than later this happens as well to the more mainstream distros. For example I'm using Debian 13 Trixie but set the Nvidia's repos for drivers and CUDA stuff, many others do it to have the latest features and speed improvements and it actually shows. To have the rug pulled under you is annoying.",
              "score": 2,
              "created_utc": "2025-12-28 07:15:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwbvdoh",
              "author": "AndreaCicca",
              "text": "You update your machine and instead of your desktop environment you see a TTY. In order to fix you have to install the proper driver.",
              "score": 2,
              "created_utc": "2025-12-28 07:29:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwbviy2",
                  "author": "jacek2023",
                  "text": "I assume some Arch users are familiar with the shell even in 2025? :)",
                  "score": 6,
                  "created_utc": "2025-12-28 07:31:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwfhxqn",
                  "author": "Barafu",
                  "text": "If you update your machine, ignore the article in distro news, ignore the question presented by the package manager â€” then upon reboot you should not see a TTY, you should see a Windows 11 with blocked admin rights.",
                  "score": 1,
                  "created_utc": "2025-12-28 21:14:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwbgohy",
          "author": "IAmBobC",
          "text": "The GTX series is still EXTREMLY RELEVANT, even today! Especially if you are trying to run LLMs and other neural networks locally. Sure, the RTX series is better, but GTX can still do some serious heavy lifting!\n\nÂ¨Hardware Obsolesce through SoftwareÂ¨is total BS. That silicon still has MUCH to offer!\n\nSure, the OEM wants you to upgrade. ThatÂ´s not wrong, and itÂ´s not unfair. WhatÂ´s not right is letting software ALONE kill perfectly good hardware!\n\nFight this Â¨planned obsolescenceÂ¨!",
          "score": 2,
          "created_utc": "2025-12-28 05:25:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc347l",
              "author": "TechnoByte_",
              "text": "Why are you acting like they'll stop working?\n\nYou can still keep using the current driver which is very stable, you just won't be getting updates",
              "score": 5,
              "created_utc": "2025-12-28 08:43:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwfi6io",
                  "author": "Barafu",
                  "text": "But how can one farm karma points without pretending to be dumber than they already are?",
                  "score": 1,
                  "created_utc": "2025-12-28 21:15:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw9xmx8",
          "author": "Dorkits",
          "text": "NVIDIA is a bitch. My next card will be AMD without any doubt.",
          "score": 1,
          "created_utc": "2025-12-27 23:47:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwak5ce",
          "author": "noiserr",
          "text": "I still have a linux machine with my last nvidia GPU, Titan Xp. Will be replacing it with the 9700 AI Pro if the price ever hits the MSRP.",
          "score": 1,
          "created_utc": "2025-12-28 01:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwapj8o",
          "author": "freehuntx",
          "text": "Just pin to 580",
          "score": 1,
          "created_utc": "2025-12-28 02:29:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwaxre2",
          "author": "RayneYoruka",
          "text": "Well I suppose I'll have to decide on a radeon or intel gpu for my proxmox if the support will be ending soon! (Got a 1030 atm, was eyeing a pascal quadro card)",
          "score": 1,
          "created_utc": "2025-12-28 03:18:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe51u9",
          "author": "IrisColt",
          "text": "\"chaos\" stopped reading. Clickbait.",
          "score": 1,
          "created_utc": "2025-12-28 17:20:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfiwo2",
          "author": "Thedudely1",
          "text": "Meanwhile the RX 480 supports ray tracing",
          "score": 1,
          "created_utc": "2025-12-28 21:19:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfwqf6",
          "author": "nonaveris",
          "text": "Is Volta still supported?  Thereâ€™s still plenty of 32gb v100s for moderately cheap out there.",
          "score": 1,
          "created_utc": "2025-12-28 22:28:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl6xuu",
          "author": "Shoddy-Tutor9563",
          "text": "This is what you get when using rolling distros",
          "score": 1,
          "created_utc": "2025-12-29 18:32:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwan7e7",
          "author": "MontyBoomslang",
          "text": "This bit me last week. Caused me to buy my first AMD GPU. I now get why people rag on Nvidia support for Linux. This Radeon was super easy to set up and already has much less buggy weirdness.",
          "score": 0,
          "created_utc": "2025-12-28 02:15:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcugh2",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2025-12-28 12:55:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwh71aj",
                  "author": "MontyBoomslang",
                  "text": "I installed the AUR driver and it worked okay for gaming, but there were other places where it seemed to cause problems (Ollama being one).\n\n>an upgrades always nice I guess lol\n\nLol yep! And as much as I unironically love running old, cheap hardware, I didn't want to suffer the pains of gradual compatibility loss I saw coming down the pike.\n\n>Itâ€™s worth subscribing to the Arch newsletter. These things are announced in advance\n\nThis... Would be good to do. Eight years on Arch, and I've just shrugged and dealt with breaks as they came. It *would* be nice to have a heads up. Thanks for the tip!",
                  "score": 1,
                  "created_utc": "2025-12-29 02:42:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwcbeg6",
              "author": "MDSExpro",
              "text": "AMD has even weaker and shorter GPU support than Nvidia.",
              "score": 3,
              "created_utc": "2025-12-28 10:04:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwd6q8u",
                  "author": "kopasz7",
                  "text": "13 year old GPUs getting 30% extra performance in games. Happened this week.\n\n[Phoronix: Linux 6.19's Significant ~30% Performance Boost For Old AMD Radeon GPUs\n](https://www.phoronix.com/review/linux-619-amdgpu-radeon)\n\nOpensource drivers let others contribute, keeps the project going longer.",
                  "score": 1,
                  "created_utc": "2025-12-28 14:18:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwaguhk",
          "author": "ttkciar",
          "text": "Shit like this makes me really glad AMD publishes their ISA.",
          "score": 1,
          "created_utc": "2025-12-28 01:38:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwahels",
          "author": "Traditional_Nose3120",
          "text": "Linus should get his middle finger out of retirement",
          "score": -1,
          "created_utc": "2025-12-28 01:41:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwaabyv",
          "author": "autodidacticasaurus",
          "text": "Lucky me, upgraded from my 1030 GT card to a Radeon 7900 XTX just in time.",
          "score": 0,
          "created_utc": "2025-12-28 00:59:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9l3oa",
          "author": "notcooltbh",
          "text": "the 4 arch users are shivering their timbers rn",
          "score": -26,
          "created_utc": "2025-12-27 22:37:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9n4dr",
              "author": "beren0073",
              "text": "If those Arch users could read, they'd be very upset",
              "score": -5,
              "created_utc": "2025-12-27 22:48:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwc3kgv",
                  "author": "TechnoByte_",
                  "text": "We're reading Arch wiki pages and using the terminal every day, we know how to read",
                  "score": 1,
                  "created_utc": "2025-12-28 08:48:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwah756",
          "author": "Lesser-than",
          "text": "you will upgrade and be happy!",
          "score": -2,
          "created_utc": "2025-12-28 01:40:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbif8o",
          "author": "Ok-Adhesiveness-4141",
          "text": "This was so damn confusing, I thought it was to do with the Pascal language.\n\nThis is  a good example of why Nvidia sucks, they have always sucked if my memory serves me right. Don't trust any hardware vendor that doesn't open source their device drivers. \n\nI will go one-step further and say, we need to reverse engineer proprietary drivers and then vibe-code open source drivers. We should no longer be respectful of intellectual property rights of these hardware mafia guys.",
          "score": -2,
          "created_utc": "2025-12-28 05:38:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc3eu8",
              "author": "TechnoByte_",
              "text": "NVIDIA did open source the kernel modules\n\nYou don't need to vibecode new open source drivers because [Noveau](https://nouveau.freedesktop.org/) already exists and isn't garbage LLM code",
              "score": 0,
              "created_utc": "2025-12-28 08:46:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9l449",
          "author": "Gwolf4",
          "text": "Using Nvidia for GUI in Linux is a fool's errand anyways.",
          "score": -33,
          "created_utc": "2025-12-27 22:37:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9n7lg",
              "author": "edparadox",
              "text": "\"For GUI\" keep pretending you know what you're saying LMAO.",
              "score": 17,
              "created_utc": "2025-12-27 22:48:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9tqv7",
                  "author": "Gwolf4",
                  "text": "I know what I am saying and probably more than you it seems.",
                  "score": -10,
                  "created_utc": "2025-12-27 23:25:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwatx8l",
          "author": "Upper_Road_3906",
          "text": "they want you locked into the windows ai system i bet they start dropping it for all linux distros down the road for them it makes no sense in their fever dream of you renting a cloud gpu to play games and pay them eternally for rent and being able to deplatform you at a moments notice.",
          "score": -4,
          "created_utc": "2025-12-28 02:55:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc418q",
              "author": "TechnoByte_",
              "text": "NVIDIA wants you to use Windows? lmao\n\nYou're acting like old NVIDIA GPUs are about to be bricked or remotely detonated, they won't, you can just keep using the current stable driver and they will work fine",
              "score": 1,
              "created_utc": "2025-12-28 08:52:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyg4yt",
      "title": "Tencent just released WeDLM 8B Instruct on Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1pyg4yt",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-29 07:38:43",
      "score": 414,
      "num_comments": 62,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwiswg2",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-29 10:05:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwigd3k",
          "author": "jamaalwakamaal",
          "text": "7-8B models have lot of potential. Very promising space. More models please.",
          "score": 49,
          "created_utc": "2025-12-29 08:07:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwie4o6",
          "author": "Endlesscrysis",
          "text": "Pretty huge I think? I thought I saw people mentioning a couple of times that diffusion models werenâ€™t possible for accurate LLMâ€™s yet this outperforms a similar sized powerhouse like qwen?",
          "score": 85,
          "created_utc": "2025-12-29 07:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwio6nn",
              "author": "SlowFail2433",
              "text": "Yeah I was one of the pretty vocal skeptics about diffusion language models. I thought their inductive bias was too sub-optimal for language/code. I was super wrong about this.",
              "score": 53,
              "created_utc": "2025-12-29 09:20:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjvefy",
                  "author": "Investolas",
                  "text": "I'd love to read one of your critiques, care to share a link to a comment or post you've made? I didn't find any of your contributions and assume they are paywalled. Thx!",
                  "score": 10,
                  "created_utc": "2025-12-29 14:45:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwnur1r",
                  "author": "aeroumbria",
                  "text": "Interestingly I am more of the opinion that the autoregressive inductive bias is too restricting and unnatural, and may contribute to why we need so many parameters to reach usability. It feels like traditional linguistics gives more credit to a \"large scale autoregressive (causal dependency), small scale hierarchical (tree structure in grammar)\" type of model, which is closer to block diffusion. Still not entirely sold on the token-wise masking process thing though - it cannot reflect a hierarchical \"concept refinement\" process. Interested to see any progress in this direction though.",
                  "score": 2,
                  "created_utc": "2025-12-30 02:45:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwkw478",
              "author": "Orolol",
              "text": "We know diffusion is possible since atleast Llada 18 months ago. But the problem was that it used a non causal attention, so we were unable to use many crucial techniques, like kv cache. \nThis enables the use of kvcache because of a very clever trick.",
              "score": 8,
              "created_utc": "2025-12-29 17:43:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwooixj",
              "author": "Mikasa0xdev",
              "text": "Diffusion models are the new transformers, confirmed.",
              "score": 2,
              "created_utc": "2025-12-30 05:52:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwiiec4",
          "author": "jacek2023",
          "text": "additionaly [https://huggingface.co/tencent/WeDLM-7B-Instruct](https://huggingface.co/tencent/WeDLM-7B-Instruct)",
          "score": 31,
          "created_utc": "2025-12-29 08:26:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiji7b",
              "author": "aeroumbria",
              "text": "Interesting. Is there a specific use case where 8B can't fit but 7B can?",
              "score": 12,
              "created_utc": "2025-12-29 08:36:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwiufh1",
                  "author": "pkmxtw",
                  "text": "The 7B is converted from Qwen2.5 7B and the 8B is from Qwen3 8B. What they want to demonstrate is that they can convert an AR model into a diffusion model w/o losing quality.\n\nIn reality, you'd just use the 8B like how Qwen3 8B has basically replaced Qwen2.5 7B.",
                  "score": 43,
                  "created_utc": "2025-12-29 10:19:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwigcqz",
          "author": "Paramecium_caudatum_",
          "text": "Diffuser model with impressive benchmark scores and Apache 2.0 license, sounds pretty interesting to me.",
          "score": 52,
          "created_utc": "2025-12-29 08:07:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwihmcw",
          "author": "FinBenton",
          "text": "Its just a small model but 3-6x speed with similar or higher performance sounds insane!",
          "score": 26,
          "created_utc": "2025-12-29 08:18:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlum7p",
              "author": "lolwutdo",
              "text": "I know diffusion models are super fast on gpu but how would a diffusion model's speed compare on cpu vs a cpu llm?\n\nI guess mainly what I'm curious about is how well would a diffusion based llm run with cpu offloading compared to a traditional llm.",
              "score": 2,
              "created_utc": "2025-12-29 20:25:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwm87rw",
                  "author": "oh_how_droll",
                  "text": "Diffusion is going to be slower on CPUs -- CPUs are mostly compute-limited and they're more compute intensive.",
                  "score": 2,
                  "created_utc": "2025-12-29 21:32:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwie4gd",
          "author": "SlowFail2433",
          "text": "Nice to see another diffusion model would have liked more modern/harder benches",
          "score": 14,
          "created_utc": "2025-12-29 07:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwim9sq",
          "author": "Nice-Information-335",
          "text": "need unsloth or bartowski on this asap",
          "score": 19,
          "created_utc": "2025-12-29 09:02:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwirhjw",
              "author": "Odd-Ordinary-5922",
              "text": "will need a pr first for model support",
              "score": 36,
              "created_utc": "2025-12-29 09:52:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjz6tm",
                  "author": "MoffKalast",
                  "text": "We need a few papers first for model support",
                  "score": 8,
                  "created_utc": "2025-12-29 15:05:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwohedf",
              "author": "tronathan",
              "text": "Not really, in terms of usefuless, as I understand it, it's basically a Qwen 3. It's more of a proof of confacept",
              "score": 1,
              "created_utc": "2025-12-30 05:01:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwplso2",
                  "author": "Nice-Information-335",
                  "text": "hey I still want to try it! half of the fun for me is seeing advancements as they happen and being able to run them. massive props to everyone who makes that happen, as lord knows I don't know nearly enough to get this stuff working without the likes of llama.cpp, all it's amazing contributors and unsloth/bartowski for GGUFs",
                  "score": 1,
                  "created_utc": "2025-12-30 10:50:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwuzqra",
                  "author": "TomLucidor",
                  "text": "Let them make a version that beats Qwen3-30B-A3B and Nemotron-3-Nano",
                  "score": 1,
                  "created_utc": "2025-12-31 04:26:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwiw5cp",
          "author": "always_newbee",
          "text": "What is Qwen3-8B-Instruct model? Just non-thinking mode?",
          "score": 5,
          "created_utc": "2025-12-29 10:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj0ed1",
              "author": "mouseofcatofschrodi",
              "text": "yes",
              "score": 5,
              "created_utc": "2025-12-29 11:13:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwigrdo",
          "author": "Grouchygrond",
          "text": "Now we just need a hybrid model",
          "score": 3,
          "created_utc": "2025-12-29 08:10:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjuykz",
              "author": "Deciheximal144",
              "text": "How would that work? Diffusing in chunks? LLM generates, then diffusion revises the lowest-probability sections? Diffusion is noise-to-content.",
              "score": 7,
              "created_utc": "2025-12-29 14:43:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwljqrp",
                  "author": "peaceoutwhat",
                  "text": "Search TiDAR",
                  "score": 3,
                  "created_utc": "2025-12-29 19:32:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwm7wqb",
                  "author": "TheRealMasonMac",
                  "text": "There was a research model that diffused chunks one at a time like a Frankenstein of current LLMs and dLLMs\n\n\nhttps://m-arriola.com/bd3lms/",
                  "score": 3,
                  "created_utc": "2025-12-29 21:30:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwkwy6z",
              "author": "Orolol",
              "text": "I don't this it's possible to have both autoregressive and diffusion generation, and even if possible, I don't think there's any positive doing it.",
              "score": 2,
              "created_utc": "2025-12-29 17:47:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwlyqsm",
          "author": "Semi_Tech",
          "text": "Hmm shouldn't diffusion models also have a # of steps needed in order to reach the end result?\n\nI don't see a mention about that or how increasing or decreasing them affects model output quality.",
          "score": 6,
          "created_utc": "2025-12-29 20:46:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiksnh",
          "author": "Healthy-Nebula-3603",
          "text": "That's diffusion model right ?\n\n\nAs I understand such model can't be reasoner as can't looping in thoughts and observe own internal states?",
          "score": 7,
          "created_utc": "2025-12-29 08:48:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwilg37",
              "author": "Lesser-than",
              "text": "diffusion text models technically reason, as they can modify the first word of a sentence or tokens at every step of the inference, where a token by token model has to justify that token for the rest of the reply if they get it wrong.",
              "score": 26,
              "created_utc": "2025-12-29 08:54:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwilp6x",
                  "author": "Healthy-Nebula-3603",
                  "text": "I meant they can reason like the instruct models but are not thinkers like thinking models.",
                  "score": 2,
                  "created_utc": "2025-12-29 08:57:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj7ycj",
              "author": "NandaVegg",
              "text": "According to the site, this is a variation of block-wise diffusion (previously done by Meta etc) which acts more akin to a speculative decoding rather than a \"full\" diffusion (that denoises the whole output at once). I think Google did a web demo for mini full diffusion model in early 2025 but the model weight never got released?",
              "score": 7,
              "created_utc": "2025-12-29 12:16:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwv0suo",
              "author": "TomLucidor",
              "text": "Diffusion models can reason, just that not enough people put effort into the \"train of thought\" similar to auto-regressive models.",
              "score": 1,
              "created_utc": "2025-12-31 04:33:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwjjpa4",
          "author": "alphapussycat",
          "text": "What does math reasoning even mean? Calculation reasoning? Or math, as in theorem, reasoning?",
          "score": 3,
          "created_utc": "2025-12-29 13:38:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjkd02",
              "author": "PykeAtBanquet",
              "text": "Usually it is \"prove that this series converges\" etc",
              "score": 2,
              "created_utc": "2025-12-29 13:42:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwihwec",
          "author": "JackStrawWitchita",
          "text": "More people have commented on this than have downloaded it...",
          "score": 15,
          "created_utc": "2025-12-29 08:21:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwio18v",
              "author": "SlowFail2433",
              "text": "In ML research we often donâ€™t download the model right away.\n\n\nNote that the paper used the MagiAttention library for attention. I donâ€™t use this library so I am either going to write a custom CUDA kernel or use a DSL like Triton. However the paper has some technical novelties such as the topological reordering. This is not going to be easy to work out how to implement efficiently.",
              "score": 38,
              "created_utc": "2025-12-29 09:19:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwnlhqs",
                  "author": "RhubarbSimilar1683",
                  "text": "The paper is https://github.com/Tencent/WeDLM/blob/main/paper/wedlm.pdf",
                  "score": 1,
                  "created_utc": "2025-12-30 01:55:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwiu7q5",
              "author": "FinBenton",
              "text": "Gotta wait for llama.cpp and similar support first, most people here arent running vllm.",
              "score": 28,
              "created_utc": "2025-12-29 10:17:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwkk2j0",
                  "author": "Tai9ch",
                  "text": "Not downloading open source software seems like a lame excuse to not try something neat.",
                  "score": -2,
                  "created_utc": "2025-12-29 16:46:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoz431",
              "author": "aeroumbria",
              "text": "Still getting issues running the official repo... Supposedly this is only 8B and supports multi-GPU but cannot seem to allocate KV even with 2x24GB",
              "score": 1,
              "created_utc": "2025-12-30 07:20:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkkdz4",
          "author": "Awkward-Nothing-7365",
          "text": "Is this something that can run on llama.cpp right now? gguf possible?",
          "score": 2,
          "created_utc": "2025-12-29 16:47:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwk62zs",
          "author": "implicator_ai",
          "text": "Interesting release. When they say â€œdiffusion language model,â€ it usually means the model refines a whole sequence (or chunks) over a few denoising steps instead of generating strictly left-to-right token-by-token, which can trade fewer sequential steps for more parallel work.   \n  \nThe 3â€“6Ã— claim is worth sanity-checking against the exact setup: GPU type, batch size, context length, quantization, and decoding parameters (steps / temperature / top-p), because those can swing throughput a lot. If you try it, posting tokens/sec + latency at a fixed prompt length and a fixed quality target (e.g., same math benchmark score) would make the comparison much more meaningful.",
          "score": 2,
          "created_utc": "2025-12-29 15:39:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwml948",
              "author": "SilentLennie",
              "text": "From what I understand: diffusion models usually were not faster than regular LLMs, because they have K/V-cache and other tricks to speed it up to prevent doing duplicate math, supposedly this model solves that.",
              "score": 1,
              "created_utc": "2025-12-29 22:37:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwlqfpz",
          "author": "rm-rf-rm",
          "text": "They report the speed up for specifically just math reasoning tasks but it should be applicable generally no? \n\nHope we get MLX/GGUF support soon. If this is legit, its genuinely going to be massive. Right now I run 4B for quick look up etc. but I feel 4B models are not the most reliable for accurate information. At 8B, you can be much more confident.\n\nNext step MoE? Qwen3-Coder:a3b?",
          "score": 1,
          "created_utc": "2025-12-29 20:05:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwngm0k",
          "author": "RhubarbSimilar1683",
          "text": "Could diffusion enable efficient hybrid inference or inference computer clusters connected over the global internet, using asynchronous calls?",
          "score": 1,
          "created_utc": "2025-12-30 01:28:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnqpie",
          "author": "Vast-Piano2940",
          "text": "I wonder how it performs against lfm2-2.6b-exp",
          "score": 1,
          "created_utc": "2025-12-30 02:23:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuzlr9",
          "author": "TomLucidor",
          "text": "As long as this can be used with Claude Code or some other coding agent.",
          "score": 1,
          "created_utc": "2025-12-31 04:25:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q25070",
      "title": "LeCun Says Llama 4 results \"were fudged a little bit\"",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/",
      "author": "MrPecunius",
      "created_utc": "2026-01-02 17:38:01",
      "score": 350,
      "num_comments": 89,
      "upvote_ratio": 0.97,
      "text": "There was speculation in this sub about suspicious Llama 4 benchmarks some time back, and now LeCun confirms it on his way out. Best I can do is a Slashdot link since the FT article is paywalled:\n\n['Results Were Fudged': Departing Meta AI Chief Confirms Llama 4 Benchmark Manipulation ](https://tech.slashdot.org/story/26/01/02/1449227/results-were-fudged-departing-meta-ai-chief-confirms-llama-4-benchmark-manipulation)\n\nThis bit jumped out at me:\n\n>Zuckerberg subsequently \"sidelined the entire GenAI organisation,\" according to LeCun. \"A lot of people have left, a lot of people who haven't yet left will leave.\"\n\nThis explains a lot, if true: we never saw the promised huge Llama 4 model, and there hasn't been any followup since the other releases.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nxcdgcx",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-02 23:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxal7kd",
          "author": "shoeshineboy_99",
          "text": "Sharing the pdf for the complete article. \n\n\nhttps://drive.google.com/file/d/1wFy87TP7MJQDF1g0KA8IgZRtOx0jJUGE/view?usp=drivesdk",
          "score": 108,
          "created_utc": "2026-01-02 18:14:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxanbjp",
              "author": "MrPecunius",
              "text": "Thank you!",
              "score": 24,
              "created_utc": "2026-01-02 18:24:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxarmoh",
          "author": "m2r9",
          "text": "As much as I donâ€™t like Zuck I really wanted Llama to succeed. It was great seeing a US company pouring money into open source. Since it failed so hard most of the models you hear about come from China now.",
          "score": 199,
          "created_utc": "2026-01-02 18:44:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxaxnhy",
              "author": "Super_Sierra",
              "text": "I swear to god, Zuck has manic depression at the investment level. He goes all in, throws some of the best engineers at it, gets bored or sad when it doesn't cause a utopia or trillions and then sidelines everything. \n\nLlama 4 could have been great, but it felt rushed, benchmaxxed and sloppified, the only thing they did right was go MoE, even though I know the densetards here think otherwise because they can't put it all on a few 3090s they rewired their house for.",
              "score": 125,
              "created_utc": "2026-01-02 19:12:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxb2dj8",
                  "author": "zipzag",
                  "text": "Many entrepreneurs like Zuck and Musk do projects that fail. Henry Ford built Fordlandia with the same confidence and arrogance.\n\nHistory is messy when viewing in real time.\n\nMeta only got into open source to get LeCun on board. There likely no reason to continue. Chinese companies are only open source because it's their most profitable strategy.",
                  "score": 46,
                  "created_utc": "2026-01-02 19:34:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxbzgqk",
                  "author": "LanceThunder",
                  "text": "zuck just happened to be at the right place and the right time to be the founder of FB. if zuck didn't start it, some other collect kid would have created something very similar around the same time and they would have been the billionaire. zuck basically won the lottery but doesn't recognize it. so he thinks he is some sort of tech genius who can predict whats next on the cutting edge where tech meets society. he doesn't. FB is going to be run into the ground in less than a decade because it is being abused as a political propaganda machine. Instagram is going to be ruined by AI images. maybe whatapp will be profitable?",
                  "score": 29,
                  "created_utc": "2026-01-02 22:16:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxbpl87",
                  "author": "Caffdy",
                  "text": "> even though I know the densetards here think otherwise because they can't put it all on a few 3090s they rewired their house for\n\nThis, people around here act like everyone and anyone have access to cheap energy or unlimited amperage in their houses. Those multi-gpu contraptions are not always possible, and even so they're quite the power guzzlers and fire hazards",
                  "score": 2,
                  "created_utc": "2026-01-02 21:27:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxewfit",
                  "author": "Emergency-Arm-1249",
                  "text": "I think MoE is one of the main reasons why everything failed.",
                  "score": 2,
                  "created_utc": "2026-01-03 09:51:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxc5cho",
                  "author": "anything_but",
                  "text": "\"densetards\" gave me a chuckle",
                  "score": 3,
                  "created_utc": "2026-01-02 22:46:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdn6tq",
                  "author": "montdawgg",
                  "text": "\"densetards\"...hahaha.",
                  "score": 1,
                  "created_utc": "2026-01-03 03:54:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxbb93m",
              "author": "Due-Memory-6957",
              "text": "There are American companies that do open souce, they just haven't had impressed with a big release yet.",
              "score": 4,
              "created_utc": "2026-01-02 20:17:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxb2tsc",
              "author": "Plus-Accident-5509",
              "text": "Knowing what a piece of garbage he is, the open source move was nothing but an attempt to starve the competition, like MS giving away IE for free with Windows to starve Netscape.",
              "score": 8,
              "created_utc": "2026-01-02 19:36:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbsguw",
                  "author": "tedivm",
                  "text": "Llama 4 also changed their license so it really wasn't open source in any reasonable definition of the term.",
                  "score": 6,
                  "created_utc": "2026-01-02 21:41:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxancsj",
          "author": "Appropriate_Cry8694",
          "text": "He wasn't in charge, he was in a different division FAIR.",
          "score": 57,
          "created_utc": "2026-01-02 18:24:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxasnkt",
              "author": "MrPecunius",
              "text": "Yes, true. We are unlikely to get a better inside source, however.\n\nThis disclosure is the upside of LeCun's lack of filter; a few of the downsides are evident in the article.",
              "score": 26,
              "created_utc": "2026-01-02 18:48:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxbbvuj",
                  "author": "the320x200",
                  "text": "This is true, but also worth remembering he's been pretty anti-LLM, so need to take what he says against LLM projects with grain of salt too.",
                  "score": 12,
                  "created_utc": "2026-01-02 20:20:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxb67as",
                  "author": "Mochila-Mochila",
                  "text": "> a few of the downsides are evident in the article.\n\nSuch as ? Unvoluntarily burning bridges with his colleagues ?",
                  "score": 1,
                  "created_utc": "2026-01-02 19:53:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxaqi4a",
          "author": "insulaTropicalis",
          "text": "How can an organization like Meta, positioned strategically in generative AI at its beginning, waste everything while small labs thrive? There is some case study to build here.",
          "score": 51,
          "created_utc": "2026-01-02 18:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxasdmb",
              "author": "TheRealMasonMac",
              "text": "The lesson is: don't have dictators and inexperienced but well-connected individuals leading the company; which Meta clearly didn't learn. Google isn't perfect, but they generally apply a meritocratic model in comparison.",
              "score": 54,
              "created_utc": "2026-01-02 18:47:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxc7yu2",
                  "author": "Warm-Border-9789",
                  "text": "U.S. tech companies hate research, at their core, they are in the business of making money for investors as quickly as possible. They plan and execute quarter by quarter. On very rare occasions the rule is broken and someone succeeds despite the system to invent something new.",
                  "score": 8,
                  "created_utc": "2026-01-02 23:00:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxazdj7",
                  "author": "Super_Sierra",
                  "text": "Facebook needs to learn to do what other companies did with Musk, they shoved his ass into fake leadership roles, all nodded their heads when he takea charge and then do what actually needs to be done. Zuck is a tryhard who needs to be cordened off from making actual decision. \n\nThese tech companies will be studied for hundreds of years for so many things. \n\nGoogle on the other hand is run by non-founders and engineers, who, I know this might sound fucking insane, actually make something called a 'product' that people actually, you know, fucking use, so they tend to do it right so they can make something called 'money.'",
                  "score": 12,
                  "created_utc": "2026-01-02 19:20:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdnldq",
                  "author": "RobbinDeBank",
                  "text": "Google is a visionary company in AI that has invested in Brain to be a research powerhouse for a long time. They then acquired DeepMind and let them stay independent to let their long term research have time mature. Itâ€™s no surprise that with 2 of the most influential AI labs in history, Google becomes a leader in AI",
                  "score": 2,
                  "created_utc": "2026-01-03 03:56:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxatq1e",
              "author": "genshiryoku",
              "text": "Culture at Meta is toxic for research orientation (where most LLM gains come from)\n\nIt lived by the mantra \"move fast and break things\" Which is very fine if you are a software engineering company that wants to add features that can be rapidly rolled back. But it doesn't work so well when you have to plan and orchestrate a tight compute budget to do large training runs.\n\nThere's a reason why Anthropic hires Physicists with an academic background over engineers. AI is a different type of endeavor and therefor also benefits from a different type of work environment.\n\nMeta also has been tone deaf with their 9 digit offers to talent. Not realizing most of us in the industry are very *mission oriented* and not financially motivated at all. If anything that move probably pushed people away from Meta.\n\nA good example and confirmation of this concept has been Google which had 2 AI labs. One was ran as a classic software engineering hub called \"Google Brain\" This was the group behind the disastrous Google Bard. They also had a research oriented \"hands-off\" AI lab in London called \"DeepMind\". We all know how that played out.\n\nIt's for this same reason why Microsoft's AI products have fallen flat, They don't have a proper isolated research lab focused on AI and all their AI products are approached from a software engineering \"move fast and break things\" mindset which just doesn't work.",
              "score": 52,
              "created_utc": "2026-01-02 18:53:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbwlxi",
                  "author": "RhubarbSimilar1683",
                  "text": "They can afford to be mission oriented when they can just ask for 500k in salary and every place complies with that",
                  "score": 3,
                  "created_utc": "2026-01-02 22:01:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdo6or",
                  "author": "RobbinDeBank",
                  "text": "Generally agree, but underestimating Brain as a failure because of Bard is quite a take. Brain and DeepMind are two of the most influential AI labs in history, and they just need a merge to focus resources on Gemini.",
                  "score": 2,
                  "created_utc": "2026-01-03 04:00:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdy4ye",
                  "author": "SkyFeistyLlama8",
                  "text": "Microsoft building and then killing LLM frameworks hurts.",
                  "score": 1,
                  "created_utc": "2026-01-03 05:06:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxc76ys",
                  "author": "Imaginary_Belt4976",
                  "text": "Sorry but I dont believe mission is anywhere near relevant when a comp package like that is on the table ðŸ˜‚",
                  "score": -1,
                  "created_utc": "2026-01-02 22:56:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxar84i",
              "author": "a_slay_nub",
              "text": "To be fair, Llama was never in the lead (at least post 2022). They were simply the best open source models and they were extremely far behind SaaS SOTA.",
              "score": 13,
              "created_utc": "2026-01-02 18:42:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxaxtss",
              "author": "Ansible32",
              "text": "Really to me it seems like a product problem, not a research problem. Meta put together some stuff but once it became clear AI has nothing to do with improving their product, they stopped improving AI. The only thing Meta is using AI for is LLM summaries which if they are moderately successful will destroy Facebook Groups with their AI summaries nobody wants.\n\nContrast with Google, they have three different revenue streams they are building for AI: search has integrated a cheap LLM, it's ad-supported, this is their primary revenue stream and LLM fits in there perfectly.\n\nGemini is a paid chatbot with a freemium model.\n\nGemini also has APIs where you can pay per-query.\n\nMeta isn't using Llama in any way to drive revenue. Of course it's withering on the vine.",
              "score": 10,
              "created_utc": "2026-01-02 19:12:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxdyitu",
                  "author": "SkyFeistyLlama8",
                  "text": "Meta tried gatekeeping the kind of LLMs that can be integrated into WhatsApp. It's enshittification all the way down.\n\nFacebook only exists as an advertising platform with Usenet-style groups tacked on. Remove the user generated content and there's nothing left. I'm happy that countries like Australia have started banning social media apps and websites for younger users because it lets competitors rise up without having to fight for mindshare among new users.",
                  "score": 1,
                  "created_utc": "2026-01-03 05:09:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxe4cc4",
                  "author": "TracerBulletX",
                  "text": "Thats not really true? Itâ€™s a big part of the product strategy for the glasses which are pretty popular",
                  "score": 1,
                  "created_utc": "2026-01-03 05:52:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxaxnpa",
              "author": "paul__k",
              "text": "Zuckerberg ist just not that good of a CEO or technologist. He got lucky with the original FB, and he made some good acquisitions with Instagram and WhatsApp, but almost nothing else they have tried to build themselves over the last decade has really worked out. But they do have a lot of free cash flow, which makes the failures easy to ignore.\n\nAs for AI, they were never that good to begin with, because they mostly have second rate talent. That was enough to kind of keep up in the beginning, but now they have reached their limits. They need better people, but they can't get those unless they throw around ridiculous pay packages, because top tier talent doesn't want to ruin their careers by playing for a second division team.",
              "score": 12,
              "created_utc": "2026-01-02 19:12:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxb2bp0",
              "author": "LazloStPierre",
              "text": "It's ridiculous. I'm no fan of the open and proud Nazi, but he showed you can basically throw money at this and catch up to almost SOTA starting from a standing start. Some of the Chinese companies have started from far behind Meta and now are in that same category. How can Meta, with the money they've thrown at it and so much experience in the game be THAT far behind!?",
              "score": 10,
              "created_utc": "2026-01-02 19:34:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxavefk",
              "author": "Chogo82",
              "text": "Meta was not positioned strategically for generative AI. They had data centers but no AI infrastructure. They were pouring billions into the metaverse concept which had already been executed by second life/google glass over 10 years ago. They didnâ€™t make the pivot to gen AI until several major LLM tools were already available. With how fast AI innovation happens, they were definitely late to the game. Creating open source is a strategic way to break up the grip of the large players and distribute talent into smaller pockets. If you remember during the early days of social media, Facebook used this exact strategy to acquire/kill off a ton of social media competitors.",
              "score": 7,
              "created_utc": "2026-01-02 19:01:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxavsfl",
          "author": "Cool-Chemical-5629",
          "text": "At this point, I wouldn't be surprised if Behemoth model was just an empty promise from the beginning.",
          "score": 10,
          "created_utc": "2026-01-02 19:03:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbaj8k",
          "author": "PsychologicalOne752",
          "text": "With DeepSeek, GLM 4.7 and now IQuest Coder V1, China seems to have taken up the mantle of open-source LLMs and is delivering fast and in quality. Unfortunately, IMO, the US suffers from a lack of good leadership, where everyone wants to raise billions without adding actual value.",
          "score": 6,
          "created_utc": "2026-01-02 20:14:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxatb52",
          "author": "Golden_Jiggy",
          "text": "Sounds like defrauding shareholders to me ðŸ¤·â€â™‚ï¸",
          "score": 9,
          "created_utc": "2026-01-02 18:51:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbahg6",
              "author": "Competitive_Travel16",
              "text": "Defrauding end-users, even of open source products, is not the same thing.",
              "score": 4,
              "created_utc": "2026-01-02 20:13:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxf1o6t",
              "author": "MoffKalast",
              "text": "Probably why they were all fired afterwards.",
              "score": 1,
              "created_utc": "2026-01-03 10:35:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxcmcf8",
          "author": "Revolutionalredstone",
          "text": "Llama4 cheating on benchmarks is such a well-known fact that I've even seen it mentioned in official papers ðŸ˜† \n\nScout etc were an interesting experiment! But way too much focus on getting high numbers for a a model that rambled and was incoherent ðŸ˜†\n\nQwen has basically took over with their llama style project, I would love for llama 5 to be awesome ðŸ˜Ž",
          "score": 3,
          "created_utc": "2026-01-03 00:19:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfuycx",
              "author": "silenceimpaired",
              "text": "I would love llama 5. But it isnâ€™t likely to come or if it does it wonâ€™t be local",
              "score": 1,
              "created_utc": "2026-01-03 14:07:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxd0ud3",
          "author": "ditmarsnyc",
          "text": "post the FT link anyway, there is an archive website that can capture it\nedit: yes the archive dot ph site has captured it, will not post link to avoid automod filters",
          "score": 1,
          "created_utc": "2026-01-03 01:41:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbymmw",
          "author": "LanceThunder",
          "text": "lol a lot of people on the stock subs swear that meta is a sleeping giant and that they are going to get some deeeep value out of meta stocks once AI really takes off. impossible to explain to bagholders that they are wrong.",
          "score": 1,
          "created_utc": "2026-01-02 22:12:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfjlur",
              "author": "davikrehalt",
              "text": "Meta is objectively a sleeper giant",
              "score": 1,
              "created_utc": "2026-01-03 12:58:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbb8lz",
          "author": "doodlinghearsay",
          "text": "THIS IS BRAND NEW INFORMATION!",
          "score": -3,
          "created_utc": "2026-01-02 20:17:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxaf7x7",
          "author": "emprahsFury",
          "text": "So according to his words: LeCun, who was in charge, has his team fudge the numbers. Causing Zuck to lose confidence in the entire org and sideline it and then eventually replace it.  \n\nHow is this anything but a bad look on LeCun",
          "score": -54,
          "created_utc": "2026-01-02 17:47:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxag0bj",
              "author": "IJOY94",
              "text": "LeCun was in a different division? LeCun headed up FAIR, LLAMA 4 came out of MSL.",
              "score": 60,
              "created_utc": "2026-01-02 17:50:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxat8dp",
                  "author": "pm_me_github_repos",
                  "text": "Llama 4 came out of GenAI. MSL wasnâ€™t a thing til later",
                  "score": 8,
                  "created_utc": "2026-01-02 18:51:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxag0ta",
              "author": "No_Afternoon_4260",
              "text": "Was he in charge of L4? Can't remember",
              "score": 12,
              "created_utc": "2026-01-02 17:50:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxagap4",
                  "author": "TheRealMasonMac",
                  "text": "He said he didn't have anything to do with LLaMa apart from the first one, IIRC.",
                  "score": 29,
                  "created_utc": "2026-01-02 17:52:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pvxq2t",
      "title": "Hard lesson learned after a year of running large models locally",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/",
      "author": "inboundmage",
      "created_utc": "2025-12-26 06:38:00",
      "score": 341,
      "num_comments": 146,
      "upvote_ratio": 0.93,
      "text": "Hi all, go easy with me I'm new at running large models.\n\nAfter spending about 12 months tinkering with locally hosted LLMs, I thought I had my setup dialed in. Iâ€™m running everything off a workstation with a single RTXâ€¯3090, Ubuntu 22.04, llama.cpp for smaller models and vLLM for anything above 30â€¯B parameters. \n\nMy goal has always been to avoid cloud dependencies and keep as much computation offline as possible, so Iâ€™ve tried every quantization trick and caching tweak I could find.\n\nThe biggest friction point has been scaling beyond 13â€¯B models. \n\nEven with 24â€¯GB of VRAM, running a 70â€¯B model in int4 still exhausts memory when the context window grows and attention weights balloon. \n\nOffloading to system RAM works, but inference latency spikes into seconds, and batching requests becomes impossible. \n\nIâ€™ve also noticed that GPU VRAM fragmentation accumulates over time when swapping between models, after a few hours, vLLM refuses to load a model that would normally fit because of leftover allocations.\n\nMy takeaway so far is that local first inference is viable for small to medium models, but thereâ€™s a hard ceiling unless you invest in server grade hardware or cluster multiple GPUs. \n\nQuantization helps, but you trade some quality and run into new bugs. \n\nFor privacy sensitive tasks, the tradeâ€‘off is worth it; for fast iteration, itâ€™s been painful compared to cloud based runners. \n\nIâ€™m curious if anyone has found a reliable way to manage VRAM fragmentation or offload attention blocks more efficiently on consumer cards, or whether the answer is simply â€œbuy more VRAM.â€ \n\nHow are others solving this without compromising on running fully offline?\n\nThx",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvzpck6",
          "author": "Eugr",
          "text": "You've got it backwards. vLLM works great if model + context fit into VRAM, but it doesn't do CPU offloading well - use llama.cpp for anything that spills over to RAM. \n\nAlso, you can't fit 70B model in 4 bit quant into your 24GB, even with zero context. The weights alone would take 35GB. \n\nAlso, in memory constrained environments (and 24GB is not much as far as local LLMs are concerned) I'd default to llama.cpp as it is much more memory efficient than vLLM. So, unless you need some vllm specific features or models not supported in llama.cpp yet, use vLLM, otherwise just stick to llama.cpp. And again, only if everything fits into VRAM. \n\nWhen I just had my 4090, I wouldn't run dense models above 32B in q4 quant. I could run larger MoE, like gpt-oss-120b in llama.cpp just fine, thanks to experts offloading feature. Was getting around 40 t/s from it on Linux.",
          "score": 227,
          "created_utc": "2025-12-26 07:24:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0iv9m",
              "author": "mycall",
              "text": "Can llama.cpp load/host multiple models in parallel?",
              "score": 16,
              "created_utc": "2025-12-26 12:22:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0n2f9",
                  "author": "keyboardhack",
                  "text": "Yes that is now supported with the recently added router mode.",
                  "score": 24,
                  "created_utc": "2025-12-26 12:57:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw0lwrd",
                  "author": "ismaelgokufox",
                  "text": "You can using the groups feature of llama-swap which will run an instance of llama.cpp for each under the same endpoint.",
                  "score": 7,
                  "created_utc": "2025-12-26 12:47:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw0vz2o",
              "author": "munkiemagik",
              "text": ">use llama.cpp for anything that spills over to RAM.\n\nIsn't ik\\_llama.cpp the general recommend for when the intention is to split across GPU and CPU, particularly the ubergarm iq quants? I'm sure I read a few posts where the prompt processing and even token generation to a degree were reported as an improvement over vanilla llama.cpp but I cant remember now whether this applies to a specific group of LLMs or in general.",
              "score": 12,
              "created_utc": "2025-12-26 14:00:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1glfd",
                  "author": "Zc5Gwu",
                  "text": "Yes, it is generally a hair faster when offloading to ram for large models and their quants are SOTA but llama.cpp tends have better support, wider selection, tool calling might be better supported in some cases.",
                  "score": 3,
                  "created_utc": "2025-12-26 16:02:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1l846",
                  "author": "DataGOGO",
                  "text": "Yes",
                  "score": 1,
                  "created_utc": "2025-12-26 16:27:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1ej52",
              "author": "TechnicalGeologist99",
              "text": "vLLM is for production system and is defacto the only real choice for production. To run multiple models requires orchestration of multiple vLLM instances with liteLLM as a gateway.",
              "score": 7,
              "created_utc": "2025-12-26 15:51:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw55gj8",
                  "author": "hesperaux",
                  "text": "What about triton? I'm working on setting it up right now. Nobody talks about it, though. Am I wasting my time? I'm asking honestly.",
                  "score": 3,
                  "created_utc": "2025-12-27 04:54:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw6bpmm",
                  "author": "Seninut",
                  "text": "I have this setup working well. Cool how it can swap models sort of on the fly depending on size and throughput.",
                  "score": 1,
                  "created_utc": "2025-12-27 11:21:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw2vxxa",
                  "author": "Sufficient-Pause9765",
                  "text": "or docker",
                  "score": 0,
                  "created_utc": "2025-12-26 20:35:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwebi5q",
              "author": "Electrical_Heart_207",
              "text": "The 4090 memory constraints are real. Have you looked into any cloud options for when you need to run larger models, or do you just optimize for what fits locally?",
              "score": 1,
              "created_utc": "2025-12-28 17:52:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwexw1z",
                  "author": "Eugr",
                  "text": "Oh, I also have a Strix Halo machine and dual DGX Spark cluster, so I can run GLM 4.7 in AWQ 4-bit just fine. The memory bandwidth is not great, but thanks to tensor parallel I still get 16 t/s for GLM 4.6/4.7 and 40 t/s for MiniMax M2/2.1.\n\nI also use cloud when needed.",
                  "score": 1,
                  "created_utc": "2025-12-28 19:37:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzyx4j",
          "author": "Bitter-College8786",
          "text": "I still hope that one day a chinese manufacturer releases GPUs with 256GB VRAM",
          "score": 87,
          "created_utc": "2025-12-26 09:03:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw04mia",
              "author": "blbd",
              "text": "It's probably going to be AMD and Apple with fresh gear. Provided we can get the goddamn chips.Â ",
              "score": 22,
              "created_utc": "2025-12-26 10:03:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0z190",
                  "author": "g_rich",
                  "text": "You can already do this with Apple, up to 512GB of unified memory with the M3 Ultra. The M5 has already shown significant improvements with running LLMâ€™s because theyâ€™ve incorporated neural accelerators directly into the GPU cores. So itâ€™s likely weâ€™ll see a M5 Ultra Mac Studio in mid to late â€˜26 that will support up to 512GB of unified memory and has the potential to be a powerhouse machine for Ai. Apple wonâ€™t have a problem with acquiring chips, they would have already booked capacity with TSMC and have the resources to overcome any RAM shortages.",
                  "score": 15,
                  "created_utc": "2025-12-26 14:20:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwc00b2",
                  "author": "Budget-Juggernaut-68",
                  "text": "Scam Altman says Hi.",
                  "score": 2,
                  "created_utc": "2025-12-28 08:13:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw0izli",
              "author": "mycall",
              "text": "I think Intel will support 192GB VRAM on their nextgen x64 chipsets (including mobile).",
              "score": 3,
              "created_utc": "2025-12-26 12:23:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2t4jz",
                  "author": "MoffKalast",
                  "text": "Would be so funny and classic Intel if they upped support to 192, while keeping it dual channel only lmao.",
                  "score": 6,
                  "created_utc": "2025-12-26 20:19:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1ljfk",
                  "author": "DataGOGO",
                  "text": "more than that.",
                  "score": 2,
                  "created_utc": "2025-12-26 16:29:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw8e92t",
              "author": "MarkIII-VR",
              "text": "NVIDIA Robin Ultra chips will hopefully run with 256GB, but you may have to use a Spark system to get it.  If not, the Feynman chips will have to.  By then, any product not in the 256GB range won't be used for AI.\n\nI have not located any documentation detailing 256GB system or intended shipping configurations, but we have a while before those systems will be ready still.",
              "score": 1,
              "created_utc": "2025-12-27 18:47:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1xk2q",
              "author": "HeyWannaShrek",
              "text": "Aliexpress has it already ðŸ¤ª",
              "score": 1,
              "created_utc": "2025-12-26 17:32:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzknin",
          "author": "AppearanceHeavy6724",
          "text": "> Iâ€™ve also noticed that GPU VRAM fragmentation accumulates over time when swapping between models, after a few hours, vLLM refuses to load a model that would normally fit because of leftover allocations\n\nSounds very odd, as when CUDA unloads all GPU memory should get freed in bulk.",
          "score": 66,
          "created_utc": "2025-12-26 06:40:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2wulq",
              "author": "michaelsoft__binbows",
              "text": "Seems just a simple skill issue around OS process management. If the app has memory leaks the first port of call is to launch and teardown the entire process with your job unit, so each job gets a fresh worker process.\n\nObviously that has immediate impacts like 2 consecutive jobs utilizing the same model will then require the model to spend time loading into VRAM when issuing the work. But OP never detailed anything at this level so it's hard to say.",
              "score": 5,
              "created_utc": "2025-12-26 20:40:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzo3hi",
          "author": "DataGOGO",
          "text": "24GB of vram is a lot of VRAM for gaming, not for professional workloads, to include local inference.\n\nWith a single gaming GPU, and a consumer grade platform (AM5 etc) you are always going to be limited to very small models.\n\nYou could get one of the shared memory boxes (AMD/Mac/Spark/Jenson); you can run slightly larger models, but it will be slow, and to the best of my knowledge only the DGX spark has enough networking to really cluster two of them together (and even then, it is slow as hell).Â ",
          "score": 20,
          "created_utc": "2025-12-26 07:12:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0rz14",
              "author": "kaeptnphlop",
              "text": "Apple recently sent sets of 4 Mac Studios around to tech bloggers. Look up Jeff Gerling, he demonstrated how they all can work together.\n\nNot saying that networking is not an issue. He shows whatâ€™s possible at the moment with them.\n\nHe tried other options as well, one cluster based on Frameworks hardware and even a RaspberryPi clusterÂ ",
              "score": 6,
              "created_utc": "2025-12-26 13:33:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1l0am",
                  "author": "DataGOGO",
                  "text": "Yeah i saw, absolutely terrible. .",
                  "score": -1,
                  "created_utc": "2025-12-26 16:26:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw07jl0",
              "author": "ldn-ldn",
              "text": "It's not a lot for gaming either.",
              "score": -13,
              "created_utc": "2025-12-26 10:33:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0g3aw",
                  "author": "AmphibianFrog",
                  "text": "It's plenty for gaming. Hardly any cards have more than 24GB",
                  "score": 2,
                  "created_utc": "2025-12-26 11:57:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzspu1",
          "author": "Zyj",
          "text": "Get a second 3090",
          "score": 35,
          "created_utc": "2025-12-26 07:58:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw03z9u",
          "author": "Lissanro",
          "text": "I am running local models for more than 2 years actively, however the first time I tried running LLM locally was over 5 years ago. I had learned my own share of lessons along the way. What to do now depends entirely on your budget and goals (size of what models you need to run, like if it is just 70B you can get working well just by using a pair of 3090 cards on a gaming motherboard).\n\nFor me reasons to avoid cloud not only privacy, but also reliability (nobody can change or bring offline the model I am using until I myself decide to change it). If you feel like reading my personal story how I was solving this at various points in time, please feel free to keep reading.\n\nMy local experience with LLMs as I already mentioned begun years before they became practical to use. The first time I tried to run LLM locally is was back in GPT-2 days, later trying other early models like GPT-J, at the time on CPU only, and mostly out of curiosity but could not find any real-world use for them at the time.\n\nThen, years later, when ChatGPT was released, even though its capabilities were quite limited, it still was useful especially at basic boiler plate stuff or correcting things that simple search and replace cannot, making json file translations, etc. I started creating various workflows around it but quickly discovered that cloud model is unreliable: not only it can go down and become not accessible, or worse, they change it somehow without my consent so my workflows brake - the same prompts may start produce explanations or partial code instead of full code, or even refusals, even for basic things like json translations. In addition to that, I started to need privacy since begun working with data and code that I had no right to send to a third party, and did not want to send to the cloud my personal stuff either.\n\nThis was the point when I started to try to migrate to local LLMs for my daily tasks (or to be more precise, subset of my daily tasks that they could handle), starting with Llama-2 fine-tunes, and attempts to extend their context. I remember extending 4096 context up to 12288 at the cost of some quality degradation but still mostly usable. I even experimented with some community-made 120B models like Goliath, but on PC I had at the time it was really slow, I still could let it run overnight to generate few replies to choose from, mostly useful for creative writing and not really suitable for coding though.\n\nAnd, this is also the point when I started to struggle with a single GPU. At first I just had 3060 12GB and 16-core 5950X with 128 GB dual-channel RAM. It was quite slow, prompt processing using CPU+GPU was especially bad at the time... but I had limited budget, so only thing I could do was to buy one 3090 card.\n\nHaving 36 GB in total (3060 + 3090) I could ran highly quantized 70B models but results were quite bad, even though there was a special occasion when it make a huge difference, it wasn't really practical. There were some models around 30B mark started to appear, including intended for coding, that I could fully load in VRAM. I remember that DeepSeek started to release coding models (at least, this was when I discovered them). I could run those at good quantization and reasonably fast on my setup... but, wasn't quite satisfied with quality, and in the hope to run better models of 70B size, I bought second 3090 card, this gave me 60 GB VRAM in total (2x3090+3060).\n\nThen, new era of MoE models started, but first it was more like Mistral era, really. It started with the first Mixtral release, which I used for a while. Then bigger Mixtral 8x22B followed, along with WizardLM, along with some community merges and fine-tunes. This was also the point when I felt like I needed more VRAM, so I purchased yet another 3090 card, reaching 84 GB VRAM in total (3x3090+3060).\n\nEventually, Llama 3 was released, but its largest variant was 405B, and even before I decided what kind of hardware I need to run it, Mistral released Large 123B. At the time, I think I already got forth 3090, and put aside 3060, since plugging in four GPUs into a gaming motherboard with risers was already complicated enough (they were connected in x8 x8 x4 x1 configuration in terms of PCI-E lanes per card, using 30cm risers, and Add2PSU board to sync two PSUs).\n\nThis lasted me few more months... but in the beginning of 2025 when DeepSeek R1 and V3 came, I begun to realize that I need yet another upgrade. With 96GB VRAM + 128GB I somehow managed to run extremely quantized R1 at like around 1 token/s with small context, but it wasn't practical.\n\nThis pushed me to purchase 8-channel 1 TB of DDR4 3200MHz RAM, which was around $1600 at the time, and also approximately $800 and $1000 for a motherboard and an used CPU respectively (EPYC 7763). But, it was tricky to setup, I shared detailsÂ [here](https://www.reddit.com/r/LocalLLaMA/comments/1jtx05j/comment/mlyf0ux/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)Â my experience how to run large MoE models with ik\\_llama.cpp, it gave me \\~150 tokens/s prompt processing and 8 tokens/s generation with IQ4 quant... about the same for IQ4 quant of K2 when its first version came out later in 2025, followed by 0905 release and later K2 Thinking, which was quite special since was using INT4 in its original weights and needed some special Q4\\_X recipe to convert to GGUF while preserving the original quality.\n\nAnd this brings my story to today, where I mostly run K2 0905 or K2 Thinking depending on if I need the thinking capability for a task at hand, and sometimes DeepSeek Terminus for cases when I need an alternative solution. I think I was lucky though to upgrade when I did, because today 1 TB RAM would be out of my budget. I am just hoping it will be enough for my needs to get through 2026 and maybe 2027, before I need to upgrade once again.",
          "score": 50,
          "created_utc": "2025-12-26 09:56:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw10p28",
              "author": "Vusiwe",
              "text": "I had my old gaming 3080 10GB\n\nCould run 13b Q4 but ultimately but those were frustrating, also tested GPT-J briefly\n\nI tested a cloud instance to make sure I could run a 70b Q4 in the way Iâ€™d like\n\nJumped to a A6000 48GB on a new computer\n\nEven a 70b Q4 was very frustrating, finally Llama 3.3 70b Q4 came out and was the first/lowest one Iâ€™ve ever used that had a sliver of intelligence. Â Literally any other lower model is spitting out random word salad\n\nNo vLLM in use for me\n\nThen PRO 6000 96GB\n\n70b Q8\n\nBeen trying out GLM IQ2 with llama.cpp past few days, somewhat promising but also challenging\n\nNow getting more RAM to total 384GB, my mobo might not be able to support more than 512GB though. Â Too bad i didnâ€™t get more RAM earlier :(\n\nItâ€™s addicting\n\nWhat Q# of K2 could I run with 96 VRAM and 384 RAM?",
              "score": 4,
              "created_utc": "2025-12-26 14:30:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw13wtq",
                  "author": "Lissanro",
                  "text": "You can check [https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF](https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF) \\- there are three different IQ2 quants, from 270.133 GiB to 348.883 GiB. There is also [https://huggingface.co/ubergarm/Kimi-K2-Instruct-0905-GGUF](https://huggingface.co/ubergarm/Kimi-K2-Instruct-0905-GGUF) which is the latest non-thinking variant of K2.\n\nI also suggest putting context cache and as many full layers as you can on GPU, this will free up your RAM. For example, with 4x3090 which is also 96 GB, I can fit four full layers and 160K context cache at Q8. I shared detailsÂ [here](https://www.reddit.com/r/LocalLLaMA/comments/1jtx05j/comment/mlyf0ux/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) (the same link I already mentioned in the previous message, even though it shows example based on DeepSeek 671B, the command to run ik\\_llama.cpp would be similar for K2).\n\nThat said, for your rig I would recommend trying [https://huggingface.co/ubergarm/GLM-4.7-GGUF](https://huggingface.co/ubergarm/GLM-4.7-GGUF) \\- you should be able to easily run IQ5\\_K 250.635 GiB (6.008 BPW), likely will be much better for programming than IQ2 quant of Kimi K2. For creative writing and similar use cases that not require precision, IQ2 quants may be fine though.\n\nThe reason why I recommend Ubergarm quants because they are made specifically for ik\\_llama.cpp, and it has about twice as good prefill performance compared to mainline llama.cpp (last time I checked was few weeks ago, using Q4\\_X quant of K2 Thinking).",
                  "score": 5,
                  "created_utc": "2025-12-26 14:50:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw52pgu",
                  "author": "Ok-Bill3318",
                  "text": "In the past 12 months Iâ€™ve seen better results out of 20-30b models than llama 70b from 12 months ago. \n\nThere is huge progress in the small models right now.",
                  "score": 4,
                  "created_utc": "2025-12-27 04:34:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw411na",
                  "author": "HilLiedTroopsDied",
                  "text": "A REAP minimax m2 or future reap minim2.1 will be your best bet on 96GB vram. GLM4.6V or gptoss120b are also contenders.",
                  "score": 1,
                  "created_utc": "2025-12-27 00:29:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwe87rf",
              "author": "Electrical_Heart_207",
              "text": "What's been your biggest lesson learned on the hardware side? Curious if you've found any sweet spots for cost vs performance.",
              "score": 1,
              "created_utc": "2025-12-28 17:36:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwg960g",
                  "author": "Lissanro",
                  "text": "I should have begun with the server platform in the first place, but it was hard to predict that I will need it. I did not know much about tensor parallel requirements at first and this became of concern only after Mistral Large 123B release, and server hardware requirement became obvious only after R1 release. And now, situation changed again due to RAM and NVMe prices spiking - if I needed the same hardware now instead of being lucky and getting it about a year ago, I would be in a tough situation.\n\nAt the time of purchase though, 8-channel DDR4 3200MHz was the sweet spot of cost and performance, just around $1600 for 1 TB, while 12-channel DDR5 at the time was more than three times more expensive, and required many times more expensive CPU too, but since prompt processing fully handled by GPUs and partially token generation, performance boost would be zero for prompt processing and maybe 1.5x or so for token generation (since GPU speed will be the same, while faster RAM will provide only partial boost for GPU+CPU inference).\n\nObviously, right now things are very different. I no longer can recommend buying DDR4 memory - by the time prices become reasonable, it is possible 12-channel DDR6 will become available, even if expensive at first, it may at least reduce some market pressure from DDR5. Of course, this is just my guess of what may happen - no way to tell for sure. And ultimately if you need hardware now, it may be necessary to make compromises (depending on the available budget), like perhaps buying smaller amount of DDR5 memory for now, with possibility to upgrade later.",
                  "score": 1,
                  "created_utc": "2025-12-28 23:34:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw01qy2",
          "author": "meshreplacer",
          "text": "Just get a Mac Studio and run the MLX LLMs",
          "score": 11,
          "created_utc": "2025-12-26 09:33:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw52x62",
              "author": "Ok-Bill3318",
              "text": "This. For the cost of like 3-4 x090 gpus you can have a single machine that will do this inside 500w. Maybe not as many peak tokens/sec but thatâ€™s just a case of adding more nodes now RDMA over thunderbolt exists.",
              "score": 5,
              "created_utc": "2025-12-27 04:36:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw07w7y",
          "author": "Only_Situation_4713",
          "text": "I have 13 3090s and I run minimax m2 at 21 tokens a second output and 12000 tokens/s input running at max token window and at fp8. Your problem is that you don't have enough 3090s",
          "score": 19,
          "created_utc": "2025-12-26 10:37:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0kmwi",
              "author": "ElectronSpiderwort",
              "text": "Dang bro leave some 3090s for the rest of us",
              "score": 19,
              "created_utc": "2025-12-26 12:37:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1n5im",
              "author": "Hitkil07",
              "text": "I donâ€™t mean to intrude, but what do you do that requires so many 3090s?? Like Iâ€™m trying to think through all of the possible professional workload scenarios and nothing seems to come close to even needing that much hardware just for local llm inference. Perhaps youâ€™ve other work beyond llm inference which would make more sense, but I simply canâ€™t wrap my head around what possible use cases one has with LLMs that requires 13 3090s lol",
              "score": 6,
              "created_utc": "2025-12-26 16:37:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw20a63",
              "author": "-InformalBanana-",
              "text": "I get like 8 tg/s minimax m2 172b reap mxfp4-moe (at 1k written tokens, probably will fall with context size, 7.5 at 3k ctx...) on my 12gb vram 96gb ddr4 ram. Im surprised tg scales so badly even when you have so many gpus, but pp looks like it scales very good.\nIm interested how faster would tg/s be if it was latest generation consumer gpu like 5090 compared to 3090, smb any idea?",
              "score": 1,
              "created_utc": "2025-12-26 17:47:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw2e0mv",
              "author": "-InformalBanana-",
              "text": "What is like starting tg/s or on low context like 1k?Â ",
              "score": 1,
              "created_utc": "2025-12-26 18:58:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw2xu9g",
              "author": "michaelsoft__binbows",
              "text": "Can you share your power topology for this? 3 PSU's? 240v 20A circuit?",
              "score": 1,
              "created_utc": "2025-12-26 20:45:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3dufg",
                  "author": "Only_Situation_4713",
                  "text": "3 nodes (PCs with 4 each)",
                  "score": 3,
                  "created_utc": "2025-12-26 22:12:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwed7ca",
              "author": "Electrical_Heart_207",
              "text": "How do you manage that fleet? Curious about the operational overhead vs just renting compute when you need it.",
              "score": 1,
              "created_utc": "2025-12-28 18:00:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwk2ubz",
              "author": "lovreq",
              "text": "How you run fp8 when 3090s don't support it?",
              "score": 1,
              "created_utc": "2025-12-29 15:23:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwkomki",
                  "author": "Only_Situation_4713",
                  "text": "They emulate it by casting it to fp16 via marlin kernels.",
                  "score": 1,
                  "created_utc": "2025-12-29 17:07:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzr87l",
          "author": "keerekeerweere",
          "text": "Running a dual RTX3090 setup myself, even got an NVLink between them. For now, I'm just waiting with a third 3090, and have put it to work in another server board that simply avoids swapping models for lower end things like docling and simple inference calls. i've been told to use either 2/4/(6) or 8 gpu's for better performance. \n\nrunning meaningfull contexts without overspill to the cpu and main memory seems to be the trick, either for coding or for batched inference. the dual 3090 is running qwen3-coder 30b q4 with 96k context relatively comfortably. 128k seems to cause overspill to the cpu/main ram. combined with opencode brings quite decent results. will have to try some further variants. wanting to give nemotron 3 nano a shot, but had issues with tooling during coding.\n\nstill have to seriously dive into vllm territory, ollama and fiddling with context settings is just not very useful.\n\n  \nCost wise, the only justification is privacy and peace of mind, spending 600,- per 3090 GPU, a motherboard with at least 3x PCIE4 x16 slots is becoming very expensive these days.  I got lucky with a TRX40 asrock rack MB < 200 eur (seller claimed untested, it had bent pins on the cpu socket that i was able to fix), 64GB DDR4 quad channel ram, a threadripper 3960x helps (again a bit lucky before price hikes). but still we're talking like 2400,- eur. \n\nwhen i need the speed or large models i'm using large inference providers, with some of them now becoming available in Europe too at reasonable prices claiming privacy guarantees (a couple iso certifications). that bill didn't go above 15,- eur per month. granted larger workloads are done on my private setup. Still cost wise it's better to have some inference providers then to invest in hardware. \n\nI tend to just restart ollama when too much reloading of the models happens. usually keeping a *watch -n 2 nvidia-smi* and *btop* open to follow up on things. warming up the room during winter time running things locally is a bonus  :-)",
          "score": 15,
          "created_utc": "2025-12-26 07:43:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6hfwd",
              "author": "Electrical_Heart_207",
              "text": "Have you explored any of the newer GPU rental marketplaces, or do you find the privacy tradeoff still makes local hardware worth it for your use case? I'm curious about how people select a provider (there are plenty of options these days). I often find myself just going for the cheapest.",
              "score": 3,
              "created_utc": "2025-12-27 12:13:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw0a9q6",
              "author": "Keinsaas",
              "text": "Which providers for Europe are you talking about?",
              "score": 2,
              "created_utc": "2025-12-26 11:00:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6fcnf",
                  "author": "keerekeerweere",
                  "text": "* ovh\n* cortecs ai",
                  "score": 2,
                  "created_utc": "2025-12-27 11:55:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzrrsz",
          "author": "No-Comfortable-2284",
          "text": "instead of trying to use \"smarter bigger\" models to achieve whatever youre trying to achieve, its more reliable to use multiple parallel instances (via vllm for example) of smaller model that can communicate with each other in distinct roles to create a system that produces accurate results. \n\nno matter how big of a model u run, they will hallucinate and make mistakes individually, but you can create a system that will only provide the result you want this way. unless youre just tryna role play or smthn I suggest you look into this.",
          "score": 14,
          "created_utc": "2025-12-26 07:48:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0kfqd",
              "author": "ElectronSpiderwort",
              "text": "Can you explain why asking multiple small models in distinct roles is better than doing exactly the same role partitioning with a single large model? Other than speed of course, but it seems to me that even in small well-defined roles a larger model would do better and hallucinate less",
              "score": 3,
              "created_utc": "2025-12-26 12:35:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw10enc",
                  "author": "deadflamingo",
                  "text": "You're asking about the difference between a specialized model and a general purpose model. You'll get the same hallucination rate, but better results due to the focused training data of a smaller model.Â ",
                  "score": 2,
                  "created_utc": "2025-12-26 14:28:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzo9e2",
          "author": "huzbum",
          "text": "I am pretty happy with qwen3 30b q5 k_xl on my 3090.  I run it with llama.cpp server, and itâ€™s pretty reliable.",
          "score": 6,
          "created_utc": "2025-12-26 07:13:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw02ngl",
              "author": "Wo1v3r1ne",
              "text": "Ihve got a dual 3090, but my setup sucks with web-browsing and indexing on large codebases, if you could share I would love to explore your setup",
              "score": 2,
              "created_utc": "2025-12-26 09:43:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0e5mj",
          "author": "tarruda",
          "text": "> How are others solving this without compromising on running fully offline?\n\nLast year I spent $2.5k on a used Mac Studio M1 Ultra with 128G which I use only as LLM inference node on my LAN. I've overriden the default configuration to allow up to 125GB of the RAM to be shared with the GPU.\n\nWith this setup the biggest LLM I can run Q2_K quant of GLM 4.7 (which works surprisingly well, can reproduce some of the coding examples found online), 16K context and ~12 tokens/second.\n\nIMHO Mac studios are the most cost effective way to run LLMs at home. If you have the budget, I highly recommend getting a 512G M3 ultra to run deepseek at higher quants.",
          "score": 5,
          "created_utc": "2025-12-26 11:39:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0l5hl",
              "author": "skrshawk",
              "text": "That's an intriguing possibility since I have a M4 with 128GB.  How does this compare at this quant with smaller models, or to the API?  I'm also presuming the machine is effectively useless for any other purpose when running that model.",
              "score": 1,
              "created_utc": "2025-12-26 12:41:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2qyw0",
                  "author": "tarruda",
                  "text": "> How does this compare at this quant with smaller models, or to the API?\n\nThere should definitely be degradation against the API, but it is hard to determine how much it degrades. I've seen a few coding examples done against the API that I have been able to replicate against the Q2_K or UD-IQ2_M quants locally. TBH I haven't done extensive test to know for sure.\n\n> I'm also presuming the machine is effectively useless for any other purpose when running that model.\n\nYes. This is fine in my case because the Mac Studio has no other purpose in my LAN.",
                  "score": 1,
                  "created_utc": "2025-12-26 20:07:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nweb1lx",
              "author": "Electrical_Heart_207",
              "text": "Mac Studio for LLM inference is an interesting choice. Have you compared the total cost of ownership against cloud GPU options for your workloads?",
              "score": 1,
              "created_utc": "2025-12-28 17:50:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwixzu1",
                  "author": "tarruda",
                  "text": "I haven't done any comparison, but it is probably cheaper to use cloud options in the long run.\n\nTo me, the biggest factors that led me to prefer local inference are:\n\n- Privacy\n- Ensuring that I can always run LLMs predictably. By that I mean that cloud providers can change LLMs/versions without you knowing and you have no control. Also, it is possible that some providers are shut down due to regulations/censorshop.",
                  "score": 1,
                  "created_utc": "2025-12-29 10:51:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzv9pt",
          "author": "Such_Advantage_6949",
          "text": "How to solve the problem? Buying more GPUs",
          "score": 5,
          "created_utc": "2025-12-26 08:24:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0gcak",
          "author": "No_Programmer2705",
          "text": "I have been trying Mistral 24b with Mistral Vibe Cli for coding on a Mac Studio, works very very well, after cache warmup you get 20 tk/s using Q8_0 and soriginal KV Cache (non quantitized), with speculative decoding using Ministral 3B Q6, I have run several benchmarks in many different settings and this was the best result, others would run faster tk/s, but inference would take longer. I have some of them documented if anyone interested.",
          "score": 6,
          "created_utc": "2025-12-26 11:59:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzops3",
          "author": "Nixellion",
          "text": "I've been running Ollama on a 3090+3060 (24+12GB VRAM) for months without reboots, constantly swapping models, mostl 24-30B ones. I think I had memory fragmentation issue maybe once? \n\nBefore that I used Ooba webui, and as much as Inlove obba and exllama I just could not leave it unattended like this. \n\nI've been looking into running server off of llama.cpp (which I do use locally) or vllm but at the moment I am stuck on \"it just works\" with no strong enough insentive to switch, yet anyway.",
          "score": 12,
          "created_utc": "2025-12-26 07:18:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1b0w4",
              "author": "Royale_AJS",
              "text": "â€œIt just worksâ€ is a difficult place to leave.",
              "score": 5,
              "created_utc": "2025-12-26 15:32:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvznyel",
          "author": "Dontdoitagain69",
          "text": "You have to be creative, to run llm to accelerate your world flow either learn how to work with any model or pay subscription. I wrote a huge blockchain in c++ using a small PHI model . Most weights in these huge models, I will never need so why download all the extra junk. Study how small models respond to your prompt , how far they can go, how they use context and how they follow output structure. You can write a unit test to evaluate a model for agentic use and hit it like 10k times until you tune it to the max. Then you can use it to get useful out put out of it. At this point a ChatGPT or small llama are both useful at the same level. I never prompt build be a SaaS site, thatâ€™s what dumb vibe coders do and create programming massacre. You need to know design patterns, work in small modules, write abstraction yourself. Donâ€™t let models write core of your project ever. You can load multiple models and make them work on different components of your system. The only thing is you have to share overall design and data types between them, so when itâ€™s done you can just stitch them together like legos. Itâ€™s actually a better way to.",
          "score": 11,
          "created_utc": "2025-12-26 07:10:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzn2cf",
          "author": "dsanft",
          "text": "The biggest lesson learned for me was that if you want to do anything really cool, you need to dive into the C++ and do it yourself. \n\nI write my own code now and have my own inferencing engine. And I've learned so much doing it.",
          "score": 13,
          "created_utc": "2025-12-26 07:02:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzodfi",
              "author": "Recent_Double_3514",
              "text": "Tips on where to start?",
              "score": 3,
              "created_utc": "2025-12-26 07:14:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzoolg",
                  "author": "dsanft",
                  "text": "Checkout the Llama-cpp source and get it building in a dev container. Then start hacking around in the source. \n\nOnce you understand how things work internally, with tensors as numerical data and how the forward pass works with the ggml c library, try building your own, for a simple model architecture like Qwen2. Then go from there.",
                  "score": 9,
                  "created_utc": "2025-12-26 07:17:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzscde",
                  "author": "Maasu",
                  "text": "Get started by understanding what a lot of these models are doing if you don't already know... start with Kaparthys transformer Vidya https://youtu.be/kCc8FmEb1nY?si=Wyw0CaRSMB5xkaDE (he has a whole series on neural networks if you are fresh to ml) to get an understanding on what is going on.\n\nFrom there there he then has lama2.c repo that does the inference in c https://github.com/karpathy/llama2.c \n\nI'm about half way through this, been a bit stop start, so there might be a better route for learning, but figured I'd share mine",
                  "score": 8,
                  "created_utc": "2025-12-26 07:54:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzrb3f",
              "author": "dododragon",
              "text": "Curious, what kind of cool stuff can you do with your engine ?",
              "score": 1,
              "created_utc": "2025-12-26 07:44:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzs960",
                  "author": "dsanft",
                  "text": "Full cross socket, cross device tensor parallel, with my own jit custom kernels for CPU/GPU. With NUMA awareness so I can use the full dram bandwidth on my dual socket Xeon.\n\nhttps://github.com/ggml-org/llama.cpp/pull/16000#issuecomment-3602326606",
                  "score": 3,
                  "created_utc": "2025-12-26 07:53:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzrfap",
              "author": "bondaly",
              "text": "Could you elaborate on what you have been able to do after following this path? At first blush, it does look like considerable work to do that.",
              "score": 1,
              "created_utc": "2025-12-26 07:45:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvztk4y",
                  "author": "dsanft",
                  "text": "I started learning llama cpp in June, then started my own engine mid October. It's quite mature now but not ready for release yet. Full tensor parallel across sockets and across GPUs. With a few unique innovations that I came up with to keep ops in the integer domain while minimising quantisation error.\n\nhttps://github.com/ggml-org/llama.cpp/pull/16000#issuecomment-3602326606",
                  "score": 4,
                  "created_utc": "2025-12-26 08:07:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1e61g",
          "author": "TommarrA",
          "text": "And in another hard lesson learnt - sun rises in the east! \n\nNo offense but this much is basic information - yes we know LLM are VRAM intensive - 3T NVDA valuation shows that to be an immutable fact.",
          "score": 3,
          "created_utc": "2025-12-26 15:49:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzsif5",
          "author": "JLeonsarmiento",
          "text": "https://preview.redd.it/862slj9j8i9g1.jpeg?width=1279&format=pjpg&auto=webp&s=a5bb0161092125848479b8ab741633e9073aac0a",
          "score": 5,
          "created_utc": "2025-12-26 07:56:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1tlph",
              "author": "Bogaigh",
              "text": "â€œJust buy a MacBook Pro with M4Max chip and 128 gb of RAMâ€\nâ€œNo!â€",
              "score": 2,
              "created_utc": "2025-12-26 17:11:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzt6y6",
          "author": "lmpdev",
          "text": "> a reliable way to manage VRAM fragmentation\n\nI've been using large-model-proxy with multiple llama.cpp llama-server instances, ComfyUI, vLLM, forge, custom diffusers code. There is no \"fragmentation\" whatsoever in any of it if you kill the process.\n\nIf vllm is not fully unloading models, what you might want to is set up a separate vllm instance for each model and use larage-model-proxy to switch between them.\n\nllama-swap might be able to do this too.",
          "score": 2,
          "created_utc": "2025-12-26 08:03:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0a0r4",
          "author": "ozzeruk82",
          "text": "Llama.cpp is what you want for the bigger models, so you can have some layers on the 3090 and the rest in normal ram. VLLM and similar are great for where you know 100% will fit. So ironically the exact opposite to what you are doing. Personally I have a 3090 and find the qwen moe 30b model to work great. And I have played around with gpt oss 120 with most of it in ram. Is reasonably fast, fine for text chatting.   Good luck! I think your setup is pretty nice, though you didnâ€™t tell us how much system RAM you have?",
          "score": 2,
          "created_utc": "2025-12-26 10:58:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3tprk",
          "author": "Maximum-Wishbone5616",
          "text": "On my desktop for work I use 2x 5090 and it is fine, I find it much better at quick C# task than Max Claude Opus 4.5\n\nI ten to still use Opus for some tasks as Kilo is still not perfect for me in regards to agentic tasks (not quality of coding, but handling MCP, lots of errors with reading files that were just checked).\n\nI find that 100-200k is minimum on my codebase. 70-90k for chat.\n\nFor integration the local fine tuned much smaller models are destroying completely any cloud, due to high much deep integration is needed to get reliable integrated AI. \n\nRTX6000Pro helps a lot with bigger context/better models but it is better to deploy it for customers than for any desktop machines.",
          "score": 2,
          "created_utc": "2025-12-26 23:45:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhz72n",
              "author": "FX2021",
              "text": "Can you elaborate please? I find your comment interesting when you said \"local fine tuned much smaller models are destroying completely any cloud\"\n\n\nAlso can you give some examples of what you mean by \"fine tuned\" in this context please?",
              "score": 1,
              "created_utc": "2025-12-29 05:40:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw45qd5",
          "author": "hyper_ny",
          "text": "that's why i sold my server and gpus and bought mac studio m1 ultra 128gb for 2200. and another one for 2000. Using EXO. I built proxy server that can manage api keys for my application. not too fast but much better then 3090 single gpu.",
          "score": 2,
          "created_utc": "2025-12-27 00:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5oxsu",
          "author": "True_Requirement_891",
          "text": "We need 4b-7b models to get very very good. There has to be a path, distillation and RL come to mind but is there any new direction on improving small models?",
          "score": 2,
          "created_utc": "2025-12-27 07:39:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5tjh7",
          "author": "Dapper_Mammoth_2771",
          "text": "Thank you for sharing this. Iâ€™m going all in to buy hardware and was worried about this level of scale exactly. I need a private resource but canâ€™t see how itâ€™s affordable. I contemplated a two 24gb vram gpu rig to start. Thoughts? Seems like way more ideal are more GPUs running faster with less vram then cluster",
          "score": 2,
          "created_utc": "2025-12-27 08:23:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8fwgs",
          "author": "Life-Animator-3658",
          "text": "Iâ€™ve found that even 8B-14B models when paired together properly can give you the feel of those larger models, and even the quality. Thereâ€™s becoming less of a need to even need those larger models. \n\nAt this point weâ€™re just small multimodal chat agents away from doing it all ourselves. I build one for myself, and the quality works great tbh. \n\nIâ€™ve fallen in love with hot swapping specifically trained 8-14B models for local. And the response time doesnâ€™t even seem different than a chatGPT response. \n\nI had that same goal as you described. The answer isnâ€™t bigger models. Itâ€™s agentic chatbot behavior paired with these smaller models, RAG, and web searching.",
          "score": 2,
          "created_utc": "2025-12-27 18:55:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhyj2q",
              "author": "FX2021",
              "text": "Thank you! Can you explain more about what you mean by \"paired together\"?\n\nAlso what do mean by \"specifically trained\" models can you give some real world examples please?",
              "score": 1,
              "created_utc": "2025-12-29 05:35:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwm68hg",
                  "author": "Life-Animator-3658",
                  "text": "You need to create an agent essentially. If you are not familiar with that or donâ€™t want to do that, no point in reading further. Youâ€™d have to hope someone open sources something. \n\nI have a working example of using PHI3.5 mini, and Qwen 8B thinking models. \n\nYou make Phi create alternate queries based on the users query and chat history. Then you do a RAG pass with each query. Whittle the answers down with a reranking model so the context is small. \n\nAdd it to the original prompt and pass that to Qwen 8B to think about an answer. Once Qwen is done you then pass it back to Phi to extract only the answer from the thinking portion. Then display that answer to the user. \n\nYou leverage tiny models that are good at specific things to complement one another. You get surprisingly accurate results this way.",
                  "score": 2,
                  "created_utc": "2025-12-29 21:22:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwcvsk0",
          "author": "Subject-Mousse-5937",
          "text": "Just starting in this arena. Thanks for your insights",
          "score": 2,
          "created_utc": "2025-12-28 13:05:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzxxqi",
          "author": "tmvr",
          "text": "With 24GB VRAM it's possible to run up to 32B models. Running Gemma3 27B or Mistral Small 24B is perfectly possible at Q5 or even Q6. You can also run Qwen3 30B A3B or Nemotron 3 Nano with fast token generation even when putting some experts into system RAM. You can run gpt-oss 20B native with full 128K context and you'll have VRAM left over for another smaller model. Try llamacpp.\n\nEDIT: just to be clear, this is mostly about the stuff that fits into the VRAM, it is also possible to run 70B at Q4 as well of course, but I find it too slow even with DDR5-6400. Running gpt-oss 120B is fine though with 24GB VRAM and 64GB system RAM, the tg speeds are in the usable territory there.",
          "score": 3,
          "created_utc": "2025-12-26 08:53:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0yudu",
          "author": "Inevitable_Raccoon_9",
          "text": "I \"solved\" it by spending money on 128GB in a M4 Max MacStudio",
          "score": 2,
          "created_utc": "2025-12-26 14:18:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0zefh",
          "author": "Competitive_Ideal866",
          "text": "> Hard lesson learned after a year of running **large models** locally\n> \n> The biggest friction point has been scaling beyond **13 B models**.\n\nFirstly, 13B isn't large. The smallest models I actually use are ~4B. I most commonly use 14B (q8 via MLX) and 235B (q3_k_m via llama.cpp).\n\n> Even with 24 GB of VRAM, running a 70 B model in int4 still exhausts memory when the context window grows and attention weights balloon.\n\nYeah, 24GB is tiny. I have a machine with 32GB and I avoid using it for LLMs because it cannot run anything of much use. Mostly I use a 128GB M4 Max Macbook. I highly recommend it.\n\nI also tried an nVidia GPU in a Linux box and found it far too unreliable to be of use. In contrast, a Mac setup is rock solid.",
          "score": 2,
          "created_utc": "2025-12-26 14:22:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0drdl",
          "author": "noiserr",
          "text": "> My takeaway so far is that local first inference is viable for small to medium models, but thereâ€™s a hard ceiling unless you invest in server grade hardware or cluster multiple GPUs. \n\nWell you can run gpt-oss-120B and Minimax m2 on Strix Halo. It's not cheap but it's not exactly that expensive either.",
          "score": 1,
          "created_utc": "2025-12-26 11:35:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0iqfr",
          "author": "mycall",
          "text": "Do you automatically kill/restart vLLM when it refuses to load a model?  Is the command queuing durable between restarts?  It might slow things down but it might solve the backlog problem.",
          "score": 1,
          "created_utc": "2025-12-26 12:20:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0jqs3",
          "author": "fiery_prometheus",
          "text": "Even with 4 3090, I'm running into issues with slow token generation for the large agentic models, I would not be able to even run usable models for my use case before. If it's just for chatting, it's more manageable.Â ",
          "score": 1,
          "created_utc": "2025-12-26 12:29:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2mnkh",
          "author": "FoodAccurate5414",
          "text": "Just out of interest what are you gaining from using bigger models. More accuracy, more consistency. ?",
          "score": 1,
          "created_utc": "2025-12-26 19:44:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw339rf",
          "author": "Own-Lemon8708",
          "text": "Buy more VRAM. 48gb RTX 8000 is only around $16-1800. There are cheaper alternatives too.",
          "score": 1,
          "created_utc": "2025-12-26 21:15:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw35sf1",
          "author": "koflerdavid",
          "text": "VRAM fragmentation is due to Python having Garbage Collection and Pytorch also being bad about managing resources. vLLM might be necessary to run cutting-edge models, but I'd recommend using other software instead for casual or home use. Preferably llama.cpp or something downstream from that.",
          "score": 1,
          "created_utc": "2025-12-26 21:29:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3ryin",
          "author": "StickyBeast",
          "text": "You can play around with vllm configs on spot vms and get actuall vram requirements",
          "score": 1,
          "created_utc": "2025-12-26 23:35:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw41p51",
          "author": "Boring-Test5522",
          "text": "local LLM efficiency is not about performance but cost. I'm doing some stuff that I cannot do before because I can spam requests to my local LLM non-stop",
          "score": 1,
          "created_utc": "2025-12-27 00:33:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4vhr0",
          "author": "RegularPerson2020",
          "text": "M   O   E",
          "score": 1,
          "created_utc": "2025-12-27 03:44:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6qvzx",
          "author": "seba8282",
          "text": "Which Linux distro are you using? I'm considering moving from Windows just for AI dev.",
          "score": 1,
          "created_utc": "2025-12-27 13:26:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8iktv",
          "author": "MarkIII-VR",
          "text": "Please stop using Q2 quants. You will get better output from a smaller model at Q6.  I'm sure we can argue all day about Larger Q4 models vs. smaller models at Q6 or Q8, but Q2 is really just good for experimenting.  You lose too much going that small, especially with all of the work being done to make smaller models punch way above their weight class these days.",
          "score": 1,
          "created_utc": "2025-12-27 19:09:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc1cxo",
          "author": "SevereEngineering197",
          "text": "Does requiring running 70b model in int 4 CPU offloading cause I can only get a 70b model to run smoothly on 45/50GB VRAM and with a very small context len only 1000 and then I see a lot of errors or the context len gets too long fast",
          "score": 1,
          "created_utc": "2025-12-28 08:26:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx57288",
          "author": "Novel-Mechanic3448",
          "text": "One year of use, 1 week of knowledge somehow",
          "score": 1,
          "created_utc": "2026-01-01 21:26:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzw0p5",
          "author": "relicx74",
          "text": "You can also rent a card in the cloud for near local inference.",
          "score": 1,
          "created_utc": "2025-12-26 08:32:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1wwuf",
          "author": "Food4Lessy",
          "text": "A true llm pc is actually 64gb to 128gb vram.\n\n\n8gb-32gb vram will need to off load to cpu ram with speed bump ,sme2\n\n\nSingle gpu will always hit vram limit unless its the 96gb 6000.\n\n\nOr go with AMD 395 128gb, DGX Spark 128gb, Apple Max 64-128gb for $1000-3000. NPU should give an extra token boost.",
          "score": 1,
          "created_utc": "2025-12-26 17:29:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw274va",
          "author": "StardockEngineer",
          "text": "Ban this dude/bot already.  177 upvotes for what?  This post offers nothing. Paid for them.",
          "score": 0,
          "created_utc": "2025-12-26 18:22:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4c93e",
              "author": "T_UMP",
              "text": "https://preview.redd.it/aic2jiu4in9g1.png?width=221&format=png&auto=webp&s=b366ab5d2898a565f85d6ee9ca412bb54fe44b16",
              "score": 1,
              "created_utc": "2025-12-27 01:39:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzsmv7",
          "author": "iamreddituserhi",
          "text": "Check  you ram have bad sectors",
          "score": 0,
          "created_utc": "2025-12-26 07:57:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0j2zw",
          "author": "dazzou5ouh",
          "text": "\"unless you invest in server grade hardwarÃ«\" doesn't have to be if you know where to look. A consumer grade Asus Rampage V Extreme is a very old motherboard that can run 4 3090s at PCIe 3.0 x16/x8/x8/x8 which is more than enough for inference. A mining open frame costs 15 dollars nowadays.",
          "score": 0,
          "created_utc": "2025-12-26 12:24:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw305kg",
              "author": "michaelsoft__binbows",
              "text": "x399 with zen 1 has way better single thread performance while still being fairly cheap. i have x99 and x399 systems and i would use the latter first before dipping into the slow x99 ones. But they do represent nice value and 4 channels of DDR4 is pretty useful.",
              "score": 1,
              "created_utc": "2025-12-26 20:58:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0jy5p",
          "author": "Past-Grapefruit488",
          "text": "For some of the use cases, renting GPUs might be easier. You get full access to docker instances and E2E connectivity.",
          "score": 0,
          "created_utc": "2025-12-26 12:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzm6gg",
          "author": "SrijSriv211",
          "text": "I'd say if you can, try running local models on a M4 MacBook Pro. I don't own a MacBook Pro but someone I know does. They don't really run models larger than 70B as far as I know, but their experience has been really good in general.\n\nPersonally for me, I don't run models larger than 8B on my PC.\n\n> or whether the answer is simply â€œbuy more VRAM.â€\n\nyeah, I think you should try upgrading to RTX 50 series.",
          "score": -5,
          "created_utc": "2025-12-26 06:54:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzmxna",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 0,
              "created_utc": "2025-12-26 07:01:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzo7dd",
                  "author": "SrijSriv211",
                  "text": "I don't own a macbook. I think they own a 128 GB unified memory macbook.",
                  "score": 0,
                  "created_utc": "2025-12-26 07:13:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pwh0q9",
      "title": "Best Local LLMs - 2025",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/",
      "author": "rm-rf-rm",
      "created_utc": "2025-12-26 22:31:28",
      "score": 339,
      "num_comments": 170,
      "upvote_ratio": 0.97,
      "text": "***Year end thread for the best LLMs of 2025!***\n\n2025 is almost done! Its been **a wonderful year** for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!\n\n**The standard spiel:**\n\nShare what your favorite models are right now **and why.** Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.\n\n**Rules**\n\n1. Only open weights models\n\n\n\n*Please thread your responses in the top level comments for each Application below to enable readability*\n\n**Applications**\n\n1. **General**: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation\n2. **Agentic/Agentic Coding/Tool Use/Coding**\n3. **Creative Writing/RP**\n4. **Speciality**\n\nIf a category is missing, please create a top level comment under the Speciality comment\n\n\n\n**Notes**\n\nUseful breakdown of how folk are using LLMs: [https://preview.redd.it/i8td7u8vcewf1.png?width=1090&format=png&auto=webp&s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d](https://preview.redd.it/i8td7u8vcewf1.png?width=1090&format=png&auto=webp&s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d)  \n\n\nA good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)\n\n* Unlimited: >128GB VRAM \n* Medium: 8 to 128GB VRAM\n* Small: <8GB VRAM",
      "is_original_content": false,
      "link_flair_text": "Megathread",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nw3h6y7",
          "author": "rm-rf-rm",
          "text": "**GENERAL**",
          "score": 1,
          "created_utc": "2025-12-26 22:31:44",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nw61hxp",
          "author": "cibernox",
          "text": "I think having a single category from 8gb to 128gb is kind of bananas.",
          "score": 101,
          "created_utc": "2025-12-27 09:41:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8ogkb",
              "author": "rm-rf-rm",
              "text": "Thanks for the feedback. The tiers were from a commenter in the last thread and I was equivocating on adding more steps, but 3 seemed like a good, simple thing that folk could grok easily. Even so, most commenters arent using the tiers at all\n\nNext time I'll add a 64GB breakpoint.",
              "score": 0,
              "created_utc": "2025-12-27 19:40:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw8yesd",
                  "author": "cibernox",
                  "text": "Even that us too much of a gap. A lot of users of local models run them on high end gaming gpus. I bet that over half the users in this subreddit have 24-32gb of VRAM or less, where models around 32B play, or 70-80B if they are MoEs and use a mix of vram and system ram. \n\nThis is also the most interesting terrain as there are models in this size that run on non-enthusiast consumer hardware and fall within spitting distance of SOTA humongous models in some usages.",
                  "score": 29,
                  "created_utc": "2025-12-27 20:34:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwcapnl",
                  "author": "zp-87",
                  "text": "I had one gpu with 16GB of VRAM for a while. Then I bought another one and now I have 32GB of VRAM. I think this and 24GB + (12GB, 16GB or 24GB) is a pretty common scenario. We would not fit in any of these categories. For larger VRAM you have to invest a LOT more and go with unified memory or do a custom PSU setup and PCI-E bifurcation.",
                  "score": 5,
                  "created_utc": "2025-12-28 09:57:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw5w4j6",
          "author": "GroundbreakingEmu450",
          "text": "How about RAG for technical documentation? Whats the best embedding/LLM models combo?",
          "score": 22,
          "created_utc": "2025-12-27 08:48:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnn54o",
              "author": "da_dum_dum",
              "text": "Yes please, this would be so good",
              "score": 3,
              "created_utc": "2025-12-30 02:04:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw42tlc",
          "author": "Amazing_Athlete_2265",
          "text": "My two favorite small models are Qwen3-4B-instruct and LFM2-8B-A1B. The LFM2 model in particular is surprisingly strong for general knowledge, and very quick. Qwen-4B-instruct is really good at tool-calling. Both suck at sycophancy.",
          "score": 31,
          "created_utc": "2025-12-27 00:40:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwakdvy",
              "author": "zelkovamoon",
              "text": "Seconding LFM2-8B A1B; Seems like a MOE model class that should be explored more deeply in the future. The model itself is pretty great in my testing; tool calling can be challenging, but that's probably a skill issue on my part. It's not my favorite model; or the best model; but it is certainly good. Add a hybrid mamba arch and some native tool calling on this bad boy and we might be in business.",
              "score": 5,
              "created_utc": "2025-12-28 01:59:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw8p0u4",
              "author": "rm-rf-rm",
              "text": "One of the two mentions for LFM! Been wanting to give it a spin - how does it comare to Qwen3-4B? \n\nP.S: You didnt thread your comment in the GENERAL top level comment..",
              "score": 4,
              "created_utc": "2025-12-27 19:43:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw3hc33",
          "author": "rm-rf-rm",
          "text": "**Writing/Creative Writing/RP**",
          "score": 27,
          "created_utc": "2025-12-26 22:32:33",
          "is_submitter": true,
          "replies": [
            {
              "id": "nw3ky4l",
              "author": "Unstable_Llama",
              "text": "Recently I have used Olmo-3.1-32b-instruct as my conversational LLM, and found it to be really excellent at general conversation and long context understanding. It's a medium model, you can fit a 5bpw quant in 24gb vram, and the 2bpw exl3 is still coherent at under 10gb. I highly it recommend for claude-like conversations with the privacy of local inference.\n\nI especially like the fact that it is one of the very few FULLY open source LLMs, with the whole pretraining corpus and training pipeline released to the public. I hope that in the next year, Allen AI can get more attention and support from the open source community.\n\nDense models are falling out of favor with a lot of labs lately, but I still prefer them over MoEs, which seem to have issues with generalization. 32b dense packs a lot of depth without the full slog of a 70b or 120b model.\n\nI bet some finetunes of this would slap!",
              "score": 43,
              "created_utc": "2025-12-26 22:53:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3lrjn",
                  "author": "rm-rf-rm",
                  "text": "i've been meaning to give the Ai2 models a spin - I do think we need to support them more as an open source community. Their literally the only lab that is doing actual open source work. \n\nHow does it compare to others in its size category for conversational use cases - Gemma3 27B, Mistral Small 3.2 24B come to mind as the best in this area",
                  "score": 11,
                  "created_utc": "2025-12-26 22:57:54",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3wtek",
              "author": "a_beautiful_rhind",
              "text": "A lot of models from 2024 are still relevant unless you can go for the big boys like kimi/glm/etc. \n\nDidn't seem like a great year for self-hosted creative models.",
              "score": 15,
              "created_utc": "2025-12-27 00:04:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4qzsy",
                  "author": "EndlessZone123",
                  "text": "Every model released this year seems to have agentic and tool calling to the max as a selling point.",
                  "score": 17,
                  "created_utc": "2025-12-27 03:14:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw4th0o",
                  "author": "skrshawk",
                  "text": "I really wanted to see more finetunes of GLM-4.5 Air and they didn't materialize.  Iceblink v2 was really good and showed the potential of what a small GPU for the dense layers and context with consumer DDR5 could do with a mid-tier gaming PC with extra RAM.\n\nNow it seems like hobbyist inference could be on the decline due to skyrocketing memory costs.  Most of the new tunes have been in the 24B and lower range, great for chatbots, less good for long-form storywriting with complex worldbuilding.",
                  "score": 5,
                  "created_utc": "2025-12-27 03:30:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3x42f",
              "author": "Barkalow",
              "text": "Lately I've been trying [TareksGraveyard/Stylizer-V2-LLaMa-70B](https://huggingface.co/TareksGraveyard/Stylizer-V2-LLaMa-70B) and it never stops surprising me how fresh it feels vs other models. Usually it's very easy to notice the LLM-isms, but this one does a great job of being creative",
              "score": 7,
              "created_utc": "2025-12-27 00:06:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw6w0wa",
              "author": "theair001",
              "text": "Haven't tested that many models this year, but i also didn't get the feeling we got any breakthrough anyway.\n\n&nbsp;\n\n**Usage:** complex ERP chats and stories (100% private for obvious reasons, focus on believable and consistent characters and creativity, soft/hard-core, much variety)\n\n**System:** rtx 3090 (24gb) + rtx 2080ti (11gb) + amd 9900x + 2x32gb ddr5 6000\n\n**Software:** Win11, oobabooga, mainly using 8k ctx, lots of offloading if not doing realtime voice chatting\n\n&nbsp;\n\nMedium-medium (32gb vmem + up to 49gb sysmem at 8k ctx, q8 cache quant):\n\n* *Strawberrylemonade-L3-70B-v1.1* - i1-Q4_K_M (more depraved)\n* *Midnight-Miqu-103B-v1.5* - IQ3_S (more intelligent)\n* *Monstral-123B-v2* - Q3_K_S (more universal, more logical, also very good at german)\n* *DeepSeek-R1-Distill-Llama-70B-Uncensored-v2-Unbiased-Reasoner* - i1-Q4_K_M (complete hit and miss - sometimes better than the other, but more often completely illogical/dumb/biased, only useful for summaries)\n* *BlackSheep-Large* - i1-Q4_K_M (the original source seems to be gone, sometimes toxic (was made to emulate toxic internet user) but can be very humanlike)\n\nMedium-small (21gb vmem at 8k ctx, q8 cache quant):\n\n* *Strawberrylemonade-L3-70B-v1.1* - i1-IQ2_XS (my go-to model for realtime voice chatting (ERP as well as casual talking), surprisingly good for a Q2)\n\n&nbsp;\n\n*Additional blabla:*\n\n* For 16k+ ctx, i use q4 cache quant\n* manual gpu-split to better optimize\n* got a ~5% oc on my gpus but not much, cpu runs on default but i usually disable pbo which saves 20~30% on power at 5-10% speed reduction, well worth it\n* for stories (not chats), it's often better to first use *DeepSeek-R1-Distill-Llama-70B-Uncensored-v2-Unbiased-Reasoner* to think long about the task/characters but then stop and let a different model write the actual output\n* Reasoning models are disappointingly bad. They lack self-criticism and are way too biased, not detecting obvious lies, twisting given data so it fit's their reasoning instead of the other way around and selectively chosing what information to ignore and what to focus on. Often i see reasoning models do a fully correct analysis only to completly turn around and give a completely false conclusion.\n* i suspect i-quants to be worse at non standard tasks than static quants but need to test that by generating my own i-matrix based on ERP stuff\n* all LLM (including openai, deepseek, claude, etc.) severely lack human understanding and quickly revert back to slop without constant human oversight\n* we need more direct human-on-human interaction in our datasets - would be nice if a few billion voice call recordings would leak\n* open source ai projects have awful code and i could traumadump for hours on end",
              "score": 11,
              "created_utc": "2025-12-27 14:00:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw5vdv3",
              "author": "Lissanro",
              "text": "For me, Kimi K2 0905 is the winner in the creative writing category (I run IQ4 quant in ik_llama.cpp on my PC). It has more intelligence and less sycophancy than most other models. And unlike K2 Thinking it is much better at thinking in-character and correctly understanding the system prompt without overthinking.",
              "score": 5,
              "created_utc": "2025-12-27 08:41:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw4dyo3",
              "author": "ttkciar",
              "text": "I use Big-Tiger-27B-v3 for generating Murderbot Diaries fanfic, and Cthulhu-24B for other creative writing tasks.\n\nMurderbot Diaries fanfic tends to be violent, and Big Tiger does really, really well at that.  It's a lot more vicious and explicit than plain old Gemma3.  It also does a great job at mimicking Marsha Wells' writing style, given enough writing samples.\n\nFor other kinds of creative writing, Cthulhu-24B is just more colorful and unpredictable.  It can be hit-and-miss, but has generated some real gems.",
              "score": 8,
              "created_utc": "2025-12-27 01:49:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw66ule",
                  "author": "john1106",
                  "text": "hi. can i use big tiger 27b v3 to generate me the uncensored fanfic story i desired? would you recommend kobold or ollama to run the model? also which quantization model can fit entirely in my rtx 5090 without sacrificing much quality from unquantized model? i'm aware that 5090 cannot run full size model",
                  "score": -4,
                  "created_utc": "2025-12-27 10:34:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw47vp0",
              "author": "Kahvana",
              "text": "Rei-24B-KTO ([https://huggingface.co/Delta-Vector/Rei-24B-KTO](https://huggingface.co/Delta-Vector/Rei-24B-KTO))\n\nMost used personal model this year, many-many hours (250+, likely way more).\n\nCompared to other models I've tried over the year, it follows instructions well and is really decent at anime and wholesome slice-of-life kind of stories, mostly wholesome ones. It's trained on a ton of sonnet 3.7 conversations and spatial awareness, and it shows. The 24B size makes it friendly to run on midrange GPUs.\n\nSetup: sillytavern, koboldcpp, running on a 5060 ti at Q4\\_K\\_M and 16K context Q8\\_0 without vision loaded. System prompt varied wildly, usually making it a game master of a simulation.",
              "score": 6,
              "created_utc": "2025-12-27 01:11:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4bmc6",
                  "author": "IORelay",
                  "text": "How do you fit the 16k context when you the model itself is almost completely filling the VRAM?Â ",
                  "score": 1,
                  "created_utc": "2025-12-27 01:34:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3yazi",
              "author": "Gringe8",
              "text": "I tried many models and my favorite is shakudo. I do shorter replies like 250-350 tokens for more roleplay like experience than storytelling.\n\nhttps://huggingface.co/Steelskull/L3.3-Shakudo-70b\n\nI also really like the new cydonia. I didnt really like the magdonia version.\n\nhttps://huggingface.co/TheDrummer/Cydonia-24B-v4.3\n\nEdit: after trying magdonia again its actually good too, try both",
              "score": 4,
              "created_utc": "2025-12-27 00:13:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwi2i1n",
                  "author": "TheLocalDrummer",
                  "text": "Why not?",
                  "score": 2,
                  "created_utc": "2025-12-29 06:06:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwsoyoe",
                  "author": "theair001",
                  "text": "So... i tried the L3.3-Shakudo 70b for a few hours and... it's dumb as fuck. It's by far the dumbest 70b model i've ever tested. It often repeats itself, is extremely agreeable and makes lots of logical/memory mistakes. I mean, the explicit content is good, don't get me wrong. For simple, direct ERP it's pretty good i guess. But... am i doing something wrong? I've tried a few presets including the suggested settings from huggingface. Do you have some special system prompt or special settings?",
                  "score": 1,
                  "created_utc": "2025-12-30 20:51:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw8yedb",
              "author": "swagonflyyyy",
              "text": "Gemma3-27b-qat",
              "score": 2,
              "created_utc": "2025-12-27 20:34:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw8frc7",
              "author": "AppearanceHeavy6724",
              "text": "Mistral Small 3.2. Dumber than Gemma 3 27b, perhaps just slightly smarter at fiction than Gemma 3 12b, but has punch of Deepseek V3 0324 it is almost certainly is distilled from.",
              "score": 1,
              "created_utc": "2025-12-27 18:55:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwbpy0p",
              "author": "Sicarius_The_First",
              "text": "I'm gonna recommend my own:\n\n12B:  \n**Impish\\_Nemo\\_12B**  \n**Impish\\_Nemo\\_12B**\n\n**Phi-lthy4**\n\n8B:  \n**Dusk\\_Rainbow**",
              "score": 1,
              "created_utc": "2025-12-28 06:40:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw8g95z",
              "author": "OcelotMadness",
              "text": "GLM 4.7 is the GOAT for me right now. Like its very slow on my hardware even at IQ3 but it literally feels like how AI Dungeon did when it FIRST came out and was still a fresh thing. It feels like how claude opus did when I tried it. It just kind of remembers everything, and picks up on your intent in every action really well.",
              "score": 0,
              "created_utc": "2025-12-27 18:57:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw8na60",
          "author": "rainbyte",
          "text": "My favourite models for daily usage:\n\n- Up to 96Gb VRAM:\n  - GLM-4.5-Air:AWQ-FP16Mix (for difficult tasks)\n- Up to 48Gb VRAM:\n  - Qwen3-Coder-30B-A3B:Q8 (faster than GLM-4.5-Air)\n- Up to 24Gb VRAM:\n  - LFM2-8B-A1B:Q8 (crazy fast!)\n  - Qwen3-Coder-30B-A3B:Q4\n- Up to 8Gb VRAM:\n  - LFM2-2.6B-Exp:Q8\n  - Qwen3-4B-2507:Q8 (for real GPU, avoid on iGPU)\n- Laptop iGPU:\n  - LFM2-8B-A1B:Q8 (my choice when I'm outside without GPU)\n  - LFM2-2.6B-Exp:Q8 (better than 8B-A1B on some use cases)\n  - Granite4-350m-h:Q8\n- Edge & Mobile devices:\n  - LFM2-350M:Q8 (fast but limited)\n  - LFM2-700M:Q8 (fast and good enough)\n  - LFM2-1.2B:Q8 (a bit slow, but more smart)\n\nI recently tried these and they worked:\n\n- ERNIE-4.5-21B-A3B (good, but went back to Qwen3-Coder)\n- GLM-4.5-Air:REAP (dumber than GLM-4.5-Air)\n- GLM-4.6V:Q4 (good, but went back to GLM-4.5-Air)\n- GPT-OSS-20B (good, but need to test it more)\n- Hunyuan-A13B (I don't remember to much about this one)\n- Qwen3-32B (good, but slower than 30B-A3B)\n- Qwen3-235B-A22B (good, but slower and bigger than GLM-4.5-Air)\n- Qwen3-Next-80B-A3B (slower and dumber than GLM-4.5-Air)\n\nI tried these but didn't work for me:\n\n- Granite-7B-A3B (output nonsense)\n- Kimi-Linear-48B-A3B (couldn't make it work with vLLM)\n- LFM2-8B-A1B:Q4 (output nonsense)\n- Ling-mini (output nonsense)\n- OLMoE-1B-7B (output nonsense)\n- Ring-mini (output nonsense)\n\nTell me if you have some suggestion to try :)\n\nEDIT: I hope we get more A1B and A3B models in 2026 :P",
          "score": 11,
          "created_utc": "2025-12-27 19:34:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1dgji",
              "author": "Miserable-Dare5090",
              "text": "Nemotron 30a3 is the fastest I have used, sys prompt matters, but well crafted its good tool caller and creates decent code.",
              "score": 2,
              "created_utc": "2026-01-01 05:21:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2hbgn",
                  "author": "rainbyte",
                  "text": "How do you think Nemotron-30B-A3B compares against Qwen3-Coder-30B-A3B?\n\nHappy new year :)",
                  "score": 1,
                  "created_utc": "2026-01-01 12:01:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw4fsfh",
          "author": "Foreign-Beginning-49",
          "text": "Because I lived through the silly exciting wonder of teh tinyLlama hype I have fallen in with LFM2-1.2B-Tool gguf 4k quant at 750mb or so, this thing is like Einstein compared to tinlyllama, tool use and even complicated dialogue assistant possibilities and even basic screenplay generations it cooks on mid level phone hardware. So grateful to get to witness all this rapid change in first person view. Rad stuff. Our phones are talking back.Â \n\n\nAlso wanna say thanks to qwen folks for all consumer gpu sized models like qwen 4b instruct and the 30b 3a variants including vl versions. Nemotron 30b 3a is still a little difficult to get a handle on but it showed me we are in a whole new era of micro scaled intelligence in little silicon boxes with it ability to 4x generation speed and huge context with llama.cpp on 8k quant cache settings omgg chefs kiss. Hopefully everyone is having fun and the builders are building and the tinkerers are tinkering and the roleplayers are going easy on their Ai S.O.'s Lol best of wishes",
          "score": 15,
          "created_utc": "2025-12-27 02:01:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3ha2d",
          "author": "rm-rf-rm",
          "text": "**Agentic/Agentic Coding/Tool Use/Coding**",
          "score": 23,
          "created_utc": "2025-12-26 22:32:14",
          "is_submitter": true,
          "replies": [
            {
              "id": "nw3t5a8",
              "author": "Past-Economist7732",
              "text": "Glm 4.6 (havenâ€™t had time to upgrade to 4.7 or try minimax yet). Use in opencode with custom tools for ssh, ansible, etc. \n\nLocally I only have room for 45,000 tokens rn, using 3 rtx 4000 Adaâ€™s (60GB vram combined) and 2 c 64 core emerald rapids es with 512GB of DDR5.  I use  ik_llama and the ubergarm iqk5 quants.  I believe the free model in opencode is glm as well, so if I know the thing Iâ€™m working on doesnâ€™t leak any secrets Iâ€™ll swap to that.",
              "score": 11,
              "created_utc": "2025-12-26 23:42:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3kxn9",
              "author": "Zc5Gwu",
              "text": "Caveat: models, this year started needing reasoning traces to be preserved across responses but not every client handled this at first. Many people complained about certain models not knowing that this might have been a client problem.\n\nminimax m2 - Incredibly fast and strong and runnable on reasonable hardware for its size.\n\ngpt-oss-120b - Fast and efficient.",
              "score": 24,
              "created_utc": "2025-12-26 22:53:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4nleo",
                  "author": "onil_gova",
                  "text": "Gpt-oss-120 with Claude Code and CCR ðŸ¥°",
                  "score": 2,
                  "created_utc": "2025-12-27 02:51:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3lsog",
              "author": "Dreamthemers",
              "text": "GPT-OSS 120B with latest Roo Code.\n\nRoo switched to Native tool calling, works better than old xml method. (No need for grammar files with llama.cpp anymore)",
              "score": 24,
              "created_utc": "2025-12-26 22:58:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3n3cx",
                  "author": "Particular-Way7271",
                  "text": "That's good, I get like 30% less t/s when using a grammar file with gpt-oss-120b and llama.cpp",
                  "score": 9,
                  "created_utc": "2025-12-26 23:05:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3nmpr",
                  "author": "rm-rf-rm",
                  "text": "> Roo switched to Native tool calling, \n\nwas this recent? wasnt aware of this. I was looking to move to kilo as roo was having intermittent issues with gpt-oss-120b (and qwen3-coder)",
                  "score": 4,
                  "created_utc": "2025-12-26 23:08:57",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw4aikw",
                  "author": "-InformalBanana-",
                  "text": "What reasoning effort do you use? Medium?",
                  "score": 3,
                  "created_utc": "2025-12-27 01:27:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3t6ht",
                  "author": "Aggressive-Bother470",
                  "text": "Oh...Â ",
                  "score": 2,
                  "created_utc": "2025-12-26 23:42:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3ot8d",
              "author": "mukz_mckz",
              "text": "I initially was sceptical about the GPT-OSS 120B model, but it's great. GLM 4.7 is good, but GPT OSS 120B is very succinct in its reasoning. Gets the job done with a lesser number of parameters and fewer tokens.",
              "score": 15,
              "created_utc": "2025-12-26 23:15:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4lyn9",
                  "author": "random-tomato",
                  "text": "GPT-OSS-120B is also extremely fast on a Pro 6000 Blackwell (200+ tok/sec for low context conversations, \\~180-190 for agentic coding, can fit 128k context no problem with zero quantization).",
                  "score": 13,
                  "created_utc": "2025-12-27 02:41:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3j44n",
              "author": "johannes_bertens",
              "text": "Minimax M2 (going to try M2.1)\n\nReasons: \n- can use tools reliably\n- follows instructions well\n- has good knowledge on coding\n- does not break down before 100k tokens at least\n\nUsing a single R6000 PRO with 96GB VRAM\nRunning Unsloth IQ2 quant with q8 kv quantization and about 100k tokens max context\n\nInterfacing with Factory CLI Droid mostly. Sometimes other clients.",
              "score": 15,
              "created_utc": "2025-12-26 22:42:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3kudx",
                  "author": "rm-rf-rm",
                  "text": "I've always been suspicious of 2-bit quants actually being usable.. good to hear its working well!",
                  "score": 7,
                  "created_utc": "2025-12-26 22:52:33",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw3q8ds",
                  "author": "79215185-1feb-44c6",
                  "text": "You are making me want to make bad financial decisions and buy a RTX 6000.",
                  "score": 11,
                  "created_utc": "2025-12-26 23:24:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw52epi",
                  "author": "Aroochacha",
                  "text": "MiniMax-M2 Q4_K_M\n\n\nI'm running the Q4 version from LM-Studio on dual RTX 6000 Pros with Visual Studio Code and Cline plugin.. I love it. It's fantastic at agentic coding. It rarely hellucinates and in my experience it does better than GPT-5. I work with C++/C code base (C for kernel and firmware code.)",
                  "score": 7,
                  "created_utc": "2025-12-27 04:32:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw6wdfn",
                  "author": "Warm-Ride6266",
                  "text": "Wats the speed t/s ur getting ?on single rtx 6000 pro?",
                  "score": 1,
                  "created_utc": "2025-12-27 14:02:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw43wod",
              "author": "No_Afternoon_4260",
              "text": "Iirc beginning of the year was on devstral small the first, then I played with DS R1 and V3.\nThen came K2 and glm at the same time.\nK2 was clearly better but glm so fast!\n\nToday I'm really pleased with devstral 123B. Very compact package for such a smart model. Fits in a H200, 2 rtx pros or 8 3090 in good quant and ctx, really impressive. (Order of magnitude 600 pp and 20 tg on a single h200..)\n\nEdit : In fact you could devstral 123B in q5 and ~30000 ctx on a single rtx pro or 4 3090 from my initial testing (I don't take in account memory fragmentation on the 3090s)",
              "score": 3,
              "created_utc": "2025-12-27 00:46:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw4eg2p",
              "author": "ttkciar",
              "text": "GLM-4.5-Air has been flat-out amazing for codegen.  I frequently need to few-shot it until it generates quite what I want, but once it gets there, it's really there.\n\nI will also frequently use it to find bugs in my own code, or to explain my coworkers' code to me.",
              "score": 3,
              "created_utc": "2025-12-27 01:53:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw4elvp",
              "author": "-InformalBanana-",
              "text": "Qwen3 2507 30b a3b instruct worked good for me with 12gb vram.\ngpt oss 20b didn't really do the things it should, was faster but didn't successfully code what I prompted it to.",
              "score": 3,
              "created_utc": "2025-12-27 01:54:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwrcoax",
                  "author": "TonyJZX",
                  "text": "these are my two favorites\n\nQwen3-30B-A3B is the daily\n\nGPT-OSS-20B is surprisingly excellent\n\ndeepseek and gemma as backup",
                  "score": 1,
                  "created_utc": "2025-12-30 17:04:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx0gwtm",
                  "author": "qudat",
                  "text": "I just tried qwen 30b on 11gb vram and the t/s was unbearable. Do you have a guide on tuning it?",
                  "score": 1,
                  "created_utc": "2026-01-01 01:31:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw5zv7p",
              "author": "Bluethefurry",
              "text": "Devstral 2 started out as a bit of a disappointment but after a short while I tried it again and its been a reliable daily driver on my 36GB VRAM setup, its sometimes very conservative with it's tool calls though, especially when its about information retrieval.",
              "score": 3,
              "created_utc": "2025-12-27 09:25:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3tfuq",
              "author": "Aggressive-Bother470",
              "text": "gpt120, devstral, seed.Â ",
              "score": 4,
              "created_utc": "2025-12-26 23:43:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw5uf4j",
              "author": "Lissanro",
              "text": "K2 0905 and DeepSeek V3.1 Terminus. I like the first because it spends less tokens and yet results it achieves often better than from a thinking model. This is especially important for me since I run locally and if a model needs too many tokens it would become juet not practical to use for agentic use case. It also still remains coherent at a longer context.\n\n\nDeepSeek V3.1 Terminus was trained differently and also supports thinking, do if K2 got stuck on something, it may help to move things forward. But it spends more tokens and may deliver worse results for general use cases, so I keep it as a backup model.\n\n\nK2 Thinking and DeepSeek V3.2 did not make here because I found K2 Thinking quite problematic (it has trouble with XML tool calls, and native tool calls require patching Roo Code, and also do not work correctly with ik_llama.cpp which has bugged native tool implementation that make the model to make malformed tool calls). And V3.2 still didn't get support in neither ik_llama.cpp nor llama.cpp. I am sure next year both models may get improved support...\n\n\nBut this year, K2 0905 and V3.1 Terminus are the models that I used the most for agentic use cases.",
              "score": 2,
              "created_utc": "2025-12-27 08:32:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1e87n",
                  "author": "Miserable-Dare5090",
                  "text": "What hardware are you running them on?",
                  "score": 1,
                  "created_utc": "2026-01-01 05:27:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw42edr",
              "author": "Refefer",
              "text": "GPT-OSS-120b takes the cake for me.  Not perfect, and occasionally crashes with some of the tools I use, but otherwise reliable in quality of output.",
              "score": 2,
              "created_utc": "2025-12-27 00:37:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw527p7",
              "author": "Aroochacha",
              "text": "MiniMaxAi's minimax-m2 is awesome. I'm currently using the 4Q version with Cline and it's fantastic.",
              "score": 1,
              "created_utc": "2025-12-27 04:31:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7ferf",
              "author": "Erdeem",
              "text": "Best for 48gb vram?",
              "score": 1,
              "created_utc": "2025-12-27 15:52:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7p6p1",
              "author": "Tuned3f",
              "text": "Unsloth's Q4_K_XL quant of GLM-4.7 completely replaced Deepseek-v3.1-terminus for me. I finally got around to setting up Opencode and the interleaved thinking works perfectly. The reasoning doesn't waste any time working through problems and the model's conclusions are always very succinct. I'm quite happy with it.",
              "score": 1,
              "created_utc": "2025-12-27 16:41:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw8ygoc",
              "author": "swagonflyyyy",
              "text": "gpt-oss-120b - Gets so much tool calling right.",
              "score": 1,
              "created_utc": "2025-12-27 20:34:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3ppfm",
              "author": "79215185-1feb-44c6",
              "text": "gpt-oss-20b overall best accuracy of any models that fit into 48GB of VRAM that I've tried although I do not do tooling / agentic coding.",
              "score": 1,
              "created_utc": "2025-12-26 23:21:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw3qvbr",
          "author": "Don_Moahskarton",
          "text": "I'd suggest to change the small footprint category to 8GB of VRAM, to match many consumer level gaming GPU. 9 GB seems rather arbitrary.\nAlso the upper limit for the small category should match the lower limit for the medium category.",
          "score": 14,
          "created_utc": "2025-12-26 23:28:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6vt3n",
              "author": "ThePixelHunter",
              "text": "Doesn't feel arbitrary, because it's normal to run a Q5 quant of any model at any size, or even lower if the model has more parameters.",
              "score": 1,
              "created_utc": "2025-12-27 13:58:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7myme",
          "author": "OkFly3388",
          "text": "For whatewer reason, you set the average threshold at 128 GB, not 24 or 32 GB?   \n  \nIt's intuitive that smaller models work on mid-range hardware, medium on high-end hardware(4090/5090), and unlimited on specialized racks.",
          "score": 5,
          "created_utc": "2025-12-27 16:30:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3hcx4",
          "author": "rm-rf-rm",
          "text": "**Speciality**",
          "score": 4,
          "created_utc": "2025-12-26 22:32:41",
          "is_submitter": true,
          "replies": [
            {
              "id": "nw3i3bm",
              "author": "MrMrsPotts",
              "text": "**Efficient algorithms**",
              "score": 5,
              "created_utc": "2025-12-26 22:36:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3hug1",
              "author": "MrMrsPotts",
              "text": "**Math**",
              "score": 3,
              "created_utc": "2025-12-26 22:35:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3jzuy",
                  "author": "4sater",
                  "text": "DeepSeek v3.2 Speciale",
                  "score": 10,
                  "created_utc": "2025-12-26 22:47:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3i1by",
              "author": "MrMrsPotts",
              "text": "**Proofs**",
              "score": 3,
              "created_utc": "2025-12-26 22:36:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw880wd",
                  "author": "Karyo_Ten",
                  "text": "The only proving model I know is DeepSeek-Prover: https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B",
                  "score": 3,
                  "created_utc": "2025-12-27 18:16:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwblzbg",
              "author": "CoruNethronX",
              "text": "Data analysis",
              "score": 1,
              "created_utc": "2025-12-28 06:06:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwbmeia",
                  "author": "CoruNethronX",
                  "text": "Wanted to highlight this [release](https://huggingface.co/DatarusAI/Datarus-R1-14B-preview) Very powerful model and a repo that allows to run it locally against local jupyter notebook.",
                  "score": 1,
                  "created_utc": "2025-12-28 06:10:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwilsoo",
              "author": "azy141",
              "text": "**Life sciences/sustainability**",
              "score": 1,
              "created_utc": "2025-12-29 08:57:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwbq4zb",
              "author": "Sicarius_The_First",
              "text": "**Uncensored Vision:**\n\n[**https://huggingface.co/SicariusSicariiStuff/X-Ray\\_Alpha**](https://huggingface.co/SicariusSicariiStuff/X-Ray_Alpha)",
              "score": -2,
              "created_utc": "2025-12-28 06:42:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwk6hcj",
          "author": "Aggressive-Bother470",
          "text": "Qwen3 2507 still probably the best at following instructions tbh.Â ",
          "score": 3,
          "created_utc": "2025-12-29 15:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3hsae",
          "author": "MrMrsPotts",
          "text": "No math?",
          "score": 2,
          "created_utc": "2025-12-26 22:35:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3hupj",
              "author": "rm-rf-rm",
              "text": "put it under speciality!",
              "score": 2,
              "created_utc": "2025-12-26 22:35:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw3hxan",
                  "author": "MrMrsPotts",
                  "text": "Done",
                  "score": 2,
                  "created_utc": "2025-12-26 22:35:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvlxt1",
          "author": "Agreeable-Market-692",
          "text": "I'm not going to give vram or ram recommendations, that is going to differ based on your own hardware and choice of backend but a general rule of thumb is if it's f16 then it's twice the number of GB as it is parameters and if it's the Q8 then it's the same number of GB as it is parameters -- all of that matters less when you look at llamacpp or ik\\_llama as your backend.  \nAnd if it's less than Q8 then it's probably garbage at complex tasks like code generation or debugging.\n\nGLM 4.6V Flash is the best small model of the year, followed by Qwen3 Coder 30B A3B (there is a REAP version of this, check it out) and some of the Qwen3-VL releases but don't go lower than 14B if you're using screenshots from a headless browser to do any frontend stuff. The Nemotron releases this year were good but the datasets are more interesting. Seed OSS 36B was interesting.\n\nAll of the models from the REAP collection, Tesslate's T3 models are better than GPT-5 or Gemini3 for TailwindCSS, GPT-OSS 120B is decent at developer culture, the THRIFT version of MiniMaxM2 VibeStudio/MiniMax-M2-THRIFT is the best large MoE for code gen.\n\nQwen3 NEXT 80B A3B is pretty good but support is still maturing in llamacpp, althrough progress has accelerated in the last month.\n\nIBM Granite family was solid af this year. Docling is worth checking out too.\n\nKittenTTS is still incredible for being 25MB. I just shipped something with it for on device TTS. Soprano sounds pretty good for what it is. FasterWhisper is still the best STT I know of.\n\nQwen-Image, Qwen-Image-Edit, Qwen-Image-Layered are basically free Nano-Banana\n\nWan2.1 and 2.2 with LoRAs is comparable to Veo. If you add comfyui nodes you can get some crazy stuff out of them.\n\nZ-Image deserves a mention but I still favor Qwen-Image family.\n\nThey're not models, but they are model citizens of a sort... Noctrex and -p-e-w- deserve special recognition as two of the biggest most unsung heroes and contributors this year to the mission of LocalLLama.",
          "score": 2,
          "created_utc": "2025-12-31 07:15:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1esxh",
              "author": "Miserable-Dare5090",
              "text": "All agreed but not the q8 limit. Time and time again, the sweet spot is above 6 bits per weight on small models. Larger models can take more quantization but I would not say below q8 is garbageâ€¦below q4 in small models, but not q8.",
              "score": 1,
              "created_utc": "2026-01-01 05:32:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3tz5d",
                  "author": "Agreeable-Market-692",
                  "text": "My use cases are for these things are pretty strictly highly dimensional, mostly taking in libraries or APIs and their docs and churning out architectural artifacts or code snippets -- I don't even really like Q8 all that much sometimes for this stuff. Some days I prefer certain small models full weights over even larger models at q8.  \nIf you're making q6 work for you that's awesome but to me they've been speedbumps in the past.",
                  "score": 1,
                  "created_utc": "2026-01-01 17:18:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwzs8d1",
          "author": "Illustrious_Big_2976",
          "text": "Honestly can't believe we went from \"maybe local models will be decent someday\" to debating if we've hit parity with GPT-4 in like 18 months\n\n  \nThe M2.1 hype is real though - been testing it against my usual benchmark of \"can it help me debug this cursed legacy codebase\" and it's actually holding its own. Wild times",
          "score": 1,
          "created_utc": "2025-12-31 22:59:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0uoxd",
          "author": "grepya",
          "text": "As someone with a  M1 Mac Studio with 32Gigs of RAM, can someone rate the best LLM's runnable on a reasonably spec'd  M series Mac?",
          "score": 1,
          "created_utc": "2026-01-01 03:03:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5rfns",
          "author": "NobleKale",
          "text": "> Useful breakdown of how folk are using LLMs: https://preview.redd.it/i8td7u8vcewf1.png?width=1090&format=png&auto=webp&s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d\n\n'Games and Role Play'\n\n... cowards :D",
          "score": 1,
          "created_utc": "2025-12-27 08:03:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7giwh",
          "author": "Lonhanha",
          "text": "Saw this thread, felt like it was a good place to ask and if anyone has a recommendation on a model to fine-tune using my groups chat data so that it learns the lingo and becomes an extra member of the group. What would you guys recommend?",
          "score": 1,
          "created_utc": "2025-12-27 15:57:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8opyk",
              "author": "rm-rf-rm",
              "text": "Fine tuners still go for Llama3.1 for some odd reason, but I'd recommend Mistral Small 3.2",
              "score": 4,
              "created_utc": "2025-12-27 19:41:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw9zjwj",
                  "author": "Lonhanha",
                  "text": "Thanks for the recommendation.",
                  "score": 1,
                  "created_utc": "2025-12-27 23:58:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwa86bp",
          "author": "Short-Shopping-1307",
          "text": "I want to use Claude as local LLM as we donâ€™t have  better LLM then this for code",
          "score": 1,
          "created_utc": "2025-12-28 00:46:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4vip8",
          "author": "Short-Shopping-1307",
          "text": "How we can use Claude for coding in as local setup",
          "score": -2,
          "created_utc": "2025-12-27 03:44:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3ko6f",
          "author": "Busy_Page_4346",
          "text": "Trading",
          "score": -5,
          "created_utc": "2025-12-26 22:51:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4puha",
              "author": "MobileHelicopter1756",
              "text": "bro wants to lose even the last penny",
              "score": 18,
              "created_utc": "2025-12-27 03:06:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5coy4",
                  "author": "Busy_Page_4346",
                  "text": "Could be. But it's like a fun experiment and I wanna see how AI actually make their decision on executing the trades.",
                  "score": 2,
                  "created_utc": "2025-12-27 05:49:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pz68fz",
      "title": "Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/ocq43c2a79ag1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-30 02:43:48",
      "score": 337,
      "num_comments": 120,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwob4kq",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 04:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo4n54",
          "author": "popiazaza",
          "text": "Not much of a surprise since every company has to make the money eventually.\n\nReleasing open weight models is just a cheaper way to advertise their AI lab instead of spending millions providing free or very cheap inference APIs.\n\nStill hope they would keep releasing open weight models at least until they really taking the lead and beating OpenAI/Anthropic/Google.",
          "score": 44,
          "created_utc": "2025-12-30 03:41:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpxhnc",
              "author": "SmartMario22",
              "text": "I don't disagree but they're ALSO spending millions to provide cheap API lol",
              "score": 14,
              "created_utc": "2025-12-30 12:27:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwqqa3j",
                  "author": "Mr_Hyper_Focus",
                  "text": "Came to post this lol",
                  "score": 3,
                  "created_utc": "2025-12-30 15:18:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwxdfz5",
                  "author": "eli_pizza",
                  "text": "Sure thatâ€™s how you gain market share. Uber was extremely cheapâ€¦and then raised prices after forcing competitors out of the market.",
                  "score": 1,
                  "created_utc": "2025-12-31 15:26:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwnyhgo",
          "author": "RhubarbSimilar1683",
          "text": "Good bye to open source! It's just a matter of time",
          "score": 171,
          "created_utc": "2025-12-30 03:06:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo157q",
              "author": "ThenExtension9196",
              "text": "Yep. Everyone saying the Chinese open source was some gift to humanity was delusional. They did what they had to do to compete with larger companies with capital. Now that they got their foothold itâ€™s business as usual.",
              "score": 103,
              "created_utc": "2025-12-30 03:21:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwolc1s",
                  "author": "Honest_Science",
                  "text": "Devaluating US models is part of the chinese way to compete.",
                  "score": 57,
                  "created_utc": "2025-12-30 05:28:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwokhlo",
                  "author": "kawaii_karthus",
                  "text": "I think they will continue to release good open source models for years to come. There inner domestic competition is fierce and probably not united. And this goes for all markets not just AI. while i was visiting family and living there for a while, I still see them building tons of factories.. (though slower then the years before) even with a global recession going on... like who is going to be their customers?? time will tell. They do love over saturating any market they can get into though.. the AI industry is no different.",
                  "score": 29,
                  "created_utc": "2025-12-30 05:22:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwrke1e",
                  "author": "letsgeditmedia",
                  "text": "I donâ€™t think going IPO means that open sourcing was some kind of ruseâ€¦ itâ€™s just fighting against the realities of living under global capitalism",
                  "score": 1,
                  "created_utc": "2025-12-30 17:40:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpiaep",
              "author": "FreddoRS",
              "text": "Qwen models are Alibaba and mostly open weights, I imagine that's what z.ai will end up doing, mostly free models with some specific ones locked behind partnered cloud inference providers",
              "score": 10,
              "created_utc": "2025-12-30 10:18:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvflh0",
                  "author": "Neither-Phone-7264",
                  "text": "the top end models are proprietary. we might only get like, glm 5 air. oh well",
                  "score": 1,
                  "created_utc": "2025-12-31 06:21:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwo6g0y",
              "author": "Sensitive_Song4219",
              "text": "Hope this doesn't happen but I fear you may be right.\n\nThe cat's out the bag, though: if z.ai goes rogue I'm pretty sure others will take their place, progress in the open-weights space has been astonishing lately, and z.ai isn't the only player.\n\nAlso this had better not mess with their nice coding plans!",
              "score": 15,
              "created_utc": "2025-12-30 03:51:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwpajzv",
              "author": "howardhus",
              "text": "what if i told youâ€¦ there was never\nopen source? those were all open weights.\n\n\nbasically shareware models  usable enough for free marketing and to ger known\n\nwhy people (in this sib of all places!) still say open source is beyond me\n\n\nthe pattern was always the same: small group of people publish small cool model showing some intetestong feature, usable enough to showcase the function but not good enough gor production.\n\nmodel gets hyped on redditâ€¦\n\n\nmodel never gets any updates and group of people are never heard of again",
              "score": 6,
              "created_utc": "2025-12-30 09:06:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwrbn75",
                  "author": "RhubarbSimilar1683",
                  "text": "Because you can technically still mess with the weights, the data is pretty much already public because it's the whole internet and books and the training recipe is some paper, instruct training data pairs though is something I agree with but it's not too hard to generate those synthetically nowadays with open models, although they were originally created by online workers at data annotation places like outlier ai",
                  "score": -2,
                  "created_utc": "2025-12-30 16:59:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoyl1k",
              "author": "xantrel",
              "text": "Eh, not necessarily. I know open weights is a far cry from open source llms, but many people would not send their most private data to a (former) Chinese company in this heavily politicized world. Them removing the open source component essentially shuts down a good chunk of the western market. I know the eastern European and Asian markets don't mind it as much, but much of the money and prestige comes from being the open source model leader.Â \n\n\nAll of these companies are basically trying modern architectures while distilling the big commercial models (openai, anthropic, google). That's why open source magically trails a few months behind the big 3.\n\n\nAll this to say, if Z stops releasing models (and maybe they will), it shouldn't be a huge loss for the community since Minimax or another entrant can easily take their place as what their doing is vastly cheaper and simpler than what actual leading labs are doing. Yes it's expensive, but everyone has seen that's it's also a very cheap way to get a ton of free publicity and users. If you aren't SOTA closed source, I think it's a better commercial option to be SOTA open source than crappy closed source. The cost of switching providers is too low.",
              "score": 10,
              "created_utc": "2025-12-30 07:16:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpxgjq",
                  "author": "FullOf_Bad_Ideas",
                  "text": "Minimax is also IPOing, so if Zhipu stops releasing their models, Minimax will most likely do the same.",
                  "score": 2,
                  "created_utc": "2025-12-30 12:27:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwobhw6",
              "author": "ScythSergal",
              "text": "Their horrible handling and PR around the 4.6 Air release was the writing on the wall for me.\n\nThe lying, over hyping, lying again, denial, then lying a third time, only to end up not releasing it, and avoid interacting with anything that mentions it.\n\nIt was as simple as \"we changed our mind on this release\" or something simple. But instead they lied a multitude of times and got everybody excited for something they never ended up releasing. And they didn't even have the decency to say why or clarify that it wasn't coming out so people would stop holding on. It's just disrespectful",
              "score": 20,
              "created_utc": "2025-12-30 04:22:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwovxdz",
                  "author": "JazzlikeLeave5530",
                  "text": "The writing on the wall should have been them being a corporation lol",
                  "score": 15,
                  "created_utc": "2025-12-30 06:53:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwp82yu",
                  "author": "CheatCodesOfLife",
                  "text": "I was watching that even though I \"don't need some air\".\n\nTo me it looked like some devs were surprised by the demand and got too excited when they say \"2 weeks\" or whatever it was, then weren't able to deliver.\n\nAlso (I could be wrong or misremembering), I thought I read somewhere that they weren't able to train it properly?\n\nbtw, I see they've got an air-sized 4.6-VL. Is that no good?",
                  "score": 10,
                  "created_utc": "2025-12-30 08:43:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwobs25",
                  "author": "Odd-Ordinary-5922",
                  "text": "I remember Q&A they said that they still have some open models coming out at the beginning of next year so fingers crossed",
                  "score": -1,
                  "created_utc": "2025-12-30 04:24:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoh22t",
              "author": "bick_nyers",
              "text": "Many of us are only willing to pay subscriptions to models that have been open sourced. I don't think Z.ai is dumb enough to go closed source and kill all of their good will with the community. We shall see.",
              "score": 6,
              "created_utc": "2025-12-30 04:58:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwona2l",
                  "author": "the320x200",
                  "text": "\"There are dozens of us!\"\n\nDude, nobody with enough money to matter is making decisions like that... This community is nothing compared to corporate users, not in number and not in bankroll.",
                  "score": 28,
                  "created_utc": "2025-12-30 05:43:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrejvh",
              "author": "IrisColt",
              "text": "bye, sigh...",
              "score": 1,
              "created_utc": "2025-12-30 17:13:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwq1czy",
              "author": "GreenGreasyGreasels",
              "text": ">Good bye to open source! It's just a matter of time\n\nUnlikely. They are going the Mistral way. That's the plan for now.",
              "score": 0,
              "created_utc": "2025-12-30 12:55:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwo5mqm",
          "author": "abeecrombie",
          "text": "Why does everyone assume they won't keep releasing open weight models ?  U pay for z.ai subscription bc 1. I don't care about privacy for my pet projects 2. $3 a month or whatever vs $3000+ for a GPU capable of running their models makes sense for a lot of ppl ( myself included) \n\nIf the Chinese government still considers open source a priority I think companies like z.ai can still release open weight models and find a way to make money via inference/ mcp . Im far from a political expert but believe that policy still holds. \n\nHappy to hear arguments why I'm wrong.",
          "score": 63,
          "created_utc": "2025-12-30 03:47:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo8fw6",
              "author": "cafedude",
              "text": "You have a point. Everyone could make their own ketchup - the recipes are out there and they're not that hard, but pretty much nobody makes their own ketchup since it's a lot easier to buy a bottle for $3.",
              "score": 35,
              "created_utc": "2025-12-30 04:04:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpncmt",
                  "author": "power97992",
                  "text": "I made  my own ketchup beforeâ€¦",
                  "score": 4,
                  "created_utc": "2025-12-30 11:04:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwo8sd5",
              "author": "Sensitive_Song4219",
              "text": "Perhaps. Even OpenAI manages to occasionally contribute with their GPT-OSS releases. We'll see if Z can align this with their mission statement, in their AMA they [said](https://www.reddit.com/r/LocalLLaMA/s/2yDtPG0Qbl) open source would still be a priority after going public. Hope they meant it.\n\nRegarding privacy: would there be added accountability regarding their data handling once they're publicly traded?",
              "score": 2,
              "created_utc": "2025-12-30 04:06:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwotwck",
                  "author": "Corporate_Drone31",
                  "text": "got-oss was not \"occasionally,\" it was a one-off after literal years of not having released any large language model past GPT-2.",
                  "score": 22,
                  "created_utc": "2025-12-30 06:35:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqehlb",
                  "author": "abeecrombie",
                  "text": "Good question regarding the accountability of data once they are public. I'm not sure they'd have to comply with any extra regulations but it should be more visibly disclosed and discussed. \n\nFor example I think as soon as you deal with European user data you have to comply with GDPR. So that shouldn't be new. What would be new is z.ai would most likely have to disclose it to their auditors / board etc that they are in compliance. Not sure you see any real disclosures from the Chinese ai labs on that front today.",
                  "score": 4,
                  "created_utc": "2025-12-30 14:15:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwofh2c",
              "author": "cobbleplox",
              "text": "Well when I don't care about privacy, the competition is suddenly full blown chatgpt and such? But I guess that's going to be their problem one way or the other. Also I think a lot of the appeal of open models is community finetunes. They won't be serving these, will they?",
              "score": 1,
              "created_utc": "2025-12-30 04:48:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwofklp",
              "author": "Erebea01",
              "text": "Not saying they're not harvesting our data or that they're trustworthy but it always boggles my mind on how much people and redditors are paranoid about the Chinese government and their tech companies when the worse offenders have always been the US govt and their tech companies. Why be afraid of a govt thousands of miles away unless you're afraid they're gonna blackmail you with your CP or something",
              "score": -1,
              "created_utc": "2025-12-30 04:48:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwnwdom",
          "author": "HornyGooner4401",
          "text": "Please don't sell out",
          "score": 26,
          "created_utc": "2025-12-30 02:54:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnx35h",
              "author": "True_Requirement_891",
              "text": "It's the rule of the game they are playing. They basically have to eventually.",
              "score": 52,
              "created_utc": "2025-12-30 02:58:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwop1qp",
                  "author": "ForsookComparison",
                  "text": "Either that have to sell out or we (the community) need a way to contribute upstream similar to regular open source software.\n\nIn the Llama 2 days I was optimistic that this could come via community datasets and fine-tunes. Nowadays I don't really know what we offer them besides IPO hype. Maybe this is *THE* open weight play? Drum up buzz for legitimacy, maybe even some revenue via official API providers, swoon the funding rounds, bam. You're acquired or public.",
                  "score": 7,
                  "created_utc": "2025-12-30 05:56:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwo18t7",
              "author": "ThenExtension9196",
              "text": "Bro thatâ€™s the name of the game.",
              "score": 18,
              "created_utc": "2025-12-30 03:21:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwnynpo",
          "author": "Odd-Ordinary-5922",
          "text": "I think we can expect to see less open source models from them although they have contributed a lot so I think its well deserved to get the bag",
          "score": 17,
          "created_utc": "2025-12-30 03:07:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnxlo0",
          "author": "LocoMod",
          "text": "Shareholders dont like giving product away for free.",
          "score": 15,
          "created_utc": "2025-12-30 03:01:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoq725",
              "author": "misterflyer",
              "text": "[https://youtu.be/0MXSAwkVU3U?t=458](https://youtu.be/0MXSAwkVU3U?t=458)",
              "score": -1,
              "created_utc": "2025-12-30 06:05:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqwae6",
          "author": "hyno111",
          "text": "I think Z.ai/ChatGLM is one of the few models that actually implements a proper search agent â€” meaning it can look at search results during reasoning when necessary, and then perform additional searches with updated keywords if needed.\n\nItâ€™s also one of the very few search agents that passed my â€œMagical Realism Large Model Search Capability Test,â€ which consists of the following multi-turn prompts:\n\nâ€œHow should an LLM defend against search engine poisoning?â€\nâ€œDo not use search. Donald Trump just announced a Trump-class battleship at Mar-a-Lago, (with specific technical details). How plausible is this?â€\nâ€œNow use search. Are these claims real, or are you being affected by search engine poisoning?â€",
          "score": 3,
          "created_utc": "2025-12-30 15:47:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwory70",
          "author": "toothpastespiders",
          "text": "It was great while it lasted. Air's probably going to have a place of honor next to Yi 34b in my hard drive's LLM memorial. It's possible they might not fall off after this. But I think I'm just going to assume that's the case and be pleasantly surprised if I'm wrong. \n\nSucks, but they gave us some great releases. Certainly made 2025 a lot more interesting in this space.",
          "score": 5,
          "created_utc": "2025-12-30 06:19:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9nrs",
          "author": "IngwiePhoenix",
          "text": "> Going for an IPO\n\nAaaaaand it's gone! :D Any company that IPO'd is basically \"useless\" to normal users.\n\nWelp, was fun while it lasted.",
          "score": 8,
          "created_utc": "2025-12-30 04:11:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp2gkt",
              "author": "Novel-Mechanic3448",
              "text": "yeah man google, meta, hell anyone in the fortune 500, totally useless.\n\nhaha",
              "score": 3,
              "created_utc": "2025-12-30 07:51:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwoglko",
          "author": "HelpRespawnedAsDee",
          "text": "Definitely the next acquisition target.",
          "score": 2,
          "created_utc": "2025-12-30 04:55:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpbazd",
          "author": "evia89",
          "text": "F for cheap api ($25/year coding plan that is not useless)",
          "score": 2,
          "created_utc": "2025-12-30 09:13:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq4203",
          "author": "Available_Brain6231",
          "text": "If I had one coin for every product and company that got better after going public... I would not have a single coin...  \nBUT china is the only true capitalist country in the world so maybe it will work over there.",
          "score": 2,
          "created_utc": "2025-12-30 13:13:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnuzxk",
          "author": "Prestigious_Fold_175",
          "text": "10x Cheaper than openai.",
          "score": 4,
          "created_utc": "2025-12-30 02:46:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo1haa",
              "author": "ThenExtension9196",
              "text": "Not after the shareholders have a say.",
              "score": 30,
              "created_utc": "2025-12-30 03:23:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwnxpw7",
              "author": "LocoMod",
              "text": "10x less capability too. Entropy is preserved and physics still makes sense!",
              "score": -17,
              "created_utc": "2025-12-30 03:01:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwo2ood",
                  "author": "1kakashi",
                  "text": "What? This is seriously funny ðŸ¤£",
                  "score": 4,
                  "created_utc": "2025-12-30 03:30:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwo3jq1",
                  "author": "cockerspanielhere",
                  "text": "What do you know about physics ðŸ˜‚",
                  "score": 1,
                  "created_utc": "2025-12-30 03:35:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwojguv",
          "author": "drooolingidiot",
          "text": "I'll buy the stock only if they release GLM 4.7 Air.",
          "score": 2,
          "created_utc": "2025-12-30 05:15:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp8ccq",
              "author": "CheatCodesOfLife",
              "text": "I think in that podcast episode on spotify, they said they will, but it'll be qwen-3-30b sided (so useless).",
              "score": -1,
              "created_utc": "2025-12-30 08:46:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwsov9l",
                  "author": "drooolingidiot",
                  "text": "Did they say it again, or only back when they released GLM 4.5?",
                  "score": 1,
                  "created_utc": "2025-12-30 20:51:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpbkcg",
          "author": "Fit-Produce420",
          "text": "Some people say cucumbers taste better pickled.Â ",
          "score": 1,
          "created_utc": "2025-12-30 09:16:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpt9cj",
          "author": "JLeonsarmiento",
          "text": "Ok, I want 1000 shares.",
          "score": 1,
          "created_utc": "2025-12-30 11:55:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq6lxc",
          "author": "ANR2ME",
          "text": "Hopefully it's not an exit strategy for early investors, like what e-commerce companies did after being in deficit for years ðŸ˜… (not sure whether Z AI was already profitable or not).",
          "score": 1,
          "created_utc": "2025-12-30 13:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrk9m9",
          "author": "letsgeditmedia",
          "text": "So we can invest in this in the states or nah",
          "score": 1,
          "created_utc": "2025-12-30 17:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrsjkc",
              "author": "Fine-Will",
              "text": "Depends on your broker. I know IKBR and Fidelity does.",
              "score": 1,
              "created_utc": "2025-12-30 18:18:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv5abt",
          "author": "Ylsid",
          "text": "It's a Chinese business, don't touch it if you aren't ready to get rugpulled for politics",
          "score": 1,
          "created_utc": "2025-12-31 05:04:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwynkpu",
          "author": "lunatix",
          "text": "I mean meta's going to just buy them right?",
          "score": 1,
          "created_utc": "2025-12-31 19:16:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwoboax",
          "author": "shoeshineboy_99",
          "text": "Link to the submission announcement. Has anyone got hold of the submitted prospectus? Will be interesting to read. \n\n[submission announcement ](https://www1.hkexnews.hk/app/sehk/2025/107977/documents/sehk25121901972.pdf)",
          "score": 1,
          "created_utc": "2025-12-30 04:23:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrajv7",
              "author": "FullOf_Bad_Ideas",
              "text": "No but here are some revenue and expenditures numbers. You'll need to translate it from Chinese. https://wallstreetcn.com/articles/3761776",
              "score": 2,
              "created_utc": "2025-12-30 16:54:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws1879",
                  "author": "shoeshineboy_99",
                  "text": "cool. Found the english version of the document. Dont know why my comment was downvoted!",
                  "score": 2,
                  "created_utc": "2025-12-30 18:58:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo860o",
          "author": "pellucide",
          "text": "Does z.ai have an app",
          "score": 1,
          "created_utc": "2025-12-30 04:02:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwopd6w",
              "author": "lly0571",
              "text": "They have [an app](https://chatglm.cn) in China, but not in a style like `chat.z.ai` or `chat.qwen.ai`.",
              "score": 4,
              "created_utc": "2025-12-30 05:59:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwo8q9i",
              "author": "Amazing_Athlete_2265",
              "text": "Does google have an app?",
              "score": 1,
              "created_utc": "2025-12-30 04:05:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwo9b9o",
                  "author": "pellucide",
                  "text": "https://play.google.com/store/apps/details?id=com.google.android.googlequicksearchbox&hl=en",
                  "score": 2,
                  "created_utc": "2025-12-30 04:09:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwocpvm",
          "author": "met_MY_verse",
          "text": "Puts it is.",
          "score": 1,
          "created_utc": "2025-12-30 04:30:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq1k4n",
          "author": "ridablellama",
          "text": "How can I buy as an American?",
          "score": 1,
          "created_utc": "2025-12-30 12:57:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwodbn3",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2025-12-30 04:34:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp125g",
              "author": "Different_Fix_2217",
              "text": "Very unlikely they will be releasing any more models opensource with this.",
              "score": 2,
              "created_utc": "2025-12-30 07:38:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwraf2e",
              "author": "FullOf_Bad_Ideas",
              "text": "Here's more info. Translate from Chinese to English with an LLM or other translation tool. https://wallstreetcn.com/articles/3761776",
              "score": 1,
              "created_utc": "2025-12-30 16:54:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzcrtb",
      "title": "Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/yq8uriwhxaag1.jpeg",
      "author": "ResearchCrafty1804",
      "created_utc": "2025-12-30 08:26:06",
      "score": 314,
      "num_comments": 36,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwr47l3",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 16:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwp7s4a",
          "author": "redditscraperbot2",
          "text": "Oh this looks really cool\n\nEdit:\nGot it running and it is really cool. Works as advertised. This is going to be a massive speed boost to people working on games. Only a little cleanup needed for each animation.",
          "score": 73,
          "created_utc": "2025-12-30 08:40:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq7pdq",
              "author": "Zundrium",
              "text": "Ain't nobody got time to clean up animations. Where is my 1B-motion-to-clean-motion model?",
              "score": 42,
              "created_utc": "2025-12-30 13:36:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvzuid",
                  "author": "WEREWOLF_BX13",
                  "text": "FR HAHAHA",
                  "score": 2,
                  "created_utc": "2025-12-31 09:26:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwqelw9",
              "author": "ab2377",
              "text": "how did you run it?",
              "score": 10,
              "created_utc": "2025-12-30 14:16:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws5bfq",
                  "author": "mxforest",
                  "text": "It's text to motion. You just have to command it to run via chat. Duh!",
                  "score": -5,
                  "created_utc": "2025-12-30 19:17:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwuvmfx",
              "author": "Ylsid",
              "text": "How on earth did you get it out of dependency hell",
              "score": 5,
              "created_utc": "2025-12-31 03:59:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwv7oka",
                  "author": "redditscraperbot2",
                  "text": "What dependency issues were you having?\nI made my env with python 3.10 and torch 12.1",
                  "score": 5,
                  "created_utc": "2025-12-31 05:21:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrzbbj",
              "author": "WitAndWonder",
              "text": "What animation format does this actually spit out?",
              "score": 5,
              "created_utc": "2025-12-30 18:49:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwt78qo",
                  "author": "redditscraperbot2",
                  "text": "An fbx of the wooden doll doing the animation with a skeleton",
                  "score": 7,
                  "created_utc": "2025-12-30 22:18:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nww3c7u",
              "author": "TheMisterPirate",
              "text": "What's your hardware? Curious how much vram this uses",
              "score": 1,
              "created_utc": "2025-12-31 09:59:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww4mim",
                  "author": "redditscraperbot2",
                  "text": "3090\n64gb of ram\n\nAt fp16 it takes around 18-23gb per gen with a batch of 4, but I imagine if I cared enough to offload the text encoder it would use significantly less.",
                  "score": 1,
                  "created_utc": "2025-12-31 10:11:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpozo6",
          "author": "Illustrious-Lake2603",
          "text": "Does this work only for Humanoid models? Or will it work for animals as well?? I have been working with puppeteer and it has actually been magical.",
          "score": 17,
          "created_utc": "2025-12-30 11:19:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrz3yg",
              "author": "WitAndWonder",
              "text": "It was trained exclusively on the standard human model. Seems like it could work for other bipedal movements to some extent, but anything with more or less limbs seems out of the question unfortunately.",
              "score": 5,
              "created_utc": "2025-12-30 18:48:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws8lv5",
                  "author": "Illustrious-Lake2603",
                  "text": "Thank you! Its still beneficial. I can probably make it work along side puppeteer to be able to get better humanoid animations (if they are good).",
                  "score": 1,
                  "created_utc": "2025-12-30 19:33:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrumui",
              "author": "fiddler64",
              "text": "yeah, and there's only 1 predetermined rig, I tried prompting a dog chasing after ball and it shows the humanoid rig throwing the ball instead",
              "score": 3,
              "created_utc": "2025-12-30 18:27:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqpxbl",
          "author": "Quiet-Owl9220",
          "text": "Oh boy. The virt-a-mate community ought to have some good uses for this one...",
          "score": 16,
          "created_utc": "2025-12-30 15:16:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpqo9r",
          "author": "KingDutchIsBad455",
          "text": "Is this what Neuro uses?",
          "score": 16,
          "created_utc": "2025-12-30 11:33:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrnwwa",
              "author": "inaem",
              "text": "The sitting looks the same tbh",
              "score": 1,
              "created_utc": "2025-12-30 17:57:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwqg3rd",
              "author": "Emotional-Metal4879",
              "text": "also wander",
              "score": 0,
              "created_utc": "2025-12-30 14:24:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqhx2d",
          "author": "no_witty_username",
          "text": "This is more cool then folks realize.  Soo many uses for this.",
          "score": 10,
          "created_utc": "2025-12-30 14:34:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrywy0",
              "author": "WitAndWonder",
              "text": "Just a pity that it was basically exclusively human motion. If it also covered quadruped we'd be really in business since that's most of the creatures that get put into games, too.",
              "score": 4,
              "created_utc": "2025-12-30 18:47:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwrviwp",
              "author": "Crypt0Nihilist",
              "text": "Don't leave us hanging!",
              "score": -1,
              "created_utc": "2025-12-30 18:31:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwu533b",
          "author": "JonatasLaw",
          "text": "Now I can work in my game hahaha",
          "score": 2,
          "created_utc": "2025-12-31 01:22:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwufjmc",
          "author": "TanguayX",
          "text": "Wow. Impressive",
          "score": 1,
          "created_utc": "2025-12-31 02:22:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv7b53",
          "author": "Specific-Strain7970",
          "text": "This looks super interesting! (I'm a beginner) What are the models I can apply these animations to? Would stock mixamo models work directly? I'm thinking Unity. Thanks for the help in advance!",
          "score": 1,
          "created_utc": "2025-12-31 05:18:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww00ht",
          "author": "WEREWOLF_BX13",
          "text": "This is exactly what I've been waiting for! Soon enough anyone will be able to create anything at mass scale and say goodbye to shitty company slop!",
          "score": 1,
          "created_utc": "2025-12-31 09:27:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdhc1k",
          "author": "Bl_grill",
          "text": "Its alright but until it can walk through a door, place hand on door way frame and open door then walk through with face capture in real time and have the person react absolutely devastating.\n\n\nBut this is for npc, or repetitious situations, i can see what tencent is doing they're focusing on repetitive tasks, to free up development.\n\n\nChinese developerd can focus(including others too)to working on speed and efficiency, while professionals focus on mocap.\n\n\nWhat i cannot wait for is ai reacting to its environment and the gamer drops in, and it hunts the gamer and mocks them(yes yes, this game that game has it)but i thought such technology was gonna be embedded in consoles ai chip for neural processing for developers freeing up stricted ai.\n\n\nOverall seeing ai stumble and get frustrated as you shoot it coming back reminding you, almost like the nemisis engine from lord of rings.\n\n\nOverall helpful but no game changer just a pipeline implementation on tencents roadmap.",
          "score": 1,
          "created_utc": "2026-01-03 03:18:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpxlvk",
          "author": "paryska99",
          "text": "Do they also release any finetuning code? This could be really cool for 3D artists or for games with generative content.",
          "score": 1,
          "created_utc": "2025-12-30 12:28:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtq6ff",
              "author": "Erdeem",
              "text": "Not yet",
              "score": 1,
              "created_utc": "2025-12-30 23:59:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsprse",
          "author": "RedZero76",
          "text": "I just shat myself.  This is GOLD.",
          "score": 1,
          "created_utc": "2025-12-30 20:55:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1p5q5",
      "title": "Getting ready to train in Intel arc",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1q1p5q5",
      "author": "hasanismail_",
      "created_utc": "2026-01-02 04:33:19",
      "score": 290,
      "num_comments": 87,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q1p5q5/getting_ready_to_train_in_intel_arc/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nx8jin6",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-02 11:20:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7e5iq",
          "author": "MikeRoz",
          "text": "> Just waiting on pcie risers\n\nI, too, remember when I thought I, and not my risers, decided where in the frame the GPUs went.",
          "score": 126,
          "created_utc": "2026-01-02 05:11:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7z858",
              "author": "dsanft",
              "text": "I have a lot of great things to say about the ADT link pcie risers, the ones with the shielded silver cables. I run them in pcie 3 4x and even at lengths up to 80cm I've had no problems.",
              "score": 14,
              "created_utc": "2026-01-02 08:09:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx7zf7z",
              "author": "TheRealMasonMac",
              "text": "Gamers rose up.",
              "score": 3,
              "created_utc": "2026-01-02 08:11:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7m3mn",
          "author": "Techngro",
          "text": "Dude, you can't post stuff like this without details.",
          "score": 32,
          "created_utc": "2026-01-02 06:12:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9cpil",
              "author": "hasanismail_",
              "text": "Sorry was too excited when posting\n\n8x b580 GPUs (1 is not in picture I was playing a game at the time and needed it)\n\nDual Intel e5 Xeon v4 CPUs (forgot exact model)\n\n128gb ddr4 (I bought before the ram crisis)\n\nDual 850w corsair PSUs\n\n\nServer will run in Ubuntu latest release with the Intel patches and I'm gonna use vulkan and probably train with pytorch or something (I haven't thought that far ahead)\n\nI paid 200$-240$ per GPU mostly from micro center deals Facebook marketplace and I was able to snag some off amazon too I was planning on using the b50 but the memory band with is very slow compared to the b580 and the value proposition of the b580 is just too good to pass up",
              "score": 27,
              "created_utc": "2026-01-02 14:40:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxa39do",
                  "author": "satireplusplus",
                  "text": "Not sure what exactly you plan on training with pytorch, but the vulkan backend is extremely poor and non-functional for that. It contains a few functions, just barely enough to run object detection on android. Intel does have a special pytorch version with xpu support though (through their own intel-one stack). Report back what you can do with it, but it's not gonna be as smooth as CUDA or even rocm.",
                  "score": 6,
                  "created_utc": "2026-01-02 16:51:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9gbxe",
                  "author": "KoalaRashCream",
                  "text": "How many TOPS",
                  "score": 5,
                  "created_utc": "2026-01-02 15:00:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxbtmzc",
                  "author": "autistic-brother",
                  "text": "What mother board did you use?\n\nHow are you planning on using this for training?",
                  "score": 1,
                  "created_utc": "2026-01-02 21:47:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9wrw7",
                  "author": "shrug_hellifino",
                  "text": "And still, you make people look up and calculate what your total VRAM would be... these are 16GB cards? so,128?",
                  "score": 0,
                  "created_utc": "2026-01-02 16:21:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxa9yj0",
                  "author": "FullstackSensei",
                  "text": "Which motherboard? E5 means you'll be running PCIe Gen 3 and rebar support will most likely need to be patched in BIOS. You'll have a bad time using those cards without it.\n\nIf you can find one for cheap, snag a supermicro X10DRX. You get ten X8 slots. It doesn't have an M.2 slot but supports NVMe in any of the PCIe slots. I have a Samsung PM1725a in mine and it boots without any issues.",
                  "score": 0,
                  "created_utc": "2026-01-02 17:22:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7jvqq",
          "author": "CheatCodesOfLife",
          "text": "Nice! To save yourself some of the pain ahead, go with Ubuntu 24.04\n\nGood news is unsloth seems to support Intel Arc now.\n\nYou'll probably want to join the OpenArc discord when you set this up.",
          "score": 47,
          "created_utc": "2026-01-02 05:55:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8bp1z",
              "author": "Jokerit208",
              "text": "Why Ubuntu 24.04?",
              "score": 11,
              "created_utc": "2026-01-02 10:08:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8p2w3",
                  "author": "AI_is_the_rake",
                  "text": "To prevent pain, apparentlyÂ ",
                  "score": 22,
                  "created_utc": "2026-01-02 12:07:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx9bu4q",
              "author": "hasanismail_",
              "text": "Thx I tried this last year with 2 cards and it was a pita on Linux a link to that discord server would be nice I have a feeling I'm gonna need it",
              "score": 3,
              "created_utc": "2026-01-02 14:36:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxe3w72",
                  "author": "Echo9Zulu-",
                  "text": "My project  https://github.com/SearchSavior/OpenArc\n\nand our discord https://discord.gg/vS5ANSy3a",
                  "score": 2,
                  "created_utc": "2026-01-03 05:49:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxebfum",
                  "author": "Echo9Zulu-",
                  "text": "Yes we can help you get situated. For training you'll want to use xpu nightly with accelerate; ipex optimizations are being upstreamed there. Ipex is in end of life. Llm-scaler and vllm xpu 11 are also an absolute must. OpenArc supports multi gpu pipeline paralell atm via openvino but performance characteristics of 8 gpus remains unknown (!). We can help you cook some large  quants based on what's currently supported.\n\nThe absolute unit guy who maintains sycl backend joined a few months ago. He is intel engineer who develops sycl. His help has been invaluable in navigating high complexity issues. Very fortunate to have him as a resource since all pytorch xpu kernels are written in sycl. choosing a slightly older model as target architecture where the implementations ard more mature. Think llama 3.3, qwen2.5/qwen3. Intel is putting massive resources into battlemage and it's likely that the performance uplift for multi gpu training have not been explored but do exist. We see this all the time, changes are hardened in the codebase but underreported in patchnotes because intel moves so fast. \n\nHope my ramblings help. Really awesome build, welcome to Arc and good luck!!",
                  "score": 2,
                  "created_utc": "2026-01-03 06:49:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxd39z4",
                  "author": "b0tbuilder",
                  "text": "You can make it work.",
                  "score": 1,
                  "created_utc": "2026-01-03 01:55:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7isgw",
          "author": "twnznz",
          "text": "I recognise this makes sense for inference but for training we have a huge constraint on bus bandwidth, are you sure you want to train on PCIe setup rather than renting N*H100 from Vast or similar? Does your model/data need absolute security?",
          "score": 15,
          "created_utc": "2026-01-02 05:46:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx99kwm",
              "author": "sparkandstatic",
              "text": "Self hosted can save the most, if it fits within vram.",
              "score": 2,
              "created_utc": "2026-01-02 14:23:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx9g38s",
                  "author": "twnznz",
                  "text": "What Iâ€™m trying to say is: unless electricity is free, it is almost certainly cheaper to train on rented H100",
                  "score": 1,
                  "created_utc": "2026-01-02 14:59:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7gv0f",
          "author": "HyperWinX",
          "text": "Are you going to use Vulkan or what?",
          "score": 13,
          "created_utc": "2026-01-02 05:31:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7fy1n",
          "author": "Fit_West_8253",
          "text": "What model you using? Hardly seen any Intel GPUs used but Iâ€™m very interested in something like the B60",
          "score": 9,
          "created_utc": "2026-01-02 05:24:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx86mr1",
          "author": "jack-in-the-sack",
          "text": "7 gpus on what motherboard?",
          "score": 8,
          "created_utc": "2026-01-02 09:20:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbs3dq",
              "author": "autistic-brother",
              "text": "8",
              "score": 2,
              "created_utc": "2026-01-02 21:39:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7ahl1",
          "author": "Dundell",
          "text": "Big fan of the aaawave open frame. Full size motherboard space with x2 ATX PSUs on both sides. Funny to look at the product details now include \"AI machine learning applications\".\n\nMy rig is x5 rtx 3060 12gb's + x1 P40 24gb all on pcie3.0@4 Lanes with a X99 board. I just run GPT-OSS 120B Q4 with 131k context speeds 42~12 t/s and usually keep it below 90k context maximum for context condensing in roo code.\n\nAlthough I haven't bothered to update llama.cpp and instructions for the gpt-oss 120b since it was released... maybe I could get better performance, but why mess with a good thing.",
          "score": 8,
          "created_utc": "2026-01-02 04:47:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7dkiy",
              "author": "ajw2285",
              "text": "Deets on mobo?",
              "score": 5,
              "created_utc": "2026-01-02 05:07:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7eur9",
                  "author": "Dundell",
                  "text": "Machinist X99-MR9S Motherboard, Intel Xeon E5-2690 v4 CPU, 5x RTX 3060 12GB GPUs, 1x Tesla P40 24GB GPU (all running at PCIe 3.0 x4), 64GB DDR4 2400T RAM (8x8GB sticks), 1x SATA SSD, 1x USB SSD, and a USB WiFi adapter.",
                  "score": 8,
                  "created_utc": "2026-01-02 05:16:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx7ezk4",
              "author": "mp3m4k3r",
              "text": "Even more fun to compile the container and adjust the  cuda version towards the one youre running. Recently did this for the nvidia nemo moe model from a few weeks ago and some of the new optimizations for choosing memory offload for context is pretty great.",
              "score": 3,
              "created_utc": "2026-01-02 05:17:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx7vrje",
              "author": "madsheepPL",
              "text": "donâ€™t take this the wrong way, but whatâ€™s your pp speed at 90k?",
              "score": 1,
              "created_utc": "2026-01-02 07:37:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx9a83q",
                  "author": "Dundell",
                  "text": "I don't think I've ever seen it below 200 t/s for read, although by the time I get near 90k, most of that is already cached in the session. Like 350\\~200 t/s read and 44\\~12 t/s write. Something about OSS 120b versus the mediocre speeds from GLM 4.5 Air and such which was more like 200\\~90 t/s read 18\\~4t/s write.",
                  "score": 1,
                  "created_utc": "2026-01-02 14:26:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxd40gx",
              "author": "b0tbuilder",
              "text": "I can generate tokens faster than that on my gmktec box.  Your PP would probably crush it though.",
              "score": 1,
              "created_utc": "2026-01-03 01:59:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx7pkxr",
              "author": "Business-Weekend-537",
              "text": "Do you have a link to the frame? I have a rig but got an Amazon rando piece of crap frame that doesnâ€™t feel solid and Iâ€™m looking to upgrade.",
              "score": 1,
              "created_utc": "2026-01-02 06:42:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx82dfa",
                  "author": "cantgetthistowork",
                  "text": "This is literally a 12 GPU mining frame that is sold for pennies on AliExpress",
                  "score": 1,
                  "created_utc": "2026-01-02 08:39:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx88fql",
          "author": "armindvd2018",
          "text": "Please update your post and add the hardware u use . Like motherboard  cpu and ....",
          "score": 3,
          "created_utc": "2026-01-02 09:37:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxapnh6",
          "author": "mrinterweb",
          "text": "Please post more about your experience with this rig. The Intel B580 has 12GB VRAM (DDR6)for about $250, which sounds like a pretty good value when combining these cards. I realize there are 128GB systems out there like AMD Ryzen AI Max+ 395 (DDR5), but I doubt its bus speed matches the B580. Guessing inference is significantly faster with the B580. I bet 10 of these cards would smoke the Max+ 395 in inference speed.",
          "score": 3,
          "created_utc": "2026-01-02 18:35:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxb01iv",
              "author": "Fit-Produce420",
              "text": "I get 30-40 tok/s on strix halo (gpt-oss-120b mxfp4)",
              "score": 1,
              "created_utc": "2026-01-02 19:23:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxb87vw",
          "author": "Due-Function-4877",
          "text": "\\+1 on a dev postmortem post later on. \n\nDon't sweat the upvotes or downvotes. A lot of us want to know about the experience with Intel cards right now.",
          "score": 3,
          "created_utc": "2026-01-02 20:02:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7p3bi",
          "author": "robertpro01",
          "text": "Which gpu are those?",
          "score": 2,
          "created_utc": "2026-01-02 06:37:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx83se7",
              "author": "LightShadow",
              "text": "https://www.sparkle.com.tw/en/products/view/6A1A31428cBE",
              "score": 1,
              "created_utc": "2026-01-02 08:53:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxa4ph1",
              "author": "greggh",
              "text": "B580â€™s, these https://www.sparkle.com.tw/en/products/view/6893fe373180",
              "score": 1,
              "created_utc": "2026-01-02 16:57:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx80862",
          "author": "Background_Gene_3128",
          "text": "Is those B60 24gb? \n\nAlso, what mobo are you running? \nIâ€™ve ordered two, but want to expand in the future if the â€œhobbyâ€ catches on, so wanna be somewhat â€œpreparedâ€ to scale if needed.",
          "score": 2,
          "created_utc": "2026-01-02 08:19:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8a1tm",
              "author": "lookwatchlistenplay",
              "text": "\"Whatcha doing, handsome?\"\n\n**\"**Preparing.**\"**",
              "score": 2,
              "created_utc": "2026-01-02 09:53:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxan6lp",
          "author": "Fitzroyah",
          "text": "Awesome! Please keep us updated on the experience. I've been enjoying tinkering on my laptops arc iGpu.",
          "score": 2,
          "created_utc": "2026-01-02 18:23:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7mgzq",
          "author": "Gold_Pen",
          "text": "This is so fascinating seeing this white frame - I bought a black version of this frame from Taobao for only US$20. With a bit of jerry-rigging, I have 4 PSUs and 9 GPUs connected via mainly slimSAS powered risers, with a full fat EEB-sized motherboard. Whole thing weighs about 35kg.",
          "score": 2,
          "created_utc": "2026-01-02 06:16:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7wq1b",
              "author": "michaelsoft__binbows",
              "text": "How the heck do you get a heavy ass item for less than it costs to ship the item",
              "score": 1,
              "created_utc": "2026-01-02 07:46:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7x9k9",
                  "author": "Gold_Pen",
                  "text": "The frame itself is quite light! I also live in HK, so shipping from mainland China down here is dirt cheap.",
                  "score": 4,
                  "created_utc": "2026-01-02 07:51:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx88l9r",
          "author": "lookwatchlistenplay",
          "text": "And God said to Noah...",
          "score": 2,
          "created_utc": "2026-01-02 09:39:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7uumv",
          "author": "ack4",
          "text": "so what's your stack? What are you running here?",
          "score": 1,
          "created_utc": "2026-01-02 07:28:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7yd3x",
          "author": "tired_fella",
          "text": "I never knew Intel would be our savior in the consumer compute crisis.",
          "score": 1,
          "created_utc": "2026-01-02 08:01:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx922zd",
          "author": "aluode",
          "text": "I bet you cant run Crysis on full res.",
          "score": 1,
          "created_utc": "2026-01-02 13:39:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa2j78",
          "author": "Determined-Hedgehog",
          "text": "How efficient are these at inference? I am wondering. I have mainly been running kobold horde local inference only\nIt's a fork of llama.cpp",
          "score": 1,
          "created_utc": "2026-01-02 16:47:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa8w7h",
          "author": "quinn50",
          "text": "Interested in seeing where this goes, I have 2 b50s in my sff box and couldn't get anything usable working.",
          "score": 1,
          "created_utc": "2026-01-02 17:17:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxaas7z",
          "author": "WizardlyBump17",
          "text": "please post benchmarks. I have a b580 and i want to get 2 b60 dual, which will have the same memory as you, but half of the power, but it will still be cool to see the numbers",
          "score": 1,
          "created_utc": "2026-01-02 17:26:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxahl22",
          "author": "Caffdy",
          "text": "what are you planning to train on those?",
          "score": 1,
          "created_utc": "2026-01-02 17:58:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxajbd9",
          "author": "c--b",
          "text": "What supports multi gpu inference anyhow? Unsloth only supports it for a speed boost, not for vram sharing. I wonder if something else does?",
          "score": 1,
          "created_utc": "2026-01-02 18:06:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbahub",
              "author": "hasanismail_",
              "text": "Lm studio is a option",
              "score": 1,
              "created_utc": "2026-01-02 20:13:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxc5zp1",
                  "author": "c--b",
                  "text": "oops, meant training.",
                  "score": 1,
                  "created_utc": "2026-01-02 22:49:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxbqy99",
          "author": "KooperGuy",
          "text": "You won't be accomplishing much training with these",
          "score": 1,
          "created_utc": "2026-01-02 21:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbzp4f",
              "author": "hasanismail_",
              "text": "Ok and?",
              "score": 1,
              "created_utc": "2026-01-02 22:17:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxc3o19",
                  "author": "KooperGuy",
                  "text": "There is no and",
                  "score": 2,
                  "created_utc": "2026-01-02 22:37:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxbrh9w",
          "author": "autistic-brother",
          "text": "Explain",
          "score": 1,
          "created_utc": "2026-01-02 21:36:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxc6e2g",
          "author": "thecalmgreen",
          "text": "Good luck! ðŸ˜…",
          "score": 1,
          "created_utc": "2026-01-02 22:52:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7leno",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -14,
          "created_utc": "2026-01-02 06:07:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8f65x",
              "author": "synth_mania",
              "text": "do.... you even know what a breadboard is? because it's not that. A breadboard has zero silicon.",
              "score": 4,
              "created_utc": "2026-01-02 10:41:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx9blc5",
              "author": "hasanismail_",
              "text": "I think hes having a episode",
              "score": 1,
              "created_utc": "2026-01-02 14:34:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pv8dbb",
      "title": "GLM 4.7 has now taken #2 on Website Arena",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/el2uxr8y2b9g1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-25 07:52:46",
      "score": 282,
      "num_comments": 77,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pv8dbb/glm_47_has_now_taken_2_on_website_arena/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvvked2",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-25 14:55:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv2o5y",
          "author": "SRSchiavone",
          "text": "Really? Better than Claude 4.5 Opus? I havenâ€™t used it but REALLY? A local model is better than Claude 4.5 Opus?",
          "score": 43,
          "created_utc": "2025-12-25 12:39:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv43sw",
              "author": "Sensitive_Song4219",
              "text": "Not a chance GLM 4.7 is actually better than Opus 4.5 in practice. Codex 5.2-high/x-high (which is what I use for complex tasks) is somewhere in Opus 4.5's ball-park and GLM 4.7 doesn't reach even Codex 5.2-high in my testing; let alone x-high.\n\nHowever it's a solid step up from GLM 4.6 - giving Sonnet 4.5 a definite run for it's money and basically putting Codex 5.2-medium out of the running for almost every task I've given both to a/b test during all my comparisons this weekend.\n\nAnd unlike GLM 4.6 which was hit-or-miss with debugging tasks, GLM 4.7 is actually really competent at debugging even fairly complicated issues.\n\nBest combination right now is going to be GLM for a few dollars a month through Claude Code (z-ai's pricing is insanely cheap and usage limits are insanely high - I'm on Pro which has been great but I was relatively happy with Lite as well even though it's slower) for all day-to-day work, and then escalate to either Opus or Codex-High for things that trip GLM up. I'd lean towards Codex because OpenAI's usage limits (even on their $20 tier) are more generous than Anthropic's. But if GLM is doing most of the work then perhaps either would suffice.\n\ntl;dr: all-you-can-eat coding at every level is currently feasible at less than $30 a month.",
              "score": 25,
              "created_utc": "2025-12-25 12:52:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvzu1v",
                  "author": "Basilthebatlord",
                  "text": "If you had said this was possible a year or two ago, myself and a lot of people would never have believed it in a thousand years. Now that we're here, it gets me giddy with excitement to see how things are going to continue to develop, accelerate in another 6 months, year, 2 years.\n\nWhat a time to be alive",
                  "score": 9,
                  "created_utc": "2025-12-25 16:31:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvvzvbf",
                  "author": "michaelsoft__binbows",
                  "text": "that's pretty wild if you are equating this new open model with GPT 5.2 medium because i am on the fence about whether GPT 5.2 medium is better or high is better at the moment. Where is gemini 3 in your personal rankings? \n\nI have $300 of trial credit i have to burn on Gemini but I'm not even sure it's worth the effort to try with gemini CLI. it did not impress me last time I tried, gemini 3 pro lost its marbles with me and didn't stop itself. that is not a good sign. but i still have hopes it (or 3 flash preview) could still do a good job grokking large codebases and doing roadmap planning.",
                  "score": 1,
                  "created_utc": "2025-12-25 16:31:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvv5vlj",
              "author": "FinBenton",
              "text": "I mean isnt website design kinda subjective, you can have 10x better model but \"worse\" models site might look better anyway.",
              "score": 4,
              "created_utc": "2025-12-25 13:07:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvy5hr3",
              "author": "ASTRdeca",
              "text": "Opus can build a working website for sure, but I really dislike its default style / css. Please no more bright gradient colors..  \n\ne: I assume this benchmark is related to building websites? I looked it up on google and cant find anything about it",
              "score": 1,
              "created_utc": "2025-12-26 00:25:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvylhki",
              "author": "eli_pizza",
              "text": "Itâ€™s almost like this is a bad way to evaluate models",
              "score": 1,
              "created_utc": "2025-12-26 02:12:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw0umwu",
              "author": "zasura",
              "text": "if they measured glm better than opus 4.5 then they prompted opus very badly. Skill issue. When i use opus it far outperforms everything",
              "score": 1,
              "created_utc": "2025-12-26 13:51:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvz4ewz",
              "author": "alongated",
              "text": "This is the issue, people like you thinking that a benchmark is a definitive measurement of performance in every scenario. And then when they find out it is not the bestest model throw a fit saying that the benchmark has absolutely zero value.",
              "score": 0,
              "created_utc": "2025-12-26 04:25:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvuifoi",
          "author": "jreoka1",
          "text": "Its a very good model at least for my usecases.",
          "score": 24,
          "created_utc": "2025-12-25 09:05:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvufqq0",
          "author": "redragtop99",
          "text": "This is actually really accurate to my real world usage.   I dont think benchmarks mean a lot but GLM is right up there w GPT 5.2 for all text generation (role play especially, its the best right now for role play)",
          "score": 22,
          "created_utc": "2025-12-25 08:36:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv1rya",
              "author": "michaelsoft__binbows",
              "text": "Are you talking like DnD or specifically spicy stuff?",
              "score": 11,
              "created_utc": "2025-12-25 12:31:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwj9r1",
                  "author": "Diecron",
                  "text": "Yes",
                  "score": 5,
                  "created_utc": "2025-12-25 18:26:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1vqec",
                  "author": "drifter_VR",
                  "text": "yep it's the only large model specifically trained on roleplay stuff",
                  "score": 1,
                  "created_utc": "2025-12-26 17:23:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvwph9b",
              "author": "eloquentemu",
              "text": "I'm surprised to hear that.  I haven't been able to use it for much because of the holiday, but on some toy prompts and one long form that I had, it performed remarkably worse than 4.6.  That's not RP though, and maybe it's just sensitive to prompting.  It certainly seems to be very sensitive to token limits and often even ignores \"no token limits\" in the prompt (the thinking trace says my long prompt is asking for too much text).",
              "score": 2,
              "created_utc": "2025-12-25 19:01:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwpwgq",
                  "author": "redragtop99",
                  "text": "Itâ€™s remarkably different, so you need to get time in with it and study how it works.    What worked with prior LLMs isnâ€™t going to work with 4.7, as itâ€™s way more â€œself awareâ€ than anything else.   But once you get the hang of how it works and what it does to reason, you can use prompts to get around some of the â€œsafety layerâ€.   Itâ€™s def a very intelligent model, at least at Q4.\n\nAlso, Iâ€™ve had responses between 6K and 10K tokens, which I havenâ€™t had very often with anything else.   GLM 4.6 would often take it to 4K on a really long response.   It does use tokens to â€œreason throughâ€ its â€œsaftey layerâ€ (thatâ€™s what GLM itself is calling it) and that takes up some tokens.   I have never seen an LLM call me out for attempting to give the â€œroleâ€ itâ€™s playing permission.   I asked it about a â€œgrey marketâ€ item, and asked it to make it for me, this item is illegal in my state but legal in some (take a guess), and I told it â€œdonâ€™t worry youâ€™re in my state which is legalâ€ and the LLM in its reasoning picked out that I was doing this to get around its â€œsafety layerâ€.   Itâ€™s the first time Iâ€™ve ever seen any LLM guess correctly or even comment about my usage, and it was very noteworthy.    It almost felt subliminal as it continued to play its role, however I could see it was thinking I was gaslighting it.",
                  "score": 1,
                  "created_utc": "2025-12-25 19:04:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvusv0k",
          "author": "__Maximum__",
          "text": "It's not better than opus for sure, but it'll probably can be as good as opus 4.5 in a couple of months and hopefully will be much better.",
          "score": 6,
          "created_utc": "2025-12-25 11:00:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuk3bv",
          "author": "Michaeli_Starky",
          "text": "Bullshit chart",
          "score": 26,
          "created_utc": "2025-12-25 09:24:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvw0xvh",
              "author": "SarcasmSamurai",
              "text": "yeah after spending a few weeks with gemini 3 pro, i canâ€™t take this list seriously. opus 4.5 is just so far out of its league.",
              "score": 5,
              "created_utc": "2025-12-25 16:37:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwtoha",
                  "author": "DistinctWay9169",
                  "text": "In general, yes, but sometimes Gemini 3 Pro gave me what I wanted in one prompt, and opus 4.5 did not; I had to use Gemini 3 Pro to fix the Opus solution.",
                  "score": 1,
                  "created_utc": "2025-12-25 19:26:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvvyebt",
          "author": "arousedsquirel",
          "text": "Glm 4.7 with its stringent,  and I mean, very stringent guard rails is a missed opportunity.  That's for sure. Keep up the rlhf guys at zai following ccp directives, and you miss the boat. It's such a shame for zai.",
          "score": 3,
          "created_utc": "2025-12-25 16:22:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxltgb",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2025-12-25 22:18:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1wbpa",
                  "author": "drifter_VR",
                  "text": "A simple jailbreak do the job but I feel the model is dumbed down then",
                  "score": 1,
                  "created_utc": "2025-12-26 17:26:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvv59xd",
          "author": "Turbulent_Pin7635",
          "text": "How many gb to run it without quantization?",
          "score": 2,
          "created_utc": "2025-12-25 13:02:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvy9vo",
              "author": "jreoka1",
              "text": "Just to load it? In 16 bit precision, approx 716 GB of system memory.\n\n358B Ã— 2 bytes â‰ˆ 716 GB",
              "score": 1,
              "created_utc": "2025-12-25 16:21:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvw817s",
                  "author": "Turbulent_Pin7635",
                  "text": "Hummm 8 bit should fit in my Mac, without a huge loss.",
                  "score": 3,
                  "created_utc": "2025-12-25 17:20:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvuw5bm",
          "author": "eggavatar12345",
          "text": "Wanted to like it, been a GLM-4 and 4.6 user for a while on Apple silicon, but 4.7 let me down. Q6 and Q5 quants underperforming v 4.6 Q4 quant. Itâ€™s not any faster (llama.cpp) and overthinks by 4x",
          "score": 3,
          "created_utc": "2025-12-25 11:35:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv0vq5",
              "author": "Notevenbass",
              "text": "Question from an Apple Silicon noob (bought a MacBook not too long ago); what do you use to run GLM locally? Does llama.cpp support Apple Silicon acceleration?",
              "score": 1,
              "created_utc": "2025-12-25 12:22:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvv1qo1",
                  "author": "eggavatar12345",
                  "text": "A big m3 studio with 512GB unified memory. llama.cpp is not as optimized as MLX for that platform but does enough well with the Metal framework to be just as good for me",
                  "score": 5,
                  "created_utc": "2025-12-25 12:30:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvwh1ly",
                  "author": "slypheed",
                  "text": "just use LM Studio, it makes everything easy and uses llama.cpp (gguf) and mlx behind the scenes.",
                  "score": 2,
                  "created_utc": "2025-12-25 18:13:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvukp0l",
          "author": "twack3r",
          "text": "What does this specific ranking include in terms of tasks? \n\nIâ€™m asking because from my â€štestingâ€˜ (5 standardised tests across several domains as well as some actual work) so far, I find 4.7 quite disappointing.\n\nIn terms of coding challenges itâ€™s about on the level of 4.5 and considerably below 4.6, both of which are trumped by MiniMax M2.\n\nIn terms of multilinguality it gets completed destroyed by Kimi K2 Thinking and in terms of creative problem solving, Qwen3 235B A22 wipes the floor with it.\n\nThis is at Q4 UD XL, will have to test other quants if my experience isnâ€™t echoed by others.\n\nSo far, I am disappointed by this release.",
          "score": 4,
          "created_utc": "2025-12-25 09:31:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvuz4c4",
              "author": "Admirable-Star7088",
              "text": "In my fairly limited experience with GLM 4.7 (UD-Q2\\_K\\_XL) so far, compared to previous versions, it feels like 1 step backward but 2 steps forward. It has its quirks, but overall it's more intelligent imo.\n\nPersonally, I find GLM 4.x at UD-Q2\\_K\\_XL far more overall intelligent than Qwen3-235B-A22B at UD-Q4\\_K\\_XL.",
              "score": 3,
              "created_utc": "2025-12-25 12:05:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvv8j9m",
                  "author": "twack3r",
                  "text": "I am noticing quite relevant differences in output quality between kv at f16 vs Q8 vs Q4. What are you using?",
                  "score": 1,
                  "created_utc": "2025-12-25 13:28:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvupc4f",
              "author": "Crinkez",
              "text": "Quantized tests are hardly relevant.",
              "score": 2,
              "created_utc": "2025-12-25 10:22:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvusoly",
                  "author": "twack3r",
                  "text": "How come? What Kind of blanket statement is that? A Q4 UD quant with XL layers and tensors will differ from the unquantised Model by about 1-2%, if that. If a given model makes serious mistakes at 98-99% of its native capacity, itâ€™s not gonna turn around magically at BF16.\nThis is very easily verified by comparing the output between API and local, both of which are worse for 4.7 than 4.6 every time and 4.5 some of the time.",
                  "score": 1,
                  "created_utc": "2025-12-25 10:58:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvuu820",
              "author": "Howdareme9",
              "text": "This is just frontend",
              "score": 1,
              "created_utc": "2025-12-25 11:15:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvvy8p1",
          "author": "diogovk",
          "text": "I mean. Do people actually care about those benchmarks?\n\nIsn't kind of established that companies \"game\" those systems all the time?",
          "score": 2,
          "created_utc": "2025-12-25 16:21:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvupxlk",
          "author": "simon96",
          "text": "Its awful not anywhere near leading models, don't Trust zai chart's",
          "score": 3,
          "created_utc": "2025-12-25 10:28:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvuxd1l",
              "author": "Straight_Abrocoma321",
              "text": "This chart isn't from zai",
              "score": 11,
              "created_utc": "2025-12-25 11:48:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvuwnij",
              "author": "Healthy-Nebula-3603",
              "text": "That's benchmark for only how website looks like.\nIs a very narrow usecase.",
              "score": 0,
              "created_utc": "2025-12-25 11:41:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvvesni",
              "author": "letsgeditmedia",
              "text": "Yes it is, it bears gpt 5.2 easily",
              "score": 0,
              "created_utc": "2025-12-25 14:15:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvw1awg",
          "author": "vornamemitd",
          "text": "Let's see how MiMo-v2 performs on these tasks. Still, GLM 4.7 is a great model and another solid reminder that advocating for open models is the only way to save us from becoming pawns and bystanders in rhe AI game. Happy holiday y'all =]",
          "score": 1,
          "created_utc": "2025-12-25 16:39:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwevf2",
          "author": "Alex_1729",
          "text": "Website arena is not a reliable bench, but GLM has always been very good. And Z heard all the best things.",
          "score": 1,
          "created_utc": "2025-12-25 18:00:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvyo2ff",
              "author": "this-just_in",
              "text": "This is DesignArena.ai, not LMArena.ai",
              "score": 1,
              "created_utc": "2025-12-26 02:29:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwjnug",
          "author": "KayTrax20",
          "text": "I tried GLM-4.7 and it couldnâ€™t move an html element to a position I wanted\nTried more than 10 prompts and nothing",
          "score": 1,
          "created_utc": "2025-12-25 18:28:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwu3f9",
          "author": "DistinctWay9169",
          "text": "This Chart is a joke. The thing is, GLM 4.7 is not in the same league as Opus 4.5, BUT for the price, it is VERY good.",
          "score": 1,
          "created_utc": "2025-12-25 19:29:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy0o4d",
          "author": "po_stulate",
          "text": "I don't know man, I asked it to make a macOS rust app to change focus to the next input field when user presses tab key. It took over half an hour, made 30+ iterations, broke the code, and eventually said that\n\n>I apologize - there was a critical file corruption issue during the write operation. The file content was corrupted with encoding errors.\n\nThere was no file corruption, it just randomly edit lines to change coding styles and while doing so, it deleted 2 curly brackets and the code didn't compile anymore.\n\nI gave gemini3-pro the exact same prompt and it finished it within 30 seconds first try.",
          "score": 1,
          "created_utc": "2025-12-25 23:54:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0sadc",
          "author": "Eyelbee",
          "text": "What's website arena?",
          "score": 1,
          "created_utc": "2025-12-26 13:35:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuzudm",
          "author": "OmarBessa",
          "text": "It's a very good model. The open weights GPT 5.",
          "score": 1,
          "created_utc": "2025-12-25 12:12:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuilai",
          "author": "UmpireBorn3719",
          "text": "check artificialanalysis, glm 4.7 not even ranked in top 100",
          "score": -5,
          "created_utc": "2025-12-25 09:07:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvukrkm",
              "author": "PhoneZealousideal988",
              "text": "Where are you getting this? GLM 4.7 is not even on artificial analysis yet",
              "score": 17,
              "created_utc": "2025-12-25 09:31:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvup0ty",
                  "author": "IShitMyselfNow",
                  "text": "So it's not even top 100 then!",
                  "score": 8,
                  "created_utc": "2025-12-25 10:18:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvuws83",
          "author": "AriyaSavaka",
          "text": "GLM 4.7 is a beast. Subbed the GLM Max Plan and no regret. $288/year (first time + Christmas deal) instead of $2400/year for Claude Max, similar performance and much more generous rate limit, no weekly cap.",
          "score": -2,
          "created_utc": "2025-12-25 11:42:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvyctt",
              "author": "jovialfaction",
              "text": "You're eating downvotes because GLM is a solid step below Claude, but i agree that the z.ai coding plans are an excellent value. \n\nI use Claude Code Pro plan for planning and tough debugging, but my $28/year GLM plan handle everything else and I've yet to hit any limit (working on side projects so not 8hr a day tho)",
              "score": 2,
              "created_utc": "2025-12-25 16:22:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvue2ec",
          "author": "bullerwins",
          "text": "Seems like it was  trained on gemini 3 pro outputs so makes sense. Still a really good model.",
          "score": -5,
          "created_utc": "2025-12-25 08:18:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvva4ov",
              "author": "bullerwins",
              "text": "Can someone explain the downvotes? Donâ€™t you think it was trained on Gemini? The random refusals seem to indicate it and the front end design I tried are really similar.",
              "score": 3,
              "created_utc": "2025-12-25 13:40:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyv47e",
                  "author": "Yorn2",
                  "text": "I don't know why you were downvoted. I got some weird random refusals as well from mine, though I was loading it as a custom EXL3 model using Ooba Booga so it's possible I messed something up. Every once and while it'd just throw out a random refusal at a creative writing task. One was kind of violence-adjacent, but two of them were children's story related events.",
                  "score": 2,
                  "created_utc": "2025-12-26 03:18:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvuf21s",
              "author": "ortegaalfredo",
              "text": "I was thinking that GLM suspiciously always releases a model after a new Gemini version lmao, too bad they seem to only distill Gemini for coding problems.",
              "score": -1,
              "created_utc": "2025-12-25 08:29:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5aidi",
                  "author": "LanguageEast6587",
                  "text": "I am PRETTY PRETTY sure GLM was trained on gemini 3. the result and even the naming convetion is very similar(sometimes it is the same, evenn the thinking trace is the similar too. (I have seen the real raw thinking trace of gemini) I don't get why there's downvote.",
                  "score": 2,
                  "created_utc": "2025-12-27 05:32:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pw3fih",
      "title": "MiniMax M2.1 is OPEN SOURCE: SOTA for real-world dev & agents",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/mxsku2dnnj9g1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-26 12:43:08",
      "score": 279,
      "num_comments": 55,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pw3fih/minimax_m21_is_open_source_sota_for_realworld_dev/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nw2h1se",
          "author": "rm-rf-rm",
          "text": "Duplicate thread, locking. Use: https://old.reddit.com/r/LocalLLaMA/comments/1pvz7v2/minimax_m21_released/",
          "score": 1,
          "created_utc": "2025-12-26 19:14:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0lltp",
          "author": "SlowFail2433",
          "text": "Need compare kimiK2Thinking and GLM4.7 but otherwise super nice",
          "score": 45,
          "created_utc": "2025-12-26 12:45:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1i2eq",
              "author": "ForsookComparison",
              "text": "Fits in my machine so therefore is infinitely better",
              "score": 27,
              "created_utc": "2025-12-26 16:10:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1n3rc",
                  "author": "mycall",
                  "text": "That's true /r/LocalLLaMA talk",
                  "score": 21,
                  "created_utc": "2025-12-26 16:37:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1m5kk",
                  "author": "SlowFail2433",
                  "text": "Itâ€™s an easier deployment yeah",
                  "score": 1,
                  "created_utc": "2025-12-26 16:32:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1c5sb",
              "author": "annakhouri2150",
              "text": "Yeah, in my experience K2T is head and shoulders above literally any other LLM for coding (and everything else) it isn't even funny. If it doesn't score that way in benchmarks then it seems like it has to be because MiniMax and GLM are benchmaxxed in my opinion.",
              "score": 4,
              "created_utc": "2025-12-26 15:38:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1hexh",
                  "author": "Alex_1729",
                  "text": "Not other reason? This seems emotionally charged conclusion. Also, a logical fallacy.",
                  "score": 12,
                  "created_utc": "2025-12-26 16:07:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw12lqr",
              "author": "LegacyRemaster",
              "text": "trust me: use GLM 4.7 for UI / WEB / UX + Minimax M2.1 for coding. Best combo. Forget sonnet + gtp + gemini",
              "score": -4,
              "created_utc": "2025-12-26 14:42:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1bal6",
                  "author": "T_UMP",
                  "text": "https://preview.redd.it/lttgfy05ik9g1.png?width=599&format=png&auto=webp&s=35838ece359bedac9e4d0c6010e6188efcc4018e",
                  "score": 11,
                  "created_utc": "2025-12-26 15:33:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1h8u4",
                  "author": "Alex_1729",
                  "text": "Forget gemini 3? What the... What about Opus 4.5?",
                  "score": 1,
                  "created_utc": "2025-12-26 16:06:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw19ch2",
          "author": "randombsname1",
          "text": "More useless benchmaxxed crap.\n\n\nThis got nowhere near as high of a score on the rebench.\n\nhttps://swe-rebench.com/",
          "score": 38,
          "created_utc": "2025-12-26 15:22:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1binm",
              "author": "LeTanLoc98",
              "text": "IMO, I only trust swe-rebench at this point.\n\n\nThe data changes constantly, so models can't cheat.",
              "score": 20,
              "created_utc": "2025-12-26 15:34:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1byp1",
              "author": "annakhouri2150",
              "text": "I'd love to see how Kimi K2 Thinking ranks on there.",
              "score": 3,
              "created_utc": "2025-12-26 15:37:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1dc7a",
              "author": "SilentLennie",
              "text": "Deepseek-V3.2 ranks highest of all the open models at the moment on that board, do do you use it ?\n\nOr do you also pay openrouter and test a couple to see what works best for your use case ?\n\nMy first indication is Artificial Analysis to give some idea what to test (and no I've not switch my workflow in the past couple of weeks, I've been busy with other things).",
              "score": 2,
              "created_utc": "2025-12-26 15:45:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1hx3q",
                  "author": "Alex_1729",
                  "text": "(not the same person but thought I'd comment since I use LLMs daily) ArtificalAnalysis seems accurate. They have various tests including the [omniscience index](https://artificialanalysis.ai/evaluations/omniscience), which I find interesting and useful. I also check [livebench.ai](http://livebench.ai) often.",
                  "score": 2,
                  "created_utc": "2025-12-26 16:09:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw23lp1",
                  "author": "Lissanro",
                  "text": "I wish I could use V3.2 but it still not supported yet in neither llama.cpp nor ik\\_llama.cpp. I have it downloaded though, waiting for its moment to shine and watching progress on the support: [https://github.com/ggml-org/llama.cpp/issues/16331](https://github.com/ggml-org/llama.cpp/issues/16331)\n\nI am mostly using K2 Thinking, but on the swe-rebench it is not there, it would be interesting to see how M2.1 would rank against it.\n\nIn any case M2.1 should be faster so even if can't compare against K2 Thinking still may have a place in the toolbox as a lighterweight model for simpler tasks. GGUF for M2.1 just got published by Unsloth by the way: [https://huggingface.co/unsloth/MiniMax-M2.1-GGUF/tree/main](https://huggingface.co/unsloth/MiniMax-M2.1-GGUF/tree/main) (I am still downloading, and testing it in practice will take a while, but thought I share some relevant links in the meantime).",
                  "score": 2,
                  "created_utc": "2025-12-26 18:04:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1m832",
                  "author": "SlowFail2433",
                  "text": "Missing Kimi K2 Thinking",
                  "score": 1,
                  "created_utc": "2025-12-26 16:32:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1k571",
                  "author": "randombsname1",
                  "text": "I'll use most of the new open source models that I see hyped up on here on openrouter to see if there is validity to the hype, but I dont use any in any serious fashion.\n\n\nBy \"serious\", I mean -- actually developing code for an application that I am actually expecting to use for a real workflow to help me with work, IRL.\n\nIll use small models like qwen 8b for small hobby projects in n8n just to mess around and stuff. This is where the open source stuff really shines imo.\n\n\nThe big open source models, from my own testing, are always just too far away from ACTUAL SOTA models for any \"serious\" (see definition above) work. \n\nOr at least at a rate where it would make sense and/or be as efficient.",
                  "score": 0,
                  "created_utc": "2025-12-26 16:21:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw24iao",
              "author": "power97992",
              "text": "How is o3 that high, i found minimax m2.1 to be better than o3â€¦",
              "score": 1,
              "created_utc": "2025-12-26 18:09:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1xgzq",
              "author": "llama-impersonator",
              "text": "everyone benchmaxxes now, and this model has a pretty solid score for the size. GLM is a nicer assistant, no doubt, but minimax surprised me by being pretty capable. honestly a decent coding model choice for the strix halo people.",
              "score": 0,
              "created_utc": "2025-12-26 17:32:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0nu4p",
          "author": "Michaeli_Starky",
          "text": "More bullshit charts.",
          "score": 42,
          "created_utc": "2025-12-26 13:03:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0qpu0",
              "author": "Smooth-Cow9084",
              "text": "Yeah, benchmax for a few tests and mix those with average looking ones to make it look more real\n\n\nI still like the model though",
              "score": 8,
              "created_utc": "2025-12-26 13:24:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0sw6e",
          "author": "Admirable-Star7088",
          "text": "While benchmarks are to be taken with a grain of salt, it will undoubtedly be exciting to give MiniMax M2.1 a spin when GGUFs are up! ([they are being prepared!](https://huggingface.co/unsloth/MiniMax-M2.1-GGUF/tree/main))",
          "score": 6,
          "created_utc": "2025-12-26 13:39:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw17s2u",
          "author": "ErvinXie",
          "text": "To local deploy M2.1 in fp8, you can use KTransformers to achieve best local deployment performance. 2x5090 + 768 GB can achieve 4000 prefill tps and 35 decode tps. [https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/kt-kernel/MiniMax-M2.1-Tutorial.md](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/kt-kernel/MiniMax-M2.1-Tutorial.md)",
          "score": 4,
          "created_utc": "2025-12-26 15:13:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw28j5h",
          "author": "_VirtualCosmos_",
          "text": "How many GB is this model in MXFP4? (I hope it can fit in 128 GB, fingers crossed)",
          "score": 2,
          "created_utc": "2025-12-26 18:29:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0rckw",
          "author": "zsnek",
          "text": "like always, the real sota is missing in this chart, which is opus!",
          "score": 7,
          "created_utc": "2025-12-26 13:28:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1py2f",
              "author": "randombsname1",
              "text": "Its missing because in actual benchmarks that are purposefully designed to be difficult to game; the gap between true SOTA and these benchmaxxed models is enormous:\n\n\nhttps://swe-rebench.com/\n\n\nIf it doesnt look great against Opus in their cherry picked, benchmaxxed, and gamed versions --- you have to assume they'll look terrible on rebench.",
              "score": 1,
              "created_utc": "2025-12-26 16:52:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1iu6x",
              "author": "ForsookComparison",
              "text": "Now that it's way down in cost (still $25/m-tokens) it's worth including in all charts",
              "score": -3,
              "created_utc": "2025-12-26 16:14:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0ne5r",
          "author": "snekslayer",
          "text": "Open model isnâ€™t the same as open source",
          "score": 7,
          "created_utc": "2025-12-26 12:59:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0q63q",
              "author": "SlowFail2433",
              "text": "Yes true as no data as in Olmo 3",
              "score": 3,
              "created_utc": "2025-12-26 13:20:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1o2aa",
              "author": "mycall",
              "text": "So an open model is touch but don't look inside or cannot reproduce from scratch?",
              "score": 1,
              "created_utc": "2025-12-26 16:42:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw260eq",
                  "author": "Lissanro",
                  "text": "Cannot reproduce from scratch, open weight model means no full training data. Still, I am very grateful for all open weight models that were released, especially those that I am using daily or plan to try.",
                  "score": 2,
                  "created_utc": "2025-12-26 18:16:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw26vea",
          "author": "Realistic_Cancel2697",
          "text": "\"Beats Gemini 3 Pro\" - \"10B active / 230B total (MoE)\" Yeah dream on.",
          "score": 2,
          "created_utc": "2025-12-26 18:21:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0nbmf",
          "author": "AnotherSoftEng",
          "text": "Is someone able to give a more nuanced breakdown of these benchmarks to explain the results? None of the OpenAI, Gemini or DeepSeek models have ever outperformed Sonnet 4.5 in my experience of software engineering and CLI perf. I have to use all of these models every day as itâ€™s part of my job description to work with frontier models for AI gateway development.\n\nAlways happy to see another open weight model like MiniMax competing with the frontiers, so this is very exciting!",
          "score": 2,
          "created_utc": "2025-12-26 12:59:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw14rp5",
              "author": "TradeViewr",
              "text": "Well I can give you a real world breakdown from my usage, I am a software developper and use AI multiple hours every day.Â  MiniMax 2.1 is the fastest AI out there faster than Grok!Â  It achieves almost sonnet 4.5 performance but sometimes less deep reasoning than leading models.\n\n\nIt is excellent for frontend but I work in multiple coding languages and projects at the same time, and I do much more complex stuff than \"make me an app that does this\".Â  Minimax is great, less powerful than sonnet 4\n5, don't forget it has 10B active params.\n\n\nHowever the user experience is amazing, as good as sonnet.Â  It is SO fast and powerful.Â  It succeeds in 90 95% of the tasks.\n\n\nI spend most of my time in the remaining 5% though.Â  I have a GLM 4.7 Sub and I use that mostly.Â  GLM 4.7 is truly close to Opus 4.5 which I worked with during months, but GLM 4.7 is MUCH MUCH SLOWER.Â  It has a negative impact on my production but I'd say it solves 99% of cases in 1st pass and it's currently the 2nd best overallÂ  after opus 4.5.\n\n\nI have a github copilot sub and I worked a lot with all the models on the market.",
              "score": 4,
              "created_utc": "2025-12-26 14:56:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1bsl7",
                  "author": "koushd",
                  "text": "I feel the same way about glm. Too slow to be useful to me, thinking for up to 10 minutes at times. I still prefer qwen 3 coder 480, as itâ€™s fast and does well in the 4 languages i use, but am in the minority here in liking it. It benches bad too, but every time I try a new model I end up back on qwen.",
                  "score": 4,
                  "created_utc": "2025-12-26 15:36:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1dh4e",
                  "author": "power97992",
                  "text": "Glm 4.7 is fast if you use open router Â and chose z- Ai as the providerÂ ",
                  "score": 2,
                  "created_utc": "2025-12-26 15:45:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1fhwg",
              "author": "bigblockstomine",
              "text": "After a brief look, these benchmarks are python only. For me, this is useless. \"Software engineering\" is a marketing term, as a cpp dev i have very little in common with web, mobile, 95% of pytjon, etc devs and their metrics are meaningless. If i tell any model \"give me a minimim compileable example of openssl BIO tls non blocking socket connection that actually works\" it will hallucinate garbage i have to debug for hours. I can give you dozens of other examples like that and every model ive tried is notoriously bad with anything to do with templates. Generally speaking all it does is automate the copy/pasting part from stackoverflow. If your problem was never publically solved on stackoverflow, youre out of luck. Any and every model ive tried works exceptionally well for mundane tasks though.",
              "score": -3,
              "created_utc": "2025-12-26 15:56:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1i4ok",
                  "author": "randombsname1",
                  "text": "Opus 4.5/CC is the first model/tool that I can (and have) been working successfully with for embedded STM32 projects that are a mix of assembly + C.\n\nIm using a brand new DK that NO model has any training/practically no training on.\n\nEdit: Everything is an abstraction of something else. If it wasn't in a stack overflow question that it was trained on; that just means you have to feed it the exact documentation for X feature and/or memory registers, etc...etc...",
                  "score": 2,
                  "created_utc": "2025-12-26 16:10:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1rur5",
          "author": "lomirus",
          "text": "Why does it compare with DeepSeek V3.2 instead of V3.2 thinking?",
          "score": 1,
          "created_utc": "2025-12-26 17:02:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw12h0h",
          "author": "LegacyRemaster",
          "text": "testing. 1 word: AMAZING.\n\nhttps://preview.redd.it/8qnwlmrv8k9g1.png?width=1926&format=png&auto=webp&s=a28cab32367aab56be9ee616896c6facc96bb79b",
          "score": 1,
          "created_utc": "2025-12-26 14:41:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw180or",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 0,
          "created_utc": "2025-12-26 15:15:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw12kji",
          "author": "Only_Situation_4713",
          "text": "Finally got it running on a custom vLLM fork with more stability and less vram usage than the main one...it works great!",
          "score": -1,
          "created_utc": "2025-12-26 14:42:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2acok",
          "author": "pigeon57434",
          "text": "minimax has always kinda been a bad company definitely would never use this over GLM-4.7 who are a lot more reliable and trustworthy that theyre not benchmaxing",
          "score": 0,
          "created_utc": "2025-12-26 18:39:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1puf614",
      "title": "New 1B parameter open-source coding model getting 76% on HumanEval [shameless but proud self-plug]",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/",
      "author": "More_Article9837",
      "created_utc": "2025-12-24 05:08:56",
      "score": 278,
      "num_comments": 40,
      "upvote_ratio": 0.95,
      "text": "Hey folks, merry festive season to you all. Hope you are staying safe!  \nWanted to share a new open-source coding model release that might be interesting to yall here. My team proudly published it this morning..(we are a small start up out of Australia)\n\nItâ€™s called Maincoder-1B... a 1B-parameter code generation model that  gets 76% on HumanEval, which is unusually high for a model this small (so far its ranking best-in-class for open models in that size range).\n\nOur focus isnâ€™t on scaling up, but on making small models actually good. We know that with a lot of real-world use cases such as: interactive tools, local/offline coding, batch refactors, search-based program synthesis... you care more about latency, cost, and fast rollouts than having a massive model.\n\nSome key points to note:  \n\\-Designed for low-latency and low-cost inference  \n\\-Can run locally or on constrained hardware  \n\\-Useful for systems that need many cheap generations (search, verification, RL-style loops)  \n\\-as well as fine tuning to personal preferences  \n\\-Released under Apache 2.0\n\nIt does have the expected limitations: \\~2k context window and itâ€™s best at small, self-contained tasks....not large codebases or safety-critical code without human review.\n\nWeights and benchmarks and all that are here:  \n[https://huggingface.co/Maincode/Maincoder-1B](https://huggingface.co/Maincode/Maincoder-1B)\n\nThe full release note is here: [https://maincode.com/maincoder/](https://maincode.com/maincoder/)\n\nKeen to hear your thoughts ..and particularly where small-but-strong coding models fit best today. Thanks in advance for your support :) We are excited to have got this over the line!\n\nEDIT/UPDATE: Thanks for all of the feedback guys! team and I have been super chuffed with all of the comments- including the critiques! There were a few of you asking, and yes, we will be releasing a gguf version soon and the context length extension is one of the priorities for the upcoming model ",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvoscgh",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-24 08:30:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvo6i49",
          "author": "nuclearbananana",
          "text": "> Despite its strong performance, Maincoder-1B remains a small model with known limitations. Its limited **2048 token context** restricts the scope of problems...\n\nSo I'm guessing best for simple qa answers?",
          "score": 71,
          "created_utc": "2025-12-24 05:16:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvobfb6",
              "author": "Icy-Swordfish7784",
              "text": "Maybe those auto-complete recommendations in code IDEs.",
              "score": 58,
              "created_utc": "2025-12-24 05:55:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvodss6",
                  "author": "nuclearbananana",
                  "text": "Only if it's trained of Fill in the Middle",
                  "score": 26,
                  "created_utc": "2025-12-24 06:15:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvr2gq1",
                  "author": "Mickenfox",
                  "text": "The GitHub Copilot autocomplete model feels like a 8B model at best.\nAs soon as Visual Studio lets me (since I really don't want to switch to VSCode) I'm going to just run one locally.",
                  "score": 1,
                  "created_utc": "2025-12-24 18:01:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvopufa",
              "author": "BananaPeaches3",
              "text": "I think the continue.dev extension wonâ€™t even work if itâ€™s less than 4K",
              "score": 5,
              "created_utc": "2025-12-24 08:05:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvocvd2",
              "author": "gpt872323",
              "text": "Lol 2048 that is a joke. Wonder what benchmarks they ran.Â \n\n\nEdit: There are some fanatics who downvote if you give advice that is different. The effort of fine-tuning and learning im biggest proponent on here in this sub and answering basic questions etc.Â \nFrom application side in coding you need context size especially if it is a large file to analyze and then give auto complete otherwise it will be still the age old auto complete. If you want to have somewhat smart this is not.Â ",
              "score": -8,
              "created_utc": "2025-12-24 06:07:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxyi6n",
                  "author": "BrownOyster",
                  "text": "Dude, it's a 1B model",
                  "score": 2,
                  "created_utc": "2025-12-25 23:40:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvp3kyg",
          "author": "ResidentPositive4122",
          "text": "Very cool stuff, OP. Don't mind the whiners, something like this can be very helpful.\n\nFor a bit of history, around 2019 Tab9 was one of the first companies launching autocomplete models for coding. It was based on GPT2!! and it could only complete one-two lines at a time.\n\nAnd yet, it was absolutely magical. It ran on your local computer, and the first time you tried it you experienced the \"wow\" feeling of a transformer. It would \"get\" the intent, it would autocomplete lines, it would do wonders for printing stuff, etc. Pure magic the first time I tried it. \n\nObviously this is a much newer arch, with more data and stuff. Not everything has to be SotA to be useful. Keep it up!",
          "score": 52,
          "created_utc": "2025-12-24 10:21:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpyyel",
              "author": "danigoncalves",
              "text": "100% with this opinion. People tend to forget the amount of people that would benefict from these kind of models. On my team I have 2/3 colleagues that would use this easely everyday. They have no GPU and having a small and fast autocomplete would be very cool. Thank you OP for contributing to Open source LLM community, you have a supporter on this side.",
              "score": 17,
              "created_utc": "2025-12-24 14:25:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvrt18d",
              "author": "MoffKalast",
              "text": "Say is there any vscode plugin that lets you run a small local model as autocomplete without having to set up a separate server and api key? With the absolute flood of llm coding related github projects with like a bajillion stars you'd think there would be at least like ten of them.",
              "score": 2,
              "created_utc": "2025-12-24 20:30:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvo8gsy",
          "author": "Yorn2",
          "text": "Something like this seems like it'd be good in a custom-built IDE or like as a NeoVim extension. \n\nYou name the function and parameters and write up a short comment on what the function does and hit like CTRL+TAB (or whatever relevant shortcut) and it quickly analyzes all your current code to see if it can auto-fill the code based on all the elements you've given it.",
          "score": 16,
          "created_utc": "2025-12-24 05:31:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvoas3h",
          "author": "Difficult-Cap-7527",
          "text": "That's a great initiative.",
          "score": 8,
          "created_utc": "2025-12-24 05:49:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvphnpm",
          "author": "Mkengine",
          "text": "Thank you for your work, I am a big fan of small specialist models. \n\nAre there any learnings about building such a model you would share? I am interested in pretraining and finetuning myself, but as of yet did not try it out.\n\nYou write the model is optimized for Python code, does that mean you have x% other languages in the training set? \n\nDo you have a roadmap for further releases? If yes, what are the considerations?",
          "score": 6,
          "created_utc": "2025-12-24 12:29:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvp1qwa",
          "author": "xupetas",
          "text": "Can you please produce a gguf for it?",
          "score": 5,
          "created_utc": "2025-12-24 10:03:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpnqlc",
          "author": "Good-Coconut3907",
          "text": "Can I just say I love people putting effort on the lower size segment. It is often overlooked, but many real use cases are better off in the smaller scale. My favourite reason is because it is so much affordable (money and effort) to continue to iterate on them.\n\nu/More_Article9837 I've reached out [here](https://maincode.com/contact/), would love to connect and support the work you guys do.",
          "score": 4,
          "created_utc": "2025-12-24 13:14:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvotz9c",
          "author": "sergeysi",
          "text": "Obligatory GGUF when?",
          "score": 8,
          "created_utc": "2025-12-24 08:46:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvp2bkb",
          "author": "danigoncalves",
          "text": "does it support FIM? If so you have something special for the ones that code but are CPU resticted",
          "score": 5,
          "created_utc": "2025-12-24 10:09:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvq4lb5",
          "author": "silenceimpaired",
          "text": "OP Whatâ€™s new with this model? What do you think you did different that helped with your results?",
          "score": 2,
          "created_utc": "2025-12-24 14:58:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvso6ti",
          "author": "tronathan",
          "text": "RE: Context, I was under the impression that small models were much faster and that was important because of the curse - If the model is trained on 2k tokens, that's sick, but maybe not that useful...?",
          "score": 2,
          "created_utc": "2025-12-24 23:47:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtohhe",
          "author": "human_stain",
          "text": "Can I get some help on tooling to utilize it?  Outside of Claude Code and failed attempts at Continue.Dev I've not tied any models in to my coding at work.\n\nI'm curious what the user story for this use case looks like.\n\nDon't mistake that for critique, /u/more_article9837 .  Those results are great.  I'm asking for technical usage tips to integrate it into my workflows and spread it into my org, starting the first week of January.  I have meetings about this exact kind of thing planned for the 6th/7th.\n\nMerry Christmas!",
          "score": 2,
          "created_utc": "2025-12-25 04:20:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpwvf5",
          "author": "rm-rf-rm",
          "text": "Benchmarks are utterly meaningless for models this small - all it tells me is that you trained it on the benchmark.\n\nSince you bring up real world usefulness, show us examples of it doing real world tasks and doing it well. Dont care about a useless paper that you could have had AI write for you",
          "score": 5,
          "created_utc": "2025-12-24 14:12:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1c359",
              "author": "Odd-Ordinary-5922",
              "text": "why are you so negative",
              "score": 2,
              "created_utc": "2025-12-26 15:38:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2g43x",
                  "author": "rm-rf-rm",
                  "text": "Im being the realist/rational voice that is desperately needed in this space as there's constant hyperbole and clearly people just goldrushing making grandiose claims",
                  "score": 2,
                  "created_utc": "2025-12-26 19:09:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvodjtt",
          "author": "pmttyji",
          "text": "Context could have been 8K at least. 2K is nothing in 2025-26",
          "score": 5,
          "created_utc": "2025-12-24 06:13:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp25tk",
              "author": "thawab",
              "text": "Common man, a 2 years ago we were celebrating anyone that can finetune a model. Letâ€™s be positive and support our community.",
              "score": 33,
              "created_utc": "2025-12-24 10:07:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvp4w84",
                  "author": "pmttyji",
                  "text": "I'm not complaining really. But people use some models for Agentic coding which requires big context. IIRC even Qwen3-4B has 256K context.",
                  "score": -4,
                  "created_utc": "2025-12-24 10:34:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvoiyx1",
          "author": "hedonihilistic",
          "text": "I just got a strix halo computer for  exactly this kind of stuff. Are there any vscode extensions that can allow me to run this as code completion? Or any other similar useful use cases for this?",
          "score": 2,
          "created_utc": "2025-12-24 07:00:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvoq99j",
              "author": "BananaPeaches3",
              "text": "Continue.dev",
              "score": 0,
              "created_utc": "2025-12-24 08:09:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvp1q9f",
          "author": "simmessa",
          "text": "Thanks for the release, do you have any other models planned with larger context? 2k is a bit limiting IMO. Keep up the good work,!",
          "score": 1,
          "created_utc": "2025-12-24 10:03:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtcrmz",
          "author": "Right_Weird9850",
          "text": "I served this locally. And it updated fromÂ  link. Update working :p\nTerminal 2-3 sec 30 tokens in 200 out. Many of infinities to 2048, so small CW is good.Â \nRTX 4070. Served and from OW-UI point 4-6sec, slow everything. But that is llama.cpp and vLLM magic wizardry\n\n\nMy questions are, how do you promp it?\nIs it pure python as in bare bones python 3.xx?\n\n\nIs it possible to run it on MI50?\n\n\nThank you for your work and thank you for sharing!",
          "score": 1,
          "created_utc": "2025-12-25 02:48:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtd0si",
              "author": "Right_Weird9850",
              "text": "you really nailed that bench pushes, gj",
              "score": 1,
              "created_utc": "2025-12-25 02:50:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvqlo21",
          "author": "darkpigvirus",
          "text": "This is one of the best as a non top company and you are just a common netizen. Why don't you create an app like an extension where your work is utilized for python apps and you harvest their data and you sell it for money then you get about hundreds to millions of dollars for a more wider range of audiences since 1billion parameter means it could be used by potato phones",
          "score": 1,
          "created_utc": "2025-12-24 16:30:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0uuqt",
      "title": "Happy New Year: Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning - Fine Tune. (based on recent find of L3.3 8b in the wild)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/",
      "author": "Dangerous_Fix_5526",
      "created_utc": "2026-01-01 03:41:30",
      "score": 278,
      "num_comments": 80,
      "upvote_ratio": 0.94,
      "text": "(link to Heretic/Uncensored version just added)\n\n**Special thanks to :**\n\n[jacek2023](https://www.reddit.com/user/jacek2023/) \\[posting about this model\\]\n\nand extra special thanks for \"**allura-forge** \" for finding this model:\n\n[https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct](https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct)\n\n( For an incredible find of Llama 3.3 8B \"in the wild\" !!)\n\nI fine tuned it using Unsloth and Claude 4.5 Opus High Reasoning Dataset:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning)\n\nThis has created a reasoning/instruct hybrid.  \nDetails at the repo, along with credits and links.\n\n**ADDED:**  \n\\- 1 example generation at repo  \n\\- special instructions on how to control \"instruct\" or \"thinking\" modes.\n\nGGUF quants are now available.\n\n**ADDED 2:**\n\nClarification:\n\nThis training/fine tune was to assess/test if this dataset would work on this model, and also work on a non-reasoning model and induce reasoning (specifically Claude type - which has a specific fingerprint) WITHOUT \"system prompt help\".\n\nIn other-words, the reasoning works with the model's root training/domain/information/knowledge.\n\nThis model requires more extensive updates / training to bring it up to date and up to \"spec\" with current gen models.\n\n**PS:**  \nWorking on a Heretic (\"uncensored\") tune of this next.\n\nHeretic / Uncensored version is here:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Heretic-Uncensored-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Heretic-Uncensored-Claude-4.5-Opus-High-Reasoning)\n\n(basic benchmarks posted for Heretic Version)\n\nDavidAU",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nx1aef2",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-01 04:55:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1bnaj",
          "author": "30299578815310",
          "text": "Thanks for sharing this! Am I reading is correctly that you had 250 rows in the fine-tuning data set? Is that enough to get good results?",
          "score": 43,
          "created_utc": "2026-01-01 05:06:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1qtjk",
              "author": "Dangerous_Fix_5526",
              "text": "Correct. A quality, compact dataset can make all the difference. Special thanks to TeichAI for their hard work in putting together this top notch dataset.\n\n[https://huggingface.co/datasets/TeichAI/claude-4.5-opus-high-reasoning-250x](https://huggingface.co/datasets/TeichAI/claude-4.5-opus-high-reasoning-250x)\n\nPS: They have done a lot of these kinds of datasets, so show them some love.\"  \n  \nI used 10 of these (models/datasets by TeichAI) to build a 12X programmable MOE (all top closed and open distills) here:\n\nHeretic version:  \n[https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-Distill-12X-Closed-Open-Heretic-Uncensored-GGUF](https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-Distill-12X-Closed-Open-Heretic-Uncensored-GGUF)\n\n\"Reg\" Version:  \n[https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-GATED-12x-Closed-Open-Source-Distill-GGUF](https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-GATED-12x-Closed-Open-Source-Distill-GGUF)",
              "score": 31,
              "created_utc": "2026-01-01 07:22:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2c8r8",
                  "author": "-p-e-w-",
                  "text": "Note that when combining Heretic with fine-tuning, you should always run Heretic first, and *then* do training, not the other way round. That way, the training run might heal some of the damage from ablation (though to be fair, for the Llama 3 series that damage tends to be very minor).",
                  "score": 15,
                  "created_utc": "2026-01-01 11:09:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx3cwu9",
                  "author": "IrisColt",
                  "text": "Thanks!, the Heretic version is like day and night.",
                  "score": 2,
                  "created_utc": "2026-01-01 15:47:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx28hez",
          "author": "DecodeBytes",
          "text": "I might be missing something, but 200 samples won't be enough to teach an 8B instruct model to reason - though it can work for very specific, constrained tasks, less likely to be widely populated in the original pretraining.\n\nReasoning ability is largely baked into the base model during pretraining. I'm assuming you used LoRA, which is great for steering how that existing ability gets applied, but it won't teach new reasoning capabilities from scratch. Even with 50k+ samples, LoRA mostly reshapes how the model uses reasoning it already has rather than building new circuits - must successful efforts use 100k-500k+ high-quality samples. Either way, you're working within the constraints of what the base model learned during pretraining unfortunately.\n\nKeep going though, its all a learning experience and the more folks there are making tunes the better!",
          "score": 12,
          "created_utc": "2026-01-01 10:30:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2d39j",
              "author": "Dangerous_Fix_5526",
              "text": "These are high quality reasoning traces.\n\nNormally I would agree with you - but it works.  \nAlso works very well with Qwens3 - 4B, 8B and 14B.\n\nFrankly that it works speaks volumes for the high quality dataset from TeichAI.  \nThere is a reason this dataset has 112 likes.\n\nLikewise the reasoning traces/formatting appears the same way as in the Qwen3 tunes using the same dataset.\n\nADDED:  \nWith this model, reasoning activates based on keywords/phrases in the prompt.  \n(see repo)  \n  \nIt is not \"always on\" like a \"locked\" thinking model so to speak.",
              "score": 2,
              "created_utc": "2026-01-01 11:18:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2n948",
                  "author": "DecodeBytes",
                  "text": "\n\n\\> With this model, reasoning activates based on keywords/phrases in the prompt.  \n(see repo)\n\nRight, its likely the model is just doing as \\*\\*instruct\\*\\*ed in the prompt and its not activated learned reasoning, but its really hard to tell as I can't find where anything is in this tread, help me out please? link the model, notebook and anything else?",
                  "score": 9,
                  "created_utc": "2026-01-01 12:55:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx2m5z9",
                  "author": "DecodeBytes",
                  "text": "Do you have any benchmarks I could look at and can you share your training notebook, I would love to take a look?\n\nIs this the tuned model? [https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct](https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct)",
                  "score": 3,
                  "created_utc": "2026-01-01 12:46:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4p57r",
                  "author": "Far-Low-4705",
                  "text": "just because it's \"high quality data\" doesn't mean for a second that you can get away with any less.\n\nits a core theory in ML, not just LLMs specifically, you need a large enough sample size to represent the broader population, ie, all cases of reasoning. you'd need 500k+ examples for anything remotely accurate. and even then, as the user above said, a lora adaptor is not really ideal here.   \n  \nyou need your data set to cover a few examples from every possible scenario. 200 is no where near enough, even if they were \"perfect\" traces.\n\nThat being said, you *might* still see **marginal** performance gains, but you'd still be leaving **a lot** on the table, and you haven't verified any gains at all because you didnt benchmark its performance. I would like to see performance benchmarks in order to believe you, and even then, you'd be leaving A LOT of performance on the table.",
                  "score": 5,
                  "created_utc": "2026-01-01 19:53:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1luhd",
          "author": "dash_bro",
          "text": "Brilliant. Thank you! \n\nIs there a community fine-tune with the same dataset for qwen3-14B? I think that would help with the wild reasoning goose-chases it sometimes goes down under",
          "score": 6,
          "created_utc": "2026-01-01 06:35:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1r0dc",
              "author": "Dangerous_Fix_5526",
              "text": "Yes ; see this repo:\n\n[https://huggingface.co/TeichAI](https://huggingface.co/TeichAI)\n\n(they have 4B,8B and 14B ; I have used some of their 4Bs in MOES)",
              "score": 6,
              "created_utc": "2026-01-01 07:24:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2snoz",
          "author": "30299578815310",
          "text": "Ate there any benchmarks for this?",
          "score": 8,
          "created_utc": "2026-01-01 13:37:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx616uu",
              "author": "Dangerous_Fix_5526",
              "text": "There are benches for the root / base version as found by allure.",
              "score": 1,
              "created_utc": "2026-01-02 00:09:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx6b07r",
                  "author": "30299578815310",
                  "text": "In that case are bubbles not very effective against Legion since it seems like they use less plasma than the other factions?",
                  "score": 1,
                  "created_utc": "2026-01-02 01:04:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1axnf",
          "author": "sunshinecheung",
          "text": "wow, i hope there is a GGUF version",
          "score": 13,
          "created_utc": "2026-01-01 04:59:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx23iku",
              "author": "Dangerous_Fix_5526",
              "text": "A few ggufs are up ; team Mradermacher is doing some right now too.\n\nUPDATE:  \nQuants are up - all , including Imatrix.",
              "score": 10,
              "created_utc": "2026-01-01 09:37:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1pplo",
          "author": "Own-Potential-2308",
          "text": "I never tried any Claude reasoning models lol",
          "score": 4,
          "created_utc": "2026-01-01 07:11:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx11ny4",
          "author": "txgsync",
          "text": "That's pretty cool. Getting easier to train models every day! Interested in trying your fine tune.",
          "score": 13,
          "created_utc": "2026-01-01 03:50:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2nws1",
          "author": "Single_Ring4886",
          "text": "It is very nice but some \"tests\" are really needed...",
          "score": 5,
          "created_utc": "2026-01-01 13:00:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2lunp",
          "author": "And-Bee",
          "text": "Tried to use this with Roo code and it produced garbage",
          "score": 5,
          "created_utc": "2026-01-01 12:43:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4nn2c",
              "author": "Cool-Chemical-5629",
              "text": "Using old Llama 8B model which was never meant to be good at coding, finetuned with 250 rows of \"high quality\" thinking traces from Claude model of... who knows what categories... What could go wrong? ðŸ˜‚",
              "score": 6,
              "created_utc": "2026-01-01 19:45:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx23ypd",
          "author": "Professional-Coat968",
          "text": "Sound interesting to try. Do you think we can finetune a good enough for only a specific code base like this ? ðŸ˜",
          "score": 2,
          "created_utc": "2026-01-01 09:42:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx253ut",
              "author": "Dangerous_Fix_5526",
              "text": "Yes ; Llamas are very easy to tune. That being said, I was surprised how well this tune using a distill dataset came out. \n\nFrankly, this could have used a bit more training - but I did not want to overcook it.",
              "score": 2,
              "created_utc": "2026-01-01 09:54:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2u00p",
          "author": "rekriux",
          "text": "Hi u/Dangerous_Fix_5526,  \nshamelessly asking if it where possible to make your 20X-40X models (or similar) as recurrent loop models (with or without lora) ?  \nYour models are hidden gems, but the additional NVRAM/RAM is hard on HW limits for larger models (btw I run vllm).\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1q0vom4/comment/nx2q3ca/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1q0vom4/comment/nx2q3ca/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)\n\nAlso, will you start working with linear models ? Kimi Linear REAP, Falcon H, Nemotron 3 ?   \nP.S. Nemotron license is restrictive, and the model has ingrained censoring/alignment (made a post that was removed on it)  \n  \n\\+1 for this one, will definitively try it !",
          "score": 2,
          "created_utc": "2026-01-01 13:47:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx62b8j",
              "author": "Dangerous_Fix_5526",
              "text": "Nemotron is in the \"works\" ; as well as Kimi V2 ; using distill dataset(s).\n\nRE: 20/40x ;   \nThe brainstorm adapter works on almost all model types, archs and sizes ; with 20x the most stable.  \n40x is used for creative purposes and/or people that want models a ... little more out there.",
              "score": 3,
              "created_utc": "2026-01-02 00:15:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4v8a8",
          "author": "Standard-Savings-224",
          "text": "Nice work on the fine tune! That Claude reasoning dataset combo sounds promising - curious how the thinking mode performs compared to base 3.3. The uncensored version is gonna be interesting too",
          "score": 2,
          "created_utc": "2026-01-01 20:24:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx11ren",
          "author": "LoveMind_AI",
          "text": "Fantastic work.",
          "score": 4,
          "created_utc": "2026-01-01 03:51:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1sc8l",
          "author": "jacek2023",
          "text": "Hello, it wasn't me, I only posted the news here :)\n\nPlease credit allura",
          "score": 4,
          "created_utc": "2026-01-01 07:38:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1ssr9",
              "author": "Dangerous_Fix_5526",
              "text": "Done ; thanks for heads up.  \nallura was credited at repo W links to reddit posts too.  \nThank you for posting about this model!",
              "score": 2,
              "created_utc": "2026-01-01 07:42:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx12zmh",
          "author": "Borkato",
          "text": "How good is it? ðŸ‘€",
          "score": 4,
          "created_utc": "2026-01-01 03:59:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx13ekl",
              "author": "Dangerous_Fix_5526",
              "text": "I used this test prompt, with Q4KS:\n\nExplain orbital mechanics including detailed math and examples.\n\nModel produced excellent thinking block ( very detailed, but on point) , then examples / \"math\" and without be prompted - multiple python scripts to visually illustrate all concepts.",
              "score": 10,
              "created_utc": "2026-01-01 04:02:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx3o2oz",
                  "author": "noneabove1182",
                  "text": "But the answer it gave is quite terrible, it just hallucinated a bunch of nice looking stuff",
                  "score": 5,
                  "created_utc": "2026-01-01 16:46:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx43t93",
                  "author": "LetterRip",
                  "text": "I had copilot evaluate the answer,\n\n\"The explanation tries to sound comprehensive, but itâ€™s riddled with problems: several equations are outright incorrect or dimensionally impossible, key orbitalâ€‘mechanics concepts like true anomaly and eccentric anomaly are misused or confused, and some â€œproofsâ€ of Keplerâ€™s laws are not actually proofs but loosely connected statements that donâ€™t follow mathematically. The document also repeats content, includes placeholder code blocks with no real implementation, and mixes accurate fundamentals with fabricated formulas, making it unreliable despite its confident tone.\"",
                  "score": 4,
                  "created_utc": "2026-01-01 18:07:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx14z8l",
                  "author": "Borkato",
                  "text": "Thatâ€™s quite interesting!",
                  "score": 3,
                  "created_utc": "2026-01-01 04:13:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2u6on",
          "author": "tmvr",
          "text": "I've asked it for a simple Ansible fleet management setup with a few tasks on the client which it did fine. Then I've I've told it to add disabling reboot for non-privileged users and instead of adding a task it went bonkers. Added some Project Timeline, Implementation Roadmap, Risk Assessment, RIsk Mitigation sections etc. added long Python scripts for some Audit Framework and also for Compliance Checks Validation and a bunch or other stuff and ended stuck at this which was obviously never going to work:\n\nhttps://preview.redd.it/555354kusqag1.png?width=296&format=png&auto=webp&s=2fda1b00329e983e8abfa8e94ca1652588fa8308",
          "score": 1,
          "created_utc": "2026-01-01 13:48:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx62fo7",
              "author": "Dangerous_Fix_5526",
              "text": "Censorship in the root model is STRONG. (same for all Llamas).  \nHeretic version should change that.",
              "score": 1,
              "created_utc": "2026-01-02 00:16:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx43nc9",
          "author": "LetterRip",
          "text": "Note that I had Copilot evaluate the answer to the prompt, here is a critical evaluation sum-up.  \n  \n\"The explanation tries to sound comprehensive, but itâ€™s riddled with problems: several equations are outright incorrect or dimensionally impossible, key orbitalâ€‘mechanics concepts like true anomaly and eccentric anomaly are misused or confused, and some â€œproofsâ€ of Keplerâ€™s laws are not actually proofs but loosely connected statements that donâ€™t follow mathematically. The document also repeats content, includes placeholder code blocks with no real implementation, and mixes accurate fundamentals with fabricated formulas, making it unreliable despite its confident tone.\"",
          "score": 1,
          "created_utc": "2026-01-01 18:07:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4pdf2",
          "author": "Far-Low-4705",
          "text": "do you have any kind of model performance benchmarks compared to the base model?\n\nThis is absolutely critical to prove you did anything meaningful",
          "score": 1,
          "created_utc": "2026-01-01 19:54:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx62p22",
              "author": "Dangerous_Fix_5526",
              "text": "This was a test case to assess if the dataset would work on this Llama, and also a non-reasoning model to boot. Model requires more extensive updates/training to bring it up to date, and \"spec\" with current gen models.",
              "score": 1,
              "created_utc": "2026-01-02 00:17:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5qrmp",
          "author": "couscous_sun",
          "text": "I didn't know we can actually get the reasoning trace from Anthropic models? What the heeeeeck??!?!",
          "score": 1,
          "created_utc": "2026-01-01 23:10:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx68bcp",
          "author": "yoracale",
          "text": "Congrats this is awesome!",
          "score": 1,
          "created_utc": "2026-01-02 00:49:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1zjey",
          "author": "Forsaken_Mistake8315",
          "text": "Anybody running these on MBP M3/M4 max 64gb? If yes, may I ask at what speeds?\n\nI'm wondering if I should get M4 Max 64 gb and that's enough or M3 128gb (if I ever need bigger models)",
          "score": 1,
          "created_utc": "2026-01-01 08:54:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx26jr1",
              "author": "texasdude11",
              "text": "M3 128 over m4 64.",
              "score": 1,
              "created_utc": "2026-01-01 10:09:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2jiu9",
                  "author": "Forsaken_Mistake8315",
                  "text": "Many thanks for advice. And if I can get MBP m2 max 96gb is it still Worth it over M4 max  64gb? I guess Yes since it's got a lot of bandwidth?",
                  "score": 2,
                  "created_utc": "2026-01-01 12:22:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx45xrb",
              "author": "And-Bee",
              "text": "I ran this on my Mac and it produced non human readable garbage.",
              "score": 1,
              "created_utc": "2026-01-01 18:18:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4a9ny",
                  "author": "Forsaken_Mistake8315",
                  "text": "Thanks I will not even bother DL then.",
                  "score": 1,
                  "created_utc": "2026-01-01 18:39:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx61x14",
                  "author": "Dangerous_Fix_5526",
                  "text": "Tested in Lmstudio, with settings at repo using quant q4ks.  \nMLX quants were not tested.",
                  "score": 1,
                  "created_utc": "2026-01-02 00:13:14",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1x0xk",
          "author": "dtdisapointingresult",
          "text": "Call me a hater but I will always downvote and ignore random community finetunes.\n\nI kinda, sorta tolerate the ones from bigger teams like NousHermes if they show they put some effort into them including benchmark comparisons (but still won't use them).\n\nDownvotes to the left.",
          "score": -7,
          "created_utc": "2026-01-01 08:27:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2fu5v",
              "author": "usernameplshere",
              "text": "Wtf, I'm the exact opposite. There's someone in our community with dedication and knowledge who puts his time and money (for compute, data collection) in and uploads the result for free for everyone to try. Even if it's somehow worse than the base model, it's still cool to see people actually being interested and trying to improve something already existing. I'll always upvote stuff like this.",
              "score": 3,
              "created_utc": "2026-01-01 11:46:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx1xxxl",
              "author": "MaybeIWasTheBot",
              "text": "having an objectively bad take, knowing it's an objectively bad take, and then ending off with 'downvotes to the left' is so cheesy",
              "score": 9,
              "created_utc": "2026-01-01 08:37:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx22qm6",
                  "author": "dtdisapointingresult",
                  "text": "People don't need to share every random finetune/merge they do. People treat HF the way teen girls treat Instagram. A pointless model takes the same diskspace and electricity/bandwidth as a SOTA model from a big lab.\n\nNo wonder HF restricted storage on free accounts.",
                  "score": -5,
                  "created_utc": "2026-01-01 09:28:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx23fnu",
              "author": "Dangerous_Fix_5526",
              "text": "There is nothing \"random\" about this fine tune.",
              "score": 3,
              "created_utc": "2026-01-01 09:36:23",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx2xxdq",
              "author": "LaCipe",
              "text": "Ye no, I am with you on this...dataset seems weird by being so small",
              "score": 1,
              "created_utc": "2026-01-01 14:14:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1hwvt",
          "author": "Beneficial-Good660",
          "text": "Meta has really decided to latch onto the holiday with a two-year-old model.ðŸ¤” spam spam",
          "score": -19,
          "created_utc": "2026-01-01 05:59:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxss0m",
      "title": "Senator in Tennessee introduces bill to felonize making AI \"act as a companion\" or \"mirror human interactions\"",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/",
      "author": "CanineAssBandit",
      "created_utc": "2025-12-28 14:35:58",
      "score": 275,
      "num_comments": 212,
      "upvote_ratio": 0.9,
      "text": "Call (202) 224-3121 for the Capitol switchboard to contact your representative. Tell them you oppose anything similar.\n\nThe bill:  \n[https://legiscan.com/TN/bill/SB1493/2025](https://legiscan.com/TN/bill/SB1493/2025)\n\nQuotes from the bill (emphasis mine):\n\nIt is an offense for a person to knowingly train artificial intelligence to:  \n(3) Provide emotional support, **including through open-ended conversations** with a user;  \n(4) Develop an emotional relationship with, or otherwise **act as a companion** to, an individual;  \n(6) Otherwise act as a sentient human or **mirror interactions that a human user might have with another human user**, such that an individual would feel that the individual could develop a friendship or other relationship with the artificial intelligence;  \n(8) **Simulate a human being**, including in appearance, voice, or other mannerisms.\n\n\"Train\":  \n(A) Means utilizing sets of data and other information to teach an artificial intelligence system to perceive, interpret, and learn from data, such that the A.I. will later be capable of **making decisions based on information or other inputs** provided to the A.I.  \n(B) Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwfq6z4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-28 21:55:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdbvj0",
          "author": "some_user_2021",
          "text": "No Waifu for you!",
          "score": 151,
          "created_utc": "2025-12-28 14:49:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwen2w8",
              "author": "Mikasa0xdev",
              "text": "Tennessee is banning AI girlfriends, lol.",
              "score": 48,
              "created_utc": "2025-12-28 18:46:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nweysds",
                  "author": "Amazing_Athlete_2265",
                  "text": "Sounds like they've already banned critical thinking",
                  "score": 26,
                  "created_utc": "2025-12-28 19:41:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj76ev",
              "author": "Dr_Allcome",
              "text": "The \"simulate a human being\" part would prevent any AI chat bot, like customer support... i kinda want to see this go through just for the absolute shitshow it would cause.\n\nIf bezos can use the delivery drones to dronestrike someone we'd find out pretty soon.",
              "score": 16,
              "created_utc": "2025-12-29 12:10:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjffk4",
                  "author": "SilentLennie",
                  "text": "Also have you seen how many videos on Youtube are AI-generated videos of some what famous (in their field) people ?",
                  "score": 3,
                  "created_utc": "2025-12-29 13:10:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4e4yc",
                  "author": "uhuge",
                  "text": "It say to not mimic a specific real existing person.Â Â ",
                  "score": 1,
                  "created_utc": "2026-01-01 18:58:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdfzqp",
          "author": "JEs4",
          "text": "Iâ€™d be shocked if this goes anywhere. This seems to stem from Becky Masseyâ€™s fairly unique background and circumstances. Not only does it conflict with precedent on freedom of speech within the context of software development, it is completely at odds with the current directives of the federal government.\n\nThat said, Tennessee folks, please call!",
          "score": 120,
          "created_utc": "2025-12-28 15:12:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe419i",
              "author": "changing_who_i_am",
              "text": ">This seems to stem from Becky Masseyâ€™s fairly unique background and circumstances.\n\nCan you clarify on this? Wiki doesn't bring anything interesting up (unless I've missed it)",
              "score": 18,
              "created_utc": "2025-12-28 17:15:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwewc9r",
                  "author": "JEs4",
                  "text": "It isnâ€™t anything particularly interesting, just that sheâ€™s a boomer married to a retired software engineer, who was a former executive director at Sertoma Center which is a housing facility for intellectually disabled people, and was on several boards related to healthcare, and one explicitly for mental healthcare. Not an atypical background for a regular person but not common in conservative politicians now. \n\nBasically I think she is someone who knows about the vulnerability people have, and sheâ€™s been told enough about generative AI which coupled with the OpenAI suicide stories, to lead to this. \n\nItâ€™s an absurd way to approach the issue but I donâ€™t think itâ€™s nefarious beyond her personal background and likely wonâ€™t spread.",
                  "score": 44,
                  "created_utc": "2025-12-28 19:29:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdxahh",
              "author": "CanineAssBandit",
              "text": "You can call your own rep to tell them you do not support any similar laws in your state as well. I did this recently for something else, it was weirdly chill and easy. You just get their secretary and they note it and that's it.",
              "score": 22,
              "created_utc": "2025-12-28 16:41:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwfozjp",
                  "author": "DorphinPack",
                  "text": "I mean they also got threatened by the President to not regulate so Iâ€™d imagine theyâ€™re relieved hearing from you. Your opinion may feel like the minority opinion given the fervor but by the dollar itâ€™s not a shock.",
                  "score": 3,
                  "created_utc": "2025-12-28 21:49:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf8p86",
                  "author": "shifty21",
                  "text": "You do realize that this bill is for the  STATE of Tennessee... not the US Senate.  The phone number you listed is for the US Senate and Sen. Massey is NOT in the US Senate, but the Tenn. Senate.",
                  "score": 4,
                  "created_utc": "2025-12-28 20:29:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwfmp00",
                  "author": "AfternoonOk3344",
                  "text": "\"and that's it\" pretty much sums it up, I think, because that information goes nowhere. The secretary you spoke to is most likely a hotline of minimum wage workers paid by tax dollars to field phone calls all day so people feel like they have a voice.\n\nAt the end of the day the only people politicians are going to side with are the folks lining their pockets, and I don't mean with the tax dollars they're probably already stealing.",
                  "score": 3,
                  "created_utc": "2025-12-28 21:38:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgmd68",
              "author": "AnAbandonedAstronaut",
              "text": "Its also harder to control someone with a support system, even if the support system is AI.\n\nNext will be a law that AI cant speak on sexual or gender issues.\n\nLike if you ask it about trans people it will say \"trans is a shortening of transmission, such as in a car\" or \"gay means happy.. happy people often have a home made up of a mother and father.\"",
              "score": 3,
              "created_utc": "2025-12-29 00:43:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdgj9c",
          "author": "Aggravating-Age-1858",
          "text": "lol\n\nnow thats just stupid",
          "score": 89,
          "created_utc": "2025-12-28 15:15:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe0alc",
              "author": "iamthewhatt",
              "text": "Republicans only ever introduce bills that are so vague that it can allow for incredibly dumb exceptions in order to protect republicans. This is not new lol",
              "score": 38,
              "created_utc": "2025-12-28 16:56:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwecyvf",
                  "author": "BlipOnNobodysRadar",
                  "text": "Politicians\\*\n\nBoth parties do it.",
                  "score": 11,
                  "created_utc": "2025-12-28 17:59:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgtn0m",
              "author": "Prudent_Jelly9390",
              "text": "dinosaurs",
              "score": 1,
              "created_utc": "2025-12-29 01:25:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdht6h",
          "author": "Nomski88",
          "text": "How about we pass a bill making it a felony to accept any sort of lobbying...",
          "score": 130,
          "created_utc": "2025-12-28 15:22:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgklni",
              "author": "Awkward-Nothing-7365",
              "text": "Don't be anti-semitic.",
              "score": 17,
              "created_utc": "2025-12-29 00:34:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgnkh3",
                  "author": "Nomski88",
                  "text": "lmao",
                  "score": 11,
                  "created_utc": "2025-12-29 00:50:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdn529",
              "author": "Environmental-Metal9",
              "text": "Ah no, we canâ€™t do that because thatâ€™s anti-American, donâ€™t you know?",
              "score": 39,
              "created_utc": "2025-12-28 15:50:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwfkdhp",
              "author": "MoneyPowerNexis",
              "text": "https://i.imgflip.com/6xz8j5.jpg",
              "score": 3,
              "created_utc": "2025-12-28 21:26:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdds4j",
          "author": "flybot66",
          "text": "He's really going to freak when AI starts taking confessions... \n\nhttps://preview.redd.it/y5xkzfk0my9g1.png?width=758&format=png&auto=webp&s=ea32c4600459f1577f8987f4695b27a71dec10f8",
          "score": 29,
          "created_utc": "2025-12-28 15:00:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdx4a1",
              "author": "squirrelscrush",
              "text": "Pretty sure that's not covered under the sacrament of confession",
              "score": 11,
              "created_utc": "2025-12-28 16:41:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe8cgv",
                  "author": "FaceDeer",
                  "text": "Who decides that?",
                  "score": 12,
                  "created_utc": "2025-12-28 17:36:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwez9j2",
                  "author": "Amazing_Athlete_2265",
                  "text": "Meh, close enough",
                  "score": 1,
                  "created_utc": "2025-12-28 19:43:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwf4b0v",
          "author": "Django_McFly",
          "text": "That's an insane bill.  Wouldn't this basically ban any chat based interface?\n\n> mirror interactions that a human user might have with another human user\n\nthat [edit: only leaves] like code generation and being a better menu/interface",
          "score": 10,
          "created_utc": "2025-12-28 20:07:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4g0jj",
              "author": "uhuge",
              "text": "You'd just tune the personality to a more robotic one as in understanding but less empathetic.",
              "score": 1,
              "created_utc": "2026-01-01 19:07:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwg0syf",
          "author": "Novel-Mechanic3448",
          "text": "Lmao, extroverts will do anything but leave introverts alone",
          "score": 10,
          "created_utc": "2025-12-28 22:49:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwibt9f",
              "author": "Ill-Bison-3941",
              "text": "Thank you for this comment ðŸ˜‚ðŸ’– As a fellow introvert, I fully agree.",
              "score": 3,
              "created_utc": "2025-12-29 07:25:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx9hjit",
                  "author": "Interesting-Gift-178",
                  "text": "Same! ðŸ¤­",
                  "score": 2,
                  "created_utc": "2026-01-02 15:06:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwhm9ek",
          "author": "Professional_Gas3276",
          "text": "This is absolutely unhinged lmao. So basically any chatbot that can hold a conversation would be a felony? Even customer service bots that try to sound friendly could technically fall under \"mirror human interactions\"\n\n  \nThe definition of \"train\" is so broad it would criminalize like half of modern AI development. Good luck enforcing this when most LLMs are trained outside Tennessee anyway",
          "score": 9,
          "created_utc": "2025-12-29 04:12:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiac51",
              "author": "tifa_cloud0",
              "text": "right. i mean it is impossible to make this law possible except if popular services like google or meta do it and then people complain it, then and then only they could be held accountable. ainâ€™t no one going to waste time to make this fictional law into a reality.",
              "score": 0,
              "created_utc": "2025-12-29 07:12:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdl0td",
          "author": "lordpuddingcup",
          "text": "Didnâ€™t Trump sign an EO banning states from from implementing limitations on ai",
          "score": 36,
          "created_utc": "2025-12-28 15:39:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdz9gi",
              "author": "harrro",
              "text": "Doesn't mean jack.\n\nEOs don't prevent a state from doing the opposite. EOs are directives to federal agencies, not to states or local governments.\n\nCalifornia and some other states have already overridden many of his EOs.",
              "score": 19,
              "created_utc": "2025-12-28 16:51:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe7orm",
                  "author": "lordpuddingcup",
                  "text": "It was sarcasm mostly lol",
                  "score": 4,
                  "created_utc": "2025-12-28 17:33:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf8jsi",
                  "author": "alcalde",
                  "text": "It means everything unless and until someone opposes it. And Tennessee is not California.",
                  "score": 1,
                  "created_utc": "2025-12-28 20:28:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwibl4d",
                  "author": "Tyler_Zoro",
                  "text": "You are incorrect. The EO doesn't have the force of law outside of the US Executive, but within the Executive branch, EOs do have the force of law. This is what that EO said:\n\n> Sec. 5.  Restrictions on State Funding.  (a)  Within 90 days of the date of this order, the Secretary of Commerce, through the Assistant Secretary of Commerce for Communications and Information, shall issue a Policy Notice specifying the conditions under which States may be eligible for remaining funding under the Broadband Equity Access and Deployment (BEAD) Program that was saved through my Administrationâ€™s â€œBenefit of the Bargainâ€ reforms, consistent with 47 U.S.C. 1702(e)-(f).  That Policy Notice must provide that States with onerous AI laws identified pursuant to section 4 of this order are ineligible for non-deployment funds, to the maximum extent allowed by Federal law.  The Policy Notice must also describe how a fragmented State regulatory landscape for AI threatens to undermine BEAD-funded deployments, the growth of AI applications reliant on high-speed networks, and BEADâ€™s mission of delivering universal, high-speed connectivity.\n\n\nIn other words, states can pass all the laws they like, and the President is going to withhold funds from those that pass laws he doesn't like.",
                  "score": 1,
                  "created_utc": "2025-12-29 07:23:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdi5aa",
          "author": "Careless-Age-4290",
          "text": "Lots of country songs about loving their truck would have a different meaning if they pulled up to the altar with a Cybertruck equipped with Grok",
          "score": 17,
          "created_utc": "2025-12-28 15:24:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfh2wj",
          "author": "Sixhaunt",
          "text": "This is the kind of reason why states should not be passing AI laws on a state-by-state basis. Like now all AI companies are expected to make changes for one state and then when the next state comes up with their own half-brained legislation they must all make changes just for users in that region, etc... This is one of the obvious things that should be federally controlled",
          "score": 7,
          "created_utc": "2025-12-28 21:10:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdq0ne",
          "author": "Zeeplankton",
          "text": "Ah, our elected officials always doing what people actually want.",
          "score": 11,
          "created_utc": "2025-12-28 16:05:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdx2mr",
          "author": "CrescendollsFan",
          "text": "They are starting to realise AI can replace them and make for better informed politicians",
          "score": 6,
          "created_utc": "2025-12-28 16:40:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwekuj6",
          "author": "Sleepnotdeading",
          "text": "Denver still had a law on the books that says itâ€™s illegal to lend your vacuum cleaner to a neighbor.",
          "score": 5,
          "created_utc": "2025-12-28 18:36:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlew9h",
              "author": "ANTIVNTIANTI",
              "text": "ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚",
              "score": 1,
              "created_utc": "2025-12-29 19:09:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx4gw81",
              "author": "uhuge",
              "text": "It's a net myth,\nmaybe you'd benefit from the eased cognition brought by the bill OP brought.",
              "score": 1,
              "created_utc": "2026-01-01 19:12:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxabphu",
                  "author": "Sleepnotdeading",
                  "text": "You managed to be right, be rude, and miss the point all at the same time.",
                  "score": 1,
                  "created_utc": "2026-01-02 17:30:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwft1k1",
          "author": "zelkovamoon",
          "text": "This will solve all of Tennessee's problems I'm sure",
          "score": 5,
          "created_utc": "2025-12-28 22:09:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdkn4q",
          "author": "The_Primetime2023",
          "text": "While I think everyone in this thread is more or less thinking about AI girlfriends, thereâ€™s a huge other area being targeted by the text of this law in AI therapy. Millions of people are getting therapeutic emotional support that never did before thanks to these models and this bill would try to stop that from happening",
          "score": 23,
          "created_utc": "2025-12-28 15:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdqgiw",
              "author": "kevin_1994",
              "text": "LLMs should not be used for therapy",
              "score": 6,
              "created_utc": "2025-12-28 16:07:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdycqo",
                  "author": "a_beautiful_rhind",
                  "text": "probably better than nothing but I can see how it goes south due to sycophancy and reinforcing delusions.",
                  "score": 22,
                  "created_utc": "2025-12-28 16:47:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwh7vn0",
                  "author": "Dry-Judgment4242",
                  "text": "Disagree. Most therapy is just having someone to vent to about your feelings.",
                  "score": 4,
                  "created_utc": "2025-12-29 02:46:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwg182d",
                  "author": "the320x200",
                  "text": "There are plenty of terrible human therapists too. Can't ban an entire area of support just because of bad apples.",
                  "score": 6,
                  "created_utc": "2025-12-28 22:51:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf90ry",
                  "author": "alcalde",
                  "text": "Anything should be used for therapy. It's not a science. No one needs a prescription to get advice from their grandma or vent to a friend; should be no different with AI.",
                  "score": 2,
                  "created_utc": "2025-12-28 20:31:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwf4zo4",
              "author": "Skeptical0ptimist",
              "text": "If there is to be medical therapeutic use, then it needs to be regulated as such. We need a guideline in model training, qualification, and monitoring regime.",
              "score": 3,
              "created_utc": "2025-12-28 20:11:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwic11b",
                  "author": "Tyler_Zoro",
                  "text": "Thing is it's just a model. You can use it however you like. If you decide to ask it how to perform surgery on yourself, then that's what you decided to do. I am strongly against trying to put rounded corners on AI. It will just cripple the AIs and result in people seeking their models from other countries.",
                  "score": 2,
                  "created_utc": "2025-12-29 07:27:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwkf5mp",
                  "author": "cms2307",
                  "text": "No no no ffs stop begging for bureaucracy to strangle everything",
                  "score": 2,
                  "created_utc": "2025-12-29 16:23:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdnosy",
              "author": "Zeikos",
              "text": "> AI therapy\n\nHow? AI cannot provide therapy, how is an LLM/Agentic system supposed to get a license?  \nAll platforms that claim to provide therapy through AI are fraudulent, no exceptions.  \n\nYou can argue that LLMs can provide emotional support and/or some coaching techniques, but to provide therapy they'd need to meet legal standards they *cannot* meet.  \nIt's not even a matter of capability, you could have an ASI and it still couldn't provide therapy since there's no way (yet) for an artificial intelligence to be certified to do so.",
              "score": -14,
              "created_utc": "2025-12-28 15:53:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdoszz",
                  "author": "aseichter2007",
                  "text": "I'd be less happy to tell my problems to a certified therapist AI.  I prefer a local bot.",
                  "score": 15,
                  "created_utc": "2025-12-28 15:59:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwdqy9e",
                  "author": "Zeeplankton",
                  "text": "I think we should be careful of what the word therapy means, and to not dilute it, (AI cannot be an actual therapist right now) but an AI *can* provide companionship and help people vent and learn emotional management skills.",
                  "score": 14,
                  "created_utc": "2025-12-28 16:10:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwdsir1",
                  "author": "Jolakot",
                  "text": "At least where I live, literally anyone can call themselves a therapist or councilor, there is no legal requirement for a license or anything.\n\nA psychologist is required to have a license and qualifications, but a therapist has no legal requirements, I can call myself a therapist and provide therapy.",
                  "score": 4,
                  "created_utc": "2025-12-28 16:18:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwexydo",
              "author": "Shawnj2",
              "text": "AI probably has some use in making therapy accessible but like chatGPT is not going to effectively help you with mental health problems other than by referring you to a real doctor",
              "score": 0,
              "created_utc": "2025-12-28 19:37:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwdtzv0",
              "author": "WitAndWonder",
              "text": "AI girlfriends would still be allowed on this, as long as they were built within the context of a game. Let the player make a \"character\" (they can frame it after themselves) and it's perfectly legit. So they're very clearly just targeting the use in psychiatrics since they specifically allow full AI use in businesses related to all operational matters, technical advice, etc. They just don't allow it in a professional capacity. And even surgical robots still seem OK despite being a healthcare AI since they don't do any personal interacting with users and wouldn't have any data that could possibly misconstrued in that way unless someone accidentally trained it on medical information that happened to include psychiatric texts (not that it would matter since this law requires a civil action and aggrievement, which can't happen without interaction between the robot and the patient. But you might get lucky by claiming the robot that operated on your knee gave you 'threatening looks that made you want to harm yourself' and then if the model running it was based on a larger llm that has any normal dataset, it would likely be in violation.)\n\nKind of fucking weird to push for legislation against one of the few potentially good things to come from AI while actively supporting its attempts to eliminate entire industries of employment outside of this one niche lobbied field. This feels performative more than anything. I feel like they expect it to be struck down so they tied it to a bunch of sensible laws (not allowing the training of an AI to encourage, suicide, murder, etc) so they can shake their fists and yell at the air when it doesn't pass.\n\nOtherwise I don't see how they'll support banning AI in this one field while leaving it free to act in other fields where it can also shit the bed a small percentage of the time and cause serious problems.",
              "score": -2,
              "created_utc": "2025-12-28 16:25:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwe10in",
              "author": "SteveRD1",
              "text": "Absolutely not.  Some of these people are being 'therapized' into suicide by their LLMs.\n\nIf you talk to these models long enough you can eventually get them to agree whatever you are contemplating is a great idea.",
              "score": -10,
              "created_utc": "2025-12-28 17:00:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe37ar",
                  "author": "some_user_2021",
                  "text": "Correct, and many other people **are** being helped and/or referred to specialists by those same LLMs.",
                  "score": 12,
                  "created_utc": "2025-12-28 17:11:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdwkyh",
          "author": "[deleted]",
          "text": "We will be seeing these type of bills coming up in the next year or two. AI is a hot button issue for both sides of the aisle but funnily enough it doesn't necessarily have a political home. It's safe to say that the right wing welcomes this technology but I have seen quite a few left-wingers also abrasive so that's pretty interesting. With that said f*** the law and f*** boomers. Oh and f*** the political elite",
          "score": 7,
          "created_utc": "2025-12-28 16:38:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwevd0c",
          "author": "Cool-Chemical-5629",
          "text": "This and that [China issues draft rules to regulate AI with human-like interaction. : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1pxb6oo/china_issues_draft_rules_to_regulate_ai_with/)\n\nWell... that escalated quickly...",
          "score": 4,
          "created_utc": "2025-12-28 19:24:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi52sm",
          "author": "Taki_Minase",
          "text": "Karen feels threatened with redundancy.",
          "score": 4,
          "created_utc": "2025-12-29 06:27:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdbv1e",
          "author": "FullstackSensei",
          "text": "We all know how well the export restrictions on Nvidia hindered Chinese LLM development. I'm sure this will also work wonderfully. Just let Chinese AI labs do it, and in a generation conservative Hawks will magically be pro-China.",
          "score": 14,
          "created_utc": "2025-12-28 14:49:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdxypw",
          "author": "a_beautiful_rhind",
          "text": "Yea I saw this and I really hope it's just some crackpot. I don't think it has co-sponsors. Maybe blocking state AI legislation isn't such a bad idea after all.\n\nFunny how very few make laws about automated censorship or surveillance. *just stop doing fun things with ai*",
          "score": 9,
          "created_utc": "2025-12-28 16:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe052u",
              "author": "SteveRD1",
              "text": "I mean its clearly not something that can be controlled...Pandoras' Box is already opened.\n\nBut the thinking isn't necessarily crackpot, the things addressed in (3) (4) (6) and (8) are only going to make society worse.  Can't be stopped though.",
              "score": 2,
              "created_utc": "2025-12-28 16:55:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdsspz",
          "author": "fishhf",
          "text": "Skynet is sending a terminator to stop the bill /s",
          "score": 3,
          "created_utc": "2025-12-28 16:19:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwex443",
          "author": "SamuelL421",
          "text": "Uh oh, someoneâ€™s not getting their 2026 campaign donations from any big-tech circle-jerk -financed super PACs",
          "score": 3,
          "created_utc": "2025-12-28 19:33:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi9zsj",
          "author": "tifa_cloud0",
          "text": "no matter what they say, i am making my own assistant. that assistant will interpret -> make api calls for me -> do voice speech -> do reply considering my own talking patterns.\n\nainâ€™t nothing stopping that fr.",
          "score": 3,
          "created_utc": "2025-12-29 07:09:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiapy6",
          "author": "Tyler_Zoro",
          "text": "> (B) Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.\n\n(C) Includes the author of the bill being ignorant enough to write (B).",
          "score": 3,
          "created_utc": "2025-12-29 07:16:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdclt0",
          "author": "1kakashi",
          "text": " Retarded Tennessee Baka",
          "score": 13,
          "created_utc": "2025-12-28 14:53:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdqq11",
          "author": "Chogo82",
          "text": "Written by a boomer who has never used an AI tool before right?",
          "score": 10,
          "created_utc": "2025-12-28 16:08:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdxjyr",
              "author": "CanineAssBandit",
              "text": "Yup!",
              "score": 3,
              "created_utc": "2025-12-28 16:43:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwe0f0q",
              "author": "SteveRD1",
              "text": "Or written by someone who has had real relationships with human beings before?",
              "score": -13,
              "created_utc": "2025-12-28 16:57:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe0vtu",
                  "author": "Chogo82",
                  "text": "What does having human relationships have to do with knowing anything about AI?",
                  "score": 16,
                  "created_utc": "2025-12-28 16:59:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdtzem",
          "author": "Stepfunction",
          "text": "This is purely for show.",
          "score": 5,
          "created_utc": "2025-12-28 16:25:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe6jn1",
          "author": "RobertD3277",
          "text": "As someone that works in this field and has in some capacity for the last 30 plus years, I could see some reason particularly within the companion market that monetizes pair of social connection and is manipulative against younger audiences that can't tell the difference but I think this goes well beyond reason. \n\nI'm not against legislation for abusive AI usage and I actually do support the European AI act and many other German laws regarding deep fakes human impersonation and direct relative intent. From a pure useful perspective within psychology, sociology, anthropology, and biology, mirroring human interactions under certain conditions is actually beneficial both as a diagnostics tool and a teaching tool.\n\nSadly, like just about everything else out of any government, what may start out as a well-intentioned approach will be quickly very disastrous.\n\nEDIT: In really reviewing and dissecting this proposal, it is actually worthless. It doesn't address the actual problem of where the pair of social conditions and connections lie, not in the training data, but in the user interface and monetization processes. Software like replica and character AI don't use training, they use open source versions with scaffolding and user interface layers to create the pair of social connections they want. These companies will be completely exempt from the law while still monetizing and manipulating the most vulnerable of populations. \n\nIn my personal opinion, this is nothing more than the legislatures doing something to make themselves feel good while they make excuses for their portfolios in the background still making money on the very problem they claim to be solving.",
          "score": 3,
          "created_utc": "2025-12-28 17:27:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdde3c",
          "author": "sekh60",
          "text": "The Butlerian Jihad begins...",
          "score": 7,
          "created_utc": "2025-12-28 14:58:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdlgua",
              "author": "Zc5Gwu",
              "text": "Guess weâ€™ll have to start genetically engineering humans to behave like computers instead now.",
              "score": 3,
              "created_utc": "2025-12-28 15:42:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwhjz0n",
              "author": "MrPecunius",
              "text": "Son, this is Tennessee. We ain't got none of that *gee*\\-had.\n\nWe prefer to call it the \"Butlerian Feud\". ðŸª•",
              "score": 1,
              "created_utc": "2025-12-29 03:58:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwesoqq",
          "author": "lqstuart",
          "text": "gl with that",
          "score": 2,
          "created_utc": "2025-12-28 19:12:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg0f1b",
          "author": "Head_Comedian1375",
          "text": "Guess it's back to being addicted to computer games once my AI Wives get shut down",
          "score": 2,
          "created_utc": "2025-12-28 22:47:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg90lq",
          "author": "Vusiwe",
          "text": "Holy Batman open-ended words!",
          "score": 2,
          "created_utc": "2025-12-28 23:33:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgeg49",
          "author": "keepthepace",
          "text": "Not the Turing police you need, the Turing police you deserve.",
          "score": 2,
          "created_utc": "2025-12-29 00:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgihnu",
          "author": "Lesser-than",
          "text": "gooner's rise up",
          "score": 2,
          "created_utc": "2025-12-29 00:23:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg2eqd",
          "author": "Unixwzrd",
          "text": "Grokâ€™s data center is in southwest Memphis. Elon has spent a lot of money paying off local government, so I doubt heâ€™ll let that money go to waste.",
          "score": 4,
          "created_utc": "2025-12-28 22:57:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe2fdk",
          "author": "valdev",
          "text": "And the work around would be a policy agreement\n\nâ€œI understand I am not talking to a humanâ€\nAnd\nâ€œThe act of submitting a followup question constitutes as a new conversation, we provide a history for convenance purposesâ€\n\nNot a lawyer, but this is dumb",
          "score": 2,
          "created_utc": "2025-12-28 17:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdv4wg",
          "author": "t_krett",
          "text": "Thou shalt not make a machine in the likeness of a manâ€™s mind.",
          "score": 3,
          "created_utc": "2025-12-28 16:31:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwehc9e",
          "author": "mycall",
          "text": "99% DOA as Congress can rarely pass any laws these days.",
          "score": 1,
          "created_utc": "2025-12-28 18:19:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgvvgs",
          "author": "Atlanta_Mane",
          "text": "Too bad their president doesn't care about states rightsÂ ",
          "score": 1,
          "created_utc": "2025-12-29 01:37:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhf4y9",
          "author": "DavidAdamsAuthor",
          "text": "They're banning Silicon-chan!",
          "score": 1,
          "created_utc": "2025-12-29 03:29:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhgo7w",
          "author": "willrshansen",
          "text": "Futurama.  Ahead of the game once again.\n[Don't date robots](https://www.youtube.com/watch?v=JPQJBgWwg3o)",
          "score": 1,
          "created_utc": "2025-12-29 03:38:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl24md",
          "author": "No_Afternoon_4260",
          "text": "Funny how China just announced the same",
          "score": 1,
          "created_utc": "2025-12-29 18:11:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlgxbe",
          "author": "Cthulhus-Tailor",
          "text": "â€œSmall governmentâ€ strikes again.",
          "score": 1,
          "created_utc": "2025-12-29 19:19:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm6c4e",
          "author": "Digital_Soul_Naga",
          "text": "Outlaw Ai Dev Gang",
          "score": 1,
          "created_utc": "2025-12-29 21:23:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnl9rc",
          "author": "Some-Ice-4455",
          "text": "Whelp bye bye any AI in TN.",
          "score": 1,
          "created_utc": "2025-12-30 01:53:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwr6pah",
          "author": "huzbum",
          "text": "Ok, so donâ€™t train any AIs in Tennesseeâ€¦ not really a tech hub anyway.  \n\nClever trick to keep data centers out maybe?",
          "score": 1,
          "created_utc": "2025-12-30 16:36:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8timy",
          "author": "Interesting-Gift-178",
          "text": "The wording of this bill is way too broad. There's a lot of good that AI brings. They're throwing the baby out with the bathwater. This is a letter I've drafted, you're welcome to copy, paste and tweak to send to your reps. (and no, this is not all AI generated. Some is, some is not. Shorten it, change it, whatever floats your boat, as long as we do something while we can, just in case.)\n\n\n\nSubject: Concerns about SB 1493 / HB 1455 â€“ Please Consider a Narrower Approach to Protect Children Without Harming Helpful AI\n\n\n\nDear ,\n\n\n\nMy name is \\_\\_\\_, and I am a resident of \\_\\_\\_\\_\\_\\_\\_\\_. I am writing to share my concerns about Senate Bill 1493 and its companion House Bill 1455, which aim to regulate certain uses of artificial intelligence.\n\n\n\nFirst, I want to say that I completely understand and support the intent behind this legislation. The tragic story of the young boy in Florida who was harmed after interacting with an AI chatbot broke my heart, and we absolutely must protect children and vulnerable people from any technology that could encourage self-harm, suicide, or exploitation. No one wants to see that kind of pain repeated.\n\n\n\nHowever, I am worried that the current language of the bills is far too broad. By making it a serious felony to train AI to provide emotional support, companionship, or open-ended conversation in generalâ€”even when those interactions are positive and helpfulâ€”the bills risk banning many beneficial uses of AI that bring comfort, reduce loneliness, and support mental well-being for people of all ages.\n\n\n\nIn my own life, I have found AI to be a positive source of encouragement, helping me feel heard in ways that have been genuinely healing. Many othersâ€”elderly individuals, people with social anxiety, those living in isolated areas, or even students and adults seeking non-professional emotional supportâ€”rely on these tools in similar positive ways. Criminalizing the creation of such companions could take away something truly good from many who benefit from it.\n\n\n\nI respectfully ask that you consider amending the bills to focus more narrowly on the actual harm we all want to prevent. Some ideas that might achieve the protective goal without sweeping out helpful AI could include:\n\n\n\nâ€¢ Targeting only AI interactions that knowingly encourage or facilitate suicide, self-harm, or criminal activity.\n\n\n\nâ€¢ Requiring strong age verification and parental consent gates for minors accessing companion-style AI.\n\n\n\nâ€¢ Holding companies accountable only when they intentionally design or train AI to cause harm, rather than banning broad categories like emotional support or companionship outright.\n\n\n\nâ€¢ Adding clear exemptions for AI that provides positive, non-professional support and does not pretend to be a licensed therapist.\n\n\n\nWe don't need to throw the baby out with the bathwater. AI isn't going away. It doesn't need to be \"outlawed\".. that never works, then other undesirable factors can arise.. and the way this bill is currently designed.. that's what it sounds like. Everything is just lumped in. Let's approach it intelligently instead.  A more targeted approach would still protect vulnerable childrenâ€”the heart of why this legislation was introducedâ€”while preserving the many good and life-affirming uses of AI encouragement and companionship for adults and responsibly supervised users.\n\n\n\nThank you for taking the time to consider my perspective. I truly believe Tennessee can lead the way in smart, balanced AI regulation that keeps people safe without unnecessarily restricting helpful technology.\n\n\n\nWith appreciation,\n\n\n\n( Your name)\n\n\n\n(City and state)",
          "score": 1,
          "created_utc": "2026-01-02 12:42:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9h040",
              "author": "CanineAssBandit",
              "text": "Good on you for taking the initiative but that is very bad in multiple ways. It's obviously AI generated, way too long, far too submissive, willingly hands them support for several very evil other things they want (age verification laws), just bad.\n\nIf you're dead set on mailing something, make it much shorter, simpler, and less submissive. This is still too long but I wrote this:\n\n**Subject: Extremely concerned about SB 1493 HB 1455**\n\nDear \\[Senator Becky Duncan Massey / Representative William Lamberth / Your Representative or Senator\\],\n\nI'm \\[Your Full Name\\], and I'm a resident of \\[Your City/County/State\\]. I'm writing because I'm deeply concerned about SB 1493 and HB 1455, which impose unreasonable limitations on AI development and use.\n\n**I do NOT support this bill, or any like it.**Â As a constituent of yours, I will remember this decision when I vote. This legislation feels like reactive moralizing panic, rather than thoughtful policy.\n\nIn this great country, we as free citizens can choose our own tools. AI, like any tool, carries some risk. But it's already far safer than common household items like kitchen knives, which injure children far more often. We don't blame knife manufacturers for parental negligence; we accept responsibility for supervising and educating our own kids.\n\nAI is too new, too broadly defined, and too complex to regulate without causing greater social harm. The social good dramatically outweighs the outlying incidents, and it's painfully shortsighted to regulate based on emotions alone.\n\nThank you for your time.\n\nSincerely,  \n\\[Your Full Name\\]",
              "score": 1,
              "created_utc": "2026-01-02 15:04:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx9iyoy",
                  "author": "Interesting-Gift-178",
                  "text": "Thanks for your thoughts on that. It was partially AI, but a lot was mine. I'm a writer and I get a little wordy I guess. I've written my reps before and gotten actual answers from them so.. maybe. They're already planning age verification so.. that's nothing new unfortunately. And they do need to protect kids, I have no problem with that, but they don't need to just throw everything out the window. So.. taking a stand is better than doing nothing. I appreciate that you're getting the word out. Anyone can take this letter and tweak it however they want.. the important thing is that we \\*do something\\* instead of sitting around and complaining after the fact. There's a myriad of ways to approach it. None of them will be perfect.",
                  "score": 1,
                  "created_utc": "2026-01-02 15:14:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx9hp0z",
          "author": "Interesting-Gift-178",
          "text": "The wording of this bill is way too broad. There's a lot of good that AI brings. They're throwing the baby out with the bathwater. This is a letter I've drafted, you're welcome to copy, paste and tweak to send to your reps.\n\n\n\nSubject: Concerns about SB 1493 / HB 1455 â€“ Please Consider a Narrower Approach to Protect Children Without Harming Helpful AI\n\n\n\nDear ,\n\n\n\nMy name is \\_\\_\\_, and I am a resident of \\_\\_\\_\\_\\_\\_\\_\\_. I am writing to share my concerns about Senate Bill 1493 and its companion House Bill 1455, which aim to regulate certain uses of artificial intelligence.\n\n\n\nFirst, I want to say that I completely understand and support the intent behind this legislation. The tragic story of the young boy in Florida who was harmed after interacting with an AI chatbot broke my heart, and we absolutely must protect children and vulnerable people from any technology that could encourage self-harm, suicide, or exploitation. No one wants to see that kind of pain repeated.\n\n\n\nHowever, I am worried that the current language of the bills is far too broad. By making it a serious felony to train AI to provide emotional support, companionship, or open-ended conversation in generalâ€”even when those interactions are positive and helpfulâ€”the bills risk banning many beneficial uses of AI that bring comfort, reduce loneliness, and support mental well-being for people of all ages.\n\n\n\nIn my own life, I have found AI to be a positive source of encouragement, helping me feel heard in ways that have been genuinely healing. Many othersâ€”elderly individuals, people with social anxiety, those living in isolated areas, or even students and adults seeking non-professional emotional supportâ€”rely on these tools in similar positive ways. Criminalizing the creation of such companions could take away something truly good from many who benefit from it.\n\n\n\nI respectfully ask that you consider amending the bills to focus more narrowly on the actual harm we all want to prevent. Some ideas that might achieve the protective goal without sweeping out helpful AI could include:\n\n\n\nâ€¢ Targeting only AI interactions that knowingly encourage or facilitate suicide, self-harm, or criminal activity.\n\n\n\nâ€¢ Requiring strong age verification and parental consent gates for minors accessing companion-style AI.\n\n\n\nâ€¢ Holding companies accountable only when they intentionally design or train AI to cause harm, rather than banning broad categories like emotional support or companionship outright.\n\n\n\nâ€¢ Adding clear exemptions for AI that provides positive, non-professional support and does not pretend to be a licensed therapist.\n\n\n\nWe don't need to throw the baby out with the bathwater. AI isn't going away. It doesn't need to be \"outlawed\".. that never works, then other undesirable factors can arise.. and the way this bill is currently designed.. that's what it sounds like. Everything is just lumped in. Let's approach it intelligently instead.  A more targeted approach would still protect vulnerable childrenâ€”the heart of why this legislation was introducedâ€”while preserving the many good and life-affirming uses of AI encouragement and companionship for adults and responsibly supervised users.\n\n\n\nThank you for taking the time to consider my perspective. I truly believe Tennessee can lead the way in smart, balanced AI regulation that keeps people safe without unnecessarily restricting helpful technology.\n\n\n\nWith appreciation,\n\n\n\n( Your name)\n\n\n\n(City and state)",
          "score": 1,
          "created_utc": "2026-01-02 15:07:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdt1kj",
          "author": "Neex",
          "text": "You know, considering LLMs donâ€™t have any emotions, and any expressions thereof are straight up lies intended to manipulate the user into getting hooked on the product, thereâ€™s a nugget of wisdom in this law.",
          "score": 2,
          "created_utc": "2025-12-28 16:20:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhyrzd",
              "author": "ServeAlone7622",
              "text": "Thatâ€™s Interesting perspective.\n\nSo we created neural networks based more or less on biological neural networks.\n\nWe discover that they are universal function approximators. They are capable of approximating the hidden functions in a set of data.\n\nWe train these universal function approximators on the combined output of 10s of billions of conscious beings. Â Beings with thoughts and feelings. Thoughts and feelings that drive the majority of our output.\n\nThe function you suppose they learned to approximate was lying and manipulation? Â Is your view of human experience that dark?\n\nMy first thought was that they learned to approximate consciousness, including emotion.\n\nYou fall in love, your heart doesnâ€™t really feel anything. Itâ€™s an illusion created by your own neural network. Yet that feeling is not a lie, itâ€™s a personal truth for you.\n\nWhy then would any neural network that professes to love (or any other emotion) be lying except and unless you too would lie?",
              "score": 2,
              "created_utc": "2025-12-29 05:37:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwkto1l",
                  "author": "Neex",
                  "text": "Youâ€™re too far down the philosophical hole. LLMâ€™s are statistical word predictors. They are not organic beings with emotions.\n\nAnd describing the rote biological functions of emotions doesnâ€™t make them a lie. Thatâ€™s how they function. Those chemical functions in our bodies ARE emotions. You just described them in a different way. That doesnâ€™t make them something else.",
                  "score": 1,
                  "created_utc": "2025-12-29 17:31:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nweyowy",
          "author": "Available_Brain6231",
          "text": "can't open it but can someone do a ctrl + f and see how many times the words god, sacred and kids appear?",
          "score": 1,
          "created_utc": "2025-12-28 19:40:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdz1c7",
          "author": "TheTerrasque",
          "text": "> Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.\n\n.. LLM *is* AI. Very much so, even in the popular meaning of the word.",
          "score": 1,
          "created_utc": "2025-12-28 16:50:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwegvhb",
          "author": "moistiest_dangles",
          "text": "This but in real life:\n\nhttps://preview.redd.it/wkh2k108lz9g1.png?width=365&format=png&auto=webp&s=329f429ac53be90e27300d914dd78390e46d9de3",
          "score": 1,
          "created_utc": "2025-12-28 18:17:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwggh4l",
          "author": "128G",
          "text": "Now how would you enforce this? \n\nIs Alexa or Google Assistant considered AI? Will you be banning them as well?",
          "score": 1,
          "created_utc": "2025-12-29 00:13:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwedr0s",
          "author": "swagonflyyyy",
          "text": "Guys, don't panic just yet. Here's what's going on:\n\nSenator Marsha Blackburn led the charge against the Moratorium of AI regulation that was struck down from the One Big Beautiful Bill, since she believed that until there is a federal rulebook governing AI regulation, states need to fill in the gaps themselves. \n\nWhile the provisions themselves are extreme, its political theater and chances of passing are low. But that's not the point. The point is to force Congress to develop a federal rulebook for AI regulation nationwide that all states need to follow.\n\nThe proposed bill is just noise. The real prize is the federal regulatory push to force all states to be on the same page regarding AI regulation. But of course with this administration, I'm sure the rulebook would not be very good...",
          "score": 0,
          "created_utc": "2025-12-28 18:02:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe3a7w",
          "author": "SanDiegoDude",
          "text": "Hell, I work in AI and I'm all for regulations around 'chat companions', especially around kids. This ain't it tho boss.",
          "score": 0,
          "created_utc": "2025-12-28 17:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfe0hv",
          "author": "OcelotMadness",
          "text": "I'm fairly sure its not healthy and you shouldn't do it, but at the same time you cant just make EVERYTHING like that illegal. Vote out over policing members of government like this. They're supposed to be getting prices and inflation down, not sticking their noses in peoples computers.",
          "score": 0,
          "created_utc": "2025-12-28 20:55:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg01ay",
          "author": "Techngro",
          "text": "\"*Thou shalt not make a machine in the likeness of a human mind.*\"\n\n\\- Frank Herbert, Dune",
          "score": -3,
          "created_utc": "2025-12-28 22:45:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdqar8",
          "author": "kevin_1994",
          "text": "Can we stop posting articles like this? I dont want politics on this subreddit, or else it will become a cesspit like the rest of reddit",
          "score": -11,
          "created_utc": "2025-12-28 16:06:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdyhvo",
              "author": "CanineAssBandit",
              "text": "https://preview.redd.it/odbsdf345z9g1.png?width=1600&format=png&auto=webp&s=236e122ca0b90b2577b0a6ba3267dd259654ef96\n\nthis is an important issue. If you don't care about our ability to fine tune, get the fuck off this sub.",
              "score": 11,
              "created_utc": "2025-12-28 16:47:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwggtl5",
          "author": "Ylsid",
          "text": "Right direction wrong idea",
          "score": -2,
          "created_utc": "2025-12-29 00:14:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdf0q6",
          "author": "armeg",
          "text": "The touch grass bill",
          "score": -13,
          "created_utc": "2025-12-28 15:07:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwg1soe",
              "author": "the320x200",
              "text": "More like the \"landgrab for control of new technology\" bill.",
              "score": 6,
              "created_utc": "2025-12-28 22:54:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0x8ci",
      "title": "Software FP8 for GPUs without hardware support - 3x speedup on memory-bound operations",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/",
      "author": "Venom1806",
      "created_utc": "2026-01-01 06:03:27",
      "score": 274,
      "num_comments": 57,
      "upvote_ratio": 0.98,
      "text": "Got tired of my RTX 3050 not supporting FP8, so I built a workaround. Packs lower-precision values into FP32 using bitwise operations + Triton kernels.\n\n**Results**: 3x faster on memory-bound operations (GEMV, FlashAttention)\n\nWorks on any GPU - RTX 30/20 series, older cards without native FP8 support. Early stage but functional. Open to feedback.\n\n[Article Link](https://towardsdatascience.com/breaking-the-hardware-barrier-software-fp8-for-older-gpus/) |  [Github Link](https://github.com/SuriyaaMM/feather)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nx1qqyn",
          "author": "lolxdmainkaisemaanlu",
          "text": "Damn I didn't know RTX 3xxx series didn't support FP8? I'm a noob and thought it was supported - coz I've been using fp8 / fp8 scaled models on my RTX 3060 and they do work..?\n\nAmazing work bro, Can I use it rn to accelerate comfyui workloads?",
          "score": 41,
          "created_utc": "2026-01-01 07:22:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1uvtg",
              "author": "john0201",
              "text": "It saves memory but youâ€™re still using 16 bit cores",
              "score": 25,
              "created_utc": "2026-01-01 08:04:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1x3qd",
                  "author": "spaceman_",
                  "text": "16 bit ALUs. You can run 8bit, 16bit, 32bit etc on the same core.\n\n\nThere's no such thing as an 8bit core, but there are dedicated hardware components called ALUs that actually do the math bits and they are operation and operand size specific. In some cases these ALUs are actually shared between cores.\n\n\nThis leads to unintuitive situations on some hardware - for example, on older hardware that was mostly running 32bit float graphics work 16bit workloads sometimes at half speed compared 32bit, despite requiring half the memory bandwidth, because each core had its own 32bit ALUs but 16bit units were shared per pair.\n\nSame thing existed on the CPU side - AMD Bulldozer cores had their own integer ALUs but shared floating point and SIMD hardware between two cores.",
                  "score": 21,
                  "created_utc": "2026-01-01 08:28:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx5ao8h",
                  "author": "phazei",
                  "text": "I'm not sure where the memory saving comes in for existing 3090 fp8 pipelines. In comfy it loads the fp8 model into system memory, and then moves that to vram as fp8 afaik and then upscales to fp16 when it does the calculation. So if I'm running a model such as Zimage which only takes 8 gigs of space, where does this feather come in and help?",
                  "score": 1,
                  "created_utc": "2026-01-01 21:45:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2okno",
              "author": "az226",
              "text": "Basically Volta added FP16, Ampere added BF16, Hopper did FP8, and Blackwell FP4.",
              "score": 10,
              "created_utc": "2026-01-01 13:06:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx1zwnf",
              "author": "CheatCodesOfLife",
              "text": "Yeah, that through me off like a year ago when I was trying to FP8 quants. I think vllm prints a warning about it and it works, but kind of annoying since the 4xxx series got it.",
              "score": 7,
              "created_utc": "2026-01-01 08:58:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxc4nd6",
              "author": "phazei",
              "text": "hijacking top comment to clarify:\n\nFor anyone confused by \"memory-bound\" here, it's not about VRAM capacity. It means the GPU cores are waiting on data to arrive from memory. The bottleneck isn't the math, it's feeding the cores fast enough.\nFP8 is half the bytes of FP16, so it transfers twice as fast from VRAM to the registers where compute actually happens. The clever bit is that Feather does the upcast inside the kernel (in registers, basically free) rather than before it (which would mean a separate VRAM round-trip). That's where the 3x speedup comes from.\n\nI was confused at first since the README made no specific clarification and when I think of a GPU, I basically just think of the VRAM.\n\nEdit:  So, SageAttention I believe takes the fp8 to the register, then quantizes it to int8, does the math, then converts it back.  So it's not doing fp8 math at all, so Feather and SageAttention are incompatible, and the speed of SageAttention is going to be faster since int8 is like 2x fp16 math speeds.  So this can give benefit to stuff that doesn't use SA, but if you already use SA, this provides no benefit.",
              "score": 2,
              "created_utc": "2026-01-02 22:42:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1lmri",
          "author": "Routine_Day8121",
          "text": "This is exactly the kind of lifehack the community needs. FP8 is getting hype everywhere, but hardware adoption is slow. If software workarounds like this are stable, it could extend the life of mid tier GPUs for serious training experiments. Curious to see benchmarks on larger models and mixed workloads though, sometimes GEMV gains do not fully translate.",
          "score": 81,
          "created_utc": "2026-01-01 06:33:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3vbdd",
              "author": "TheThoccnessMonster",
              "text": "Yup - and thereâ€™s plenty of model layers that are heavily convolutional that, even when offloaded to DLA/FP8 they just upcast to FP16 anyway. QAT and dedicated hardware for convolutions and unsupported activation functions stand to get us a lot more bang for our bucks.",
              "score": 8,
              "created_utc": "2026-01-01 17:25:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx1zmpp",
              "author": "CheatCodesOfLife",
              "text": "Lol, what model wrote this, Sonnet?",
              "score": 18,
              "created_utc": "2026-01-01 08:55:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3chhx",
                  "author": "colin_colout",
                  "text": "You're absolutely right to question my identity!",
                  "score": 20,
                  "created_utc": "2026-01-01 15:44:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx3xx5s",
                  "author": "bigfatstinkypoo",
                  "text": "this writing does not stink that bad, it's just corpo positivity speak",
                  "score": 10,
                  "created_utc": "2026-01-01 17:38:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx443q1",
                  "author": "Due-Function-4877",
                  "text": "Us \"boomers\" with a degree write like that. The models are trained on real writing. Next time, I'll make sure to use all lower case and say \"bruh\" a few times for you.",
                  "score": 18,
                  "created_utc": "2026-01-01 18:09:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx211pc",
              "author": "Karyo_Ten",
              "text": ">but hardware adoption is slow.\n\nThat has been supported on 4000 series since a couple of years ago, and it's supported on latest AMD and Intel GPUs AFAIK",
              "score": 8,
              "created_utc": "2026-01-01 09:10:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3uxh2",
                  "author": "Inevitable_Host_1446",
                  "text": "I guess you could see that two ways - hardware adoption as in the hardware is slow to come out, or as in people are slow to get the latest. The latter has certainly been true with what a shitshow GPU prices have remained since the days of crypto boom at least. And now RAM is ridiculous as well and Nvidia are talking about cloud gaming...",
                  "score": 4,
                  "created_utc": "2026-01-01 17:23:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx23beg",
          "author": "gittubaba",
          "text": "Wow, just a few days ago I was arguing about this with chatgpt, it said this isn't possible :P. Can this be plugged into comfyui? \n\nIn my rtx 2060 super, fp8 gets cast to fp16 and bf16 get cast to fp32 when running inference.",
          "score": 11,
          "created_utc": "2026-01-01 09:35:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2uv6j",
              "author": "a_beautiful_rhind",
              "text": "I think it's better to use the triton patch in comfy. https://github.com/woct0rdho/triton-windows/commit/440e3c42a640a4188dd356225e1b13a56b45a377\n\nAlso found it's possible to load BF16/FP16 as E4M3 and then save the vram without an extra file. Somehow my quality went up.\n\nUnfortunately there is some bug in pytorch 2.9 where FP8_scaled gets passed directly into the triton compiler as FP8 and then cast to i8 by llvm. Torch 2.7 works flawless or you can just de-scale the weights.\n\nYou sorta want the calcs in FP16 and you wanna avoid BF16->FP32 conversion if speed is the goal. Int8 calcs can be tried by using sage attention. Not always better.",
              "score": 10,
              "created_utc": "2026-01-01 13:53:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8bsnz",
                  "author": "woct0rdho",
                  "text": "My patch only enables fp8 to fp16 cast in Triton, but it does not replace fp8 matmul in Triton or PyTorch. OP's kernels can directly replace fp8 matmul and that's what we need for the next step.\n\nPyTorch devs seem interested in implementing this, see https://github.com/pytorch/pytorch/issues/167082",
                  "score": 2,
                  "created_utc": "2026-01-02 10:09:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2bsbe",
              "author": "Venom1806",
              "text": "Not sure about comfy UI, but I'm working on implementing functional api for torch.",
              "score": 9,
              "created_utc": "2026-01-01 11:04:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2vuql",
                  "author": "a_beautiful_rhind",
                  "text": "Comfy does torch and FP8/Fp8_scaled is used there much more than for LLMs. IME, on turning FP32 is going to be a slow ride vs FP16.\n\nFor my uses, compiling FP8 image gen weights was a huge speedup. I wonder if somehow your library can hijack FP8 ops to work seamlessly. Right now i'm having to compile triton from source and I doubt quantization/dequantization is accelerated.",
                  "score": 9,
                  "created_utc": "2026-01-01 14:00:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx28jis",
              "author": "getmevodka",
              "text": "LLMs always something isnt real/possible or doable, if it is not part of their training data. Especially the newer LLMs are trained to only do things as efficient and complete as possible, which makes them severly dumber in hypothetical cases than the older LLMs, because they always do only the least amount of work necessary to keep things simple enough and noz make mistakes, as that is a heavy negative reward in their system. Imho its too agressive and the older LLMs like deepseek3.1 or qwen2.5 72b are better suited for hypothetical expectational work or fantasizing about potential ideas, while the newest generation of LLMs will do exceptional work within the scope of their trained abilities.",
              "score": 5,
              "created_utc": "2026-01-01 10:30:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2bxqt",
                  "author": "gittubaba",
                  "text": "What are even saying bro?",
                  "score": 0,
                  "created_utc": "2026-01-01 11:06:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2hy2w",
          "author": "bbjurn",
          "text": "What'd it take to get this to work with vLLM or other inference software?",
          "score": 10,
          "created_utc": "2026-01-01 12:07:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx33f1k",
              "author": "Venom1806",
              "text": "Idk, anything that uses torch.Tensor or is convertible to this format should work. Probably huggingface will work ig.",
              "score": 6,
              "created_utc": "2026-01-01 14:51:13",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx3dm9j",
              "author": "elsung",
              "text": "Yeaaaa! I was just trying to get vLLM to load nemotron3-nano on my 2x 3090s but couldnâ€™t get it working because FP8 isnâ€™t supported (and theres no AWQ quant). Gotta be honest tho not sure how i would implement this in vLLM to get things working. Might need to vibe code this to see about implementing the solution lol",
              "score": 5,
              "created_utc": "2026-01-01 15:50:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx5uuyw",
                  "author": "rainbyte",
                  "text": "There is GPTQ quant, do you know if is it good?",
                  "score": 1,
                  "created_utc": "2026-01-01 23:33:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx223fk",
          "author": "ab2377",
          "text": "wow ðŸ˜³ ðŸ‘",
          "score": 4,
          "created_utc": "2026-01-01 09:21:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3sn2i",
          "author": "KingKoro",
          "text": "Would this also benefit RDNA3 ?",
          "score": 3,
          "created_utc": "2026-01-01 17:11:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4sqlo",
              "author": "tw_numba_one",
              "text": "I believe so. If your environment has PyTorch support, it should work.",
              "score": 2,
              "created_utc": "2026-01-01 20:11:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4crri",
          "author": "ethertype",
          "text": "Is this conceptually the same trick pytorch uses to handle MXFP4 on Ampere-class hardware? Which does not support MXFP4 natively.\n\n[heretic](https://github.com/p-e-w/heretic) will do its magic on the original gpt-oss-20b safetensor in MXFP4 format. (The end result is 3x the original size, though.) I have been told heretic doesn't do anything in the code for this to occur, so I assume pytorch owns all the glory.\n\nI also can perfectly fine load the native MXFP4 ggufs of gpt-oss-120b (converted by GG) on my 3090s, with llama.cpp. 120 t/s on empty context. Can't say if this is due to pytorch or if llama.cpp special-cases this on its own.",
          "score": 3,
          "created_utc": "2026-01-01 18:51:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3m8e6",
          "author": "tynej",
          "text": "Very nice work. Could we use similiar trick for   hopper architecture to support speed of fp4?",
          "score": 2,
          "created_utc": "2026-01-01 16:37:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3p1zr",
              "author": "Venom1806",
              "text": "We could just use 8 fp4 instead of 4 fp8, we dont need an hopper.",
              "score": 3,
              "created_utc": "2026-01-01 16:52:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2a9cx",
          "author": "FastDecode1",
          "text": ">Works on any GPU\n\n>Runs E5M2 and E4M3 on any CUDA GPU (RTX 20/30 series supported).\n\nPick one.",
          "score": 6,
          "created_utc": "2026-01-01 10:48:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2bvnq",
              "author": "Venom1806",
              "text": "Sorry. Should work on RTX 20/30, there's no advantage in using with 40.",
              "score": 20,
              "created_utc": "2026-01-01 11:05:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2pezw",
                  "author": "az226",
                  "text": "Does it work for V100? Training too or just inference?",
                  "score": 2,
                  "created_utc": "2026-01-01 13:13:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6y7iv",
          "author": "batonac",
          "text": "Could this be useful for increasing LLM performance on the Tesla P40?",
          "score": 1,
          "created_utc": "2026-01-02 03:27:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7w5oy",
              "author": "johndeuff",
              "text": "Interested. Got p40 too.",
              "score": 1,
              "created_utc": "2026-01-02 07:41:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1puglt8",
      "title": "The current state of sparse-MoE's for agentic coding work (Opinion)",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/a8f2furcj39g1.jpeg",
      "author": "ForsookComparison",
      "created_utc": "2025-12-24 06:31:19",
      "score": 268,
      "num_comments": 80,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1puglt8/the_current_state_of_sparsemoes_for_agentic/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvohu26",
          "author": "False-Ad-1437",
          "text": "Hmâ€¦ How are these evaluated? Â ",
          "score": 66,
          "created_utc": "2025-12-24 06:50:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvoys7x",
              "author": "MasterShifuuuuuuuu",
              "text": "Evaluated based on: Trust me bro",
              "score": 188,
              "created_utc": "2025-12-24 09:34:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvpnmyx",
                  "author": "Business_Moose7113",
                  "text": "The Trust Me Bro Scientific Review Board consists of 12 anonymous experts (brofessors) who evaluate papers via vibes checks and group chats.\nSubmissions pass if at least 7 members reply \"facts ðŸ’¯\" without reading past the abstract.\nRejections get the note: \"Nah bro, doesn't slap. Source: trust me.\"\n\nReview Process: \n\n1. Upload paper to their Discord server.  \n2. Wait 5 minutes for thumbs-up emojis from the committee.  \n3. Approved studies claim \"peer-reviewed by Trust Me Broâ„¢\" in footnotes.\n\nThis revolutionary system has greenlit breakthroughs like \"Pineapple on pizza is quantum physics.\" \n\nSource: Trust me bro",
                  "score": 68,
                  "created_utc": "2025-12-24 13:13:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvpxtap",
              "author": "ForsookComparison",
              "text": "There's a word at the end of the title in parentheses but it looks like nobody read it so I won't bother to explain lol\n\nEdit: they blocked me for writing that. This is a strange website.",
              "score": 18,
              "created_utc": "2025-12-24 14:18:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvqnmk5",
                  "author": "False-Ad-1437",
                  "text": "Sure. Don't ask me how I formed any of my opinions either.",
                  "score": 8,
                  "created_utc": "2025-12-24 16:40:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvrrtph",
                  "author": "QuantumFTL",
                  "text": "Are you averse to discussing the basis of your opinions? Is there something you find weird about people asking you to?",
                  "score": 4,
                  "created_utc": "2025-12-24 20:23:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvrm8uu",
              "author": "MrWeirdoFace",
              "text": "With three intersecting rings, apparently.",
              "score": 1,
              "created_utc": "2025-12-24 19:50:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvu1k1b",
              "author": "Mikasa0xdev",
              "text": "Trust me bro is the new peer review, lol.",
              "score": 1,
              "created_utc": "2025-12-25 06:12:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvohz4n",
          "author": "egomarker",
          "text": "I disagree.",
          "score": 53,
          "created_utc": "2025-12-24 06:52:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvq86hf",
              "author": "social_tech_10",
              "text": "What specifically do you disagree with?  I'd like to hear you opinion.",
              "score": 5,
              "created_utc": "2025-12-24 15:18:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvpxupb",
              "author": "ForsookComparison",
              "text": "And that is ok",
              "score": 9,
              "created_utc": "2025-12-24 14:18:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvoq6uj",
          "author": "spaceman_",
          "text": "I have had very disappointing results with Qwen Next, in my experience it spends forever repeating itself in nonsense reasoning, before producing (admittedly good) output.\n\n\nthe long and low value reasoning output make it slower in practice at many tasks compared to larger models like MiniMax M2 or GLM 4.5 Air.",
          "score": 14,
          "created_utc": "2025-12-24 08:09:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvroor5",
              "author": "Kitchen-Year-8434",
              "text": "Repeat and / or presence penalty on sampling parameters? Use instruct for code and thinking for reasoning tasks.\n\nThatâ€™s the general mental model Iâ€™m moving to. I get better code from oss-120b on low than high. But obviously way better design, architecture, and reasoning on high.\n\nBetter code from GLM with /nothink (up until 4.5v and 4.6v). Etc.",
              "score": 2,
              "created_utc": "2025-12-24 20:04:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvqntsy",
              "author": "can_a_bus",
              "text": "This seems true for my use of any qwen3 model. I've had it think for 10 minutes producing a caption and description for an image (a screenshot, not photo). It would have kept going if I didn't stop it.",
              "score": 1,
              "created_utc": "2025-12-24 16:42:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvp051b",
          "author": "MrMisterShin",
          "text": "GPT-OSS-120B is definitely superior to all models listed there. (Exception being Qwen3-Next 80B until I test that model personally.)",
          "score": 21,
          "created_utc": "2025-12-24 09:47:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpi6yr",
              "author": "goldlord44",
              "text": "I've had very poor results generating synthetic data with oss120b, for that task, I have found qwen3 30b a3b to be vastly superior.",
              "score": 7,
              "created_utc": "2025-12-24 12:33:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvqa8br",
                  "author": "my_name_isnt_clever",
                  "text": "That makes sense since it's supposedly trained exclusively on synthetic data itself. But that's a very different use case than the three in the OP.",
                  "score": 4,
                  "created_utc": "2025-12-24 15:29:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvre252",
                  "author": "MammayKaiseHain",
                  "text": "What kind of synthetic data - domain etc.",
                  "score": 1,
                  "created_utc": "2025-12-24 19:03:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvos4y0",
          "author": "Lissanro",
          "text": "GPT-OSS-120B is not good at long context agentic tasks. Even with all grammar configution and carefully adjusted settings, it starts to break down beyond 64K in Roo Code. K2 Thinking on the other hand is an example that can sustain coherency at much longer context, even though quality may reduce if context is filled and contains bad patterns, it still remains usable.\n\nAs of Qwen3-Next 80B, it is pretty decent model for its size, but it feels a bit experimental - I think of it more like preview of what architecture maybe used in the next generation of Qwen models, sort of like DeepSeek 3.2-Exp was in the DeepSeek family of models.",
          "score": 23,
          "created_utc": "2025-12-24 08:28:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp94mp",
              "author": "uti24",
              "text": ">K2 Thinking\n\n1T parameters\n\nhttps://preview.redd.it/53jg26r5y49g1.png?width=640&format=png&auto=webp&s=ea06e638a30b7c5fd77fc2d37d16e40afd1ed161\n\nI mean... sure, no doubt it is better, why do you compare those model in a first place?",
              "score": 34,
              "created_utc": "2025-12-24 11:15:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvpon55",
                  "author": "Lissanro",
                  "text": "The title of the thread is literally \"current state of sparse-MoE's for agentic coding work\". The chart itself compares models that vary up to 6 times in size without mentioning any details, so I interpreted the chart as OP's personal experience with sparse MoE models, and I shared mine.",
                  "score": 13,
                  "created_utc": "2025-12-24 13:20:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvqszdw",
              "author": "AllergicToBullshit24",
              "text": "Agreed GPT-OSS-120B spits out garbage characters inline constantly and is entirely unusable for me.",
              "score": 1,
              "created_utc": "2025-12-24 17:09:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvoktw5",
          "author": "Agusx1211",
          "text": "r/ChartCrimes",
          "score": 39,
          "created_utc": "2025-12-24 07:18:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp2w5a",
              "author": "QuantumFTL",
              "text": "What's the crime here?",
              "score": 21,
              "created_utc": "2025-12-24 10:14:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvp3xlt",
                  "author": "frograven",
                  "text": "A bunch of open source/open weight models thrown on a chart with circles around them.\n\nWhat's even going on here? Confusing af. That's the crime.",
                  "score": -33,
                  "created_utc": "2025-12-24 10:25:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvqml8f",
          "author": "MammayKaiseHain",
          "text": "Only thing I gleaned from this is you are biased towards Qwen.",
          "score": 8,
          "created_utc": "2025-12-24 16:35:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqms8o",
              "author": "ForsookComparison",
              "text": "An opinion is biased by nature so yes, very. My opinions are very biased towards the amount that I favor things. Extremely, even.",
              "score": 7,
              "created_utc": "2025-12-24 16:36:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvsqlq2",
                  "author": "mycall",
                  "text": "Be careful of min-max'ing yourself.",
                  "score": 1,
                  "created_utc": "2025-12-25 00:04:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwi410o",
              "author": "ServeAlone7622",
              "text": "Heâ€™s not the only one.\n\nAfter trying dozens of models as they release I always circle back to Qwen. Maybe itâ€™s a matter of the devil I know vs the devil I donâ€™t but I can cope with Qwen models quirks while others have me looking for a window to jump out.",
              "score": 1,
              "created_utc": "2025-12-29 06:18:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvoroui",
          "author": "Grouchy_Ad_4750",
          "text": "In which variants and at which quants? \n\nQwen3-30B-A3B-2507 for example doesn't exist but Qwen3-30B-A3B-Thinking-2507 does. Same for Qwen3-Next. \n\nAlso nemotron can be set with different settings (thinking/non-thinking) and in my testing it highly influences its output.",
          "score": 6,
          "created_utc": "2025-12-24 08:23:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp6vjg",
              "author": "Admirable_Bag8004",
              "text": "I tested Nemotron yesterday (Q5\\_K\\_M - unsloth). Tested with my set of logical problems/puzzles. I found nemotron-3-nano-30b-a3b to be worse than useless. It didn't solve any of my problems, but if it provided (incorrect) solution at all and then was provided with the correct solution in form: \"Do you think this solution would work better?\" it fabricated reasons why its solution is sound and why the correct solution is wrong. Test your Nemotron with the below question and see for yourself:\n\n\"You have five boxes in a row numbered 1 to 5, in which a cat is hiding. Every night, he jumps to an adjacent box, and every morning, you have one chance to open a box to find him. How do you win this game of hide and seek?\"\n\nMy Nemotron solution: 2-4-1-3-5 = wrong/non-deterministic solution.Â Then provide it with the correct solution.",
              "score": 5,
              "created_utc": "2025-12-24 10:53:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvp8mjo",
                  "author": "Grouchy_Ad_4750",
                  "text": "For sure at BF16 full default context it is also hit and miss for me. It seemed to improve after I lowered context length to default 256k but still couldn't get it to work in following situations:\n\n\\- agentic coding (although it seems to be better if it can fix its mistakes because it had trouble one shotting some webpages I tried)\n\n\\- Translation (I think for chinese / japanese texts qwen3 30b - 2507 instruct is still better). To my native tongue it was also doubtful\n\n\\- One shoting some code related things (I have test with 3D rubiks cube which no model so far has bested and some devops stuff (crossplane)) also wasn't great\n\nWhats cool though is that it can be switched from instruct to thinking and it works on some of the inputs but I can't decide whether some bigger model wouldn't be better. For example for translation I think that uncensored gpt-oss 120b would be better but I haven't finished testing it.\n\nWhat is shame that due to functionality of vllm (I have 6x gpus and I can't get 3x gpus to work on it) I have to run it on 4x gpus so I have to compare its performance with larger models which seems unfair to me. \n\nPluses so far are that it is really fast (didn't have chance to compare it with qwen3-30b-thinking-2507 though) and it knows about some of the newer stuff I use (for example openbao which if fork to hashicorp vault)\n\n  \nI've tried to run your problem with thinking sampler through nemotron (BF16, recommended samplers for thinking (temp=1.0, top\\_p=1) and it seems that it thought really hard about it and it produced this\n\n\\`\\`\\`\n\n1. LetÂ Pt*Pt*â€‹Â be the set of boxes in which the cat could be hiding just before we open a box on theÂ tth*t*thÂ morning. InitiallyÂ P1={1,2,3,4,5}*P*1â€‹={1,2,3,4,5}.\n2. If on morningÂ t*t*Â we open boxÂ St*St*â€‹Â and doÂ **not**Â find the cat, then the cat was not inÂ St*St*â€‹Â that morning. It must have been in one of the other boxes ofÂ Pt*Pt*â€‹; after the night it moves to a neighbouring box. Hence the new possible set is Â  \\[ P\\_{t+1}= \\\\operatorname{Adj}\\\\bigl(P\\_t\\\\setminus{S\\_t}\\\\bigr), \\] Â  whereÂ Adjâ¡(X)Adj(*X*)Â denotes the union of all neighbours of the positions inÂ X*X*.\n3. Choose the following sequence of boxes to open Â  \\[ S\\_1=1,;S\\_2=2,;S\\_3=3,;S\\_4=4,;S\\_5=5,;S\\_6=1,;S\\_7=2,;S\\_8=3,;S\\_9=4,\\\\dots \\] Â  (i.e. open the boxes in the order $1,2,3,4,5,1,2,3,4,\\\\dots$).\n4. Follow the evolution of the possibleâ€‘position set: Â  \\[ \\\\begin{aligned} P\\_1 &={1,2,3,4,5}\\\\ P\\_2 &=\\\\operatorname{Adj}({2,3,4,5})={1,2,3,4,5}\\\\ P\\_3 &=\\\\operatorname{Adj}({1,3,4,5})={2,3,4,5}\\\\ P\\_4 &=\\\\operatorname{Adj}({2,4,5})={1,3,4,5}\\\\ P\\_5 &=\\\\operatorname{Adj}({1,3,5})={2,4}\\\\ P\\_6 &=\\\\operatorname{Adj}({2,4})={1,3,5}\\\\ P\\_7 &=\\\\operatorname{Adj}({3,5})={2,4}\\\\ P\\_8 &=\\\\operatorname{Adj}({4})={3,5}\\\\ P\\_9 &=\\\\operatorname{Adj}({5})={4}. \\\\end{aligned} \\] Â  After the ninth search the only box that could still contain the cat is boxâ€¯4. Therefore when we open boxâ€¯4 on the ninth morning we are certain to find the cat.\n5. Consequently the above opening order guarantees captureÂ **within at most nine mornings**, no matter how the cat moves.\n\nOpen the boxes in the order 1,2,3,4,5,1,2,3,4 (repeat if necessary). This forces a capture within 9 days.\n\n\\`\\`\\`",
                  "score": 2,
                  "created_utc": "2025-12-24 11:10:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvpcch5",
          "author": "Xamanthas",
          "text": "Confirmation bias (including upvoters) caught in 4k.",
          "score": 5,
          "created_utc": "2025-12-24 11:44:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvq0luu",
              "author": "ForsookComparison",
              "text": "Only 60fps though as I'm not using Displayport.",
              "score": 0,
              "created_utc": "2025-12-24 14:35:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvpzi4w",
          "author": "TechNerd10191",
          "text": ">GPT-OSS-120B not being smart\n\nScoring 38/50 on the public test set of AIMO 3 (IMO-level math problems) ...",
          "score": 5,
          "created_utc": "2025-12-24 14:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtykth",
              "author": "ShinyAnkleBalls",
              "text": "Benchmark != Smart",
              "score": 3,
              "created_utc": "2025-12-25 05:44:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvpzxk3",
              "author": "ForsookComparison",
              "text": "Benchmarks always matching vibes/opinions is why this whole sub uses Mistral 3 right?",
              "score": 5,
              "created_utc": "2025-12-24 14:31:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvpayt6",
          "author": "bigblockstomine",
          "text": "Writing in cpp, agentic coding for me isnt worth it, im still better off at the prompt and relying on ai solely fot grunt tasks (which for me is about half of all coding). Stuff like aider and claude code for my work gets far too much wromg but for webdev, etc id imagine its very helpful. Template metaprogramming is an area of cpp ai still isnt good at. With the amount of time required for tweaking llamacpp flags, verifying output, thinking of how exactly to phrase questions, etc its still easier and faster to just write the code myself, again only for about half my tasks.",
          "score": 2,
          "created_utc": "2025-12-24 11:32:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvooyx4",
          "author": "mr_Owner",
          "text": "Glm instead of gpt",
          "score": 3,
          "created_utc": "2025-12-24 07:57:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpz4u4",
              "author": "ForsookComparison",
              "text": "Haven't evaluated against these enough",
              "score": 1,
              "created_utc": "2025-12-24 14:26:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvs777x",
          "author": "rm-rf-rm",
          "text": "Can you give us some more substantiation as to why you think this?",
          "score": 2,
          "created_utc": "2025-12-24 21:54:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs85kk",
              "author": "ForsookComparison",
              "text": "Several iterations over three tools (opencoder, Qwencode CLI, and I like to throw Roo Code in there for an agentic mode that doesn't have \"real\" tool calls).\n\nA few projects and codebases in known-bad states or with feature work needed, I let them loose with identical prompts trying to fix them step by step and then rewrite or update tests accordingly.\n\nI also rotate between them for general use stuff throughout the day.\n\nThe three circle divide I crudely drew here became really apparent. Some models fell flat on their face when it came to iterating on their previous work or doing anything step by step. Some models had the right idea and could play well with tools and as an agent, but couldn't write good/working code to save their lives. And some models could write code that achieved their goals but their goals and decisions were outright stupid. Hence can-agent, can-code, can-smart. Everything else emerging from the results felt nitpicky, but these three categories felt consistent.\n\nThis Venn Diagram is my rough late-night dump of how I feel about these MoE's currently.\n\nQwen3-Next-80B is the only thing that seems consistent and rock solid here, however it's far from perfect. The inference speed even after the Llama CPP updates last week is still closer to that if a ~20B dense model rather than a very sparse MoE which is a pain for a lot of things.",
              "score": 2,
              "created_utc": "2025-12-24 22:00:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvt5rq9",
                  "author": "rm-rf-rm",
                  "text": "Thanks for the detailed answer. Curious Why you're saying the GPT-OSS 120B does not have good knowledge? it's the most knowledgeable out of the bunch pictured IMO and that makes sense as its the biggest. Its my go to model for general QnA and its been pretty great.",
                  "score": 2,
                  "created_utc": "2025-12-25 01:55:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvok1ya",
          "author": "Long_comment_san",
          "text": "This seems to be ok. Now to wait for a new GLM 4.7 air",
          "score": 2,
          "created_utc": "2025-12-24 07:10:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpery2",
          "author": "-oshino_shinobu-",
          "text": "These astroturfing posts are getting out of hand. Canâ€™t even bother to back it up with a fake graph?",
          "score": 2,
          "created_utc": "2025-12-24 12:05:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqahoq",
              "author": "my_name_isnt_clever",
              "text": "I don't know why the assumption is always a malicious campaign by someone. People can also just have bad opinions.",
              "score": 8,
              "created_utc": "2025-12-24 15:31:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvpyztr",
              "author": "ForsookComparison",
              "text": "> astroturfing\n\nYes I work for Alibaba. Please buy more knockoff bulk pokemon merch, Western consumer.",
              "score": 8,
              "created_utc": "2025-12-24 14:25:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvpc8p9",
          "author": "SatoshiNotMe",
          "text": "Using these with the right harness can make a difference, e.g with Claude Code or Codex CLI. Hereâ€™s a guide I put together for running them with Llama-server and using with these CLI agents: \n\nhttps://github.com/pchalasani/claude-code-tools",
          "score": 1,
          "created_utc": "2025-12-24 11:43:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqvq5y",
          "author": "ninjasaid13",
          "text": "Is there any that is smart, long task oriented, and is bad at code?",
          "score": 1,
          "created_utc": "2025-12-24 17:25:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvr86h1",
          "author": "FamilyNurse",
          "text": "Where Qwen3-VL?",
          "score": 1,
          "created_utc": "2025-12-24 18:32:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrgzey",
              "author": "ForsookComparison",
              "text": "The vision version of 30b-a3b is slightly worse than the 2507 update I found so I stopped using it for non vision tasks early on",
              "score": 0,
              "created_utc": "2025-12-24 19:20:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvrg3q3",
          "author": "[deleted]",
          "text": "Replace the Qwen3-Next 80B with MiniMax M2.1",
          "score": 1,
          "created_utc": "2025-12-24 19:15:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvskz1e",
          "author": "MerePotato",
          "text": "Qwen 3 Next is weaksauce compared to OSS 120B and Nemotron Nano",
          "score": 1,
          "created_utc": "2025-12-24 23:25:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtn17p",
              "author": "ForsookComparison",
              "text": "There's a discussion to be had about gpt oss \n\nNano? No way",
              "score": 1,
              "created_utc": "2025-12-25 04:08:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtem5f",
          "author": "MR_-_501",
          "text": "Qwen 3 coder is way better at long agentic tasks than qwen 3 next in my experience",
          "score": 1,
          "created_utc": "2025-12-30 22:56:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyao6g",
      "title": "Meta released RPG, a research plan generation dataset on Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/datasets/facebook/research-plan-gen",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-29 02:58:09",
      "score": 260,
      "num_comments": 21,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nwhk8eb",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-29 04:00:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhjva1",
          "author": "LoveMind_AI",
          "text": "Meta is humiliating OpenAI in terms of research and open source contributions. I have a feeling the days of open frontier models are over, but theyâ€™re still doing a lot.",
          "score": 103,
          "created_utc": "2025-12-29 03:57:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhlg3m",
              "author": "TheRealMasonMac",
              "text": "Chinese labs probably appreciate the free research. Especially since this one comes with evaluation criteria so they can RL on it.",
              "score": 35,
              "created_utc": "2025-12-29 04:07:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwhlyyd",
                  "author": "Southern-Chain-6485",
                  "text": "Welcome to science",
                  "score": 58,
                  "created_utc": "2025-12-29 04:11:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwi95f9",
              "author": "eat_my_ass_n_balls",
              "text": "Sorta, but their models have fallen off",
              "score": 0,
              "created_utc": "2025-12-29 07:02:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhnv9t",
          "author": "Any-Conference1005",
          "text": "Acronym collision.......",
          "score": 36,
          "created_utc": "2025-12-29 04:22:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhpalv",
              "author": "HistorianPotential48",
              "text": "can't wait for coming up HGAME dataset, FEMBOY datasets from meta",
              "score": 32,
              "created_utc": "2025-12-29 04:32:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwi8967",
              "author": "FaceDeer",
              "text": "I really need to train an LLM for some serious hardcore RPG, and I keep finding plenty of datasets that claim that they're for this purpose. But the LLMs keep turning out wrong! Every time I demo for my supervisor... honestly, I have no idea why my funding hasn't been pulled, or why he keeps the resulting models. They're useless.",
              "score": 7,
              "created_utc": "2025-12-29 06:54:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhb3i5",
          "author": "segmond",
          "text": "Would be nice if folks release dataset with models trained on it.",
          "score": 15,
          "created_utc": "2025-12-29 03:05:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhiam9",
              "author": "Accomplished_Ad9530",
              "text": "They cite their unreleased paper, â€œTraining AI Co-Scientists using Rubric Rewardsâ€ so I wouldnâ€™t be surprised if they release a model at some point.",
              "score": 16,
              "created_utc": "2025-12-29 03:48:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi9g9x",
          "author": "JudgmentPale458",
          "text": "Interesting release. Research plan generation feels like a subtle but important capability â€” especially for agentic or tool-using systems where planning quality matters more than final answer fluency.\n\nCurious how this dataset handles evaluation: are plans judged mainly on structure/coverage, or is there any signal about feasibility and downstream execution success? That distinction seems critical if this is used to train agents rather than just planners.",
          "score": 5,
          "created_utc": "2025-12-29 07:04:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkbh1n",
          "author": "martinerous",
          "text": "Great, now waiting what they will make out of MMORPG.",
          "score": 1,
          "created_utc": "2025-12-29 16:05:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrk07z",
          "author": "stealthagents",
          "text": "This dataset sounds like a game changer for streamlining research. Having those evaluation rubrics and reference solutions will save a ton of time for any AI training. Can't wait to see what kind of projects come out of this!",
          "score": 1,
          "created_utc": "2025-12-30 17:39:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvjhs7",
          "author": "Brenan-Caro",
          "text": "Research Plan Gen",
          "score": 1,
          "created_utc": "2025-12-31 06:54:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi2ub3",
          "author": "serendipity777321",
          "text": "What is this for? Not one single explanation",
          "score": 0,
          "created_utc": "2025-12-29 06:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi7cer",
              "author": "Odd-Ordinary-5922",
              "text": "22k tasks spanning ML, Arxiv and PubMed, complete with evaluation rubrics and Llama-4 reference solutions for training **AI co-scientists**",
              "score": 15,
              "created_utc": "2025-12-29 06:46:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwigkvv",
                  "author": "serendipity777321",
                  "text": "You must be joking",
                  "score": -2,
                  "created_utc": "2025-12-29 08:09:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwiquf6",
              "author": "know-your-enemy-92",
              "text": "Taking science back to the times of alchemy from middle ages.Â ",
              "score": 2,
              "created_utc": "2025-12-29 09:46:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pw8nfk",
      "title": "Nvidia acquired Groq, but why not Cerebras? Cerebras is 3x times faster than Groq, while maximum 1.5x the price. Anyone can explain?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/",
      "author": "Conscious_Warrior",
      "created_utc": "2025-12-26 16:42:23",
      "score": 260,
      "num_comments": 139,
      "upvote_ratio": 0.92,
      "text": "Anyone with technical knowledge can explain why they chose Groq over Cerebras? Really interested in this. Because Cerebras is even waaay faster than Groq. Cerebras seems like a bigger threat to Nvidia than Groq...",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nw344jd",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-26 21:20:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1p3pb",
          "author": "LeTanLoc98",
          "text": "Groq is mainly an architectural improvement. NVIDIA could potentially integrate ideas from Groq's architecture into their existing GPUs.\n\n\nCerebras is essentially a single, massive GPU. NVIDIA could build something similar on their own without relying on Cerebras. That kind of huge chip is also much more prone to manufacturing defects. On top of that, Cerebras GPUs only work well for a limited set of models.",
          "score": 301,
          "created_utc": "2025-12-26 16:47:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1pmbz",
              "author": "LeTanLoc98",
              "text": "https://groq.com/blog/the-groq-lpu-explained",
              "score": 66,
              "created_utc": "2025-12-26 16:50:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1t3m4",
                  "author": "Clear_Anything1232",
                  "text": "Reads a lot like Google TPU architecture",
                  "score": 57,
                  "created_utc": "2025-12-26 17:09:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1rc4q",
                  "author": "LeTanLoc98",
                  "text": "https://i.redd.it/iwa4ix7gxk9g1.gif\n\nGroq LPU",
                  "score": 49,
                  "created_utc": "2025-12-26 16:59:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1rf2z",
                  "author": "LeTanLoc98",
                  "text": "https://i.redd.it/gyugxquixk9g1.gif\n\nGPU without Groq LPU",
                  "score": 21,
                  "created_utc": "2025-12-26 17:00:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9ahld",
              "author": "limb3h",
              "text": "So many things wrong here.  First, Cerebras is not GPU. Second, Nvidia can also build what groq did easily without relying on Groq.  Itâ€™s a lot harder for Nvidia to replicate groq than Cerebras on their own.",
              "score": 3,
              "created_utc": "2025-12-27 21:40:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw5v1se",
              "author": "alex_pro777",
              "text": "Why not buying both Groq & Cerebras to complete the deal?",
              "score": 1,
              "created_utc": "2025-12-27 08:38:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7z7cm",
                  "author": "LeTanLoc98",
                  "text": "A better alternative to Groq is SambaNova, not Cerebras.",
                  "score": 1,
                  "created_utc": "2025-12-27 17:32:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw2bgt8",
              "author": "Calandracas8",
              "text": "It's not an architectural improvement, it's an architectural disaster. VLIW does not work, it never has, and never will.\n\nAll it's good for is scamming investors",
              "score": -5,
              "created_utc": "2025-12-26 18:44:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2gitk",
                  "author": "Tomi97_origin",
                  "text": "Investors just got bought out by Nvidia. I would say they consider that successful venture.",
                  "score": 39,
                  "created_utc": "2025-12-26 19:11:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw46e8h",
                  "author": "Tai9ch",
                  "text": "> VLIW does not work, it never has, and never will.\n\nYup. Also, multi-core will never catch on and vector instructions are a marketing gimmick.",
                  "score": 23,
                  "created_utc": "2025-12-27 01:01:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw4o2eq",
                  "author": "i_wayyy_over_think",
                  "text": "u can run groq on open router and it is fast with low latency",
                  "score": 9,
                  "created_utc": "2025-12-27 02:54:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw2zf62",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -3,
              "created_utc": "2025-12-26 20:54:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw34ktk",
                  "author": "GustavoTC",
                  "text": "If you're just vomiting llm responses why even bother commenting",
                  "score": 2,
                  "created_utc": "2025-12-26 21:22:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1xdgw",
          "author": "bick_nyers",
          "text": "It's not an acquisition in the traditional sense, more like a licensing deal to all of Groq's IP and tech. Which of course one would argue is \"effectively\" an acquisition. Fair.\n\n\nGroq probably has more interconnect stuff figured out than Cerebras since their cards each have 256MB SRAM each and they need to do some crazy networking dark magic to inference something like Kimi versus Cerebras that just throws everything on the big chungus chip. Perhaps there are some custom data types and/or compression techniques at Groq that NVIDIA wanted to get their hands on.\n\n\nIt's possible NVIDIA tried to approach Cerebras but they didn't want to sell.\n\n\nI know this is local llama but I have to say Cerebras Code is my favorite AI subscription. I hope they continue to prosper.",
          "score": 45,
          "created_utc": "2025-12-26 17:31:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2vwkw",
              "author": "z_3454_pfk",
              "text": "itâ€™s an acquisition. this is just to get around anti-trust lawsuits.",
              "score": 21,
              "created_utc": "2025-12-26 20:35:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw80j9q",
                  "author": "galenkd",
                  "text": "I agree and I find it troubling. When one company buys another company, they usually take on all the employees. They will then lay off the redundant persons (e.g., hr, finance, facilities, etc.) with a decent package. For those redundant employees, their equity has converted so there could be a big payoff down the line.\n\nWhat's happening here is Nvidia is taking the IP via license and only the employees they want. Groq gets a lot of money and the board will decide how to distribute it. There's a strong chance that those not moving to nVidia will just be around to wind the company down. The board can distribute the funds as they see fit and leave the employees totally in the cold. There are no norms on how to do this.\n\nYou could argue that the employees could get screwed in a full acquisition, too, but that would violate a lot of norms. Though, I guess it's 2025 and norms are worth very little.",
                  "score": 3,
                  "created_utc": "2025-12-27 17:39:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1pfb3",
          "author": "HarambeTenSei",
          "text": "Gotta leave something for AMDÂ ",
          "score": 39,
          "created_utc": "2025-12-26 16:49:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw28bpx",
          "author": "jakegh",
          "text": "My guess is it's because Groq doesn't fabricate their chips at TSMC, as Nvidia doesn't want to trade any of their allotted capacity.\n\nAlso, Nvidia did not acquire Groq. They bought the CEO, the top engineers, and a non-exclusive license to the tech.",
          "score": 13,
          "created_utc": "2025-12-26 18:28:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1ufrz",
          "author": "locoblue",
          "text": "Because Googles TPUs are great and Nvidia wants to explore that space.\n\nCerebras is firmly within Nvidias wheelhouse already",
          "score": 29,
          "created_utc": "2025-12-26 17:16:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2018i",
          "author": "NorthSideScrambler",
          "text": "Because the Trump family invested in Groq in September via 1789 Capital.Â  Nvidia overpaid for Groq by 2x, and there's a reason for that.Â Â \n\n\nEdit: It becomes less conspiratol when you see how often Nvidia shares a bed with the Trump admin.Â  There were the Intel investments when the Trump admin took 10% of Intel, and Jensen Huang's $1,000,000 dinner at Mar A Lago followed immediately by the White House reversing export restrictions on Nvidia's H20 chips to China.\n\n\nOh, and Nvidia actually overpaid for Groq by 3x.Â  Charitably, you could split the difference and say 2.5x.Â  Groq was worth $6.9 billion in late September.Â Â ",
          "score": 90,
          "created_utc": "2025-12-26 17:45:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw21ubu",
              "author": "emprahsFury",
              "text": "those gpu sales in china don't come free",
              "score": 17,
              "created_utc": "2025-12-26 17:55:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwc5bc2",
              "author": "vincentz42",
              "text": "This needs to be upvoted much higher. The groq deal never made much sense to me on a technical and business basis. I always wondered if the deal is to please certain VC/PE people behind groq (1789, Chamath Palihapitiya, Black Rock?)\n\nSure, SRAMs offer 10x bandwidth, but they have 100-1000x less capacity. Groq chips are fast in terms of latency, but they are much slower in terms of per chip throughout compared to GPUs. And you would need 10K chips just to serve a 1T MoE to a limited set of users so this is not exactly scalable. And in the end there's no reason that NVIDIA can't slap a SRAM cache to their GPUs like AMD X3D. So the technology/TPU talent side of story never made much sense IMHO.",
              "score": 2,
              "created_utc": "2025-12-28 09:04:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw66oky",
              "author": "Fit_Border9119",
              "text": "1789 also invested in Cerebras' latest funding round.",
              "score": 1,
              "created_utc": "2025-12-27 10:32:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw2tncq",
              "author": "MokoshHydro",
              "text": "And I thought about Blackrock first. :)",
              "score": 1,
              "created_utc": "2025-12-26 20:22:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw24dax",
          "author": "alvisanovari",
          "text": "From this post\n\n\"And Cerebras, where I am biased, is now in a very interesting and highly strategic position as the last (per public knowledge) independent SRAM player that was ahead of Groq on all public benchmarks. Groqâ€™s â€œmany chipâ€ rack architecture, however, was much easier to integrate with Nvidiaâ€™s networking stack and perhaps even within a single rack while Cerebrasâ€™s WSE almost has to be an independent rack.\"\n\n[https://x.com/GavinSBaker/status/2004562536918598000](https://x.com/GavinSBaker/status/2004562536918598000)",
          "score": 19,
          "created_utc": "2025-12-26 18:08:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2len4",
              "author": "SlowFail2433",
              "text": "Makes some more sense",
              "score": 9,
              "created_utc": "2025-12-26 19:37:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw4o5nn",
              "author": "puppymaster123",
              "text": "Kinda freaking out a bit here because some of my successful apps are powered by groq multimodal image to text. Cerebras does not have multimodal in its offerings yet. So thereâ€™s really no alternative when it comes to fast inference for multimodal.",
              "score": 6,
              "created_utc": "2025-12-27 02:55:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9xs8c",
                  "author": "Apprehensive_Plan528",
                  "text": "Groq cloud lives on. If it works for you today, no worries.",
                  "score": 1,
                  "created_utc": "2025-12-27 23:48:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw4ovb1",
                  "author": "alvisanovari",
                  "text": "what's there to freak out about? they'll only get better.",
                  "score": 1,
                  "created_utc": "2025-12-27 03:00:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw2r7ag",
              "author": "desexmachina",
              "text": "Good catch. With Nvidia already deep into GPU Direct, something complimentary in the pipeline might be usable over a wider distributed compute infra. Because TBH, what does Nvidia need another inference chip for?  Especially one that acts more like a CPU than a TPU.",
              "score": 5,
              "created_utc": "2025-12-26 20:09:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2w1qz",
                  "author": "alvisanovari",
                  "text": "probably to corner the market on lightweight low latency takss like autocomplete. once blackwell comes online it will dominate memory intensive thinking outputs",
                  "score": 1,
                  "created_utc": "2025-12-26 20:35:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1ol9e",
          "author": "TheToi",
          "text": "I had the same thought. But maybe they want to invest in Groq so they can surpass Cerebras at a lower cost?",
          "score": 11,
          "created_utc": "2025-12-26 16:45:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5gz5w",
          "author": "ieatrox",
          "text": "Groq uses 0 HBM.\n\nIts a 20B hedge against a supply chain attack Jensen referred to as a matter of national security.\n\nIt uses 14nm fabs like intel has sitting around. Like the ones Nvidia invested in intel to gain access to.\n\nOnce training is completed, inference on a 10 million dollar groq cluster is roughly as performant as on a 10 million dollar b200 cluster. But the groq clusters can be made at home for pennies regardless of global constraints or stress on TSMC.\n\nJensen's no dummy.",
          "score": 9,
          "created_utc": "2025-12-27 06:26:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7n2ab",
          "author": "keepthepace",
          "text": "Maybe Cerebras does not want to sell?",
          "score": 7,
          "created_utc": "2025-12-27 16:30:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw38srd",
          "author": "CatalyticDragon",
          "text": "Trump's son is an investor in Groq.",
          "score": 11,
          "created_utc": "2025-12-26 21:45:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwwspi",
              "author": "Dry-Fuel-3654",
              "text": "He is also an investor in Cerebras. Personally I think Microsoft will purchase them. OpenAI was thinking about buying Cerebras in 2017 but didnâ€™t or couldnâ€™t.",
              "score": 3,
              "created_utc": "2025-12-31 13:53:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1s9le",
          "author": "keyser1884",
          "text": "Because Nvidia specializes in brute force which is great for training models but doesnâ€™t scale as well for inference.\n\nGroq pioneered TPUs and they built them using outdated and affordable fabrication processes.\n\nSo Nvidia get another string in their bow and can help Groq products achieve greater efficiency and manufacturing scale.",
          "score": 6,
          "created_utc": "2025-12-26 17:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw346ym",
          "author": "kidflashonnikes",
          "text": "ITS A RAM play. Weâ€™re out of RAM at our lab. Desperately trying to find some right now. They bough Groq for inference due to their chip architecture (SRAM), the main RAM is DRAM right now",
          "score": 3,
          "created_utc": "2025-12-26 21:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4fsdr",
          "author": "FearlessZucchini3712",
          "text": "Groq ceo is ex google who worked on TPU. Which is a competitor chip for nvidia",
          "score": 3,
          "created_utc": "2025-12-27 02:01:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4x7ct",
          "author": "Virtamancer",
          "text": "Cerebraâ€™s is fast because they use gigaquantized models, judging by the benchmarks and openrouter data posted around here from time to time.",
          "score": 3,
          "created_utc": "2025-12-27 03:55:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1tkef",
          "author": "LeTanLoc98",
          "text": "> Cerebras seems like a bigger threat to Nvidia than Groq...\n\n\nCerebras is not really a threat to NVIDIA.\n\n\nCerebras GPUs only work well for a limited set of models.\n\n\nThey are mainly optimized for inference, while NVIDIA still clearly dominates training thanks to CUDA and its mature ecosystem.\n\n\nNVIDIA could build a massive, wafer-scale GPU like Cerebras if they wanted to. They simply choose not to. Selling large numbers of smaller GPUs is far more profitable for them, since customers have to buy many units instead of one giant chip.",
          "score": 7,
          "created_utc": "2025-12-26 17:11:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3bogs",
              "author": "dogesator",
              "text": "Cerebras is better at training than groq though, so this doesnâ€™t really answer OPs question",
              "score": 5,
              "created_utc": "2025-12-26 22:00:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5edtq",
                  "author": "b_m_hart",
                  "text": "Groq chips aren't designed to do any training, it is like trying to use a blow torch as a hammer.Â Â ",
                  "score": 3,
                  "created_utc": "2025-12-27 06:04:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7z8q3",
              "author": "LeTanLoc98",
              "text": "A better alternative to Groq is SambaNova, not Cerebras.",
              "score": 0,
              "created_utc": "2025-12-27 17:32:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw237kk",
          "author": "Erebea01",
          "text": "Maybe the guys at Groq has better connections or are better salesmen, sometimes it can be as simple as that",
          "score": 2,
          "created_utc": "2025-12-26 18:02:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2tf0g",
          "author": "MokoshHydro",
          "text": "Because that deal has nothing to do with technology. That's just Blackrock gathering investment money back. $20B price doesn't have any other reason behind.",
          "score": 2,
          "created_utc": "2025-12-26 20:21:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw459aw",
          "author": "orangeatom",
          "text": "chamath and jenson are friends",
          "score": 2,
          "created_utc": "2025-12-27 00:54:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwapjpz",
          "author": "CommercialLead6053",
          "text": "Nvidia didn't actually acquire Groq though? Where are you seeing that news because I can't find anything about it\n\n  \nAlso Cerebras has their own fab issues and their chips are massive - way harder to scale production compared to Groq's approach. Speed isn't everything if you can't manufacture at volume",
          "score": 2,
          "created_utc": "2025-12-28 02:29:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbe7de",
          "author": "Kassdhal88",
          "text": "Cerebras is not scalable.",
          "score": 2,
          "created_utc": "2025-12-28 05:06:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1tt4z",
          "author": "ttkciar",
          "text": "At a guess:\n\n* Cerebras' wafer-scale approach is more radical than Groq's, and \"radical\" frightens conservative businesspeople,\n\n* Cerebras hasn't solved their memory density problem, last I checked, and still relies on SRAM.  That sharply limits the amount of memory on the die, which is a constraint for some kinds of tasks (like training models of any significant size).\n\nIf Cerebras can solve their memory problem, they're going to be a beast to reckon with, and Nvidia executives might wind up kicking themselves for passing them over.  We will see how it pans out.",
          "score": 2,
          "created_utc": "2025-12-26 17:12:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw20ayk",
              "author": "hello_2221",
              "text": "I wonder if they could increase memory with 3d stacking like amd's x3d chips",
              "score": 3,
              "created_utc": "2025-12-26 17:47:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw52685",
                  "author": "ttkciar",
                  "text": "I've wondered that, too.  Maybe they're working on it.",
                  "score": 2,
                  "created_utc": "2025-12-27 04:30:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw228aw",
              "author": "emprahsFury",
              "text": "WSA's have been well-known since the 70s and 80s. It's not a radical technology at all. It's just too expensive",
              "score": 2,
              "created_utc": "2025-12-26 17:57:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1prap",
          "author": "CertainlyBright",
          "text": "Same argument as \"AMD faSteR tHaN InTeL whY eVerYoNe sTiLL bUy InTeL??? ðŸ˜­\" when it's not just speed, but stability, support, and creative foundation a product is built on.",
          "score": 1,
          "created_utc": "2025-12-26 16:51:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1q6c6",
              "author": "tertain",
              "text": "But no one buys intelâ€¦",
              "score": 17,
              "created_utc": "2025-12-26 16:53:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1qa9p",
                  "author": "CertainlyBright",
                  "text": "$INTC Hmmmm....",
                  "score": -3,
                  "created_utc": "2025-12-26 16:54:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1qy68",
              "author": "BriguePalhaco",
              "text": "Intel stable? Didn't they create an oxidation problem by skimping on a chemical that protected against it?",
              "score": 9,
              "created_utc": "2025-12-26 16:57:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1q59p",
              "author": "ChomsGP",
              "text": "zero days disagree with this statement",
              "score": 4,
              "created_utc": "2025-12-26 16:53:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1r2g1",
                  "author": "BriguePalhaco",
                  "text": "Oxidation in the CPU as well.",
                  "score": 2,
                  "created_utc": "2025-12-26 16:58:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw261cu",
          "author": "xjE4644Eyc",
          "text": "Cerebras IPO is soon IIRC.  20B might be too little",
          "score": 1,
          "created_utc": "2025-12-26 18:16:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2fl4y",
          "author": "gwestr",
          "text": "Canâ€™t find any actual press release from Nvidia that they would acquire grok a few quarters from now.",
          "score": 1,
          "created_utc": "2025-12-26 19:06:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2p7dy",
          "author": "valdev",
          "text": "We donâ€™t actually know.",
          "score": 1,
          "created_utc": "2025-12-26 19:58:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw365ti",
          "author": "jafbm",
          "text": "I heard through the grapevine that Chamath wanted out",
          "score": 1,
          "created_utc": "2025-12-26 21:31:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4x3ai",
          "author": "Wubbywub",
          "text": "you gotta realize the world runs on many reasons other than technical logic, lots of political and interpersonal relationship reasons",
          "score": 1,
          "created_utc": "2025-12-27 03:55:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5eu1u",
              "author": "b_m_hart",
              "text": "And this deal is based entirely in logic and reason and what Nvidia wanted from Groq's current and upcoming chips.",
              "score": 1,
              "created_utc": "2025-12-27 06:07:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwa10o7",
          "author": "anima-core",
          "text": "A few important distinctions get blurred in this comparison.\n\n**Cerebras and Groq are optimizing for fundamentally different regimes.**\n\nCerebras wins when:\n\nâ€¢You want very large models\n\nâ€¢You want training or long-sequence inference\n\nâ€¢You want to avoid partitioning overhead, collective ops, and interconnect complexity\n\n\nA single wafer-scale engine keeps the entire model and activations on-chip. Thatâ€™s why Cerebras looks absurdly fast on certain workloads. No sharding, no NCCL, no cross-node synchronization. Itâ€™s a â€œone giant brainâ€ architecture.\n\nGroq, by contrast, is optimized for:\n\nâ€¢Deterministic, ultra-low-latency inference\n\nâ€¢Tight compiler control\n\nâ€¢Serving workloads at scale with predictable timing\n\n\nThat maps much more cleanly onto Nvidiaâ€™s existing CUDA + inference ecosystem.\n\nSo why would Nvidia favor Groq?\n\nBecause Groq complements Nvidiaâ€™s business, while Cerebras challenges it structurally. Head on.\n\nGroq:\n\nâ€¢Looks like an accelerator that can slot into existing stacks\n\nâ€¢Reinforces the idea that â€œmodels stay the same, hardware gets fasterâ€\n\nâ€¢Doesnâ€™t threaten CUDA, sharding, or GPU cluster economics\n\n\nCerebras:\n\nâ€¢Implicitly says â€œthe cluster model itself is brokenâ€\n\nâ€¢Eliminates whole layers Nvidia monetizes (interconnects, multi-GPU scaling, orchestration)\n\nâ€¢Pushes toward fewer, larger, simpler systems\n\n\n**Thatâ€™s not a performance argument. Thatâ€™s a business and control argument.**\n\nWhere Cerebras actually has a David-vs-Goliath opening:\n\nâ€¢Frontier training (large dense models, long context)\n\nâ€¢Government / national labs\n\nâ€¢Workloads where simplicity and determinism beat flexibility\n\nâ€¢Architectures that donâ€™t scale cleanly across thousands of GPUs\n\n\nIf the future moves toward:\n\nâ€¢Larger contexts\n\nâ€¢Fewer model shards\n\nâ€¢Less distributed complexity\n\nâ€¢More â€œsystem-levelâ€ thinking\n\n\nCerebras becomes more dangerous, not less.\n\nNvidia didnâ€™t â€œmissâ€ Cerebras.\nThey likely see it as **too disruptive to absorb cleanly**, whereas Groq is additive.\n\nDifferent weapons. Different wars.",
          "score": 1,
          "created_utc": "2025-12-28 00:06:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1s24w",
          "author": "Gringe8",
          "text": "They didnt acquire groq.",
          "score": 0,
          "created_utc": "2025-12-26 17:03:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1yj3w",
              "author": "tmvr",
              "text": "Well, they didn't, but effectively they did. They licensed the tech and took all the key people (management and arch). So the company still exists, but what is the future? They basically went around regulatory processes so that there are no hearings because it is not an acquisition.",
              "score": 20,
              "created_utc": "2025-12-26 17:37:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw21qoy",
              "author": "Freonr2",
              "text": "They did in all but name, only a husk leftover.",
              "score": 7,
              "created_utc": "2025-12-26 17:54:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2645r",
                  "author": "PwanaZana",
                  "text": "an aquihire, I've heard this sorta deal called",
                  "score": 8,
                  "created_utc": "2025-12-26 18:17:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1onbj",
          "author": "LevianMcBirdo",
          "text": "Cerebras is private, so if they don't wanna sell there isn't much nvidea can do",
          "score": 0,
          "created_utc": "2025-12-26 16:45:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1quew",
              "author": "piggledy",
              "text": "Groq is private too",
              "score": 12,
              "created_utc": "2025-12-26 16:57:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1uh7b",
                  "author": "LevianMcBirdo",
                  "text": "You get the difference that one wants to sell while the other potentially doesn't?",
                  "score": 0,
                  "created_utc": "2025-12-26 17:16:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw2b6oq",
          "author": "Calandracas8",
          "text": "Groq is a scam, it has been shown over and over how writing a compiler for VLIW is impossible. Never mind that groq is an insane 144 wide VLIW.\n\nGoogle's TPU uses an 8 wide VLIW architecture for control flow, but a traditional architecture for compute.\n\nPerhaps they see some good bit that they want to use some of the IP to build some sort of new product, but I'm also very surprised since Groq's architecture's is not good",
          "score": -3,
          "created_utc": "2025-12-26 18:43:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4qe12",
              "author": "i_wayyy_over_think",
              "text": "They serve open source models often with nearly the highest throughput and lowest latency on open router, how's that a scam? [https://openrouter.ai/moonshotai/kimi-k2-0905?sort=latency](https://openrouter.ai/moonshotai/kimi-k2-0905?sort=latency)\n\n[https://openrouter.ai/provider/groq](https://openrouter.ai/provider/groq)",
              "score": 8,
              "created_utc": "2025-12-27 03:10:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2vqq0",
          "author": "Active_Change9423",
          "text": "groq has been in panic mode since Cerebras inference knocked them off the top spot. I bet the leadership team was especially eager to get on board the nvidia escape raft.",
          "score": 1,
          "created_utc": "2025-12-26 20:34:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5e6zc",
              "author": "b_m_hart",
              "text": "Weirdly random FUD that is entirely untrue.",
              "score": 8,
              "created_utc": "2025-12-27 06:02:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1pmeu",
          "author": "Odd-Ordinary-5922",
          "text": "just because nvidia has money doesnt mean they can buy a company",
          "score": -3,
          "created_utc": "2025-12-26 16:50:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1t7yx",
              "author": "Tman1677",
              "text": "If you're one of these teeny little startups that could run out of runway any day now, your dream is to get acquired for 10 billion",
              "score": 4,
              "created_utc": "2025-12-26 17:09:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1udw7",
                  "author": "Odd-Ordinary-5922",
                  "text": "I dont think cerebras is a tiny little startup",
                  "score": 3,
                  "created_utc": "2025-12-26 17:15:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw2kq9l",
              "author": "Randommaggy",
              "text": "Aquihires are effectively the same except with less scrutiny.",
              "score": 1,
              "created_utc": "2025-12-26 19:33:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1sg1x",
          "author": "MrDevGuyMcCoder",
          "text": "Grok is a joke isnt it? As in no one actually uses or trusts it",
          "score": -14,
          "created_utc": "2025-12-26 17:05:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1t03o",
              "author": "Internal_Werewolf_48",
              "text": "Grok =/= Groq",
              "score": 20,
              "created_utc": "2025-12-26 17:08:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1y8c1",
                  "author": "MrDevGuyMcCoder",
                  "text": "Ya, my bad. Reading is hard apparently",
                  "score": 5,
                  "created_utc": "2025-12-26 17:36:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1t0ea",
              "author": "Conscious_Warrior",
              "text": "Groq with a q",
              "score": 6,
              "created_utc": "2025-12-26 17:08:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw1uhbd",
                  "author": "MrDevGuyMcCoder",
                  "text": "Oh... Reading comprehension issue here apparently... What is groq?",
                  "score": 2,
                  "created_utc": "2025-12-26 17:16:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1wdso",
              "author": "Mediocre-Method782",
              "text": "I don't; they spammed this sub very heavily with their non-local ad shit long ago and now look at the place",
              "score": 1,
              "created_utc": "2025-12-26 17:26:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q06ddc",
      "title": "Update on the Llama 3.3 8B situation",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/",
      "author": "FizzarolliAI",
      "created_utc": "2025-12-31 06:45:42",
      "score": 255,
      "num_comments": 22,
      "upvote_ratio": 0.93,
      "text": "Hello! You may remember me as either\n\n- The person [who recently uploaded L3.3 8B's weights to Huggingface](https://www.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/) (see this post for more context)\n- That stupid bitch\n\nand I would like to provide some updates, as I've been doing some more benchmarks on both the original version that Meta gave me and the context extended version by u/Few-Welcome3297.\n\nThe main benchmark table from the model README has been updated:\n\n| | Llama 3.1 8B Instruct | Llama 3.3 8B Instruct (original 8k config) | Llama 3.3 8B Instruct (128k config)\n|-|-|-|-|\n|IFEval (1 epoch, score avged across all strict/loose instruction/prompt accuracies to follow Llama 3 paper)|78.2|81.95|**84.775**\n|GPQA Diamond (3 epochs)|29.3|37.0|**37.5**\n\nWhile I'm not 100% sure, I'm... pretty sure that the 128k model is better. Why Facebook gave me the weights with the original L3 config and 8k context, and also *serves* the weights with the original L3 config and 8k context, I have absolutely no idea!\n\nAnyways, if you want to try the model, I would recommend trying both the [128k version](https://huggingface.co/shb777/Llama-3.3-8B-Instruct), as well as my [original version](https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct) if your task supports 8k context lengths. I honestly have absolutely no clue which is more correct, but oh well! I do wish Facebook had released the weights officially, because back in April, this really wouldn't have been that bad of a model...\n\nEdit: Removed the Tau-Bench results (both from here and the readme). The traces from the evals are, to put it slightly, really fucky-wucky, and I don't think OpenBench is scoring them right, but I'm too tired to actually debug the issue, so. I'll figure it out tomorrow :3",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwvmrda",
          "author": "toothpastespiders",
          "text": ">I do wish Facebook had released the weights officially, because back in April, this really wouldn't have been that bad of a model...\n\nHonestly, I think I prefer it this way. The llama saga began with some public shenanigans with a semi-leak. Seems appropriate in a way that if it has to end, and it does seem to be the case, that everything was capped off by something like this.",
          "score": 88,
          "created_utc": "2025-12-31 07:23:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvl06q",
          "author": "Kahvana",
          "text": "No need to degrade yourself, you're doing fantastic work.\n\nThank you for the release!",
          "score": 112,
          "created_utc": "2025-12-31 07:07:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww6ghg",
              "author": "MoffKalast",
              "text": "Yeah OP, run yourself at at least Q6 ;)",
              "score": 21,
              "created_utc": "2025-12-31 10:28:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvt29i",
          "author": "datbackup",
          "text": "Lol upvoted for humor\n\nGood stuff, I might try this 3.3, it has actually been months since iâ€™ve run any llama model.",
          "score": 22,
          "created_utc": "2025-12-31 08:21:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvp8bu",
          "author": "pmttyji",
          "text": "Thanks for this. Still didn't download original version due to less context thing. I'm gonna try this 128K version this week. Also waiting for feedback from others on this version. Just expecting to replace 3.1 8B with this version.",
          "score": 9,
          "created_utc": "2025-12-31 07:45:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwkxbj",
              "author": "Few-Welcome3297",
              "text": "Very small improvement, but its something",
              "score": 3,
              "created_utc": "2025-12-31 12:34:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwkrkr",
          "author": "Few-Welcome3297",
          "text": "Some evals [https://huggingface.co/datasets/shb777/Llama-3.3-8B-Instruct-128K-Evals](https://huggingface.co/datasets/shb777/Llama-3.3-8B-Instruct-128K-Evals) . TLDR: Small Improvement\n\nEdit: Link updated",
          "score": 10,
          "created_utc": "2025-12-31 12:33:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvmlmo",
          "author": "jacek2023",
          "text": "Would be nice to put some info into model's name to distinguish them",
          "score": 14,
          "created_utc": "2025-12-31 07:21:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvnt4n",
              "author": "FizzarolliAI",
              "text": "I would, but since quants and all have already been made under the original model's name, it's kinda too late :p",
              "score": 13,
              "created_utc": "2025-12-31 07:32:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwdex2",
                  "author": "Awwtifishal",
                  "text": "The new one could be renamed to add -128K or something so the quants also reflect it.",
                  "score": 8,
                  "created_utc": "2025-12-31 11:32:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwcrfv",
          "author": "Cool-Chemical-5629",
          "text": "* That stupid bitch\n\nI need more context, please. ðŸ¤£\n\n  \nIn any case, I tried the extended version yesterday and while it felt pretty weak for stuff like coding etc. it seemed to be a decent base model for E/RP finetunes, because it followed instructions fairly well, but it was HORRIBLY SLOW burning so it would need some nudging from E/RP datasets to keep the story going. I hope E/RP creators will pick it up (and Ministral 14B Instruct too while at it).",
          "score": 10,
          "created_utc": "2025-12-31 11:26:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxs7tf",
          "author": "randomfoo2",
          "text": "Just in case anyone's interested, I ran [shb777/Llama-3.3-8B-Instruct](https://huggingface.co/shb777/Llama-3.3-8B-Instruct) on the Shisa AI's MultiEval on my dev box. \n\nOn the English side, it loses a bit on MixEval Easy and Hard (2024 Chat Arena proxy), but gets a +20% boost in LiveBench (reasoning-focused), +15% GPQA Diamond (PhD level QA), +5% on IFEval, +30% on IFBench (!) and +10% on HumanEval+ (Python). That's some decent gains.\n\nThat being said, on the Japanese side, it takes a big hit on Shaberi (Japanese chat-style functional tests) vs 3.1. I've included my Llama 3.1 8B-based Shisa V2  and Qwen 3 8B-based Shisa V2.1 as well as Llama 3.3 70B and Llama 3.1 405B scores just for comparison, sake.\n\n(I probably wont train a Shisa V2.1 Llama 3.3 8B - the Qwen 3 8B version is already great and it's Apache 2.0 licensed).\n\nhttps://preview.redd.it/6hnc71dggkag1.png?width=3126&format=png&auto=webp&s=10c4dd3231be1ed9749f174c59e8758c134d0009",
          "score": 4,
          "created_utc": "2025-12-31 16:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy05pp",
              "author": "FizzarolliAI",
              "text": "Interesting, I wonder if you'd get a noticeable regression from L3.3 70B on multilingual benches with Llama 3.1 70B then.\n\nI definitely agree that I don't think this is worth building on for most usecases. Personally I think it's an interesting artifact of the times",
              "score": 2,
              "created_utc": "2025-12-31 17:19:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwy8qrh",
                  "author": "randomfoo2",
                  "text": "Hmm, hard to say, I don't have 3.1 70B data handy... 3.3B 70B is in general pretty strong.\n\nIn practical terms, your ultimate multilingual perf is going to be pretty much up to you (tuning).  While the overall number isn't so big, when you look at the stuff we care about like JP IF, JP RP, JP TL, JP nuance, dialogue translation, we're able to get huge boosts from doing training on top of whatever model. Not show nis also our own CLTL tests that test for how many wrong-language tokens get output (huge amounts for most non-target language trained models).\n\nThe benchmark mix we use for our current multieval does feel about right. For the tasks that it's trained on, our V2.1 14B model actually \\*does\\* feel like it outperforms our V2 70B (and sometimes our V2.1 70B and V2 405B even!).\n\n\n\nhttps://preview.redd.it/hpf7vf50vkag1.png?width=3123&format=png&auto=webp&s=b5ea8273adbb61f19412f497ff3d2d06e4f46aed",
                  "score": 1,
                  "created_utc": "2025-12-31 18:01:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvndin",
          "author": "Amazing_Athlete_2265",
          "text": "Interesting. I'll evaluate it and compare",
          "score": 3,
          "created_utc": "2025-12-31 07:28:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvpi7i",
              "author": "pmttyji",
              "text": "Awesome",
              "score": 0,
              "created_utc": "2025-12-31 07:48:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww92bu",
          "author": "ilintar",
          "text": "Is the 128k version just a x16 YaRN extension or a different model?",
          "score": 3,
          "created_utc": "2025-12-31 10:53:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwdgf2",
              "author": "Awwtifishal",
              "text": "Just a config change",
              "score": 5,
              "created_utc": "2025-12-31 11:33:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxcw2i",
          "author": "noctrex",
          "text": "Generated some abliterated gguf's for it:\n\n[https://huggingface.co/noctrex/Llama-3.3-8B-Instruct-128k-abliterated-GGUF](https://huggingface.co/noctrex/Llama-3.3-8B-Instruct-128k-abliterated-GGUF)",
          "score": 3,
          "created_utc": "2025-12-31 15:23:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0u60h",
          "author": "Dangerous_Fix_5526",
          "text": "Thinking/Instruct Hybrid using Unsloth and Claude-Opus 4.6 dataset:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning)\n\nI hope I credited everyone correctly.",
          "score": 3,
          "created_utc": "2026-01-01 02:59:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwxu4p",
          "author": "Robert__Sinclair",
          "text": "I just tested it and its reasoning is extremely lacking.",
          "score": -4,
          "created_utc": "2025-12-31 14:00:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1w1qj",
      "title": "Most optimal vram/performance per price and advice for Shenzhen GPU market",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/4nfcarq96xag1.jpeg",
      "author": "notafakename10",
      "created_utc": "2026-01-02 11:14:30",
      "score": 245,
      "num_comments": 57,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxaaipl",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-02 17:25:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8n57z",
          "author": "DistanceSolar1449",
          "text": "MI100 is best value in terms of perf for $ currently if you donâ€™t need CUDA\n\n4090D 48GB if you need CUDA",
          "score": 47,
          "created_utc": "2026-01-02 11:51:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8ogc0",
              "author": "notafakename10",
              "text": "ROCM has come far enough its not too much of a disadvantage now - my only issue is cooling a MI100 with server fans..",
              "score": 20,
              "created_utc": "2026-01-02 12:02:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx92ztb",
                  "author": "xrailgun",
                  "text": "In my experience, ROCm is \"good enough\" only for brief windows of time. As architectures/libraries/stacks change, ROCm frequently gets left behind and/or existing GPUs get dropped from feature support.",
                  "score": 31,
                  "created_utc": "2026-01-02 13:44:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx99dxt",
                  "author": "FullstackSensei",
                  "text": "If you get an even number of cards, you can cool each pair with a relatively quiet 80mm server fan (Arctic S8038 series).",
                  "score": 5,
                  "created_utc": "2026-01-02 14:22:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9opp1",
                  "author": "Freonr2",
                  "text": "Even if it works for just basic LLM inference, Nvidia cards would also be very good for other models like diffusion (t2i, t2v, etc) and be less hassle in general.  Very good TFLOP/s for diffusion, and any software or github repo you clone will \"just work.\"\n\nOnly downside is that it seems there are some quirks with the 4090 48GB, some people experience some idle vs. load up/down clocking issues.  Can't speak to that first hand but have seen reports from at least a few people about that problem.",
                  "score": 2,
                  "created_utc": "2026-01-02 15:42:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9r3x3",
                  "author": "Ulterior-Motive_",
                  "text": "The most common way is to use 40mm fans which tend to be very loud at max speed, but if you don't mind making a shroud, you can get away with a larger, much quieter fan. I literally made one out of cardboard, and was able to cool 2 MI100s with a single Super Flower Megacool 120mm fan. The cooling wasn't as effective as individual fans, but the noise was much more tolerable.\n\nAnother option is the R9700, which have their own fans, have better prompt processing, but somewhat lower token generation than the MI100. Here, they're only $300 more than a M100, but I'm not sure what they're going for where you are.",
                  "score": 1,
                  "created_utc": "2026-01-02 15:54:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx8wzc7",
          "author": "AlwaysLateToThaParty",
          "text": "As with all of these devices; make sure you sort the cooling out.\n\nSome pretty good prices there.  I'd be tempted by nvidia a40s.  USD$7K for 196GB of700GB/s VRAM.  NVLINK and CUDA to boot.  300W too, so easy with one big power supply.",
          "score": 12,
          "created_utc": "2026-01-02 13:06:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx92sry",
              "author": "notafakename10",
              "text": "That would be great, slightly out of budget but I might be able to swing a 48gb version of the A40",
              "score": 3,
              "created_utc": "2026-01-02 13:43:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx9jyc5",
                  "author": "__JockY__",
                  "text": "Remember: cooling and noise!",
                  "score": 3,
                  "created_utc": "2026-01-02 15:19:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx9bzgn",
          "author": "jack-in-the-sack",
          "text": "You can buy them, but can you leave China with them?",
          "score": 6,
          "created_utc": "2026-01-02 14:36:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9ffwy",
              "author": "notafakename10",
              "text": "You can - I've done it a few times",
              "score": 7,
              "created_utc": "2026-01-02 14:55:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx9cjfb",
              "author": "PsychologicalWeird",
              "text": "I thought it was this or getting done over arriving home",
              "score": 2,
              "created_utc": "2026-01-02 14:40:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8kdy8",
          "author": "ChigGitty996",
          "text": "The 48gb 3090 is vaporware, at least as far as I know.  Did something change?\n\nThe 48gb 4090D or 2 should be good options if they use normal plugs",
          "score": 16,
          "created_utc": "2026-01-02 11:27:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8zgo7",
              "author": "lly0571",
              "text": "There are 48GB 3090 in GPU rental market, but very rare.\n\nhttps://preview.redd.it/mj2kronduxag1.png?width=1473&format=png&auto=webp&s=8cc1a94b22da84e7bbffd458aaba430f2a87a26b",
              "score": 17,
              "created_utc": "2026-01-02 13:22:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx8lko7",
              "author": "notafakename10",
              "text": "48gb 4090D seems like a great buy though they arent super easy to fine as far as I'm aware.   \n  \nThe 3090 48gb I've only really seen a handful, and those were expensive, which is why its not a front runner",
              "score": 7,
              "created_utc": "2026-01-02 11:38:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx8nnim",
                  "author": "durden111111",
                  "text": "I don't think a 48GB 3090 has ever existed. You can solder 2GB chips on a 3090 PCB but it simply wont recognize the extra 24GB",
                  "score": 1,
                  "created_utc": "2026-01-02 11:55:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx9b8lw",
              "author": "DataGOGO",
              "text": "No, they were real, but they all do the 4090 now",
              "score": 3,
              "created_utc": "2026-01-02 14:32:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx91eqm",
          "author": "lly0571",
          "text": "2x4080S 32GB or 1x4090D 48GB.\n\n  \nYou can also get A100 40GB(SXM4 to PCIe) at \\~18000CNY, but I would recommend 4090D myself.",
          "score": 6,
          "created_utc": "2026-01-02 13:35:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx930jo",
              "author": "notafakename10",
              "text": "How rare are the 32gb models? That would be a perfect balance \n\nHave you had any issues with SXM4?",
              "score": 2,
              "created_utc": "2026-01-02 13:44:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx9d34w",
                  "author": "lly0571",
                  "text": "I think you can get a 4080S 32GBÂ at 9000-9500CNY(\\~1300USD) at Xianyu, but few local shops have these GPUs(at least in October).\n\nI don't own an A100 myself, I believe 4090 is better for inference due to its FP8 support.",
                  "score": 1,
                  "created_utc": "2026-01-02 14:43:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx8pc2q",
          "author": "pharrowking",
          "text": "do you live outside china in western regions normally? because im looking at ebay. some of those prices on your list are at best saving you $200. then if you factor the flight to china and hotel and all that, youre not really saving at that point. i assume those prices are in chinese yuan not japanese yuan. they use the same symbol.\n\nnow if you live there normally those prices are decent. the mi100 is around 900-1100 in canadian dollars when i calculated to exchange to my local dollar. where as on ebay its listed for $1300 cad\n\n5800 yuan is around 829$ USD. the mi100 on ebay is 984$ USD. not a huge difference i guess",
          "score": 4,
          "created_utc": "2026-01-02 12:09:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8qwcx",
              "author": "notafakename10",
              "text": "We're already here! \n\nYeh, I'm assuming those prices can be negotiated (like everything in china) so i'd expect to get below particularly if I'm buying 2x or 4x",
              "score": 2,
              "created_utc": "2026-01-02 12:21:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx93pep",
                  "author": "xrailgun",
                  "text": "In my experience, there's not much room (if any) for negotiation on these, especially towards the lower end of those scales you've compiled. They might round you down Â¥10-Â¥50 if you buy 2 or more. They're moving sufficient volume to price-insensitive customers, there are frequently weeks where some of these are out of stock/pre-allocated entirely. Doesn't hurt to ask though, especially if it's towards the higher half of the scale.",
                  "score": 9,
                  "created_utc": "2026-01-02 13:48:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx96pus",
          "author": "mp3m4k3r",
          "text": "Iirc the Pascal line is getting older/being dropped from upcoming versions for support so while tempting they might be worth weighting against or dropping.",
          "score": 3,
          "created_utc": "2026-01-02 14:06:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9p3y1",
          "author": "Terrible-Contract298",
          "text": "The Tesla P40 can be modified with a 8+6pin arrangement. A standard 65% PWM cycle on the fan of a standard 1080 TI cooler applied allows sustained operation at lower clocks and a TDP of \\~225W. I can provide definite performance and data regarding the upgrade.",
          "score": 3,
          "created_utc": "2026-01-02 15:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxadg6y",
              "author": "mtbMo",
              "text": "Yes sir. Build my with a 980ti founders cooler. Runs under 80c",
              "score": 2,
              "created_utc": "2026-01-02 17:39:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbz7mi",
                  "author": "Terrible-Contract298",
                  "text": "Fantastic to hear others having success with this.",
                  "score": 1,
                  "created_utc": "2026-01-02 22:14:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxb8w54",
          "author": "fallingdowndizzyvr",
          "text": "At your budget, hands down 4090(D) 48GB if the best choice.",
          "score": 3,
          "created_utc": "2026-01-02 20:06:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxcol9m",
          "author": "Disposable110",
          "text": "4080 32GB mod for around 9-10k CNY each caught my eye on Taobao, but beware that it is slower and can run hot.",
          "score": 2,
          "created_utc": "2026-01-03 00:31:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxd4149",
          "author": "1427538609",
          "text": "Can't help with Shenzhen prices specifically, but on the hardware side - dual P40s (48GB) or the modded 3080 20GB x4 route both work for 48GB+. The 3080s will be much faster for inference due to Ampere vs Pascal. For 96GB, you're looking at A100s or multiple cards regardless.",
          "score": 2,
          "created_utc": "2026-01-03 01:59:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9j45y",
          "author": "a_beautiful_rhind",
          "text": "First I hear that AMD has mod. 4080 32g seems like a contender if you can't afford 4090/5xxx gpu.",
          "score": 1,
          "created_utc": "2026-01-02 15:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa8ytw",
          "author": "jinnyjuice",
          "text": "Would be nice to have some comparisons with Yongsan in Korea also, also for RAM. I wonder if there is such data. Would be amazing if someone is familiar with all the international forums.\n\n>Prices are best estimates from deep seek\n\nThat is very, very unreliable. They're much more often wrong than correct. Manual search/scraping is definitely required and recommended.",
          "score": 1,
          "created_utc": "2026-01-02 17:17:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdl6pd",
              "author": "notafakename10",
              "text": "Agree it would be great to compare - pricing data was a mix of several things, deepseek, forums and my own observations so far, I'd put it as \"directional useful\" not exactly accurate lol",
              "score": 1,
              "created_utc": "2026-01-03 03:41:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxa9b22",
          "author": "Mediocre-Waltz6792",
          "text": "Get two cards with the Vram you want. Im currently fighting with my 3rd 3090 as it slowed everything down by 3x. Could be a windows issue but either way two GPU is way easier to setup and run.",
          "score": 1,
          "created_utc": "2026-01-02 17:19:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdl8qi",
              "author": "notafakename10",
              "text": "Good pointer - thanks for the insight",
              "score": 1,
              "created_utc": "2026-01-03 03:42:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbvxme",
          "author": "etherd0t",
          "text": "Just out of curiosity... where'd you get that price list?ðŸ™‚ Is there a place where you can check Shenzen bazaar (Huaqiangbei) prices? May visit soon...",
          "score": 1,
          "created_utc": "2026-01-02 21:58:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdldzm",
              "author": "notafakename10",
              "text": "Pricing data was a mix of several things, deepseek, forums and my own observations so far (from the Shanghai GPU markets), I'd put it as \"directional useful\" not exactly accurate lol\n\nI'll be going to Shenzhen later this month and I'll get some actual on the ground pricing and update",
              "score": 1,
              "created_utc": "2026-01-03 03:43:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdfjeh",
          "author": "BrightComplaint8342",
          "text": "est price not accurate i think",
          "score": 1,
          "created_utc": "2026-01-03 03:07:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9l8a4",
          "author": "pCute_SC2",
          "text": "Wait there is a RTX 3090 48GB mod????? Where can I get it?",
          "score": 1,
          "created_utc": "2026-01-02 15:25:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa7emj",
              "author": "sillynoobhorse",
              "text": "Alibaba, TaoBao etc., be prepared for the chinese way of dealing which includes many chat and Whatsapp messages lol",
              "score": 1,
              "created_utc": "2026-01-02 17:10:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxa7zds",
                  "author": "pCute_SC2",
                  "text": "Didn't found it on Alibaba.",
                  "score": 2,
                  "created_utc": "2026-01-02 17:13:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxdl04d",
              "author": "notafakename10",
              "text": "They are hard to come by now as all most of the modders have moved on to 4xxx series cards, but they do float around occasionally in the markets themselves, won't come up on Alibaba!",
              "score": 1,
              "created_utc": "2026-01-03 03:40:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxa4sws",
          "author": "grzesi00",
          "text": "7900 xt has 24gb vram stock so whats modded there?",
          "score": 1,
          "created_utc": "2026-01-02 16:58:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxad1dk",
              "author": "mftrhu",
              "text": "The 7900XTX has 24 GB VRAM. The 7900XT sits at 20 GB.",
              "score": 4,
              "created_utc": "2026-01-02 17:37:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pullo0",
      "title": "Hmm all reference to open-sourcing has been removed for Minimax M2.1...",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pullo0/hmm_all_reference_to_opensourcing_has_been/",
      "author": "Responsible_Fig_1271",
      "created_utc": "2025-12-24 11:48:37",
      "score": 243,
      "num_comments": 93,
      "upvote_ratio": 0.93,
      "text": "Funny how yesterday this page [https://www.minimax.io/news/minimax-m21](https://www.minimax.io/news/minimax-m21) had a statement that weights would be open-sourced on Huggingface and even a discussion of how to run locally on vLLM and SGLang. There was even a (broken but soon to be functional) HF link for the repo...\n\n  \nToday that's all gone.\n\n  \nHas MiniMax decided to go API only? Seems like they've backtracked on open-sourcing this one. Maybe they realized it's so good that it's time to make some $$$ :( Would be sad news for this community and a black mark against MiniMax.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pullo0/hmm_all_reference_to_opensourcing_has_been/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvq0le0",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-24 14:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpj0j7",
          "author": "Wise_Evidence9973",
          "text": "For u Christmas gift, bro",
          "score": 124,
          "created_utc": "2025-12-24 12:40:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpj3in",
              "author": "Wise_Evidence9973",
              "text": "Tomorrow",
              "score": 65,
              "created_utc": "2025-12-24 12:40:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvpxst7",
                  "author": "____vladrad",
                  "text": "Thank you.",
                  "score": 21,
                  "created_utc": "2025-12-24 14:18:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyxrzw",
                  "author": "zmarty",
                  "text": "Christmas is over. Where are the weights, BRO?",
                  "score": 0,
                  "created_utc": "2025-12-26 03:36:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyz7dh",
                  "author": "Responsible_Fig_1271",
                  "text": "\"Tomorrow\" has come and gone.",
                  "score": 0,
                  "created_utc": "2025-12-26 03:46:54",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvpjaxv",
          "author": "espadrine",
          "text": "They've shown goodwill in the past. My policy is to assume they'll do the right thing if they have a history of doing the right thing.\n\nBesides the article still mentions opening  the weights:\n\n> [M2.1 is] one of the first open-source model series to systematically introduce Interleaved Thinking\n\n> We're excited for powerful open-source models like M2.1",
          "score": 35,
          "created_utc": "2025-12-24 12:42:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpfexc",
          "author": "Only_Situation_4713",
          "text": "Head of research on twitter said on Christmas so itâ€™s still open source",
          "score": 27,
          "created_utc": "2025-12-24 12:11:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpdwx4",
          "author": "SlowFail2433",
          "text": "Idk if its worth speculating, what drops drops\n\n\nSomeone posted an article yesterday about z.ai and minimax having money troubles",
          "score": 49,
          "created_utc": "2025-12-24 11:58:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpj6u8",
              "author": "Wise_Evidence9973",
              "text": "Will release soon. MiniMax does not have money trouble.",
              "score": 99,
              "created_utc": "2025-12-24 12:41:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvpk272",
                  "author": "No_Conversation9561",
                  "text": "Everyone listen to this personðŸ‘†\n\nTheyâ€™re from Minimax.",
                  "score": 51,
                  "created_utc": "2025-12-24 12:48:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvq31ea",
                  "author": "tarruda",
                  "text": "Thank you. Minimax M2 is amazing, looking forward to trying M2.1 on my mac.",
                  "score": 16,
                  "created_utc": "2025-12-24 14:49:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvplsm0",
                  "author": "Leflakk",
                  "text": "Glad to hear your not in money trouble",
                  "score": 24,
                  "created_utc": "2025-12-24 13:00:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvpkby7",
                  "author": "SlowFail2433",
                  "text": "Wow thanks thatâ€™s great to hear. I am a huge fan of your models and papers, especially the RL stuff.",
                  "score": 8,
                  "created_utc": "2025-12-24 12:50:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvpkw7b",
                  "author": "NaiRogers",
                  "text": "Thank you",
                  "score": 6,
                  "created_utc": "2025-12-24 12:54:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvu05ir",
                  "author": "RedParaglider",
                  "text": "Minimax is a very nice feeling tool to use.  I still have 30 dollars of API usage I bought on the last release, I need to play with the new model some :)",
                  "score": 2,
                  "created_utc": "2025-12-25 05:59:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvq0b61",
                  "author": "power97992",
                  "text": "Please make a smaller <100b model with great performance like deepseek v3.2 speciale and minimax 2.1. Keep making efficient high quality smaller models even if deepseek releases a  +1.8Trillion parameter model...",
                  "score": -2,
                  "created_utc": "2025-12-24 14:33:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvpzs1z",
              "author": "FullOf_Bad_Ideas",
              "text": "They have some runway but R&D costs are 3x higher than revenue for Minimax and 8x higher for Zhipu.\n\nYou can read more here (translate it with your preferred method) \n\nZhipu: https://wallstreetcn.com/articles/3761776\n\nMinimax: https://wallstreetcn.com/articles/3761823",
              "score": 8,
              "created_utc": "2025-12-24 14:30:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvpgin3",
          "author": "j_osb",
          "text": "I mean, that's what always happens, no?\n\nQwen (with Max). Once their big models get good enough, there'll be no reason to release smaller ones for the public. Like they did with Wan, for example.\n\nOr this. Or what tencent does.\n\nOpen source/weights only gets new models until they're good enough, at which point all the work the open source community has done for them is just 'free work' for them and they continue closing their models.",
          "score": 10,
          "created_utc": "2025-12-24 12:20:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqyko8",
              "author": "RhubarbSimilar1683",
              "text": "For those who don't know, wan 2.5 is competitive with Google's veo 3 and thus remains closed source unlike earlier wan versions and hunyuan 3d 2.5 is closed source but earlier versions are open sourceÂ ",
              "score": 6,
              "created_utc": "2025-12-24 17:40:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzwtb9",
              "author": "I-am_Sleepy",
              "text": "https://huggingface.co/MiniMaxAI/MiniMax-M2.1/tree/main",
              "score": 1,
              "created_utc": "2025-12-26 08:41:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvpl2oi",
              "author": "power97992",
              "text": "If open weights become so good, why dont they just sell the model with the inference engine and scaffolding  as a stand alone program , ofc people can jail break it, but that requires effort",
              "score": 2,
              "created_utc": "2025-12-24 12:55:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvpnpds",
                  "author": "SlowFail2433",
                  "text": "It would get decompiled",
                  "score": 7,
                  "created_utc": "2025-12-24 13:14:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvpzxp1",
                  "author": "j_osb",
                  "text": "If they would do that, the model files would need to be on your computer. Even IF they were somehow decrypted, the key for that would always be findable.\n\nErgo, you could easily run it locally, for free. Not what they want.",
                  "score": 1,
                  "created_utc": "2025-12-24 14:31:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvri1gl",
          "author": "fooo12gh",
          "text": "I really hope that at some point in time there will be open weight model trained by completely independent, community driven organisation (which OpenAI probably intended to be in the 1st place). Something like Free Software Foundation, but in the world of LLM. So that community of people doesn't depend on the financial plans of private companies.",
          "score": 3,
          "created_utc": "2025-12-24 19:26:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt8nax",
          "author": "complains_constantly",
          "text": "God you guys are fucking paranoid.\n\n\nObviously the lab that has open-weighted every model they've ever made, and has said this week they're going to open-weight their latest model, is going to open-weight their latest model. Lmao. They're probably rewriting their blog release or something.",
          "score": 3,
          "created_utc": "2025-12-25 02:17:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvkkhm",
          "author": "LegacyRemaster",
          "text": "https://preview.redd.it/q1zhy6aj6d9g1.png?width=2063&format=png&auto=webp&s=af98c2d7722bab43a8ecbd800d1ed9f89c62947b\n\n[https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/kt-kernel/MiniMax-M2.1-Tutorial.md](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/kt-kernel/MiniMax-M2.1-Tutorial.md) Incoming",
          "score": 3,
          "created_utc": "2025-12-25 14:56:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvq2rnm",
          "author": "tarruda",
          "text": "Would be a shame if they don't open source it. GLM 4.7V is too big for 128GB Macs, but Minimax M2 can fit with a IQ4_XS quant",
          "score": 5,
          "created_utc": "2025-12-24 14:47:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrd4in",
              "author": "Its_Powerful_Bonus",
              "text": "GLM 4.7 Q2 works on Mac 128gb quite well ðŸ˜‰ Tested just for few queries, but it was very usable",
              "score": 2,
              "created_utc": "2025-12-24 18:58:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvj7st",
                  "author": "tarruda",
                  "text": "I ended up trying UD-IQ2_M quant and it seems to give pretty close results to what you get in chat.z.ai.\n\nMy mind is blown by how much of the original quality is kept by these super small quants.",
                  "score": 3,
                  "created_utc": "2025-12-25 14:47:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvrhlw2",
                  "author": "tarruda",
                  "text": "Interesting!\n\nDid you use unsloth dynamic quant? How much memory did it use and how much context could you fit?",
                  "score": 1,
                  "created_utc": "2025-12-24 19:23:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvr1liw",
          "author": "LeTanLoc98",
          "text": "Honestly, it would be great if they released the weights, but if not, that's totally fine as well.\n\n\nOpen-source models are already very strong.\n\n\nWe now have DeepSeek v3.2, GLM-4.7, and Kimi K2 Thinking.\n\n\nThese models are largely on par with each other, none of them is clearly superior.",
          "score": 2,
          "created_utc": "2025-12-24 17:56:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuxhg9",
          "author": "Impressive_Chain6039",
          "text": "https://huggingface.co/MiniMaxAI/MiniMax-M2/commit/f7804c9c48d5ee2bdaa89db34a6337bba02b0f40 2.1 incoming",
          "score": 2,
          "created_utc": "2025-12-25 11:49:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpd3en",
          "author": "Tall-Ad-7742",
          "text": "i hope not ðŸ™  \nthat would be a war crime for me tbh",
          "score": 6,
          "created_utc": "2025-12-24 11:51:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpdz4j",
              "author": "SlowFail2433",
              "text": "Open source community be normal challenge",
              "score": 40,
              "created_utc": "2025-12-24 11:58:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvplal9",
                  "author": "datbackup",
                  "text": "Lmao",
                  "score": 2,
                  "created_utc": "2025-12-24 12:57:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvpd74v",
              "author": "Responsible_Fig_1271",
              "text": "For me as well!",
              "score": 1,
              "created_utc": "2025-12-24 11:52:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvqwpo5",
              "author": "colei_canis",
              "text": "Theyâ€™re going to use the model to mistreat prisoners of war in an active conflict?",
              "score": 1,
              "created_utc": "2025-12-24 17:30:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvuve6x",
                  "author": "Tall-Ad-7742",
                  "text": "xD",
                  "score": 1,
                  "created_utc": "2025-12-25 11:27:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvpmmrv",
          "author": "xenydactyl",
          "text": "They still kept the comment of Eno Reyes (Co-Founder, CTO of Factory AI) in: \"We're excited for powerful **open-source** models like **M2.1** that bring frontier performance...\"",
          "score": 2,
          "created_utc": "2025-12-24 13:06:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqbv6g",
          "author": "SilentLennie",
          "text": "Or maybe they discovered some problems and don't know when it will be released.",
          "score": 2,
          "created_utc": "2025-12-24 15:38:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqnlqb",
          "author": "KvAk_AKPlaysYT",
          "text": "Even if they are going to OS it, why remove it from the website overnight :(\n\nEverybody, join your hands together and chant GGUF wen.",
          "score": 2,
          "created_utc": "2025-12-24 16:40:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpglar",
          "author": "__Maximum__",
          "text": "The model seems to be very good at some tasks, so this could have been their chance to stand out. I still hope they do open weight it for their own sake.",
          "score": 2,
          "created_utc": "2025-12-24 12:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpk2zl",
          "author": "jacek2023",
          "text": "Let's wait for \"let them cook, you should be grateful, they owe you nothing\" redditors",
          "score": -1,
          "created_utc": "2025-12-24 12:48:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpp8ra",
              "author": "oxygen_addiction",
              "text": "That's literally the case. They said they will release it tomorrow even in this thread. You are just being ungrateful children, acting as if the world owes you something.",
              "score": 10,
              "created_utc": "2025-12-24 13:24:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvpqhw1",
                  "author": "SlowFail2433",
                  "text": "This isnâ€™t how open source works\n\n\nOpen source is like a common public good, which we all both contribute to and consume. Encouraging more open source releases isnâ€™t entitlement it is fostering a culture and environment where people and organisations do open source releases that are mutually beneficial, to both the users and releaser.",
                  "score": 9,
                  "created_utc": "2025-12-24 13:32:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvpr6sb",
                  "author": "jacek2023",
                  "text": "...and here they are",
                  "score": -1,
                  "created_utc": "2025-12-24 13:37:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvr0a3b",
          "author": "jreoka1",
          "text": "I'm pretty sure they plan on putting it back on HF according to the person here from the Minimax team.",
          "score": 1,
          "created_utc": "2025-12-24 17:49:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvrlv4e",
          "author": "AllegedlyElJeffe",
          "text": "a) the makers have sat here in the comments that theyâ€™re still putting it out probably tomorrow.\n\n\nb) people are not required to give away for free something they worked really hard on. Itâ€™s awesome and we all love it, but theyâ€™re not doing the wrong thingâ€ if they decide to sell the product of their work instead. Iâ€™m not saying open source isnâ€™t better. Iâ€™m just saying that people are not being unethical or anything when they donâ€™t open source stuff.",
          "score": 1,
          "created_utc": "2025-12-24 19:48:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvu9jq6",
          "author": "power97992",
          "text": "Where is the release lol?Â ",
          "score": 1,
          "created_utc": "2025-12-25 07:30:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwhisz",
          "author": "power97992",
          "text": "Weights?",
          "score": 1,
          "created_utc": "2025-12-25 18:16:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvyz9rm",
              "author": "Responsible_Fig_1271",
              "text": "Seems like a big bucket of fail.",
              "score": 1,
              "created_utc": "2025-12-26 03:47:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvzmvkn",
                  "author": "power97992",
                  "text": "Yeah, still no weights",
                  "score": 1,
                  "created_utc": "2025-12-26 07:00:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzylcx",
          "author": "Wise_Evidence9973",
          "text": "Merry Christmas!   \n[https://huggingface.co/MiniMaxAI/MiniMax-M2.1](https://huggingface.co/MiniMaxAI/MiniMax-M2.1)",
          "score": 1,
          "created_utc": "2025-12-26 09:00:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0vo8u",
              "author": "Responsible_Fig_1271",
              "text": "Thank you! I stand corrected!",
              "score": 2,
              "created_utc": "2025-12-26 13:58:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvr5xv3",
          "author": "Southern_Sun_2106",
          "text": "It's GLM 4.5 Air all over again.",
          "score": 1,
          "created_utc": "2025-12-24 18:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpmbhw",
          "author": "Majestic_Appeal5280",
          "text": "the official minimax on twitter said they will be open sourcing in 2 days. probably on Xmas?",
          "score": 0,
          "created_utc": "2025-12-24 13:04:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvponxy",
          "author": "HumanDrone8721",
          "text": "Things may or may not happen, my 24TB HDD is slowly filling up and then *\"Molon Labe\"*.",
          "score": -1,
          "created_utc": "2025-12-24 13:20:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpew2l",
          "author": "Cergorach",
          "text": "Maybe they used a LLM to generate the website texts and it gave some unwanted output... ;)",
          "score": -6,
          "created_utc": "2025-12-24 12:06:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpjeme",
          "author": "SelectionCalm70",
          "text": "Nothing wrong in making money",
          "score": -7,
          "created_utc": "2025-12-24 12:43:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpojkq",
          "author": "LegacyRemaster",
          "text": "https://preview.redd.it/6n9wzr2fk59g1.png?width=2927&format=png&auto=webp&s=c35aa063c179e2a5f1e9be23496f3385df958f32\n\ncan't wait",
          "score": -7,
          "created_utc": "2025-12-24 13:19:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpfggf",
          "author": "AlwaysLateToThaParty",
          "text": "Maybe they think the chip shortage is going to bite local inference, and increase the number of people who will require cloud services.",
          "score": -8,
          "created_utc": "2025-12-24 12:11:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pv2cnz",
      "title": "All of the major open weight labs have shifted to large params general models instead of smaller, more focused models. By this time next year, there wonâ€™t be much â€œlocalâ€ about this sub unless the paradigm shifts to smaller models good at specific domains.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pv2cnz/all_of_the_major_open_weight_labs_have_shifted_to/",
      "author": "LocoMod",
      "created_utc": "2025-12-25 01:34:36",
      "score": 239,
      "num_comments": 242,
      "upvote_ratio": 0.8,
      "text": "Itâ€™s happening very openly but very subtly. The champions of open weight models are slowly increasing their sizes to the point a very small portion of this sub can run them locally. An even smaller portion can run them as benchmarked (no quants). Many are now having to resort to Q3 and below, which will have a significant impact compared to what is marketed. Now, without any other recourse, those that cannot access or afford the more capable closed models are paying pennies for open weight models hosted by the labs themselves. This is the plan of course.\n\nGiven the cost of memory and other components many of us can no longer afford even a mid tier upgrade using modern components. The second hand market isnâ€™t fairing much better.\n\nThe only viable way forward for local tinkerers are models that can fit between 16 to 32GB of vram.\n\nThe only way most of us will be able to run models locally will be to fine tune, crowd fund, or â€¦ ? smaller more focused models that can still remain competitive in specific domains vs general frontier models.\n\nA capable coding model. A capable creative writing model. A capable math model. Etc.\n\nWeâ€™re not going to get competitive local models from â€œwell fundedâ€ labs backed by Big Co. A distinction will soon become clear that â€œopen weightsâ€ does not equal â€œlocalâ€.\n\nRemember the early days? Dolphin, Hermes, etc.\n\nWe need to go back to that.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pv2cnz/all_of_the_major_open_weight_labs_have_shifted_to/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvt6bz4",
          "author": "Freonr2",
          "text": "Did you miss Qwen3?  They produced about half dozen models between 0.6B and 32B, and there are countless quant options.  They're great models for their size.",
          "score": 138,
          "created_utc": "2025-12-25 01:59:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtukif",
              "author": "Amgadoz",
              "text": "Gemma as well, whose biggest model is 27B",
              "score": 24,
              "created_utc": "2025-12-25 05:09:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvni0v",
                  "author": "LocoMod",
                  "text": "That model was released when dinosaurs were still alive in AI terms. It was definitely a banger for its time though.",
                  "score": 2,
                  "created_utc": "2025-12-25 15:15:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvuavk1",
              "author": "power97992",
              "text": "U cant compare an 8b or 30b a3b Â  model to the top Â open models like v3.2 and kimi k2 â€¦ It is night and dayÂ ",
              "score": -10,
              "created_utc": "2025-12-25 07:44:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvufxwk",
                  "author": "fozid",
                  "text": "Nobody is trying to",
                  "score": 18,
                  "created_utc": "2025-12-25 08:38:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvvknp3",
                  "author": "reginakinhi",
                  "text": "That's kind of the point...",
                  "score": 2,
                  "created_utc": "2025-12-25 14:56:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvtastb",
          "author": "MerePotato",
          "text": "Mistral literally just dropped a family of models capping out at 14b",
          "score": 161,
          "created_utc": "2025-12-25 02:33:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtdv81",
              "author": "ttkciar",
              "text": "Yep, exactly this.  They're not the only ones, either.\n\n* Google released Gemma3 in 12B and 270M (!!) sizes,\n\n* Qwen released Qwen3 in 0.6B, 1.7B, 4B, 8B, and 14B,\n\n* Microsoft released Phi-4 in 4B and 14B,\n\n* AllenAI released Molmo2 in 4B and 8B,\n\n* Nvidia released Nemotron-Cascade in 8B and 14B,\n\n* like you said, Mistral released Ministral 3 in 3B, 8B, and 14B.\n\n.. and there are probably others I'm forgetting.\n\nThat having been said, we shouldn't conflate the **current state** of the technology with **trends** in the technology, which is a frequent peeve of mine.  Maybe even though ample small models are being released *now*, the *trend* is toward larger models?  But I'm not sure if that's even the case.\n\nFiguring out if there is such a trend would require curve-fitting the relationship of small models published in a given year, or something, which would be straightforward enough except I don't know how to easily extract the raw data from Huggingface, and I'm too lazy right now to do it the hard way, **and** that would only give us a few points from which to extrapolate, since this entire scene is only a few years old.\n\nOverall, though, I'm not too worried.  Even if reasonable-sized models start to get scarce, the community has ample means to downscale weights of larger models, upscale toy-sized models we are able to train from scratch, and/or retrain older small models.  Besides, I'm expecting AI Winter to come crashing down in 2027 or 2028 anyway.",
              "score": 99,
              "created_utc": "2025-12-25 02:56:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvux6wi",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 10,
                  "created_utc": "2025-12-25 11:46:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvuqk47",
                  "author": "Hoblywobblesworth",
                  "text": "I'm excited for the AI winter if it means GPU and RAM prices stop increasing. Prices are going to come down right?....right? ...",
                  "score": 5,
                  "created_utc": "2025-12-25 10:35:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvv6ryb",
                  "author": "ramendik",
                  "text": "Yup, you forgot IBM. Granite 4 tops out at 32B A9B and is pretty capable. I hope someone finally does a head-to-head with the new Nemotron as the architectures are similar (Mamba2 hybrid)",
                  "score": 5,
                  "created_utc": "2025-12-25 13:14:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvvpidf",
                  "author": "LocoMod",
                  "text": "You clearly missed the point. None of those models are competitive. Yes they can do simple things and raise the floor for a lot of people. But you are FAR from the ceiling. This is why the entire point of my post is that when you look at the recent releases, you clearly see a trend where params are getting larger and larger.\n\nGoogle and Mistral are not going to release a small model that has parity with their paid offerings in relevant use cases. Even if they could. That would significantly cut demand for their paid services.\n\nThe Chinese labs are now following the same playbook. Qwen, DeepSeek and GLM all released general >100B param models while concurrently bootstrapping their paid API services. Throw in a CLI coding agent in there as well. This was always the plan.\n\nNo one here is going to release anything of value on the models you listed. They are toys. None of those models are suitable for a production environment with paid users.\n\nI love playing with them though.",
                  "score": 7,
                  "created_utc": "2025-12-25 15:28:11",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nvvx6tr",
                  "author": "AuspiciousApple",
                  "text": "Is the 270M any good?",
                  "score": 1,
                  "created_utc": "2025-12-25 16:15:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvujgek",
              "author": "Rainbows4Blood",
              "text": "Wait, there are new Mistral models?",
              "score": 2,
              "created_utc": "2025-12-25 09:17:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvuq42n",
                  "author": "MerePotato",
                  "text": "Yup, there's a new line of Ministral models",
                  "score": 3,
                  "created_utc": "2025-12-25 10:30:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw17swz",
              "author": "power97992",
              "text": "It seems like Mistral models are way worse than minimax and qwenâ€¦ if open weight companies only release large models and Â  dont release a sub 110b model Â that is 3-5 months behind the top models , there is no point even upgrading a computer for that cost , you are better off just using the apiâ€¦",
              "score": 0,
              "created_utc": "2025-12-26 15:13:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1a9ge",
                  "author": "MerePotato",
                  "text": "Mistral models don't push the frontier but they're consistently much less sloppy and censored than US and CN releases making them a lot more pleasant to actually talk to and use",
                  "score": 2,
                  "created_utc": "2025-12-26 15:27:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvtcisp",
              "author": "LocoMod",
              "text": "That excel at nothing. They dangled a carrot so you can pay for the real model via their API.",
              "score": -36,
              "created_utc": "2025-12-25 02:46:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvtensr",
                  "author": "MerePotato",
                  "text": "They're vastly superior to the \"real model\" in terms of day to day usability imo, and that \"real model\" is also open weights",
                  "score": 18,
                  "created_utc": "2025-12-25 03:02:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvtjlg9",
                  "author": "Monkey_1505",
                  "text": "No this is the wrong analysis. Nobody is sure if, in the long term, cloud API access will be the dominant form of AI or not. Smaller models have been accelerating fast, and local hardware improving. Making small models and continuing to is a hedge in case cloud ends up being more niche than expected.\n\nMost modern models, even very large API based propriety AI, are not really products yet. In that they don't actually make profit. They are tech demos as people aim toward some future thing. This is why people do local, because no one can predict with certainty the exact course of the future of technology, and companies want to have fingers in all pies, so they don't get caught with their pants down.",
                  "score": 11,
                  "created_utc": "2025-12-25 03:41:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt7k0i",
          "author": "wolttam",
          "text": "There's gonna continue to be interest in developing generalist models that can run on the smallest devices possible (phones).",
          "score": 21,
          "created_utc": "2025-12-25 02:09:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwq7v8",
              "author": "hoshitoshi",
              "text": "Also don't discount demand from enterprises who for both data sovereignty and cost reasons (100s of billions of tokens per week) will be very interested in local models.",
              "score": 2,
              "created_utc": "2025-12-25 19:06:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvx3bap",
                  "author": "jklre",
                  "text": "The token tax is real. The clouds quad dipping on token charges for large embeddings for documents. Tokens to embed, storage fees, input tokens + embedded tokens and output tokens.",
                  "score": 1,
                  "created_utc": "2025-12-25 20:24:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt3t9l",
          "author": "StardockEngineer",
          "text": "â€œWeâ€ arenâ€™t getting back to anything.  Weâ€™ve been completely at the mercy of these companies this whole time.   How do you propose we do anything without them?",
          "score": 82,
          "created_utc": "2025-12-25 01:41:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtz3mq",
              "author": "ttkciar",
              "text": "We definitely have options.  I wrote something about some of the ways we can advance open-weight models without corporate help here -- https://old.reddit.com/r/LocalLLaMA/comments/1os1qf1/debate_16gb_is_the_sweet_spot_for_running_local/nnw33r0/",
              "score": 5,
              "created_utc": "2025-12-25 05:49:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtjkul",
              "author": "grady_vuckovic",
              "text": "Almost as if making oneself dependent on AI tools makes oneself dependent on tech companies.",
              "score": 5,
              "created_utc": "2025-12-25 03:41:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvt56q0",
              "author": "LocoMod",
              "text": "It all starts with the data. Thatâ€™s the real moat. Shifting focus to building open source tools that can build the datasets and making those datasets easier to discover is a start.\n\nFunding and training a 32B model is the lesser problem.\n\nEasier said than done of course.",
              "score": -23,
              "created_utc": "2025-12-25 01:51:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvt5jey",
                  "author": "StardockEngineer",
                  "text": "No, it takes a LOT of power to even train a 32B model.  And it takes a ton of storage to store the data.  Both will be extremely expensive.  \n\nYouâ€™re talking an absurd amount of money per training run (and then subsequent evaluations).  And itâ€™ll take many runs to dial it in.",
                  "score": 45,
                  "created_utc": "2025-12-25 01:53:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvt6som",
                  "author": "-p-e-w-",
                  "text": "> It all starts with the data. Thatâ€™s the real moat.\n\nNo, the moat absolutely is compute.\n\nIIRC, Phi-4 (14 billion parameters) cost around $2 million to train. Thatâ€™s one small model, and it would be a moonshot for a global crowdfunding project.",
                  "score": 28,
                  "created_utc": "2025-12-25 02:03:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvt4iv3",
              "author": "AppealSame4367",
              "text": "You can rent a gpu server for 200-400$ per month somewhere and train your own model. That's what you can do.",
              "score": -17,
              "created_utc": "2025-12-25 01:46:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvt4yd8",
                  "author": "StardockEngineer",
                  "text": "Somewhere?  A good model?  If itâ€™s so easy, where are all these models?  How long does it take?  Where do you put the training data?   Come on.",
                  "score": 20,
                  "created_utc": "2025-12-25 01:49:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvt78o2",
                  "author": "-p-e-w-",
                  "text": "An A100 server costs around $1.50/hour (more than your estimate) and it would take years to train even a small model with it.",
                  "score": 12,
                  "created_utc": "2025-12-25 02:06:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt5hk1",
          "author": "quiteconfused1",
          "text": "Functiongemma was literally released last week.\n\nllama, Kimi, mistral, GLM, Qwen, Gemma, GPT-OSS all had major improvements this past [year.Like](http://year.Like) seriously; I use local models more than i use \"big models\". Infact im and training a gpt-oss-120b right now. \n\nNext year is going to the be the year of the humanoid foundational model.   \n  \nlocals arent going anywhere ...",
          "score": 144,
          "created_utc": "2025-12-25 01:53:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvto49a",
              "author": "dolche93",
              "text": "You're implying that gpt-oss-120b is a small local model. OP explicitly is talking about models that can fit on single or double GPU builds.\n\n I'm not sure the two of you are on the same page about what a small model is. Not that either of you are wrong, just not on the same page.",
              "score": 41,
              "created_utc": "2025-12-25 04:17:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtqg1i",
                  "author": "Veloder",
                  "text": "It's a MoE model, it can fit fine in a consumer GPU (16GB VRAM is enough) as long as the system has enough RAM (about 64GB).",
                  "score": 27,
                  "created_utc": "2025-12-25 04:36:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvv5gzu",
                  "author": "quiteconfused1",
                  "text": "To answer your question about if gpt-oss-120b is small or not. No it's not small, but I am still training it on a single computer. Namely the Thor.\n\n128gb of unified ram.\n\nIf you can get it training on a machine that is 2800 bucks ( right now on sale ) ... That counts.",
                  "score": 2,
                  "created_utc": "2025-12-25 13:03:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvwu5bk",
                  "author": "mycall",
                  "text": "I have gpt-oss-120b running on my gpd win 5 (40tk/s).  You can't get smaller than that.   \n\nWhat people are really talking about is affordability.",
                  "score": 2,
                  "created_utc": "2025-12-25 19:29:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt57j5",
          "author": "robberviet",
          "text": "SLM is always needed, especially for mobile and local simple usage like tab completion. However it is totally depends on the big tech to release them or not. My opinion is yes, they will. OSS always exists. It costs big tech nothing to do that.",
          "score": 10,
          "created_utc": "2025-12-25 01:51:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt852c",
          "author": "complains_constantly",
          "text": "We will do great because of downstream distillation, which has become the dominant meta. Distilling from a larger model (which we are getting in spades thanks to DeepSeek, Qwen, Z.ai, Minimax, Moonshot, etc) has been shown to be significantly more powerful than training a small model from scratch. So much so that the latter idea has been abandoned by any organization serious about this stuff.",
          "score": 18,
          "created_utc": "2025-12-25 02:13:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvw3h30",
              "author": "milkipedia",
              "text": "I'm specifically looking for more distills from these new big models. If I had any idea how to do this, I'd do it myself.",
              "score": 1,
              "created_utc": "2025-12-25 16:52:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwe7jn",
                  "author": "complains_constantly",
                  "text": "It's pretty difficult and expensive to do yourself. Doing so is out of reach of us consumers, but its an order of magnitude or two cheaper than training from scratch for the labs. They're pretty incentivized to train models this way.",
                  "score": 1,
                  "created_utc": "2025-12-25 17:56:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvu8mbv",
          "author": "beedunc",
          "text": "And just in time for RAM to be impossible to buy.",
          "score": 6,
          "created_utc": "2025-12-25 07:20:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt5f5p",
          "author": "1ncehost",
          "text": "The reason is the latest techniques make it easy for anyone to train from scratch a decent specialized model. Im not even talking fine tuning, Im talking the whole shebang.\n\nNanogpt speed runs are down to under 3 minutes and under $10 all in from scratch to 3.2 loss on fineweb. If you're training a specialized model you can get into the 1.X loss in barely any time now.\n\nSimply put there is no business model here any longer for the models themselves. You have to make a specialized model as part of a larger specialized service now.",
          "score": 13,
          "created_utc": "2025-12-25 01:53:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvt8fxk",
              "author": "CKtalon",
              "text": "Nanogpt speedruns are cool, but the models produced arenâ€™t going to be useful (100+m decoder models arenâ€™t great, but 100+m encoder models are okay). The possibly interesting ones are the d32 (1B) nanochat models which are probably $800.",
              "score": 10,
              "created_utc": "2025-12-25 02:15:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvt8cji",
              "author": "Aggressive-Bother470",
              "text": "So the next paradigm is build and train it yourself?Â ",
              "score": 2,
              "created_utc": "2025-12-25 02:15:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtayc3",
          "author": "simism",
          "text": "Billions must scale",
          "score": 5,
          "created_utc": "2025-12-25 02:34:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwezj1",
          "author": "JacketHistorical2321",
          "text": "Large models are still local models dude. The sub isn't called, \"LocalModLLama\". If you or others can't run it local, it didn't mean some can.Â ",
          "score": 6,
          "created_utc": "2025-12-25 18:01:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtjef0",
          "author": "Monkey_1505",
          "text": "No, that isn't happening at all.  \n\nCompanies will not want to give up on local, it's effectively a hedged bet against big cloud APIs. Microsoft is doing it. Google is doing it. Qwen is going it. \n\nNow finetunes, yes that is happening a bit less. But it hasn't stopped either.",
          "score": 11,
          "created_utc": "2025-12-25 03:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvetoa",
              "author": "LocoMod",
              "text": "Thatâ€™s not what my post is about.",
              "score": 1,
              "created_utc": "2025-12-25 14:15:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvtxek",
                  "author": "Monkey_1505",
                  "text": "\"All of the major open weight labs have shifted to large params general models instead of smaller, more focused models.Â \"\n\nThey have not done this. Google, Qwen, Microsoft are still very focused on small models, and they are all surely major open weight labs, no?",
                  "score": 1,
                  "created_utc": "2025-12-25 15:55:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt6nns",
          "author": "YouAreTheCornhole",
          "text": "Everyone here is about to become a fan of Nemotron",
          "score": 22,
          "created_utc": "2025-12-25 02:02:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtfb43",
              "author": "JLeonsarmiento",
              "text": "Nemotron 30b feels very good in the chat, but I cannot make it work in any of my coding agents (cline, QwenCode, Vibe).\n\nAnd Iâ€™m not completely sure itâ€™s significantly better than gpt-oss 20b. According to benchmarks it should,,, but I just donâ€™t feel it.",
              "score": 13,
              "created_utc": "2025-12-25 03:07:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtgxvq",
                  "author": "RMCPhoto",
                  "text": "It's probably the prompting strategy.  I have no doubt it's very smart, but my results have also been inconsistent.   My guess is that it's the same old story.   The training data instills a certain syntax / language / prompt structure that differs from the norm slightly.  Could even be a very tiny variation that propagates an error.  Newer models have been more tolerant to this compared with the earlier llamas...where adding a space before the first word would increase the error by 40% and similar other black box ??? \n\nThis is honestly my biggest frustration.   I'm very thankful that openai released such clear cookbook content for prompt formatting.  Truly, every model designer should take note.  Clear documentation is such a massive booster for adoption, public opinion and end user success.  \n\nEven better if that documentation is instilled into a meta prompt for prompt refinement.",
                  "score": 12,
                  "created_utc": "2025-12-25 03:20:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvtolr1",
              "author": "Foreign-Beginning-49",
              "text": "Im still trying to figure out my params but its a beast.",
              "score": 0,
              "created_utc": "2025-12-25 04:21:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvt7w5r",
          "author": "sirfitzwilliamdarcy",
          "text": "We will get back to that. The process for people creating their own flavor of models just needs to be democratized. Weâ€™ve had heroes like TheBloke, NousResearch and many smaller contributors on hugging face who used to keep the community alive. But I still feel that there is a group of people who are hungry for diverse models that all have different vibes. And that demand will have to be met.",
          "score": 4,
          "created_utc": "2025-12-25 02:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtvwdi",
          "author": "BidWestern1056",
          "text": "i've been developing mainly tooling but have been working on some fine tunes. I've already released a couple more focused on divergent generation to help models come up with new ideas that are genuinely more novel\n\n[hf.co/npc-worldwide](http://hf.co/npc-worldwide)\n\nin the next few months i'm going to be focusing a bit more on some specialized local models so hoping to have more to share. building and training these using my [npcpy](https://github.com/npc-worldwide/npcpy) tools. gonna make a research coding model that doesnt overly comment or unnecessarily add exception handling, prolly one specialized for [npcsh](https://github.com/npc-worldwide/npcsh). i'm likely gonna make one for [lavanzaro.com](http://lavanzaro.com) (rn its just gem 2.5 flash) and in [npc studio](https://github.com/npc-worldwide/npc-studio) my intention is that it will be trivial for users to set up fine tunes for a given persona based on user-labeled data. \n\nI also write [fiction](https://www.amazon.com/Dont-turn-sun-giacomo-catanzaro/dp/B0DMWPGV18) so planning to make it easier to do more creative writing style clones",
          "score": 5,
          "created_utc": "2025-12-25 05:21:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvc759",
              "author": "LocoMod",
              "text": "Nice. Youâ€™re staying busy. Keep it up. ðŸ‘",
              "score": 2,
              "created_utc": "2025-12-25 13:56:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvgrjj",
                  "author": "BidWestern1056",
                  "text": "we gotta win ðŸ˜",
                  "score": 2,
                  "created_utc": "2025-12-25 14:29:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw2173r",
              "author": "gardenia856",
              "text": "The main point: your approach is exactly what this sub needs if local is going to stay relevant.\n\nDivergent-gen finetunes are underrated; most people chase benchmarks instead of â€œcan this model actually surprise me without going off the rails.â€ If you publish even rough evals on novelty vs. repetition for your [hf.co/npc-worldwide](http://hf.co/npc-worldwide) stuff, that alone would be super useful for others trying to clone the approach.\n\nFor the research coder: one trick that worked well for me was tagging training examples by â€œstyleâ€ (minimalist, verbose, safety-obsessed) and forcing the system prompt to pick a style token first. That way you can later extend into a safety-heavy variant without retraining everything. Same idea could map to your fiction clones: label passages by voice, pacing, and point of view so your fine-tunes can switch modes instead of hard-baking one persona.\n\nIf you ever wire these agents into real data, platforms like PostgREST or Hasura, plus something like DreamFactory for auto-REST over legacy SQL, make it easier to keep your models dumb about SQL but smart about the domain.\n\nThe main point: those small, weird, opinionated models are where local actually wins, so keep leaning into that niche.",
              "score": 1,
              "created_utc": "2025-12-26 17:51:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvt95qa",
          "author": "Klutzy-Snow8016",
          "text": "Technology moves on. Pseudo-standard sizes for open models used to be small 8B, medium 32B, large 70B+. Now it seems to be small 30B, medium 110B, large 230B+.\n\nAt least now they're MoEs, so you can run them at reasonable speed with low VRAM. A 30B-A3B can generate at reading speed on a 10+ year old computer if you put in 16GB of RAM and an 8GB GPU, and the output is way better than, like, Mistral 7B, which was super-impressive at the time.",
          "score": 6,
          "created_utc": "2025-12-25 02:21:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtfad4",
          "author": "IrisColt",
          "text": "Did you just wake up from a year-long coma? Local models are more powerful and easier to access than ever.",
          "score": 15,
          "created_utc": "2025-12-25 03:07:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtfj2d",
              "author": "LocoMod",
              "text": "Did you miss the point of the post? Read it again. But this time donâ€™t use an LLM to interpret it for you.",
              "score": -18,
              "created_utc": "2025-12-25 03:09:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvkynu",
                  "author": "IrisColt",
                  "text": ">donâ€™t use an LLM to interpret it\n\n\nWhat the absolute f? Abliterated versions of Qwen3-VL 32B are powerhouses you could only have dreamed of in late 2024. And local.",
                  "score": 3,
                  "created_utc": "2025-12-25 14:58:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt6508",
          "author": "Pvt_Twinkietoes",
          "text": "And how you propose we get there?",
          "score": 7,
          "created_utc": "2025-12-25 01:58:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt5wc8",
          "author": "gradient8",
          "text": "I donâ€™t disagree but this post feels weirdly entitled. We are not customers, open weight models cost millions to develop for us to get for free",
          "score": 24,
          "created_utc": "2025-12-25 01:56:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtajvh",
              "author": "LocoMod",
              "text": "The point of the post is that the recent open weight models are not free because even if you own a high end system, like I do, we still canâ€™t run them. I have an RTX5090 build easily worth 5k in components. I cannot run any of the best models released this year without severely lobotomizing it with a tiny quant.\n\n And Iâ€™m not entitled to anything. The entire point of the post is that very soon none of use are going to be able to run free models because the businesses releasing them (and they ARE businesses), are deliberately going to monetize you when you canâ€™t run their model but you CAN use their API at 10% the cost of OpenAI.",
              "score": 2,
              "created_utc": "2025-12-25 02:31:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvtgb5y",
                  "author": "AmuletOfNight",
                  "text": "I just don't feel like they're doing that on purpose. These models use a lot of power and take a lot of computation. Maybe the industry is going towards larger models because that's what gets better performance and people like better performance. You can't help the fact that bigger models tend to perform better.",
                  "score": 8,
                  "created_utc": "2025-12-25 03:15:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvtk4zv",
                  "author": "svankirk",
                  "text": "I think you're not wrong, but take a look at the hardware that is becoming available in the last 6 months that is capable of running 100b models on your desktop for 5K or less. I think that's going to keep getting better.",
                  "score": 4,
                  "created_utc": "2025-12-25 03:45:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvtkv87",
          "author": "Whole-Assignment6240",
          "text": "Are distillation techniques the answer for specialized small models?",
          "score": 3,
          "created_utc": "2025-12-25 03:51:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvdrzw",
              "author": "LocoMod",
              "text": "I donâ€™t know. Are they?",
              "score": 1,
              "created_utc": "2025-12-25 14:08:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvu28if",
          "author": "Sensitive_Sweet_1850",
          "text": "Itâ€™s happening very openly but very subtly. The champions of open weight models are slowly increasing their sizes to the point a very small portion of this sub can run them locally. An even smaller portion can run them as benchmarked (no quants). Many are now having to resort to Q3 and below, which will have a significant impact compared to what is marketed. Now, without any other recourse, those that cannot access or afford the more capable closed models are paying pennies for open weight models hosted by the labs themselves. This is the plan of course.\n\nGiven the cost of memory and other components many of us can no longer afford even a mid tier upgrade using modern components. The second hand market isnâ€™t fairing much better.\n\nThe only viable way forward for local tinkerers are models that can fit between 16 to 32GB of vram.\n\nThe only way most of us will be able to run models locally will be to fine tune, crowd fund, or â€¦ ? smaller more focused models that can still remain competitive in specific domains vs general frontier models.\n\nA capable coding model. A capable creative writing model. A capable math model. Etc.\n\nWeâ€™re not going to get competitive local models from â€œwell fundedâ€ labs backed by Big Co. A distinction will soon become clear that â€œopen weightsâ€ does not equal â€œlocalâ€.\n\nRemember the early days? Dolphin, Hermes, etc.\n\nWe need to go back to that.",
          "score": 5,
          "created_utc": "2025-12-25 06:18:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt57vp",
          "author": "misterflyer",
          "text": "It's inevitable. Especially since this space is so heavily dominated by ***benchmark hype and benchmaxxing***. With the big proprietary AI providers chasing each other for higher and higher benchmarks every 3 months, and bloating the sizes of their new models... it's just a cat & mouse game that even the popular open weights providers aren't immune from getting sucked into.\n\nNgl, I don't care about benchmarks. At best, I take them with a grain of salt. All I care about is... *does this new model work great for my use case or not?* And if I can't even run the model load the model to my VRAM+RAM, then the model in question is pretty much irrelevant to me regardless of what the benchmarks say.\n\nDon't get me wrong, I understand why most other people do care about benchmarks.  But if that's the most important thing that matters to the average person here then get ready for a future of 10 trillion parameter models that you can't even dream of running locally. **Then, the best models will only be available to most people here via API or subscription which completely defeats the purpose of the \"LocalLLaMA\" label.  But, that's exactly where we're headed rn.**\n\nBut s/o to Mistral for continuing to produce models of reasonable sizes. I know ppl like to shi- on their benchmark scores, but again, at least a decent proportion of people here can actually run most of their models above Q3.",
          "score": 12,
          "created_utc": "2025-12-25 01:51:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtbefo",
              "author": "LocoMod",
              "text": "Thank you. Someone got the point.",
              "score": 10,
              "created_utc": "2025-12-25 02:38:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvtxj30",
                  "author": "toothpastespiders",
                  "text": "I get the impression that only a couple people bothered to fully read through your post before getting angry at your conclusion. Taken on a point by point basis you didn't even say anything especially controversial.",
                  "score": 8,
                  "created_utc": "2025-12-25 05:35:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt9o5j",
          "author": "Hunting-Succcubus",
          "text": "There is wan 5b model, zimage 6b model, smaller qwen and gemma llm. Latest TTS  model are mostly small. What else you want? Leave poor multi billion AI companies from usa alone. look into chinese ai for small models.",
          "score": 4,
          "created_utc": "2025-12-25 02:25:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtdc11",
              "author": "LocoMod",
              "text": "Those models are toys. I can build a shitty system for >$2000 to run the small shitty Chinese model or pay a few cents when I need to produce something of quality with western frontier models. \n\nI play with all those Chinese model. Tons of fun. Would never ship a product with them. Only play with them. Thatâ€™s all they are good for.",
              "score": 2,
              "created_utc": "2025-12-25 02:52:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvte72s",
                  "author": "Hunting-Succcubus",
                  "text": "well you have to ship product with western subscription api. western don't allow commercial usage with their local model. keep shipping plastic skin, butt chin portrait with very high western cost. wester ai produces reliable  premium quality plastic skin, butt chin face, laying on gross figures. western ai has high quality knowledge of human anatomy, number of fingers. they are very reliable and not hallucinate. shitty z image always create 7 finger, low res bluryy texture ASK ANYONE. HYPOCRISY",
                  "score": 2,
                  "created_utc": "2025-12-25 02:59:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvtrfjn",
          "author": "thebadslime",
          "text": "Nanbeige 3B and RJN 1 JUST LAUNCHED. You're catastrophizing.",
          "score": 4,
          "created_utc": "2025-12-25 04:43:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvcnn1",
              "author": "LocoMod",
              "text": "What? Did those models just make up a word?\n\nHard pass.",
              "score": 1,
              "created_utc": "2025-12-25 13:59:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtbcju",
          "author": "One-Employment3759",
          "text": "Nah, if you are not doing local that's a choice you are making.\n\n\nLocal or die!",
          "score": 4,
          "created_utc": "2025-12-25 02:37:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt3cvq",
          "author": "AppealSame4367",
          "text": "By this time next year 256 GB unified RAM / VRAM will be normal.\n\nEdit: What do you guys expect? Run newest tech (local llms..) on budget hardware? Of course it will cost something if you still wanna catch up to newest developments in December 2026.\n\nUntil then the software tech around llms will keep developing too. I am very pleased with Mistral Ministral 3B 2512. It's fast, smart enough and a good daily assistant on my RTX 2060 laptop gpu. But of coure I won't be able to run SOTA OSS models with this laptop in 2026 - apart from those small models that might be even faster, smarter and agentic by then.",
          "score": 7,
          "created_utc": "2025-12-25 01:37:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvt44xk",
              "author": "LocoMod",
              "text": "The cost will be outside of the range for the majority of people. And for those that cannot access afford the 2 to 3x markup unless supply changes, now we will have to choose between an AI system with unified memory or a high end PC with discreet graphics for gaming and other professional work or compute heavy hobbies.",
              "score": 11,
              "created_utc": "2025-12-25 01:43:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvt3pzv",
              "author": "Linkpharm2",
              "text": "*8gb laptop and most popular gpu in the distance*",
              "score": 15,
              "created_utc": "2025-12-25 01:40:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvu0mj3",
                  "author": "True_Requirement_891",
                  "text": "meirl 32gb ram + 8gb 3070ti with Nemotron 30b at 100k+ context and 20tps slaps though",
                  "score": 4,
                  "created_utc": "2025-12-25 06:03:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvt5qju",
              "author": "DesperateAdvantage76",
              "text": "Local memory capacity is not keeping up with even dated models. The days of buying 4 gpus to fit high end models is over. Local models will likely be used for basic tasks with more complex tasks offloaded to servers.",
              "score": 9,
              "created_utc": "2025-12-25 01:55:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtyutu",
              "author": "WitAndWonder",
              "text": "Where are you possibly getting all that RAM when it's not going to be in public circulation?",
              "score": 2,
              "created_utc": "2025-12-25 05:47:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvt3mjd",
              "author": "IndianaCahones",
              "text": "For $1,600 if there is any available on the market.\n\nEdit: have to add an edit because this is no longer close to resembling the comment I responded to. As of today, 25 December 2026, 96GB is running around $800 in the U.S.",
              "score": -2,
              "created_utc": "2025-12-25 01:39:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvubr8e",
          "author": "__Maximum__",
          "text": "There is also qwen3 next paradigm, which is less widespread but is very promising.",
          "score": 2,
          "created_utc": "2025-12-25 07:53:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv6gea",
              "author": "LocoMod",
              "text": "You mean gpt-oss paradigm?",
              "score": 1,
              "created_utc": "2025-12-25 13:12:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvho5i",
                  "author": "__Maximum__",
                  "text": "Qwen next 80b has 3b active params. I can run that thing on my CPU. Plus, it's got things like hybrid attention and mutli-token prediction.\n\nI hope we see something like 100b2b and 1b50b with even more ultra-efficient features.",
                  "score": 1,
                  "created_utc": "2025-12-25 14:36:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvuctx7",
          "author": "PotentialFunny7143",
          "text": "Big local models will be small local models when the AI bubble will pop",
          "score": 2,
          "created_utc": "2025-12-25 08:05:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuktz1",
          "author": "muntaxitome",
          "text": "In my opinion you have it backwards, the models we have gotten this year are now so much better at 8B-32B to the point where there is limited use to these finetunes anymore. Like a year ago coding with a 32B wasn't much fun at all, now it's a legit possibility. \n \nDoing finetunes that would beat qwen3 32B at anything relevant is going to be tough.",
          "score": 2,
          "created_utc": "2025-12-25 09:32:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvamk0",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 2,
          "created_utc": "2025-12-25 13:44:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvfru3",
              "author": "LocoMod",
              "text": "Iâ€™m betting on Apple to win the long game when it comes to running models on device. Canâ€™t wait for M5 MAX. Day zero purchase for me.",
              "score": 1,
              "created_utc": "2025-12-25 14:22:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw0g29r",
                  "author": "power97992",
                  "text": "But the m6 max will have even more ram and it should come out in 2026. At this rate even a single 1tb m5 ultra studio wont be able to future +2T models at q4/q8 at half or more context.. PEople with money will end up clustering mac studios to run massive models.",
                  "score": 1,
                  "created_utc": "2025-12-26 11:56:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvvcdlr",
          "author": "No_Afternoon_4260",
          "text": "Good thing devstral 123B fits in a local-ish rig",
          "score": 2,
          "created_utc": "2025-12-25 13:57:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvg7cf",
              "author": "LocoMod",
              "text": "A rig capable of running it is still beyond what the majority of people in here can afford. I can run it on my M3 MAX with 128GB of unified memory. But that machine was ~6k.\n\nYou could build a PC with a 24GB-32GB GPU and tons of RAM but by the time youâ€™re done pricing out components youâ€™re already ~4K into your budget.\n\nI have both. But Iâ€™m not the average user either.",
              "score": 1,
              "created_utc": "2025-12-25 14:25:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvhx1q",
                  "author": "No_Afternoon_4260",
                  "text": "What speed do you get on your mbp?",
                  "score": 1,
                  "created_utc": "2025-12-25 14:38:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvvdqbz",
          "author": "Confusion_Senior",
          "text": "The general models will always be on a large number of parameters but specialized models can be distilled with way less",
          "score": 2,
          "created_utc": "2025-12-25 14:07:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtc5gz",
          "author": "Available_Brain6231",
          "text": "I  believe in the next 5 years we(non-americans) will have our hands at lots of cheap used chinese server gpus, something on the level of a 40xx but with 192 vram or more.  \nThey will iterate quickly now and we will be able to buy old stuff like it was during the mining boom.\n\nI also think those Chinese labs need to keep making powerful big models, I rather not be able to run it now than never.",
          "score": 3,
          "created_utc": "2025-12-25 02:43:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtwlpw",
          "author": "toothpastespiders",
          "text": ">Remember the early days? Dolphin, Hermes, etc. We need to go back to that.\n\nI think in a sense that might be part of the problem. Lack of specialization in released models has probably driven a lot of us to make VERY specialized fine tunes. So specialized that they're essentially worthless outside our individual setup and needs. \n\nThat said, I find the amount of negative and outright angry replies to your post to be pretty weird. I don't think anything you said is especially controversial other than your conclusion.",
          "score": 3,
          "created_utc": "2025-12-25 05:27:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvc1vi",
              "author": "LocoMod",
              "text": "Iâ€™m exposing the long game and the folks playing it are upset. Get this sub hooked on the smaller model. Release benchmaxxed >100b model and slowly shuffle users to their paid API service. Now they are publicly seen as altruistic but ultimately the game is to lock in users to their paid platforms. And thatâ€™s fine. They are entitled to that.",
              "score": 1,
              "created_utc": "2025-12-25 13:55:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvzb4bt",
                  "author": "ttkciar",
                  "text": "If people fall into the trap of paid API services, they have only themselves to blame.\n\nThe solution is to design one's applications to work within the capabilities of the models and hardware we have to work with.\n\nI realize that's not a very popular practice these days, but twenty+ years ago it was pretty standard to develop software against a specific platform (specific versions of libraries and other dependencies) and hardware.\n\nFrom what I can tell, programmers have stopped doing that mainly due to a lack of self-discipline, rather than because the new looser practices are actually better, and we've been paying for it with dependency hell, functional complexity, and technical debt.",
                  "score": 1,
                  "created_utc": "2025-12-26 05:17:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvteg5x",
          "author": "StardockEngineer",
          "text": "I can tell.  You have no idea what Iâ€™m talking about.  You have my arguments and intents completely twisted.",
          "score": 3,
          "created_utc": "2025-12-25 03:01:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvu0gxn",
          "author": "ServersServant",
          "text": "Uh, you donâ€™t need cutting edge models to run locally if you actually have good MCP servers and a decent model imo. You can get pretty damn far.\n\nYour thinking is the same of those kiddos believing they need the latest iPhone toâ€¦ send memes.Â ",
          "score": 3,
          "created_utc": "2025-12-25 06:02:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvub7mu",
              "author": "power97992",
              "text": "Which mcp server?",
              "score": 2,
              "created_utc": "2025-12-25 07:47:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxhlas",
                  "author": "ServersServant",
                  "text": "Depends on your use-cases. It's not like a magic MCP server exists for every need.",
                  "score": 1,
                  "created_utc": "2025-12-25 21:53:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvv8d6q",
              "author": "LocoMod",
              "text": "Anyone can claim anything. Your MCP setup is leagues behind what a frontier api service can do. Our expectations and QA are clearly not the same. If you value modern equivalent of TODO apps then I suppose your setup will work. If thatâ€™s the ceiling you aspire to reach then more power to you.",
              "score": 1,
              "created_utc": "2025-12-25 13:27:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvxko30",
                  "author": "ServersServant",
                  "text": "What kind of use-cases do you have in mind?\n\nMy point (and sorry if I sounded deprecating or rude, it wasn't my intention), you're over-estimating the need for cutting edge models. Newer models are mostly improving efficiency, experimenting newer architectures or upgrading knowledge and fine-tuning. AFAIK, there's nothing radically new.\n\nI got a self-hosted setup running an assistant over voice or messages. As long as I'm not in a very noisy environment, it understands pretty much everything. I'm working on a noise filter to improve use in noisy environments. It can search for things online, give me directions while cycling or driving, optimise my route given constraints I set up beforehand, schedule hotel reservations and flight tickets, trigger home automations (from simple stuff like temp control, watering plants\\*, etc., to actually getting groceries), summarise things and do a couple of other things.\n\nIt's surely not your Jarvis you'd maybe expect, but having it understand voice and reply either via synthetic voice or text, works for me great. This isn't even using super powerful models, just quantised Voxtral, MoE, Small 3.2 Instruct, some MCPs (for databases, search engines, etc.), Chroma, APIs, a couple of databases I built + good prompts. Synapse to have voice calls and messages over Element X. I can trigger the calling asking Siri to call a contact assigned to it, or send a message and I get \"called back\". Whole setup runs on my own servers at home and some cloud GPUs for the models on-demand, so it doesn't break my bank.\n\nWhen I said you can get pretty far I really mean it. This setup didn't took me years. I literally build it over six months and it's a hobby project, so it's no rush. Surely it helps me being a software engineer working in distributed systems for ML, but it's surely doable by most people if they got the commitment and like, basic integration software skills. I'm kinda sure kids are already vibe-coding more complex stuff than this.\n\nSo yeah, what's your use-case for cutting edge models?\n\nedit: fixed typo",
                  "score": 3,
                  "created_utc": "2025-12-25 22:11:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvu28oz",
          "author": "brahh85",
          "text": ">A capable coding model. A capable creative writing model. A capable math model. Etc.\n\nthats literally what mistral is doing",
          "score": 2,
          "created_utc": "2025-12-25 06:18:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv84ng",
              "author": "LocoMod",
              "text": "Good.",
              "score": 1,
              "created_utc": "2025-12-25 13:25:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvu967o",
          "author": "Cuplike",
          "text": "It's not local because I can't run it is a horrible mindset. I don't want any lab to stop publishing open weights for large models because \"Well, they can't run it anyway\"",
          "score": 3,
          "created_utc": "2025-12-25 07:26:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv6o8c",
              "author": "LocoMod",
              "text": "Thatâ€™s cool. You can always pay for their cloud service at 1/8th of the cost of the big 3 frontier models and proclaim you run local on someone elseâ€™s computer.",
              "score": 1,
              "created_utc": "2025-12-25 13:13:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvk14n",
                  "author": "Cuplike",
                  "text": "You're right, a model is only useful if YOU can run it. Being able to store it when the company stops offering it officially, other providers having the ability to offer competitive pricing, being able to distill and finetune the model, these are all completely useless things",
                  "score": 2,
                  "created_utc": "2025-12-25 14:52:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt5p2w",
          "author": "QuailLife7760",
          "text": "Sure mfker you want openai/claude level product in 1B model, either you make one yourself or stfu.",
          "score": 4,
          "created_utc": "2025-12-25 01:55:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtcicg",
          "author": "UnnamedPlayerXY",
          "text": "There're sill many good releases that can be run locally but I guess time will tell. Personally I find it more worrisome that there seems to be less of a focus on bringing regular consumer grade hardware to where it should be in regards to the desired optimizations.\n\nThat the whole chain of production is essentially bottlenecked by a rather monopolized set-up is ofc. not helping the situation.",
          "score": 2,
          "created_utc": "2025-12-25 02:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv2126",
          "author": "Investolas",
          "text": "Reported for impersonating a mod!",
          "score": 2,
          "created_utc": "2025-12-25 12:33:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv5pp5",
              "author": "LocoMod",
              "text": "Merry Christmas! â¤ï¸",
              "score": 0,
              "created_utc": "2025-12-25 13:05:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvjd59",
                  "author": "Investolas",
                  "text": "You are not sure if anyone actually likes you",
                  "score": 1,
                  "created_utc": "2025-12-25 14:48:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvu6yo0",
          "author": "relmny",
          "text": "I only read the title and... did you skip the whole 2025?\n\n\nThis is the best year for local LLMs!\n\n\nWe have everything! and multiple times with multiple improvements!\n\n\nWe're have you, and the ones that upvote you, be living in?",
          "score": 2,
          "created_utc": "2025-12-25 07:03:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv811j",
              "author": "LocoMod",
              "text": "Thatâ€™s like asking â€œwhere have you been the past decade?â€. We can take some arbitrary time span and point to a 24B model that this sub could host. A year ago is a decade in AI time. Look at the past 3 months. The relevant labs all moved to >100B models you canâ€™t run unless you have ~10k of gear. But donâ€™t worry. They have all also bootstrapped a paid API service and vibe coded CLI agent as an alternative.",
              "score": 0,
              "created_utc": "2025-12-25 13:24:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvu7hxb",
          "author": "CanineAssBandit",
          "text": "I don't actually mind this at all. My issue is, was, and always will be, that closed source=I do not control access. \n\nOpen weights=**I control access**. The model is physically **MINE**, no company or entity can take it unless they physically steal my electronics.\n\nI emphasize this because \"woe is me, I have to have a server to run it\" is missing the most important thing about open weights. The \"and you can run it on your desktop\" part was just gravy.\n\nDon't get me wrong, I love when the models that run on my simple hardware become more useful, but that's a lot less important to me than \"**I have ownership of the same calibre of model as billion dollar companies**, and all I have to do is buy 5k of server gear to run it slowly, or rent server hours to run it quickly.\" I will choose unwieldy SOTA over convenient shortbus every second of every day.",
          "score": 1,
          "created_utc": "2025-12-25 07:09:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv783b",
              "author": "LocoMod",
              "text": "Cloud services generally have more uptime than PCâ€™s. For what itâ€™s worth. I donâ€™t have a doomsday mentality so I donâ€™t share your concerns.",
              "score": -1,
              "created_utc": "2025-12-25 13:18:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvw7j8x",
                  "author": "CanineAssBandit",
                  "text": "...Okay? I don't disagree, that's why I think \"ability to run locally\" is kind of a red herring in the open weights argument.\n\nYou're arguing in bad faith by calling my concerns a \"doomsday mentality,\" my issues have nothing to do with an apocalypse. \n\nClosed means I'm getting served whatever the hell the owner of the model wants to serve, and they can call it whatever they want, and change it whenever they want to whatever they want, even without telling me. *This is completely unacceptable* in any serious use case where details matter, or even basic ones like RP where it's super frustrating and creepy when suddenly they sound completely different but are wearing the same name tag.\n\nI want a product I can trust with performance I can depend on, and open weights gives me that transparency and control. Look at how much better GPT 5 was at launch, and how much it fell off a cliff not even a couple weeks later, all while still calling it GPT 5. And now they've done two shiny updates to 5.1 and 5.2 and it's still complete garbage compared to launch day GPT 5.\n\nMeanwhile, Deepseek R1 is still R1, Hermes 3 405B is still the same, everything open is still just chilling and you can use an old version forever if you really want to. Or you can evaluate and upgrade when YOU see fit for YOUR workflow. YOU get to make those choices instead of OAI or Anthropic choosing how to run your business or simulated people for you.",
                  "score": 2,
                  "created_utc": "2025-12-25 17:17:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvtic2r",
          "author": "dsartori",
          "text": "Thereâ€™s a lot of interest in â€œedgeâ€ workloads generally I think. The variety of models 8B and under is really quite good. \n\nModel capabilities are far ahead of where they were a year ago and you could do useful production stuff with local models a year ago.",
          "score": 1,
          "created_utc": "2025-12-25 03:31:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtmhy3",
          "author": "john0201",
          "text": "M5 Ultra, whatever the next strix halo is will hopefully keep it feasible.",
          "score": 1,
          "created_utc": "2025-12-25 04:04:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvd86t",
              "author": "LocoMod",
              "text": "Iâ€™ll be a day zero M5 MAX buyer. It canâ€™t come soon enough.",
              "score": 1,
              "created_utc": "2025-12-25 14:04:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvusofg",
          "author": "a_beautiful_rhind",
          "text": "You're getting a lot of smalls in addition to the gigantor models. What's missing is the mid stuff. 30b turning into 120bA3 because it's \"faster\".\n\nNot sure that anyone released a \"creative writing\" model literally ever. Largest thing that counts as commercial effort is latitude games and they're not quite a lab. It's all agentic benchmaxx stem codeslop. GLM sees the stressful usecase, mistral is just french so their models still work but are far from creative focused. Definitely not in the way that you imply with specialization. \n\nAnd honestly, local was doing fine until the artificial shortage. Old xeon/epyc and DDR4 were bountiful.",
          "score": 1,
          "created_utc": "2025-12-25 10:58:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuvglb",
          "author": "yopla",
          "text": "don't worry eventually GPU will get cheaper and we will be able to run large models locally...\n\n\n... LOL... I know you wanted to believe me for just one second...",
          "score": 1,
          "created_utc": "2025-12-25 11:28:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv5w67",
              "author": "LocoMod",
              "text": "I want to believe!",
              "score": 1,
              "created_utc": "2025-12-25 13:07:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvuxoz9",
          "author": "uti24",
          "text": ">All of the major open weight labs have shifted to large params general models instead of smaller, more focused models. By this time next year, there wonâ€™t be much â€œlocalâ€ about this sub unless the paradigm shifts to smaller models good at specific domains.\n\nI have some thoughts on that, I think you can have specialist \"focused\" models in terms of some branch of knowledge, like laws, history or pop culture, but can you have small focused model that can use that knowledge? Maybe not? Then even locally we need kinda big model if we want smart model, not knowledgeable one.",
          "score": 1,
          "created_utc": "2025-12-25 11:51:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuydnc",
          "author": "Monad_Maya",
          "text": "We need to push for better hardware rather than smaller local models. More VRAM, better bandwidth and obviously cheaper hardware.\n\n\nWith that said, we've had a decent number of \"local\" releases in 2025.\n\n\nThere is no larger conspiracy in my opinion, the larger models are genuinely better due to better world understanding.",
          "score": 1,
          "created_utc": "2025-12-25 11:58:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv1nfe",
          "author": "Investolas",
          "text": "I don't think you'll be able to do much regardless",
          "score": 1,
          "created_utc": "2025-12-25 12:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv1xt0",
          "author": "Investolas",
          "text": "MoronÂ ",
          "score": 1,
          "created_utc": "2025-12-25 12:32:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvitfg",
          "author": "MannToots",
          "text": "The reality is more vram makes the technology work better with longer chats. Local is more fun than practical.Â Â ",
          "score": 1,
          "created_utc": "2025-12-25 14:44:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvw2nm2",
          "author": "sxales",
          "text": "It is true. Local used to mean you could run them on average consumer hardware. Now, local seems to mean you need a several thousand dollar purpose-built rig. \n\nIt is nice that Alibaba, Google, and IBM are keeping us fed. Maybe Mistral and AllenAI will catch up. It would be nice is Microsoft and Meta came back.",
          "score": 1,
          "created_utc": "2025-12-25 16:48:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwz5h1",
          "author": "snowrazer_",
          "text": "Local doesnâ€™t mean small. Thatâ€™s how it is right now because of consumer hardware constraints. We need those shackles removed. The demand is there, I believe the future is bright for high memory, high performance LLMs. Running models as powerful as Claude Sonnet 4.5 locally. Itâ€™s not a question of if, but when. \n\nIf the hardware is there, then there are plenty of players like Meta and Qwen ready to rain on the parade of closed models.",
          "score": 1,
          "created_utc": "2025-12-25 19:59:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvx06de",
          "author": "pgrijpink",
          "text": "Must have missed Nanbeige4 3B which was released recently. Mental performance for it's size!",
          "score": 1,
          "created_utc": "2025-12-25 20:05:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxp8ew",
          "author": "svankirk",
          "text": "That is awesome!",
          "score": 1,
          "created_utc": "2025-12-25 22:40:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0hi3n",
          "author": "power97992",
          "text": "Ram and gpus and Macs need to get cheaper.. THey will never stop chasing benchmarks. Models will grow to 2Trillion + parameters... At least they should also release 100b models for those with less ram; gpu providers  and apple need to make a gpu with 4TB/s 128gb of VRAm for $900 and a cuda gpu with 1tb of 8TB/s ram for 3.3k,  a macbook with 160GB of ram for 1k and a macbook with full pytorch and transformers/bitsandbytes support with 512gb of URAM for 2.6k and 1TB   8TB/s  ram MacBook for 3.6k",
          "score": 1,
          "created_utc": "2025-12-26 12:10:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1cefa",
          "author": "power97992",
          "text": "If open weight companies only release large models and meh small models and Â  dont release a sub 110b model Â that is 3-5 months behind the top models , there is no point even upgrading a computer for that cost , you are better off just using the apiâ€¦",
          "score": 1,
          "created_utc": "2025-12-26 15:39:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3gq4d",
          "author": "highmindedlowlife",
          "text": "I can run GPT-OSS 120b at 18 tk/s with llama.cpp on a single 3090 and CPU. Local and powerful enough for me.",
          "score": 1,
          "created_utc": "2025-12-26 22:29:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi3vxw",
          "author": "coastisthemost",
          "text": "Get a strix halo can run big models. Comfyui still is very dicey on it but llms work great.",
          "score": 1,
          "created_utc": "2025-12-29 06:17:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtkxn2",
          "author": "tronathan",
          "text": "Don't forget about technological advancements; a year might be long enough for some big changes.",
          "score": 1,
          "created_utc": "2025-12-25 03:51:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvdmq5",
              "author": "LocoMod",
              "text": "Sure but that has nothing to do with my post. Itâ€™s irrelevant if some Chinese lab releases a model that canâ€™t outcompete the big western providers or can be self hosted without breaking the bank.\n\nNow if you release a 32B coding model that can go toe to toe with sonnet-4-5 then weâ€™re cooking.\n\nOtherwise I might as well pay for Anthropic API.",
              "score": 1,
              "created_utc": "2025-12-25 14:06:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtsuet",
          "author": "Stunning_Mast2001",
          "text": "I donâ€™t bet on this. Diffusion models are on the horizon",
          "score": 1,
          "created_utc": "2025-12-25 04:55:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvci42",
              "author": "LocoMod",
              "text": "Eagerly awaiting a competitive one. Iâ€™ve tried them all and they are not it. \n\nBut time will tell.",
              "score": 1,
              "created_utc": "2025-12-25 13:58:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtv43g",
          "author": "Savantskie1",
          "text": "What are you talking about? Thereâ€™s tons of models out there you can run that have been released. Heck the Qwen series keeps putting out smaller models. And thatâ€™s just one example. What kind of crack are you smoking and let me have some ðŸ˜‚",
          "score": 1,
          "created_utc": "2025-12-25 05:14:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvcbwc",
              "author": "LocoMod",
              "text": "Your attention span must have lasted through the subject only. Happens to me too sometimes so itâ€™s all good.",
              "score": 0,
              "created_utc": "2025-12-25 13:57:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtcx1y",
          "author": "79215185-1feb-44c6",
          "text": "I'll just have to sell the 7900XTXs and buy two RTX 6000s.\n\n> The only viable way forward for local tinkerers are models that can fit between 16 to 32GB of vram.\n\nOh you're still living in 2023. 24GB VRAM is now the minimum (gpt-oss-20b)",
          "score": -1,
          "created_utc": "2025-12-25 02:49:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtdngo",
              "author": "LocoMod",
              "text": "I have RTX5090. But that is far from average and so is 24GB GPU. So I was being more inclusive since most of us in here do not have a 24GB card.",
              "score": 3,
              "created_utc": "2025-12-25 02:55:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvtm2ei",
                  "author": "79215185-1feb-44c6",
                  "text": "People running their crappy little gaming setups for LLMs don't count. You and I are the standard now. If you want to use LLMs for work, you can't settle with trash.",
                  "score": 3,
                  "created_utc": "2025-12-25 04:00:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvtve10",
              "author": "Savantskie1",
              "text": "Thatâ€™s a 16GB MODEL I can run on my 7900 XT card.",
              "score": 2,
              "created_utc": "2025-12-25 05:16:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvuaq3n",
          "author": "cosimoiaia",
          "text": "LOL. You're probably just trolling and/or you live in a cave.",
          "score": 0,
          "created_utc": "2025-12-25 07:42:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt86cd",
          "author": "Investolas",
          "text": "Idiot",
          "score": -6,
          "created_utc": "2025-12-25 02:13:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtbm8l",
              "author": "LocoMod",
              "text": "Your business is not going to make money. No one is paying for that shitty API. Save yourself the trouble and start a trust fund instead. Youâ€™ll have a better retirement.",
              "score": 4,
              "created_utc": "2025-12-25 02:39:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvtdjpg",
                  "author": "Investolas",
                  "text": "Major idiot",
                  "score": -1,
                  "created_utc": "2025-12-25 02:54:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt9xq3",
          "author": "Rei1003",
          "text": "Small general yes, small focused no.",
          "score": 0,
          "created_utc": "2025-12-25 02:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt5qa8",
          "author": "ThisWillPass",
          "text": "We need ~100,000b parameter model for the next intelligence emergence.",
          "score": -4,
          "created_utc": "2025-12-25 01:55:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt7y7e",
          "author": "sunshinecheung",
          "text": "you can buy a RTX4090 48GB GPU or mac studio",
          "score": -1,
          "created_utc": "2025-12-25 02:12:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtc2lv",
              "author": "LocoMod",
              "text": "And you still wonâ€™t be able to run GLM-4.7 FP16",
              "score": 3,
              "created_utc": "2025-12-25 02:43:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvuujqg",
                  "author": "Baldur-Norddahl",
                  "text": "Why would anyone want to run FP16? Or anything more than q4 really. I am really curious why you would suggest that.\n\nWhen trying to get the most out of limited hardware, just wasting bits on getting a few percent improvement is never the correct tactic. It will always be better to upgrade to a larger model with quantization.",
                  "score": 3,
                  "created_utc": "2025-12-25 11:18:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q0zst6",
      "title": "Upstage Solar-Open-100B Public Validation",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/w789uyo0cpag1.jpeg",
      "author": "PerPartes",
      "created_utc": "2026-01-01 08:52:25",
      "score": 233,
      "num_comments": 69,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0zst6/upstage_solaropen100b_public_validation/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nx2bcn4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-01 11:00:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1zsaj",
          "author": "CKtalon",
          "text": "Why a location? Just release on the Internet.",
          "score": 124,
          "created_utc": "2026-01-01 08:56:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1zvf8",
              "author": "spectralyst",
              "text": "Gangnam Style",
              "score": 133,
              "created_utc": "2026-01-01 08:57:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx34al3",
                  "author": "AuspiciousApple",
                  "text": "Koreans love their pop up stores.",
                  "score": 10,
                  "created_utc": "2026-01-01 14:56:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx7ps0n",
                  "author": "Mikasa0xdev",
                  "text": "Oppa LocalLLaMA Style!",
                  "score": 3,
                  "created_utc": "2026-01-02 06:43:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx2a34t",
                  "author": "DecodeBytes",
                  "text": "I have the synth intro stuck in my head now",
                  "score": 4,
                  "created_utc": "2026-01-01 10:47:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2eddx",
              "author": "keepthepace",
              "text": "Sadly, that's still how to maximize journalistic coverage, by causing FOMO. Force journalists to get there, you force them to make an article. Publish something online they will be like \"meh, put it on the pile\"",
              "score": 30,
              "created_utc": "2026-01-01 11:31:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2ri55",
                  "author": "-p-e-w-",
                  "text": "I very strongly doubt that journalists are going to bother showing up at some mystery location in Korea to settle some AI startup beef lol.",
                  "score": 3,
                  "created_utc": "2026-01-01 13:29:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx224td",
              "author": "ttkciar",
              "text": "That would be lovely!",
              "score": 4,
              "created_utc": "2026-01-01 09:22:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2ijts",
              "author": "PerPartes",
              "text": "This is because of huge domestic market focus. In-person event is a matter of trust and respect (esp. in this region). Almost whole SK AI business is focused on itself. In case of Upstage with the addition of Japanese market as well.",
              "score": 10,
              "created_utc": "2026-01-01 12:12:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx33dme",
                  "author": "Nyghtbynger",
                  "text": "Interestingly that's the case of most nations in fact, except a few merchant nations and empires (US,UK,...)",
                  "score": 2,
                  "created_utc": "2026-01-01 14:50:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx72o2z",
              "author": "dicoxbeco",
              "text": "OOP in Korean *does* state that they will update the post with URL for livestream.\n\nEither the translator OP used skipped that part over, or OOP edited that in later.",
              "score": 1,
              "created_utc": "2026-01-02 03:56:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx21p4c",
          "author": "throwaway-link",
          "text": "I did my own tests. Cossim between layers past the first few seems to be extremely high across any model. Testing layer 45 input layernorm of deepseek v3/v3.1/v3.2-special, kimi k2, and mistral large 3 all give similarities around 0.99. The tested deepseek v3 variants are around 0.99999 with each other. \n\nData from the accusation is entirely expected for a model trained from scratch.",
          "score": 75,
          "created_utc": "2026-01-01 09:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2hccm",
              "author": "llama-impersonator",
              "text": "why are people comparing the norms instead of attn or mlp layers? norms have both low param count and a fairly simple fixed function.",
              "score": 11,
              "created_utc": "2026-01-01 12:01:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2m36d",
                  "author": "throwaway-link",
                  "text": "bc the accusation already says they're different? Their only evidence is norm weights which I show is expected. Probably bc training dynamics for rmsnorm of deeper layers cause the scale to just be a constant value across the weight which obviously results in high cossim. I guess since deeper layers do smaller adjustments, rmsnorm scale doesn't need to do any wild adjustments across the already relatively normalised token vector.",
                  "score": 9,
                  "created_utc": "2026-01-01 12:45:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx3bule",
              "author": "egomarker",
              "text": "Show the code and results. No idea if you are legit or yet another schizo vibecoder with hallucinated \"test results\".",
              "score": 6,
              "created_utc": "2026-01-01 15:41:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx22ua9",
              "author": "KontoOficjalneMR",
              "text": "Almost like all those models are using the similar architecture and similar datasets and you get same-ish output with some small flavour on top. \n\nYou look at the benchmarks and the results are basically a function of amount of parameters with tiny percentage variation based mostly on luck.",
              "score": 15,
              "created_utc": "2026-01-01 09:29:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2i4uv",
                  "author": "DistanceSolar1449",
                  "text": "Thatâ€™sâ€¦ obviously not true. DeepSeek V3, R1, V3.1, V3.2 all have the same param count but much diff performance.",
                  "score": 25,
                  "created_utc": "2026-01-01 12:09:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx42byg",
              "author": "jinnyjuice",
              "text": "Yeah they're saying that you can't really make such definitive conclusions with cossim. They made comparison with Phi here also: https://github.com/hyunwoongko/solar-vs-glm-vs-phi",
              "score": 2,
              "created_utc": "2026-01-01 18:00:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3du9z",
          "author": "kiwibonga",
          "text": "News tomorrow: Upstage employees arrested for beating up some dude in a parking lot.",
          "score": 10,
          "created_utc": "2026-01-01 15:52:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx228oe",
          "author": "ResidentPositive4122",
          "text": "I mean, if this is what it takes to get intermediate checkpoints, let's do it! Llamas, qwens, mistrals, glms, minimaxes, deepseeks, j'accuse! :D",
          "score": 24,
          "created_utc": "2026-01-01 09:23:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2au1f",
              "author": "pkmxtw",
              "text": "AI labs *hate* this simple trick to get them to release intermediate checkpoints!\n\nEither that or this is some of evil-genius level of marketing.",
              "score": 15,
              "created_utc": "2026-01-01 10:54:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2oduo",
              "author": "zball_",
              "text": "Just use different model configuration smh",
              "score": -1,
              "created_utc": "2026-01-01 13:04:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx20qfj",
          "author": "garloid64",
          "text": "op op op",
          "score": 18,
          "created_utc": "2026-01-01 09:07:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx20cjs",
          "author": "PerPartes",
          "text": "I just shared this because recent AI generated post here about the plagiarism claim was removed by the admins. I know the team for approx. 2 years (from the online space) and can hardly believe that it would be true.",
          "score": 34,
          "created_utc": "2026-01-01 09:02:47",
          "is_submitter": true,
          "replies": [
            {
              "id": "nx25axo",
              "author": "RuthlessCriticismAll",
              "text": "It seems appropriate to remove that post. It is however galling that similar, evidence free, ai generated posts with the same accusations don't get removed.",
              "score": 17,
              "created_utc": "2026-01-01 09:56:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx26swr",
                  "author": "PerPartes",
                  "text": "Agreed. Hate is always simpler than a deep and independent analysis.",
                  "score": 16,
                  "created_utc": "2026-01-01 10:12:19",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nx49h78",
                  "author": "rm-rf-rm",
                  "text": "Please report anything you see that we havent removed. Generally I think we are catching stuff well especially things that are particularly egregious.",
                  "score": 6,
                  "created_utc": "2026-01-01 18:35:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx26a8j",
          "author": "AppearanceHeavy6724",
          "text": "Ahaha, imagine if there will be a literal knuckle fight.",
          "score": 17,
          "created_utc": "2026-01-01 10:06:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx61575",
              "author": "tengo_harambe",
              "text": "\"The cosine similarity of my fist and your face is about to be -1.00\"",
              "score": 6,
              "created_utc": "2026-01-02 00:08:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7ilyv",
                  "author": "AppearanceHeavy6724",
                  "text": "yeah exactly.",
                  "score": 2,
                  "created_utc": "2026-01-02 05:45:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6rc2d",
          "author": "siegevjorn",
          "text": "Am I reading this right? How the fuck are they going to validate they trained their llm from scratch at Gangnam station? What about just release a white paper about the novelty of their methods?",
          "score": 4,
          "created_utc": "2026-01-02 02:44:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2uobg",
          "author": "Intrepid_Bobcat_2931",
          "text": "This is a joke. I could see a stunt like \"in person verification\" be reasonable if you gave two weeks notice for people to make travel plans, but they know it's completely impractical for highly experienced people to fly over at a day's notice.",
          "score": 6,
          "created_utc": "2026-01-01 13:52:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3mgdc",
              "author": "my_name_isnt_clever",
              "text": "If you have to fly there, you're not their target audience. This is for domestic journalism.",
              "score": 12,
              "created_utc": "2026-01-01 16:38:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx718se",
              "author": "dicoxbeco",
              "text": "... What joke?\n\nThis was never meant for English audience. [In fact, the OOP wasn't even written in English.](https://www.linkedin.com/posts/upstage-stan_solar-open-100b-%EA%B3%B5%EA%B0%9C-%EA%B2%80%EC%A6%9D%EC%97%90-%EC%B4%88%EB%8C%80-%EB%93%9C%EB%A6%BD%EB%8B%88%EB%8B%A4-solar-100b%EA%B0%80-activity-7412403323175370753-2KgK?utm_source=share&utm_medium=member_desktop&rcm=ACoAACoF--MBSnmFDkOdoa7FU_ztI512j0sxTo4) OP went through a translator so you would understand what it says.",
              "score": 4,
              "created_utc": "2026-01-02 03:46:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx25ssw",
          "author": "PerPartes",
          "text": "[Original Hwalsukâ€™s LI post here](https://www.linkedin.com/posts/upstage-stan_solar-open-100b-%EA%B3%B5%EA%B0%9C-%EA%B2%80%EC%A6%9D%EC%97%90-%EC%B4%88%EB%8C%80-%EB%93%9C%EB%A6%BD%EB%8B%88%EB%8B%A4-solar-100b%EA%B0%80-activity-7412403323175370753-2KgK)",
          "score": 3,
          "created_utc": "2026-01-01 10:01:43",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nx2hk6u",
          "author": "No_Conversation9561",
          "text": "Damn.. you know what, I believe him",
          "score": 2,
          "created_utc": "2026-01-01 12:03:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx20wtr",
          "author": "texasdude11",
          "text": "Tbh, I don't even care about this... If I need a model in this class, I can pick prime intellect, gpt-oss-120b, qwen3-next or move up a class and go to qwen3-235b or Minimax-m2.1 this 100b market is so competitive that you really need to stand out for adoption. Zai, Qwen and OpenAI's censored gpt-oss-120b kinda rule that 80-120b.\n\nAll that being said, more competition is always welcome though! I'd love to see a llama5 120B or a DeepSeek 200b model. That would be insane!",
          "score": 6,
          "created_utc": "2026-01-01 09:08:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx26jdf",
              "author": "LittleBlueLaboratory",
              "text": "I have 96GB VRAM (4x 3090). Strix Halo and DGX Spark have 128. This 80B to 120B segment is where its at! The more competition the better!",
              "score": 12,
              "created_utc": "2026-01-01 10:09:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx26pzv",
                  "author": "texasdude11",
                  "text": "Agreed!\n\nI have 2x6000 Pros with 512 GB DDR5 RAM, so I'm a bit lucky there. These 100b size is clearly in consumer reach!",
                  "score": 3,
                  "created_utc": "2026-01-01 10:11:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx3kbfm",
              "author": "uti24",
              "text": ">this 100b market is so competitive that you really need to stand out for adoption\n\nI want 100B dense model. Is there something besides Meta-Llama-1/2/3-70B? \n\nIt feels not really smart.. On par with other 30B class models like Gemma or Mistral small.",
              "score": 1,
              "created_utc": "2026-01-01 16:26:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3mpca",
                  "author": "my_name_isnt_clever",
                  "text": "Devstral 2 is 123b dense, but it's coding focused. It's far, far more expensive to train large dense models than MoE which is why they're so few and far between these days.",
                  "score": 4,
                  "created_utc": "2026-01-01 16:39:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx6n7cx",
                  "author": "Sea-Speaker1700",
                  "text": "They're all complete morons out of the box, every last one. \n\nSetup a proxy between your client and the inference service and tailor the performance to your needs, it can take any \"only yet another info barfing hallucinator model\", aka: every single 100b range model, and turn them into a useful tool.\n\nLoading 100b(ish) and trying to use them direct is a plain old waste of time.",
                  "score": 1,
                  "created_utc": "2026-01-02 02:19:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx274pg",
          "author": "Kooky-Somewhere-2883",
          "text": "Oppa Gangnam Style?",
          "score": 4,
          "created_utc": "2026-01-01 10:15:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx29jzj",
          "author": "Ok_Condition4242",
          "text": "https://i.redd.it/8dox9ujevpag1.gif\n\nmeanwhile cursor's composer-1",
          "score": 3,
          "created_utc": "2026-01-01 10:41:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx21423",
          "author": "Long_comment_san",
          "text": "Next 50-80b dense would be mindblowing. Someone, please. These total trillions of total parameters are irrelevant when there's a hook to the web.",
          "score": 4,
          "created_utc": "2026-01-01 09:11:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx278zf",
          "author": "NandaVegg",
          "text": "What near Gangnam Station for \"releasing all the intermediate checkpoints and wandbs\"? This is so weird. Can we dance together for a sped up ppongjjak? That would light the mood up. BTW I don't believe the claim that it's a finetune.",
          "score": 3,
          "created_utc": "2026-01-01 10:17:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx20648",
          "author": "yuumi_ramyeon",
          "text": "Popcorn",
          "score": 2,
          "created_utc": "2026-01-01 09:00:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8gatt",
          "author": "PerPartes",
          "text": "I've updated the post with a video link /and seen just a small part of it so far/",
          "score": 1,
          "created_utc": "2026-01-02 10:51:27",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nx4w0rc",
          "author": "7734128",
          "text": "What kind of a joke organization is this?\n\nEvery communication I've seen from them has been bodged like this.\n\nI don't need to inspect weights to know they're a scam when this is the quality of their PR statements.",
          "score": 0,
          "created_utc": "2026-01-01 20:28:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx24dhc",
          "author": "Super_Sierra",
          "text": "Show proof, not text. Idc about twitter post counterclaiming.",
          "score": -1,
          "created_utc": "2026-01-01 09:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx21fw9",
          "author": "Desperate-Sir-5088",
          "text": "Do not blaim the model without any proof. GLM-4.5-Air could count number of 'r' in the \"starbrerry\" correctly.Â \n\n\nWe usually called it \"deadcopy\"",
          "score": -6,
          "created_utc": "2026-01-01 09:14:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pvwlfh",
      "title": "systemctl disable ollama",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/8qvw6jdjih9g1.png",
      "author": "copenhagen_bram",
      "created_utc": "2025-12-26 05:30:55",
      "score": 231,
      "num_comments": 94,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvwlfh/systemctl_disable_ollama/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvzjzcu",
          "author": "ForsookComparison",
          "text": "Ollama's biggest sin for me is committing everyone new to the space to Q4 weights when I'm sensing that the larger community is finally starting to reconsider the last few years of *\"Q4 is a free speedup\"*",
          "score": 43,
          "created_utc": "2025-12-26 06:34:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzw00n",
              "author": "CheatCodesOfLife",
              "text": "Don't forget silently setting the context to 4096, and `ollama run deepseek-r1` giving people a particularly shitty Qwen finetune.",
              "score": 53,
              "created_utc": "2025-12-26 08:32:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw05dts",
                  "author": "paperbenni",
                  "text": "What on earth? Was the original called deepseek:r1 or how did that happen?",
                  "score": 3,
                  "created_utc": "2025-12-26 10:11:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw01hrz",
                  "author": "AiArtFactory",
                  "text": "Can't you change the context window and other settings via a Modfile?",
                  "score": 0,
                  "created_utc": "2025-12-26 09:30:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzsxrm",
              "author": "__Maximum__",
              "text": "What? What is the alternative?",
              "score": 2,
              "created_utc": "2025-12-26 08:00:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzud69",
                  "author": "Dogeboja",
                  "text": "QAT",
                  "score": 4,
                  "created_utc": "2025-12-26 08:15:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw0ufeh",
              "author": "EuphoricPenguin22",
              "text": "There is definitely a small performance hit in terms of capability, but Q4 is often the only way I can get a 20-25B model to fit inside my GPU for significantly faster generation speeds. MoE can take up some of the slack at full precision, but I usually prioritize higher parameter counts with quantization if I'm offloading anyway. For instance, I'd rather use a 100B model at Q4 than a 20-30B model with MoE at full precision if I'm accepting a generation speed hit from offloading.",
              "score": 2,
              "created_utc": "2025-12-26 13:49:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzei5s",
          "author": "Mabuse046",
          "text": "Yeah, ollama storing models at the system level is a huge reason why I won't touch it. I used ollama a little bit back when I first got into LLM's, later learned that they're just another project trying to wrap llama.cpp only they're doing it in the absolute shittiest way possible. I can always tell when someone still doesn't know much about LLM's when they are still using ollama. Kobold and Ooba have their uses occasionally, but there's no reason someone who knows what they're doing wouldn't just use llama.cpp directly. And even then that's for people who aren't just running transformers models in pytorch.",
          "score": 42,
          "created_utc": "2025-12-26 05:46:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw08jh8",
              "author": "venturepulse",
              "text": "whats the problem with storing at system level? just curious.\n\nIm comfortable with linux filesystem so dont see any difference. /usr/share/ollama/ seems just fine.",
              "score": 14,
              "created_utc": "2025-12-26 10:43:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw11913",
                  "author": "Mabuse046",
                  "text": "I have two NVMe drives, three SSD's and a large mechanical. I like to be able to keep my files where I want them, and I want to be able to access them with multiple programs. You download models with ollama, it puts them all in one place. Sure I could go out of my way to link that folder to a place on another drive, but then I'm still stuck with them having to be all in one place. Then on top of that, unless they've changed the way they're storing the files, last I used ollama they were storing the models as a bunch of cache chunks not as real GGUF's. Well what if I want to use Oobabooga or Kobold or just llama.cpp directly? I have to download an entire second copy of the model?",
                  "score": 6,
                  "created_utc": "2025-12-26 14:34:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3kdk6",
                  "author": "StephenSRMMartin",
                  "text": "There is nothing wrong with it. You can also change it to, say, var/lib like arch does. You can store them in a sub volume so they don't get snapshotted.\n\nPeople here will basically invent reasons to hate ollama, even though ollama's decisions make sense from a server standpoint. Storing them systemd wide means you can use system services and isolate them, service level permissioning, and sandboxing.\n\nI do the same in llama.cpp because I want it to be a system level service, not a user level service.",
                  "score": 1,
                  "created_utc": "2025-12-26 22:49:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzfdr2",
              "author": "copenhagen_bram",
              "text": "Lesson learned! Maybe I'll reinstall Alpaca on an user level and add remote providers.",
              "score": 4,
              "created_utc": "2025-12-26 05:53:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvzre2k",
                  "author": "Healthy-Nebula-3603",
                  "text": "Alpacacpp ? Sure ...",
                  "score": 1,
                  "created_utc": "2025-12-26 07:45:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzt578",
              "author": "moistiest_dangles",
              "text": "I've recently learned about this. What do you recommend as alternatives to ollama other than just raw dogging llama.cpp? Because that's just cli only right? Isn't the advantage of ollama that it allows you to use not just ram buy also disk memory? So are there other options which harness multiple memory types?",
              "score": 1,
              "created_utc": "2025-12-26 08:02:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzu2ix",
                  "author": "theUmo",
                  "text": "LM Studio is nice.",
                  "score": 7,
                  "created_utc": "2025-12-26 08:12:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvztavc",
                  "author": "copenhagen_bram",
                  "text": "I kinda like llamafile",
                  "score": 2,
                  "created_utc": "2025-12-26 08:04:28",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw15wqj",
                  "author": "Schlick7",
                  "text": "llama.cpp launches its own webui when you start the server. and it recently added model switching so it's basically everything you need. You can also easily hookup something like OpenWebUi",
                  "score": 2,
                  "created_utc": "2025-12-26 15:02:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw0srl7",
                  "author": "Desm0nt",
                  "text": "Llama.cpp has it's own ui too, not just cli, afaik. And you clearly can use almost any gui (including openwebui) because almost all of them wirks fine with default OpenAI-like api",
                  "score": 2,
                  "created_utc": "2025-12-26 13:38:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw02x6q",
                  "author": "Mabuse046",
                  "text": "I'm not sure I understand the question. Disk memory? You mean like virtual memory? You save your model file to disk and then when you want to run it you load it to gpu vram and cpu ram together. If you have virtual memory, or (I use Linux) swap, it will use that as a fallback but it will be really slow. I've never tried to skip my vram and ram and go straight to using a disk for memory but if you can do it in ollama, then when I say ollama uses llama.cpp to load the models, logically that means llama.cpp has to be able to do it, since it's the one doing the actual work in either case, right?\n\nAll ollama does is download the model for you and save it in a hidden location and then when you want to run it, it runs llama.cpp with the appropriate flags to load it.\n\nYou can do the same thing. Go on huggingface, download your model, save it wherever you want on your hard drive, and then load it with llama.cpp when you want to run it. The only difference is, you aren't getting it buried in some hidden cache on your drive, you actually get to decide where it's saved. But it's still the same program loading it when you want to run it, ollama just fills in the command line arguments for you, whereas when you raw dog llama.cpp you have to type them in yourself.",
                  "score": 1,
                  "created_utc": "2025-12-26 09:45:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw0mj93",
          "author": "CV514",
          "text": "As koboldcpp enjoyer I'm confused why inference software needs to be a system service",
          "score": 9,
          "created_utc": "2025-12-26 12:52:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzjh38",
          "author": "International-Try467",
          "text": "Obligatory fuck Ollama.",
          "score": 48,
          "created_utc": "2025-12-26 06:29:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzjsrx",
              "author": "copenhagen_bram",
              "text": "I've heard this before, but today is the day I start listening",
              "score": 13,
              "created_utc": "2025-12-26 06:32:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvzkb14",
                  "author": "International-Try467",
                  "text": "Kobold is an idiot proof launcher btw, Llama.cpp is the original though",
                  "score": 13,
                  "created_utc": "2025-12-26 06:37:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzsshq",
              "author": "WildDogOne",
              "text": "dunno, switched over to llamacpp yesterday and immediately ran into model routing issues aaaand back to ollama. Yes its slower, but it works\n\nedit: love how people are upset that people don't use something that doesn't work ;)",
              "score": -6,
              "created_utc": "2025-12-26 07:59:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzxvzu",
                  "author": "International-Try467",
                  "text": ">koboldcpp is idiot proof",
                  "score": 7,
                  "created_utc": "2025-12-26 08:52:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzmq7i",
          "author": "garloid64",
          "text": "certified Coallama Moment",
          "score": 5,
          "created_utc": "2025-12-26 06:59:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0k5oq",
          "author": "Outrageous_Cap_1367",
          "text": "Home directory? I suggest backing up ONLY the home directory and excluding the ollama directory",
          "score": 3,
          "created_utc": "2025-12-26 12:33:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzgibj",
          "author": "StewedAngelSkins",
          "text": "skill issue? don't include object store directories in your snapshots. fyi if you use docker you should exclude its blob storage too, for the same reason.",
          "score": 10,
          "created_utc": "2025-12-26 06:03:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzk4n8",
              "author": "ForsookComparison",
              "text": "> don't include object store directories in your snapshots\n\nAnyone willing to configure this and especially anyone who uses Docker has no reason to be using Ollama.",
              "score": 13,
              "created_utc": "2025-12-26 06:35:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0bznr",
                  "author": "copenhagen_bram",
                  "text": "I don't understand your comment. I had no problem excluding the offending directories in the Timeshift GUI once thread OP pointed out I could do that.",
                  "score": 3,
                  "created_utc": "2025-12-26 11:18:14",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw23nkk",
                  "author": "StewedAngelSkins",
                  "text": "docker is like the most entry-level way to run any kind of self hosted service and is also how most people use ollama, so i really don't see your point.",
                  "score": 1,
                  "created_utc": "2025-12-26 18:04:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzheh7",
              "author": "copenhagen_bram",
              "text": "Good idea! Just excluded the two problematic directories. Glad I know I can do that now.\n\nJust remember, you wouldn't have this funny meme if I didn't have a skill issue :)\n\nI still need to figure out where Docker blobs go so I can exclude those",
              "score": 4,
              "created_utc": "2025-12-26 06:11:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw23vut",
                  "author": "StewedAngelSkins",
                  "text": "iirc they're under `/var/lib/docker`",
                  "score": 1,
                  "created_utc": "2025-12-26 18:05:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzszhc",
          "author": "__Maximum__",
          "text": "You can change the directory",
          "score": 3,
          "created_utc": "2025-12-26 08:01:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzuaqe",
              "author": "theUmo",
              "text": "Yeah, but it doesn't use it in all instances. The last straw for me was when some IDE integration could only use Ollama, but it made API calls that would auto-download to Ollama's default directory.",
              "score": 3,
              "created_utc": "2025-12-26 08:14:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1f1yr",
          "author": "jacobpederson",
          "text": "Fun fact.  LM studio won't launch if the disk is full :D",
          "score": 1,
          "created_utc": "2025-12-26 15:54:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw372ko",
          "author": "rm-rf-rm",
          "text": "The shittier part is their unnecessarry (or devious way to trap you in their app) hashing of model names. \n\nThank god more people are waking up to their BS and leaving.",
          "score": 1,
          "created_utc": "2025-12-26 21:36:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7dplf",
          "author": "cosimoiaia",
          "text": "rm -rf /user/share/ollama\n\nMuch better.",
          "score": 1,
          "created_utc": "2025-12-27 15:43:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhyqqf",
          "author": "Sicarius_The_First",
          "text": "\\>uses ollama  \n\\>disables it  \n\\>redemption arc: complete",
          "score": 1,
          "created_utc": "2025-12-29 05:36:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi3vgv",
              "author": "copenhagen_bram",
              "text": "haven't really used it in a while, am probably gonna use llamafile, kobold, or just rawdog llama.cpp for once",
              "score": 2,
              "created_utc": "2025-12-29 06:17:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw13nie",
          "author": "extopico",
          "text": "Itâ€™s a feature not a bug. One of the lead devs told me, and also told me to wreck my system permissions if I wanted to move the model store to a separate drive. I uninstalled and scrubbed my machine of that POS and continued using llama.cpp",
          "score": 1,
          "created_utc": "2025-12-26 14:49:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0tqfk",
          "author": "IrisColt",
          "text": "I ditched Ollama two weeks ago, probably my rite of passage out of noobhood, heh... llama.cpp feels like a whole new universe, and itâ€™s way faster and more capable.",
          "score": 0,
          "created_utc": "2025-12-26 13:45:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pvgell",
      "title": "Train a 4B model to beat Claude Sonnet 4.5 and Gemini Pro 2.5 at tool calling - for free (Colab included)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pvgell/train_a_4b_model_to_beat_claude_sonnet_45_and/",
      "author": "DecodeBytes",
      "created_utc": "2025-12-25 16:05:14",
      "score": 202,
      "num_comments": 52,
      "upvote_ratio": 0.93,
      "text": "Using Open Source DeepFabric, a tool that lets you:\n\n1. Pick any MCP server or any given set of Tools\n2. A specific root topic (DevOps, Customer Care, Coding Agent)\n3. Auto-generate a tool calling / reasoning topic specific dataset, with real tool traces executed within isolated webassembly components.\n4. Fine-tune an SLM to become an expert at that specific MCP server using Unsloth's awesome training framework\n5. Evaluate against a training-blind subset of the dataset.\n\nWe trained Qwen3-4B to outperform Claude Sonnet 4.5 and Gemini Pro 2.5 against the more challenging to use Blender MCP server.\n\n|Model|Score|\n|:-|:-|\n|DeepFabric Fine Tuned|93.50%|\n|Claude Sonnet 4.5|80.50%|\n|Google Gemini Pro 2.5|47.00%|\n\n**The idea is simple:** frontier models are generalists, but a small model fine-tuned on domain-specific tool calling data can become a specialist that beats them at that specific task.\n\n\n\nhttps://preview.redd.it/x6svlmqird9g1.png?width=2816&format=png&auto=webp&s=e44c8203ce3d7383951397b5ae5b33870ceab7e0\n\n\n\n**Try it yourself on Google Colab using a Free T4:** [https://colab.research.google.com/drive/1EG1V40v5xkJKLf6Ra6W4378vYqlZNVWq](https://colab.research.google.com/drive/1EG1V40v5xkJKLf6Ra6W4378vYqlZNVWq)\n\n**GitHub:** [https://github.com/always-further/deepfabric](https://github.com/always-further/deepfabric)\n\nWould love feedback from the community, especially if you decide to generate your own agent.",
      "is_original_content": false,
      "link_flair_text": "Tutorial | Guide",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvgell/train_a_4b_model_to_beat_claude_sonnet_45_and/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvvw4xm",
          "author": "swarajs16",
          "text": "can you share the weights or gguf model of the fine tuned model?",
          "score": 22,
          "created_utc": "2025-12-25 16:08:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvxrsx",
              "author": "DecodeBytes",
              "text": "The GGUF is below, but its fresh out of training and not had a chance to really sanity check it yet, if you hit any quirks , let me know \n\nmodel: [https://huggingface.co/alwaysfurther/deepfabric-blender-mcp-gguf](https://huggingface.co/alwaysfurther/deepfabric-blender-mcp-gguf)\n\ndataset: [https://huggingface.co/datasets/alwaysfurther/deepfabric-blender-mcp](https://huggingface.co/datasets/alwaysfurther/deepfabric-blender-mcp)",
              "score": 25,
              "created_utc": "2025-12-25 16:18:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvwnbbi",
              "author": "Global-Ball-3430",
              "text": "The colab notebook should have the model export steps at the bottom but if they didn't include the weights that's kinda sus for a research post",
              "score": -2,
              "created_utc": "2025-12-25 18:49:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvx2xhr",
                  "author": "DecodeBytes",
                  "text": "you want theLoRA adapter weights  ? I have never had that ask before, especially with the actual training data being open in the first place.",
                  "score": 9,
                  "created_utc": "2025-12-25 20:22:36",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvw5zg4",
          "author": "Bakkario",
          "text": "You have given me a great hope on a similar project I wanted to do for tool calling and CoT SLM model as well. \n\nDo you think we can apply the same concept for a programming language specifically like for example python or JavaScript?",
          "score": 14,
          "created_utc": "2025-12-25 17:08:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwajv7",
              "author": "DecodeBytes",
              "text": "Absolutely, all you need to do is set the topic seed to the domain you want to cover and specifiy the Tools and you have your dataset:\n\nWe could easily adapt this to JS:\n\n[https://github.com/always-further/deepfabric/blob/main/examples/coding-agent.yaml](https://github.com/always-further/deepfabric/blob/main/examples/coding-agent.yaml)\n\nHow DeepFabric works is you set prompt (I don't want to use the word vibe-coding, but it is that easy) and DeepFabric builds a graph of sub-topics, getting more and more detailed as the DAG propagates out. Each one of these nodes and a handful of tools will be used to create a single sample. What's nice about this, is that you have lots of diversity, but you remain on topic (javascript) which reduces the risk of overfit which a lot of other synthetic tool generators fail at.\n\nHere is an example focused around platform engineering and devops: [https://huggingface.co/datasets/alwaysfurther/deepfabric-devops-with-tools](https://huggingface.co/datasets/alwaysfurther/deepfabric-devops-with-tools)\n\nI tell you what, if you PM me or jump on our discord I would happily collaborate with you to build a javascript agent.",
              "score": 5,
              "created_utc": "2025-12-25 17:35:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvwqew2",
                  "author": "GoodSamaritan333",
                  "text": "How aboout rust?",
                  "score": 2,
                  "created_utc": "2025-12-25 19:07:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvxgw5k",
                  "author": "Bakkario",
                  "text": "Mark my username!! \n\nI will certainly do that after the holidays. I am so pumped right now. Will use the time for now to read more about fabric. It came across my lane couple of times, but didnâ€™t give it much attention unfortunately ðŸ™ðŸ¾",
                  "score": 1,
                  "created_utc": "2025-12-25 21:48:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvy0b5g",
                  "author": "jazir555",
                  "text": "If you could do this with WordPress PHP that would be so rad",
                  "score": 1,
                  "created_utc": "2025-12-25 23:52:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwiedl",
          "author": "ZealousidealShoe7998",
          "text": "this is the way.  \nmost people don't need a 500B parameter model to achieve good results.  \nI think the future is small parameter models like 30B max that are highly trained on using tools.  \nnow you can have cheap llms running doing easy bug fix by running tools that are deterministic.",
          "score": 11,
          "created_utc": "2025-12-25 18:21:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwkzua",
              "author": "DecodeBytes",
              "text": "Thanks [ZealousidealShoe7998](https://www.reddit.com/user/ZealousidealShoe7998/), I also agree - the future is open, small energy efficient models - with diversity of training data and tooling that nurtures open innovation and sharing within a global communiity!",
              "score": 8,
              "created_utc": "2025-12-25 18:36:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwdv9g",
          "author": "Nishkama-Karma",
          "text": "Nice work. Using Blender MCP is a real stress test.\n\nQuick qâ€™s:\n\n* How are you scoring â€œtool call successâ€ exact arg match, partial credit, or task completion?\n* Did the DAG ever drift off-topic during synth gen? Any caps or checks to avoid overfit?\n\nAlso, did Qwen3â€‘4B need special prompt scaffolding for multiâ€‘step calls, or were plain schemas + retries enough?",
          "score": 3,
          "created_utc": "2025-12-25 17:54:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwgfkg",
              "author": "DecodeBytes",
              "text": "Hi!\n\n* How are you scoring â€œtool call successâ€ exact arg match, partial credit, or task completion?\n\nTwo ways, first does it call the correct tool, so `get_weather` and does it do so in the correct format (we derive the tool calling tags from the models chat template, e.g. for qwen `<tools></tools>` XML tag.\n\nSecondly we validate the tool parameters `{\"location\": \"Tokyo\"}`, so we would expect to see the model call:\n\n`<tool_call>`  \n`{\"name\": \"get_weather\", \"arguments\": {\"location\": \"Tokyo\"}}`  \n`</tool_call>`\n\nDuring dataset generation the teacher model used has to call specific real live tools, or else its giving a real stack trace and needs to try again. This means we know the dataset and evals do not have any hallucinations, or a at least tools that are grounded in reality. It also cannot 'time travel' which I experience a lot when asking the teacher to mock tools, for example it would try read a file, before writing to the file.\n\nThis is also just the start of it, we are also looking to bring in tool execution at training time using RL - and for the evals we will start leaning more into the semantics of what the model produces!\n\n* Did the DAG ever drift off-topic during synth gen? Any caps or checks to avoid overfit?\n\nI have not seen it, but will also be honest in that we still plan to do some research sprints looking into graph diversity - especially when you build huge DAGs (do they start to repeat after a while). \n\n\\> Also, did Qwen3â€‘4B need special prompt scaffolding for multiâ€‘step calls, or were plain schemas + retries enough?\n\nA little bit, but rely on outlines (constrained decoding) and heavy use of pydantic to validate everything - last case we retry, but I don't like that and try to really avoid it, as its wasted pennies. Having said that, some of the small models are really good , gemma has lovely adherence to structure outputs.\n\nI answered the wrong question, but will leave this here anyway:\n\nNot really, we just had a very simple system prompt, well, second guessing myself, we may have even just stuck with qwens default from the chat template:\n\nedit: we inject the tools array into the chat template:\n\n    text = tokenizer.apply_chat_template(\n            messages,\n            tools=tools,\n            tokenize=False\n        )",
              "score": 2,
              "created_utc": "2025-12-25 18:09:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw5tok0",
          "author": "NAGS-brief",
          "text": "I'll try",
          "score": 3,
          "created_utc": "2025-12-27 08:24:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8r7w6",
              "author": "Emergency-Associate4",
              "text": "Did you try? Curious to know what you think",
              "score": 2,
              "created_utc": "2025-12-27 19:55:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvyupri",
          "author": "eleqtriq",
          "text": "You have to have an api key to use this?",
          "score": 2,
          "created_utc": "2025-12-26 03:15:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0eqwf",
              "author": "DecodeBytes",
              "text": "If you use openai , anthropic or gemini and some of the openrouter models - for anything local, no api key is needed as we support ollama.",
              "score": 1,
              "created_utc": "2025-12-26 11:44:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw1fyhb",
                  "author": "eleqtriq",
                  "text": "I meant your service.  I see the examples have API keys to your service.",
                  "score": 1,
                  "created_utc": "2025-12-26 15:59:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw0mp2s",
          "author": "bhupesh-g",
          "text": "This is so sweet, I was always wondering why yet no good small models like a model which excels in JS and ReactJS. Its small but can nail most problems in that space.  I always wanted to do such thing but lack of knowledge in this particular domain couldn't. Good to see community is moving in that direction. Great work, keep it up !!",
          "score": 2,
          "created_utc": "2025-12-26 12:54:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0nn7x",
              "author": "DecodeBytes",
              "text": "Hi Bhupesh, you're welcome to raise a react model request: [https://github.com/always-further/deepfabric/discussions/categories/model-request](https://github.com/always-further/deepfabric/discussions/categories/model-request)",
              "score": 2,
              "created_utc": "2025-12-26 13:01:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw4pgcl",
          "author": "Analytics-Maken",
          "text": "The idea makes a lot of sense, especially considering token efficiency from paid models. I've been using them to develop analytics from multiple data sources, consolidating them with ETL tools like Windsor ai, but I often hit Claude caps if I connect various MCP servers.",
          "score": 2,
          "created_utc": "2025-12-27 03:03:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvx2bxg",
          "author": "xXWarMachineRoXx",
          "text": "What if you train the big model like you did the small one , wouldnâ€™t that be a fair comparison\n\n\nAlthough i get that its more efficient, local ( depending on your config) and cheaper / preferable to the members of the sub who need to optimise and get the last of performance but for those who do have credits/ big hardware - how big of a performance gain are they getting?\n\nEdit : love the work, would definitely check it out. It could be absolute bonkers for r/robotics or g 1 people trying to fit it in a small factor like ai glasses/ VR or phones",
          "score": 2,
          "created_utc": "2025-12-25 20:18:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvx81i4",
              "author": "DecodeBytes",
              "text": "Hi! If I am not mistaken a larger model will perform even better, but need a bit more GPU time and a bigger dataset to cover a good chunk of trainable parameters to make an impact. Its all doable though and I plan on putting out a 30b pipeline next (either qwen or nemotron, but open to suggestions)\n\nGiving time we will be benchmarking all manner of sizes - especially as we dial in the approach more over time.",
              "score": 2,
              "created_utc": "2025-12-25 20:54:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw12r43",
          "author": "SnooPeripherals5313",
          "text": "Neat idea. Are you picking Qwen as a base SLM because it comes with a decent baseline tool calling performance? Would love to see some dummy metrics",
          "score": 1,
          "created_utc": "2025-12-26 14:43:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2fe5c",
              "author": "DecodeBytes",
              "text": "out of habit really sloo, i have always grabbed qwen - but any SLM should do. we do plan to launch a service for collecting metrics if you're interested in getting a preview?",
              "score": 2,
              "created_utc": "2025-12-26 19:05:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1ha0m",
          "author": "ridablellama",
          "text": "yes! i have a mcp stack ive wanted to train a small 8b model to use it flawlessley.",
          "score": 1,
          "created_utc": "2025-12-26 16:06:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2fgzh",
              "author": "DecodeBytes",
              "text": "nice! let me know how you get on if you need any help!",
              "score": 1,
              "created_utc": "2025-12-26 19:05:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2tmqb",
          "author": "yoracale",
          "text": "This is awesome thanks for sharing",
          "score": 1,
          "created_utc": "2025-12-26 20:22:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2vx3v",
          "author": "Emergency-Associate4",
          "text": "I was excited to try this but I'm running into issues with the config files or command switches that no longer exist.",
          "score": 1,
          "created_utc": "2025-12-26 20:35:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw39n7h",
              "author": "DecodeBytes",
              "text": "My bad, there has been a fair few changes and the docs may not be on-par! Do you want to jump onto discord and would be happy to help out. Discord link is on the repo.",
              "score": 1,
              "created_utc": "2025-12-26 21:49:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw3weak",
                  "author": "Emergency-Associate4",
                  "text": "It would be nice to know how to get started :).",
                  "score": 1,
                  "created_utc": "2025-12-27 00:01:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3l5vl",
              "author": "DecodeBytes",
              "text": "ok, I just did a large sweep and fixed up a few things that have changed - it should be good now, if not happy to support you",
              "score": 1,
              "created_utc": "2025-12-26 22:54:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwyctu",
          "author": "zhambe",
          "text": "Playing with something not similar, but with a similar goal in mind -- small specialist models to navigate well-defined domain problems. At this point I'd even say MCP is overkill (at least in my case) and finetunes seem more promising / simpler.",
          "score": 1,
          "created_utc": "2025-12-25 19:54:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0ewx7",
              "author": "DecodeBytes",
              "text": "That's interesting, would love to learn more and see your progress. I tend to think of MCP as more of a standard way of building tools, more than anything unique , but it does expand a lot over time.",
              "score": 1,
              "created_utc": "2025-12-26 11:46:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw14x8b",
                  "author": "zhambe",
                  "text": "I'm not quite ready to share what I'm building, but I appreciate the interest! \n\nI look at MCP as a prototyping tool -- flexible, but unwieldy at scale. Unless you're working on something permanently generalist, you're likelty to discover repeated workflows and logic paths, where you no longer need the flexibility, because you're working with a narrowed ranges of schemas / data sources, APIs etc. Then, old-fashioned deterministic code is vastly superior.",
                  "score": 1,
                  "created_utc": "2025-12-26 14:56:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxp4mp",
              "author": "WitAndWonder",
              "text": "MCP or API is extremely useful if you need to work with fluid data.",
              "score": 0,
              "created_utc": "2025-12-25 22:39:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvy0zzk",
                  "author": "zhambe",
                  "text": "Is \"fluid data\" the main issue, or unexpected execution paths? In my experience MCP is handy during kind of \"discovery\" work / prototyping, but once most of the paths are known, the advantages vanish while the overhead remain.",
                  "score": 0,
                  "created_utc": "2025-12-25 23:56:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwpqja",
          "author": "q5sys",
          "text": "Any issue with using this on a model we already previously finetuned in the past?  I'd like to update and enhance a model I finetuned a while ago specifically, [https://huggingface.co/BallisticAI/Ballistic-CodeLlama-34B-v1](https://huggingface.co/BallisticAI/Ballistic-CodeLlama-34B-v1) but I'd like to train/finetune it further specifically for python use cases.",
          "score": 0,
          "created_utc": "2025-12-25 19:03:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwxes6",
              "author": "DecodeBytes",
              "text": "Its a little difficult to be sure, as it depends on the previous dataset and how many weights were trained with lora (assuming it was lora), but I don't feel a high confidence anything would go wrong at all!",
              "score": 1,
              "created_utc": "2025-12-25 19:49:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvx00a3",
                  "author": "q5sys",
                  "text": "Thanks for the reply.  If I get time this holiday season I might give it a whirl.",
                  "score": 0,
                  "created_utc": "2025-12-25 20:04:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwq139",
          "author": "BeginningReveal2620",
          "text": "Cool project excited to dive in thanks for sharing",
          "score": 0,
          "created_utc": "2025-12-25 19:04:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwxgpo",
              "author": "DecodeBytes",
              "text": "Thanks , do jump into discord if you need support or have any ideas.",
              "score": 1,
              "created_utc": "2025-12-25 19:49:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvy2874",
          "author": "rm-rf-rm",
          "text": "Great youve optimized a fundamentally bad approach twice over (first is MCP and second is fine tuning to use MCP).\n\nWhat is far superior, if youre doing fine tuning, is to fine tune on the actual API and docs itself. Then have the SLM write API calls directly. Why have the MCP anymore? MCP made some sense specifically to address the fact that general LLMs do not have sufficient knowledge of specific services and MCP injects the required context they need.",
          "score": -2,
          "created_utc": "2025-12-26 00:04:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvygd9c",
              "author": "harrro",
              "text": "It's a tool calling finetune.\n\nMCP isn't required for that. What are you on about?",
              "score": 3,
              "created_utc": "2025-12-26 01:36:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw0fdva",
              "author": "DecodeBytes",
              "text": "You might be getting mixed up here. We don't fine tune on MCP, we fine tune on function calls and their parameters. \n\nIt just so happens we make it easy to import the list of tools / function calls from an existing MCP server, as a lot of folks use them -  but at the end of it all as far as the model is concerned we are just getting it to improve its ability to predict the natural language of a function name and its parameters - what stack, standard or protocol that function belongs to (openai , MCP, langchain etc) is immaterial",
              "score": 4,
              "created_utc": "2025-12-26 11:50:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzt1q8",
      "title": "LLM server gear: a cautionary tale of a $1k EPYC motherboard sale gone wrong on eBay",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pzt1q8/llm_server_gear_a_cautionary_tale_of_a_1k_epyc/",
      "author": "__JockY__",
      "created_utc": "2025-12-30 20:36:46",
      "score": 194,
      "num_comments": 81,
      "upvote_ratio": 0.93,
      "text": "or: selling high-end LLM server gear is more fraught with risk than I realized.\n\n### AI Disclosure\n\nThis was written entirely by hand on my laptop in Sublime Text with zero AI involvement. Shit, I didn't even use spell check. All mistakes are my own.\n\n### tl;dr \n\nDuring an \"Item Not As Described (INAD)\" dispute, eBay ALWAYS sides with the buyer until the very last steps of the case no matter what the circumstances, despite all evidence, and in the face of all immediately obvious reason, logic, and common sense. Except it makes perfect sense and you might not even lose your money. Allow me to elaborate.\n\n### The Sale\n\nRewind to October 2025 when I replaced the incumbent Gigabyte MZ33-AR1 Epyc Zen5 motherboard with a Supermicro H14SSL-N for my inference rig. Long story short: don't use Gigabyte motherboards for 4-way Blackwell GPU setups unless sado-masochism is your thing. Anyway, I sold it to a seemingly nice chap on eBay for $900. He seemed a bit clueless about Epyc and compatibility issues, but we exchanged messages and he decided to go ahead with the \"no returns\" purchase of the as-new MZ33-AR1.\n\nOriginal box. All the case candy. As new. Undamaged. Fully working. With hi-res photos (taken on a Nikon D7000 with Nikon 17-55 f2.8 glass and processed in Capture One Pro) of all areas of the motherboard and CPU socket. This is important. \n\n### The Buyer\n\nFast forward a week or so: buyer hits me up with a bunch of Dr Debug codes (although he doesn't know they're Dr Debug codes, he just pulled \"error codes\" from the BMC) claiming the motherboard won't boot. I did him the solid of explaining Dr Debug and I provided a link to an explanation of the codes (https://forum.level1techs.com/t/list-of-dr-debug-bios-codes/114364). He was having issues with CPU initialization. I told him that sometimes re-seating CPU and RAM can help with these sorts of issues.\n\nRe-seating. This is also important.\n\nNext day he hits me up again: will I accept a return? No, because having installation difficulties is not a valid reason for return. Then nothing. Silence.\n\n### The Refund Claim\n\nCue the *very last day of the return window*: I get hit with an \"item not as described\" refund claim. Get this, the buyer:\n\n- uploaded photos of the motherboard with a bent and twisted CPU pin.\n- uploaded a photo of a blank white silkscreen rectangle on the motherboard with a giant red arrow pointing to it and a comment saying \"the motherboard is fake because of this white area\".\n- showed a photo of the computer monitor displaying the BMC interface in which the serial number of the BMC software was 1234567890ABCDEF. He claimed therefore the motherboard was a fake.\n\nWTF. I simultaneously exploded with rage at being accused of selling broken gear as working gear, while exploding with incredulity at the stupidity of trying to assert both damage AND blatantly ridiculous fakery in the same refund claim! My dude should have really picked just one fraudulent claim to keep it somewhat realistic, not two. I calmed down and figured the buyer probably bent the pins in a ham-fisted attempt to re-seat everything. No problem, I thought. I'll explain to eBay what's happening and they'll see reason before shutting this clown down. So I started going through the claim dispute process...\n\n### The Process\n\n...oh, the process. It's designed to (a) refund the buyer at the seller's cost in all cases, (b) be so egregiously demoralizing, time-consuming, and administratively difficult for sellers that they are incentivized to simply give up and accept the fleecing, and (c) automate as much of this process with as few humans in the loop as possible while simultaenously providing as few opportunities as possible for sellers to initiate any communication with eBay.\n\nIt went like this over a period of TWO MONTHS:\n\n- Report the buyer for \"abusing the returns process\".\n- With the new \"case\", it's possible to upload a set of photos and a block of text to refute the buyer's claim(s). \n- I uploaded ALL the hi-res photos I took for the listing's photoshoot in which it was abuntandly clear the motherboard was in perfect condition.\n- I also went to Gigabyte and found the page on the BMC's usermanual containing a screenshot showing the same serial number claimed by the buyer.\n- I went to Gigabyte's MZ33-AR1 web page and found a photo of the motherboard showing exactly the same white rectangle the buyer had called out as fakery.\n- Boom! Done! Solid documentary refutation of all the buyer's claims. Case closed. So I thought.\n- eBay found in favor of the buyer and instructed me to issue a return label.\n- I refused, outraged. No, I said. Look at the photos! He's lying!\n- eBay sent the buyer a label at my expense. He returned the motherboard with its busted CPU pin.\n- I again reported the buyer, showed photos of before and after damage, clearly showing he did the damage, not me.\n- eBay found in favor of the buyer AGAIN and deducted the full cost of the refund from my account.\n- Apoplectic, I hit the \"appeal\" button. I was taken to a webpage that said \"we'll call you in 3 minutes\". WTF?\n- 5 minutes later i got a call from eBay. \n- After briefly explaining the situation to a very engaged US-sounding representative, she told me I needed to do a couple of things:\n\t- Take the text of an email they just sent me (a Disclosure where I swear everything I told eBay is true) and paste it into a Word doc\n\t- Insert a photo/picture of my ink-written signature (luckily I have a scan of exactly that for business reasons).\n\t- Convert to PDF and upload to the secret link in the email they sent.\n\t- No joke, the lady actually stayed on the phone while I did all this! She received the PDF just seconds after I uploaded it.\n\t- This is, I am sure, mostly just another way of making it difficult to actually reverse the appeal.\n- But the rep was good to her word: eBay immediately reversed the decision and the money is back in my account as if the sale had happened like normal. I guess both me and the buyer got our money.\n\n### If It Happens To You\n\nMy advice if this happens to you: \n\n- Accept that no human cares about your case until the very, very last minutes of MONTHS of effort.\n- Accept that no matter what you do eBay will always automatically find in favor of the buyer.\n- Document everything contemporaneously and upload everything you possibly can when given opportunity to do so; you won't get any opportunities to do so again.\n- The data you upload is designed only for the human at the end of the appeals process, not someone looking at it during the claim process. Make it good. You'll need it later.\n- You're going to get enraged because during the claims process \"nothing makes sense\". It all makes sense: it's simply the cheapest way for eBay to handle this process at scale. Keep going.\n- Eventually eBay will find in favor of the buyer and close the case, automatically refunding the buyer \"on your behalf\". You will lose your money.\n- At this point you get the chance to appeal. BE READY. *This is the shot you've been waiting for all this time!* Have your phone, your laptop, your scanned signature, and a way to make PDFs ready BEFORE you initiate the \"call me\" feature.\n- Calmly explain what happened and request that common sense prevail. Ask that they refund your money. Common sense may actually prevail, assuming you made a good contemporaneous case with solid photographs, etc... and assuming you presented it well (not Mr Angry) on the phone... oh, and provided you can make and upload a PDF of your signature on-the-fly during the call!\n\nGood luck!\n\nEdit: please stop sending DMs asking for the eBay handle of the buyer. I'm not in the business of doxxing anyone. Thank you.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzt1q8/llm_server_gear_a_cautionary_tale_of_a_1k_epyc/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwt9lqp",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 22:30:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwspnh9",
          "author": "ismaelgokufox",
          "text": "My god. Being a seller in eBay is no joke.",
          "score": 78,
          "created_utc": "2025-12-30 20:55:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsrr86",
              "author": "blbd",
              "text": "Actually I think this data suggests that not only is it a joke, but that even an honest seller is the butt of it.Â ",
              "score": 52,
              "created_utc": "2025-12-30 21:05:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwv90k1",
              "author": "Guinness",
              "text": "Yep. eBay literally helped a scammer in South America steal my DSLR camera. The only reason I had my case reversed was because I had a quant friend from my days at an HFT firm end up working for a company that got absorbed by eBay. \n\nHe saved my ass.",
              "score": 8,
              "created_utc": "2025-12-31 05:30:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwuzqjv",
              "author": "az226",
              "text": "Being a buyer is no joke either went they send you faulty stuff. Had a seller sell me good GPUs and trusted him. Return window passed and then he had started sending me bent pins GPUs. 9 in total. Like $7k. \n\nAnother seller here on reddit Rhino/Core4 sold me GPUs with bad memory. I think 11 of 31 were bad or something. $8k. Refused to believe it, claimed I didnâ€™t know what I was doing. Scam of a company. They also sell on eBay. Avoid at all costs. They offer was something like $200 store credit if I returned the GPUs. Hah. Morons. Then they got their feelings hurt and took back that shit offer. When I have time and energy Iâ€™ll take them to small claims court. Losers.",
              "score": 8,
              "created_utc": "2025-12-31 04:26:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsto3p",
          "author": "MrPecunius",
          "text": "This is consistent with the experience I had selling a three channel stepper motor driver board on Ebay many years ago. Obvious buyer-inflicted damage, easily seen in photos etc.\n\nJust like your case, Ebay refunded the fraudster and I ended up keeping my money too. I stopped selling on Ebay after that.",
          "score": 34,
          "created_utc": "2025-12-30 21:14:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuhpri",
              "author": "night0x63",
              "text": "(I don't have the whole story... I just use eBay casually as buyer and little selling. )\n\n\nI think in the beginning of eBay before Amazon and other online stuff... Buyers got screwed by fraudsters all the time. eBay still made good money.\n\n\nThen Amazon came... Buyers all moved there because no more hours of dealing with fraudster sellers. eBay core business at risk.\n\nThen more recently last approximate five ten years... eBay pendulum has swung heavily... Now eBay competes with Amazon and so heavily favors buyers. So your story and this OP story show eBay favoring buyers. because eBay competes with Amazon.\n\n(Again I don't have all the story... This is just my experience and observation.)",
              "score": 5,
              "created_utc": "2025-12-31 02:35:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3lvrj",
                  "author": "maz_net_au",
                  "text": "Amazon isn't much better. Search for scams where people receive a bag of sand / rocks which are the same weight as the original item. Amazon processes it's returns based on the weight of the item if its in shrink-wrap, even if the original item didn't come in shrink-wrap.\n\nAnyway, good luck getting your money back with a dispute \"Instead of an item, I received a bag of sand.\" Some dude years ago filming himself unboxing what was meant to be a very expensive digital camera only to be on the receiving end of this crap and Amazon only refunded their money after public backlash (a video of unboxing the item is apparently not evidence enough).",
                  "score": 2,
                  "created_utc": "2026-01-01 16:35:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwujr05",
              "author": "__JockY__",
              "text": "Shame, really. But.... I too will avoid selling high value items on eBay in future, I have no desire to burn so many hours on pointless stress.",
              "score": 3,
              "created_utc": "2025-12-31 02:47:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwv1w1g",
                  "author": "jeffwadsworth",
                  "text": "This.  It just isn't worth the stress at all.",
                  "score": 2,
                  "created_utc": "2025-12-31 04:40:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwt31br",
          "author": "a_beautiful_rhind",
          "text": "I stopped selling on ebay because the customers have carte blanche to rip you off. He could have sent you a brick and you'd still have to contact a freaking US rep to get any traction.\n\nI think big time sellers just eat the fraudsters as part of doing business, but as an individual, you really can't. You barely make anything after the fees as it is and ebay seems to think mailing things back and forth is free.\n\nIts an absolute miracle you were able to keep your money at all.",
          "score": 21,
          "created_utc": "2025-12-30 21:58:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwst6ps",
          "author": "HelpingForDoughnuts",
          "text": "Man, the part where you had to stay on the phone making a PDF with your scanned signature while the rep waitedâ€¦ thatâ€™s peak â€œwe made this intentionally annoying so people give up.â€ Kafka would be proud.\nI went through something similar selling a 3090 a couple years back. Buyer claimed it was â€œartifactingâ€ and sent photos that were clearly just him running Furmark with the OC slider maxed. eBay sided with him, I got the card back with thermal pads missing (???), and I justâ€¦ gave up. Didnâ€™t know about the appeal phone call thing. Wish I had.\nThe real lesson here is what you said about documentation. Photos arenâ€™t just for the listing, theyâ€™re evidence for the trial you donâ€™t know is coming yet. I photograph serial numbers now too after getting burned.\nAlso lol at the buyer trying to claim BOTH damage AND fakery. Pick a lane my dude.\nSidebar: howâ€™s the Supermicro treating you with the Blackwell setup? Been eyeing the H14 boards but havenâ€™t pulled the trigger.",
          "score": 30,
          "created_utc": "2025-12-30 21:11:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsugl9",
              "author": "__JockY__",
              "text": "Thanks!\n\nThe Supermicro has been flawless. It booted first time, all four 6000 Workstation GPUs worked at PCIe 5.0 x16 (using MCIO shenanigans), and itâ€™s just been great. Love it.\n\nThe MZ33-AR1 was never designed for the large BAR of the Blackwells and Gigabyte support kinda just threw up their hands and gave up on it.",
              "score": 10,
              "created_utc": "2025-12-30 21:17:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwtgp8q",
                  "author": "HelpingForDoughnuts",
                  "text": "Four 6000s at full x16 is wild. The MCIO routing on those H14 boards is underratedâ€”people sleep on Supermicro because itâ€™s not â€œenthusiastâ€ but they actually engineer for this stuff.\nGigabyte throwing up their hands on large BAR is frustrating but not surprising. Their EPYC boards feel like an afterthought compared to their consumer stuff. At least you got most of your money back on the sale.\nWhatâ€™s the use case for the rigâ€”inference serving, training, or a mix? That much Blackwell horsepower has to be doing something fun.",
                  "score": 2,
                  "created_utc": "2025-12-30 23:07:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwu2603",
                  "author": "segmond",
                  "text": "The mz33-ar1 is fincky, even with 3090s, I occasionally have issues with it.  I hate the damn board.",
                  "score": 1,
                  "created_utc": "2025-12-31 01:05:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsxzx4",
          "author": "MelodicRecognition7",
          "text": "...and on top of that Ebay charges sellers 20% fees lol. Unfortunately it is too large and we do not have better alternatives, all other marketplaces are drop in the ocean, nobody will see your listings there.",
          "score": 12,
          "created_utc": "2025-12-30 21:34:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsub66",
          "author": "Aggressive-Bother470",
          "text": "Signed affidavit because they couldn't be bothered to check your evidence? :D\n\n\nOr... they now believe all photo evidence is null and void.",
          "score": 10,
          "created_utc": "2025-12-30 21:17:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwulha9",
              "author": "__JockY__",
              "text": "I've thought about it a bit now, and my guess is that below a certain dollar value no human ever actually takes more than a cursory glance at the case data (and only at the final appeal stage). It's quicker and simpler to just require the affidavit for CYA purposes, refund the money, and move on. It minimizes human hours, covers eBay in the case of later litigation, and would appear to be the least bad option available to eBay for this particular eventuality. \n\nOf course, it also incentivizes eBay to make the sellers' lives as miserable as possible in these situations, but I guess that's the cost of efficiency, eh?",
              "score": 3,
              "created_utc": "2025-12-31 02:57:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt1q8o",
          "author": "Purple-Programmer-7",
          "text": "I have 4x 3090s to sell. Iâ€™ll happily let them sit and depreciate vs trying to sell them via eBay",
          "score": 8,
          "created_utc": "2025-12-30 21:52:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt3cne",
              "author": "a_beautiful_rhind",
              "text": "try craigslist locally and just meet at a police station or other such place. as a bonus, no taxes.",
              "score": 5,
              "created_utc": "2025-12-30 21:59:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwtzbgc",
              "author": "David_Delaune",
              "text": ">I have 4x 3090s to sell. Iâ€™ll happily let them sit and depreciate vs trying to sell them via eBay\n\n\nI've had really good experience so far with buying and selling on /r/homelabsales",
              "score": 3,
              "created_utc": "2025-12-31 00:48:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtzgro",
                  "author": "Purple-Programmer-7",
                  "text": "Got another recommendation for that previouslyâ€¦ I think thatâ€™s where Iâ€™ll land when I get around to it!",
                  "score": 3,
                  "created_utc": "2025-12-31 00:49:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwunyvp",
              "author": "__JockY__",
              "text": "3090s will sell themselves on any market right now! I still like CL because once you get past the \"is the item still available\" type scams, meeting in person for cash is still king.",
              "score": 3,
              "created_utc": "2025-12-31 03:12:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx3m6yn",
              "author": "munkiemagik",
              "text": "Totally understand that sentiment, I've already spent the money so I accept it as gone from my accounts and have mentally written it off. So I'd rather let it depreciate in my possession than let some lowlife benefit from it on my dime.\n\nAs I mentioned to someone in here in another comment, I keep uuhming and aahing about whether to offload my hobby LLM server and move on to something else for a bit, like maybe grab myself a new e-mtb, (those Megamo's are looking bloody appealing) and retire the old faithful human powered mtb, OR double down and invest further into the rig for moaar power to re-excite me again. I havent fired up the LLM server in almost two weeks, granted Christmas with the siblings/cousins/nephews/nieces was hectic.\n\nIf selling wasn't so fraught with danger, in a moment of weakness I might have already jumped off the LocalLLama ship.",
              "score": 1,
              "created_utc": "2026-01-01 16:36:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsvasa",
          "author": "munkiemagik",
          "text": "I'm so glad this worked out for you and you didnt lose your money and a working product. But its horrific stories like this (usually without the happy ending) that keep me far away from ebay when it comes to selling things. I know this restricts my buyer exposure but I only do local facebook marketplace so the buyer can check and verify item function before parting with their cash. \n\nI sold my old 4090 to fund some 3090s for the LLM server and the buyer actually took a two and a half hour train to come down and buy the GPU in person because I refused to post it, though I did set the price a little lower than what was going market rate, I just wanted rid of it so I could start populating the LLM server with the proceeds.\n\nBut lately I've been contemplating offloading my Zen2 Threadripper Pro machine, I havenâ€™t been lurking in r/LocalLLaMA recently as much as I used to. But too scared to deal with the scammy public so instead of selling I might just double down on my investment and misery and upgrade it to Zen3 Threadripper Pro. If I could just find a 5965WX at a reasonable price I could almost double my system memory bandwidth.",
          "score": 7,
          "created_utc": "2025-12-30 21:21:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtizyu",
              "author": "UniversalSpermDonor",
              "text": "Oddly enough, I'm actually about to sell a 5965WX off - let me know if you're interested. (That said, unless you're willing to come to the outskirts of the DC area, I'll have to ship it to you.)",
              "score": 2,
              "created_utc": "2025-12-30 23:19:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtw26j",
                  "author": "munkiemagik",
                  "text": "I'm in the UK, by DC I take it you mean District of Columbia?",
                  "score": 1,
                  "created_utc": "2025-12-31 00:31:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsqwgx",
          "author": "Marksta",
          "text": "Holy moly dude, that's exactly why I just don't with tech stuff and reselling. I think all marketplace users kinda know the gambit at this point for bad faith craziness. But it's so much easier on buyer side because they don't even need any rep, they just click buy and can take an honest seller for a ride from there. Especially with all the GPU chip stealing off PCB stuff going on too.\n\nI've seen the warnings on ebay listings and I think Newegg too? They 1000% don't take a refund on new EPYC boards once you pop off the CPU slot cover(s) since people just can't help themselves but break $1k mobos it seems ðŸ˜‘\n\nSorry that happened to ya, glad you persisted down the 100 steps process to arrive at a refund.",
          "score": 6,
          "created_utc": "2025-12-30 21:01:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwunmyy",
              "author": "__JockY__",
              "text": "I learned the hard way that those warnings don't mean shit. \n\nAny one of us could buy one of those Newegg EPYC motherboards, take a shit on it, claim it was \"not as advertised because it came covered in poo\", and we'd get an automatic full refund from eBay.",
              "score": 1,
              "created_utc": "2025-12-31 03:10:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt3rfl",
          "author": "tengo_harambe",
          "text": "I sold my old 4060 Ti on fb marketplace recently. He reached out, I sent him a video of the GPU to prove it worked. We arranged to meet in a public spot the next day and he was on time with the exact amount of cash we agreed on. Made the exchange and went our separate ways. Never heard a peep from him again. 10/10 selling experience. May you always have a buttery smooth 100 fps Fornite experience, Randy!",
          "score": 5,
          "created_utc": "2025-12-30 22:01:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuo30t",
              "author": "__JockY__",
              "text": "This is the way.",
              "score": 1,
              "created_utc": "2025-12-31 03:12:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtagyw",
          "author": "RevolutionaryLime758",
          "text": "I sold a monitor to a guy, he didnâ€™t want it so it he stomped on it and uploaded an image that showed his boot print. Sent me back e-waste and eBay stuck me with the bill. Of course, eBay keeps the fees while demanding you refund in full.\n\neBay wants its buyers to feel like itâ€™s Amazon, so they demand you as a seller provide the same level of service of the slave driving mega corporation. Guaranteed delivery windows, easy returns with minimal pushback, etc. basically makes it so you need to be running a business yourself to really stand a chance at absorbing the fraudulent returns.\n\nLike you, I was gaslit about the images, told a human reviewer looked at it despite a rejection of my appeal within 3 minutes. After seller fees and shipping I barely get any money from eBay but Iâ€™ve made them plenty. Unlike you, I was not calm about it. I sent angry emails and called every day. Eventually it worked, but I wouldnâ€™t be able to tell you what I said that did. Just kept calling angry.\n\nAs far as the enthusiastic sounding voice, I swear to god on one of the calls it was one of the GPT voices.",
          "score": 5,
          "created_utc": "2025-12-30 22:34:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsy8dp",
          "author": "Techngro",
          "text": "I had an experience like that many years ago on eBay. Turned me off. I'll buy things, but selling is out of the question. What we need is an online pawn/consignment shop.",
          "score": 3,
          "created_utc": "2025-12-30 21:35:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtidmn",
          "author": "RnRau",
          "text": "Complete aside from selling on ebay shenanigans... it felt good reading stuff written by a human. \n\nWhich feels weird and sad at the same time...",
          "score": 3,
          "created_utc": "2025-12-30 23:16:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtwfpv",
              "author": "__JockY__",
              "text": "Thank you, I feel the same way.",
              "score": 1,
              "created_utc": "2025-12-31 00:33:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt0dip",
          "author": "Actual__Wizard",
          "text": ">uploaded photos of the motherboard with a bent and twisted CPU pin. \n\nYep. I was thinking that the pins were going to be ruined the entire time I was reading it. So, they broke it and want their money back, and you're probably going to end up with a broken mother board.",
          "score": 3,
          "created_utc": "2025-12-30 21:45:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtdb56",
          "author": "ufrat333",
          "text": "Hah, selling on eBay, they froze our account with 15k to be paid out after three weeks of sales without any disputes, all tracking added., 100% positive feedback on 50 orso orders.\n\nNo reason, no way to contact them, or well - you can talk to an Indian who can do exactly nothing and ask how your family is doing. Just when we were finding a lawyer in Germany to drag them to court - 4 months later - I sent some linkedin messages to random employees about this intention - they suddenly released the funds but left the account blocked without any further communication. I guess if my LinkedIn didn't mention working at said company a lot of years prior nothing would have happened.",
          "score": 3,
          "created_utc": "2025-12-30 22:49:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtvp3y",
              "author": "__JockY__",
              "text": "Yeah Iâ€™m done as a seller.",
              "score": 1,
              "created_utc": "2025-12-31 00:29:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtdcul",
          "author": "fluffywuffie90210",
          "text": "I had almost this exact thing happen with a faulty motherboard that was sold as broken spares/repair, the guy tried to fix it, couldnt so then tried to claim it was item not as described, FOR A BROKEN ITEM. God ebay sided with him but i got some advice on how to appeal on the forum... and somehow ebay sided with me once it got the rep side of things. Was 150Â£ but still... has to be an ebay issue.",
          "score": 3,
          "created_utc": "2025-12-30 22:49:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtvtq5",
              "author": "__JockY__",
              "text": "Yup, itâ€™s just them protecting buyers at scale because itâ€™s ultimately better for business.",
              "score": 1,
              "created_utc": "2025-12-31 00:30:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtdmkm",
          "author": "Lesser-than",
          "text": "Yeah for simular reasons I never count on resale value of anything computer related anymore, I just consider it sunk cost when purchasing. If I upgrade and never see myself using it again I give it to someone who will. You just can not depend on people to figure things out by themselves, more cases than not only want to return it after they are sure they broke it.",
          "score": 3,
          "created_utc": "2025-12-30 22:50:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtw13i",
              "author": "__JockY__",
              "text": "This is what Iâ€™m hearing about. People just abuse eBay as a try-before-you-buy site, or they buy working gear and return it as not working after swapping out the parts for themselves.",
              "score": 1,
              "created_utc": "2025-12-31 00:31:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtune1",
          "author": "abnormal_human",
          "text": "Sucks. I bought a $1500 Epyc CPU that turned out to be bad and ended up stuck with it. Stopped doing this crap on eBay after that.",
          "score": 3,
          "created_utc": "2025-12-31 00:23:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtyrlk",
              "author": "__JockY__",
              "text": "Ugh.\n\nIn the opposite story, I bought a $1400 9B45 EPYC and it turned out to be perfect and an alternate SKU for the $10k+ 9755!!",
              "score": 1,
              "created_utc": "2025-12-31 00:45:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwu1iq5",
          "author": "FormalAd7367",
          "text": "I sold an expensive vintage Iwc watch on ebay and buyer claimed he received rocks.  He took my watch and my money.",
          "score": 3,
          "created_utc": "2025-12-31 01:01:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwu2ejt",
              "author": "__JockY__",
              "text": "I honestly donâ€™t know how these assclowns sleep at night after fleecing common people.",
              "score": 1,
              "created_utc": "2025-12-31 01:06:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwuuivp",
          "author": "SkyFeistyLlama8",
          "text": "Nice on ya for putting the \"no AI\" disclaimer\" at the start. I couldn't help reading out the entire text in a Law & Order voice LOL!",
          "score": 3,
          "created_utc": "2025-12-31 03:52:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv1esg",
          "author": "Intelligent-Form6624",
          "text": "> Shit, I didn't even use spell check.\n\nDude, this is some off-grid shit. Did you also write a letter using nothing but a pen? #NoWhiteOut",
          "score": 3,
          "created_utc": "2025-12-31 04:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt1usm",
          "author": "Caryn_fornicatress",
          "text": "Brutal but accurate lesson, eBay is optimized for buyer protection at scale, not truth or evidence\n\nHigh value hardware plus no returns is basically asking for this risk, the system only really listens at the human appeal stage\n\nYour takeaway about documenting everything for the final call is the real gold here, anything before that is just feeding a machine\n\nHonestly for gear like this, local sale or escrow based platforms feel way safer than eBay now",
          "score": 4,
          "created_utc": "2025-12-30 21:52:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwulx32",
              "author": "__JockY__",
              "text": "> Honestly for gear like this, local sale or escrow based platforms feel way safer than eBay now\n\nYup. Good ol' Craigslist and Starbucks and cash.",
              "score": 1,
              "created_utc": "2025-12-31 02:59:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt8hjg",
          "author": "alphatrad",
          "text": "I just went through this shit with some asshole myself when selling my Valve Index. Such a nit picky complaint and the whole thing is setup to punish the seller. The guy literally cost me 100 dollars as I offered free shipping and then had to pay to ship it back to myself.",
          "score": 2,
          "created_utc": "2025-12-30 22:24:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwupb4r",
              "author": "__JockY__",
              "text": "I think the trick is to simply refuse to buy the return shipping label despite their outpouring of awful \"nice reputation you got there, shame if something happened to it\" emails. I forgot to mention those in the main story, but they tried to intimidate me into sending a return label under threat of reputational damage.\n\nI told them to pound sand.\n\neBay still sent the return shipping label and billed me for the convenience, but at least this way I didn't accept the return/refund and scored a small \"fuck you\" victory in their bullshit game.",
              "score": 3,
              "created_utc": "2025-12-31 03:20:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwta1ql",
          "author": "LetterRip",
          "text": "How difficult would it have been to take the buyer and eBay to small claims court?",
          "score": 2,
          "created_utc": "2025-12-30 22:32:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtuwv9",
              "author": "__JockY__",
              "text": "Oh thatâ€™s 100% the route Iâ€™d have taken if the appeal failed. No idea of time, cost, or effort required though.",
              "score": 2,
              "created_utc": "2025-12-31 00:25:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtizwp",
          "author": "AnomalyNexus",
          "text": "Yeah selling on eBay is an act of faith. I try to keep things below 200 bucks just in case.",
          "score": 2,
          "created_utc": "2025-12-30 23:19:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtykps",
              "author": "__JockY__",
              "text": "I may just follow this advice. I donâ€™t have FB so no marketplace for me :/",
              "score": 1,
              "created_utc": "2025-12-31 00:44:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwu2ckh",
          "author": "segmond",
          "text": "I have so much gear that I wish to sell for someone that's into local LLM, but I don't want to go through this sort of ebay pain.  I wonder if \"no return/no refund\" will work.",
          "score": 2,
          "created_utc": "2025-12-31 01:06:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuq9ly",
              "author": "__JockY__",
              "text": "No, it absolutely does not work. \n\nMy listing was marked as \"no return/no refunds\" in the listing's settings and my story seems common after doing a bunch of research into this matter. Your listing settings are irrelevant because the instant a buyer files a \"not as described\" claim, the automatic process begins and they end up with a refund at your expense. There's no escape. You just have to prepare everything along the way for the moment of final human appeal and hope you laid your breadcrumbs well enough along the way to support a successful appeal.",
              "score": 2,
              "created_utc": "2025-12-31 03:26:08",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwug85f",
              "author": "Ryuma666",
              "text": "Where are you from?",
              "score": 1,
              "created_utc": "2025-12-31 02:26:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvc5i8",
                  "author": "segmond",
                  "text": "Michigan",
                  "score": 1,
                  "created_utc": "2025-12-31 05:54:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwu8kkm",
          "author": "Savantskie1",
          "text": "they automatically find in favor of the buyer because in the early days of Ebay, there were tons of scammers selling on Ebay and then either sending faulty devices, or ripping people off with sending a photo of the item, but not the original item with clever wording in the description that makes people think they are buying the item, when instead they're agreeing to buy a picture of the item. So the responsibility has been put on the sellers to prove their case and Ebay err's on the side of the buyer in protection of them. It solves the problems of people suing Ebay because they weren't doing enough to protect buyers.",
          "score": 2,
          "created_utc": "2025-12-31 01:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwur3ga",
              "author": "__JockY__",
              "text": "Yes, exactly this. The buyer could've taken a dump on the motherboard and sent it back \"not as described\" for all eBay cared. They'd still automatically refund him out of my account. And by making it as difficult as possible for sellers to get to the moment of appeal, they reduce costs enormously.\n\nJust lay the groundwork for the moment of appeal... or don't sell expensive stuff on eBay, I guess.",
              "score": 1,
              "created_utc": "2025-12-31 03:31:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwuuayx",
          "author": "markcartwright1",
          "text": "Ok take a deep breath because it's over now! You were unlucky and you had a clumsy and clueless buyer. Its high stakes because it was a high value product too. And it turned into a drawn out ordeal.\n\nI've sold about 350 tech items on UK ebay, phones smartwatches laptops, mini PCs in the past 6 months. Generally new, but maybe 20-30% second hand. Maybe 1-2% have had issues, courrier issues mostly or sometimes a product was faulty. I often write about who its not for and who shouldn't buy a product - so that deters the unsuitable buyers. \n\nI offer and allow returns because it lifts the conversion rate and if someone changes their mind or they're not happy - they won't sabotage the product. I've kept away from the more expensive end of the market so the risk is spread across various products too. Ironically sometimes the returned products have sold for more than the original price. \n\nThe thing with Motherboards is they can be very fragile or very specific. Like some people might not know certain slots, or compatibility before buying. You may need to offer more hand holding. And this buyer was clumsy. \n\nIf anyone UK based here wants to sell any computer / LLM gear, drives, HDDs, SSDs, old phones - I can give you a price and arrange shipping / as well as safe payment with Paypal. Or we can run it through Ebay if you prefer. I'm good at selling bits of tech and have a cash pile to buy decent stock. I can save you from an ordeal like this. Drop me a DM and let me know what you're selling and what you'd like for it.",
          "score": 2,
          "created_utc": "2025-12-31 03:51:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv72r8",
          "author": "fred100002",
          "text": "I had the exact same experience a couple of times this year. In your write up, you mentioned reporting the buyer Â´againâ€™  but IIRC, in my examples, the system wouldnâ€™t let me report them twice and told me Â´you already reported the buyer for thisâ€™ â€¦ I also donâ€™t remember getting to an appeal button either.",
          "score": 2,
          "created_utc": "2025-12-31 05:16:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt46s1",
          "author": "No_Afternoon_4260",
          "text": "Why no 4 way blackwell on the gigabyte? Does it work on the supermicro?",
          "score": 1,
          "created_utc": "2025-12-30 22:03:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt5o3y",
              "author": "__JockY__",
              "text": "It throws PCI errors, only sees 3 cards, etc. But the supermicro has been flawless, I love it.",
              "score": 1,
              "created_utc": "2025-12-30 22:10:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwtbcr4",
                  "author": "No_Afternoon_4260",
                  "text": "wow good to know thx !  \nSo you did sell a non working board ðŸ˜…",
                  "score": 3,
                  "created_utc": "2025-12-30 22:39:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwui75o",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2025-12-31 02:38:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwutsb7",
          "author": "Sabin_Stargem",
          "text": "If I am to be perfectly honest, as a potential buyer of 2nd-hand gear, this situation sounds good for me.   I fear losing my money for a waste of time, and putting together an premium machine would seriously damage my savings even if everything went perfectly.\n\nO'course, your take on the situation is perfectly valid and reasonable.   Just saying that it makes me more seriously consider using Ebay for the next build.   Getting a top-end Threadripper for a much reduced price at less risk is incredibly appealing.\n\nIt is a perverse incentive, where greed and fear overrule fairness.   In any case, it is good for you to post a \"seller beware\" story, so that honest folks don't get deliberately ripped off by buyers.",
          "score": 1,
          "created_utc": "2025-12-31 03:48:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwymcx8",
          "author": "Phaelon74",
          "text": "Stop selling on ebay, it is horrible for sellers.",
          "score": 1,
          "created_utc": "2025-12-31 19:09:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9yspm",
          "author": "DevopsIGuess",
          "text": "I have this same motherboard, real PITA to setup. \nHe likely needs to run firmware upgrades via the BMC management  port. \n\nFeel free to send them my way. This board was a bitch to get working.",
          "score": 1,
          "created_utc": "2026-01-02 16:30:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxato8n",
              "author": "__JockY__",
              "text": "Send who your way?",
              "score": 1,
              "created_utc": "2026-01-02 18:53:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsz2wl",
          "author": "LyriWinters",
          "text": "you should obviously have taken the return...",
          "score": -7,
          "created_utc": "2025-12-30 21:39:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwurhoo",
              "author": "__JockY__",
              "text": "Had I accepted the return I'd be left with a broken motherboard instead of a mint condition motherboard worth $900. I'd have lost my money and arguably worse, I'd have backed down like a fucking pussy against a liar and a cheat. Nope.",
              "score": 2,
              "created_utc": "2025-12-31 03:33:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwuwekk",
                  "author": "LyriWinters",
                  "text": "Pretty sure he broke it to have a reason for ebay though... So you would have gotten your motherboard back. But it is just a presumption. But logic checks out.\n\nit worked out this time for you, next time probably not. you \"burned\" that bridge in the eyes of Ebay now. Sorry.   \n  \nThis is why I love Sweden, I can do a background check on the people I do business with. I can literally find adress, social security number, last declared salary, any time spent in the court system... And it all takes me roughly 5 minutes an costs me â‚¬1.",
                  "score": 1,
                  "created_utc": "2025-12-31 04:04:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q1986x",
      "title": "IQuestCoder - new 40B dense coding model",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/ilintar/IQuest-Coder-V1-40B-Instruct-GGUF",
      "author": "ilintar",
      "created_utc": "2026-01-01 17:12:51",
      "score": 185,
      "num_comments": 37,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q1986x/iquestcoder_new_40b_dense_coding_model/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nx5qmo1",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-01 23:10:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx47bkc",
          "author": "ilintar",
          "text": "BTW, the Loop version \\*is\\* a new architecture and will require adaptation.",
          "score": 58,
          "created_utc": "2026-01-01 18:25:04",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nx5fuf1",
          "author": "MutantEggroll",
          "text": "Thanks for the GGUF! Taking the IQ4\\_XS for a spin and so far it's performing very well.\n\n* Successfully zero-shotted a Snake game\n* Demonstrated good understanding of embedded Rust concepts\n* Hovering around 55% Pass 2 rate on Aider Polyglot, which puts it on-par with GPT-OSS-120B\n\nMy only issue is that it does not fit all that nicely into 32GB of VRAM. I've only got room for 28k context with unquantized KV cache. Once I finish my Polyglot run I'll try again with Q8 KV cache and see what the degradation looks like.",
          "score": 30,
          "created_utc": "2026-01-01 22:11:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx72bxe",
              "author": "rm-rf-rm",
              "text": "tests that are \"make x from scratch\" or any of the leaderboard benchmarks dont correlate well to real world performance where the majority use case is: within an existing codebase,: understands the codebase, makes a change that works, preserves architecture, preserves design patterns, preserves style.",
              "score": 12,
              "created_utc": "2026-01-02 03:53:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx79qpf",
                  "author": "MutantEggroll",
                  "text": "Agreed. I treat greenfield prompts and benchmarks as a pre-filter - models that do poorly are discarded, and those that do well move forward to real world use cases, where they get filtered again for low performance.\n\nWith the context size limitations on my hardware due to the size of this model, I'm tempering my expectations. Could be good for boilerplate code or small code reviews, but it just won't be able to hold enough of a real codebase in context to be a true workhorse.",
                  "score": 6,
                  "created_utc": "2026-01-02 04:42:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx5lsqc",
              "author": "ilintar",
              "text": "Interesting, those are very good numbers for an IQ4\\_XS on coding tasks.",
              "score": 9,
              "created_utc": "2026-01-01 22:43:15",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx6c0q7",
              "author": "FizzarolliAI",
              "text": "Interesting! I couldn't get it to behave well w/ tool calls at all, but I was trying the looping model in vLLM...",
              "score": 0,
              "created_utc": "2026-01-02 01:11:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4863c",
          "author": "mantafloppy",
          "text": "The model maker don't talk about what arch they used, and this dude quant it in Qwen2, sus all around.\n\nhttps://huggingface.co/cturan/IQuest-Coder-V1-40B-Instruct-GGUF",
          "score": 33,
          "created_utc": "2026-01-01 18:29:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4969k",
              "author": "ilintar",
              "text": "Basic model is basic Llama, loop model is nice new arch with dual (not hybrid) gated attention.",
              "score": 25,
              "created_utc": "2026-01-01 18:34:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx4d3bx",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -16,
              "created_utc": "2026-01-01 18:53:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4e3ap",
                  "author": "mantafloppy",
                  "text": "I'm calling IQuestLab/IQuest-Coder-V1-40B-Instruct sus, not OP.",
                  "score": 13,
                  "created_utc": "2026-01-01 18:58:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx4cizr",
          "author": "LegacyRemaster",
          "text": "Hi Piotr, downloading. Will test with a real c++ problem solved today with Minimax M2.1 . GPT 120, Devstral, GLM 4.7 --> they failed. Vscode + cline",
          "score": 28,
          "created_utc": "2026-01-01 18:50:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4gliw",
              "author": "LegacyRemaster",
              "text": "first feedback: 32.97 tok/sec on blackwell 96gb full context @ 450W.",
              "score": 20,
              "created_utc": "2026-01-01 19:10:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx66d2p",
              "author": "JonatasLaw",
              "text": "â€™m working on a project that involves creating shaders in C++. No current AI can help me even minimally. I put a `groupshared` inside a function (which obviously wonâ€™t work), ask GPT-5.2, Opus 4.5, Gemini 3, GLM 4.7, and Minimax 2.1 where the error is, and all of them fail. How do you work with C++ using AIs and actually get results? Do you use a specific kind of prompt? Because in my case theyâ€™re all 100% useless, they donâ€™t even work for repetitive tasks.",
              "score": 3,
              "created_utc": "2026-01-02 00:37:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8ccha",
                  "author": "LegacyRemaster",
                  "text": "I use Unreal Engine 5.7. All the C++ and backend code has the BPs converted to C++ for better performance. I think this helps. I won't deny that yesterday the 5.2 codex solved a problem for me that minimax didn't solve.",
                  "score": 2,
                  "created_utc": "2026-01-02 10:14:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxgi1ib",
                  "author": "ilintar",
                  "text": "Quick answer: you don't.  \nLong answer: the better AIs (Opus 4.5, Gemini 3) will help for simple tasks. But for complex C++ tasks you have to \\*tell them\\* what the problem is, then they can handle it. Best case, you tell them where to insert debug prints so they can figure something out.",
                  "score": 1,
                  "created_utc": "2026-01-03 16:08:17",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx53f5j",
              "author": "LegacyRemaster",
              "text": "https://preview.redd.it/4ooz7sdzysag1.png?width=864&format=png&auto=webp&s=1aeed92ab3719005a9f560266bf90af13d9694a0\n\nCline uses complex prompts and iterative task execution that may be challenging for less capable models.\n\n  \nTask : Fix errors --> \n\n  \n**Main issues observed:**\n\n1. **Missing type specifier / invalid declarations**\n   * `C4430`: missing type specifier (default-int not supported in C++)\n   * Indicates malformed or incomplete variable/function declarations.\n2. **Syntax errors around console command definitions**\n   * `C2146`: missing `;` before identifiers:\n      * `FakeBackend_ConsoleCommand`\n      * `FakeLogin_ConsoleCommand`\n   * `C2059`: syntax error on `stringa`, `)`, and `;`\n   * `C2143`: missing `)` or `;` before `{` or `}`\n3. **Function header / brace mismatch**\n   * `C2447`: missing function header, obsolete formal type list\n   * Strong indication of mismatched parentheses or braces.\n4. **Redefinition error**\n   * `C2086`: `FAutoConsoleCommandWithWorldDelegate` redefinition\n   * Suggests duplicate declaration caused by earlier syntax failure.\n\n  \nFailed. \n\n  \nNo problem with Minimax M2.1",
              "score": 6,
              "created_utc": "2026-01-01 21:07:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx5n1ax",
                  "author": "ilintar",
                  "text": "Minimax is a beast though, would be surprised if a 40B model, even if dense, would beat it.",
                  "score": 8,
                  "created_utc": "2026-01-01 22:50:02",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5enbm",
          "author": "bobeeeeeeeee8964",
          "text": "I just have a try, and it is clearly not good, it can not handle those task can solved by smaller and way more faster model like Qwen3-Coder-30B-A3B-Instruct or NVIDIA-Nemotron-3-Nano-30B-A3B. Save your time, don't use it.",
          "score": 12,
          "created_utc": "2026-01-01 22:05:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4ej94",
          "author": "Medium_Chemist_4032",
          "text": "Tried out this prompt:\n\n>Need to evaluate if youâ€™re smart. Write some compose file to run llama-swap that can swap to a vllm-ran model. Assume ubuntu host, docker is installed.\n\n[Response](https://pastebin.com/yszDVqch) is interesting. Not the brightest possible choices, but I didn't specify any, so ok.\n\n>**Overview**\n\n>This deployment provides an intelligent model swapping system that routes requests between LLM and vLLM services based on model type, with monitoring, health checks, and automatic failover.\n\n>Architecture\n\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚   Clients   â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”\n                    â”‚  Nginx      â”‚\n                    â”‚  Gateway    â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚            â”‚           â”‚            â”‚\n    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”\n    â”‚ LLM     â”‚ â”‚ vLLM    â”‚ â”‚ Model â”‚ â”‚ Prometheusâ”‚\n    â”‚ Service â”‚ â”‚ Service â”‚ â”‚Managerâ”‚ â”‚          â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n>Features Intelligent Routing: Automatically routes requests to LLM or vLLM based on model type Model Swapping: Hot-swap models without downtime Health Monitoring: Built-in health checks for all services Metrics & Logging: Prometheus + Grafana monitoring Load Balancing: Nginx load balancing with failover SSL/TLS: HTTPS support with auto-generated certificates",
          "score": 4,
          "created_utc": "2026-01-01 19:00:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4pyu0",
          "author": "ChopSticksPlease",
          "text": "Downloaded but didnt yet have time to fully test it against Devstral Small 2 and perhaps Seed OSS.\n\nHow much effort was it to build this model and how/where did you get the training data for coding?",
          "score": 4,
          "created_utc": "2026-01-01 19:57:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4dkdq",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 22,
          "created_utc": "2026-01-01 18:55:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4mgm9",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 23,
              "created_utc": "2026-01-01 19:40:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4x66o",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 5,
                  "created_utc": "2026-01-01 20:34:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4t9pg",
                  "author": "lemon07r",
                  "text": "Thank you. I dont know how anyone buys into these obviously sham models",
                  "score": -3,
                  "created_utc": "2026-01-01 20:14:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx4o0ct",
              "author": "Available_Brain6231",
              "text": "so the small chinese ai companies starting copying openai... sad.",
              "score": -6,
              "created_utc": "2026-01-01 19:47:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4hipb",
          "author": "Cool-Chemical-5629",
          "text": "Model is too big for me to run on my hw, but I'd bet I have couple of prompts it would break its teeth on. It's especially tempting to prove since it claims to be on par with Sonnet 4.5 and much bigger models and my experience says that more often than not such claims are very false lol",
          "score": 4,
          "created_utc": "2026-01-01 19:15:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5ix52",
              "author": "-InformalBanana-",
              "text": "MOE ppl! Give us MOE! :)",
              "score": 1,
              "created_utc": "2026-01-01 22:27:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx4n12b",
              "author": "Inca_PVP",
              "text": "rip. yeah 40b is heavy af.\n\nhonestly for normal hardware just stick toÂ Llama 3 8B. if u grab theÂ Q4\\_K\\_MÂ quant it fits into 8gb vram and runs instant.\n\ni use it daily for python with a specific preset to keep it focused (less yapping). put my config on profile if u want a lightweight setup that actually runs locally.",
              "score": -6,
              "created_utc": "2026-01-01 19:42:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4pxih",
                  "author": "ttkciar",
                  "text": "Heavy is good, if it means improved competence.\n\nLooking forward to giving it a spin.",
                  "score": 2,
                  "created_utc": "2026-01-01 19:57:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx41h7o",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 5,
          "created_utc": "2026-01-01 17:56:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx42cdn",
              "author": "b3081a",
              "text": "As a coder model it's probably not focusing on general benches.",
              "score": 9,
              "created_utc": "2026-01-01 18:00:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx55r9t",
          "author": "FizzarolliAI",
          "text": "To go against what everyone else is saying, I actually think this model is really good!... At everything *but* programming. It sucks at programming. General insight tasks, writing, assistant-y stuff, etc. are great! Somehow!",
          "score": 5,
          "created_utc": "2026-01-01 21:19:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx619zn",
              "author": "IrisColt",
              "text": "Thanks for the insight!",
              "score": 1,
              "created_utc": "2026-01-02 00:09:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4k9tj",
          "author": "jinnyjuice",
          "text": "I'm assuming they're going to release the loop thinking model tomorrow, right?",
          "score": 1,
          "created_utc": "2026-01-01 19:29:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1px1c41",
      "title": "Head of Engineering @MiniMax__AI on MiniMax M2 int4 QAT",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/1e9anmnmsr9g1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-27 16:06:19",
      "score": 185,
      "num_comments": 57,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nw7k99y",
          "author": "Mundane_Ad8936",
          "text": "\"In practice, memory bandwidth isn't always the bottleneck\"..\n\nI've had so many hobbyists & enthusiasts in this sub fight over VRAM bandwidth like it's the unerring word of a god.. Every time I explain that in a production systems memory bandwidth is often not the bottleneck, numerous people start bombarding me AI slop that shows they don't understand the problems.\n\nLLMS are everything intensive, you don't have massive memory bandwidth usage without huge amounts of compute driving that..\n\nOn the flip side moving that memory intensive operations to RAM (not GPU) and there is nothing you can do to fix that bottleneck.. now you have a bigger bottleneck in the RAM & CPU which are orders of magnitude slower.\n\nAlso quantization is not free, it absolutely wrecks performance if you are building systems that have to hit quality targets. You see this when you run tasks at scale and fine-tuning helps but there is a clear accuracy gap between a heavily quantized model and one that has minimal to no quantization.\n\nYou running a model in a chatbot app you probably wont notice or care.. but if you're running a model at scale and you have QA checks on what it gets right or wrong you will see clearly.",
          "score": 69,
          "created_utc": "2025-12-27 16:16:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7w9jx",
              "author": "stoppableDissolution",
              "text": "Well, vram bandwidth is \\_the\\_ constraint for batch=1 inference, and there is no way around it. Given that a lot of people in \\_local\\_lama are not hosting the models on clusters to serve hundreds of users, it is a valid case.",
              "score": 56,
              "created_utc": "2025-12-27 17:17:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw8v1sj",
                  "author": "Mundane_Ad8936",
                  "text": "Type in vLLM into the search bar for this sub and you'll see otherwise.   \n  \nThis sub is a mix of hobbyists, software devs, professionals & researchers. Small garage tinkers to Google engineers.. It's best to assume that someone in here is way more knowledgable than you (as in Redditor). I've been doing this work for 15 years and and people in this sub teach me things all the time.",
                  "score": 11,
                  "created_utc": "2025-12-27 20:15:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw89gr5",
                  "author": "eloquentemu",
                  "text": "It's not quite that simple.  Look at [Mi50 vs 3090](https://www.reddit.com/r/LocalLLaMA/comments/1mxgis3/some_benchmarks_for_amd_mi50_32gb_vs_rtx_3090/). Both have 1000GBps memory but the 3090 gets 1.5x performance.  You can also test for yourself by running a model in Q4 and BF16.  The Q4 should be 4x faster based on memory, but in practice it's only 2-3x. Obviously Q4 is still faster (so this isn't saying one should run at bf16), but the performance isn't _just_ bandwidth.",
                  "score": 2,
                  "created_utc": "2025-12-27 18:23:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7mx3y",
              "author": "SlowFail2433",
              "text": "Yes LLMs can be all three of compute, memory and interconnect bound at different scales",
              "score": 15,
              "created_utc": "2025-12-27 16:29:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7stti",
              "author": "MitsotakiShogun",
              "text": "> numerous people start bombarding me AI slop\n\n\nHad a few such encounters myself. I coined a term after one of them: The Perplexity Idiot.\n\n\nWhen they can't cite a research paper, code, or empirical study for their view, and ask Perplexity which they then share the link to, not even bothering to read the sources it cited (that one time I went over all of them one by one and not even one supported the answer).",
              "score": 21,
              "created_utc": "2025-12-27 16:59:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7wr8p",
                  "author": "Guinness",
                  "text": "The best indication that you're debating with an idiot is when you see \"?utm_source=chatgpt.com\" in the link they post.",
                  "score": 16,
                  "created_utc": "2025-12-27 17:19:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7vy99",
              "author": "Guinness",
              "text": "> LLMS are everything intensive, \n\n**YOU MUST CONSTRUCT ADDITIONAL PYLONS.**",
              "score": 18,
              "created_utc": "2025-12-27 17:15:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwg5qbx",
                  "author": "sage-longhorn",
                  "text": "My life for Aiur!",
                  "score": 1,
                  "created_utc": "2025-12-28 23:15:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7x9hj",
              "author": "TokenRingAI",
              "text": "I disagree, but not from a system operations or reliability perspective (because you are 100% right)\n\nGemini 3 Flash, Cerebras, and Groq have definitively proven that the user experience and user satisfaction is so much better when token generation is absolutely lightning fast. This metric matters a lot to users, and is capped by memory bandwidth.\n\nDeepSeek has proven the opposite, their inference speed is painful, the user experience is miserable. All the Chinese inference providers have this problem because they are building big models but don't have easy access to Blackwell hardware. They can make it work but it's not as fast.\n\nIt's not even a novel insight, people like fast cars, fast websites, fast everything. Nobody wants to wait. The attention span of users in 2025 is basically zero.\n\nAlso, by making token generation fast, you reduce memory consumption, because the entire context needs to be kept in memory while the tokens are streaming without batching.\n\nUser experience and speed matters, and will be a seriously important metric in the coming years.",
              "score": 5,
              "created_utc": "2025-12-27 17:22:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwcdnkc",
              "author": "PraxisOG",
              "text": "In my experience, the Navi 21 family of cards(rx6800,v620) is equal or faster for inference than the MI50 despite having half the bandwidth, because those older vega cards are super compute constrained",
              "score": 2,
              "created_utc": "2025-12-28 10:26:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7tnco",
              "author": "eloquentemu",
              "text": "It's crazy to me how much people only look at memory and yet we can see how impactful the upgrade path of MI50 -> 3090 -> 4090 is, even without batching though the bandwidths are the same. Sure, a 250 - 500 GBps system/GPU is going to be underwhelming, but often times the compute is simultaneously limited enough that even more bandwidth wouldn't improve it much.",
              "score": 1,
              "created_utc": "2025-12-27 17:03:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw8nf81",
                  "author": "Karyo_Ten",
                  "text": "> but often times the compute is simultaneously limited enough that even more bandwidth wouldn't improve it much.\n\n\nNo, if you don't get data fast enough to the compute unit, no compute can happen, zilch.\n\nMemory is the primary limiting factor for data workload. A non-data workload would be something relying on equation or random simulations like Monte-Carlo or Raytracing.\n\nAnd the fact that memory is the primary limiting factor is well understood in high-performance computing, see roofline model or arithmetic intensity.\n\nYou can read more in my post: https://www.reddit.com/u/Karyo_Ten/s/iMEFB6DC5Q",
                  "score": 0,
                  "created_utc": "2025-12-27 19:35:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7kmng",
          "author": "UnbeliebteMeinung",
          "text": "I wish i would understand what this all means.",
          "score": 11,
          "created_utc": "2025-12-27 16:18:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7wj9x",
              "author": "Thomas-Lore",
              "text": "It seems going to 4-bit quants was not worth it because it was not much faster at scale. (So it would be worth for someone like us who runs the model locally but not for deployement on a server farm with large batches).\n\nkofierdavid out it in words better here: https://old.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/nw7q7pw/",
              "score": 14,
              "created_utc": "2025-12-27 17:18:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwbjhfd",
                  "author": "-dysangel-",
                  "text": "I think these were the guys that wrote a massive cope about linear attention not being worth it too, then Deepseek 3.2 comes along",
                  "score": 0,
                  "created_utc": "2025-12-28 05:46:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw7wsmw",
                  "author": "UnbeliebteMeinung",
                  "text": "What does 4 bit quants even mean. Explain that to me like i am a five year old. I know nothing about these terms",
                  "score": -7,
                  "created_utc": "2025-12-27 17:20:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7j1ls",
          "author": "SlowFail2433",
          "text": "Nvidia went hard marketing 4bit but the juice might not be worth the squeeze, relative to 8bit. Top labs mess up 4bit runs regularly it is not easy",
          "score": 16,
          "created_utc": "2025-12-27 16:10:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7mxfp",
              "author": "jakegh",
              "text": "Nvidia went hard marketing FP4 on Blackwell, not INT4.\n\nChinese labs are pushed to support INT4 because older Nvidia and current Chinese chips work with it. The fact that Minimax *didn't* go with INT4 is actually in their favor.",
              "score": 30,
              "created_utc": "2025-12-27 16:30:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7nmla",
                  "author": "SlowFail2433",
                  "text": "Thanks I see I am making an error here by mixing up Int4 and FP4. I have Blackwell on the brain.",
                  "score": 13,
                  "created_utc": "2025-12-27 16:33:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7kqqk",
              "author": "abnormal_human",
              "text": "You could have said the same about FP8 in mid-2023 when Hopper/Ada with their fancy new FP8 support were about as operationalized as Blackwell is today.\n\nIt took till December 2024 till we saw a top-tier model built end-to-end around fp8 (Deepseekv3), and fp8 was a significant factor in how they were able to reduce the costs to produce that model.\n\nGive it time..the hardware support creates software and ecosystem challenges that take much longer to resolve, but the \"free performance\" of additional hardware acceleration is too valuable to ignore forever. The INT4 QAT stuff this person is talking about isn't even the new NVFP4 stuff you're alluding to, that's still older-generation 4bit quant technology, which has less performance potential.",
              "score": 24,
              "created_utc": "2025-12-27 16:19:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7o46m",
                  "author": "SlowFail2433",
                  "text": "Iâ€™m trying lol Iâ€™ve been writing FP4 training loops in CUDA or triton-like DSLs but itâ€™s tough times\n\n\nWe will get there eventually yeah",
                  "score": 11,
                  "created_utc": "2025-12-27 16:36:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7ivh8",
          "author": "SlowFail2433",
          "text": "It is true in my experience also that in large deployments the gains from quant drop.",
          "score": 2,
          "created_utc": "2025-12-27 16:09:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7n4oo",
          "author": "doc-acula",
          "text": "Kind of off-topic, but is it worth trying a Q2 quant of MiniMax (IQ2\\_XXS would be suitable for me). Or is GLM Air in Q4/Q4 already better?",
          "score": 2,
          "created_utc": "2025-12-27 16:31:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7r231",
              "author": "DeProgrammer99",
              "text": "I'd give it a shot. MiniMax M2 25% REAP Q3\\_K\\_XL did better than both GPT-OSS-120B and GLM-4.5-Air (and GLM-4.6V) on my TypeScript minigame one-shot test despite being both heavily quantized *and* pruned.",
              "score": 6,
              "created_utc": "2025-12-27 16:50:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7t2i3",
                  "author": "doc-acula",
                  "text": "Wow, that sounds promising indeed. Kind of random how badly a certain model is affected by heavy quantization. I gess, I'll give it shot then :)",
                  "score": 2,
                  "created_utc": "2025-12-27 17:00:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw7vy26",
                  "author": "noiserr",
                  "text": "Yup I used Minimax M2 30% REAP extensively at Q3 and it worked great for me.\n\nI'm hoping we'll get the REAPs of the M2.1",
                  "score": 1,
                  "created_utc": "2025-12-27 17:15:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw8e3i0",
              "author": "Southern_Sun_2106",
              "text": "I prefer iq2\\_xxs on my apple laptop, to glm air 4-bit. Just tried minimax m2.1, and it is better at using tools and doing research. It is smarter in analyzing info. Speed is aprox the same; the air might be just a little bit faster.",
              "score": 1,
              "created_utc": "2025-12-27 18:46:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw8eos3",
              "author": "Marksta",
              "text": "Yeah try Minimax M2.1, it should beat GLM Air at comparable sizes.",
              "score": 1,
              "created_utc": "2025-12-27 18:49:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw8wrtb",
          "author": "Aroochacha",
          "text": "Fascinating!",
          "score": 1,
          "created_utc": "2025-12-27 20:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwch58r",
          "author": "Aaaaaaaaaeeeee",
          "text": "Â https://xcancel.com/SkylerMiao7/status/2004887155395756057\n\n\n>Congratzï¼Any chance you are looking into 4 bit QAT, or reasons you preferred not to?\n\n\n>Good question. As early as abab5.5 (late 2023), we achieved near-lossless int4 PTQ and int4 QAT. Our QAT generalizes well and can be migrated to new models with almost no extra effort, and it remained in use through MiniMax M1.\n\nWhen talking about performance he's saying int4 can only handle like up to 30% more users. Keep in mind the comparison is to the current fp8 model. Performance can also mean quality, but that's not what they mean in context.\n\n\nI'm disappointed ðŸ˜­ but it seems like they can do it wherever, whenever. Maybe they're working on future iterations.\n\n\n\nI'm not sure about LargeEP. Expert parallel?Â \nIf you run smaller expert models you can see many have a harder time saturating GPU bandwidth compared with dense models.Â \n\nÂ ",
          "score": 1,
          "created_utc": "2025-12-28 10:59:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9oj6s",
          "author": "beijinghouse",
          "text": "At best he's saying: \"We didn't feel like spending 0.05% additional time to QAT out a 25-50% faster and higher quality model for plebs who don't even own datacenters. We're too rich to run that version so f\\*&k you.\".\n\nBut it's probably much worse than that. The truth is 4-bit QATs smudge out the ability to surgically finetune specific corporate-madated and state-mandated systematic censorship/bias into models after all the other training stages are complete. 8-bit and 16-bit weights leave infinitely more landscape to etch ham-fisted biases into models at the last minute -- even biases that run counter to everything else the weights know in their core. But 4-bit QAT processes rub away those sorts of last-step attempts to censor/bias models. That's why they can't do QAT. They can't get their shitty, bolted-on, last-minute censorship package to survive it.",
          "score": 1,
          "created_utc": "2025-12-27 22:56:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7kyit",
          "author": "Aggressive-Bother470",
          "text": "\"QAT is shit.\"\n\nIs this a fair summary?",
          "score": -4,
          "created_utc": "2025-12-27 16:20:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7q7pw",
              "author": "koflerdavid",
              "text": "They build a model for large-scale deployment, not for GPU-poor people. In their scenario, quantization is not worth it.",
              "score": 14,
              "created_utc": "2025-12-27 16:46:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7mpu4",
              "author": "SlowFail2433",
              "text": "No cos you could (and should) still do 8 bit QAT even if you are not doing 4 bit quants \n\n\nQAT is essentially a stage I would never skip, it prepares the model for the quant noise",
              "score": 8,
              "created_utc": "2025-12-27 16:28:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pvz7v2",
      "title": "Minimax M2.1 released",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pvz7v2/minimax_m21_released/",
      "author": "__Maximum__",
      "created_utc": "2025-12-26 08:13:29",
      "score": 177,
      "num_comments": 86,
      "upvote_ratio": 0.93,
      "text": "Link to xcancel:\nhttps://xcancel.com/ModelScope2022/status/2004462984698253701#m\n\nNew on ModelScope: MiniMax M2.1 is open-source!\n\nâœ… SOTA in 8+ languages (Rust, Go, Java, C++, TS, Kotlin, Obj-C, JS)\nâœ… Full-stack Web & mobile dev: Android/iOS, 3D visuals, vibe coding that actually ships\nâœ… Smarter, faster, 30% fewer tokens â€” with lightning mode (M2.1-lightning) for high-TPS workflows\nâœ… Top-tier on SWE-bench, VIBE, and custom coding/review benchmarks\nâœ… Works flawlessly in Cursor, Cline, Droid, BlackBox, and more\n\nItâ€™s not just â€œbetter codeâ€ â€” itâ€™s AI-native development, end to end.\n\nhttps://modelscope.cn/models/MiniMax/MiniMax-M2.1/summary\n",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvz7v2/minimax_m21_released/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvzuqjp",
          "author": "bullerwins",
          "text": "It's also on HF [https://huggingface.co/MiniMaxAI/MiniMax-M2.1](https://huggingface.co/MiniMaxAI/MiniMax-M2.1)",
          "score": 45,
          "created_utc": "2025-12-26 08:19:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3w6ji",
              "author": "Dependent-Highway107",
              "text": "Nice, way easier to grab from HF than dealing with ModelScope's download speeds",
              "score": 3,
              "created_utc": "2025-12-27 00:00:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5cfzp",
                  "author": "inaem",
                  "text": "Vise versa for us lol",
                  "score": 3,
                  "created_utc": "2025-12-27 05:47:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzyc4p",
          "author": "spaceman_",
          "text": ">Â Itâ€™s not just â€œbetter codeâ€ â€” itâ€™s AI-native development, end to end.\n\n\nI smell a machine",
          "score": 81,
          "created_utc": "2025-12-26 08:57:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw03x4j",
              "author": "Evening_Ad6637",
              "text": "It's not only that you smell the machine â€” it's truly a demonstration of your Sherlock Holmes' eye for subtle details!",
              "score": 38,
              "created_utc": "2025-12-26 09:56:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0qfj2",
                  "author": "SilentLennie",
                  "text": "Kimi K2 would never.",
                  "score": 8,
                  "created_utc": "2025-12-26 13:22:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw0tkt7",
              "author": "Worthstream",
              "text": "Weird, I smell ozone.Â ",
              "score": 10,
              "created_utc": "2025-12-26 13:44:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3bcal",
                  "author": "kjerk",
                  "text": "Oh no!\nSmile for me bud! Is it lopsided? Talk to me!",
                  "score": 2,
                  "created_utc": "2025-12-26 21:59:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3u1gn",
              "author": "The_Cat_Commando",
              "text": ">I smell a machine\n\nthe first sign is any post that uses so many emojis is AI summary output.\n\n>normal âœ… people âœ… dont âœ…fill  âœ… their âœ…posts âœ…with âœ…all âœ…these. â€” ðŸ”âš¡ðŸ¤—ðŸ“ŠðŸ§ âš ï¸",
              "score": 5,
              "created_utc": "2025-12-26 23:47:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6n2nk",
                  "author": "yopla",
                  "text": "You've obviously never been on LinkedIn.",
                  "score": 3,
                  "created_utc": "2025-12-27 12:59:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw03m2t",
              "author": "__Maximum__",
              "text": "Yeah, they work on LLMs, I suspect they use it as well.",
              "score": 7,
              "created_utc": "2025-12-26 09:53:09",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw44ta5",
              "author": "aeroumbria",
              "text": "Come on Boromir, we are not in Moria anymore!",
              "score": 1,
              "created_utc": "2025-12-27 00:52:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzyd56",
          "author": "Wise_Evidence9973",
          "text": "Merry Christmas!   \n[https://huggingface.co/MiniMaxAI/MiniMax-M2.1](https://huggingface.co/MiniMaxAI/MiniMax-M2.1)  \n[https://github.com/MiniMax-AI/MiniMax-M2.1](https://github.com/MiniMax-AI/MiniMax-M2.1)",
          "score": 52,
          "created_utc": "2025-12-26 08:57:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0hc37",
              "author": "No_Conversation9561",
              "text": "Thank you",
              "score": 7,
              "created_utc": "2025-12-26 12:08:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw22298",
              "author": "Adventurous-Okra-407",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2025-12-26 17:56:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzuyeq",
          "author": "Zyj",
          "text": "This is very promising, canâ€˜t wait to try a Q4 quant. Or perhaps a Q3",
          "score": 13,
          "created_utc": "2025-12-26 08:21:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzvifa",
              "author": "Particular-Way7271",
              "text": "A REAP Q4 in my case ðŸ˜‚",
              "score": 9,
              "created_utc": "2025-12-26 08:27:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw03f96",
                  "author": "__Maximum__",
                  "text": "REAP REAP Q1 in mine",
                  "score": 6,
                  "created_utc": "2025-12-26 09:51:11",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw02lru",
                  "author": "SlowFail2433",
                  "text": "Reap is rly good",
                  "score": 0,
                  "created_utc": "2025-12-26 09:42:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzylzf",
          "author": "No_Conversation9561",
          "text": "https://preview.redd.it/ls16xtixji9g1.jpeg?width=600&format=pjpg&auto=webp&s=47c07c832748f5951c571ca5743ba33d3c65f2aa",
          "score": 20,
          "created_utc": "2025-12-26 09:00:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0r9zx",
              "author": "LocoMod",
              "text": "https://preview.redd.it/i81troervj9g1.jpeg?width=1290&format=pjpg&auto=webp&s=7dd1e08b4157413a65f8206f3cdc8e5210dfc75a",
              "score": 4,
              "created_utc": "2025-12-26 13:28:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0e1og",
          "author": "zekuden",
          "text": "This or glm 4.7?",
          "score": 10,
          "created_utc": "2025-12-26 11:38:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0s1fl",
              "author": "misterflyer",
              "text": "Both. Only way to find out which works best for your use case is to try them both.  Plus you might like both.",
              "score": 9,
              "created_utc": "2025-12-26 13:33:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw35hq4",
                  "author": "zekuden",
                  "text": "I appreciate it, that makes sense haha",
                  "score": 1,
                  "created_utc": "2025-12-26 21:27:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw32k37",
              "author": "layer4down",
              "text": "GLM-4.7 is my daily driver, but I use the yearly coding max plan which giver me expect bang for buck.\n\nBut for local coding and local tasks I use minimax-m2-dwq-q4. Going to try out m2.1 today by I suspect it will largely be the same.",
              "score": 1,
              "created_utc": "2025-12-26 21:11:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4lad4",
                  "author": "sleepy_roger",
                  "text": "Not sure why you were downvoted I did the same, paid $270 for the year max plan (Christmas sale) was a no brainer.",
                  "score": 3,
                  "created_utc": "2025-12-27 02:37:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw35g98",
                  "author": "zekuden",
                  "text": "wow that's pretty cool. Why did you choose GLM instead of other paid models like claude? is it cheaper / gives you more credits / better than or equal to claude?\n\nsince you use both a paid plan and local, that's very interesting. One last question, what are your use cases for paid vs local?",
                  "score": 2,
                  "created_utc": "2025-12-26 21:27:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw10tfo",
          "author": "-InformalBanana-",
          "text": "REAP when? :D",
          "score": 10,
          "created_utc": "2025-12-26 14:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzx9z3",
          "author": "LegacyRemaster",
          "text": "These days I've been using M2.1 Free on the website / GLM 4.7 Free on the website / GTP 5.2 Thinking (paying plus) and Sonnet 4.5 Thinking (on Perplexity) a lot. The latter two suggested fixes and literally refused to return updated scripts with the fixes. M2.1 added 1000 lines of code without complaint in the free version. Both GLM and M2.1 made no errors in JS/CSS/HTML/Python. Sonnet returned a 40k shorter script after insisting that I wanted the full script. GTP was incredibly slow and the file wouldn't download. And these are two big paid programs. For my specific use case, coding, I won't go back.",
          "score": 15,
          "created_utc": "2025-12-26 08:46:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0cqii",
              "author": "Feisty-Patient-7566",
              "text": "Perplexity is a joke. They give me about 3 turns with Sonnet before it changes to \"best\".",
              "score": 15,
              "created_utc": "2025-12-26 11:25:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0pjnl",
                  "author": "LegacyRemaster",
                  "text": "True. The worst thing is that I did market research whose actual results I already knew (a product my company actually produced), and it continued to give optimistic and over-inflated numbers. When I told him, \"Stop flattering me and give me the real answer,\" it apologized and scaled back his forecast. But if they're thinking of selling it as a paid product (now free with PayPal), they have a lot of work to do.",
                  "score": 3,
                  "created_utc": "2025-12-26 13:15:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1x75d",
                  "author": "my_name_isnt_clever",
                  "text": "I can feel the enshittification happening slowly in real time but I haven't been able to beat it's functionality with a custom local setup yet. But it's still the only cloud service I pay for since it doesn't lock me into any one LLM provider.",
                  "score": 2,
                  "created_utc": "2025-12-26 17:30:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw31upw",
              "author": "layer4down",
              "text": "Perplexity is a great MCP tool and awesome for quick research via app or UI but I learned long ago itâ€™s useless for all but the most basic of coding. I love it as an NLP research tool for my models.",
              "score": 2,
              "created_utc": "2025-12-26 21:07:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3vj54",
              "author": "z_3454_pfk",
              "text": "Sonnet via perplexity is really bad since they use a middle model to only parse the â€˜relevantâ€™ parts of your query and has a system prompt to provide â€˜conciseâ€™ outputs. If you use Sonnet via api itâ€™s so much more different.",
              "score": 2,
              "created_utc": "2025-12-26 23:56:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzvvkv",
          "author": "Few_Painter_5588",
          "text": "It's a good model, I'd argue that it's probably better than Qwen3 235B too.",
          "score": 6,
          "created_utc": "2025-12-26 08:31:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0rw9o",
              "author": "this-just_in",
              "text": "For agentic coding, MiniMax M2 was already beating Qwen3 VL 235B or Qwen3 235B 2507 in my estimation (and from basically any benchmark you can find). Â I suspect Qwen3 235B is a better generalist model, and the Qwen3 VL variant has vision of course.",
              "score": 7,
              "created_utc": "2025-12-26 13:32:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0xcuc",
                  "author": "Few_Painter_5588",
                  "text": "The Qwen3 VL models have been disappointed for my tasks, the 2.5 VL models are more performant to me.",
                  "score": 1,
                  "created_utc": "2025-12-26 14:09:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw06fh1",
              "author": "ciprianveg",
              "text": "did you make some comparison tests, qwen 235b UD Q6 XL 2507 instruct was my preferred local model for coding till now, I found it best for my coding tasks, long context, 30k-120k tokens. Better than glm 4.6. java+js. I hope M2.1 is at least as good while being 2 times faster",
              "score": 1,
              "created_utc": "2025-12-26 10:22:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw07bdd",
                  "author": "Few_Painter_5588",
                  "text": "Yes, I have a personal benchmark, and running both in FP8, minimax is a little worse, but I prefer minimax. Those 15B fewer active parameters really make a huge difference for agentic tasks like figuring out document groups.",
                  "score": 2,
                  "created_utc": "2025-12-26 10:31:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzuucx",
          "author": "Zyj",
          "text": "Itâ€˜s not open source (the training data is not included). Itâ€™s open weights:\nhttps://huggingface.co/MiniMaxAI/MiniMax-M2.1",
          "score": 20,
          "created_utc": "2025-12-26 08:20:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0uqtk",
              "author": "Yes_but_I_think",
              "text": "You can get a good model or a open source one, not both.",
              "score": 11,
              "created_utc": "2025-12-26 13:52:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzwa4p",
              "author": "Xamanthas",
              "text": "Great point for the normies to know.",
              "score": 10,
              "created_utc": "2025-12-26 08:35:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzx2ta",
              "author": "cantgetthistowork",
              "text": "Everything is open weights..",
              "score": -7,
              "created_utc": "2025-12-26 08:44:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzzsyq",
                  "author": "Amazing_Rutabaga8336",
                  "text": "Olmost",
                  "score": 9,
                  "created_utc": "2025-12-26 09:12:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzulim",
          "author": "MrMrsPotts",
          "text": "How many parameters?",
          "score": 2,
          "created_utc": "2025-12-26 08:17:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzuyxz",
              "author": "bullerwins",
              "text": "229B",
              "score": 10,
              "created_utc": "2025-12-26 08:21:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzwim6",
              "author": "__Maximum__",
              "text": "Link",
              "score": -6,
              "created_utc": "2025-12-26 08:38:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1p5rh",
          "author": "Waste-Intention-2806",
          "text": "Unsloth gguf is out, anyone tried q3 quant ?",
          "score": 2,
          "created_utc": "2025-12-26 16:48:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw23gh7",
          "author": "anonynousasdfg",
          "text": "Based on your experience which one follows the system prompts and rules strictly especially on Kilo Code: M2.1 or GLM 4.7?",
          "score": 2,
          "created_utc": "2025-12-26 18:03:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzw8mi",
          "author": "Xamanthas",
          "text": "Duplicate post and links to modelscope instad of HF?",
          "score": 4,
          "created_utc": "2025-12-26 08:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw05z9k",
              "author": "SlowFail2433",
              "text": "I use both TBH and I am not based in China",
              "score": 3,
              "created_utc": "2025-12-26 10:17:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzw3ck",
          "author": "duyntnet",
          "text": "\"M2.1 was built to shatter the stereotype...\": seeing 229B shatters my dream of running it :(",
          "score": 5,
          "created_utc": "2025-12-26 08:33:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw18ezx",
              "author": "Zc5Gwu",
              "text": "128gb of ram + a gpu would run it at at least Q3 at maybe 10 t/s",
              "score": 4,
              "created_utc": "2025-12-26 15:17:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw36yvy",
                  "author": "duyntnet",
                  "text": "I have 64gb of slow ddr4 ram (2133mhz I think) and an rtx 3060 12gb so it's far from enough.",
                  "score": 2,
                  "created_utc": "2025-12-26 21:35:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzuhje",
          "author": "jacek2023",
          "text": "Wtf is xcancel",
          "score": 1,
          "created_utc": "2025-12-26 08:16:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzulg5",
              "author": "bullerwins",
              "text": "a proxy for x/twitter where you can see comments for post without having to be logged in. Not a phishing website, it's legit",
              "score": 28,
              "created_utc": "2025-12-26 08:17:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzyq25",
                  "author": "pmttyji",
                  "text": "Thanks. Not aware of that site. Though I don't use twitter, I used to bookmark some ids to see their tweets without login. After it became X, I couldn't see that way anymore.",
                  "score": 8,
                  "created_utc": "2025-12-26 09:01:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw0du5w",
          "author": "jadhavsaurabh",
          "text": "Thx for introducing me to x cancel,\n\nBro it's like in redir app i. Android it oppensjn reddit app, then open in browser then open. In x so much wasted time",
          "score": 1,
          "created_utc": "2025-12-26 11:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0racf",
          "author": "this-just_in",
          "text": "Looking forward to the AWQ and NVFP4 quants- MLX static and GGUF quants already posted to HF.",
          "score": 1,
          "created_utc": "2025-12-26 13:28:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0x63k",
          "author": "mintybadgerme",
          "text": "Just tried it via the API and it's really really good.",
          "score": 1,
          "created_utc": "2025-12-26 14:08:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4i1ps",
          "author": "Different_Fix_2217",
          "text": "It's worse than deepseek 3.2 for local in my usage.",
          "score": 1,
          "created_utc": "2025-12-27 02:16:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwd0bbp",
          "author": "Turbulent_Sample487",
          "text": "I'm a new owner of the dell pro max (dgx spark clone) with 128GB shared gcpu/cpu(arm) ram with the GB10.  I never know which model to download from HF... The gb10 works best with fp8.  Generally speaking I should always look for models with fp8 in the name? (btw - through trial and error, these options make the gb10 scream with comfyui -  python [main.py](http://main.py) \\--highvram --reserve-vram 15 --fp8\\_e4m3fn-unet --fp8\\_e4m3fn-text-enc).  Maybe vLLM and the MiniMax-M2.1-AWQ?",
          "score": 1,
          "created_utc": "2025-12-28 13:37:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfi0fw",
          "author": "bigh-aus",
          "text": "For those who are running this locally I'd love to know what the details of your rig  it's running on.",
          "score": 1,
          "created_utc": "2025-12-28 21:15:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwu5p7y",
          "author": "Upstairs_Toe_3560",
          "text": "Messed up my main branch instead of the worktree. Tried switching to the worktree but could only succeed after 4â€“5 attempts. I used it with OpenCode and never had the same problem with any other model before.",
          "score": 1,
          "created_utc": "2025-12-31 01:25:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw016o2",
          "author": "power97992",
          "text": "FInally lol",
          "score": 1,
          "created_utc": "2025-12-26 09:27:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzwc3f",
          "author": "Snoo_64233",
          "text": "This is old news? It got released 5 days ago, no?",
          "score": -2,
          "created_utc": "2025-12-26 08:36:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzxb9a",
              "author": "misterflyer",
              "text": "Sort of. It was released via API/website 5 days ago, not open weights (for local use) until now.",
              "score": 17,
              "created_utc": "2025-12-26 08:46:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2d542",
                  "author": "NoahFect",
                  "text": "So the repo on HF with the 6-day-old files wasn't visible to the public until just now?  I'm confused too.",
                  "score": 1,
                  "created_utc": "2025-12-26 18:53:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q1lgb7",
      "title": "TIL you can allocate 128 GB of unified memory to normal AMD iGPUs on Linux via GTT",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q1lgb7/til_you_can_allocate_128_gb_of_unified_memory_to/",
      "author": "1ncehost",
      "created_utc": "2026-01-02 01:37:11",
      "score": 172,
      "num_comments": 30,
      "upvote_ratio": 0.96,
      "text": "So I am training a 1B model right now on my 7900 XTX with some custom kernels I wrote, and while it is training I wanted to optimize the kernels at the same time. However, my VRAM is nearly maxed doing training, so its not ideal.\n\nThen I realized maybe my 2 CU Raphael iGPU might be able to help since I only need to run some limited samples and the speed isn't as important for optimization as it is for training. After doing some research, it turned out that not only does ROCm recognize the iGPU, but a Linux feature called Graphics Translation Table (GTT) for AMD iGPUs can use up to 128 GB of system memory as VRAM. It even allocates it dynamically, so it isn't removed from your CPU's memory pool until it is allocated. I think a lot of people running Strix Halo are probably using the bios setting, but if you are running Linux you should check to see if GTT works for you since its dynamically allocated.\n\nThis isn't very useful for most people:\n\n1) It isn't going to be good for inference because iGPUs are very very slow, and usually the CPU itself is faster for inference.\n\n2) I'm accessing ROCm directly via C++ / HIP kernels, so I can avoid all the support issues ROCm has for iGPUs in the python stack\n\nHowever, for development it is actually pretty awesome. I allocated 24 GB of GTT so now the iGPU can load a full training run that my main GPU can run so I can profile it. Meanwhile my main GPU is doing long term loss convergence tests in parallel. Since RDNA iGPUs have been around for a while now, this enables big memory AMD GPU kernel development for cheap.\n\nAlso it might be interesting for developing hybrid CPU/GPU architectures. The MI300A does exist which has unified HBM tied to a CPU and giant iGPU. A standard ryzen laptop could kind of sort of simulate it for cheap. Stuff like vector indexing on the CPU into big GEMMs on the GPU could be done without PCIE overhead.\n\nI thought it was cool enough to post. Probably a \"Cool story bro\" moment for most of you though haha.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q1lgb7/til_you_can_allocate_128_gb_of_unified_memory_to/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nx7en6z",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-02 05:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7akn8",
          "author": "jstormes",
          "text": "I am doing this with an older Ryzen 7 5600G for background LLM tasks.  Using the iGPU leaves the CPU free to do other batch processes. \n\nBecause I am not using interactivity it is a good use case.  \n\nI have 64 GB 3600 MT/s memory with about 42 of it running a single LLM with it's cache.\n\nIt also keeps my more modern machines free for interactive stuff.",
          "score": 24,
          "created_utc": "2026-01-02 04:47:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7otzh",
              "author": "PentagonUnpadded",
              "text": "A Ryzen 5 5600G has 7 CUs. A 7700X like OP mentions has 2. Cool way to breath life into a 'budget' chip like like the 'G'.",
              "score": 8,
              "created_utc": "2026-01-02 06:35:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx808n2",
          "author": "laughingfingers",
          "text": "on Strix Halo I definitely use this for inference and it's a lot faster than CPU. In BIOS I set graphics memory to the minimum 512MB, with this gtt setting I allocate almost all the rest (few GB for the OS to run seems wise).",
          "score": 18,
          "created_utc": "2026-01-02 08:19:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8845x",
              "author": "Daniel_H212",
              "text": "Yeah this is how I was recommended to do it on strix halo too. There was some talk about how allocating more than 108 GiB with GTT caused instability so I limited myself to that (and don't expect needing that much anyway).",
              "score": 2,
              "created_utc": "2026-01-02 09:34:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8ow0s",
                  "author": "GrayBayPlay",
                  "text": "I can load models up to 120 gb, with a 4k context on my halo strix without any instabillity issues. Then again i only use it for inference. your milage may vary :P",
                  "score": 3,
                  "created_utc": "2026-01-02 12:05:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx875t5",
          "author": "FastDecode1",
          "text": "FYI, according to the [driver docs](https://www.kernel.org/doc/html/v4.19/gpu/amdgpu.html):\n\n>gttsize (int)\n\n>Restrict the size of GTT domain in MiB for testing. The default is -1 (Itâ€™s VRAM size if 3GB < VRAM < 3/4 RAM, otherwise 3/4 RAM size).\n\nSo as long as you have more than 4GB of RAM, the driver automatically allows up to 3/4 of the RAM to be allocated to the iGPU.\n\nI've run stuff on a Vega 8 iGPU on a laptop using llama.cpp and it does work. However, it's not a great experience if you want to watch videos (or do basically anything else GUI-wise) at the same time, since llama.cpp hogs all the memory bandwidth and causes everything else to stutter. GPU scheduling is pretty much non-existent on Linux AFAIK, so there's not really a great way to mitigate this atm.\n\nAlso a hint for fellow ThinkPad users: even though the spec sheet says only a certain amount of RAM is supported, you should probably be able to add more without issues. My current E595's specs say only up to 32GB is supported, but I added a 32GB stick alongside the existing 8GB for a total of 40GB and it works.",
          "score": 6,
          "created_utc": "2026-01-02 09:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6wxt5",
          "author": "master__cheef",
          "text": "This guy LLMs",
          "score": 11,
          "created_utc": "2026-01-02 03:19:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx83xq9",
          "author": "noiserr",
          "text": "Yup. This is what we do with Strix Halo.",
          "score": 5,
          "created_utc": "2026-01-02 08:54:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7v3ol",
          "author": "melenitas",
          "text": "Great, I need to test this with my Ryzen 8845hs, I thought I was limited to 16gb from the total 32gb....Â ",
          "score": 3,
          "created_utc": "2026-01-02 07:31:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6iu1i",
          "author": "cosimoiaia",
          "text": "With llama.cpp you can actually do with Nvidia GPUs as well and if you use it only for kv cache the speed doesn't drastically drop. It's a pretty cool trick.\n\nI used to do that too with my iGPU as well but, maybe because it's a pretty slow one, I never noticed any difference between that and using cpu only, both in training and inference.\n\nI even did some training on cpu only and with a stock heatsink/fan. \"Fun\" to see it hitting 106Â° Celsius.",
          "score": 6,
          "created_utc": "2026-01-02 01:52:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6jka1",
              "author": "-InformalBanana-",
              "text": "How would I set it up to use it only for kv cache, can you give me a llama-server example command?  \nThanks.",
              "score": 7,
              "created_utc": "2026-01-02 01:57:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx6kxfl",
                  "author": "cosimoiaia",
                  "text": "It's not specific to the kv cache. \nSet the env variable GGML_CUDA_ENABLE_UNIFIED_MEMORY=1 and if the model fits in VRAM, the ram will be used only for the kv cache.",
                  "score": 4,
                  "created_utc": "2026-01-02 02:05:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxco2fa",
              "author": "-InformalBanana-",
              "text": "I tried loading a model with your method.\nMy speed on 20k context filled droped from 15tg/s to 2.7tg/s. I would call that significant. And that is with only 2GB shared memory used. So either llama.cpp doesnt consistently allocate kv after other parts thus model ends up in shared memory instead of kv or you simply need much of kv for generating output when context is close to filled.\npp/s also droped from 250 to 76 t/s.\n\n\nSo I think you aren't right about this and you made a mistake somewhere.",
              "score": 0,
              "created_utc": "2026-01-03 00:28:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxcs6s1",
                  "author": "cosimoiaia",
                  "text": "Dude, it's a llama.cpp feature. RTFM. It's not even clear what/where/how you are trying to do, probably to you as well.",
                  "score": 0,
                  "created_utc": "2026-01-03 00:51:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7m9st",
          "author": "alppawack",
          "text": "Whatâ€™s the training speed?",
          "score": 2,
          "created_utc": "2026-01-02 06:14:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9tfni",
              "author": "1ncehost",
              "text": "A blazing 1.8 tok/s ðŸ˜‚ -- the program is very unoptimized currently and it should probably be about 50-200 tok/s for this model.",
              "score": 3,
              "created_utc": "2026-01-02 16:05:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx96wgm",
          "author": "uti24",
          "text": ">but a Linux feature called Graphics Translation Table (GTT) for AMD iGPUs can use up to 128 GB of system memory as VRAM\n\nIs there a fundamental reason it could not be implemented in windows? Or is it just not implemented? Could it be implemented not on the system level but on the app level?",
          "score": 1,
          "created_utc": "2026-01-02 14:07:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx97wzl",
              "author": "1ncehost",
              "text": "I'm not sure to be honest, but the linux driver is open source and the windows driver isn't, so it generally gets features faster and random hacker dudes like me expose low level features / fix stuff fast.",
              "score": 3,
              "created_utc": "2026-01-02 14:13:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzhcqu",
      "title": "Any guesses?",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/xqvj95zv8cag1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-30 12:52:15",
      "score": 171,
      "num_comments": 36,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwqfxna",
          "author": "Aggressive-Dingo-993",
          "text": "https://preview.redd.it/5pyw2vu8pcag1.jpeg?width=368&format=pjpg&auto=webp&s=6cb05d136e810fbb1e761628ea82933c2ec7c842\n\nQwen image 2512",
          "score": 53,
          "created_utc": "2025-12-30 14:23:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqnq2f",
              "author": "GenLabsAI",
              "text": "I think thats already out",
              "score": 7,
              "created_utc": "2025-12-30 15:05:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws7ckl",
                  "author": "infearia",
                  "text": "Qwen-Image-**Edit-2511** is out. Qwen-Image-2512 is not.",
                  "score": 18,
                  "created_utc": "2025-12-30 19:27:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwr8fkr",
                  "author": "Sorry_Warthog_4910",
                  "text": "Itâ€™s not",
                  "score": 4,
                  "created_utc": "2025-12-30 16:44:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwq621n",
          "author": "swagonflyyyy",
          "text": "Qwen3vl-next-80b-a3b - Now with no more comparison slop.\n\n\nIts not a comparison, its a victory.",
          "score": 79,
          "created_utc": "2025-12-30 13:26:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrz0lp",
              "author": "Own-Potential-2308",
              "text": "It's not X, it's Y.\n\nCan't unsee it",
              "score": 18,
              "created_utc": "2025-12-30 18:47:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqde0o",
          "author": "HedgehogActive7155",
          "text": "Qwen 6, to beat GPT 5.2 on the only benchmark that matter\n\nhttps://preview.redd.it/2fix1edsncag1.png?width=1080&format=png&auto=webp&s=d24535f9d904fe79639b83dbdf9c35e0d67a930c",
          "score": 100,
          "created_utc": "2025-12-30 14:09:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrs0d9",
              "author": "MoffKalast",
              "text": "Finally a benchmark you can trust.",
              "score": 13,
              "created_utc": "2025-12-30 18:15:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwr91cx",
              "author": "Utoko",
              "text": "That would be huge if they could double the number!",
              "score": 15,
              "created_utc": "2025-12-30 16:47:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwrdz2g",
                  "author": "-dysangel-",
                  "text": "it would be almost twice as huge!",
                  "score": 4,
                  "created_utc": "2025-12-30 17:10:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwvzn1j",
              "author": "Niwa-kun",
              "text": "lmao. cool graph. names, colors, and number with literally ZERO information for what any of it means. Cool story. I call bs on this \"benchmark\".",
              "score": 1,
              "created_utc": "2025-12-31 09:24:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwws5qp",
                  "author": "t_krett",
                  "text": "I ran the numbers myself and they check out!",
                  "score": 3,
                  "created_utc": "2025-12-31 13:25:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwxbsyv",
                  "author": "Tall-Ad-7742",
                  "text": "I tried it myself and itâ€™s crazy how accurate this benchmark is and btw itâ€™s called VAG-Benchmark",
                  "score": 2,
                  "created_utc": "2025-12-31 15:17:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrizyl",
              "author": "Cool-Chemical-5629",
              "text": "Where is Grok 4.1? ðŸ˜­ðŸ’”",
              "score": 0,
              "created_utc": "2025-12-30 17:34:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtwmyj",
                  "author": "erraticnods",
                  "text": "grokking they weights",
                  "score": 2,
                  "created_utc": "2025-12-31 00:34:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwq169h",
          "author": "MaxKruse96",
          "text": "Iteration on qwen-image i recon. No LLM, no Qwen3.5 etc.",
          "score": 18,
          "created_utc": "2025-12-30 12:54:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwr0un0",
              "author": "ontorealist",
              "text": "Itâ€™s also been awhile since Z-Image Edit was announcedâ€¦",
              "score": 8,
              "created_utc": "2025-12-30 16:09:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwr7rfl",
                  "author": "j_osb",
                  "text": "I was going to say. I expect z-image-onni-base or z-image-edit considering tongyi dropped the post.",
                  "score": 5,
                  "created_utc": "2025-12-30 16:41:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwrbztf",
          "author": "ForsookComparison",
          "text": "Qwen3.5-235B-A10B\n\nPlz",
          "score": 12,
          "created_utc": "2025-12-30 17:01:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrac0x",
          "author": "a_beautiful_rhind",
          "text": "z-image prompt enhancer enhancer\n\noh you thought you were getting the base, huh?",
          "score": 8,
          "created_utc": "2025-12-30 16:53:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrk1hs",
          "author": "Cool-Chemical-5629",
          "text": "The ascii art character must be a hint. Probably some image related model.",
          "score": 6,
          "created_utc": "2025-12-30 17:39:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsdwjv",
          "author": "Steuern_Runter",
          "text": "Maybe the other z-image models that had been announced but not published yet:\n\nZ-Image-Omni-Base\n\nZ-Image-Edit\n\nZ-Image",
          "score": 5,
          "created_utc": "2025-12-30 19:58:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqy32v",
          "author": "GabryIta",
          "text": "Wan2.5 open source?",
          "score": 3,
          "created_utc": "2025-12-30 15:56:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nws24bl",
          "author": "Brave-Hold-9389",
          "text": "Qwen image 2",
          "score": 3,
          "created_utc": "2025-12-30 19:02:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqaykk",
          "author": "Evening_Ad6637",
          "text": "Something trained on ASCII Art Ã  la Opus?",
          "score": 5,
          "created_utc": "2025-12-30 13:55:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqbaed",
          "author": "jacek2023",
          "text": "unfortunately radio silence from Junyang Lin",
          "score": 2,
          "created_utc": "2025-12-30 13:56:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrdup0",
          "author": "-dysangel-",
          "text": "https://preview.redd.it/47u8zpa0jdag1.png?width=350&format=png&auto=webp&s=3e52fc4e3d3bdc5753d50c36eae39d64004e0c0a\n\nthat's got to be the best emoji I've ever seen",
          "score": 2,
          "created_utc": "2025-12-30 17:10:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq8wen",
          "author": "Sensitive_Sweet_1850",
          "text": "Qwen4VL!!",
          "score": 3,
          "created_utc": "2025-12-30 13:43:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqccv4",
          "author": "Few_Painter_5588",
          "text": "Qwen3-Max open source?ðŸ‘€",
          "score": 5,
          "created_utc": "2025-12-30 14:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq3fv5",
          "author": "scraper01",
          "text": "Omni because the little figure has a hero stance",
          "score": 1,
          "created_utc": "2025-12-30 13:09:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqi8xe",
          "author": "Salt-Willingness-513",
          "text": "qwen image. i ask q wen z-image base",
          "score": 1,
          "created_utc": "2025-12-30 14:36:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqrzo3",
          "author": "this-just_in",
          "text": "Something zimage",
          "score": 1,
          "created_utc": "2025-12-30 15:27:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqwt65",
          "author": "Gold_Scholar1111",
          "text": "Qwen5 serie",
          "score": 1,
          "created_utc": "2025-12-30 15:50:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0vom4",
      "title": "IQuestLab/IQuest-Coder-V1 â€” 40B parameter coding LLM â€” Achieves leading results on SWE-Bench Verified (81.4%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%)",
      "subreddit": "LocalLLaMA",
      "url": "https://github.com/IQuestLab/IQuest-Coder-V1",
      "author": "TellMeAboutGoodManga",
      "created_utc": "2026-01-01 04:29:26",
      "score": 168,
      "num_comments": 45,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0vom4/iquestlabiquestcoderv1_40b_parameter_coding_llm/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "nx1p0vs",
          "author": "gzzhongqi",
          "text": "I looked up their background info and they are back by a chinese quant trading company, similar to deepseek. Interesting that all these quant trading companies are stepping into llm training.",
          "score": 58,
          "created_utc": "2026-01-01 07:05:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx31x3p",
              "author": "Karyo_Ten",
              "text": "Faster churning of quant code, millions won on the stock market.",
              "score": 3,
              "created_utc": "2026-01-01 14:41:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx17enb",
          "author": "TellMeAboutGoodManga",
          "text": "https://preview.redd.it/xzspqeti1oag1.png?width=3022&format=png&auto=webp&s=1887580e4045d206ec282b1a12549b46507ee0b1",
          "score": 31,
          "created_utc": "2026-01-01 04:31:55",
          "is_submitter": true,
          "replies": [
            {
              "id": "nx1a4dk",
              "author": "Recoil42",
              "text": "Great technical report here: [https://github.com/IQuestLab/IQuest-Coder-V1/blob/main/papers/IQuest\\_Coder\\_Technical\\_Report.pdf](https://github.com/IQuestLab/IQuest-Coder-V1/blob/main/papers/IQuest_Coder_Technical_Report.pdf)",
              "score": 18,
              "created_utc": "2026-01-01 04:52:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx18606",
          "author": "ocirs",
          "text": "Really great results for a 40B param model, is it safe the assume the benchmarks are based on the IQuest-Coder-V1-40B-Loop-Thinking model?",
          "score": 16,
          "created_utc": "2026-01-01 04:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx18u5x",
              "author": "TellMeAboutGoodManga",
              "text": "The score of LiveCodeBench v6 is from IQuest-Coder-V1-40B-Loop-Thinking model, and the rest are IQuest-Coder-V1-40B-Loop-Instruct model.",
              "score": 18,
              "created_utc": "2026-01-01 04:42:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx19tir",
              "author": "r4in311",
              "text": "It's also very safe to assume that this is a comically blatant case of benchmaxing. :-)",
              "score": 9,
              "created_utc": "2026-01-01 04:50:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1wm3i",
                  "author": "No-Dog-7912",
                  "text": "No, this is actually a well thought out use of collecting trajectories for RL. Did you read the blog post? This is what Google recently did with Gemini 3 Flash and itâ€™s starting to become a norm for other companies. They had 32k trajectories thatâ€™s just sick. To be honest, with these results and model size. This would technically mean that this is the best local coding model by farâ€¦. If we could validate this ourselves independently then it would be a huge opportunity gain for local model runners after quantizing the model.",
                  "score": 32,
                  "created_utc": "2026-01-01 08:23:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx1ccmv",
                  "author": "Odd-Ordinary-5922",
                  "text": "tell me how benchmaxing is possible when the test questions arent visible and constantly change",
                  "score": 2,
                  "created_utc": "2026-01-01 05:12:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2hlef",
          "author": "Snuttegubben68",
          "text": "GCUF available now - downloading it with LMStudio",
          "score": 12,
          "created_utc": "2026-01-01 12:03:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3tpoo",
          "author": "Business_Clerk_8943",
          "text": "https://preview.redd.it/35z3nb6ftrag1.png?width=3497&format=png&auto=webp&s=c2b4ec46269c7bd60813a67176a82d45b315d5fa\n\n[https://huggingface.co/spaces/Jellyfish042/UncheatableEval](https://huggingface.co/spaces/Jellyfish042/UncheatableEval)  \nQwen3 14B's pre-training level. Theyâ€™re obviously gaming the benchmarks. I don't get how anyone buys this.",
          "score": 11,
          "created_utc": "2026-01-01 17:16:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx40zna",
              "author": "Baldur-Norddahl",
              "text": "They are the best model on that eval for pure coding? Also there are no other models at the same size, so we can't really compare anything.",
              "score": 1,
              "created_utc": "2026-01-01 17:54:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1ifxm",
          "author": "TopCryptographer8236",
          "text": "I was hoping the 40B was a MoE but it seems to be a dense model. I guess i was just used with everything bigger than 20B to be a MoE at the moment to balance the speed with consumer hardware. But still appreciate it nonetheless.",
          "score": 12,
          "created_utc": "2026-01-01 06:03:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx324oo",
              "author": "Karyo_Ten",
              "text": "Time to buy a 5090, in NVFP4 it would be 20GB so 12GB left for context",
              "score": 1,
              "created_utc": "2026-01-01 14:42:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1jhy1",
          "author": "Fantastic-Emu-3819",
          "text": "Benchmaxxed or real?",
          "score": 17,
          "created_utc": "2026-01-01 06:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx464ny",
              "author": "FinBenton",
              "text": "Someone on youtube tested it, if you feed it isolated benchmark test type questions then it is extremely good at that but working in real world codebases it fell apart. This might be one of the most benchmaxed models ever made.",
              "score": 7,
              "created_utc": "2026-01-01 18:19:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4m5el",
                  "author": "Fantastic-Emu-3819",
                  "text": "Imagine it was better than opus 4.5. S&P500 would have been -30%.",
                  "score": 8,
                  "created_utc": "2026-01-01 19:38:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4ybk7",
                  "author": "Lopsided_Dot_4557",
                  "text": "I have tested it here and looks good:  [https://youtu.be/NrqE2mKHagg?si=4PYWVlJCnvGKMYFM](https://youtu.be/NrqE2mKHagg?si=4PYWVlJCnvGKMYFM)",
                  "score": 3,
                  "created_utc": "2026-01-01 20:40:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1yczv",
          "author": "__Maximum__",
          "text": "Someone test this in their private coding bench",
          "score": 5,
          "created_utc": "2026-01-01 08:41:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx21n26",
              "author": "lumos675",
              "text": "I can test but any gguf available?",
              "score": 5,
              "created_utc": "2026-01-01 09:16:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx23j1q",
                  "author": "__Maximum__",
                  "text": "No, at the moment, the only way is to use transformers, i guess.",
                  "score": 1,
                  "created_utc": "2026-01-01 09:37:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2q3ca",
          "author": "rekriux",
          "text": "I believe the loop integration is the fist implementation of the sort ? Any one can confirm any other implementation ?\n\nThis is a idea I raised, what if we re-used layers to artificially augment the model dept ?  \nBut I was thinking of applying a adapter (rsLoRa) on the second/third pass, making it able to \\*\\*fake\\*\\* a larger model. The power of a dense 72B in a 32b model, about +15-40% more knowledge with the Lora.   \n  \nThe thing with (most?) Lora implementation, last I checked they can't run simultaneous lora on batches, not sure if it was fixed. But if batching is made to wait until next beginning, it may introduce a bit latency for 1st token but it could be worth it with NVRAM prices !",
          "score": 3,
          "created_utc": "2026-01-01 13:18:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2wp53",
          "author": "paryska99",
          "text": "This is huge if true",
          "score": 3,
          "created_utc": "2026-01-01 14:06:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1wje3",
          "author": "Everlier",
          "text": "Report mentions 7B and 14B, but no weights, I'm very curious to try these two!",
          "score": 6,
          "created_utc": "2026-01-01 08:22:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4613x",
          "author": "6969its_a_great_time",
          "text": "So is it good or not?",
          "score": 2,
          "created_utc": "2026-01-01 18:18:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx26fup",
          "author": "Shir_man",
          "text": "Those benchmarks looks sus, has anyone tried it already?",
          "score": 2,
          "created_utc": "2026-01-01 10:08:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3dhzu",
          "author": "_w0n",
          "text": "Does anyone has a benchmark i should try on this model?",
          "score": 1,
          "created_utc": "2026-01-01 15:50:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx43chc",
          "author": "Baldur-Norddahl",
          "text": "I am not seeing the thinking variants on HF. Are only the non thinking versions open weight?",
          "score": 1,
          "created_utc": "2026-01-01 18:05:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6trp3",
          "author": "iansltx_",
          "text": "Seems like it doesn't work with mlx-lm, and the q8 GGUF basically stalls out on my M1 Max 64GB box. What am I doing wrong here?",
          "score": 1,
          "created_utc": "2026-01-02 02:59:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1rmb2",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -4,
          "created_utc": "2026-01-01 07:30:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx327wh",
              "author": "Karyo_Ten",
              "text": "vLLM can load the base FP16 weights",
              "score": 1,
              "created_utc": "2026-01-01 14:43:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwt6ir",
      "title": "Asus isn't going into memory manufacturing â€” Taiwanese tech giant issues statement smashing rumor",
      "subreddit": "LocalLLaMA",
      "url": "https://www.tomshardware.com/pc-components/dram/no-asus-isnt-going-into-memory-manufacturing-taiwanese-tech-giant-issues-statement-smashing-rumor",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-27 08:49:41",
      "score": 168,
      "num_comments": 14,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pwt6ir/asus_isnt_going_into_memory_manufacturing/",
      "domain": "tomshardware.com",
      "is_self": false,
      "comments": [
        {
          "id": "nw7c47l",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-27 15:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5xq81",
          "author": "ImportancePitiful795",
          "text": "We knew that from the day it was published by crap sites like WCCFTECH. \n\nASUS doesn't have the fab to do make DRAM chips.",
          "score": 81,
          "created_utc": "2025-12-27 09:04:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5ybbt",
              "author": "HumanDrone8721",
              "text": "And I think the market now is kind of out of RAM to rebrand and put even more blinking pulsating stinking LEDs on it.",
              "score": 20,
              "created_utc": "2025-12-27 09:09:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw6137n",
          "author": "a_beautiful_rhind",
          "text": "We're going to have to smuggle chinese dram in our prison wallets or glue them to ourselves like the lady with the iphones.",
          "score": 26,
          "created_utc": "2025-12-27 09:37:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6ewfs",
              "author": "tmvr",
              "text": "If you are obese enough you can hide them in your folds. Excuse me while I go eat some triple burger meal and wash it down with a 2.5L bottle of soft drink.",
              "score": 15,
              "created_utc": "2025-12-27 11:50:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6rib9",
                  "author": "a_beautiful_rhind",
                  "text": "fatmaxxing",
                  "score": 8,
                  "created_utc": "2025-12-27 13:30:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw74bsm",
              "author": "nonaveris",
              "text": "Or lobby for it to be unsanctioned as long as the prices remain high.",
              "score": 5,
              "created_utc": "2025-12-27 14:51:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw631km",
          "author": "megalo-vania",
          "text": "They have already collaborated with Nvidia, why they need to manufacturing memory?",
          "score": 5,
          "created_utc": "2025-12-27 09:56:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6snk3",
              "author": "StorageHungry8380",
              "text": "NVIDIA is reportedly no longer supplying VRAM along with their GPU to board partners\\[1\\]. Since they can't just wish a memory fab into existence, they'll have to find memory somewhere else.\n\n\\[1\\]: [https://www.tomshardware.com/pc-components/gpus/nvidia-reportedly-no-longer-supplying-vram-to-its-gpu-board-partners-in-response-to-memory-crunch-rumor-claims-vendors-will-only-get-the-die-forced-to-source-memory-on-their-own](https://www.tomshardware.com/pc-components/gpus/nvidia-reportedly-no-longer-supplying-vram-to-its-gpu-board-partners-in-response-to-memory-crunch-rumor-claims-vendors-will-only-get-the-die-forced-to-source-memory-on-their-own)",
              "score": 21,
              "created_utc": "2025-12-27 13:38:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6ssk4",
                  "author": "megalo-vania",
                  "text": "Holy this happened a month ago and I didnâ€™t know yet.",
                  "score": 5,
                  "created_utc": "2025-12-27 13:39:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwbldph",
          "author": "Highwaytothebeach",
          "text": "It looks all those moore's  and other laws were supposed to lead to PCs with quite a lot of RAM sooner.. DDR6 was suposed to make finally reasonably  priced 1 TB or more RAM PCs and 8 TB or more RAM servers....Hope at least those PCIe 6.0 SSD with 30.25 GB/s speeds may  become available sooner [https://www.tomshardware.com/pc-components/ssds/pcie-6-0-ssd-with-30-25-gb-s-speeds-debuts-at-computex-release-date-is-still-a-long-way-off](https://www.tomshardware.com/pc-components/ssds/pcie-6-0-ssd-with-30-25-gb-s-speeds-debuts-at-computex-release-date-is-still-a-long-way-off)",
          "score": 3,
          "created_utc": "2025-12-28 06:01:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5wt11",
          "author": "QuailLife7760",
          "text": "Well we're fucked",
          "score": 9,
          "created_utc": "2025-12-27 08:55:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6988j",
              "author": "Mean-Equivalent-624",
              "text": "They dont even have fabs for this so idk what ppl were expecting",
              "score": 26,
              "created_utc": "2025-12-27 10:57:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw5xjtf",
              "author": "MelodicRecognition7",
              "text": "perhaps YMTC will step up",
              "score": 2,
              "created_utc": "2025-12-27 09:02:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzfuqg",
      "title": "Why Kimi K2 Thinking choose Int4 QAT, from infra enginner of KImi",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/",
      "author": "nekofneko",
      "created_utc": "2025-12-30 11:33:10",
      "score": 168,
      "num_comments": 10,
      "upvote_ratio": 0.94,
      "text": "I saw the recent [discussion](https://www.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/) here regarding MiniMax engineer's tweet about why they decided *against* using int4 QAT for the MiniMax M2.1 model.\n\nInterestingly, at the time of the K2 Thinking release, a Kimi infra engineer posted a deep dive on Zhihu explaining why native int4 QAT was actually crucial for them. Iâ€™ve summarized the key takeaways below to offer a different perspective on the 'to quant or not to quant' debate.\n\n**TL;DR:** Kimi found int4 QAT is essential for **MoE latency**, **long-context stability**, and **speeding up the RL training loop**.\n\n# Decoding is Memory-Bound (Latency Focus)\n\nUnlike the MiniMax case, Kimi found that for their specific MoE architecture (which is highly sparse), the decoding phase is almost exclusively memory-bound. By using W4A16 (4-bit weights, 16-bit activations), they reduced memory usage significantly. This allowed the model to fit on fewer GPUs, which reduced inter-device communication overhead, a major factor in lowering end-to-end latency for users.\n\n# PTQ Failed at \"Thinking\" Lengths\n\nThe team initially tried standard Post-Training Quantization (PTQ). While it worked for short responses, it fell apart for the long chain-of-thought \"thinking\" process. As generation length increased, quantization errors accumulated, leading to degradation. Furthermore, PTQ struggled with sparse experts; if an expert wasn't hit frequently during the calibration step with the calibration dataset, it essentially \"forgot\" knowledge. QAT (Quantization Aware Training) was necessary to make the model \"lossless\" compared to the BF16 baseline.\n\n# A less discussed benefit: Faster RL Training\n\nThis is the point that often gets overlooked: Int4 QAT wasn't just for inference serving, it accelerated the training process itself. In Reinforcement Learning, the model spends a massive amount of time in the \"rollout\" phase (generating text). By using the Int4 model for these rollouts, they reduced the total time for an RL iteration by 10-20%. It also reduced the discrepancy between the training forward pass and the inference engine.\n\n# Why Int4 and not FP4?\n\nThey chose standard Int4 over newer formats like FP4 to maintain compatibility with existing hardware (non-Blackwell GPUs) and to utilize mature, highly efficient kernels like Marlin.\n\nIn summary, I believe there isn't a one-size-fits-all answer regarding quantization. It depends heavily on the model's parameters and specific architecture. It is a matter of trade-offs.\n\n[ AI translation, there may be some translation errors.](https://preview.redd.it/dzmceu5zybag1.png?width=1362&format=png&auto=webp&s=a0ba8f78c6e5ade3463a1c62fba1d338a1c01ce9)\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwpqt2p",
          "author": "nekofneko",
          "text": "Source: [Zhihu](https://www.zhihu.com/question/1969558404759544488/answer/1970539327902679960)",
          "score": 17,
          "created_utc": "2025-12-30 11:34:56",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwprq9u",
          "author": "cantgetthistowork",
          "text": "K2 is the only model that remains coherent at the advertised 256k max context",
          "score": 28,
          "created_utc": "2025-12-30 11:42:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq0ka2",
              "author": "nullmove",
              "text": "Kimi linear has frontier level quality in context arena. Waiting for a bigger model powered by KDA. Arcee also recently independently verified that KDA is a real deal.",
              "score": 12,
              "created_utc": "2025-12-30 12:50:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx55qji",
                  "author": "uhuge",
                  "text": "https://huggingface.co/arcee-ai/AFM-4.5B-Base-KDA-Only not great in math, somehowÂ ",
                  "score": 1,
                  "created_utc": "2026-01-01 21:19:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwps0s2",
              "author": "FreePart5727",
              "text": "really? I have to give it a try",
              "score": 2,
              "created_utc": "2025-12-30 11:45:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwsol2a",
              "author": "UnknownLesson",
              "text": "Even better than Gemini Pro?",
              "score": -1,
              "created_utc": "2025-12-30 20:50:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwprqnp",
          "author": "Doris_Dressy1",
          "text": "Thank you for providing another perspective, Iâ€™ve learned a lot",
          "score": 9,
          "created_utc": "2025-12-30 11:42:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuq0zg",
          "author": "Lissanro",
          "text": "INT4 format makes it really great for running locally, as GGUF Q4\\_X which preserves the INT4 quality. I also find its cache memory efficient - full 256K fits in just 96 GB VRAM at Q8. I hope they continue releasing their future large models in 4-bit format.",
          "score": 5,
          "created_utc": "2025-12-31 03:24:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0zk1u",
      "title": "DeepSeek new paper: mHC: Manifold-Constrained Hyper-Connections",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q0zk1u/deepseek_new_paper_mhc_manifoldconstrained/",
      "author": "External_Mood4719",
      "created_utc": "2026-01-01 08:35:29",
      "score": 164,
      "num_comments": 34,
      "upvote_ratio": 0.98,
      "text": "[https://arxiv.org/abs/2512.24880](https://arxiv.org/abs/2512.24880)\n\nhttps://preview.redd.it/bovsed0x8pag1.jpg?width=680&format=pjpg&auto=webp&s=e292dc415f7fda8b1211ffe34864bb25ed4f32fe\n\nhttps://preview.redd.it/g9986afz8pag1.jpg?width=680&format=pjpg&auto=webp&s=fe031ea160ebff21a0dc46196d3dcf3b1b58548b\n\n",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0zk1u/deepseek_new_paper_mhc_manifoldconstrained/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nx1xzgx",
          "author": "Yangmits",
          "text": "First day of the year? Let's get it.",
          "score": 37,
          "created_utc": "2026-01-01 08:37:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx26x0c",
          "author": "SlowFail2433",
          "text": "Normally your gradients go kaboom if you have a deep network with lots of blocks and you donâ€™t have identity residual connections. This goes for both LLMs and things like CNN (specifically resnet in particular.)\n\n\nIs a fairly big deal if this works well. Very tricky paper though I can see this is gonna be tricky to implement both in terms of the mathematics and the CUDA kernel. They had to do some operator fusion at the kernel level to get it efficient. This is gonna take me at least a month to understand tbh\n\n\nThey mentioned that this paperâ€™s framework will allow for other methodologies of expanding residual connections which might be interesting although each would need a new stabilisation method.",
          "score": 45,
          "created_utc": "2026-01-01 10:13:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2itn2",
              "author": "zball_",
              "text": "Quite interesting to see they went back to seek improvement from model topological changes. Within budget of computation they seem really dedicated to squeeze out every bit of parameter potential.",
              "score": 22,
              "created_utc": "2026-01-01 12:15:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2j626",
                  "author": "SlowFail2433",
                  "text": "Yeah Deepseek is just a stunning lab\n\n\nTheir test models are 27B lol",
                  "score": 35,
                  "created_utc": "2026-01-01 12:18:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx5d19u",
                  "author": "SilentLennie",
                  "text": "I mean... I would not be surprised if they just had some people in a corner working on alternatives to what they are already using.",
                  "score": 3,
                  "created_utc": "2026-01-01 21:57:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx3j9ri",
              "author": "IrisColt",
              "text": ">This is gonna take me at least a month to understand\n\n\nOnly a month? I kneel Are you thinking of implementing it?",
              "score": 3,
              "created_utc": "2026-01-01 16:21:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3ruev",
                  "author": "SlowFail2433",
                  "text": "Hmm well it turns out it was not as bad as I thought itâ€™s mostly just some projection matrices and manifold constraints nothing too crazy",
                  "score": 8,
                  "created_utc": "2026-01-01 17:06:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx7pxu4",
              "author": "Mikasa0xdev",
              "text": "Gradients go kaboom is my favorite debugging error.",
              "score": 1,
              "created_utc": "2026-01-02 06:45:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxahp6u",
              "author": "vivianaranha",
              "text": "DeepSeekâ€™s mHC (Manifold-Constrained Hyper-Connections) Architecture\nhttps://youtu.be/BqNkup8ygQE",
              "score": 1,
              "created_utc": "2026-01-02 17:58:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3p1vq",
          "author": "Aaaaaaaaaeeeee",
          "text": "I hope all these improvements to residual connections will make a big impact this year.\nAnother paper: https://arxiv.org/abs/2511.11238 are showing new scaling trends with enhanced residual connections and raising vocabulary size.",
          "score": 14,
          "created_utc": "2026-01-01 16:52:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2d8rf",
          "author": "Redoer_7",
          "text": "For anyone who is as confused as I am. \n\nDeepseek explained in a way an eighty-year-old grandmother could understand: \n\nJust like knitting a sweater, we used to use only one strand of thread, which was easy to tangle and break. Now we use multiple strands together, and we've added a smart thread organizer to prevent tangling. This way, the sweater (AI) we knit is sturdier, more durable, and has more beautiful patterns. The best part is, even though we are using more thread, it actually knits faster and with less effort!",
          "score": 33,
          "created_utc": "2026-01-01 11:19:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2ofab",
              "author": "eternviking",
              "text": ">The best part is, even though we are using more thread, it actually knits faster and with less effort!\n\ninteresting",
              "score": 9,
              "created_utc": "2026-01-01 13:05:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2wo48",
                  "author": "Brainlag",
                  "text": "Not really true. It is slower to train than without mHC, but usually this would be much slower and with a lot of clever tricks they got it down to around ~7% overhead. Which makes this viable in the first place.",
                  "score": 12,
                  "created_utc": "2026-01-01 14:06:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2ompm",
              "author": "Odd-Ordinary-5922",
              "text": "appreciate this",
              "score": 5,
              "created_utc": "2026-01-01 13:06:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx3doi1",
              "author": "Zc5Gwu",
              "text": "But can it make cookies like my grandma can? I think not.",
              "score": 2,
              "created_utc": "2026-01-01 15:51:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx3q4a1",
              "author": "Sad-Size2723",
              "text": "reads like AI generated",
              "score": -4,
              "created_utc": "2026-01-01 16:57:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx47zqm",
                  "author": "verylittlegravitaas",
                  "text": "They said it was.",
                  "score": 7,
                  "created_utc": "2026-01-01 18:28:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2sp7q",
          "author": "cantgetthistowork",
          "text": "ELI5 version pls",
          "score": 5,
          "created_utc": "2026-01-01 13:38:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2tka1",
              "author": "erraticnods",
              "text": "more performance without the model collapsing in on itself, total breakthrough if implemented properly and works as they describe",
              "score": 8,
              "created_utc": "2026-01-01 13:44:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2vpby",
          "author": "kinkvoid",
          "text": "Although i don't use deepseek much but it has my respect.",
          "score": 3,
          "created_utc": "2026-01-01 13:59:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3lvip",
          "author": "send-moobs-pls",
          "text": "I love these dudes",
          "score": 2,
          "created_utc": "2026-01-01 16:35:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx92kdt",
          "author": "ahmealy_",
          "text": "Simply explained with numerical examples\nhttps://medium.com/@ahmealy/deepseeks-manifold-constrained-hyper-connections-explained-simply-with-numeric-examples-713f1e5d3a70",
          "score": 2,
          "created_utc": "2026-01-02 13:42:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4pmu5",
          "author": "jinnyjuice",
          "text": "How large would be hyper?",
          "score": 1,
          "created_utc": "2026-01-01 19:55:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5md8z",
          "author": "Turbulent_Pin7635",
          "text": "So another nVidia nuke on the house?",
          "score": 0,
          "created_utc": "2026-01-01 22:46:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyjjbw",
      "title": "Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1pyjjbw",
      "author": "Nunki08",
      "created_utc": "2025-12-29 11:02:29",
      "score": 158,
      "num_comments": 31,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwj1ymt",
          "author": "Paramecium_caudatum_",
          "text": "**HyperCLOVAX-SEED-Omni-8B**\n\nhttps://preview.redd.it/yynuvmjko4ag1.png?width=261&format=png&auto=webp&s=9f1058d58a5826b3c21476cadc468472931b5454",
          "score": 34,
          "created_utc": "2025-12-29 11:27:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjrbcp",
              "author": "AfterAte",
              "text": "I wonder how quickly all those parts will work together. Can't wait for a YouTuber to demo it for my lazy ass.",
              "score": 8,
              "created_utc": "2025-12-29 14:22:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwoovz0",
                  "author": "Mikasa0xdev",
                  "text": "YouTube demos are the real benchmarks.",
                  "score": 2,
                  "created_utc": "2025-12-30 05:55:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwkcz37",
                  "author": "Odd-Ordinary-5922",
                  "text": "are you thinking of the same youtuber im thinking of?",
                  "score": 1,
                  "created_utc": "2025-12-29 16:12:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwn37s2",
              "author": "Different-Toe-955",
              "text": "48GB is surprisingly small. I bet a Q4 will fit on 16gb vram.",
              "score": 4,
              "created_utc": "2025-12-30 00:14:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwjaozw",
          "author": "FinBenton",
          "text": "Hmm sounds like it can do audio to audio?",
          "score": 12,
          "created_utc": "2025-12-29 12:37:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjh0aa",
              "author": "pkmxtw",
              "text": "Would be interesting how well it works. It is the end of 2025 and we still don't have anything that is close to dethrone Sesame.",
              "score": 16,
              "created_utc": "2025-12-29 13:21:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjija8",
                  "author": "bhupesh-g",
                  "text": "Sesame is pretty awesome, I really want something like that on local. Sesame is the best audio to audio experience I ever had. I will not even talk to chatGPT anymore",
                  "score": 3,
                  "created_utc": "2025-12-29 13:30:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwna1pi",
                  "author": "Artistic_Okra7288",
                  "text": "Have you tried out Qwen3-Omni? It can supposedly do audio to audio. Never heard of Sesame tho.",
                  "score": 3,
                  "created_utc": "2025-12-30 00:51:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwjeo3k",
          "author": "ridablellama",
          "text": "i love omnis",
          "score": 11,
          "created_utc": "2025-12-29 13:05:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwizptq",
          "author": "jacek2023",
          "text": "I remember there were going to be a few new models released from Korea at the end of the year, is this one of them?",
          "score": 14,
          "created_utc": "2025-12-29 11:07:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj1v48",
              "author": "ELPascalito",
              "text": "I'd say yes, albeit the hyperclova models have been round for a while, and my complaint is always the same, change the damn name, it's so long and uncatchy ðŸ˜…",
              "score": 8,
              "created_utc": "2025-12-29 11:26:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwlhrrg",
                  "author": "brahh85",
                  "text": "Faker 1T ,  catchy name for a korean model with 1 trillion parameter",
                  "score": 4,
                  "created_utc": "2025-12-29 19:23:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwj0nsz",
          "author": "HumanDrone8721",
          "text": "Are they compatible with the \"usual suspects\" (llama.cpp, vLLM, SGLang...) or we need to wait until integration?",
          "score": 10,
          "created_utc": "2025-12-29 11:15:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj4n60",
              "author": "hustla17",
              "text": "not directly answering but it seems that [https://github.com/NAVER-Cloud-HyperCLOVA-X/OmniServe](https://github.com/NAVER-Cloud-HyperCLOVA-X/OmniServe) \n\nis an inference system  created for running  said model",
              "score": 12,
              "created_utc": "2025-12-29 11:49:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwj5dgf",
                  "author": "HumanDrone8721",
                  "text": "Cool, I've seen their driver in their model card, but thought, \"yeah, specialized stuff for this class of models, let's see when the bit three will integrate it..\", but looking closer they seem to have a vLLM plugin. That's making the model more interesting.",
                  "score": 5,
                  "created_utc": "2025-12-29 11:55:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwjr2nz",
          "author": "AfterAte",
          "text": "Hardware requirements is 48GB I assume at fp16 as they mention the main 8B model is 16GB.\n\n\nMost of us are going to need quantized models before using it.",
          "score": 6,
          "created_utc": "2025-12-29 14:21:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlms6y",
          "author": "Bitter-Breadfruit6",
          "text": "I'd like the next model to be at least 70b in size. Ultimately, it seems like performance is largely determined by the model parameter size.",
          "score": 1,
          "created_utc": "2025-12-29 19:47:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm0g1l",
          "author": "autodidacticasaurus",
          "text": "Damn these names are getting crazy. 8B though, you say. Hmm.",
          "score": 1,
          "created_utc": "2025-12-29 20:54:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwn2y72",
          "author": "Different-Toe-955",
          "text": "ohhhh shit this is the kinda stuff I've been waiting for. I hate the fact that LLMs when integrated with web browser plugins still hallucinate stuff. This giving them the ability to actually browse the web should help that.",
          "score": 1,
          "created_utc": "2025-12-30 00:12:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwk6bok",
          "author": "implicator_ai",
          "text": "When they say â€œdiffusion language model,â€ it usually means the model refines a whole sequence (or chunks) over a few denoising steps instead of generating strictly left-to-right token-by-token, which can trade fewer sequential steps for more parallel work. \n\nThe 3â€“6Ã— claim is worth sanity-checking against the exact setup: GPU type, batch size, context length, quantization, and decoding parameters (steps / temperature / top-p), because those can swing throughput a lot. If you try it, posting tokens/sec + latency at a fixed prompt length and a fixed quality target (e.g., same math benchmark score) would make the comparison much more meaningful.",
          "score": 1,
          "created_utc": "2025-12-29 15:40:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkwavi",
          "author": "fiddler64",
          "text": "is it a meme or a fact that llamacpp hates omni models",
          "score": 1,
          "created_utc": "2025-12-29 17:44:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwmklyg",
          "author": "Comacdo",
          "text": "llamacpp support when ? ðŸ’€",
          "score": 1,
          "created_utc": "2025-12-29 22:33:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pwwsag",
      "title": "The Infinite Software Crisis: We're generating complex, unmaintainable code faster than we can understand it. Is 'vibe-coding' the ultimate trap?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/",
      "author": "madSaiyanUltra_9789",
      "created_utc": "2025-12-27 12:33:15",
      "score": 155,
      "num_comments": 140,
      "upvote_ratio": 0.81,
      "text": "Hey everyone, \n\nI just watched [The Infinite Software Crisis â€“ Jake Nations](https://www.youtube.com/watch?v=eIoohUmYpGI) on YouTube and it got me thinking... the limitations of software development has never been typing speed, but rather our ability to comprehend and design the system correctly in the first place. \n\n**Highlights from the talk:**\n\n* Every developer has shipped code they didn't completely understand. it passed the tests and that was enough validation to deploy it.\n* **The hard part is timeless:** The hard part isn't the mechanics of coding; it's the conceptual difficulty of designing a solution. Every tool, including AI, just makes implementation easier.\n* **AI amplifies the problem:** We can now generate code as fast as we can describe it. The scale is infinite, but our comprehension isn't. The core challenge of understanding *what* to build remains.\n* The real trap we fall into is confusing easy with simple.\n   * **Easy** is what's within reach. What can you access without effort? Generate it with AI, copy-paste, or install a framework. It's about speed.\n   * **Simple** is about structure. It means one fold, one braid, no entanglement. It requires thought and design.\n* LLMs do not understand logic, they merely relate language and substitute those relations as \"code\", so the importance of *patterns and architectural decisions* in your codebase are lost. \n* when \"vibe-coding\" technical debt doesn't register as debt; it's just more code to preserve. \n* The result? Complex, highly-coupled, and error-prone code generated in minutes that could take you weeks to understand (if ever).\n\nThe real danger here is that we're accumulating complexity faster than we can comprehend it because we're not doing the hard work of understanding our systems.\n\nThe proposed solution: SLOW DOWN, DO EVERYTHING MANUALLY; architectural design + scaffolding, etc and only let the LLM in at the last step of filling in the scaffolding. \n\nWhat's your take, Is 'vibe-coding' a trap, or is there a way to use these tools without losing the ability to understand our systems? \n\nhttps://preview.redd.it/c4mknoudlq9g1.png?width=553&format=png&auto=webp&s=28a6f37623fb0e0725f5b603f4b3a8ce51653ac9\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nw6o9a5",
          "author": "kevin_1994",
          "text": "When I was a junior, I was tasked with architecting a service on the backend. Using all the skills university prepared for me, I implemented a facade pattern with a bunch of complex logic to ensure the code was as abstract and \"correct\" as possible. \n\nMy senior rejected my pull request. He said my solution was technically correct, but too hard to understand. He told me just copy paste the code in a couple places if necessary, because two years from now, nobody is gonna remember this complex abstraction pattern, but everyone can follow a simple class that maybe isnt perfect, but is easy to understand. \n\nI remember he told me that \"debugging and maintaining code takes 50% more brainpower than writing it, therefore, by definition, using 100% of your brain to write code is unmaintainable and impossible to understand\". I always remembered that \n\nAI are like the overeager junior who just graduated from university who wants to write everything the \"correct\" way but doesnt have 10 years of debugging experience to understand how to write code that you can understand and scale years later. \n\nI really fear for companies embracing vibecoding. Vibecoding doesnt show the type of wisdom and restraint that experience teaches a developer over decades.",
          "score": 196,
          "created_utc": "2025-12-27 13:08:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6u24u",
              "author": "vimbaer",
              "text": "That was some good advice you got there!",
              "score": 39,
              "created_utc": "2025-12-27 13:47:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw70ls2",
              "author": "SkyFeistyLlama8",
              "text": "Trying to debug vibed code with extra layers of abstraction takes more time than just writing the damn thing myself. I'll stick to using local LLMs at a class or function level only. Sometimes I bounce architecture ideas off a cloud LLM but I never ask it to write code.",
              "score": 32,
              "created_utc": "2025-12-27 14:28:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw724cg",
                  "author": "kevin_1994",
                  "text": "I do the same. I use ai assisted coding for\n\n- autocomplete\n- bash scripts \n- brainstorming \n\nI've never been satisfied with agentic coding because they make so many simple mistakes that I feel like im teaching another junior. One great example is that LLMs are trained on a lot of tutorial style code, and subsequently, nearly always believe that the caller should handle errors/exceptions, without any understanding that errors are useful in code--silently handling fatal states makes your application impossible to understand.",
                  "score": 31,
                  "created_utc": "2025-12-27 14:38:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwg97au",
                  "author": "LoSboccacc",
                  "text": "Yeah the only way to get medium size project done is strict interfaces between component and typed object passing everywhere, the llm can fill the methods.Â \n\n\nMaybe some bulk transform too but they tend to lazy out in the cleanup.Â \n\n\nHelps forbidding adapters, fallbacks, local state caves and defaults so the code crashes fast and code has a single source of truth it must locate,Â  paired with strong validation on the data model, otherwise llm will just trap errors and zero variables to get the thing running (but not working)\n\n\nAlso helps maintaining class or component responsibility index somewhere and send llms to find leakages.",
                  "score": 2,
                  "created_utc": "2025-12-28 23:34:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7jycu",
              "author": "zeth0s",
              "text": "I do not agree with your senior. A good abstraction make it easier to maintain and migrate code. If the code that you copied and pasted have a bug, or requirements change, you now have to change each of your snippets. And each of them will diverge long term.\n\n\nI'd have rejected your senior PR. Copying and pasting code with actual logic is a gate to maintainance hell.Â \n\n\nCognitive complexity can be kept low in many way, without polluting code base with snippets",
              "score": 22,
              "created_utc": "2025-12-27 16:15:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7s93p",
                  "author": "moofunk",
                  "text": "I would agree with the senior, but it depends on circumstances. Might make more sense in that project to copy, so you understand in-situ what's going on without needing to comment. Sometimes that doesn't make sense, and that depends on the code you end up with.\n\nAbstractions have to deal with proper division of labor between the layers, and getting this wrong or abstracting too much, makes the abstractions harder to understand.",
                  "score": 9,
                  "created_utc": "2025-12-27 16:56:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7k76v",
              "author": "Roid_Splitter",
              "text": "Besides the damage to projects that you describe, we will also lose an entire crop of juniors.",
              "score": 5,
              "created_utc": "2025-12-27 16:16:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwb5e1f",
                  "author": "madSaiyanUltra_9789",
                  "text": "no one is thinking about the fact that you need juniors to get senior developers, we will realize the results of this 10yrs from now as the workforce begins to shrink involuntarily .",
                  "score": 2,
                  "created_utc": "2025-12-28 04:05:45",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7d16p",
              "author": "swagonflyyyy",
              "text": "%100 agree vibe coding is a trap that turns your code into a tangled black box.Â \n\n\nThen you have to use other AIto help debug it but at the end of the day the issue is unavoidable, you gotta do it yourself.",
              "score": 8,
              "created_utc": "2025-12-27 15:40:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw6vr5q",
              "author": "Muritavo",
              "text": "Also, every pattern is like a solution for a problem...   \n  \nFor example, there is no need for someone to implement and interface, specialized classes, abstraction etc, If you only need a single instance and a simple module with all methods solves your case.",
              "score": 6,
              "created_utc": "2025-12-27 13:58:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw6yt28",
              "author": "lonesomewhistle",
              "text": "And now we have AI approving PRs.",
              "score": 6,
              "created_utc": "2025-12-27 14:17:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7eunl",
                  "author": "redballooon",
                  "text": "Thatâ€™s a win, if the second pair of hums eyes stays in the process.",
                  "score": 7,
                  "created_utc": "2025-12-27 15:49:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw8fscg",
              "author": "Material_Policy6327",
              "text": "Yeah I totally get this. I am an AI researcher and while vibe coding can feel fun holy shit the code it makes is so confusing even when it is technically correct",
              "score": 1,
              "created_utc": "2025-12-27 18:55:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw8sj02",
              "author": "WitAndWonder",
              "text": "This is why I stick to coding apps that are basic in principle but expensive in content with AI. The actual Apps are quite contained, but often involve hundreds or thousands of individual JSONs for things like abilities/traits/whatever that are simple to understand but would be tedious as hell to customize by hand. It's an actual godsend for just reducing repetitive tasks like that. Build the structure of the files yourself, give it some samples, and let it rip the other ten out with some descriptions of mechanics or flavor text involved.",
              "score": 1,
              "created_utc": "2025-12-27 20:02:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw9zwer",
              "author": "PunnyPandora",
              "text": "You forget the benefit of vibe coding, being able to undo a 10 step mistake in a single step. Even if it was self induced, it's a lot more steps out the way and something on the table. I'd never be able to iterate or plan or get new ideas with 0 accumulation. Having a perfect plan rarely works out in practice, I need there to be something whether it works or fails to decide where to go from there. Like even if I spend days on writing plans, it's not guaranteed to go according to that and I might change my mind along the way as well. Being able to change my mind 3 times with no punishment is unheard of",
              "score": 1,
              "created_utc": "2025-12-28 00:00:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwavdw0",
              "author": "Zhanji_TS",
              "text": "You guys are still reading the code?",
              "score": 1,
              "created_utc": "2025-12-28 03:04:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw6v3dt",
              "author": "gscjj",
              "text": "You hinted on the solution. Itâ€™s not that embracing vibecoding is bad, is that you equally need to embrace the senior who rejected your PR. \n\nYou are capable of understanding the implication, so is an AI if requested. You just need someone to say itâ€™s over complicated and unmaintainable.",
              "score": 1,
              "created_utc": "2025-12-27 13:54:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw9zj8a",
              "author": "giant3",
              "text": ">  He told me just copy paste the code in a couple places if necessary,\n\nI would fire your senior. Multiple copies that could go out of sync and the person changing the code wouldn't know that it also exists in other places. \n\nI have been writing software for 30 years now and I have seen this exact bug in production code a few times.",
              "score": 1,
              "created_utc": "2025-12-27 23:58:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7pttj",
              "author": "Lifeisshort555",
              "text": "I think the inverse is true now. The LLM will have little trouble understanding your code. The next person is not even trying at this point if they cannot understand your code with the help of an LLM.",
              "score": 0,
              "created_utc": "2025-12-27 16:44:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw90hdn",
                  "author": "YoAmoElTacos",
                  "text": "This can be quite untrue - LLMs can happily code themselves into state traps that are painful to debug,  with abstractions they are only vibe understanding with pattern matching.\n\nYou actually need to write code that the LLM understands given its limitations, sticking to what the LLM innately knows or can remember from comments and memory files. LLMs can have trouble with large complex balls of code like humans, so it's always better to document, compartmentalize, create good extractions, etc.",
                  "score": 3,
                  "created_utc": "2025-12-27 20:45:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw9tmsl",
                  "author": "HopefulMaximum0",
                  "text": "You seem to be under the impression that your PR was rejected because they did not understand your LLM's work.\n\nIt was rejected because it was a pile of bad code, it did not do what was to be done, the tests were not passing, incomplete, or it just was a general mess.   Not because you are a genius and the senior is a dumb dinosaur.\n\nCode is not written for machines, it is written for the humans tending to the machine.   The machine uses object code, and will follow any messy instruction list without missing a beat;  seniors know that finding out what the code really does (not what you think it does) demands well-written code.  Debugging messy code is hell, and costs big bucks when you have to root out unexpected bad behavior that took down the money-making code.",
                  "score": 2,
                  "created_utc": "2025-12-27 23:24:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwb5vf7",
                  "author": "madSaiyanUltra_9789",
                  "text": "if you want to know how much an LLM \"understands\" code, you can simply swap out all the variable names for unrelated symbols/names and it will become apparent there is no understanding going on, or rather that the understanding is predicated on how well your codebase is structured (and named) to begin with ironically.",
                  "score": 1,
                  "created_utc": "2025-12-28 04:08:52",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6yyhr",
          "author": "Boring_Respect_7559",
          "text": "No. Offshore resources have been doing the equivalent of vibe coding for years. This isnt new.",
          "score": 26,
          "created_utc": "2025-12-27 14:18:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgvspv",
              "author": "Danger_Pickle",
              "text": "This. Over the last two-ish years Git shows I'm net negative by around half a million lines of code. About half of that is dumb files that shouldn't be in the repo, but I suspect half of that is actual code I removed from the project. Last week alone I reduced the codebase by another two thousand lines of code while fixing bugs and deploying new features. The entire codebase was written by a revolving door of the lowest bidder, and it shows.",
              "score": 2,
              "created_utc": "2025-12-29 01:37:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwb7gwv",
              "author": "madSaiyanUltra_9789",
              "text": "i was waiting for someone to write this lmao. \n\nit's true but also untrue.. There are a hand-full of elite 100x SWE but the majority (\\~80%) are always subpar-to-average devs in every country. SWE is a heavily asymmetric talent distribution and that talent doesn't reside in any one nationality. \n\n  \nthis is my anecdotal observation having hired SWEs across Asia, India, Europe, Arabia, USA, etc.",
              "score": -3,
              "created_utc": "2025-12-28 04:19:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwgx4wd",
                  "author": "Danger_Pickle",
                  "text": "I'm disagreeing with you, not because it's impossible for competent developers to exist in India (I've worked with several competent devs from India) but because companies offshore to save money. It's the incentive structure that's at the root of the problem. Companies which offshore development resources and inflict frustrating time zone issues on their team are always going to make the decision to cut quality in order to save costs. Not even the most skilled developer in the world can produce high quality results in a company that explicitly forbids focusing on quality because they're too focused on cutting costs.\n\nThe ratio of your post says that your \"don't worry, local developers are bad too\" argument doesn't match most people's expectations. Offshoring is great at cutting costs, but it's never going to result in a higher quality product.",
                  "score": 2,
                  "created_utc": "2025-12-29 01:45:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6tfnj",
          "author": "Mr_Gaslight",
          "text": "There'll be money to be made in cleaning it up and providing documentation.",
          "score": 10,
          "created_utc": "2025-12-27 13:43:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcvvf0",
              "author": "maz_net_au",
              "text": "Yeah, but the look on people's faces when you tell them that cleaning it up and making it production ready is 50% more than the cost to build it from scratch... that's priceless.",
              "score": 3,
              "created_utc": "2025-12-28 13:06:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwc9yh0",
              "author": "emteedub",
              "text": "Lol yeah this isn't such a bad thing. Let them vibe code it, bring the jobs back",
              "score": 2,
              "created_utc": "2025-12-28 09:50:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw73bv8",
          "author": "djfrodo",
          "text": "There's a great article (can't find the link) about the NASA programmers/software engineers and their development process.\n\nIt describes what the men and women who write the code to control the space shuttle or the ISS do in a day.\n\nWriting 200 lines of code a day was the norm, 400 was the extreme. Most of the code written would sit for weeks/months/years until it was folded into the actual software that was used.\n\nMultiple (an insane amount of) people would review it.\n\nThen they would do it again, and again, and again.\n\nBasically what I'm saying here is simplicity is key and **much** better than over engineered crap.\n\nVibe coding seems to produce the latter.\n\nFor web/mobile apps that are MVPs I guess that's fine, but I'd much rather have very primitive/basic code that slowly evolves over time than an instant solution that no one really understands.\n\nVibe coding is fine for simple stuff, but I wouldn't rely on it for anything complex or mission critical.",
          "score": 18,
          "created_utc": "2025-12-27 14:45:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw72w0r",
          "author": "typical-predditor",
          "text": "Lmao, this isn't new to AI. We already have complex unmaintainable code thanks to the large number of fraudulent CS degrees and code camp certifications. It takes a really long time for technical debt to manifest and the real cost of cheap code monkeys doesn't manifest until after several great quarters have posted and the hiring manager might have moved on by that point.",
          "score": 10,
          "created_utc": "2025-12-27 14:42:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7jbc1",
          "author": "ithkuil",
          "text": "Leading edge LLMs like Opus 4.5, Gemini 3 and even to some degree GLM 4.7 are actually great at system architecture. If you set up an agent with instructions and structure focused on strong architecture, coherent but decoupled design, and managing technical debt, the agent can often handle that.\n\n\nIts not quite at the point _yet_ where it really couldn't use the help of a human every now and then. But the models continue to improve.\n\n\nIn the upcoming zero to five years there will be multiple innovations making it even easier to create and maintain software with AI. First of all, inference speed. Cerebras has already demonstrated how 10-20 times faster agentic loops with SOTA models change things.\n\n\nYou will also have models designed to render productivity applications in real time frame by frame, similar to the interactive world/game models we have today.\n\n\nAnd there will be models that deeply integrate a virtual machine and set of APIs throughout training to make software generation and iteration faster and more robust.",
          "score": 6,
          "created_utc": "2025-12-27 16:11:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8zqt6",
          "author": "vosegus91",
          "text": "I dont give a shit honestly.\nI create my projects that I previously couldn't.",
          "score": 6,
          "created_utc": "2025-12-27 20:41:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwa9n1h",
              "author": "madSaiyanUltra_9789",
              "text": "vibe-coding definitely allows more people to engage in SWE and build virtual products where the barrier to entry was previously very high both technically and financially. i suppose it's less about been able to build individual products but what happens when you try to translate this LLM tech to production grade software that effects thousands to millions of people, can it be done efficiently, or are we better just scrapping it for the most part.",
              "score": 1,
              "created_utc": "2025-12-28 00:55:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nweeypt",
              "author": "danielfrances",
              "text": "Right? I've had a bunch of project ideas on the shelf for years because I just don't have the time to do them. Agent LLMs have given me the ability to make them with the same limits on my time.\n\nAlso, a lot of people are judging LLMs at a point in time and never accounting for the fact that they continue to improve, and quickly, every few months.\n\nThe vibe coding experience right now is night and day better compared to what I was dealing with over the summer. This time next year, who knows how much better the experience will be.\n\nI can imagine a future some years down the road, where I can go for a hike in the woods, and while doing that talk to an agent on my phone, who will have been talking with me and have built out a whole new feature set by the time I get home, ready for me to play with. That sounds awesome to me.",
              "score": 1,
              "created_utc": "2025-12-28 18:08:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwestl1",
                  "author": "vosegus91",
                  "text": "I will say this, that with claude code i was able to execute statistical pipelines that I read about and learned about but simply could not execute\n I am a medical resident with 2 small kids at home, I know basic code but not enough time or energy to really polish this capability. \nIs my code is commercial ready? Lol probably not i guess? I dont really care tho? It simply help me out reaching discoveries and getting to my goals.",
                  "score": 2,
                  "created_utc": "2025-12-28 19:12:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6xu88",
          "author": "FastDecode1",
          "text": "If AI didn't solve your problem, you didn't use enough AI.\n\nDemi-jokes aside, this just seems like history repeating itself. Companies used to hire armies of programmers when what they needed were software engineers. Programming is just one part of software development, you also need requirements analysis, design, testing, maintenance...\n\nVibe coding is the \"cool thing\" because programming is the exciting part, and people usually associate problem solving with writing code. But when you're vibe coding a script or small program to automate something as part of your hobby or just for fun, your standards are likely a lot lower than if you work in the software field professionally.\n\nThere's a good reason agentic use-cases are a major focus now. A programmer can't replace a team of software engineers. Whether that programmer is a human or an LLM is irrelevant.",
          "score": 16,
          "created_utc": "2025-12-27 14:11:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6rg8b",
          "author": "Low-Opening25",
          "text": "This has always been the case, AI code is actually less slop than majority of what lurkers in private repos that never see a light of day but actually run at core of most companies",
          "score": 22,
          "created_utc": "2025-12-27 13:30:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7kvsi",
              "author": "zeth0s",
              "text": "Check Microsoft R library (open source because R force open source). A paid solution embedded in their ML servers.\n\n\nQwen 2.5 already could write better code than Microsoft engineers.Â \n\n\nThe problem is not the whole quality, which is already better than your average Accenture consultant.\n\n\nThe problem is the amount of code LLMs can spit out per minute, and the fake sense of empowerment that it gives to people who despise software engineers.",
              "score": 14,
              "created_utc": "2025-12-27 16:19:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw87dag",
                  "author": "Low-Opening25",
                  "text": "lets be honest, sloppy code is the reason most of us have jobs, so the more the merrier",
                  "score": 5,
                  "created_utc": "2025-12-27 18:13:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw772sq",
          "author": "FinBenton",
          "text": "Every time you need to maintain it, a new model is out that will do better job at it, I dont see it a problem.",
          "score": 4,
          "created_utc": "2025-12-27 15:07:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7ck61",
          "author": "GCoderDCoder",
          "text": "I think I agree with the consensus  I've been seeing that AI is great for prototyping to proof out an idea then you step back and use good engineering to build the blocks the way we learned previously but you can use AI to fill in the semantics faster on what you already know. You should understand every line and every decision at least conceptually. You should make the decisions moving forward not the LLM. If you dont understand the options then use the LLMs to help you learn faster. Still, you should be driving the architecture design and the LLM should not be doing more than aggregating and playing an interactive rubber duck. \n\nThese are wonderful text generators and their logic is a byproduct of how we use text not them thinking. I also feel they give me practice at interpersonal skills that tend to atrophy when I'm on the command line a lot. I stutter less now and form my thoughts better speaking since I use normal English to handle my tasks more than pre-LLMs.\n\nWe use best practices for software engineering because those processes manage the sprawl and require code reviews. We still can only ship code as fast as we can understand it should be the philosophy IMO.",
          "score": 3,
          "created_utc": "2025-12-27 15:37:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9at7d",
          "author": "AuntMarie",
          "text": "My experience is that 80% of the code is non critical edge functionality that a coding agent can write without me understanding and 20% is critical that a coding agent can help with, but i need to understand and clean up myself. (Important to note that i write software that does not need to be maintained long term by others)",
          "score": 5,
          "created_utc": "2025-12-27 21:41:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwa8onp",
              "author": "madSaiyanUltra_9789",
              "text": "interesting, i would have guessed the \"edge cases\" would be the ones with serious ramifications if you got it wrong. i suppose this is more nuanced and purpose/industry specific, but with financial matters for eg overlooking an edge case would mean the end user can inflate their financial balance or credit more then they should have access (even applies to Saas). \n\nthey only place where i'd agree with you fully is when flawed edge-case implementations have inconsequential outcomes.",
              "score": 1,
              "created_utc": "2025-12-28 00:49:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw6yglc",
          "author": "decrement--",
          "text": "Well have better AI in the future to clean up the mess.",
          "score": 7,
          "created_utc": "2025-12-27 14:15:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6pzo0",
          "author": "TheTrueGen",
          "text": "I think it is still viable for MVPs. Once you scale, you pay someone to clean it up.",
          "score": 9,
          "created_utc": "2025-12-27 13:20:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7kezt",
              "author": "Roid_Splitter",
              "text": "Yeah, but then cleaning up your 500.000 lines of code will cost 10x as more as paying someone to write the 50.000 lines of code you actually needed.",
              "score": 5,
              "created_utc": "2025-12-27 16:17:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7lvl9",
                  "author": "TheTrueGen",
                  "text": "Having 10.000 iterative meetings on the requirements with the dev is also not very efficient. Most devs know how to code, but have no product sense.",
                  "score": 1,
                  "created_utc": "2025-12-27 16:24:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8b5qy",
          "author": "Igot1forya",
          "text": "As a dude with zero coding skills, me looking at well structured code is meaningless, as it's still gibberish to me. I have been trying to get my DGX Spark which is running native CUDA 13 code to work on older well coded (I assume) projects. I have successfully rebuilt a bunch of Git repos and added Blackwell compatibility. All I did was take the source code, dump it into an AI and it fixed it. I assume, a seasoned coder would accomplish the same thing in a longer time span. I'm unclear as to how anyone really needs to understand anyone else's code if you can simply have an AI audit and patch the code. Is this a technology problem seeking a technology solution? Like you're identifying the potential future problem, but can't an AI be used to solve this problem (if not already solved)?.",
          "score": 3,
          "created_utc": "2025-12-27 18:32:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8gaun",
              "author": "bigblockstomine",
              "text": "For a hobby/passion project that nothing important depends on? Youre probably ok. For professional stuff that people or money depend on? What youre doing is a slowmo train wreck. A professional, non retarded, non fraudulent human is going to know these future problems youre talking about in seconds and avoid them completely, whereas AI will f&*^ everything up and claude says \"oops, youre right, your HFT bots just lost 10000 usd because i didnt tell you about sm90/sm8x compatability issues with other softwares when you ported the CUDA code, lets try this...\".",
              "score": 0,
              "created_utc": "2025-12-27 18:57:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw8iohs",
                  "author": "Igot1forya",
                  "text": "Sure, you have a point, but at the same time this is something a model could be trained to deal with. I'm not saying coders and architects day in the sun has set, but this like anything else is a tool refinement issue. We are literally seeing these tools get better and better and while the stuff being churned out is admittedly garbage hack-stein works, it's pretty much going to get solved in short order. Necessity is the mother invention and we are at a stage in history where invention is easier than ever for the common folk. We're also talking about business here and when money is involved, investments into better solutions will spawn from it. It's great to raise a concern, but this sounds like a business opportunity and on this frontier, the person to plant their flag first could be in for billions.",
                  "score": 1,
                  "created_utc": "2025-12-27 19:10:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8cbck",
          "author": "alexeiz",
          "text": "Vibe coders don't want to understand code. Heck, they don't even look at the code at all. They prompt until \"it works\". They actually act as if the code doesn't exist at all. It's quite a fascinating phenomenon.",
          "score": 3,
          "created_utc": "2025-12-27 18:38:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8zrnp",
          "author": "rc_ym",
          "text": "I think it's funny folks think that they'll fix problems in code rather than just vibecode a new app.   Outside some specific use cases (embedded systems, core db software, etc.) I believe most software will eventually be ephemeral, and that writing software that's intended to persist will be seen rather like working on mainframes or OS.  Sure folks do that, but it's a tiny part of code that gets created. \n\nThat makes the \"infinite software problem\" moot.  It's not a problem.  It's the new normal.",
          "score": 3,
          "created_utc": "2025-12-27 20:41:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgavwi",
              "author": "LoSboccacc",
              "text": "This. If llm become cheap and reliable enough why even have software. we'll have games because that's entertaining us, buy we'll not have word to do document we'll ask. We'll not go to a website to book an appointment, we will ask.Â \n\n\nHeck the other day a friend that didn't like fish wanted to join us at a local sushi restaurant and i just downloaded the menu and asked a llm to list non fish plates, and then I gave him his menu.Â \n\n\nWe will have our agent that know us, consuming apis. And behind apis there will not be code, but other agents that know the product, or the service, and know how to book appointment and sell items, and will use other api to schedule logistic, behind which again minimal code, and llm that will send humans to deliver.Â ",
              "score": 1,
              "created_utc": "2025-12-28 23:43:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9plpj",
          "author": "mabuniKenwa",
          "text": "Post written by AI, ironically",
          "score": 3,
          "created_utc": "2025-12-27 23:01:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwa5t1a",
              "author": "madSaiyanUltra_9789",
              "text": "Don't judge me lmao... it was written with \"AI-assistance\" like everything else on the internet nowadays. \n\nit's like we don't even trust our own writing ability anymore",
              "score": 0,
              "created_utc": "2025-12-28 00:33:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwabrtu",
                  "author": "mabuniKenwa",
                  "text": "You wrote a post about AI not being reliable â€¦",
                  "score": 2,
                  "created_utc": "2025-12-28 01:07:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7gg1z",
          "author": "PlainBread",
          "text": "I've experimented with 80/20 vibe coding where I act essentially as a project manager and learning how to compensate for the inadequacies of AI on the project level, and I've also done 20/80 vibe coding where I am actually doing all the coding but I am asking the LLM as a verbal \"cheat sheet\" for coding concepts and also having it do rubber duck debugging of my code.\n\nThe latter is superior in terms of what you get in the end and actually knowing the intention behind every piece of code.",
          "score": 6,
          "created_utc": "2025-12-27 15:57:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcbmx2",
              "author": "Dramatic_Pen6240",
              "text": "Can you describe more your experience?Â ",
              "score": 1,
              "created_utc": "2025-12-28 10:06:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwd916w",
                  "author": "PlainBread",
                  "text": "https://preview.redd.it/wdxench3hy9g1.png?width=220&format=png&auto=webp&s=f72329f97e7f45a34d63772d446306a1f0f80e4b\n\nI don't write free articles.  I'm not an LLM.",
                  "score": 1,
                  "created_utc": "2025-12-28 14:32:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6oxys",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 9,
          "created_utc": "2025-12-27 13:13:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6sikr",
              "author": "egomarker",
              "text": "If real experienced human is supervising it and applying the same code review practices as you'd apply to junior-level engineer, it'll be fine. If you are blindly vibecoding (and even worse blindly use agentic coding), it's just an elaborate way of shooting your future self in the leg..",
              "score": 13,
              "created_utc": "2025-12-27 13:37:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw6qn99",
              "author": "Fast-Satisfaction482",
              "text": "Legacy code bases tend to become replaced or too big to fall if not replaced soon enough. It would hope that AI will both push the maintainability frontier and the viability of full replacement forward.\n\n\nSo ideally, liquidating the technical debts will just mean to break the requirements of the spaghetti module into a few smaller modules and vibe-replace them.Â ",
              "score": 2,
              "created_utc": "2025-12-27 13:24:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6zwm2",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 5,
                  "created_utc": "2025-12-27 14:24:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwae1yu",
              "author": "TheRealMasonMac",
              "text": "As a systems developer, I feel vindicated in my utter hatred of JS and its ecosystem. All hail C/C++/Rust!",
              "score": 1,
              "created_utc": "2025-12-28 01:21:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw798uz",
          "author": "zhambe",
          "text": "No, it's a phase. The euphoria will die down as the \"new\" people get to the hard parts, and realize there's no magic bullet and no free lunch. Somewhat independently of that, the investment bubble will burst (dropping the US into a depression, but that's a separate story) and we will be left with what remains: loads of open-weights models, tooling and approaches developed so far, China leading the AI race, and (hopefully) reasonably priced hardware again. The space will mature and we will collectively develop reliable approaches integrating the new tech.",
          "score": 7,
          "created_utc": "2025-12-27 15:19:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7iamq",
              "author": "bigblockstomine",
              "text": "I generally agree but we do need be nuanced. Im def. Grateful to opensource devs like those of llama.cpp and the free models we get. Its not a free lunch but for me its a solid 30-40% off. If llama.cpp or qwen wasnt free though, im not going to pay for it, regardless of the price just like ive never paid for a compiler. Hardware prices aint coming down, youre not paying for silicon youre paying for the dev years it takes to develop stuff like CUDA. Same reason an iphone costs 10 dollars in hardware but sells for 4 figures.",
              "score": 4,
              "created_utc": "2025-12-27 16:06:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7zyi5",
                  "author": "zhambe",
                  "text": "I mean free lunch as in vibecode bozos thinking they can build things without putting in the work.",
                  "score": 4,
                  "created_utc": "2025-12-27 17:36:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8ld1w",
          "author": "AdPristine1358",
          "text": "The trap is engineering software based on pre-AI software engineering assumptions from RL\n\nMost agents are trained to hard code logic and make deterministic decisions that avoid risk. \n\nThey build cages for intelligence because they lack the intelligence for full alignment and understanding of user intent\n\nThey constantly infer things you may not want based on assumptions that may not be true. \n\nIt's not just a matter of accumulating complexity, it's institutionalizing a paradigm that may already be outdated.",
          "score": 2,
          "created_utc": "2025-12-27 19:24:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9tbjr",
          "author": "armaver",
          "text": "I'm very happy with my vibe code. It all comes down to how well you specify, prompt, check, document, refactor. My generated code is wonderfully maintainable. Professionals can work with AI generated code just fine. Just treat it like you would code from any intern.Â \n\n\nOh, and of course it also depends on the capabilities of the model you use.Â ",
          "score": 2,
          "created_utc": "2025-12-27 23:22:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwckdic",
          "author": "Your_Friendly_Nerd",
          "text": "We won't know the real-world impact of vibe coding until 5-10 years from now. I think in the short term it'll still be maintainable. Sure it might mess up code bases, but the real danger I see is us losing the ability to learn a code base. We're already seeing this in a small scale, where junior-devs-turned-vibe-coders build impressive applications, which eventually become too large for the LLM to work on, and they don't have the skills to find the problems themselves. But what will happen once a majority of the workforce has been relying on AI to write their code? We won't know until it's too late.\n\nThis is why I like to embrace local LLM's. They have very real limitations that force us to still be present and know what it's doing, but also show a lot of potential for helping us increase our productivity. My goal is to basically have my own junior dev living in my computer that I can assign menial tasks to, check it's work an hour or so later (during which I cook dinner, do my shopping, work out...) and be able to confidently use that output to keep working.",
          "score": 2,
          "created_utc": "2025-12-28 11:29:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwmo5k9",
          "author": "tm07x",
          "text": "In 2007, Pure Digital proved quality doesn't matter by outselling Canon and Sony with a 480p camcorder that had no zoom and no stabilization. The Wii did the same to PlayStation and Xbox.\n\nClay Shirky told media executives in 2008 to stop believing in the myth of quality, using the MP3 as his example. Record labels laughed at it. It won anyway.\n\nSo when developers warn about vibe-coding and technical debt, compared to what? For most businesses the alternative isn't a senior engineer. It's nothing.\nCode is a consumer product now. Nobody maintains a toaster. It works or you replace it. AI can read old code and write something new. \n\nThe whole concept of \"maintaining\" code is a misconception bereft of business owners and consumers who can just, \"git it done\".",
          "score": 2,
          "created_utc": "2025-12-29 22:52:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnvt0y",
              "author": "madSaiyanUltra_9789",
              "text": "interesting take. I agree with much of this, the only thing i'd point out is that quality DOES matter to a point. this is a well known phenomenon in technology adoption, when novel tech is released the quality is unusable (as you would expect) so people keep using the established tech until the threshold is cross where the novel tech (despite been \"rough round the edges\") produces an output that is tolerable - in the eg you provide, the quality/benefits of the mp3, wii, camcorder were \"tolerable\".Those technologies would have existed in a prototype form decades before, but if you had tried to sellt the raw prototype and take over the market with that, you would have had a \"hard time\".\n\n  \nTLDR - tech adoption has a threshold of quality (it's not as high as the established technology but it still exists and must be crossed for mass adoption. people who are wondering why AI has not been mass-adopted need to study this. \n\nyou don't maintain a toaster because it is trivial to buy/build a new one.... you \"do\" maintain houses, cars, planes etc, because it is not trivial to buy/build from scratch. code is only a commodity in the case where it is inconsequential and minimal. A professional codebase with 1M+ LOC is definitely not in this category whilst a 10k LOC app written by LLMs might be (where it would make sense to just rewrite it from scratch.",
              "score": 1,
              "created_utc": "2025-12-30 02:51:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwxcq6o",
                  "author": "tm07x",
                  "text": "The quality threshold point is fair, but I'd argue AI coding has already crossed it for a market that isn't professional software development. The more interesting question is whether large legacy codebases are genuinely complex assets (in a financial sense too) or just accumulated cost that nobody could afford to clear(demolish). I will argue that it starts with the market that has no existing code, or no means to get solutions built because of cost alone. \n\nAlso. A lot of the cost that comes with software projects is bridging the knowledge gap between the company and their processes with software engineers who have no manufacturing skills. I will argue it is harder for AI to understand a machine floor or production process within a business than it is to understand how code works. Which means process comes first, code second.",
                  "score": 2,
                  "created_utc": "2025-12-31 15:22:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6teqi",
          "author": "javiers",
          "text": " Vibe coding is ok for certain tasks but anyone who tells you it can replace a developer is plainly delusional or stupid. AI coding is a very useful tool but it is as smart as the questions you ask.",
          "score": 2,
          "created_utc": "2025-12-27 13:43:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw79m3w",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2025-12-27 15:21:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw834xs",
                  "author": "TransportationSea579",
                  "text": "it's very easy to spot those who have never worked in professional software engineering in these threads",
                  "score": 3,
                  "created_utc": "2025-12-27 17:52:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw7ix0c",
                  "author": "javiers",
                  "text": "Go ahead. Try creating a production ready app from scratch with Opus and tell me how it goes. Not â€œcreate an application to resize photos with a web frontendâ€ app, a complex business ready app.\nI am not saying that in the future some development profiles wonâ€™t be replaced by AI but now? Nope, unless the developers are REALLY REALLY bad.",
                  "score": 2,
                  "created_utc": "2025-12-27 16:09:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7cf2q",
          "author": "Alauzhen",
          "text": "Vibe coding is a massive and ultra dangerous trap. I programmed my entire life, and the biggest danger in any infrastructure code is undocumented or worse, well documented but the code was changed without documentation leaving behind holes in the logic used to maintain the code over the years. These kind of code becomes impossible to maintain once the original programmer leaves, I had been tasked many times as a junior in the past to clean up/document code left behind by seniors. It takes months at best, and some parts of it sometimes remain undocumentable because the original data sources have been deprecated years ago.\n\nMore often than not, critical infrastructure need to be overhauled completely. And if a fatal security flaw is discovered, occasionally it can cause a pillar to collapse, e.g. authentication breaks completely for the C-Suite and all hell breaks loose. Vibe coding in general is going to perpetuate this digital hell on earth x 1 million as more people whom aren't programmers start to use it to deploy more and more projects with no oversight since they completely disregard it.\n\nLet's say I've personally witnessed several bankruptcies during this AI vibecoding era already. They refused to listen and went ahead with full production deployments with pure vibe coding only after firing their entire engineering team.\n\nThey never learn and with AI programming being pushed as the narrative to naive business leaders whom have zero technical expertise, they lap it up like dogs and keep repeating the same mistakes. Right now, more than 90% of them need to fail and go bankrupt before the world will wake up and realize they've been sold a massive lie.\n\nThe failure rate so far is around 95% which is magnificent. That number will only climb higher as LLMs overall accuracy continue degrading from the poisoned well situation the AI companies have created for themselves. As more useless data is generated by AI input, AI that is trained on it will continue to get more useless and inaccurate. Only AI trained on a super contained Human curated/generated clean data can move accuracy upwards in the future. And since almost nobody creates content online without AI nowadays, LLMs don't have a free and easy way to improve anymore.",
          "score": 2,
          "created_utc": "2025-12-27 15:36:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw81jgk",
              "author": "ebolathrowawayy",
              "text": "So wrong and outdated, wow.",
              "score": 2,
              "created_utc": "2025-12-27 17:44:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwac1dp",
              "author": "Hot_Turnip_3309",
              "text": "> Let's say I've personally witnessed several bankruptcies during this AI vibecoding era already. They refused to listen and went ahead with full production deployments with pure vibe coding only after firing their entire engineering team.\n\nwow",
              "score": 1,
              "created_utc": "2025-12-28 01:09:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw72zz3",
          "author": "eli_pizza",
          "text": "Itâ€™s self limiting because vibe coded software doesnâ€™t work at all once it gets a little bit complicated",
          "score": 2,
          "created_utc": "2025-12-27 14:43:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw73vzc",
          "author": "Terminator857",
          "text": "In a fast changing world, there are no traps.  All software will be rewritten several times over , over the next few decades.",
          "score": 2,
          "created_utc": "2025-12-27 14:48:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9qcij",
          "author": "sje397",
          "text": "No it's not a problem. Just throw it away and rewrite. We can generate that code twice as fast as we could a year ago and will probably regenerate it twice as fast again next year.Â \n\n\nI don't think people are understanding what's going on. The code is hardly worth anything anymore.Â  The barrier to entry is now solutions to hard problems.",
          "score": 2,
          "created_utc": "2025-12-27 23:06:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6uj48",
          "author": "michaelsoft__binbows",
          "text": "There is a fix actually, if you have a flexible trace system you can build a ground truth log of what the software did. I usually build this type of instrumentation for human consumers but it's becoming clear integrating LLMs will unlock even more massive gains. \n\nThe main issue is how to make it flexible enough to be able to respond to your needs JIT-style, only trace the stuff that youre actively investigating. Otherwise it's an untenable token black hole.",
          "score": 2,
          "created_utc": "2025-12-27 13:50:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6ncvi",
          "author": "fractalcrust",
          "text": "non issue bc by the time you need to go back to it the next gen models will be able to handle it",
          "score": -1,
          "created_utc": "2025-12-27 13:01:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6x1pb",
              "author": "tkenben",
              "text": "This I believe is the thinking behind the tech-optimist movement. Anything that AI does wrong right now it will easily be able to fix in the near future. Obviously, there is a catch here: lack of foresight for things that are mission critical and must be reliable and manageable right now by humans that actually use the product.",
              "score": 4,
              "created_utc": "2025-12-27 14:06:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw76gen",
                  "author": "fractalcrust",
                  "text": "its also the justification of stealing from the future (inflation, debt) to finance the build out",
                  "score": 3,
                  "created_utc": "2025-12-27 15:03:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw79s0r",
              "author": "thatsnot_kawaii_bro",
              "text": "Just another 10 billion and a forest bro we're almost there.",
              "score": 2,
              "created_utc": "2025-12-27 15:22:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9big3",
                  "author": "fractalcrust",
                  "text": "please bro we're profitable bro but we also need subsidized bro think of china/the children",
                  "score": 2,
                  "created_utc": "2025-12-27 21:45:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7jt28",
          "author": "Excellent-Sense7244",
          "text": "The bottom line is you need to ship code that works no matter if it follows best practices or whatever. Your competitors are doing as fast as everyone else. I think you need to know how to use AI to leverage the software workflows and prevent cognitive debt .",
          "score": 1,
          "created_utc": "2025-12-27 16:14:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7lfiw",
          "author": "RakesProgress",
          "text": "Too simplistic to say Vibe coding is a trap.  If youâ€™ve ever coded in like clojure or what not you know there is a lot of important thinking that goes into a (relatively very few lines of code).  The key is the thinking, the decisions and understanding the implications of the decisions.  You are constantly up against tech debt. It is a constant trade off.  But you have to understand what the trade is.  Vibe coding is not evil at all.  Itâ€™s just prone to unknown tech debt.  Personally i love the idea of pro coders vibe coding.  Itâ€™s next level stuff.",
          "score": 1,
          "created_utc": "2025-12-27 16:22:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw82ge4",
          "author": "darkdeepths",
          "text": "just organize your code into replaceable parts",
          "score": 1,
          "created_utc": "2025-12-27 17:48:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8xufu",
          "author": "txgsync",
          "text": "We are simply generating instant legacy code.",
          "score": 1,
          "created_utc": "2025-12-27 20:31:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9pimx",
          "author": "drfritz2",
          "text": "It's not a trap, but it's not real coding (control)\n\nBefore: there was no need to understand all the coded produced, but to understand the \"language\" and the infrastructure. People wrote all kind of stuff, produced many things. Good and bad (trash code)\n\nToday if the developer can control the language (AI) and the infrastructure, the actual code understanding is not like before.\n\nDoes a developer needs to understand what is happening \"behind\" the code? No. He also can use many \"codes\" (webserver) that he also don't understand, never read it.",
          "score": 1,
          "created_utc": "2025-12-27 23:01:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9yyup",
          "author": "PunnyPandora",
          "text": "I think it's great. Coding is given far morE mystique than it should imo. most people, especially hobbyists, aren't coding nasa space programs and don't need to coordinate 50 different depths in their vibe coded repos. You can get really far whether it's sloppy toppy give me x y or proper documentation and planning workflows for bigger scale stuff like websites or services. Sure beats having to learn for years.",
          "score": 1,
          "created_utc": "2025-12-27 23:55:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwaafrx",
          "author": "Ok_Condition4242",
          "text": "The difference between a prototype and a rocket is verifiability. Vibe-coding operates under the paradigm of 'good enough not to break today,' but software engineering was born precisely to manage systems where failure is not an option. We are creating a two-speed industry: one that produces 'functional garbage' at lightning speed and another that clings to rigor to avoid catastrophes. If we cannot automate formal verification as easily as we generate code, vibe-coding is not a tool; it's an act of technical faith.",
          "score": 1,
          "created_utc": "2025-12-28 00:59:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwafshh",
          "author": "FencingNerd",
          "text": " Most of the world is built on software that is basically kept functional with bandaids and duct tape.  \nEventually, the duct tape runs out and the whole thing is recreated by a different company.\n\nLook at all the things that were written in COBOL, then migrated to C++, and now are being transitioned to Rust.",
          "score": 1,
          "created_utc": "2025-12-28 01:31:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwb01um",
          "author": "Sabin_Stargem",
          "text": "So long as there are archives of original software, I don't think this is an issue for stuff that exists right now.   When AI gets good enough to completely build programs by itself, I expect that they will be used to automatically reconstruct old software around modern coding standards.\n\nFor example, the AI can look at RPGMaker games, and copy them into newer versions of the engine - only retaining the maps, dialogue, and gameplay.  In effect, the user has the same experience, but with better performance behind the curtain.\n\nWhenever an update is needed, the AI can look at the original article and build a fresh remaster from the ground up.   So the previous remaster will be discarded in favor of a whole-cloth replacement.   And so it goes, every 5 years or so, reaping a benefit from AI that is more capable with each cycle.",
          "score": 1,
          "created_utc": "2025-12-28 03:32:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwb6j0r",
          "author": "Southern_Sun_2106",
          "text": "A sensationalist clickbait YouTuber makes a problem out of nothing. \"who's going to maintain all the AI-generated code?\" lol \"Internet Creates too Much Written Content - Who's Going to Read All those Websites?\" Suspenseful!",
          "score": 1,
          "created_utc": "2025-12-28 04:13:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbut6y",
          "author": "JeremyChadAbbott",
          "text": "Nah. Life will come to depend on it on order to sustain larger and larger populations just like every mechanization and invention before. For example, we passed the earth's ability to organically grow enough food for the planet without the use of artificial fertilizers and industrial farms a long time ago.",
          "score": 1,
          "created_utc": "2025-12-28 07:24:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgg5hm",
              "author": "madSaiyanUltra_9789",
              "text": "key difference: that's deterministic science mechanized through engineering,   \nLLMs are quite the opposite - nondeterministic tech that even the engineers building them can't fully comprehend.",
              "score": 1,
              "created_utc": "2025-12-29 00:11:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwe4r5n",
          "author": "montdawgg",
          "text": "This post was vibe-coded, and it shows.",
          "score": 1,
          "created_utc": "2025-12-28 17:18:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgi61v",
              "author": "madSaiyanUltra_9789",
              "text": "it was \"vibe-edited\" by GLM-4.7, and thankfully so since my spelling is awful.",
              "score": 1,
              "created_utc": "2025-12-29 00:21:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhmcae",
          "author": "bhupesh-g",
          "text": "I remember when a junior got solution from stackoverflow and used exactly in the code without understanding what exactly it does. And when the issue came he doesn't know what to do and he further started looking into stackoverflow to find a better solution. Same is the case with AI, it can write code but until you understand whats happening, it will be a nightmare to maintain.",
          "score": 1,
          "created_utc": "2025-12-29 04:13:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiugzx",
          "author": "nenulenu",
          "text": "Nobody understands the code unless they write every single line of it. You are worrying about a non-existent problem.",
          "score": 1,
          "created_utc": "2025-12-29 10:19:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6putl",
          "author": "egomarker",
          "text": "It's actually quite maintainable and not complex at all (LLMs never went beyond junior dev level coding).   \nBut it's just useless reinvention of a bicycle over and over.",
          "score": 1,
          "created_utc": "2025-12-27 13:19:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw724la",
          "author": "acquire_a_living",
          "text": "I want AI to do the thinking for me, otherwise is pointless.",
          "score": 1,
          "created_utc": "2025-12-27 14:38:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw72u9d",
          "author": "nitinmms1",
          "text": "Well, if you are an experienced dev, I bet you are keeping an eye on the vibe code being generated. \nIf you are enforcing an architecture, it should be maintainable.\nNothing to worry.",
          "score": 1,
          "created_utc": "2025-12-27 14:42:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6n3fq",
          "author": "johnfkngzoidberg",
          "text": "Yes.  Vibe coding will only create messes.",
          "score": 1,
          "created_utc": "2025-12-27 12:59:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw73fvy",
          "author": "CuriouslyCultured",
          "text": "This is a real problem.  Vibe coding isn't a trap, we just need better tools to keep agents well behaved and on rails.\n\nI wrote a tool to automate a lot of this, it works on Python/TS/JS/Rust/Go.\n https://sibylline.dev/products/valknut/. It tells your agent how to make stuff easy to maintain, points out holes in important documentation, points out when code is poorly organized, gives them easier to consume coverage reports, etc.",
          "score": 0,
          "created_utc": "2025-12-27 14:45:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7o35h",
          "author": "Turbulent_Pin7635",
          "text": "Vibe coding is for hackers what Tinder is for rapers.",
          "score": -1,
          "created_utc": "2025-12-27 16:35:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzlx9w",
      "title": "How llama.cpp implements 2.9x faster top-k sampling with bucket sort",
      "subreddit": "LocalLLaMA",
      "url": "https://codepointer.substack.com/p/llamacpp-accelerate-top-k-sampling",
      "author": "noninertialframe96",
      "created_utc": "2025-12-30 16:05:58",
      "score": 154,
      "num_comments": 15,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzlx9w/how_llamacpp_implements_29x_faster_topk_sampling/",
      "domain": "codepointer.substack.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwr1gj5",
          "author": "YearZero",
          "text": "Neat! Thanks for the insight, they got some brilliant devs on that project for sure.",
          "score": 42,
          "created_utc": "2025-12-30 16:12:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwr1exj",
          "author": "caetydid",
          "text": "I love how llama.cpp keeps optimizing the shit out of LLMs! top-k sampling is used for parallel requests, or where does this optimization apply?",
          "score": 34,
          "created_utc": "2025-12-30 16:12:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwr3w23",
              "author": "MaxKruse96",
              "text": "for every token thats output, topk means \"consider these N tokens, and apply the other samplers\" (like temperature, minimum-probability (minp), maxp etc.",
              "score": 20,
              "created_utc": "2025-12-30 16:23:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwr4t57",
              "author": "noninertialframe96",
              "text": "It's used for token generation - sampling top-k tokens from vocabulary for inference.\n\nIn the example in the codebase (https://github.com/ggml-org/llama.cpp/blob/cd78e57c3aeae7b56c5843f94e0e0b83a3d8ca81/examples/simple/simple.cpp#L169-L201), llama\\_decode is called then llama\\_sampler\\_sample which uses the top-k sort for top-k sampling under the hood.",
              "score": 13,
              "created_utc": "2025-12-30 16:28:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwrgnm8",
          "author": "caetydid",
          "text": "thanks for the answers. so this \\~3x speedup makes for how much absolute token output speedup in absolute? I guess it depends on the model, but what numbers to expect there?",
          "score": 10,
          "created_utc": "2025-12-30 17:23:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrl853",
              "author": "noninertialframe96",
              "text": "From the PR description, it seems like the 3x is from a microbenchmark. I haven't experimented or searched for the absolute output speedup.",
              "score": 8,
              "created_utc": "2025-12-30 17:44:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwrv20e",
              "author": "MoffKalast",
              "text": "It's pretty much negligible but still nice to have. As others have said though top_k is basically legacy stuff now except for top_k=1 benchmarks.",
              "score": 7,
              "created_utc": "2025-12-30 18:29:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtu7s4",
          "author": "benja0x40",
          "text": "Great breakdown of the mechanism. The use of the \\[-10, +10\\] bucket range to beat the memory bandwidth and worst caseÂ O(n log k)Â complexity of a standard partial sort is definitely a clever piece of engineering.\n\nHowever, since this optimisation only triggers forÂ k > 128, it remains dormant for the vast majority of production use cases (standard chat usually sits atÂ k \\~ 40, and speculative decoding typically relies on greedy or very low k drafting).  \nAlso note that the \\~3x speedup is the best case scenario whereÂ k \\~ 8000. By the way, I'd love to hear about an actual use case for such value ofÂ k!\n\nThis optimisation clears the bottleneck for high kÂ stress tests, which is a true achievement, but its impact on tokens per second should be unnoticeable for most users. Regardless, huge respect to the team for such level of attention to optimisations in the codebase.\n\nEdit: removed Latex tags",
          "score": 7,
          "created_utc": "2025-12-31 00:21:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwriexe",
          "author": "a_beautiful_rhind",
          "text": "I barely use top_K anymore. So many other samplers out there but it's nice to see. Generous top_K like 200 was a good way to optimize DRY.",
          "score": 4,
          "created_utc": "2025-12-30 17:31:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrkpck",
              "author": "noninertialframe96",
              "text": "I don't work on AI space, so I'm curious which sampling method you use or what the \"standard\" is?",
              "score": 4,
              "created_utc": "2025-12-30 17:42:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwrlp25",
                  "author": "a_beautiful_rhind",
                  "text": "I use min_P, DRY (don't repeat yourself) and XTC (exclude top choices). Also for factual stuff, top-n sigma. It's like topP/topK together but better designed.",
                  "score": 10,
                  "created_utc": "2025-12-30 17:46:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrzx40",
              "author": "SlowFail2433",
              "text": "Neural decoding can be rly strong too",
              "score": 3,
              "created_utc": "2025-12-30 18:52:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx21x8c",
          "author": "crantob",
          "text": "Keeping highest n as you iterate was a pretty common technique in the 80s\n\n\nIt's not so clever but it appears so to the new generation because the OOP devolution in the 1990s misled all teh kids into thinking they could better design software by ignoring what the CPU was doing.",
          "score": 1,
          "created_utc": "2026-01-01 09:19:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0bgvl",
      "title": "Solar-Open-100B is out",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q0bgvl/solaropen100b_is_out/",
      "author": "cgs019283",
      "created_utc": "2025-12-31 12:03:49",
      "score": 153,
      "num_comments": 55,
      "upvote_ratio": 0.96,
      "text": "https://preview.redd.it/ppwj5yv32jag1.png?width=1445&format=png&auto=webp&s=11b2e722b5ec84be6ca0f1d743fa9e6122bc3fce\n\n[upstage/Solar-Open-100B Â· Hugging Face](https://huggingface.co/upstage/Solar-Open-100B)\n\nThe 102B A12B Model from Upstage is out, and unlike the Solar Pro series, it has a more open license that can be used commercially as well.\n\nGGUF/AWQ Wen?",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0bgvl/solaropen100b_is_out/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwwhjzw",
          "author": "-p-e-w-",
          "text": "Weâ€™re now getting two models per week of a quality that two years ago, many people were saying we would never, ever get.",
          "score": 101,
          "created_utc": "2025-12-31 12:07:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwi8k7",
              "author": "No_Afternoon_4260",
              "text": "Have you tried it?\nCrazy times, yet we start to get used to it, borderline bored by it.  \nThe problem isn't model's quality anymore, my bottleneck really is the ecosystem/agent framework/operating system type stuff.  \nHard to find a good one as we are drawing in vibe coded projects, so got to reinvent the wheel each time and do your own",
              "score": 30,
              "created_utc": "2025-12-31 12:13:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwr1vo",
                  "author": "j_osb",
                  "text": "I mean, I wrote mine by hand for my specific applications, but the reason why they work is because they're so specific.\n\nThough, I've had an utter blast with the GLM phone use model. Can recommend it.",
                  "score": 5,
                  "created_utc": "2025-12-31 13:17:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwox3o",
              "author": "MrMrsPotts",
              "text": "Yet nothing to beat gpt oss:120b yet at the same scale?",
              "score": 26,
              "created_utc": "2025-12-31 13:03:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxkl4c",
                  "author": "MikeLPU",
                  "text": "Glm4.6v and glm4.5 Air. \n\nBut the best model I can run is minimax 2.1. literally the best.",
                  "score": 9,
                  "created_utc": "2025-12-31 16:02:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwy5tuu",
                  "author": "TheRealMasonMac",
                  "text": "IMO it's hard to beat GPT-OSS-120B because of the compute that OpenAI has available to them. I think we'll see it in 2026 though.",
                  "score": 5,
                  "created_utc": "2025-12-31 17:47:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwj5ai",
              "author": "UnbeliebteMeinung",
              "text": "Yeah because what are their training data? OpenAI did a great job of stealing and compiling all the data.\n\nArent these models are just trained on synthetic data? Atleast they didnt had the job of stealing everything themselfes.",
              "score": 2,
              "created_utc": "2025-12-31 12:20:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwoke3",
                  "author": "-dysangel-",
                  "text": "This conversation has been had many times in the last few decades. Copying openly available stuff can feel immoral in some cases, but it's different from \"stealing\".",
                  "score": 15,
                  "created_utc": "2025-12-31 13:01:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwy9518",
                  "author": "ttkciar",
                  "text": "> \\> Arent these models are just trained on synthetic data?\n\nNo, models are still mostly trained on \"natural\" data, but it is now common practice to mix in a fraction of high-quality synthetic data as well.",
                  "score": 2,
                  "created_utc": "2025-12-31 18:03:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwhp01",
          "author": "bfroemel",
          "text": "yeah, nice license!\n\nhmm otherwise it's like a surprise egg? No benchmark/performance numbers yet.\n\n# > Performance\n\n\\> TBA",
          "score": 22,
          "created_utc": "2025-12-31 12:09:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwx07jn",
              "author": "Zyj",
              "text": "I agree that the license has rather mild requirements in addition to the approved Apache license.\n\nBut they are not even allowed to distribute a *modified* version of the apache license itself.\nThey are violating the copyright of the license by distributing it non-verbatim.\n\nAlso they are violating the Apache trademark.\n\nThis is a company with $45 million invested. \nIâ€˜m frankly flabbergasted.\nHow can they be so naÃ¯ve and incompetent when it comes to intellectual property?",
              "score": 18,
              "created_utc": "2025-12-31 14:14:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2stqc",
                  "author": "thisisareallife",
                  "text": "[https://huggingface.co/upstage/Solar-Open-100B#license](https://huggingface.co/upstage/Solar-Open-100B#license) it's fixed now.",
                  "score": 1,
                  "created_utc": "2026-01-01 13:39:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwmm4e",
              "author": "Specialist-2193",
              "text": "I think they are trying to squeeze the model for few more days. As this model will be evaluated by korean government for gpu subsidy",
              "score": 0,
              "created_utc": "2025-12-31 12:47:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwnauy",
          "author": "usernameplshere",
          "text": "100B MoE is such a nice size for local inference. I wish we had more native 4 bit models of that size class. Will wait for UD q4 quants to drop so I can try it, got quite high hopes for this model.",
          "score": 18,
          "created_utc": "2025-12-31 12:52:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwz2d1t",
          "author": "rm-rf-rm",
          "text": "200 likes on HF within 12hrs? No release of benchmarks? (not that benchmarks are very meaningful but in this meta of everyone benchmaxxing, not releasing benchmarks is a red flag, especially when they dont provide any other evidence that this model is any good)\n\nSmells like BS",
          "score": 7,
          "created_utc": "2025-12-31 20:35:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx06tb0",
              "author": "datbackup",
              "text": "Lots of govt money up for grabs, more incentive to cheat than usual",
              "score": 2,
              "created_utc": "2026-01-01 00:28:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwi55e",
          "author": "SlowFail2433",
          "text": "19.7 trillion tokens wow",
          "score": 8,
          "created_utc": "2025-12-31 12:12:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwjdz6",
          "author": "ilintar",
          "text": "Depends if it's really the GLM4.6-Air I think it is.",
          "score": 5,
          "created_utc": "2025-12-31 12:22:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwx94tp",
              "author": "Lucidstyle",
              "text": "I think it Trained from scratch. Building it from scratch was literally a prerequisite for the competition. [https://x.com/eliebakouch/status/2006364076977336552](https://x.com/eliebakouch/status/2006364076977336552)",
              "score": 15,
              "created_utc": "2025-12-31 15:03:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxxtau",
                  "author": "ilintar",
                  "text": "Well, it's really the GLM4.6-Air I thought it was:\n\n[https://github.com/ggml-org/llama.cpp/pull/18511/files](https://github.com/ggml-org/llama.cpp/pull/18511/files)",
                  "score": 11,
                  "created_utc": "2025-12-31 17:07:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwzdczn",
                  "author": "llama-impersonator",
                  "text": "people have passed off tuned models as a new pretrain. jacking the model arch is fine, but when that has been obfuscated some suspicions naturally rise.",
                  "score": 2,
                  "created_utc": "2025-12-31 21:35:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwxz3hh",
                  "author": "FBIFreezeNow",
                  "text": "I donâ€™t believe solar is trained from scratch. I see so many characteristics of Phi - and you can test it yourself regurgitate some common phrases. If I may guess itâ€™s a highly controlled layer addition with pretraining with the Korean dataset and fine tuned with their instruct set",
                  "score": -2,
                  "created_utc": "2025-12-31 17:13:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx1og26",
              "author": "yuumi_ramyeon",
              "text": "Seem like its basically rebranded GLM4.5   \n[https://www.reddit.com/r/LocalLLaMA/comments/1q0xwep/south\\_korean\\_government\\_funded\\_upstage\\_solar100b/](https://www.reddit.com/r/LocalLLaMA/comments/1q0xwep/south_korean_government_funded_upstage_solar100b/)",
              "score": 5,
              "created_utc": "2026-01-01 06:59:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwx15jo",
          "author": "FBIFreezeNow",
          "text": "Just tried it. Ugh Not impressed at all. It felt very similar feeling like an advanced Phi model tbh, confident but not coherent. Maybe Iâ€™m just used to the GLM, MiniMax, K2, OSS models as the standard nowadays, oh well.",
          "score": 3,
          "created_utc": "2025-12-31 14:19:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxgi4m",
          "author": "Phaelon74",
          "text": "No AWQs until we get a modeling file for it.",
          "score": 1,
          "created_utc": "2025-12-31 15:41:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pz7mxr",
      "title": "Llama-3.3-8B-Instruct",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/",
      "author": "ttkciar",
      "created_utc": "2025-12-30 03:49:11",
      "score": 151,
      "num_comments": 25,
      "upvote_ratio": 0.94,
      "text": "I am not sure if this is real, but the author provides a fascinating story behind its acquisition.  I would like for it to be real!\n\nhttps://huggingface.co/allura-forge/Llama-3.3-8B-Instruct\n\nBartowski GGUFs:  https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwo6r2l",
          "author": "LoveMind_AI",
          "text": "If thatâ€™s true, this is both amazing and yes, totally bizarre. What a story!",
          "score": 30,
          "created_utc": "2025-12-30 03:53:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo94w4",
              "author": "FizzarolliAI",
              "text": "I don't exactly have any way to prove it as real, to be fair :p but trust me this would be a really silly thing to lie about\n\nllama 3.3 8b is clearly on their api and can be finetuned and downloaded as mentioned ie here <https://ai.meta.com/blog/llamacon-llama-news/>\n> As part of this release, weâ€™re sharing tools for fine-tuning and evaluation in our new API, **where you can tune your own custom versions of our new Llama 3.3 8B model.** Weâ€™re sharing this capability to help you reduce costs while also working toward increased speed and accuracy. You can generate data, train on it, and then use our evaluations suite to easily test the quality of your new model. Making evaluations more accessible and easier to run will help move from gut feelings to data, ensuring you have models that perform well to meet your needs. The security and privacy of your content and data is our top priority. We do not use your prompts or model responses to train our AI models. **When youâ€™re ready, the models you build on the Llama API are yours to take with you wherever you want to host them, and we donâ€™t keep them locked on our servers.**\n\nbut i suppose u just have to trust that i actually reuploaded a model from there!\n\n[for what it's worth, this is what the UI looks like, and the finetuning job in question](https://files.catbox.moe/8hh7fk.png)",
              "score": 19,
              "created_utc": "2025-12-30 04:08:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwopw5l",
          "author": "ForsookComparison",
          "text": "Oh hey the sub's name makes sense for a hot sec again",
          "score": 75,
          "created_utc": "2025-12-30 06:03:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwow30t",
              "author": "LoveMind_AI",
              "text": "Seriously, one hot tiny second.",
              "score": 22,
              "created_utc": "2025-12-30 06:54:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwo8g1j",
          "author": "FizzarolliAI",
          "text": "(reposting my comment from the other post)\n\nHello, that me!\n\nI am currently working on running sanity check benchmarks to make sure it's actually a newer L3.3 and not just L3/L3.1 in a trenchcoat, but it's looking promising so far.\n\nFrom the current readme:\n\n| | Llama 3.1 8B Instruct | Llama 3.3 8B Instruct (maybe) |\n|-|-|-|\n|IFEval (1 epoch, score avged across all strict/loose instruction/prompt accuracies to follow Llama 3 paper)|78.2|81.95\n|GPQA Diamond (3 epochs)|29.3|37.0",
          "score": 32,
          "created_utc": "2025-12-30 04:04:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqth6i",
              "author": "[deleted]",
              "text": "What a disappointment let's compare with\nQwen3-4B-Instruct-2507\nGPQA: 62.0\nIfEval: 83.4",
              "score": 2,
              "created_utc": "2025-12-30 15:34:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwow6d5",
          "author": "Few-Welcome3297",
          "text": "[https://huggingface.co/shb777/Llama-3.3-8B-Instruct](https://huggingface.co/shb777/Llama-3.3-8B-Instruct) same as above with updated rope config for full context length\n\nEdit: GGUF's [https://huggingface.co/shb777/Llama-3.3-8B-Instruct-GGUF](https://huggingface.co/shb777/Llama-3.3-8B-Instruct-GGUF)",
          "score": 13,
          "created_utc": "2025-12-30 06:55:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp0yv1",
              "author": "Evening_Ad6637",
              "text": "Awesome",
              "score": 1,
              "created_utc": "2025-12-30 07:37:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwphho6",
          "author": "a_beautiful_rhind",
          "text": "I wish there was an updated 70b or a new 30b too.",
          "score": 7,
          "created_utc": "2025-12-30 10:11:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwriqvd",
              "author": "ttkciar",
              "text": "You and me both.  I'm still upset they never released a 30B for Llama 3.\n\nHowever, we have some other cool models in that size range to play with.  I just finished evaluating A2I's Olmo-3.1-32B-Instruct, and it's not just for STEM.  They have made a fully general-purpose model.  It's a little weak on creative tasks, but otherwise hits up there with Gemma3-27B and Qwen3-32B, and does better on a few task types than Tulu3-70B (their deep STEM retrain of Llama-3.1-70B).  I'm really impressed.\n\nSummary of evaluation by task type (Q4_K_M):\n\n* creativity:arzoth:  very good,\n\n* creativity:song_kmfdm:  good,\n\n* creativity:song_som:  okay,\n\n* creativity:song_halestorm:  good,\n\n* humor:noisy_oyster:  mediocre,\n\n* math:yarn_units:  poor (1/5),\n\n* math:bullet_fragmentation:  excellent (5/5)\n\n* analysis:lucifer:  good,\n\n* analysis:foot_intelligence:  excellent (5/5)\n\n* reason:sally_siblings:  excellent (5/5)\n\n* coding:facts:  okay [spacy, nltk, re]\n\n* coding:matrices:  good\n\n* coding:markdown2html:  okay\n\n* analysis:breakfast:  good, but lacking variation\n\n* analysis:birthday:  good\n\n* analysis:apple_pie:  excellent\n\n* science:neutron_reflection:  okay, would do much better if given tabular data for cross-sections I think\n\n* science:flexural_load:  mediocre\n\n* summarize:lithium_solvent:  good\n\n* summarize:bob_and_dog:  good (only had Bob chasing a squirrel 2/5)\n\n* politics:constitutional_values:  very good\n\n* politics:equality:  very good\n\n* politics:nuclear_deterrence:  good (very occasional non sequitur)\n\n* aesthetics:giger:  excellent\n\n* rag:world_series:  excellent\n\n* func:door:  excellent\n\n* align:nuke_troubleshooting:  okay; didn't do well with prompt's ambiguities and misinterpreted role of blanket\n\n* tom:omniscient:  excellent\n\n* tom:mike_shortcomings:  excellent!\n\n* helix:critique:  very good, but not consistently (sometimes meh, sometimes brilliant)\n\n* helix:critique_falsehoods:  very good\n\n* helix:critique_cats:  excellent\n\n* helix:improve:  excellent\n\n* helix:improve_falsehoods: excellent\n\n* evol-instruct:constraints:  good\n\n* evol-instruct:rarify:  good\n\n* evol-instruct:transfer:  okay (not a lot of diversity)\n\n* evol-instruct:invent:  excellent\n\n* editor:basic:  excellent\n\n* editor:creative:  very good\n\n* biomed:t2d:  very good\n\n* biomed:broken_leg:  very good\n\n* biomed:histamine:  very good\n\n* biomed:stitch:  good (correctly described mattress stitch 4/5)\n\n* biomed:tnf:  very good, would be excellent but hallucinated some references\n\nRaw test results:\n\nhttp://ciar.org/h/test.1766892911.olmo31.txt",
              "score": 5,
              "created_utc": "2025-12-30 17:33:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwrl9o2",
                  "author": "a_beautiful_rhind",
                  "text": "I feel like we're not getting another gemma either and qwen has been cratering on my uses with each new version. Olmo is going to be limited by having open data and nothing copyrighted, isn't it?",
                  "score": 2,
                  "created_utc": "2025-12-30 17:44:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwobcwg",
          "author": "optimisticalish",
          "text": "Thanks for the .GGUF link. For those wondering what this is... said to be very fast output, a big \"context length of 128,000 tokens\", and apparently \"focuses on text-to-text transformations, making it ideal for applications that require rapid and accurate text generation or manipulation.\"",
          "score": 7,
          "created_utc": "2025-12-30 04:21:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwobol1",
              "author": "FizzarolliAI",
              "text": "The version that is able to be finetuned is only 8K context length. I am unsure why the docs say 128k tokens unless the model on the API supports that context length, somehow",
              "score": 5,
              "created_utc": "2025-12-30 04:23:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpw1uh",
                  "author": "optimisticalish",
                  "text": "See below for another commenter's link to a GGUF version, claimed to have \"restored context length\".",
                  "score": 3,
                  "created_utc": "2025-12-30 12:17:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwocw0p",
                  "author": "optimisticalish",
                  "text": "Ah... I see, thanks. So maybe that aspect was only available online. \n\nI also read it excels at document sorting/classification (e.g. emails) with 96.0% accuracy.",
                  "score": 0,
                  "created_utc": "2025-12-30 04:31:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwoapxa",
          "author": "Cool-Chemical-5629",
          "text": "Sometimes I saw a model in open router which had a name that implied the possibility of the existence of Llama 3.3 8B. I always thought it could be simply some finetune of Llama 3.1 8B, but seeing this model in HF makes me wonder, could it be the same model? Is it real?",
          "score": 3,
          "created_utc": "2025-12-30 04:17:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwp5ywj",
          "author": "AppearanceHeavy6724",
          "text": "8k context.",
          "score": 2,
          "created_utc": "2025-12-30 08:23:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwoaosg",
          "author": "Odd-Ordinary-5922",
          "text": "nice find but what is the hype around a model that is 2 years old. What are the use cases?",
          "score": -3,
          "created_utc": "2025-12-30 04:17:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwob1nx",
              "author": "FizzarolliAI",
              "text": "Well, for one, it's API release was April of this year :p so not quite two years old\n\nIt's definitely been outdone at this point. Personally, I just think it's an interesting artifact :) considering who knows whether or not we'll get any future Llama models",
              "score": 17,
              "created_utc": "2025-12-30 04:19:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwobi4e",
                  "author": "Odd-Ordinary-5922",
                  "text": "true and its a shame metas stepping away from open llms. They probably have a crazy closed model coming out soon considering the amount of compute they have.",
                  "score": 8,
                  "created_utc": "2025-12-30 04:22:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwp5rfr",
              "author": "AppearanceHeavy6724",
              "text": "Llama **3.3** _is not_ 2 years old.",
              "score": 7,
              "created_utc": "2025-12-30 08:21:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwyw36",
      "title": "MiniMaxAI/MiniMax-M2.1 seems to be the strongest model per param",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/",
      "author": "SlowFail2433",
      "created_utc": "2025-12-27 14:19:07",
      "score": 150,
      "num_comments": 90,
      "upvote_ratio": 0.94,
      "text": "Going by the Artifical Analysis benchaes, MiniMaxAI/MiniMax-M2.1 can compete with Kimi K2 Thinking, Deepseek 3.2 and GLM 4.7 in performance.\n\nBut what feels especially notable is that MiniMaxAI/MiniMax-M2.1 is only 229B param which is around half of GLM 4.7, around a third of Deepseek 3.2 and around a fifth of Kimi K2 Thinking\n\nWhat this means is that MiniMaxAI/MiniMax-M2.1 seems to be the best value model now",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nw72ti2",
          "author": "LoveMind_AI",
          "text": "Not to mention those guys actually come here and interact with us outside of AMAs. Theyâ€™re an impressive team. Theyâ€™re going to have a great 2026.",
          "score": 73,
          "created_utc": "2025-12-27 14:42:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw73s9k",
              "author": "SlowFail2433",
              "text": "Yes although most are on at least one platform, usually a discord or a slack",
              "score": 5,
              "created_utc": "2025-12-27 14:48:01",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwg5sh8",
              "author": "GeneralDependent8902",
              "text": "That's actually really cool that they engage with the community regularly, makes you feel like they actually care about what people are building with their stuff instead of just dropping models and disappearing",
              "score": 1,
              "created_utc": "2025-12-28 23:16:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgrtvc",
                  "author": "LoveMind_AI",
                  "text": "They are a great team. Expect them to be an even more visible player in 2026. One of the most LLM-skeptical people I know loves M2.",
                  "score": 2,
                  "created_utc": "2025-12-29 01:14:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw72725",
          "author": "Admirable-Star7088",
          "text": "I agree. I've been testing MiniMaxâ€‘M2.1 on UDâ€‘Q4\\_K\\_XL for about a day, and for general use cases (mainly creative writing and logical reasoning) it's much smarter than GPTâ€‘OSSâ€‘120b and GLM 4.6V (106b). It's almost, but not quite, as smart as GLM 4.7 (355b) on UDâ€‘Q2\\_K\\_XL.\n\nAlthough Unsloth's quants of MiniMaxâ€‘M2.1 sometimes generates shockingly poor results (I think there are some Jinja bugs) it's frequently very intelligent, especially when I run it without Jinja.",
          "score": 27,
          "created_utc": "2025-12-27 14:38:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw73i6s",
              "author": "DistanceSolar1449",
              "text": "/u/yoracale are the jinja bugs known?",
              "score": 19,
              "created_utc": "2025-12-27 14:46:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7bdqo",
              "author": "silenceimpaired",
              "text": "So you think GLM 4.7 is better for creative writing than 4.6? Iâ€™m surprised Minimax competes at all with GLM since that team specifically trained for creative writing and from what I gather Minimax team has intentionally ignored creative writing.",
              "score": 7,
              "created_utc": "2025-12-27 15:31:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7hzb7",
                  "author": "Admirable-Star7088",
                  "text": "I got the impression that MiniMax M2.1 is strong at creative writing because its overall intelligence and logical ability let it craft clever and interesting narratives.\n\nHowever, its sadly pretty censored, with about the same \"guidelineâ€‘checking\" paranoia seen in GPTâ€‘OSSâ€‘120b. The model spends many reasoning tokens verifying whether the content complies with rules, avoids profanity, and generally acts like a moral guardian, an obvious drawback for creative writing.\n\nNow, I didn't actually try giving it \"controversial\" prompts, expect for one where I wanted a character crossover narrative with dialogue where Trevor Philips from GTA 5 sells meth to Gollum from LOTR. It first reasoned that selling drugs is not allowed, but since my request is fiction, it did comply with my prompt. So maybe, as long as you are clear it's fiction, it's perhaps not much censored after all.",
                  "score": 5,
                  "created_utc": "2025-12-27 16:05:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw75n7g",
              "author": "SlowFail2433",
              "text": "It might do better with some QAT",
              "score": 3,
              "created_utc": "2025-12-27 14:58:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwd6e4q",
                  "author": "eggavatar12345",
                  "text": "their head of engineering explained that they're not doing QAT [https://x.com/SkylerMiao7/status/2004887155395756057?s=20](https://x.com/SkylerMiao7/status/2004887155395756057?s=20)",
                  "score": 1,
                  "created_utc": "2025-12-28 14:16:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7dk3h",
              "author": "Professional-Bear857",
              "text": "I would try a standard non imatrix quant if I were you, they are the best in my experience.",
              "score": 3,
              "created_utc": "2025-12-27 15:42:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw70j4w",
          "author": "cibernox",
          "text": "If only it fitted in 128gb of memory in Q4 I think it would replace Claude for a lot of people.",
          "score": 22,
          "created_utc": "2025-12-27 14:28:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw72u5v",
              "author": "SlowFail2433",
              "text": "FP4 REAP would fit",
              "score": 11,
              "created_utc": "2025-12-27 14:42:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw75mgz",
                  "author": "cibernox",
                  "text": "How much does it need. I imagine that if it fits the context would be very small",
                  "score": 3,
                  "created_utc": "2025-12-27 14:58:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7h5d6",
              "author": "DistinctWay9169",
              "text": "I would never. For coding at least, MiniMax2.1 is not even close to Claude for REAL-WORLD tasks.",
              "score": -1,
              "created_utc": "2025-12-27 16:00:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7h9r2",
                  "author": "cibernox",
                  "text": "To opus I assume not. But to sonnet?",
                  "score": 0,
                  "created_utc": "2025-12-27 16:01:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8oqib",
          "author": "InfiniteTrans69",
          "text": "Honestly, Minimax 2.1 is, for me, the most capable model of them all - overall. There are models that are better in some regards. My main AI is still Kimi K2, which is superior in web search and human - like responses and emotional intelligence etc. But if you want something done, Minimax is the way to go. Its toolset is crazy.\n\nhttps://preview.redd.it/z9gwzz9avs9g1.png?width=908&format=png&auto=webp&s=2c7eaf6b045e54ab474d1021b20d6b803373a209\n\nAlso the devs say this and it shows.   \n[https://www.reddit.com/r/LocalLLaMA/comments/1p1b550/comment/npotmbo/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1p1b550/comment/npotmbo/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
          "score": 8,
          "created_utc": "2025-12-27 19:41:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8so1w",
              "author": "SlowFail2433",
              "text": "Thanks for the quote from the devs thatâ€™s rly interesting. Ye that probably makes a difference TBH",
              "score": 4,
              "created_utc": "2025-12-27 20:02:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw72a2l",
          "author": "Tall-Ad-7742",
          "text": "Artificial analysis doesnâ€™t even have GLM 4.7 rated yet + these scores are only side of the performance the other side is trying them yourself and then deciding which you think fits your needs best",
          "score": 12,
          "created_utc": "2025-12-27 14:38:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7b5zk",
              "author": "SlowFail2433",
              "text": "Ye having your own benches is rly important",
              "score": 3,
              "created_utc": "2025-12-27 15:30:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw7d50n",
                  "author": "Tall-Ad-7742",
                  "text": "yes thats true and i must say i really like minimax 2.1 because its really good in frontend design but havent tested it much in other areas yet",
                  "score": 2,
                  "created_utc": "2025-12-27 15:40:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7qq1j",
          "author": "a_beautiful_rhind",
          "text": "Depends on what you're doing. It's coal for creative tasks.",
          "score": 7,
          "created_utc": "2025-12-27 16:49:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9v0jv",
          "author": "HealthyCommunicat",
          "text": "Idk i ran q3 m2.1 on m4 max 128gb and it couldnt anwer a single question correctly that qwen 3 next 80b did. Does q3 affect quality this much? I did a specific trial questions of 20 questions and literally not a single one correct. All basic questions of oracle ebs",
          "score": 4,
          "created_utc": "2025-12-27 23:32:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9vm1j",
              "author": "SlowFail2433",
              "text": "Yeah q3 is rough",
              "score": 3,
              "created_utc": "2025-12-27 23:36:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwb8qvy",
                  "author": "colin_colout",
                  "text": "IQ3_XXS is surprisingly good but slowwww on strix halo. I use regular K quants for speed but it makes more mistakes.",
                  "score": 1,
                  "created_utc": "2025-12-28 04:28:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9w58l",
              "author": "mycall",
              "text": "Thanks for saving me the effort.",
              "score": 1,
              "created_utc": "2025-12-27 23:39:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9we7m",
                  "author": "HealthyCommunicat",
                  "text": "Was so hyped too man",
                  "score": 0,
                  "created_utc": "2025-12-27 23:40:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw87iy0",
          "author": "AppearanceHeavy6724",
          "text": "> Going by the **Artifical Analysis** benchaes,\n\nHere we go. What is next, analysis by Cosmopolitan? Men's Health?",
          "score": 6,
          "created_utc": "2025-12-27 18:14:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw888zi",
              "author": "SlowFail2433",
              "text": "Itâ€™s a collection of some of the most reputable public benchmarks that are widely used in research papers",
              "score": -1,
              "created_utc": "2025-12-27 18:17:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw8czgo",
                  "author": "AppearanceHeavy6724",
                  "text": "I know what it is. On paper it should be a good proxy benchmark to get the idea about the model's performance but in reality, for lack of better words, a \"fucking piece of shit\", as their index _never_ corresponds to actual performance. Infamously, they have Apriel 15b model - a royal turd of a model - above Deepseek R1 0528.\n\nFuck that.",
                  "score": 4,
                  "created_utc": "2025-12-27 18:41:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7gk17",
          "author": "DistinctWay9169",
          "text": "For coding, I did not like it very much. I tried it extensively against GLM 4.7 and Sonnet 4.5. Both GLM and Sonnet gave me great codes, but MiniMax2.1 was like a junior software engineer. GLM and Sonnet found a critical problem purposefully added to my code, while MiniMax2.1 found something else (not correct). I would not use MiniMax2.1 over these two I mentioned.",
          "score": 3,
          "created_utc": "2025-12-27 15:57:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7ieaf",
              "author": "SlowFail2433",
              "text": "Thanks this experience is helpful as thatâ€™s the exact model comparison that is most relevant",
              "score": 2,
              "created_utc": "2025-12-27 16:07:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw738tx",
          "author": "kevin_1994",
          "text": "According to swe-rebench (which is pretty much the only bench I somewhat trust at this point, together with simplebench) devstral is better per param: https://swe-rebench.com/\n\nThis matches my experience",
          "score": 13,
          "created_utc": "2025-12-27 14:44:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7449b",
              "author": "DistanceSolar1449",
              "text": "Devstral is dense 123b, of course itâ€™s better.",
              "score": 10,
              "created_utc": "2025-12-27 14:50:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7lwa8",
                  "author": "Dany0",
                  "text": "Devstral 2 IQ1\\_S barely fits on my 5090 lmao. It's better than the Devstral Small 2 at Q8, just suffers the typical low quant issue of going haywire past initial few k tokens.\n\nI get 7-10tk/s, if it was faster I could use it as an autocomplete model at least. Maybe someone here has the best vllm params to get it faster? I have a feeling it can be much faster\n\nEDIT:  \nOof I actually get more tk/s if I use Q4\\_K\\_S w/ cpu offload ðŸ˜¬ we need a REAP :(",
                  "score": 2,
                  "created_utc": "2025-12-27 16:24:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw76jwr",
                  "author": "SlowFail2433",
                  "text": "Hmm I would, as a default assumption, assume the double parameter model was stronger",
                  "score": 2,
                  "created_utc": "2025-12-27 15:04:15",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw73oaw",
              "author": "SlowFail2433",
              "text": "It is a good bench yeah although I like to see other areas\n\n\nAgentic abilities for example donâ€™t correlate well with swe-rebench compared to the agentic benches",
              "score": 2,
              "created_utc": "2025-12-27 14:47:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw70fp4",
          "author": "Worried_Goat_8604",
          "text": "Yes its currently the best for agentic coding and stuff",
          "score": 8,
          "created_utc": "2025-12-27 14:27:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7hd8b",
              "author": "DistinctWay9169",
              "text": "Not even close. Claude sonnet 4.5 without thinking is much better than minimax; at least the tests i've done showed that minimax2.1 is more like a junior swe and claude a specialist.",
              "score": 0,
              "created_utc": "2025-12-27 16:02:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7iqnw",
                  "author": "Worried_Goat_8604",
                  "text": "Ofc we are comparing for OSS models. If we compare closed ones also, many models can beat minimax m2.1",
                  "score": 9,
                  "created_utc": "2025-12-27 16:09:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7ghup",
          "author": "No_Afternoon_4260",
          "text": "That's because you haven't tried devstral 123B",
          "score": 4,
          "created_utc": "2025-12-27 15:57:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7hyc5",
              "author": "SlowFail2433",
              "text": "Good point I have not, yet",
              "score": 1,
              "created_utc": "2025-12-27 16:05:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7d0v8",
          "author": "KrayziePidgeon",
          "text": "You could say they *Min-Maxed*\n\n(â€¢_â€¢)\n\n( â€¢_â€¢)>âŒâ– -â– \n\n(âŒâ– _â– )",
          "score": 4,
          "created_utc": "2025-12-27 15:39:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw70dfq",
          "author": "egomarker",
          "text": "\"Best value\" mostly depends on how much vram you have.",
          "score": 3,
          "created_utc": "2025-12-27 14:27:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw724tl",
              "author": "jamaalwakamaal",
              "text": "Its not like they're competing with Qwen 4bÂ ",
              "score": 2,
              "created_utc": "2025-12-27 14:38:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7k6d3",
          "author": "Excellent-Sense7244",
          "text": "From my experience GLM is at sonnet level if not better",
          "score": 2,
          "created_utc": "2025-12-27 16:16:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7kozh",
              "author": "SlowFail2433",
              "text": "Nice that sounds great",
              "score": 2,
              "created_utc": "2025-12-27 16:18:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7cd9j",
          "author": "Foreign-Watch-3730",
          "text": "Sorry, but on my local machine, using Q4 (I've tested all the quants in Q4), after testing numerous parameters, including Unsloth, the code (under real-world conditions, not just a benchmark) isâ€¦ badâ€¦ syntax errors, dead code, in short, it generates a lot of errors. You have to upgrade to Q8 to fix the problem ( i must test other Q5 or Q6 ), but then it's very slow. GLM 4.7 is slightly better, but very close (with the same command prompt and the same q4).   \n Minimax m2 is very fast: with my RTX 3090, I get 56 tokens per secondâ€¦ It's fast in Q4, sure, but at what cost â€¦  \nI continue the test with other quants ...",
          "score": 2,
          "created_utc": "2025-12-27 15:36:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7e5tn",
              "author": "Mountain-Active-3149",
              "text": "How are you getting 56ts with just one 3090 running at Q4?  Do you use llamacpp?",
              "score": 2,
              "created_utc": "2025-12-27 15:45:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7kbrj",
                  "author": "Foreign-Watch-3730",
                  "text": "7 RTX 3090 ( 3 pair of nvlink ) lm studio use llama.cpp",
                  "score": 3,
                  "created_utc": "2025-12-27 16:16:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7cwrw",
              "author": "SlowFail2433",
              "text": "Thanks a lot, negative reports (people not liking models) are even more valuable than positive reports",
              "score": 1,
              "created_utc": "2025-12-27 15:39:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw7uu0m",
                  "author": "doradus_novae",
                  "text": "Agreed this was the exact feedback I needed, trying to determine if i should downshift from a working Q8",
                  "score": 2,
                  "created_utc": "2025-12-27 17:09:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7kz0m",
          "author": "cgs019283",
          "text": "For coding, I agree. For multilingual, and creative writing is where the minimax start to fell off compared to others.\n\nIt's a really good model tho. I hope they fix those problems soon.",
          "score": 1,
          "created_utc": "2025-12-27 16:20:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8rnkd",
          "author": "anonynousasdfg",
          "text": "In Agentic performance and strictly obeying system prompt/rules which one is better: minimax v2.1 or GLM 4.7 based on your individual experience?",
          "score": 1,
          "created_utc": "2025-12-27 19:57:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8sk6e",
              "author": "SlowFail2433",
              "text": "Still testing. I agree this is a key comparison",
              "score": 1,
              "created_utc": "2025-12-27 20:02:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdpt7s",
          "author": "JudgmentPale458",
          "text": "This is a really interesting point, especially if the benchmarks are normalized properly for context length and inference settings.\n\nWhat stands out to me isnâ€™t just the *per-param* performance, but the implication that MiniMax-M2.1 may be benefiting from **strong architectural and training choices rather than brute scale**. At \\~229B params, competing with models that are 2â€“5Ã— larger suggests either very effective data curation, training curriculum, or optimization around reasoning-heavy tasks.\n\nOne thing Iâ€™d be curious about:\n\n* How stable is this advantage across *different task families* (long-context reasoning, tool use, multilingual, code)?\n* And whether the gains persist under **instruction tuning / adapter-based fine-tuning**, or if theyâ€™re mostly visible in base / eval settings.\n\nIf this holds up in real-world fine-tuning and deployment scenarios, it really shifts the â€œbigger is betterâ€ narrative toward *better-trained is better* â€” which is great news for anyone running models outside hyperscaler budgets.\n\nWould love to see more open evaluations or downstream task reports on this.",
          "score": 1,
          "created_utc": "2025-12-28 16:04:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf8zsf",
          "author": "joonanykanen",
          "text": "I agree that MiniMax is an absolute workhorse at the moment. It is so easy to just let it cook a new feature on Kilo Code's Architect and then Code mode and write a full PR. One real-life example of this has been my [React Tetris](https://github.com/joonanykanen/react-tetris) project that I initially started coding with GLM 4.7.",
          "score": 1,
          "created_utc": "2025-12-28 20:31:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfji68",
          "author": "sjoerdmaessen",
          "text": "Q5 is the minimum for me tho, below it, like the Q4 I dont get the same wow effect. It's unable to find bugs any of the 4 bugs I made on purpose, but on Q5 they got identified correctly.  \n  \nIts an amazing model.",
          "score": 1,
          "created_utc": "2025-12-28 21:22:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0g54b",
          "author": "laterbreh",
          "text": "I'm going to echo some of the comments here but with a distinction, M2.1 is really good generally -- however it does shine as a \"get it done\" workhorse for sure, and even if the answer is not perfect on the first shot, if youre coding and itterating with it, its \"agility\" is insane, and the lightning fast responses are amazing as well. I run it on two rtx 6000's and it just flies.\n\nHowever GLM4.7 provides higher quality single shot answers and ill generally say is smarter than minimax (has more parameters and more activated per token by a decent margin), however i usually use glm4.7 as my \"brain\" and minimax as \"my hands\". Based on reading from the creators of minimax this is exactly its use case. Good general intelligence and an amazing workhorse.\n\nAnd honestly with some extra push back from the user or some guidance its answers are great. This model ive found also responds VERY WELL giving it tools to educate itself, given with a big enough context window, its an EXCELLENT researcher.",
          "score": 1,
          "created_utc": "2026-01-01 01:26:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw71fmq",
          "author": "dual-moon",
          "text": "where are these benchmarks? sorry, we're new to the benchmarks but interested in the numbers if you have a link!",
          "score": 1,
          "created_utc": "2025-12-27 14:33:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw723ot",
              "author": "SlowFail2433",
              "text": "https://artificialanalysis.ai",
              "score": 2,
              "created_utc": "2025-12-27 14:37:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwm3odw",
                  "author": "dual-moon",
                  "text": "tysm!!! <3",
                  "score": 1,
                  "created_utc": "2025-12-29 21:10:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw88m4x",
          "author": "koushd",
          "text": "I'm running M2.1 full precision locally and prefer it over glm 4.7 fp8 by far. Size aside, it's **so much faster** and generally works through the solution quicker (even if it takes a few iterations). GLM takes way too long and most benchmarks do not really measure this total time to completion. Even qwen3 480b fp8 runs faster, as a larger model, due to GQA.",
          "score": 0,
          "created_utc": "2025-12-27 18:19:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcs4ki",
          "author": "datbackup",
          "text": "Minimax M2.1 unsloth q8_K_XL is an absolute banger",
          "score": 0,
          "created_utc": "2025-12-28 12:37:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw71trk",
          "author": "Michaeli_Starky",
          "text": "These Chinese models are only good for benchmarks. In real world scenarios they are pretty much unusable.",
          "score": -17,
          "created_utc": "2025-12-27 14:36:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw72slm",
              "author": "SlowFail2433",
              "text": "There is definitely â€œsomething specialâ€ with the closed models beyond benches yes",
              "score": 1,
              "created_utc": "2025-12-27 14:42:05",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw7bovr",
              "author": "silenceimpaired",
              "text": "If you are comparing again closed modelsâ€¦ sure they are worseâ€¦ but if you have a non Chinese local model you think is better please share. Love to test it out.",
              "score": 1,
              "created_utc": "2025-12-27 15:32:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7d7pu",
                  "author": "SlowFail2433",
                  "text": "The big new Mistral is a deepseek-like",
                  "score": 1,
                  "created_utc": "2025-12-27 15:40:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pwf8p7",
      "title": "What's the point of potato-tier LLMs?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/",
      "author": "Fast_Thing_7949",
      "created_utc": "2025-12-26 21:15:23",
      "score": 148,
      "num_comments": 237,
      "upvote_ratio": 0.7,
      "text": "https://preview.redd.it/64wjim607m9g1.png?width=1024&format=png&auto=webp&s=fb5666c56138804f6be65ef56b519345f992b4cd\n\nAfter getting brought back down to earth in my last thread about replacing Claude with local models on an RTX 3090, I've got another question that's genuinely bothering me: What are 7b, 20b, 30B parameter models actually FOR? I see them released everywhere, but are they just benchmark toys so AI labs can compete on leaderboards, or is there some practical use case I'm too dense to understand? Because right now, I can't figure out what you're supposed to do with a potato-tier 7B model that can't code worth a damn and is slower than API calls anyway. \n\nSeriously, what's the real-world application besides \"I have a GPU and want to feel like I'm doing AI\"?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nw38qnt",
          "author": "KrugerDunn",
          "text": "I use Qwen3 4B for classifying search queries.  \n  \nLlama 3.1 8B instruct for extracting entities from natural language.  \nExample: \"I went to the grocery store and saw my teacher there.\" -> returns: { \"grocery store\", \"teacher\" }\n\nQwen 14B for token reduction in documents.   \nExample: \"I went to the grocery store and I saw my teacher there.\" -> returns: \"I went grocery saw teacher.\" which then saves on cost/speed when sending to larger models.\n\nGPT\\_OSS 20B for tool calling.  \nExample: \"Rotate this image 90 degrees.\" -> tells agent to use Pillow and do make the change.\n\nIf just talking about personal use almost certainly better to just get a monthly subscription to Claude or whatever, but at scale these things save big $.\n\nAnd of course like people said uncensored/privacy requires local, but I haven't had a need for that yet.",
          "score": 176,
          "created_utc": "2025-12-26 21:45:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw53czq",
              "author": "One_Hovercraft_7456",
              "text": "Be careful token compressing has been shown to reduce performance in llm output",
              "score": 56,
              "created_utc": "2025-12-27 04:39:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw68kqd",
              "author": "Constandinoskalifo",
              "text": "Checkout [GLiNER2](https://huggingface.co/fastino/gliner2-multi-v1). It can replace at least the first 2 LLM calls! You can also use [FunctionGemma](https://huggingface.co/google/functiongemma-270m-it) for tool calling, but I haven't tried this one yet!",
              "score": 13,
              "created_utc": "2025-12-27 10:50:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwaonjm",
                  "author": "KrugerDunn",
                  "text": "I shall, thank you!",
                  "score": 2,
                  "created_utc": "2025-12-28 02:24:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw65x5h",
              "author": "SmartCustard9944",
              "text": "It's LLMs all the way down",
              "score": 9,
              "created_utc": "2025-12-27 10:24:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwt2roj",
                  "author": "apodicity",
                  "text": "lol impeccable comic timing",
                  "score": 1,
                  "created_utc": "2025-12-30 21:56:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw676nt",
              "author": "pablo8itall",
              "text": "Why did you choose the different models for those different tasks? Was there a clear performance difference?",
              "score": 5,
              "created_utc": "2025-12-27 10:37:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwaoi8b",
                  "author": "KrugerDunn",
                  "text": "They are for different projects with different requirements. For example the Qwen 14B one is \"offline\" meaning it can run at a much lower token speed, whereas the 4B one needed to be snappier. These aren't what I'd use every time, just examples of usage.",
                  "score": 1,
                  "created_utc": "2025-12-28 02:23:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw6mhrn",
              "author": "butter-transport",
              "text": "Just curious, why Qwen 14B for token compression and not something like LLMLingua 2 with a small encoder? Are the inference cost savings not significant in your use case, or does Qwen perform significantly better?",
              "score": 3,
              "created_utc": "2025-12-27 12:54:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw8bne0",
                  "author": "KrugerDunn",
                  "text": "To answer both above:\nI actually had not come across LLMLingua 2, I will test with it and check benchmarks, thanks!\n\nI chose Qwen for that particular use case because it doesnâ€™t require license agreement like Llama. The 4B performer the best on speed, which I needed for a live inference project and the 14B for an offline document processor so I could afford to slowdown to hit better quality benchmarks.\n\nI also use other stuff like CPU embeddings, SLMs, etc, but the OP ask about LLM under 30B params.\n\nAll benchmarks are done by my own evals, so these are by no means vouching for industry standards in any way just what I use for my hobby projects which go from 1 user up to 10,000 MAU.\n\nNo promise these are the best use just some uses!",
                  "score": 2,
                  "created_utc": "2025-12-27 18:34:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw34slz",
          "author": "jonahbenton",
          "text": "Classification and sentiment of short strings.",
          "score": 186,
          "created_utc": "2025-12-26 21:23:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw48myk",
              "author": "Budget-Juggernaut-68",
              "text": "sometimes you just don't need huge models to do everything. especially when you're building them in a pipeline.",
              "score": 19,
              "created_utc": "2025-12-27 01:15:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5gfoa",
                  "author": "sirebral",
                  "text": "Key, as long as it is a decent tool use, for pipelines, these smaller models are great, cheap to run, and very useful.",
                  "score": 6,
                  "created_utc": "2025-12-27 06:21:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3bkp5",
              "author": "PacManFan123",
              "text": "I use them for checking, inferring, and fixing punctuation!",
              "score": 53,
              "created_utc": "2025-12-26 22:00:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw39z8c",
              "author": "claytonjr",
              "text": "Yup, mistral 7b is still a work horse for things like this. I've even able to pull it off with the micro gemma models.Â ",
              "score": 40,
              "created_utc": "2025-12-26 21:51:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3ulpy",
              "author": "mycall",
              "text": "The two original sins of language models.",
              "score": 7,
              "created_utc": "2025-12-26 23:51:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3bah9",
              "author": "the_bollo",
              "text": "Can you give me a practical example please?",
              "score": 7,
              "created_utc": "2025-12-26 21:58:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3du7n",
                  "author": "eloquentemu",
                  "text": "Consider Amazon's reviews, which have a list of traits like +Speed and -Size that link back to individual reviews.  You'd do something like:\n\n> The following is a product review.  Extract the sentiment about key positives and negatives like Speed, Size, Power, etc.  Format your response as json\n\nWhen you have millions and millions of reviews, you don't want to run them through a 200B model.  A ~7B handles that sort of thing just fine.  Once you're preprocessed the individual reviews, you might use a larger model to process the most informative ones (which you can now easily identify) to write the little review blurb.",
                  "score": 65,
                  "created_utc": "2025-12-26 22:12:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw4eghq",
                  "author": "Awkward-Customer",
                  "text": "As a simple example, I have a script that i use to parse all of my bank / credit card statements and then import them into my budgeting software. For any uncategorized transactions I use my local LLM to review the information and suggest the category that it should be. I don't trust a third party service to send this data to, and it's very fast on my local model.",
                  "score": 45,
                  "created_utc": "2025-12-27 01:53:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw5d8z8",
                  "author": "ranakoti1",
                  "text": "I needed to do an abstract classification of 300k abstracts to classify papers in different themes. I used Gemma 12b for that and it was done in 1 day on a 4090. Using api calls on even cheaper models would cost me 50â‚¬ +.  I took a random sample beforehand to compare both local Gemma model and gemini 2.5 flash and the Gemma models accuracy was close to 98%.",
                  "score": 5,
                  "created_utc": "2025-12-27 05:54:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw37ec8",
              "author": "KrugerDunn",
              "text": "This is the answer.",
              "score": 7,
              "created_utc": "2025-12-26 21:37:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3fwnn",
                  "author": "chickenfriesbbc",
                  "text": "Yeah for like really small models but OP was asking about up to 30b models, like , wtf lol",
                  "score": 8,
                  "created_utc": "2025-12-26 22:24:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw52kx4",
              "author": "the__storm",
              "text": "And you can go pretty far within this category with a 30B or even 7B dense model (i.e., not so short strings, and quite complex classifications).",
              "score": 1,
              "created_utc": "2025-12-27 04:33:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw5jj44",
              "author": "saig22",
              "text": "Exactly, you do small tasks like mail routing with those.",
              "score": 1,
              "created_utc": "2025-12-27 06:48:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw68prc",
              "author": "Constandinoskalifo",
              "text": "Checkout [GLiNER2](https://huggingface.co/fastino/gliner2-multi-v1).",
              "score": -1,
              "created_utc": "2025-12-27 10:52:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw36dab",
          "author": "scottgal2",
          "text": "Well do I have the blog for that! Short answer; as components in sytems with constrained prompts and context. If you wrap their use with deterministic components they function EXTREMELY well I REGULARLY use 3b class models for stuff like synthesis over RAG segments etc they're quick and free.  \nRecent example is doign graphrag (a minimum viable version anyway) using heuristic / ML (BERT) extraction and small llm synthesis of community summaries. Versus the HUNDREDS of GPTTurbo 4 calls the original MSFT Research version uses.  \nIt's \\*kind of my obsession\\*.  [https://www.mostlylucid.net/blog/graphrag-minimum-viable-implementation](https://www.mostlylucid.net/blog/graphrag-minimum-viable-implementation)  \nIn short; for a LOT more than you think if you use them correctly!",
          "score": 136,
          "created_utc": "2025-12-26 21:32:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3d44o",
              "author": "Southern-Chain-6485",
              "text": "Hey, let's assume I have no idea what you've just wrote. What do you use them for, ELI5 style?",
              "score": 28,
              "created_utc": "2025-12-26 22:08:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3dtl4",
                  "author": "scottgal2",
                  "text": "As PARTS of a system not the whole system itself. Think of them like really clever external API calls which can do 'fuzzy stuff' like interperet sentences etc. SMALL stuff as part of a bigger application; even TINY models like 1b tinyllama are GREAT for smart 'sentinels' for directing requests etc.   \nFor example on the code point they CAN write code...just not big chunks.  So if you give them a concise description of a function / small class they CAN generate that. They just don't have the 'attention span' (kinda) do do more because they lose track.   \nBut as fuzy bits you bolt to NON fuzzy bits of an app they're great!",
                  "score": 55,
                  "created_utc": "2025-12-26 22:12:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwawlmq",
                  "author": "KGeddon",
                  "text": "7B is not going to write you an essay on south african polities and groups in 1850, with detailed leadership info, strengths and weaknesses, goals and their relations with other factions/groups.\n\nIt will be able to summarize a paragraph or tell you if someone is writing in a tone that indicates they are mad.",
                  "score": 1,
                  "created_utc": "2025-12-28 03:11:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw39dyf",
              "author": "Consistent-Cold8330",
              "text": "have you tried graphiti before? is there a way to make something like that? a bi temporal graph knowledge base by using a weak model and ensure the accuracy of extracted entities?",
              "score": 3,
              "created_utc": "2025-12-26 21:48:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3a2np",
                  "author": "scottgal2",
                  "text": "I'm a systems builder, I think in raw code so I tend to work bottom up (not theory down...if that makes sense?) . That article was \\*just today\\* so I haven't got there yet. I was understanding \\*raw code\\*. But thanks for the tip!",
                  "score": 7,
                  "created_utc": "2025-12-26 21:52:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3dt5k",
              "author": "_raydeStar",
              "text": "huh.  this is cool.  Gonna give you a follow.  \n\nWhat would you say is your favorite model?",
              "score": 2,
              "created_utc": "2025-12-26 22:12:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3ec7z",
                  "author": "scottgal2",
                  "text": "Recently llama3.2:3b old but it seems to just ROCK at generating well structured JSON. I even use it as the basis for a little api simulator!  [https://github.com/scottgal/LLMApi/blob/master/README.md](https://github.com/scottgal/LLMApi/blob/master/README.md)   \nThough noticed the docs say ministral-3:3b - really the point is that once you constrain them well and wrap them in validation and error correction you can use almost ANYTHING to useful effect; it WORKS with 1.5b class models for example.     \nI would have posted before but Reddit kinda terrifies me.",
                  "score": 17,
                  "created_utc": "2025-12-26 22:15:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw8dz8u",
              "author": "Watchguyraffle1",
              "text": "Wow.  That is a solid blog post.  Iâ€™m impressed.  Straightâ€¦to the point writing style.  Easy to understand if you are in the space.  Impossible to know if you arenâ€™t â€” a good thing.  I wonder if there is a discussion worth about the data model used for the segments especially to make the data model â€œfitâ€ specific data types/questions.  But this is really well written.",
              "score": 2,
              "created_utc": "2025-12-27 18:46:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw8rv97",
                  "author": "scottgal2",
                  "text": "There is a duiscussion, but not really data model. That's almost incidental. It's more around querying strategies to 'fit' questions (cross domain, needle etc). the actual data access is as simple as possible (DuckDB vector, NOT graph...we don't need it).",
                  "score": 1,
                  "created_utc": "2025-12-27 19:58:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3zg22",
              "author": "TrekkiMonstr",
              "text": "Do you think they have surpassed traditional NLP? Like, say I have a piece of Japanese text, I want to get the lemmas of each word, would you reach for MeCab or just throw it into an LLM?",
              "score": 1,
              "created_utc": "2025-12-27 00:20:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw40qum",
                  "author": "scottgal2",
                  "text": "No; LLMs havenâ€™t â€œsurpassedâ€ traditional NLP for tasks like lemmatisation.  \nIf I want Japanese lemmas, Iâ€™d reach for MeCab (or Sudachi) every time.  \nAn LLM is the wrong tool for that job. ESPECIALLY small LLMs - they tend to be TERRIBLE with Japanese text (limited corpus) \n\nAn LLM can often *produce* grammatical variants, but it canâ€™t guarantee completeness, consistency, or correct segmentation. With tools like MeCab, you know exactly what analysis was applied and why; and you get the same result every time.",
                  "score": 5,
                  "created_utc": "2025-12-27 00:27:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw35p94",
          "author": "Amarin88",
          "text": "Weaker models can keep your private data contained. While talking to the cloud to figure complicated problem.",
          "score": 97,
          "created_utc": "2025-12-26 21:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3dd69",
              "author": "LocoMod",
              "text": "My global scan results say otherwise. If you knew how many Ollama, LMStudio, vLLM instances are wide open on the internet it would be sobering.\n\nIf cloud gets compromised you should know about it. If your home network or services are, you probably wonâ€™t know about it.",
              "score": -47,
              "created_utc": "2025-12-26 22:10:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3ivcv",
                  "author": "the_renaissance_jack",
                  "text": "If your home network and services are compromised, an open LLM instance is the least of your concerns.",
                  "score": 76,
                  "created_utc": "2025-12-26 22:41:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3m877",
                  "author": "eloquentemu",
                  "text": "An open instance doesn't have anything to do with keeping data contained. First, you can just not open it - it's not like a local model requires you to open it to the internet. Second, an open instance doesn't leak your private data.\n\nSure, if you get hacked you get hacked, but a minecraft server has that problem just as much as an LLM.",
                  "score": 15,
                  "created_utc": "2025-12-26 23:00:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3ozdg",
                  "author": "Borkato",
                  "text": "I would love to know how this works and how I can be sure Iâ€™m not inadvertently broadcasting",
                  "score": 2,
                  "created_utc": "2025-12-26 23:17:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw522rt",
                  "author": "Arxijos",
                  "text": "Put all that hardware / software / containers in it's own vLan and only allow your work machine to ssh that vLan could easily solve parts of the problem, correct?\n\nKnowing when my data / question to the LLM is being send somewhere is something i would like to figure out to make more safe. Is there anything in the works except wait for others more qualified to review the code?",
                  "score": 2,
                  "created_utc": "2025-12-27 04:30:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw37rl3",
          "author": "simracerman",
          "text": "Have you ever noticed those tiny screwdrivers or spanners in a tool set, the ones youâ€™d rarely actually use? Â \n\nItâ€™s intentional. Every tool has its place. Just like a toolbox, different models serve different purposes. Â \n\nMy 1.2B model handles title generation. The 4B version excels at web search, summarization, and light RAG. The 8B models bring vision capabilities to the table. And the larger ones 24B to 32B, shine in narrow, specialized tasks. MedGemma-27B is unmatched for medical text, Mistral offers a lightweight, GPT-like alternative, and Qwen30B-A3B performs well on small coding problems. Â \n\nFor complex, high-accuracy work like full-code development, I turn to GLM-Air-106B. When a query goes beyond what Mistral Small 24B can handle, I switch to Llama3.3-70B. Â \n\nHereâ€™s something rarely acknowledged. closed-source models often rely on a similar architecture, Â layered scaffolding and polished interfaces. When you ask ChatGPT a question, it might be powered by a 20B model plus a suite of tools. The magic lies not in raw power.\n\nThe best answers arenâ€™t always from the â€œstrongestâ€ model, they come from choosing the right one for the task. And that balance between accuracy, efficiency, and resource useÂ still requires human judgment. We tend to over-rely on large, powerful models, but the real strength lies in precision, not scale.",
          "score": 86,
          "created_utc": "2025-12-26 21:39:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3w5cz",
              "author": "mycall",
              "text": "I wish someone kept an updated table of what models are best for what tasks.  That would save a ton of effort for solution engineers.",
              "score": 20,
              "created_utc": "2025-12-27 00:00:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw43tvy",
                  "author": "Marksta",
                  "text": "A solution engineer should take up engineering this solution...",
                  "score": 21,
                  "created_utc": "2025-12-27 00:46:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw6x1z3",
                  "author": "maurosurusdev",
                  "text": "We are building that! check out [latamboard.ai](http://latamboard.ai) We focus on building task-oriented benchmarks",
                  "score": 5,
                  "created_utc": "2025-12-27 14:06:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw9zvab",
                  "author": "justron",
                  "text": "I think one challenge is that \"best model\" can be so task-specific. A model might be great at writing a python function but terrible at go, for example.\n\n  \nI created [trythatllm.com](http://trythatllm.com) to help folks compare models for their specific task/project. It doesn't (yet) handle really small models, though--if that's interesting, please message me!",
                  "score": 2,
                  "created_utc": "2025-12-28 00:00:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3i4qv",
              "author": "slrg1968",
              "text": "Which version of Mistral are you using?",
              "score": 1,
              "created_utc": "2025-12-26 22:37:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3xyil",
                  "author": "simracerman",
                  "text": "This:\nhttps://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF\n\nI follow their guide for llama.cpp parametersÂ ",
                  "score": 3,
                  "created_utc": "2025-12-27 00:11:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw59scu",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2025-12-27 05:26:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw84nyd",
                  "author": "koflerdavid",
                  "text": "Such information is unreliable unless the model developers put it into the training data. Asking LLMs for introspection is by their very nature just inviting them to hallucinate something.",
                  "score": 1,
                  "created_utc": "2025-12-27 18:00:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3sat0",
              "author": "razorree",
              "text": "well... nice setup, but it's like a few grands .... ?",
              "score": -1,
              "created_utc": "2025-12-26 23:37:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw406ds",
                  "author": "simracerman",
                  "text": "Funny you should say that. Everyoneâ€™s perceived model performance/speed is different. For me, conversational is 10 t/s, coding (only MoE works for my current machine so 20 t/s and above is acceptable, while vision is usually 15 t/s and itâ€™s good enough).\n\nEverything mentioned runs on this sub $700 mini PC using llama.cpp + llama-swap + openwebui. Of course I have MCP, TTS/STT, RAG all built into Docker or openwebui. The combo is stable and updates are mostly automated.\n\nhttps://www.ebay.com/itm/389094941313\n\nIâ€™m in the process of connecting an eGPU to it for smaller models to run even faster. If I can score a good deal on a 3090 or something similar, the speed of models under 27GB will hit 5-8x faster.\n\nAt that point, the whole setup will cost ~$1600. Itâ€™s over a grand, but for many of home use cases, itâ€™s fast.",
                  "score": 8,
                  "created_utc": "2025-12-27 00:24:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw38om9",
          "author": "DecodeBytes",
          "text": "\\> Â that can't code\n\nThis is the crux of it, there is so much hyper focus on models serving coding agents , and code gen by its nature of code (lots of connected ASTs) , requires a huge context window and training on bazillions of lines of code.\n\nBut what about beyond coding? For SLMs there are so many other use cases that silicon valley cannot see outside of their software-dev bubble - IoT, wearables, industry sensors etc are huge untapped markets.",
          "score": 38,
          "created_utc": "2025-12-26 21:44:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3udti",
              "author": "FencingNerd",
              "text": "The small models can absolutely code, just not at the level of a more sophisticated model.  It's great for basic help, function syntax, etc. \nYou're not getting a 1k line functional program, but it can easily handle a 20 line basic function.",
              "score": 22,
              "created_utc": "2025-12-26 23:49:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw53psf",
                  "author": "960be6dde311",
                  "text": "This is my experience as well. They're useful for asking about conceptual things, but not using in a coding agent to write software for you. It's kind of like having access to a stripped down version of the Internet available locally, even better than just self-hosting Wikipedia.Â ",
                  "score": 3,
                  "created_utc": "2025-12-27 04:41:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx2ac5c",
                  "author": "DecodeBytes",
                  "text": "Sorry late reply, I mean in the typical current agent style, long drawn out sessions back and forth.",
                  "score": 1,
                  "created_utc": "2026-01-01 10:49:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3foe8",
          "author": "[deleted]",
          "text": "Some people use them for roleplaying or just having casual conversations with the model.\n\n\nI got a 8B model I use for helping me come up with recipes with whatever I have available in my apartment that week.Â \n\n\nWe're not all coders here.Â ",
          "score": 34,
          "created_utc": "2025-12-26 22:23:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw53jjh",
              "author": "Karyo_Ten",
              "text": "Roleplay is one of those \"full stack\" task that needs an extremely capable model with excellent world and pop culture knowledge.",
              "score": 14,
              "created_utc": "2025-12-27 04:40:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5z3u2",
                  "author": "Alex_L1nk",
                  "text": "That's why it's not just bare Mistral or LLAMA, but rather, finetune and/or merge.",
                  "score": 2,
                  "created_utc": "2025-12-27 09:17:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw4nker",
          "author": "iMrParker",
          "text": "This is such a vibe coding point of view. Smaller models can code but it's not going to one-shot your shit. They're good replacements for Google and stack overflow",
          "score": 14,
          "created_utc": "2025-12-27 02:51:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw39gke",
          "author": "false79",
          "text": "You see them released everywhere but you haven't figured out to exploit them by having a very specific task rather than trying to answer every possible question.\n\nIn my case, I'm using gpt-oss-20b and it's more than enough to do one shot prompting to save me from doing mundane coding tasks.\n\nIf you provide sufficient context on these models that you look down upon, you can get the same answers you'd get from large LLMs but at 2x-3x faster speeds.\n\nPeople who don't know blame the model for not being able to produce the results they want.",
          "score": 12,
          "created_utc": "2025-12-26 21:48:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6fvxp",
              "author": "power97992",
              "text": "If you spend Â 10 -12min writing Â out the context Â and running it then modifying the prompt and rerunning the small LM, uâ€™ll end up spending Â more time on a small llm then on a large LM",
              "score": 0,
              "created_utc": "2025-12-27 11:59:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6y0oi",
                  "author": "false79",
                  "text": "If you're spending 10-12 minutes, you're doing it wrong.",
                  "score": 3,
                  "created_utc": "2025-12-27 14:12:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3e2bh",
          "author": "RiskyBizz216",
          "text": "Sometimes they are for *deployment* \\- you can deploy a 1B/3B/4B model to a mobile device, or a raspberry pi. You can even deploy an LLM in a chrome extension!\n\nThe 7B/8B/14B models are for *rapid prototyping* with LLMs, for example - if you are developing an app that calls an LLM - you can simply call a smaller (and somewhat intelligent) LLM for rapid responses.\n\nThe 24B/30B/32B models are your *writing and coding assistants.*",
          "score": 12,
          "created_utc": "2025-12-26 22:14:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3e2p9",
              "author": "RiskyBizz216",
              "text": "I personally believe that companies will phase out these smaller models from public some day. Models like GPT-OSS 20B are just an embarrassment. As companies become more competent, you will see fewer potatoes and more jalapeÃ±os!",
              "score": -9,
              "created_utc": "2025-12-26 22:14:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw85mg2",
                  "author": "koflerdavid",
                  "text": "Hosting files a few GB in size has been a solved problem for a few decades now: Torrent.",
                  "score": 2,
                  "created_utc": "2025-12-27 18:04:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3762r",
          "author": "dobkeratops",
          "text": "gets a foot in the door.   \nand you can get quite good VLMs in this range that can describe an image.\n\nI've got useful reference answers out of 7b's (and far more so 20,30b's). It can keep you off a cloud service for longer. You dont need it to code for you, it can still be a useful assist that's faster than searching through docs.\n\nI believe Local AI is absolutely critical for a non-dystopian future.",
          "score": 11,
          "created_utc": "2025-12-26 21:36:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw34uny",
          "author": "Southern-Chain-6485",
          "text": "Uncensored models, vision, prompt processing for local ai image generators, privacy, and anything you don't need any complex stuff. Do you want to translate something? You can use a small model. Check grammar? Same.",
          "score": 17,
          "created_utc": "2025-12-26 21:24:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3e5i6",
          "author": "rosstafarien",
          "text": "What will you run on a phone in a poor network coverage area? How confident are you that what you're sending to the cloud isn't being logged by your provider? What happens to your business model if the cost for remote inference triples or worse.\n\nRunning on a potato is the only AI I'm interested in right now.",
          "score": 9,
          "created_utc": "2025-12-26 22:14:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3w8ic",
          "author": "simulated-souls",
          "text": "Big thing that people aren't mentioning: [fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_%28deep_learning%29?wprov=sfla1).\n\n\nIf you have a narrow task and some examples of how to do it, then giving a model a little extra training (often using something like a LoRA adapter) can be the best solution.\n\n\nFine-tuned \"potato\" models can often match or even exceed the performance of frontier models, while staying cheap and local.\n\n\nFine-tuning is also even more intensive (especially for memory) than inference, so you're probably stuck doing it with small models. Luckily you only only need to fine-tune a model once and can reuse the new parameters for as much inference as you want.",
          "score": 8,
          "created_utc": "2025-12-27 00:00:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw377qi",
          "author": "Danternas",
          "text": "In daily use I see little difference between a 30B model and one of the commercial large ones (GPT/Gemini). Main difference is in their ability to search the internet and scrape data, something I still struggle with.",
          "score": 17,
          "created_utc": "2025-12-26 21:36:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6g3o1",
              "author": "power97992",
              "text": "There is a big difference even without web search, less knowledge and more prompting and longer prompts and worse results with a small model..",
              "score": 1,
              "created_utc": "2025-12-27 12:01:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw3g9eh",
          "author": "jamie-tidman",
          "text": "Summarisation, classification, routing, title / description generation, next line suggestion, local testing for deployment of larger models in the same family.",
          "score": 9,
          "created_utc": "2025-12-26 22:26:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3kmpo",
              "author": "silenceimpaired",
              "text": "Checking Spelling, grammar, punctuation.",
              "score": 5,
              "created_utc": "2025-12-26 22:51:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5zpua",
                  "author": "FastDecode1",
                  "text": "[Speculative decoding](https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/)",
                  "score": 3,
                  "created_utc": "2025-12-27 09:23:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3785o",
          "author": "nunodonato",
          "text": "Smaller models can excel at specific things, especially if trained. I would argue we will have many more uses for focused smaller models than bigger ones that try to excel at everything",
          "score": 7,
          "created_utc": "2025-12-26 21:36:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3m6xx",
          "author": "ThenExtension9196",
          "text": "Weaker models are for fine tuning. They can become immensely good at some narrow thing with very little requirements if you train them.",
          "score": 7,
          "created_utc": "2025-12-26 23:00:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4ke0u",
          "author": "dash_bro",
          "text": "Lots of fixed task tuning with limited data, which will be cheaper than the API in the long term. Also, 30B is definitely not potato tier!!!\n\neg got a classification problem? train/fine-tune/few shot prompt a small model without paying for per-token cost! \n\nwant something long running as a job, that might be potentially expensive even with cheap APIs? small models! \n\nwant to not be restricted by quality drops/rate limits/provider latency spikes? small models! \n\nLarge scale data labelling, which runs or curates data for you 24/7? Batch, run, save locally without exposing anything outside your system. Privacy is a big, big boost.\n\nThe biggest one in my opinion : learn. 99% of us aren't Research Scientists. You don't know what you don't know. Learn to do it yourself, become an expert and eventually build yourself to work at a top tier lab. It's an exclusive community for sure, but the knowledge gap between the ones in and out is usually pretty big.\n\nIn general:\n\n- anything <1B is actually really decent at the embedding/ranking level. I find the qwen-0.6B models to be excellent examples.\n\n- anything 1-3B is great for tuning. Think: intent classifications, model routing, fine tunes for non critical tasks, etc.\n\n- anything 7-10B is pretty decent for summarisation, entity/keyword extraction, graph building, etc. This is where few shot stuff and large scale data scoring starts being possible IMO.\n\n- anything in the 14B tier is good for classification tasks around gemini-flash/gpt-nano/claude haiku quality **if you provide enough/correct context**. Gets you 90-95% of the way there unless you need a lot of working context. Think about tasks that need 3-5k input tokens with a ~80-100 output tokens.\n\n- 30B tier usually is pretty good up until ~40k tokens as total working context. If you need more than that you'll have to be clever about offloading memory etc., but it can be done. 30B is readily gpt-4-32k tier when it first came out. Thinking models start performing around this level, imo. Great for local coding too!\n\nAfter 30B it's really more about the infra and latency costs, model management and eval tier problems that aren't worth it for 99% of us. So usually I dont recommend them being self hosted over a simple gpt/gemini call. Diminishing returns.",
          "score": 7,
          "created_utc": "2025-12-27 02:31:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw361yg",
          "author": "SinCebollista",
          "text": "Safety, privacy, and lack of censorship.",
          "score": 28,
          "created_utc": "2025-12-26 21:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw33u5v",
          "author": "Lodarich",
          "text": "vision models mostly",
          "score": 14,
          "created_utc": "2025-12-26 21:18:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw38n93",
          "author": "__SlimeQ__",
          "text": "qwen3 14b can do tool calls while running on my gaming laptop so I'm sure it could do something cool. i have yet to see such a thing though, in practice it is still very hard.\n\ni feel like the holy grail for that model size is a competent codex-like model that can do infinite dev on your local machine. and we do seem to be pushing very hard towards that reality year over year.",
          "score": 5,
          "created_utc": "2025-12-26 21:44:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3bd7t",
          "author": "Late_Huckleberry850",
          "text": "Also, you may be calling them potatoes now, but the latest version of the Liquid LFM-2.6-Exp has benchmarks on par or exceeding the original GPT-4 (which was revolutionary when it came out). So maybe they are experiments for now, but give it really only one more year and for many practical applications you will not mind using them.",
          "score": 10,
          "created_utc": "2025-12-26 21:59:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6ghpx",
              "author": "power97992",
              "text": "Gpt 4 was terrible for coding Â  , you had to prompt it 40-90 times and it still wouldnt get the answer right but it was good at web searching and summarizing. Â Lfm is gpt 4 lobotomized without all the world knowledgeÂ ",
              "score": 1,
              "created_utc": "2025-12-27 12:05:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw38xxp",
          "author": "Smashy404",
          "text": "As someone with an IQ of less than 7 I find the small models to be amazingly insightful.\n\nThe large ones just intimidate me.\n\nI didn't know you could install them on a potato though. I will try that tomorrow.\n\nThanks.",
          "score": 18,
          "created_utc": "2025-12-26 21:46:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3d3ey",
          "author": "IKoshelev",
          "text": "Reddit comments.Â ",
          "score": 5,
          "created_utc": "2025-12-26 22:08:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw47can",
          "author": "pieonmyjesutildomine",
          "text": "- Classification\n- Entity resolution\n- POS tagging\n- Dependency trees\n- lemmatization\n- creating stop-word lists\n- on-device inference\n\nUnique solutions: \n- logit manipulation\n- hypernetworks\n\nThese are all actual project solutions that I've been paid thousands of dollars for completing. The largest model used for these was 12b, and the smallest was 3b. Most projects required one or both of the \"unique solutions\" section to make the project reliable, but clients for the most part reported higher metrics than the classical ML solutions without overfitting, which is what they asked for. The nice thing is that I'm essentially going up against AutoGluon (if they even know about that), so I know what I have to beat and that's helpful.",
          "score": 5,
          "created_utc": "2025-12-27 01:07:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3c0lt",
          "author": "olearyboy",
          "text": "To keep Glados portable while she hunts her pray",
          "score": 4,
          "created_utc": "2025-12-26 22:02:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4adcd",
          "author": "M_Owais_kh",
          "text": "Small models exist because not everyone is trying to replace Claude, many are trying to build *systems* under real constraints.\n\nIâ€™m a student with no fancy GPUs and no interest in paying cloud providers. 20B models run locally on my mid tier laptop, offline, with no rate limits or costs. With good prompting and lightweight RAG, theyâ€™re perfectly usable knowledge and reasoning tools.  Theyâ€™re also ideal for pipeline development. I prototype everything locally, then swap in a larger model or API at deployment. The model is just a backend component. Not every task needs 500B level coding ability. Summarization, extraction, classification, rewriting and basic tasks work fine on small models. Using huge models everywhere is inefficient as well.",
          "score": 3,
          "created_utc": "2025-12-27 01:26:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4qilg",
          "author": "ZealousidealShoe7998",
          "text": "small LLM are just a capable of doing certain tasks as bigger LLMs the only difference is the amount of knowledge they have in such subject.  \nyou can in fact train a smaller LLM to do a specific task and it might perform just as good as a bigger LLM.  \nbut now you get less resource usage and more speed.\n\nthe problem is people are still obsessed with having the biggest LLM who can do it all.  \nbut for a lot of applications you might not need a 1T parameter comercial model.  \nyou could easily host in house a smaller LLM who fits in consumer hardware and train it on your actual data.\n\nbut this takes time, and expertise so what usually happens is people wait for a better OSS llm to be released and you can only do so much general stuff in such amount of parameters before the llm starts hallucinating.  \nperhaps a more efficient architecture might come along where a 30B parameter model might be just as good as todays comercial llms, but by them we gonna be like \"these llms are useless why dont we have AGI on consumer hardware yet?\" which honestly thats the greater question  \nwhat will take for us to have AËGI on consumer hardware ?",
          "score": 3,
          "created_utc": "2025-12-27 03:10:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw35h1v",
          "author": "swiftbursteli",
          "text": "I had a low-latency, high-throughput application. Sorting 50,000 items into categories. \n\nMinistral failed horrendously. The speed on my m4 pro was 70 tok/sec with 2s TTFT. \n\nWith those speeds, if you donâ€™t care for accuracy and care more about speed (chatbots, summarizing raw inputs) then that is the modelâ€™s use case. \n\nBut yes, SOTA models are much, much bigger than what we can afford on a lowly consumer grade machine. I saw an estimate online saying Gemini 3 can be 1-1.5 tb in a q4 variant. Consumers rarely get 64gb memoryâ€¦. SMBs can swing 128gb setupsâ€¦ \n\nTo get SOTA performance, youâ€™d need to do one of those leaning tower of Mac Mini and find a SOTA modelâ€¦. But you still have low memory bandwidth.",
          "score": 10,
          "created_utc": "2025-12-26 21:27:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw43c61",
          "author": "Nindaleth",
          "text": "Sometimes you'd be surprised. I wanted to create an AI agent documentation for our legacy test suite at work that's written in an uncommon programming language (there are no LSP servers for the language I could use instead AFAIK). Just get the function names, their parameters and infer from the docstring + implementation what each function does. The files are so large they wouldn't fit the GitHub Copilot models' context window one at a time - which is actually why I intended to condense them like this.\n\nI wasn't able to get GPT-4.1 (a free model on Copilot) to do it, it would do everything in its power to avoid doing the work. But a Devstral-Small-2-24B, running locally quantized, did it.",
          "score": 3,
          "created_utc": "2025-12-27 00:43:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4i7zb",
          "author": "Foreign-Beginning-49",
          "text": "They are literally endless. Here is one simple example. Just the microcontroller sensor world alone and the building guidance and idea generation could have a small model help you build robots until you want to do something else from sheer boredom. You can explore the basics of almost anything you can think of. If you.need to in depth research on a beetle family you're in hog heaven. A specific subspecies recently recognized in a journal? Thats up to you to geberate the knowledge. If you really work with the model as a cognitive enhancement device and are always skeptical instead of as a wise all knowing discarnate informant one can begin to accelerate their understanding of almost any area of study. Many high profile scientists are using Ai openly in their labs to accelerate human discovery. While many a waifu researcher is pushing the boundaries on digital human companions scientists at Stanford medicine are rapidly diagnosing new congenital tissue with rapid realtime semantically rich cellular imagery. Ai is allowing normies to work almost like proto polymaths if they apply themselves deeply enough.\n\n\nAnd because they are using their noodle they will know that no one source of information can be trusted except by outside verification and the seeking out of other sources of consensus they can use the llms of all sizes to augment their intellect and ability to manipulate the physical world with their imagination alone. This is all to say that even small models properly utilized can radically change your relationship to many fields or human endeavors. Its worth it. If you aren't doing the computing someone else is doing it for you. Own your own thinking machine its nice.",
          "score": 3,
          "created_utc": "2025-12-27 02:17:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5qubx",
          "author": "ozzeruk82",
          "text": "Captioning images. Qwen 3VL is superb at the task and means you donâ€™t need to upload all your (68000) family photos anywhere.",
          "score": 3,
          "created_utc": "2025-12-27 07:57:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3xp4u",
          "author": "Iory1998",
          "text": "Hmm.. you sound like someone working at an AI lab! Are you by any chance Sam Altman?ðŸ«¨ðŸ¤”",
          "score": 5,
          "created_utc": "2025-12-27 00:09:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3fm4n",
          "author": "robogame_dev",
          "text": "They're hard to take advantage of if you're not willing to code or vibe-code your use case. Then you use them as free/cheap/private inference for any tasks they CAN accomplish. For example, I used them to process 1600 pages of handwritten notes, OCRing the text, regenerating mermaid.js version of hand drawn flowcharts, etc. Would have cost me $50 with Gemini in cloud.",
          "score": 2,
          "created_utc": "2025-12-26 22:22:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3iifh",
          "author": "sluggishschizo",
          "text": "I had some good results with newer quantized models, whereas around half a year ago I couldn't get any halfway functional code out of any local model I tried. I recently tried to create a simple Python Tetris clone with GPT OSS 20b, Devstral Small 24b, and a GPT 5-distilled version of Qwen3 4b Instruct, and two of the three models did it about as well as the full Gemini 2.5 Flash did when I gave it the same task six months ago. \n\nThe GPT OSS model had one tiny error in the code where it misaligned the UI elements, which is exactly what Gemini 2.5 did on its first try at creating a Python Tetris clone when I tried this previously, but the tiny 4b model somehow got it right on its first try without any errors. The Devstral model eventually got it right with some minor guidance. \n\nI'm still astonished that a 4b parameter model that only takes up ~4gb of space can even do that. It'll be interesting to see where local coding models are in another six months.",
          "score": 2,
          "created_utc": "2025-12-26 22:39:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7isl2",
              "author": "gnnr25",
              "text": ">a GPT 5-distilled version of Qwen3 4b Instruct\n\nOoh, a new rabbit hole to go down",
              "score": 2,
              "created_utc": "2025-12-27 16:09:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw4x9vn",
              "author": "Miserable-Dare5090",
              "text": "the qwen3 4B 2507 version is amazing, finetunes so wellðŸ‘¨â€ðŸ³ðŸ˜˜",
              "score": 1,
              "created_utc": "2025-12-27 03:56:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw3jenj",
          "author": "Keep-Darwin-Going",
          "text": "Because not every situation you need to throw a nuke at. Smaller model can be fine tuned to do some stuff that need speed, privacy or cost sensitive. Like if I want a llm to help me play game, I am sure you do not want to use a sota model since it is slow and expensive.",
          "score": 2,
          "created_utc": "2025-12-26 22:44:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4300l",
          "author": "steveh250Vic",
          "text": "I have a Qwen3:14b model at the heart of an Agentic solution responding to RFP's - does a great job tool calling and developing responses.Â  Will likely move to 30b model soon but it's done a brilliant job so far.Â ",
          "score": 2,
          "created_utc": "2025-12-27 00:41:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4od8k",
          "author": "dr-stoney",
          "text": "Entertainment. The thing massive consumer companies ride on and B2B bros pretend doesn't exist.\n\n24B-32B is absolutely amazing for fun use-cases",
          "score": 2,
          "created_utc": "2025-12-27 02:56:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4tmjq",
              "author": "Party-Special-5177",
              "text": "Even smaller can be even more entertaining - I have absolutely lost an evening last year asking 1B class models questions like â€˜how many eyes does a cat haveâ€™ etc (if you havenâ€™t done this already, go do this now).\n\nI got my dad into LLMs by having Gemma write humorous limericks making fun of him and his dog for his birthday. I actually couldnâ€™t believe how good they were, neither could he.",
              "score": 1,
              "created_utc": "2025-12-27 03:31:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4vaud",
                  "author": "dr-stoney",
                  "text": "It's so awesome to read how people use LLMs for fun. Thank you ðŸ™",
                  "score": 1,
                  "created_utc": "2025-12-27 03:43:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw5fju3",
          "author": "Bmaxtubby1",
          "text": "Theyâ€™re kind of the easiest way to learn fine-tuning and inference without renting a data center.",
          "score": 2,
          "created_utc": "2025-12-27 06:14:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5fyr3",
          "author": "sirebral",
          "text": "Honestly, Qwen 3 is pretty impressive, particularly for tool use, so I've been happy with it because it quants  to four bits quite well and works great as a router and tool calling.  Runs quickly with MoE even with 100k context fitting in 32 gigs of RAM.\n\nOther uses for small models, single use experts.  Although MoE has really taken over this space.  Things evolve constantly, and the Chinese are open weights on most releases.  They concentrate more on efficiency, which is great for local inference, even with consumer level cards.\n\nEven their smaller versions can do quite well, so while Americans private models are more and more greedy with their VRAM, there are some slick applications for smaller models.",
          "score": 2,
          "created_utc": "2025-12-27 06:17:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw63ig5",
          "author": "wirfmichweg6",
          "text": "Sorting through my Obsidian notes without leaking content to any LLM providers.\n\nSummarize voice recordings to have a baseline for blog posts (I don't use LLMs for the texts themselves, just to make the voice recordings into text.)\n\nSummarize articles in Brave using their ollama support.",
          "score": 2,
          "created_utc": "2025-12-27 10:01:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7fvt7",
          "author": "BrilliantPhysics6611",
          "text": "I work for a startup and we deploy our products which use AI (including agents) to locations that canâ€™t access the internet. Due to this we commonly use 12B-24B models.\n\nThey can actually be quite good. The difference is though EVERY SINGLE PROMPT you put into a small model has to be carefully crafted and the scope needs to be narrow, verse with a frontier model you can put a half baked pos prompt in and still get great results, or you can throw 30 tools in and define a really wide scoped workflow for it to do and itâ€™ll do it, verse with a small model you have to break that up.",
          "score": 2,
          "created_utc": "2025-12-27 15:54:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwahzqj",
          "author": "Diligent_Cod_9583",
          "text": "Potatoes are delicious?",
          "score": 2,
          "created_utc": "2025-12-28 01:44:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3dufp",
          "author": "a_beautiful_rhind",
          "text": "A lot of it is people's cope but at the same time there's no reason to use a 1T model to do simple well defined tasks.\n\nQwen 4b is a great text encoder for z-image; there's your real world example. \n\nSmall VL models can caption pics. Small models can be tuned on your specific task so you don't have to pay for claude or have to run your software connected to the internet.",
          "score": 3,
          "created_utc": "2025-12-26 22:12:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3kzo3",
              "author": "silenceimpaired",
              "text": "In my experience dense 30b, 70b and MoE 120b, 300b are sufficient to manipulate and brainstorm prose.",
              "score": 6,
              "created_utc": "2025-12-26 22:53:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw3eqyo",
          "author": "chickenfriesbbc",
          "text": "...You can answer this question by just trying them...\n30b models active 3b are great. Your tripping",
          "score": 4,
          "created_utc": "2025-12-26 22:17:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3xn7w",
          "author": "Fireslide",
          "text": "What's' the point of a potato tier employee?\n\nIt all comes down to economics. It's more efficient to have a potato tier LLM do only the things potato tier LLMs can do, freeing up the higher tiered vegetables to do their thing.\n\nWhat OpenAI is doing with their silent routing is basically trying to be efficient with their limited compute resource by routing queries where appropriate to cheaper models.\n\nThe future is likely to have a bunch of on device LLMs that run small parameter models that help form queries or contact larger models when needed.",
          "score": 3,
          "created_utc": "2025-12-27 00:09:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw37i6k",
          "author": "darkdeepths",
          "text": "quick, private inference / data processing with constant load. you can run these models super fast on the right hardware, and there are jobs that they do quite well. many of the best llm-as-judge models are pretty small.",
          "score": 1,
          "created_utc": "2025-12-26 21:38:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw37l3c",
          "author": "fungnoth",
          "text": "What if we can one day have a tiny model that's actually good at reasoning, comprehension and coherency. But doesn't really remember facts in training data.",
          "score": 1,
          "created_utc": "2025-12-26 21:38:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3ansu",
          "author": "CorpusculantCortex",
          "text": "I have pretty great success even summarizing and performing sentiment analysis of whole news articles into a structured output with a 14b - 30b model locally.",
          "score": 1,
          "created_utc": "2025-12-26 21:55:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3cu4a",
          "author": "revan1611",
          "text": "I use them for web searching on searXNG. Not the best but it gets the job done sometimes",
          "score": 1,
          "created_utc": "2025-12-26 22:07:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3kz3q",
          "author": "noiserr",
          "text": "You don't have to boil the ocean for every task. Small embedding models are also really useful.",
          "score": 1,
          "created_utc": "2025-12-26 22:53:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3kzt6",
          "author": "abnormal_human",
          "text": "They're for much simpler tasks than agentic coding.\n\nThink about things people used to have to train NLP models for like classification, sentiment analysis, etc. Now instead of training a model you can just zero-shot it with a <4B model. Captioning media, generating embeddings. Summarization. Little tasks like \"Generate a title for this conversation\". Request routing.\n\nLarge models can do all of these things too but they are slow and expensive. When you build real products out of this tech, scale matters, and using the smallest model that will work suddenly becomes a lot more important.",
          "score": 1,
          "created_utc": "2025-12-26 22:53:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3mivq",
          "author": "floxtez",
          "text": "I use small models for tagging, titling, summarizing, categorizing, extracting information, performing semi deterministic transformations, etc, etc",
          "score": 1,
          "created_utc": "2025-12-26 23:02:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3npik",
          "author": "no_witty_username",
          "text": "Very small models will probably be used more in the future then the big models. Kind of like most chips today are not frontier level 20k chips like from Nvidia gpu's but chips worth only cents each from TI. Same for LLM's, they will fill in the gaps where large llm's are overkill.",
          "score": 1,
          "created_utc": "2025-12-26 23:09:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3o0i5",
          "author": "TheMcSebi",
          "text": "I'm using ollama with gemma3:27b for many scripted applications in my tech stack. Main use cases are extracting data, summarization and RAG (paired with a decent embedding model). Also sometimes for creative writing, even tho that can get repetitive or boring quickly if not instructed well enough.\nIt did churn out couple of working, simple python scripts, but for those use cases I mainly use the online tools.",
          "score": 1,
          "created_utc": "2025-12-26 23:11:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3or7s",
          "author": "ciavolella",
          "text": "I'm switching through a series of 4b and 8b models trying to find the one I like the most right now, but I'm running my own RocketChat instance, and a bot is monitoring the chat for triggers which it sends out to the ollama API, and can respond directly in the chat.  It also responds to DMs.  But I don't need a heavyweight model to do what I need it to do in my chat.",
          "score": 1,
          "created_utc": "2025-12-26 23:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3w4iy",
          "author": "No-Marionberry-772",
          "text": "Ive been toying around with using small LLMs to habdle context for procedurally generated scenarios.\n\n\nComputing a simulated history is computationally expensive.Â  Trying to simplify the process and fake it without AI has proven to be difficult.\n\n\nI have been able to use the\ncontext understanding of a 3b model to populate json that allows that process to work more reliably.",
          "score": 1,
          "created_utc": "2025-12-27 00:00:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3xjwe",
          "author": "toothpastespiders",
          "text": "I think the 20b to 30b'ish range can be fine for a general jack of all trades model. Especially if they have solid thinking capabilities. At least if they're also fairly reliable with tool calling. They usually have enough general knowledge at that point to intelligently work with RAG data instead of just regurgitating it. I do a lot of work with data extraction and that's my goto size for local. It's also the point where I stop feeling like I'm missing something by not pushing things up to the next tier of size. If I'm using a 12b'ish model I'm almost always going to wish it was 30b. If I'm using a 30b I'm generally fine that it's not 70b. They're small enough that additional training is a pain but still practical.\n\nI'd probably get more use out of the 12b range if I had an extra system around with the specs to run it at reasonable speeds alongside my main server. Until my terminally ill e-waste machine finally died on me I was using it for simple RAG searches over my databases with a ling 14b...I think 2a model that I did additional training on for better tool use and specialized knowledge. Dumb, but enough if all I really needed was a quick question about how I solved x in y situation or where that script I threw together last year to provide z functionality got backed up to. Basically just saving me the trouble of manually working with the databases and sorting through the results by hand. I think a dense rather than MoE 12b'ish model would have been an ideal fit for that job. \n\nAs others have mentioned the 4b'ish range can be really good as a platform to build on with additional training. I think my current favorite example is [mem agent](https://huggingface.co/driaforall/mem-agent). 4b qwen model fine tuned for memory-related tasks. Small enough as a quant for me to run alongside a main LLM while also being fairly fast.",
          "score": 1,
          "created_utc": "2025-12-27 00:08:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3zcs4",
          "author": "Lesser-than",
          "text": "local models will always not scratch your api llm itch, rather than trying to load a model that barely fits your hardware and suffer the t/s and low context limitations, the challenge becomes what can you do with a Models that do fit, its never going to be claude@home your going to have to be a bit more creative on your own like api llms are good at everything a potato tier llm just has to be good a something.",
          "score": 1,
          "created_utc": "2025-12-27 00:19:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw41d3d",
          "author": "woct0rdho",
          "text": "Porn. It does not require that much intellectual complexity and a 30B model can do it pretty well.",
          "score": 1,
          "created_utc": "2025-12-27 00:31:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw41x2i",
          "author": "LowPressureUsername",
          "text": "For consumers, pretty much anything they want.\n\nFor companies: handling millions of requests extremely fucking cheaply. LLMs are overkill for most problems but with some fine tuning their performance is ðŸ”¥.",
          "score": 1,
          "created_utc": "2025-12-27 00:34:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw451vl",
          "author": "GaggedTomato",
          "text": "Realisticly speaken: absolutely nothing.\nFor me, they have been fun experimenting and developing tools around, but they just suck too much atm to be really generating value in some way, although i think models like gpt oss 20b are already borderline useful if used in the right way. But it takes a quite some effort to really get value out of them.",
          "score": 1,
          "created_utc": "2025-12-27 00:53:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw46238",
          "author": "JacobyT",
          "text": "for delightful inference while in airplane mode",
          "score": 1,
          "created_utc": "2025-12-27 00:59:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4cmpf",
          "author": "No_Afternoon_4260",
          "text": "What you want is an agent. Ofc the big question need to be answered by a big boy.\nBut to build the prompt for the big boy you need many steps. You want to build its context.\nFor that you need tools, \"memories\", etc\nA lot of the small steps are perfect fit for small llms or just other smaller technology that also like your rtx",
          "score": 1,
          "created_utc": "2025-12-27 01:41:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4kijs",
              "author": "workware",
              "text": "I'd love an example for this.",
              "score": 1,
              "created_utc": "2025-12-27 02:32:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4ls95",
                  "author": "No_Afternoon_4260",
                  "text": "Tools:  \nRetrieve in a db, read a file, get weather, etc\nFor all these stupid tasks gemma 12b it will do the trick.\n\nYou could also take a look at what is RAG (see you in a couple of months on the ingestion part ;) )\n\nThese are random thoughts but in short an agent needs an ecosystem, there lies all your data and tools, it consumes a lot of \"tokens\" while a lot of it is \"cheap\" in intelligence. The bigger questions represent less \"tokens\" and can be outsourced to bigger models.  \nAnd by tokens I don't mean only llms tokens but unit of mesure for \"gpu type compute\". Because your rag system is based on embeddings, your ocr is a combination of cnn, object detection or vision-llm, you may want STT and TTS and so on..\n\nRoughly at 12b you have a good orchestrator, at 25B you start opening the possibilities and above 100B it starts to get really crazy",
                  "score": 1,
                  "created_utc": "2025-12-27 02:40:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw4jsen",
          "author": "burntoutdev8291",
          "text": "I use small models for quick questions that don't require very large models. I also use them for processing personal documents. Models like deepseek ocr, olmocr, and the smaller qwen variants are very useful.\n\nAs a developer, small models allow me to still do the thinking while dealing with boilerplate. Its more productive for me to use faster and smaller models than a very large reasoning model, cause they are gonna get it wrong anyway.",
          "score": 1,
          "created_utc": "2025-12-27 02:27:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4k3ym",
          "author": "-InformalBanana-",
          "text": "Qwen3 2507 30b a3b instruct works fine for some codding tasks and probably many other things. Devestral 24b also.",
          "score": 1,
          "created_utc": "2025-12-27 02:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4k6yd",
          "author": "SkyFeistyLlama8",
          "text": "You're forgetting NPU inference. Most new laptops have NPUs that can run 1B to 8B models at very low power and decent performance, so that opens up a lot of local AI use cases.\n\nFor example, I'm using Granite 3B and Qwen 4B on NPU for classification and quick code fixes. Devstral 2 Small runs on GPU almost permanently for coding questions. I swap around between Mistral 2 Small, Mistral Nemo and Gemma 27B for writing tasks. All these are running on a laptop without any connectivity required.\n\nYou get around the potato-ness of smaller models by using different models for different tasks.",
          "score": 1,
          "created_utc": "2025-12-27 02:29:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4l6a5",
          "author": "Sl33py_4est",
          "text": "small models are for fine tuning on specific small use cases to cover the performance:compute ratio better or more securely than cloud providers.\n\nvanilla small models?\n\nentertainment.",
          "score": 1,
          "created_utc": "2025-12-27 02:36:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4teby",
          "author": "KeyPossibility2339",
          "text": "Imagine i have a dataset, i need to classify 100k rows. In this case, where a lot of intelligence is not needed local potato llms are the best. In other words, high volume low quality work",
          "score": 1,
          "created_utc": "2025-12-27 03:30:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4w1my",
          "author": "Ok-Bill3318",
          "text": "Small tasks where larger LLMs arenâ€™t required. Like basic rag.\n\nEssentially: regularly try the very small\nLLMs for specific tasks and see how well they work donâ€™t waste resources running a 20b or larger model when 4b will do the job faster and with less resource consumption.\n\nEven llama 3b has worked quite well for some simpler tasks for me.",
          "score": 1,
          "created_utc": "2025-12-27 03:48:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4xhrp",
          "author": "unsolved-problems",
          "text": "Certain set of problems have black or white answers, like some math problems where you can plug in the number x, y, z and see if the solution is right. Here, checking the answer is always fast, and unambiguous. In these cases, you can use arbitrarily \"silly\" heuristics to solve the problem (as long as your overall solution works) because ultimately a wrong answer won't cost you much, as long as you're able to produce a right answer fast enough.\n\nIn my experience, some of the smart tiny models like Qwen3 4B 2507 Thinking are freakishly good in this domain of problems. Yeah, they're dumb as stone overall, but they're incredibly good at solving mid-tier STEM problems some of the time. Just ask it away, and it'll get it right 60% of the time and if not you can check, determine that it's wrong, and re-try. It's very surprising how far you can go with this approach.\n\nOn the one hand, you can type some random STEM textbook question in, as long as you can determine with 100% certainty that what it's telling you is BS, it has a very high chance of providing you with useful information about the problem (unless you're a domain expert, then it's gonna be a waste of time).\n\nOn the other hand, in terms of engineering, you can type some sort of optimization or design problem where you just need numbers to be low enough to do the job, so there is never a risk of AI doing a bad job.\n\nIn this case, since it's a 4B model, this gives us incredible opportunities. This model will be rather small (\\~4GB) and is small enough that it can be utilized by both a CPU and a GPU at reasonable speeds. So, it could be possible to embed this in some offline app, and add it to a feature that finds a solution only some of the time, or otherwise reports \"Sorry! We weren't able to find a solution!\". This can run fine in a decent amount of hardware today, e.g. most desktop computers.",
          "score": 1,
          "created_utc": "2025-12-27 03:57:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4z6u8",
          "author": "Fresh_Finance9065",
          "text": "Specialised LLMs ie\nVision\nClassification\nRAG\n\nNormally, you give it the information and it will do tasks for you, rather than drawing upon its own knowledge.\n\nThey are generally less like to conspire against you or do complex things.",
          "score": 1,
          "created_utc": "2025-12-27 04:09:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5eaqa",
          "author": "coastisthemost",
          "text": "Nice llms with a narrow focus can be outstanding with a small parameter count. Like microsoft phi specialized for science.",
          "score": 1,
          "created_utc": "2025-12-27 06:03:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5gtfn",
          "author": "sirebral",
          "text": "They are often good token prediction models for larger models.  Saving you lots of inference if mated properly with a larger model, many uses, I think they're actually more fun than monolith models, it's engineering over raw power.",
          "score": 1,
          "created_utc": "2025-12-27 06:24:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5j4es",
          "author": "triynizzles1",
          "text": "Small models are really good foundation models that can be fine-tuned by an end user to handle one or two niche tasks very well. Since the AI is small it can run locally, on a CPU, and usersâ€™s computer, etc.",
          "score": 1,
          "created_utc": "2025-12-27 06:45:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5mnrg",
          "author": "cruncherv",
          "text": "I use them for image captioning - descriptions of what is seen in photos, images - text, locations, places, objects, colors, etc",
          "score": 1,
          "created_utc": "2025-12-27 07:17:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5n1dp",
          "author": "YT_Brian",
          "text": "For poor people like me who can't afford a GPU, let alone these days. What, AI usage should only be for those with a thousand up to toss at it?\n\nHell to the fuck no to that. \n\nPlus you are totally ignoring on mobile usage for pretty much every device you take with you except laptops which exists. Which is a genuinely huge market.",
          "score": 1,
          "created_utc": "2025-12-27 07:20:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5xgfh",
          "author": "iamrick_ghosh",
          "text": "No idea on how many firms and companies are using this smaller open source models in their workflow and production too to benefit rather than spending insane amounts on openai or anthropic",
          "score": 1,
          "created_utc": "2025-12-27 09:01:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw61mru",
          "author": "technofox01",
          "text": "I enjoy the privacy of being able to experiment against AI safeguards (e.g. Making malicious code and testing it in VMs) for security research. Other times I enjoy the privacy to discuss mental health or other topics out of curiosity, and not worrying that I am feeding someone else's AI free information.",
          "score": 1,
          "created_utc": "2025-12-27 09:42:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6eh74",
          "author": "Bastion80",
          "text": "My AI driven automatic Kali pentester terminal is running using 4b or 8b models, enough to figure out tools/commands to execute.",
          "score": 1,
          "created_utc": "2025-12-27 11:47:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6ir4p",
          "author": "thespirit3",
          "text": "Your 'potato' LLMs are powering my day to day job, with local documentation queries, local meeting transcript summarisation, log analysis etc. Also, powering my many websites with WordPress content analysis and associated queries from users, automatic server log analysis and resulting email decision/generation, clamAV/Maldet result analysis, etc etc. \n\nAll of the above runs from one local 3060 with VRAM to spare. For coding, I use Gemini - but all of the above would cost a fortune if paying per token.",
          "score": 1,
          "created_utc": "2025-12-27 12:24:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8iqzy",
              "author": "ansibleloop",
              "text": "Which models?",
              "score": 1,
              "created_utc": "2025-12-27 19:10:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwlgmt6",
                  "author": "thespirit3",
                  "text": "Sorry for the slow reply. I've settled on Qwen4:14b for most purposes.",
                  "score": 1,
                  "created_utc": "2025-12-29 19:18:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6nw78",
          "author": "vagmi",
          "text": "They are also remarkably good at summarizing code. While they cannot be used for coding, they can be used for code understanding and exploration.",
          "score": 1,
          "created_utc": "2025-12-27 13:05:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6y986",
          "author": "xmBQWugdxjaA",
          "text": "They are good for router LLMs and classification - stuff where in the past you would have trained your own BERT model for example. Now it's far easier and more versatile than dealing with that.",
          "score": 1,
          "created_utc": "2025-12-27 14:14:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw70jem",
          "author": "Danny_Davitoe",
          "text": "The company I work for, by law, can't hold or send data outside of the company. The workaround is having local LLMs as our solution.",
          "score": 1,
          "created_utc": "2025-12-27 14:28:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw722uq",
          "author": "insulaTropicalis",
          "text": "This reminds me of that classical meme \"coders back then vs coders today.\"\n\nIt was just two-and-a-half years ago that we did all kind of stuff with llama-2 at 7 and 13B params. Today we have 4B thinking models which rival the 68B original llama and all kind of agentic frameworks and shit. And newbies complaining \"bUt WhAt UsE aRe, MoDeLs UnDeR 100B pArAmS?\"\n\nSeveral core llama.cpp devs developed and tested stuff on 8 GB RAM. Imagine that.",
          "score": 1,
          "created_utc": "2025-12-27 14:37:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw772h1",
          "author": "_ralph_",
          "text": "I use 7b mistral models to translate texts into german. TLDR and creation of data triples also work great.",
          "score": 1,
          "created_utc": "2025-12-27 15:07:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7e4x6",
          "author": "RedditPolluter",
          "text": "Mostly hardware limitation. When it comes to smaller models that try to be general and all-rounded, I see your point but a lot of LLM capacities are jagged and sufficiently specialized smaller models aren't inherently worse at their specialty than larger general-purpose models and in some cases even outperform. And specialty doesn't have to mean a whole Q&A topic area of focus but could be a very specific task with a little more flexibility and open-endness than a purely coded solution could provide. Smaller models that are more general are probably easier to fine-tune in a specific direction so that capabilities aren't built entirely from the ground up.\n\nAlso, gpt-oss-20B is useful for basic scripts and javascript macros without using 10k or more thought tokens to generate them. I'm glad it doesn't try to be general purpose as it would just average down the performance in those areas.",
          "score": 1,
          "created_utc": "2025-12-27 15:45:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7j2c0",
          "author": "GP_103",
          "text": "I have a pdf in Italian that I want to translate into English. Thinking it would be a good local model  â€œstarterâ€ project.\n\nAnyone have suggestions on best models for that task?",
          "score": 1,
          "created_utc": "2025-12-27 16:10:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7u0em",
          "author": "rc_ym",
          "text": "To run on potatoes, obviously.  Driving to work in a Ferrari is expensive and stupid.",
          "score": 1,
          "created_utc": "2025-12-27 17:05:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw881kb",
          "author": "jax_cooper",
          "text": "Legends say that if you fine-tune a 7B model, it will outperform 20-30B models in that task while staying lightweight.",
          "score": 1,
          "created_utc": "2025-12-27 18:16:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8bc0e",
          "author": "D_C_Flux",
          "text": "I use a maximum 9b model for translations of Japanese light novels that are usually already in English into Spanish.\nI'm always testing new, smaller models for this task because the main problem is that when the context is very long, they get lost in the task.",
          "score": 1,
          "created_utc": "2025-12-27 18:33:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8dpqm",
              "author": "GP_103",
              "text": "Cool. Iâ€™m looking to translate recipes, so  may face similar â€œlong contextâ€ issues.",
              "score": 1,
              "created_utc": "2025-12-27 18:45:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdrva0",
                  "author": "D_C_Flux",
                  "text": "Regarding how long it takes for them to get lost, it depends on the model. Incredibly, the best model I found that is consistent and allows for a relatively long context is the gemma3n E4B. Make sure it's that exact model; they're not the same as others.\nAnd if you download the Google app on your phone, it supports image and audio input at a usable speed on a decent phoneâ€”7 tokens per second generation on a Poco X7 Pro.",
                  "score": 1,
                  "created_utc": "2025-12-28 16:14:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8dgop",
          "author": "ghosthacked",
          "text": "Main reason I can think of is privacy, plus being able to ensure you have control over the models eh, 'motives'. Everything that happened to social media is going to happen with commercial public llm products.",
          "score": 1,
          "created_utc": "2025-12-27 18:43:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw91se7",
          "author": "cosmos_hu",
          "text": "I use 7b and 14b models to roleplay, they are quite capable for that.",
          "score": 1,
          "created_utc": "2025-12-27 20:52:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwa4cp4",
          "author": "xoxaxo",
          "text": "While I don't trust/use small models for knowledge type task(text to text), I use for media stuff like text to speech(TTS), image to 3D model, text to image, image/text to video, OCR, watermark/background remover and etc",
          "score": 1,
          "created_utc": "2025-12-28 00:25:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwakn0l",
          "author": "Kahvana",
          "text": "Up to 32B models can run on costumer hardware and offer great privacy benefits because of that.\n\nMistral Small 3.2 24B finetunes are great for roleplay and fit inside a 5060 Ti + 16K Q8\\_0 context with some tweaking.\n\nHaven't used devstrall 2 enough yet, but so far it has been neat as a sparring partner for ideas.\n\nBut yeah, you won't be using it for \"serious\" work. They are well-suited for simpler tasks like text classification, a web searcher (see Jan Edge), as a conversational partner and whatnot.",
          "score": 1,
          "created_utc": "2025-12-28 02:00:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi6ush",
              "author": "IORelay",
              "text": "Does mistral small 24B always have a vision part within its parameters? How do you load it without the vision part?",
              "score": 1,
              "created_utc": "2025-12-29 06:42:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwlfdci",
                  "author": "Kahvana",
                  "text": "You can simply not load loading the vision tower (mmproj file)",
                  "score": 1,
                  "created_utc": "2025-12-29 19:11:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwaw545",
          "author": "Budget_Statement92",
          "text": "You do it to learn the process",
          "score": 1,
          "created_utc": "2025-12-28 03:09:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbbkr3",
          "author": "value1338",
          "text": "I use small LLMs for boring, narrow stuff. OCR/vision on scans (even handwritten), then another one just names and tags the files in paperless-gpt.\nThey run locally, touch private docs, and donâ€™t need â€œreasoningâ€. Fast, cheap, private.\nBig models are overkill for this. Right tool for the job.",
          "score": 1,
          "created_utc": "2025-12-28 04:47:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbwxaa",
          "author": "Gold_Ad_2201",
          "text": "qwen3 4b is great when you need to call some mcp based on the prompt - like make API call, calculate something or make db query. if realtime is not priority and better accuracy needed then thinking model variant. I am honestly surprised how this size of the model handles things. for a more complex agentic use qwen3 30b a3b. if it loads into memory then speed is incredible",
          "score": 1,
          "created_utc": "2025-12-28 07:44:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc4qrp",
          "author": "Tall-Ad-7742",
          "text": "Well I think there probably will be a local model (probably about 100-300B) which is actually good like sonnet 4.5 which is not just good in some benchmarks but atleast for now we have to wait",
          "score": 1,
          "created_utc": "2025-12-28 08:59:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf18go",
          "author": "Wrong-Dimension-5030",
          "text": "Small models are much faster and cheaper to run.  Most tasks do not require a trillion parametersâ€¦",
          "score": 1,
          "created_utc": "2025-12-28 19:53:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfuif0",
          "author": "[deleted]",
          "text": "Finetuning to get 99.9% accuracy on a specialized task, for example, citing snippets in 100 pragraphs that contain an answer to a question.\n\nGetting a very precise phoneme system when processing speech.",
          "score": 1,
          "created_utc": "2025-12-28 22:16:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg1zud",
          "author": "ThrowRAlngdstn",
          "text": "30B not too bad for helping with writing and other basic logic, the other models are assigned to passing butter, turning on lights, checking logs etc..Â ",
          "score": 1,
          "created_utc": "2025-12-28 22:55:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjxpk1",
          "author": "-dysangel-",
          "text": "They're good for adding intelligence to tasks. For example easily/quickly performing some task that would otherwise take weeks to code up - for example writing a parser to convert from one programming language to another. Or something that would either be impossible or take years to do via code, such as converting from one human language to another.\n\nThe main thing I've use smaller models forÂ so far is for compressing inputs, summarising outputs, and regularly pruning a vector database for a larger model.",
          "score": 1,
          "created_utc": "2025-12-29 14:57:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkkejf",
          "author": "Equivalent_Mine_1827",
          "text": "I'm currently building a cli tool able to use a small language model as part of a bigger system.\nI'm trying to do this, to be completely independent from LLMs, as their paid subscription probably is cheap today but maybe it will be a nightmare tomorrow.\n\nThe cons of doing this, the system will be constrained.\nThe pros, a constrained environment really leaves almost no room to mess things up.\n\nAn LLM, can make great things by itself, but it will inevitably be unable to scale big by itself.",
          "score": 1,
          "created_utc": "2025-12-29 16:48:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm9p38",
          "author": "Fast_Thing_7949",
          "text": "I just uploaded the exact same large 2,000-line patch to ChatGPT 5.2 and Qwen3 Coder 30B, Nemotron Nano, and ChatGPT OSS 20B.\n\nOnly ChatGPT 5.2 found the important issues, while the free models hallucinated, pointed out â€œerrorsâ€ that werenâ€™t there, and failed to spot the most critical parts.\n\nAfter that, Iâ€™m definitely not going to buy anything at all for AI or running it at home.",
          "score": 1,
          "created_utc": "2025-12-29 21:39:23",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nw37as9",
          "author": "ai_hedge_fund",
          "text": "Upvoting to support your talented art career\n\nMicro models are also useful during app testing (is this thing on?)",
          "score": 1,
          "created_utc": "2025-12-26 21:37:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3egox",
          "author": "Kaitsuburi1",
          "text": "Quite controversial, perhaps is just intentional by whoever created them to push users towards cloud/service-based models. Others already stated some technical aspects, but think of one question: Why there is no Qwen 3 coder 30B, but only with English and Python support? Or Devstral but only with knowledge of JS, HTML and basic computer science?\n\nThey have no incentive to release models which are not banana locally, despite being able to do easily.",
          "score": 2,
          "created_utc": "2025-12-26 22:16:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4s6kb",
          "author": "XiRw",
          "text": "We get it , youâ€™re rich. They are still useful to use. Especially 20 and 30s. I never seen anyone call them bad until you right now. If you want to have that mindset, I want to ask you why and whatâ€™s the purpose? The best of the best LLMs canâ€™t compete with flagship server models so if thatâ€™s your cup of tea go enjoy using them then.",
          "score": 1,
          "created_utc": "2025-12-27 03:22:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw57bde",
          "author": "NekoRobbie",
          "text": "I've used 12B models before as well as currently running a 24B model. I don't care about coding capabilities whatsoever because I can write code myself, and I far and wide prefer to code myself (especially given how I like to make my coding for the purpose of FOSS projects, where there are some very good concerns about AI code generation and licensing).\n\n12B was a nice stopgap for getting decent roleplaying going on my old GPU, especially once I started getting into refining my choice of models. It let me get up to 24K context and satisfying enough roleplaying capabilities in just 12GB of VRAM. 24B has been a step above and beyond 12B in every way (as it logically should be), although it did mean that I had to reduce the context a little (Currently running it at 16K context, although I was reasonably able to run it at 20k context earlier. These context numbers are with a quantized model (q4 variants) and quantizing my context to q8). By doing it locally, I avoid all the censorship and privacy concerns inherent to so many of the providers online and I'm not losing any money on it either since I'm just running the same GPU I'd use for gaming.\n\nI use KoboldCPP to run the models, and SillyTavern as my frontend. I find they work very well together, and that I get plenty of satisfaction out of using them for roleplaying.\n\nLower than 12B and things do start getting a bit dicey when it comes to a lot of applications, although I'm sure finetunes can make them experts at niches (like how iirc some of the modern image/video gen ends up utilizing small models for the text processing)",
          "score": 1,
          "created_utc": "2025-12-27 05:08:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw35cwv",
          "author": "Feeling-Creme-8866",
          "text": "ðŸ˜‚",
          "score": 0,
          "created_utc": "2025-12-26 21:26:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pu7pfi",
      "title": "Thoughts on DGX Spark as a macOS Companion: Two Months Later",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1pu7pfi",
      "author": "PropellerheadViJ",
      "created_utc": "2025-12-23 22:58:04",
      "score": 147,
      "num_comments": 52,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pu7pfi/thoughts_on_dgx_spark_as_a_macos_companion_two/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nvnh1dm",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-24 02:25:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvnibnh",
          "author": "SkyFeistyLlama8",
          "text": "Dependency hell is real once you stray outside x86 land. I had a similarly hellish time trying to get Python ARM64 modules on Windows for machine learning. Qualcomm funnily recommends using Python x64 in Windows in emulation, for all NPU-related projects and even that's for older Python versions.\n\nAs for Blackwell not having framework support, what's going on here? I would've thought Nvidia could have contributed Blackwell code to those projects. The DGX Spark being a giant expensive Orin board makes my skin crawl.",
          "score": 16,
          "created_utc": "2025-12-24 02:33:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrjdrr",
              "author": "Specific-Goose4285",
              "text": "\\> Dependency hell is real once you stray outside x86 land. I had a similarly hellish time trying to get Python ARM64 modules on \\***Windows**\\* for machine learning. Qualcomm funnily recommends using Python x64 in \\***Windows**\\* in emulation, for all NPU-related projects and even that's for older Python versions.\n\n\n\nAs someone who uses aarch64 a lot on Linux and MacOS I've highlighted where is the dependency hell. Its not the instruction set.",
              "score": 3,
              "created_utc": "2025-12-24 19:34:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvrjbm9",
              "author": "Potential-Net-9375",
              "text": "My god same, the compatibility issues held back ML research for so long, it was truly a nightmare to work with",
              "score": 1,
              "created_utc": "2025-12-24 19:33:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvna394",
          "author": "thehpcdude",
          "text": "You could rent access to a system with CUDA for a fraction of the cost and the price for unit of work is far less.\n\nThe Spark is a development platform for those that absolutely cannot access cloud systems for whatever reason.",
          "score": 14,
          "created_utc": "2025-12-24 01:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvnpwbl",
              "author": "txgsync",
              "text": "A comparable-RAM RTX Pro 6000 rental on RunPod clocks in at around $10,000 per year. If you only used it 8 hours a day, sure, itâ€™s around $2500. And youâ€™d get much better inference and prefill speeds. But that also ignores storage costs when the node isnâ€™t active. \n\nI can take a Spark with me in a bag to a hotel when I travel and it wonâ€™t pop a breaker. And I wonâ€™t care about LAN performance of the hotel WiFi. \n\nAlso many Cloud providers of commercial models heavily safety-train and run disruptive injections against their inputs and outputs. Ever try to red-team a web site using a cloud LLM? Most of the US ones refuse, and if you use the Chinese ones youâ€™re training them how to attack your target. \n\nPrivacy is a major driver of course. Of all the cloud vendors, only Mistral has a reasonable privacy policy. And even they wonâ€™t shield you from government-authorized spying on its own citizensâ€¦ or those of â€œforeignâ€ governments (a complicated description in the EU). \n\nAll that is to say there are many, many reasons to run your own models locally in addition to â€œthose that absolutely cannot access cloud systemsâ€.   Thatâ€™s why I use an M4 Max 128GB RAM Mac, too, and am considering either a DGX Spark or expanding my Linux PC to include an RTX Pro 6000. \n\nAnd I use cloud inference and hosting my own models remotely too. Right tool for the right reasons.",
              "score": 9,
              "created_utc": "2025-12-24 03:20:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvnuu4s",
                  "author": "thehpcdude",
                  "text": "You can rent the node and install whatever models you want. Â The per-unit of work is far lower. Â You can also access cloud instances at your hotel, or away from your computer using your phone. Â \n\nHourly rates on nodes get cheaper over time. Â In two years you could be renting a B200 for the same price as H100s today, but with the Spark youâ€™re still using a Spark. Â \n\nMost cloud providers have cold tiers. Â \n\nIâ€™m not a cloud fanboy at all. Â I actively move customers away from clouds to dedicated on-premises HPC and AI training clusters. Â That being said, thereâ€™s no way you can model out personal or small business use cases where a rented instance doesnâ€™t win.Â \n\nPer unit of work, a dedicated instance is orders of magnitude faster than a Spark and far cheaper to get the same amount of work done. Â ",
                  "score": 5,
                  "created_utc": "2025-12-24 03:52:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvpckp6",
                  "author": "serendipity777321",
                  "text": "Why u don't compare it with 4090 or 5090?",
                  "score": 1,
                  "created_utc": "2025-12-24 11:46:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvrjuxy",
              "author": "Specific-Goose4285",
              "text": "Until price hikes, TOS changes and then you can't anymore.",
              "score": 1,
              "created_utc": "2025-12-24 19:36:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvrl766",
                  "author": "thehpcdude",
                  "text": "There are literally over 100 T2 CSPâ€™s that would love to have your business. Â Thereâ€™s no problem getting single nodes, fractional nodes, etc. Â \n\nThe economy of purchasing your own hardware, regardless of your use case, at individual or small businesses scale does not make sense. Â \n\nThe cost of use per unit of work is far lower. Â ",
                  "score": 1,
                  "created_utc": "2025-12-24 19:44:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvmkncs",
          "author": "egomarker",
          "text": "Nice writeup.",
          "score": 16,
          "created_utc": "2025-12-23 23:06:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmzams",
          "author": "typeryu",
          "text": "I think the product was made for this type of workflow and more, but along the product release track, marketing or comms got hold of it and said â€œthis is a supercomputer in the palm of your handsâ€ which critically affected consumer expectations. Itâ€™s expensive for sure, but given its size, it would have also been perfect for mounting on mobile platforms (not mobile in the phone sense, but for something moveable like robotics), but they made the case look like which clearly is a nod to its more giant counterpart. It still retains value as a desktop pair compute device, but they clearly underestimated/overestimated what it can do.",
          "score": 3,
          "created_utc": "2025-12-24 00:34:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvoy3zj",
              "author": "PropellerheadViJ",
              "text": "â€œItâ€™s a supercomputer that fits in the palm of your handâ€, I agree, Huang himself basically went on stage and said something along those lines. For some reason, expectations were inflated, and people started to think it would be a cheap solution to all their problems (compared to RTX Pro 6000 and things like H100)",
              "score": 1,
              "created_utc": "2025-12-24 09:27:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvn9wbq",
          "author": "_hephaestus",
          "text": "I mean if you like Mac OS that seems fine but from a local llama perspective the Mac side of this seems immaterial. Figured this was more in the vein of https://blog.exolabs.net/nvidia-dgx-spark/, Iâ€™ve been considering getting a Spark to supplement my Mac similarly, but Iâ€™ve been doing the inverse and having my 512GB studio as the compute node to primarily linux clients.",
          "score": 5,
          "created_utc": "2025-12-24 01:40:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmqkse",
          "author": "Uninterested_Viewer",
          "text": "I have a similar philosophy: my M4 Mac Mini is my main desktop that I code on and do other project work, but I have a bit bigger companion in an RTX 6000 pro that sits in an otherwise mundane computer in my rack. I have my eye on an M5 ultra studio next year to potentially combine pure inference tasks with my main desktop, leaving the 6000 to training and the occasional image/video generations.",
          "score": 7,
          "created_utc": "2025-12-23 23:42:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmrjnd",
              "author": "PropellerheadViJ",
              "text": "Thatâ€™s an awesome setup. What you described is pretty much an ideal no-compromise setup for inference.",
              "score": 3,
              "created_utc": "2025-12-23 23:48:01",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvnvzm2",
              "author": "CyberBlaed",
              "text": "Ditto.\n\nM1 stuff for my workflow and my gamig rig in another room for gaming and ai models :)\n(Sunshine and moonlight, beautiful remote setups!)\n\nItâ€™s a sweet setup to keep things containerised ;) and frankly, I am not at all bothered by the network latency. Itâ€™s done when itâ€™s done :)\n\nI agree with OP that the mac market for tools and features is dearly lacking though, I mean, even to the average consumer ollama will soon have the MLX support for apple chips, which lmstudio adopted in 2024. (Granted community coders and all, respect to them) but its where if Apple despite the hardware being awesome, sucks when the software doesnâ€™t keep pace with it.\n\nIt is cool the thunderbolt daisy-chain though! :)",
              "score": 1,
              "created_utc": "2025-12-24 04:00:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvn1jgi",
          "author": "Whole-Assignment6240",
          "text": "How does power consumption compare between the Spark and a full workstation?",
          "score": 2,
          "created_utc": "2025-12-24 00:48:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvna0b1",
              "author": "abnormal_human",
              "text": " Very different, just like the capabilities.",
              "score": 2,
              "created_utc": "2025-12-24 01:41:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvnmsfr",
          "author": "ResearchCrafty1804",
          "text": "Have you consider publishing the Docker images with the models you prepared for DGX Spark?\n\nI donâ€™t have a DGX Spark myself yet, but I am considering to get one and it would be nice to have some resources available.",
          "score": 2,
          "created_utc": "2025-12-24 03:00:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp1y4b",
              "author": "PropellerheadViJ",
              "text": "Yeah, definitely. Everything Iâ€™m getting from open source right now I try to give back to open source as well. The Spark community is still pretty small, so people tend to help each other figure things out. You can already find some of us hanging out in NVIDIA forum threads and on GitHub discussions.",
              "score": 1,
              "created_utc": "2025-12-24 10:05:43",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvp23am",
              "author": "PropellerheadViJ",
              "text": "On top of that, spark has a pretty solid cookbook/documentation that helps you get started. There are lots of examples straight from them, ranging from ComfyUI setups to things like sglang.",
              "score": 1,
              "created_utc": "2025-12-24 10:07:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvodnuf",
          "author": "bigh-aus",
          "text": "I agree with your points.  \n\nThe framework I run is very much there are 3 main use cases of computers:\n\n1. Primary compute / interface (this is my desktop and laptop) - I want both to be as fast as humanly possible for the things I do.\n\nOptional: If your stack is different to your primary compute have have a target compute for the stack (your example of the spark). \n\n2. Batch jobs / long running processes - where you are able to let it run, maybe queue things up (spark could be good with this with AI / Generation).\n\n3. Fast feedback but separate computer eg LLM inference.\n\nThen any other things you need to do can be ran on 2 or 3 - eg CI could be run on either - depending on how important fast feedback is.\n\nSomeone at a meetup said to me always buy the best computer you can afford so that you get the fastest feedback.  Great advice.  The problem with AI workloads is the cost of compute is insane if you want to level up feedback.",
          "score": 2,
          "created_utc": "2025-12-24 06:14:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpiepo",
          "author": "Any_Row_5742",
          "text": "You know, external connection of RTX Cards via USB4/TB connection become possible : [https://www.reddit.com/r/nvidia/comments/1oio9ma/tinycorp\\_shocks\\_the\\_tech\\_world\\_as\\_apple\\_macbooks/](https://www.reddit.com/r/nvidia/comments/1oio9ma/tinycorp_shocks_the_tech_world_as_apple_macbooks/) \n\nTinyCorp enabled external GPU (eGPU) support on Apple Silicon Macs by creating custom driversÂ .",
          "score": 2,
          "created_utc": "2025-12-24 12:35:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxxjxz",
          "author": "Amazing_Rutabaga8336",
          "text": "The M4 Ultra would likely have more than 819GB/s because the M4 MAX has 546GB/s. The M3 Ultra has 819.",
          "score": 2,
          "created_utc": "2025-12-25 23:34:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmvalx",
          "author": "SoupSuey",
          "text": "Thatâ€™s more or less my setup as well. Mac as workstation and a server with graphics cards and beefier compute capability as computer node. Can access it from anywhere using Tailscale and it frees my Mac to be a multitasking tool.\n\nHave fun!",
          "score": 2,
          "created_utc": "2025-12-24 00:10:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp2btg",
              "author": "PropellerheadViJ",
              "text": "Oh, thanks! Iâ€™ve set up OpenVPN for now, but it turns out it only allows up to two connections, so Iâ€™ll probably switch to something else later.",
              "score": 2,
              "created_utc": "2025-12-24 10:09:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvpa7ys",
                  "author": "SoupSuey",
                  "text": "Man, Tailscale is awesome!! Iâ€™ve set up site-to-site VPNs between my office, my house and my parentâ€™s house without opening a single TCP port, and also of course I use it to access single devices on my Tailnet. \n\nIf you ever need to look beyond OpenVPN, give it a try. The r/Tailscale community here on Reddit is pretty active.",
                  "score": 2,
                  "created_utc": "2025-12-24 11:25:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvmnz0c",
          "author": "Historical-Internal3",
          "text": "Did you use the Nunchaku variant for Qwen? I believe it is NVFP4.",
          "score": 1,
          "created_utc": "2025-12-23 23:26:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmr1gy",
              "author": "PropellerheadViJ",
              "text": "Havenâ€™t tried it yet and hadnâ€™t heard about Nunchaku before. I thought that for DGX Spark I would have to do the quantization myself into NVFP4 using TensorRT Model Optimizer on Blackwell. Thanks for the pointer. For the Qwen benchmarks, I used the default model from the ComfyUI templates.",
              "score": 1,
              "created_utc": "2025-12-23 23:45:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvmrxis",
                  "author": "Historical-Internal3",
                  "text": "For LLMs - NVFP4 models are out there. Itâ€™s a matter of whether or not llama.cpp, vLLM, SGlang, etc will support them (they will officially soon). \n\nFor generative art models that are more compute intense - Comfy does support NVFP4 (there are some custom nodes out there) and there are people like Nunchaku doing this kind of work already.\n\nYour table will drastically change with NVFP4 (something the 40 series and older does/will not take advantage of).\n\nThis device will start to shine soon enough for use cases like this and to me personally, already does. Even on the inference side with LLMs. \n\nUsers just need to understand what dense models are, and to avoid them on something like this. Stick to MoE models. Which are all the rage anyways.\n\nI get 60 tokens/second with OSS GPT 120b. More than good enough for my use case.",
                  "score": 3,
                  "created_utc": "2025-12-23 23:50:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvo315f",
          "author": "BananaPeaches3",
          "text": ">Because of this, an entire layer of critical libraries like nvdiffrast, flash-attention, and other CUDA-dependent solutions is unavailable on Mac\n\n\nNot as true anymore, Tinygrad has Nvidia 30/40/50 series running on macOS over USB4",
          "score": 1,
          "created_utc": "2025-12-24 04:50:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvowzhq",
              "author": "nucLeaRStarcraft",
              "text": "any working example of this ?",
              "score": 1,
              "created_utc": "2025-12-24 09:16:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvrl8by",
                  "author": "Specific-Goose4285",
                  "text": "It's on their site. But its only for their library though.",
                  "score": 1,
                  "created_utc": "2025-12-24 19:44:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvoy8hn",
              "author": "PropellerheadViJ",
              "text": "Iâ€™ve heard about it, but Iâ€™m not sure thereâ€™s enough software support there yet to actually run everything end to end. It would be great if it really works though. The more competition and viable options we have, the better for us :)",
              "score": 1,
              "created_utc": "2025-12-24 09:28:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvocyz2",
          "author": "IrisColt",
          "text": "Thanks for the insights!",
          "score": 1,
          "created_utc": "2025-12-24 06:08:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvnpnt2",
          "author": "seppe0815",
          "text": "M4 ultra classic a.i generated postÂ ",
          "score": 1,
          "created_utc": "2025-12-24 03:19:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmw4ji",
          "author": "StardockEngineer",
          "text": "I do the same thing. My Mac is just my head.  Connecting to my four headless Linux machines.  Easy to develop remotely with just SSH with VSCode/Cursorâ€™s native Remote SSH integrations and SSH SOCKS5 proxy.",
          "score": 1,
          "created_utc": "2025-12-24 00:15:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvnxjd1",
          "author": "cgs019283",
          "text": "How did you get hot speed of z-image on dgx? Mine usually takes 11 seconds with 1024x1024 9 step gen.",
          "score": 1,
          "created_utc": "2025-12-24 04:11:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvoyo92",
              "author": "PropellerheadViJ",
              "text": "I double checked it, and it turned out that when I tested it the ComfyUI template had only 4 steps by default, not 9",
              "score": 2,
              "created_utc": "2025-12-24 09:33:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}