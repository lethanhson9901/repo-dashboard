{
  "metadata": {
    "last_updated": "2026-02-10 09:19:59",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 16,
    "total_comments": 499,
    "file_size_bytes": 626729
  },
  "items": [
    {
      "id": "1qxcm5g",
      "title": "No NVIDIA? No Problem. My 2018 \"Potato\" 8th Gen i3 hits 10 TPS on 16B MoE.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qxcm5g",
      "author": "RelativeOperation483",
      "created_utc": "2026-02-06 08:56:17",
      "score": 1033,
      "num_comments": 118,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial | Guide",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qxcm5g/no_nvidia_no_problem_my_2018_potato_8th_gen_i3/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3x3z56",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-06 15:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3viylc",
          "author": "koibKop4",
          "text": "Just logged into reddit to upvote this true localllama post! ",
          "score": 195,
          "created_utc": "2026-02-06 09:44:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vjzcn",
          "author": "Top_Fisherman9619",
          "text": "Posts like this are why I browse this sub. Cool stuff!",
          "score": 154,
          "created_utc": "2026-02-06 09:54:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vv2im",
              "author": "artisticMink",
              "text": "But aren't you interested in my buzzwords buzzwords buzzwords agent i vibe coded and now provide for F R E E ? ",
              "score": 47,
              "created_utc": "2026-02-06 11:32:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3wamzw",
                  "author": "behohippy",
                  "text": "If you add this sub to your RSS reader, which gets a raw feed of everything posted, you'll see how bad it actually is. There's some superheros that are downvoting most of them before they even hit the front page of the sub.",
                  "score": 19,
                  "created_utc": "2026-02-06 13:18:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3y85q9",
                  "author": "Downtown-Community26",
                  "text": "lol",
                  "score": 1,
                  "created_utc": "2026-02-06 18:59:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3whjjh",
              "author": "Terrible-Detail-1364",
              "text": "yeah its very refreshing vs what model should I‚Ä¶",
              "score": 8,
              "created_utc": "2026-02-06 13:56:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vks2l",
          "author": "justserg",
          "text": "honestly love seeing these posts. feels like the gpu shortage era taught us all to optimize way better. whats your daily driver model for actual coding tasks?",
          "score": 74,
          "created_utc": "2026-02-06 10:01:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vlmk5",
              "author": "RelativeOperation483",
              "text": "Not 100% sure yet‚ÄîI'm still hunting for that perfect 'smart and fast' model to really squeeze my laptop. It‚Äôs not just the model, the engine matters just as much. For now, that **DeepSeek-Lite** running on **OpenVINO** backend is the peak daily driver.",
              "score": 24,
              "created_utc": "2026-02-06 10:09:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3vmg0d",
                  "author": "Silver-Champion-4846",
                  "text": "any tutorials for us noobs?",
                  "score": 3,
                  "created_utc": "2026-02-06 10:16:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3wxkkx",
                  "author": "MythOfDarkness",
                  "text": "Are you seriously using AI to write comments??????",
                  "score": 3,
                  "created_utc": "2026-02-06 15:19:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3vq667",
          "author": "ruibranco",
          "text": "The dual-channel RAM point can't be overstated. Memory bandwidth is the actual bottleneck for CPU inference, not compute, and going from single to dual-channel literally doubles your throughput ceiling. People overlook this constantly and blame the CPU when their 32GB single stick setup crawls. The MoE architecture choice is smart too since you're only hitting 2.4B active parameters per token, which keeps the working set small enough to stay in cache on that i3. The Chinese token drift on the iGPU is interesting, I wonder if that's a precision issue with OpenVINO's INT8/FP16 path on UHD 620 since those older iGPUs have limited compute precision. Great writeup and respect for sharing this from Burma, this is exactly the kind of accessibility content this sub needs more of.",
          "score": 31,
          "created_utc": "2026-02-06 10:50:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vritr",
              "author": "RelativeOperation483",
              "text": "I'm running GGUF because it's hard to find OpenVINO files these days, and it's nearly impossible to convert them myself with my limited RAM. I‚Äôm using the Q4\\_K\\_M quantization. I did notice some Chinese tokens appeared about five times across 20 questions , not a lot, just a little bit each time\n\nhttps://preview.redd.it/2s95fisqvuhg1.png?width=878&format=png&auto=webp&s=e5c5d7edc72019e3598e650fabe5022bceede333\n\n",
              "score": 9,
              "created_utc": "2026-02-06 11:02:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zjixq",
                  "author": "JustSayin_thatuknow",
                  "text": "That chinese/gibberish tokens I had them because of flash attention being enabled.. with fa turned off it didn‚Äôt happen with me, but as I‚Äôm stubborn af and wanted to use fa, I finally found out (after a week of thousands of trial and errors) that if I use a model with the flag ‚Äú-c 0‚Äù (which makes lcpp uses the context length from the n_ctx_training (the declared context length for which the model has been trained on)) it outputs everything perfectly well! But for this you need to make sure model is small enough, else lcpp will use the ‚Äúfit‚Äù feature to decrease context length to the default 4096 (which again returns to the gibberish/chinese/non-stop-always-on-a-loop inference state).",
                  "score": 2,
                  "created_utc": "2026-02-06 22:57:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4f5ndo",
                  "author": "Echo9Zulu-",
                  "text": "Nice post! Glad to see some benchmarks on that PR.  I have a ton of openvino models on my HF :). Would be happy to take some requests if you need something quanted.\n\nhttps://huggingface.co/Echo9Zulu",
                  "score": 1,
                  "created_utc": "2026-02-09 12:11:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3w44w3",
          "author": "iamapizza",
          "text": "I genuinely find this more impressive then many other posts here. Running LLMs should be a commodity activity and not exclusive to a few select type of machines. It's a double bonus you did this on Linux which means a big win for privacy and control.",
          "score": 38,
          "created_utc": "2026-02-06 12:38:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w6sys",
              "author": "RelativeOperation483",
              "text": "Thank Man.",
              "score": 6,
              "created_utc": "2026-02-06 12:55:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3viy7t",
          "author": "pmttyji",
          "text": "Try similar size Ling models [which gave me good t/s](https://www.reddit.com/r/LocalLLaMA/comments/1qp7so2/bailingmoe_ling17b_models_speed_is_better_now/) even for CPU only.",
          "score": 16,
          "created_utc": "2026-02-06 09:44:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wocj2",
              "author": "rainbyte",
              "text": "Ling-mini-2.0 üòé",
              "score": 2,
              "created_utc": "2026-02-06 14:32:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vw5e7",
          "author": "j0j0n4th4n",
          "text": "You probably can run gpt-oss-20b as well.\n\nI got about the same speeds in my setup here using the IQ4_XS quant of the bartowski's DeepSeek-Coder-V2-Lite-Instruct (haven't tried other quants yet) than I did gpt-oss-20b-Derestricted-MXFP4_MOE.",
          "score": 8,
          "created_utc": "2026-02-06 11:40:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vy6qb",
              "author": "RelativeOperation483",
              "text": "I will try it, big thank for suggestion.",
              "score": 2,
              "created_utc": "2026-02-06 11:56:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3wlut0",
                  "author": "emaiksiaime",
                  "text": "I second this. I always fall back to gpt-oss-20b after trying out models, and I was able to run qwen3next 80b a3b coder on my setup. I have a i7-8700 with 64gb of ram and a ...tesla p4... it runs at 10-12 t/s prompt processing is slow.. but the 20b is great, still.",
                  "score": 2,
                  "created_utc": "2026-02-06 14:19:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wrq5d",
          "author": "Alarming_Bluebird648",
          "text": "actually wild that you're getting 10 tps on an i3. fr i love seeing people optimize older infrastructure instead of just throwing 4090s at every problem.",
          "score": 8,
          "created_utc": "2026-02-06 14:49:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wrxfw",
              "author": "RelativeOperation483",
              "text": "Thank",
              "score": 1,
              "created_utc": "2026-02-06 14:50:57",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4eexzf",
              "author": "Idea_Guyz",
              "text": "I‚Äôve had my 4090 for three years and the most I‚Äôve thrown at it is 20 chrome tabs to repose an articles and videos that I‚Äôll never watch read",
              "score": 1,
              "created_utc": "2026-02-09 08:00:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wzn0g",
          "author": "stutteringp0et",
          "text": "I'm getting surprising results out of GPT-OSS:120b using a Ryzen 5 with 128GB ram.\n\n72.54 t/s\n\nI do have a Tesla P4 in the system, but during inference it only sees 2% utilization.  The model is just too big for the dinky 8GB in that GPU.\n\nI only see that performance out of GPT-OSS:120b and the 20b variant.  Every other model is way slower on that machine.  Some special sauce in that MXFP4 quantization methinks.",
          "score": 6,
          "created_utc": "2026-02-06 15:29:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xaywi",
              "author": "layer4down",
              "text": "They are also both MoE‚Äôs. I‚Äôm sure that helps üòâ actually 2025 really seems to have been the year of MoE‚Äôs I guess.",
              "score": 3,
              "created_utc": "2026-02-06 16:23:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o47ryte",
              "author": "Icy_Distribution_361",
              "text": "Could you share a bit more about your setup? And about performance of other models?",
              "score": 1,
              "created_utc": "2026-02-08 07:10:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vq44b",
          "author": "AsrielPlay52",
          "text": "Gotta tell us what set up you got, and good MoE models?",
          "score": 4,
          "created_utc": "2026-02-06 10:50:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vu8wg",
              "author": "RelativeOperation483",
              "text": "For the 'potato' setup, here are the specs that got me to **10 TPS** on this 2018 laptop:\n\n* **Hardware:** HP ProBook 650 G5 w/ **Intel i3-8145U** & **16GB Dual-Channel RAM**.\n* **OS:** **Ubuntu (Linux)**‚Äîdon't bother with Windows if you want every MB of RAM for the model. and I've tried Debian 13 -- but fallback to Ubuntu,\n* **The Engine:** **llama-cpp-python** with the **OpenVINO backend**. This is the only way I've found to effectively offload to the **Intel UHD 620 iGPU**.\n* **The Model:** **DeepSeek-Coder-V2-Lite-Instruct (16B MoE)**. Mixture-of-Experts is the ultimate 'cheat code' because it only activates \\~2.4B parameters per token, making it incredibly fast for its intelligence level.\n\nIf you have an Intel chip and 16GB of RAM, definitely try the OpenVINO build. It bridges the gap between 'unusable' and 'daily driver' for budget builds.\n\nBest MoE models are based on your RAM. if You have more Ram and can find the best optimization - Try Qwen 30B-A3B , it's seems like gold standard for most case. ",
              "score": 13,
              "created_utc": "2026-02-06 11:25:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wpn5u",
          "author": "rob417",
          "text": "Very cool. Did you write this with the DeepSeek model on your potato? Reads very much like AI.",
          "score": 4,
          "created_utc": "2026-02-06 14:39:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wr8lp",
              "author": "RelativeOperation483",
              "text": "I thought Reddit support Md. unfortunately, my post turned out to be Ai generated copy-paste.  ",
              "score": -2,
              "created_utc": "2026-02-06 14:47:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vpgf3",
          "author": "RelativeOperation483",
          "text": "I've been testing dense models ranging from 3.8B to 8B, and while they peak at 4 TPS, they aren't as fast as the 16B (A2.6B) MoE model. Here‚Äôs the catch: if you want something smarter yet lighter, go with an MoE. They‚Äôre incredibly effective, even if you‚Äôre stuck with low-end integrated graphics (iGPU) like a UHD 620, just use it.\n\nhttps://preview.redd.it/ucl1et2msuhg1.png?width=1020&format=png&auto=webp&s=0649be11efc5aeb3006674428731bf38fbf103fc\n\n",
          "score": 9,
          "created_utc": "2026-02-06 10:44:26",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o3wdwb5",
          "author": "MelodicRecognition7",
          "text": "you can squeeze a bit more juice from the potato with some BIOS and Linux settings: https://old.reddit.com/r/LocalLLaMA/comments/1qxgnqa/running_kimik25_on_cpuonly_amd_epyc_9175f/o3w9bjw/",
          "score": 5,
          "created_utc": "2026-02-06 13:36:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vyaom",
          "author": "jonjonijanagan",
          "text": "Man, this humbles me. Here I am strategizing how to justify to the wife and get a Strix Halo 128GB ram setip cause my Mac Mini M4 Pro 24GB can only run GPT OSS 20B. You rock, my guy. This is the way.",
          "score": 3,
          "created_utc": "2026-02-06 11:57:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3w4f8d",
          "author": "Ne00n",
          "text": "Same, I got a cheap DDR4 dual channel dedi, depending on model I can get up to 11t/s.  \n8GB VRAM isn't really doing it for me either, so I just use RAM.",
          "score": 3,
          "created_utc": "2026-02-06 12:40:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w6orv",
              "author": "RelativeOperation483",
              "text": "if you're using intel CPUs or iGPUs. try Openvino -- if you've already tried OpenVino , that's might be package missing or need optimizing. But 8GB VRAM eGPU can accelerate than any lower iGPU.",
              "score": 0,
              "created_utc": "2026-02-06 12:54:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3w7165",
                  "author": "Ne00n",
                  "text": "I am talking like E3-1270 v6, old, but if OpenVino supports that, I give it a try.  \nI got like a 64GB DDR4 box for 10$/m, which I mainly use for LLM's.\n\nI only have like 8GB VRAM in my gaming rig and it also runs windows so yikes.",
                  "score": 1,
                  "created_utc": "2026-02-06 12:57:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wle4k",
          "author": "emaiksiaime",
          "text": "We need a gpupoor flair! I want to filter out the rich guy stuff! posts about p40 mi50, cpu inference, running on janky rigs!",
          "score": 3,
          "created_utc": "2026-02-06 14:17:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wmqhf",
              "author": "RelativeOperation483",
              "text": "I hope some guys like me revolt this era and make LLMs more efficient  on typical hardware that everyone can affords, ",
              "score": 1,
              "created_utc": "2026-02-06 14:24:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vwwe0",
          "author": "tmvr",
          "text": "Even here the memory bandwidth is the limiting factor. That CPU supports 2133-2400MT/s RAM so dual-channel the nominal bandwidth is 34-38GB/s. That's fine for any of the MoE models, though you are limited with the 16GB size unfortunately. I have a machine with 32GB of DDR4-2666 and it does 8 tok/s with the Q6\\_K\\_XL quant of Qwen3 30B A3B. ",
          "score": 2,
          "created_utc": "2026-02-06 11:46:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vyc4a",
              "author": "RelativeOperation483",
              "text": "Ram prices are higher than I expected. I went to shop and they said 100$ equivalent of MMK for just 8GB ram stick DDR4 - 2666",
              "score": 3,
              "created_utc": "2026-02-06 11:57:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3w1wv0",
                  "author": "tmvr",
                  "text": "I bought a 64GB kit (4x16) for 90eur last spring. When I checked at the end of the year after prices shot up with was 360eur for the same.",
                  "score": 2,
                  "created_utc": "2026-02-06 12:23:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3w14gl",
          "author": "ANR2ME",
          "text": "I wondered how many t/s will vulkan gives ü§î then again, does such iGPU can works with vulkan backend? üòÖ",
          "score": 2,
          "created_utc": "2026-02-06 12:17:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w1ndy",
              "author": "RelativeOperation483",
              "text": "Technically, yes, the UHD 620 supports **Vulkan**, so you *can* run the backend. But from my testing on this exact i3 'potato,' you really shouldn't. **Vulkan on iGPU is actually slower than the CPU.** ",
              "score": 5,
              "created_utc": "2026-02-06 12:21:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3w9vml",
          "author": "danigoncalves",
          "text": "Sorry if I missed but which backend did you used? and you tweak with parameters to achieve such performance?",
          "score": 2,
          "created_utc": "2026-02-06 13:14:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wcpv8",
              "author": "RelativeOperation483",
              "text": "I use llama-cpp-python with the **OpenVINO backend**   \nn\\_gpu\\_layers**=-1** and **device=\"GPU\"**\n\n**Without OpenVino backend. It will not work.** ",
              "score": 3,
              "created_utc": "2026-02-06 13:30:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wk0st",
          "author": "brickout",
          "text": "Nice! I just built a small cluster from old unused PCs that have been sitting in storage at my school. 7th Gen i7's with Radeon 480s. They run great. I also can't afford new GPUs. I don't mind it being a little slow since I'm basically doing this for free.",
          "score": 2,
          "created_utc": "2026-02-06 14:09:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3x37a1",
              "author": "RelativeOperation483",
              "text": "That has more TPS potential than mine. ",
              "score": 1,
              "created_utc": "2026-02-06 15:46:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xezug",
          "author": "LostHisDog",
          "text": "So I tested this recently on a 10th gen i7 with 32gb of ram just using llama.cpp w/ gpt-oss-20b and the performance was fine... until I tried feeding it any sort of context. My use case is book editing but it's not too unlike code review... the less you can put into context the less useful the LLM is. For me, without a GPU, I just couldn't interact with a reasonable amount of context at usable (for me) t/s. \n\nI might have to try something other than llama.cpp and I'm sure there was performance left on the table even with that but it wasn't even close to something I would use for 10's or thousands of tokens of context when I tried it.",
          "score": 2,
          "created_utc": "2026-02-06 16:41:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xuwx0",
              "author": "ossm1db",
              "text": "What you need is a Hybrid Mamba‚Äë2 MoE model like Nemotron-3 Nano: 30B total parameters, ~3.5B active per token, ~25 GB RAM usage. The key is that for these models, long context does not scale memory the same way as a pure Transformer. The safe max context for 32GB is about 64k tokens (not bad) out of the 1M (150GB-250GB RAM) the model supports according to Copilot.",
              "score": 2,
              "created_utc": "2026-02-06 17:57:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3xsg89",
              "author": "andreasntr",
              "text": "This.\n\nAs much as I love posts like this one, this kind of \"reality checks\" never emerge unfortunately. Even loading 1000 tokens with these constraints will kill the usability. If one runs batch jobs, however, it should be ok but i highly doubt it",
              "score": 1,
              "created_utc": "2026-02-06 17:45:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o42dxjv",
          "author": "im_fukin_op",
          "text": "How do you learn to do this? Where to find the literature? This is the first time I hear of OpenVINO and it seems like exactly the thing I should have been using but I never found out.",
          "score": 2,
          "created_utc": "2026-02-07 11:47:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42mp19",
              "author": "RelativeOperation483",
              "text": "Just browsing, I thought if there's MXL for Mac, why not something special about Intel and Found OpenVINO. I tried to use it plain. It's good unless you need extras. So, I tried with llama-cpp-python with OpenVINO backend.\n\n",
              "score": 2,
              "created_utc": "2026-02-07 12:57:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vxowl",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-06 11:52:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vyhuf",
              "author": "RelativeOperation483",
              "text": "https://preview.redd.it/ws94hwcx5vhg1.png?width=1085&format=png&auto=webp&s=906534bff938adf2d109cf3dd7f38286d2a76157\n\nit's 18.6GB -- I'm thinking about it, I will try it later after my school days.",
              "score": 1,
              "created_utc": "2026-02-06 11:58:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3w3mju",
          "author": "street_melody",
          "text": "you're a rockstar",
          "score": 1,
          "created_utc": "2026-02-06 12:35:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wctk2",
              "author": "RelativeOperation483",
              "text": "Thank",
              "score": 1,
              "created_utc": "2026-02-06 13:30:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3whjy1",
          "author": "jacek2023",
          "text": "great work, thanks for sharing!",
          "score": 1,
          "created_utc": "2026-02-06 13:56:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wij15",
              "author": "RelativeOperation483",
              "text": "Thank Man",
              "score": 1,
              "created_utc": "2026-02-06 14:01:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wq0s1",
          "author": "gambiter",
          "text": "> I‚Äôm writing this from Burma.\n\nNei kaun la :)\n\n> MoE is the \"Cheat Code\": 16B parameters sounds huge, but it only calculates 2.4B per token. It‚Äôs faster and smarter than 3B-4B dense models.\n\nWait, seriously? TIL. I have a project I've been struggling with, and this just may be the answer to it!\n\nThis is very cool. Great job!",
          "score": 1,
          "created_utc": "2026-02-06 14:41:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wqvrh",
              "author": "RelativeOperation483",
              "text": "I guess you're asking \"How are you\" or \"Are you good\". instead of Nei Kaun La, just use \"Nay Kaung Lar\". By the way I'm glad if my post is helpful for somebody. ",
              "score": 1,
              "created_utc": "2026-02-06 14:45:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3wsqlp",
                  "author": "gambiter",
                  "text": "Haha, it's been about a decade since I was trying to learn the language. I was just excited to see someone from there, and wanted to try to say hello properly!",
                  "score": 1,
                  "created_utc": "2026-02-06 14:55:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wqpif",
          "author": "roguefunction",
          "text": "Hell yea!",
          "score": 1,
          "created_utc": "2026-02-06 14:44:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3x4p5t",
          "author": "Michaeli_Starky",
          "text": "10 TPS with how many input tokens? What are you going to do with that practically?",
          "score": 1,
          "created_utc": "2026-02-06 15:53:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3x8d23",
          "author": "layer4down",
          "text": "Very nice! A gentleman recently distilled GLM-4.7 onto an LFM2.5-1.2B model. Curious to know how something like that might perform for you?\n\nhttps://www.linkedin.com/posts/moyasser_ai-machinelearning-largelanguagemodels-activity-7423664844626608128-b2OO\n\n\nhttps://huggingface.co/yasserrmd/GLM4.7-Distill-LFM2.5-1.2B",
          "score": 1,
          "created_utc": "2026-02-06 16:10:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xtn29",
          "author": "Neither-Bite",
          "text": "üëèüëèüëèüëè",
          "score": 1,
          "created_utc": "2026-02-06 17:51:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xu3l3",
          "author": "Neither-Bite",
          "text": "Can you make a video explaining your setup?",
          "score": 1,
          "created_utc": "2026-02-06 17:53:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xvlyv",
          "author": "IrisColt",
          "text": "I kneel, as usually",
          "score": 1,
          "created_utc": "2026-02-06 18:00:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xz119",
          "author": "Jayden_Ha",
          "text": "I would better touching grass that suffering from this speed",
          "score": 1,
          "created_utc": "2026-02-06 18:16:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y00d7",
          "author": "Lesser-than",
          "text": "the man who would not accept no for an answer.",
          "score": 1,
          "created_utc": "2026-02-06 18:21:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y4tt7",
          "author": "hobcatz14",
          "text": "This is really impressive.  I‚Äôm curious about the list of MoE models you tested and how they fared in your opinion‚Ä¶",
          "score": 1,
          "created_utc": "2026-02-06 18:44:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yrkvh",
          "author": "BrianJThomas",
          "text": "I ran full Kimi K2.5 on an n97 mini pc with a single channel 16GB of RAM. I got 22 seconds per token!",
          "score": 1,
          "created_utc": "2026-02-06 20:35:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yylq2",
          "author": "msgs",
          "text": "Now you have me curious to see if my Lunar Lake laptop with 16GB of ram with a built in (toy level) NPU would do.",
          "score": 1,
          "created_utc": "2026-02-06 21:10:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40aups",
          "author": "itsnotKelsey",
          "text": "lol love it",
          "score": 1,
          "created_utc": "2026-02-07 01:37:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41vnd3",
          "author": "Born_Owl7750",
          "text": "Amazing",
          "score": 1,
          "created_utc": "2026-02-07 08:50:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41zk6c",
          "author": "theGamer2K",
          "text": "OpenVINO is underrated. They are doing some impressive work.",
          "score": 1,
          "created_utc": "2026-02-07 09:29:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42k96h",
          "author": "RelativeOperation483",
          "text": "[https://www.reddit.com/r/LocalLLaMA/comments/1qycn5s/deepseekv2lite\\_vs\\_gptoss20b\\_on\\_my\\_2018\\_potato/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1qycn5s/deepseekv2lite_vs_gptoss20b_on_my_2018_potato/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)\n\nDeepseekV2Lite Vs Gpt-OSS-20B Comparison on the same hardware.",
          "score": 1,
          "created_utc": "2026-02-07 12:39:18",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o42rw9h",
          "author": "therauch1",
          "text": "I was very intrigued and just went down the rabbit hole and I just need to know: did you use AI for all of this and did it hallucinate everything?\n\nHere my findings:\n\n\\* There is no CMAKE variable for \\`DGGML\\_OPENVINO\\` in llama-cpp-python ([https://raw.githubusercontent.com/abetlen/llama-cpp-python/refs/heads/main/Makefile](https://raw.githubusercontent.com/abetlen/llama-cpp-python/refs/heads/main/Makefile))\n\n\\* No \\`DGGML\\_OPENVINO\\` in llama.cpp ([https://github.com/search?q=repo%3Aggml-org%2Fllama.cpp%20DGGML\\_OPENVINO&type=code](https://github.com/search?q=repo%3Aggml-org%2Fllama.cpp%20DGGML_OPENVINO&type=code)).\n\n\\* There is one in a seperate (unmerged branch) which maybe will use that variable for building ([https://github.com/ggml-org/llama.cpp/pull/15307/changes](https://github.com/ggml-org/llama.cpp/pull/15307/changes))\n\n\\* Your benchmark script ([https://www.reddit.com/r/LocalLLaMA/comments/1qxcm5g/comment/o3vn0fn/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1qxcm5g/comment/o3vn0fn/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)) does not actually do something: ([https://raw.githubusercontent.com/esterzollar/benchmark-on-potato/refs/heads/main/deep.py](https://raw.githubusercontent.com/esterzollar/benchmark-on-potato/refs/heads/main/deep.py)) the variable \\`device\\_label\\` is not used. SO YOUR BENCHMARK IS NOT WORKING!?",
          "score": 1,
          "created_utc": "2026-02-07 13:31:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42uzd4",
              "author": "RelativeOperation483",
              "text": "check deep\\_decode.py in the same folder -- \n\nDeepSeek-Coder-V2-Lite-Instruct-Q4\\_K\\_M\\_result.txt\n\nis the output of [deep.py](http://deep.py)\n\ntest2output.txt is the output of deep\\_decode.py.\n\nhttps://preview.redd.it/134r03oku2ig1.png?width=862&format=png&auto=webp&s=5511f2d74de9760f0946474e0e81db639f9a5ad1\n\n",
              "score": 1,
              "created_utc": "2026-02-07 13:49:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o42wkxr",
                  "author": "therauch1",
                  "text": "Okay I see that it should in theory load a single layer onto a gpu if available. What happens if you offload everything? So setting that value to \\`-1\\`?",
                  "score": 1,
                  "created_utc": "2026-02-07 13:59:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o42xohp",
          "author": "Neither_Sort_2479",
          "text": "Guys, I'm relatively new to local LLM and this may be a stupid question, but can you tell me what is the best model right now to run locally for coding tasks as an agent with rtx4060ri 8GB (32gb ram) and what settings (lm studio)? Because I haven't been able to use anything so far (I tried qwen3 8b, 14b, deepseek r1, qwen2.5 coder instruct, codellama 7b instruct, and several others), none of those that I tested can work as an agent with cline or roo code, there is not enough context even for something simple. Or maybe there is some kind of hint about the workflow for such limited local models that I need to know",
          "score": 1,
          "created_utc": "2026-02-07 14:05:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o442tla",
          "author": "Forsaken-Truth-697",
          "text": "No latest GPUs? No problem.\n\nI can use cloud service or remotely connect it to my laptop, and run the best GPUs on the market.",
          "score": 1,
          "created_utc": "2026-02-07 17:35:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45k9z1",
          "author": "sebuzdugan",
          "text": "nice result\n\ncurious what quant and cache layout you‚Äôre using on openvino\n\nalso did you test smaller ctx like 2k to see if igpu scales better than cpu there",
          "score": 1,
          "created_utc": "2026-02-07 22:13:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o486a69",
          "author": "Qxz3",
          "text": "\"## The Reality Check\"",
          "score": 1,
          "created_utc": "2026-02-08 09:24:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49t4rp",
          "author": "-InformalBanana-",
          "text": "What did you optimize here exactly? You installed 2 programs - openvino and llama.cpp and thats it?\nAlso what is the t/s for prompt processing speed?",
          "score": 1,
          "created_utc": "2026-02-08 16:15:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4f7s5j",
          "author": "SoobjaCat",
          "text": "This is soo cool and impressive",
          "score": 1,
          "created_utc": "2026-02-09 12:27:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hmd1a",
          "author": "hobbywine-2148",
          "text": "Bonjour,  \nEst-ce que vous auriez un tutoriel pour expliquer comment vous faites ?  \nJ'ai un processeur ultra 9 285h avec arc 140t je ne trouve pas de tutoriel pour installer ollama et openwebui  \nsur ubntu 24.04 pour le gpu arc140t qui ont l'air tr√®s bien comme indiqu√© dans le blog :https://www.robwillis.info/2025/05/ultimate-local-ai-setup-guide-ubuntu-ollama-open-webui/\n\nEn attendant, j'ai clon√© le projet :  \n[https://github.com/balaragavan2007/Mistral\\_on\\_Intel\\_NPU](https://github.com/balaragavan2007/Mistral_on_Intel_NPU)   \net apr√®s avoir install√© ce qui est recommand√© au lien intel :  \n[https://dgpu-docs.intel.com/driver/client/overview.html](https://dgpu-docs.intel.com/driver/client/overview.html)  \nj'arrive √† faire fonctionner ce mod√®le mistral √† environ 15-17 token/s sur le GPU arc140t.  \nmais √ßa n'est qu'avec ce mod√®le l√†, celui du projet Mistral\\_on\\_Intel\\_NPU  \nP.S. je n'ai pas r√©ussi √† faire reconnaitre le NPU mais comme apparemment le GPU arc 140t   \nc'est l√† o√π c'est le plus puissant √ßa n'est pas g√™nant.  \nDu coup j'aimerai arriver √† installer ollama + openweb ui pour pouvoir chopper les mod√®les  \nqui s'am√©liorent avec le temps.  \nD√©j√† dans windows 11 dans une VM ubuntu 24.04 j'ai install√© LM studio qui fonctionne pas mal du tout avec le mod√®le ministral 3 (moins vite (VM) mais mieux que le projet Mistral\\_on\\_Intel\\_NPU dual-boot ubuntu 24.04).  \nDonc auriez vous un tutoriel quelque part ?  \n",
          "score": 1,
          "created_utc": "2026-02-09 19:53:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4jr6tm",
          "author": "ceeeej1141",
          "text": "Great! I don't have a \"4090/5090\" either but, no thanks I won't let my AI Chatbot uses every drop of performance lol. I prefer to multitask, that's why I have a dual-monitor setup.",
          "score": 1,
          "created_utc": "2026-02-10 02:46:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kp4w4",
          "author": "Emotional-Debate3310",
          "text": "I appreciate your hard work, but also like to indicate there might be some easier way to achieve the same level of efficiency and performance.\n\n# Have you tried MatFormer architecture (Nested Transformer)?\n\nFor example Gemma3N 27B LiteRT model or similar \n\n- *Architecture:* It utilizes the MatFormer architecture (Nested Transformer). It physically has ~27B parameters but utilizes a \"dynamic slice\" of roughly 4B \"Effective\"  parameters during inference.\n\n\n - *Why it feels fast:* Unlike traditional quantization (which just shrinks weights), MatFormer natively skips blocks of computation. When running on LiteRT (Google's optimized runtime), it leverages the NPU / GPU / CPU based on availability resulting in near-zero thermal throttling.\n\nAll the best.",
          "score": 1,
          "created_utc": "2026-02-10 06:49:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vel3r",
          "author": "RelativeOperation483",
          "text": "PS: it's Q4KM GGUF version -- if you dare Go With Q5KM\n\n\\# Known Weaknesses\n\niGPU Wake-up Call: The iGPU takes significantly longer to compile the first time (Shader compilation). It might look like it's stuck‚Äîdon't panic. It's just the \"GPU\" having his coffee before he starts teaching.\n\nLanguage Drift: On the iGPU, DeepSeek occasionally hallucinates Chinese characters (it‚Äôs a Chinese-base model). The logic remains 100% solid, but it might forget it's speaking English for a second.\n\nReading Speed: While not as fast as a $40/mo cloud subscription, 10 t/s is faster than the average human can read (5-6 t/s). Why pay for speed you can't even use?",
          "score": 2,
          "created_utc": "2026-02-06 09:02:00",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3xikvp",
              "author": "Not_FinancialAdvice",
              "text": "I get language drift on most of the Chinese models I've tried.",
              "score": 1,
              "created_utc": "2026-02-06 16:58:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xitw0",
          "author": "Fine_Purpose6870",
          "text": "That's the power of linux. Windows can shuckabrick. Not to mention Windows was giving peoples encryption keys over to the FBI pfft. That's absolutely sick.  I bet you could get an old pentium to run a 3b LLM on linux lol.",
          "score": 1,
          "created_utc": "2026-02-06 16:59:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xnjzj",
          "author": "x8code",
          "text": "Meh, I'll keep my RTX 5080 / 5070 Ti setup, thanks.",
          "score": 0,
          "created_utc": "2026-02-06 17:22:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o46qvm5",
              "author": "rog-uk",
              "text": "What a useful contribution üôÑ",
              "score": 1,
              "created_utc": "2026-02-08 02:32:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vjq8n",
          "author": "xrvz",
          "text": "No high-end Macbook is necessary ‚Äì the 600$ base Mac mini has 12GB VRAM at 120 GB/s bandwidth (150 GB/s with the coming M5).\n\nIt'd run the mentioned model (deepseek-coder-v2:16b-lite-instruct-q4_0) at about 50 t/s at low context.",
          "score": -3,
          "created_utc": "2026-02-06 09:51:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvq0xe",
      "title": "Bashing Ollama isn‚Äôt just a pleasure, it‚Äôs a duty",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/ad5zhvq0nhhg1.png",
      "author": "jacek2023",
      "created_utc": "2026-02-04 14:29:48",
      "score": 1010,
      "num_comments": 199,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qvq0xe/bashing_ollama_isnt_just_a_pleasure_its_a_duty/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3jj32i",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-04 15:15:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jhaqv",
          "author": "LinkSea8324",
          "text": "Any direct link on this message ?\n\nEdit : https://github.com/ggml-org/llama.cpp/pull/19324#issuecomment-3847213274",
          "score": 75,
          "created_utc": "2026-02-04 15:06:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jhiwq",
              "author": "jacek2023",
              "text": "[https://github.com/ggml-org/llama.cpp/pull/19324#issuecomment-3847213274](https://github.com/ggml-org/llama.cpp/pull/19324#issuecomment-3847213274)",
              "score": 37,
              "created_utc": "2026-02-04 15:07:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jahw0",
          "author": "swagonflyyyy",
          "text": "Lmao",
          "score": 172,
          "created_utc": "2026-02-04 14:31:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jmhbc",
              "author": "No_Afternoon_4260",
              "text": "Be quiet... They'll hear us and fix it..",
              "score": 81,
              "created_utc": "2026-02-04 15:31:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jmv5y",
                  "author": "swagonflyyyy",
                  "text": "No please don't fix it I don't want more RAM blowups.",
                  "score": 27,
                  "created_utc": "2026-02-04 15:33:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3k6fb4",
              "author": "Educational_Rent1059",
              "text": "llamao",
              "score": 37,
              "created_utc": "2026-02-04 17:03:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3liwk9",
                  "author": "MoffKalast",
                  "text": "llamao.cackle",
                  "score": 7,
                  "created_utc": "2026-02-04 20:47:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ji4g9",
          "author": "debackerl",
          "text": "I'm not sure, but isn't Ollama looking for venture cash? So they got to say that they invented and own a lot of intellectual property, or at least, have a great expertise. If they say we only \"daemonized\" llama.cpp and made it into a model jukebox, it's less impressive. That would explain why they never gave a lot of credit to llama.cpp...",
          "score": 148,
          "created_utc": "2026-02-04 15:10:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n34dk",
              "author": "SkyFeistyLlama8",
              "text": "Is there anything legally enforceable in llama.cpp code that could blow up in Ollama's faces?",
              "score": 9,
              "created_utc": "2026-02-05 01:42:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ncb6x",
                  "author": "fish312",
                  "text": "No, the entire llama.cpp project is MIT licensed, except for the third party vendor libraries",
                  "score": 26,
                  "created_utc": "2026-02-05 02:35:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jhrhp",
          "author": "Ne00n",
          "text": "I started with ollama, since I read this sub, I ditched it, I just compile llama.cpp and use their webinterface, its just better.",
          "score": 97,
          "created_utc": "2026-02-04 15:08:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3m50iz",
              "author": "AngryDemonoid",
              "text": "Same thing. I liked ollama at first, then started having trouble the more I got into it. Been on llama.cpp and llama-swap for months now with no regrets.",
              "score": 15,
              "created_utc": "2026-02-04 22:34:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mep2m",
                  "author": "Svenstaro",
                  "text": "Doesn't llama.cpp now come with native model management/swapping support? Or do you have another reason to use llama-swap?",
                  "score": 7,
                  "created_utc": "2026-02-04 23:25:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3pf3hc",
              "author": "cobbleplox",
              "text": "I don't think you even have to compile it these days, they have lots of binary releases.",
              "score": 1,
              "created_utc": "2026-02-05 12:28:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pf95r",
                  "author": "Ne00n",
                  "text": "Yes but there are some performance benefits, when you finetune it for your platform.  \nE.g I only run on it on a CPU server, I can get a few extra tokens out of it.",
                  "score": 2,
                  "created_utc": "2026-02-05 12:29:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3m01ov",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -1,
              "created_utc": "2026-02-04 22:09:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3m5xfn",
                  "author": "jwpbe",
                  "text": "    cargo install rust-hf-downloader && \\\n    uv tool install huggingface-hub --with hf_transfer && \\\n    rust-hf-downloader",
                  "score": 5,
                  "created_utc": "2026-02-04 22:39:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m2bd0",
                  "author": "Ne00n",
                  "text": "hugginface cli, downloading models is a oneliner.",
                  "score": 8,
                  "created_utc": "2026-02-04 22:20:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jb65o",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 43,
          "created_utc": "2026-02-04 14:35:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kh6hw",
              "author": "FaatmanSlim",
              "text": "In case its helpful for anyone, more details about the bug itself in the code he shared: the state update applies only the decay factor from the most recent chunk (*gLastExp* sliced to *chunk:chunk+1*) to a state that actually accumulates contributions from all previous chunks, implicitly assuming earlier decays were already applied; this assumption breaks under chunking, cache reuse, or long sequences, causing silent numerical drift and incorrect state values that only become visible at long context lengths.\n\nAnd a bit more ELI5: the code updates a long-running ‚Äúmemory‚Äù using only the latest shrink factor, so over time the memory quietly drifts away from what it should be.",
              "score": 70,
              "created_utc": "2026-02-04 17:52:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3lni4e",
                  "author": "AbheekG",
                  "text": "Thank you!!",
                  "score": 7,
                  "created_utc": "2026-02-04 21:09:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3p1ac6",
                  "author": "MysticPlasma",
                  "text": "ain't no way the only explanation I find is as a response to a different comment. Thank you a lot (given you're right), but I feel a little excluded, having to roll a dice to find an explanation. Guess I'm just too out of machine learning.",
                  "score": 3,
                  "created_utc": "2026-02-05 10:36:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3kequo",
          "author": "SCphotog",
          "text": "Someone give me the 'explain like I'm 5' thing about Ollama?\n\nI know what it is, but what's wrong with it exactly?",
          "score": 27,
          "created_utc": "2026-02-04 17:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l2pqd",
              "author": "ahjorth",
              "text": "ELI5: Do you know that feeling when you always share your candy with your friends but then there's *that one guy* who never shares?\n\n\n\nLlama.cpp is a seriously impressive open source project built through tens of thousands of highly qualified person hours and hard work. Said a little bluntly, llama.cpp is highly specialized code; Ollama could be vibe coded by a 1st/2nd year student. \n\n  \nOllama took llama.cpp and wrapped it in their own app, making things like swapping models extremely easy.  Ollama built a strong reputation and user base as open source software, tightly coupled with the llama.cpp project. They were the usability and utility people, drawing on llama.cpps contstant optimizations and development to accommodate new models, new quantizations, etc. But up until this point, everything that Ollama did was open source. All good. \n\nAbout a year ago, Ollama changed direction towards monetizing their app. They introduced a closed-source desktop app, and made it the default download on their website. They \"forked\" llama.cpp, meaning they started making changes to llama.cpp's code instead of building on top of/wrapping around it (and doing a poor job of it, as you can see). The implications of their fork became clear when Ollama announced 'day 1 support' of some new model. I don't remember which, but it was a big deal. Except their implementation was complete shit, it literally didn't work. A week or so later, Ollama copy/pasted llama.cpp's new code for that model into their fork, and *as far as I remember* either didn't mention it in their release notes, or pretended they had fixed a bug unrelated to their implementation of that model.\n\nThis is a big fucking no-no in open source software. It's not illegal - everything they do complies with llama.cpp's license. But it goes against all norms and conventions.",
              "score": 115,
              "created_utc": "2026-02-04 19:30:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3lxt81",
                  "author": "SCphotog",
                  "text": "Thank you for taking the time to explain. I get it now.",
                  "score": 17,
                  "created_utc": "2026-02-04 21:58:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3n7npf",
                  "author": "rockbandit",
                  "text": "> This is a big fucking no-no in open source software. It's not illegal - everything they do complies with llama.cpp's license. But it goes against all norms and conventions.\n\n\nThere‚Äôs some nuance here because what you‚Äôre saying is not entirely correct. llama.cpp is MIT license, which is one of the most permissive types of licenses. \n\nYou‚Äôre allowed to use the code commercially, modify it, distribute / redistribute it, sublicense it, and include it in proprietary/closed-source software.\n\nSaying it‚Äôs a big no-no isn‚Äôt really accurate. Commercial software is built on the back of open source software with permissive licenses. \n\nSome examples:\n- SQLite: embedded in browsers, phones, and operating systems. Most users aren‚Äôt aware of or never see attribution.\n- curl: ships inside huge amounts of software and firmware with credit usually buried in a license file somewhere that no one is aware of.\n- React UI libraries: countless packages power proprietary SaaS products and are used on nearly every major site you visit. And many are MIT licensed. \n- BSD networking stack: foundational to macOS, PlayStation, and many WiFi routers.\n- zlib: compression library used in PNG, game engines, operating systems, and tons of commercial software and is rarely visibly credited.\n\nSo, it‚Äôs really rich when open source maintainers like ggerganov get salty about people using their code. If you wanted attribution and it was that important to you, an MIT license was the wrong license to choose when setting up the project.",
                  "score": 9,
                  "created_utc": "2026-02-05 02:08:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lq696",
                  "author": "catch-10110",
                  "text": "This might sound like a pointed question but I swear I mean it genuinely. \n\nAs an end user who just wants to play around with local llms why should I care? Eg if Llama.cpp‚Äôs licence entirely allows for this then it‚Äôs fine right? I‚Äôm not ‚Äúinto‚Äù open source - I just want to use an app that‚Äôs functional. Like, the vast majority of apps on the App Store aren‚Äôt open source so what‚Äôs the big deal?\n\nAgain I am completely genuinely asking!",
                  "score": 12,
                  "created_utc": "2026-02-04 21:22:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3pcsro",
                  "author": "Mickenfox",
                  "text": "> \n> This is a big fucking no-no in open source software. It's not illegal - everything they do complies with llama.cpp's license. But it goes against all norms and conventions.\n\nWhat a load of crap. Making commercial software from MIT libraries is normal and good. If you don't want people using your code commercially, don't give them explicit permission to do so.",
                  "score": 2,
                  "created_utc": "2026-02-05 12:12:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ktwzo",
              "author": "Normal-Ad-7114",
              "text": "https://preview.redd.it/sx8541zmxihg1.jpeg?width=450&format=pjpg&auto=webp&s=2878e26f1b75bfeae4ded658bf8bc5b478ec8f54",
              "score": 25,
              "created_utc": "2026-02-04 18:50:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3r0sst",
                  "author": "horsethebandthemovie",
                  "text": "You can‚Äôt include llamafile here. The author is a pretty big contributor to upstream llama.cpp and also wrote IMO the pinnacle of modern systems software in Cosmopolitan Libc",
                  "score": 4,
                  "created_utc": "2026-02-05 17:27:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3kw4v9",
                  "author": "SCphotog",
                  "text": "Thanks for nothing.",
                  "score": -11,
                  "created_utc": "2026-02-04 18:59:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3pckvd",
              "author": "Mickenfox",
              "text": "Open source people really fucking hate when someone tries to build a usable commercial product that doesn't require a hundred command line parameters.",
              "score": 2,
              "created_utc": "2026-02-05 12:10:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jem08",
          "author": "frozen_tuna",
          "text": "Ollama unloading and loading models based on API requests was a game changer for me though. Loading qwen-coder for copilot when im tweaking code,  couple minutes later my script requests a difficult structured output from qwen3, couple minutes later the script calls mistral small. Ollama makes that dead easy.",
          "score": 74,
          "created_utc": "2026-02-04 14:53:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jfy9o",
              "author": "Marksta",
              "text": "It was indeed a good idea, so now a better implementation of it is in llama.cpp with router mode. With that and -hf to do the \"run it now, download if I don't have it\" method of Ollama and I don't think they have any other single unique feature? \n\nBesides tons of issues they created like the one that blocks them from handling split ggufs for the entire last year...",
              "score": 71,
              "created_utc": "2026-02-04 14:59:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3l5xda",
                  "author": "overand",
                  "text": "The main missing feature I see is changing the context size on the fly. (As in, when it unloads and reloads a model.) I understand that's not useful for everyone, but it's the main thing I miss when using llama.cpp's router mode - restarting it to change the context window. (I also have to restart it when I add a new model, though I do suspect there's a better way for both of those things.)",
                  "score": 7,
                  "created_utc": "2026-02-04 19:45:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3k3frl",
                  "author": "deepspace86",
                  "text": "Yep, this is what finally got me to switch. On-demand swapping of models was a big deal for my workflows and with router mode I don't need to bother with ollama anymore.",
                  "score": 10,
                  "created_utc": "2026-02-04 16:49:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lgvgo",
                  "author": "StephenSRMMartin",
                  "text": "I used ollama for a long time, and swapped to llama.cpp once they added in the router mode + automatic timeout based unloading.\n\nI'm not sure I would say that llama.cpp's implementation is \"better\". I still have to fiddle with the router configs more than I did with ollama. E.g., with ollama, it will automatically decide whether it can load one or two models if api requests come in with different models. Last I tried this in llama.cpp's router mode, it just OOM'd instead. So I have to manually tell llama.cpp, only one model can be loaded at once.\n\nLikewise, I haven't really hit OOM issues in Ollama despite cranking up context size. In llama.cpp, I still have to tweak context size and such, even with the -fit option, because llama.cpp seems to underestimate the amount of ram/vram needed to run models + their context + their calculations.",
                  "score": 3,
                  "created_utc": "2026-02-04 20:37:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m5dy9",
                  "author": "AngryDemonoid",
                  "text": "Does this mean I don't need llama-swap any more? Model switching is built into llama.cpp?",
                  "score": 1,
                  "created_utc": "2026-02-04 22:36:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jfnce",
              "author": "Dr4x_",
              "text": "Llama-swap does that pretty well",
              "score": 47,
              "created_utc": "2026-02-04 14:58:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jfyw4",
                  "author": "Prof_ChaosGeography",
                  "text": "Llamacpp server has swapping models built in now",
                  "score": 60,
                  "created_utc": "2026-02-04 14:59:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jfwu8",
              "author": "fish312",
              "text": "KoboldCpp has that feature too, with API support and you can even do it in a few clicks from the built in UI.",
              "score": 11,
              "created_utc": "2026-02-04 14:59:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3lnhmx",
              "author": "Freonr2",
              "text": "LM studio handles this as well, you can set \"JIT model loading\", and optionally flip \"Auto unload unused HIT loaded models\" to evict all but the most recently used if you lack enough memory to keep several loaded.\n\nLM Studio is just a better product overall if you want \"simple click button receive bacon\" software.  LM Studio also recently added batch/concurrent decode (ala -np K in llama-server).  \n\nFrom LM Studio, then you can move to Llama.cpp if you want something bit more advanced, then finally vllm and sglang for more professional GPU-focused serving with tensor parallel or more powerful sharding for efficient/fast GPU serving, or maybe ik llama or a few other more boutique solutions for specific use cases.\n\nI just don't see where ollama fits in.",
              "score": 7,
              "created_utc": "2026-02-04 21:09:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3lup5x",
                  "author": "frozen_tuna",
                  "text": "Ollama fits in because I don't have to read instructions on how to set all that up or configure llama-swap like other comments mention. I can browse a few popular models and be ready to go within minutes.\n\nBy all means, I used to turn up my nose at people using ollama back in the day when I was hype about tgwui. Now I actually build stuff downstream instead of just playing with models themselves. Ollama is great for that. I don't want to focus on that part of the stack anymore.",
                  "score": -2,
                  "created_utc": "2026-02-04 21:43:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jkdxy",
              "author": "gered",
              "text": "This is exactly the thing, yeah. People will point to stuff like llama-swap, etc but that misses the point entirely which is that Ollama gave you an out-of-the-box experience with all those quality of life features that people want.\n\nI started out with Ollama myself, but have switched to Llama.cpp now that this model swapping feature is finally built-in (llama-swap no longer required), and that the built-in web UI for Llama.cpp is quite good overall. Basically because the out-of-the-box experience with Llama.cpp is \"good enough\" _for me_ as of the past couple months.\n\nBut everyone's needs and comfort-levels will vary. That is what some of the anti-Ollama crowd here doesn't get.",
              "score": 21,
              "created_utc": "2026-02-04 15:21:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3k1m5n",
                  "author": "FaceDeer",
                  "text": "Yeah. I went with Ollama because I wanted to work *with* LLM models, not work *on* them, and Ollama just gave me everything I needed out of the box with a nice simple interface.\n\nOthers are catching up to that now, but I've already got Ollama set up and working so I'll need impetus other than \"this is now just as good as Ollama\" to make me spend the time and hassle to switch.",
                  "score": 12,
                  "created_utc": "2026-02-04 16:41:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3k01mx",
                  "author": "ConsistentGent",
                  "text": "What machine do you have for that? I am still using api from z.ai for coding.",
                  "score": 1,
                  "created_utc": "2026-02-04 16:34:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3jt3kx",
                  "author": "CharacterEvening4407",
                  "text": "I will never understand this stupid flame wars. It's not Ollama or llama.cpp it's Ollama and llama.cpp. I use both (and others like LM Studio MX runtime).",
                  "score": -2,
                  "created_utc": "2026-02-04 16:02:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mxo0i",
              "author": "cosmicr",
              "text": "No no, we're not allowed to like Ollama here.",
              "score": 1,
              "created_utc": "2026-02-05 01:11:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3myxiz",
                  "author": "frozen_tuna",
                  "text": "Advocating for proprietary solutions are all the rage here on /r/localllama apparently.",
                  "score": -1,
                  "created_utc": "2026-02-05 01:18:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3n4jlt",
          "author": "Boricua-vet",
          "text": "https://preview.redd.it/7rd1nj1p0lhg1.png?width=1125&format=png&auto=webp&s=6dec22db3d555f1f3a83d1a01e6a143986b7127c\n\nI was so tempted...\n\n\n\n  \nLOL",
          "score": 4,
          "created_utc": "2026-02-05 01:50:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jgcui",
          "author": "superkickstart",
          "text": "What's wrong with ollama? Why are people bashing it?",
          "score": 26,
          "created_utc": "2026-02-04 15:01:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jh7p9",
              "author": "henk717",
              "text": "The context of the screenshot is that ollama makes their own engine alongside instead of contributing to llamacpp upstream. Which duplicates effort and has in the past caused models to then not be supported by the rest of the ecosystem when prominent model makers skip upstream llamacpp. So you'd then expect that if they go their own way and have their own unique engine alongside the llamacpp one that it is unique, but if they are copying the bugs of llamacpp it means its really just rebuilding what llamacpp is doing already. They could have just used it since llamacpp is part of their program.\n\nCombine that with very little credit for llamacpp and you can imagine why ggerganov finds this amusing.",
              "score": 121,
              "created_utc": "2026-02-04 15:06:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jhpdw",
                  "author": "superkickstart",
                  "text": "Yes i got that from the screenshot. So is ollama generally bad  and should be avoided?\n\n\nedit.\nDownvoted for asking questions lol. These niche subs sure are something else.",
                  "score": 12,
                  "created_utc": "2026-02-04 15:08:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jhbvv",
              "author": "Pristine-Woodpecker",
              "text": "Closed source ripoff that repackages an open source project (which is the exact thing being pointed out by OP). Claimed to be \"easier to use\", but mostly got people terrible results because of defaulting to tiny context windows, pretending much smaller models were DeepSeek R1, and a host of similar issues.\n\nBasically an easy way for people to get an LLM running very crappily combined with very effective marketing.\n\nIf you are able to find a sub like this, there's zero reasons to ever touch ollama with a ten feet pole.",
              "score": 56,
              "created_utc": "2026-02-04 15:06:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jizub",
                  "author": "g_rich",
                  "text": "The fact that LM Studio exists, is better in every way and is easier to use makes the fact that Ollama is still the default method for getting started with local models is baffling.",
                  "score": 20,
                  "created_utc": "2026-02-04 15:14:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3jhrp3",
                  "author": "C0rn3j",
                  "text": "https://github.com/ollama/ollama\n\nBut it's MIT licensed?",
                  "score": 2,
                  "created_utc": "2026-02-04 15:08:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lb00i",
                  "author": "teleprint-me",
                  "text": "This is exactly why I dont use or like permissive licensing anymore.\n\n\nYou cant use a permissive license, then complain people are stealing your code.\n\n\nThe moment you use a permissive license, you open the flood gates to proprietary competitors who benefit from all that hard won labor.\n\n\nAnyone who uses permissive licensing has no right to complain. The license basically releases any ownership, liability, etc. The only reason the copyright matters on a permissive license is that it legally releases that copyright.\n\n\nThe only people who like this are the ones interested in creating their own proprietary software without the involved labor of building something similar. They only benefit from it.",
                  "score": -4,
                  "created_utc": "2026-02-04 20:09:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lu4i1",
                  "author": "azentrix",
                  "text": "ollama isn't closed source, it's MIT licensed.  Do you not even know this basic information, it shows how little we should trust the other things you say. Who is upvoting this guy, more ignorant people.  ",
                  "score": -1,
                  "created_utc": "2026-02-04 21:40:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3krycu",
              "author": "pmttyji",
              "text": "Multiple reasons\n\n[https://www.reddit.com/r/LocalLLaMA/search/?q=ollama+bad](https://www.reddit.com/r/LocalLLaMA/search/?q=ollama+bad)",
              "score": 2,
              "created_utc": "2026-02-04 18:41:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3tw1dn",
              "author": "ANTIVNTIANTI",
              "text": "they‚Äôre children",
              "score": 1,
              "created_utc": "2026-02-06 02:12:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3on4lp",
          "author": "AsliReddington",
          "text": "# LLAMABarn will end all this garb",
          "score": 2,
          "created_utc": "2026-02-05 08:20:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r7hmz",
          "author": "TokenRingAI",
          "text": "I saw that in github, very amusing",
          "score": 2,
          "created_utc": "2026-02-05 17:58:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kwnf6",
          "author": "jrussellfreelance",
          "text": "Can someone tell me what's going on I have my ollama models serving my open web UI and I want to know if that's the right choice still",
          "score": 4,
          "created_utc": "2026-02-04 19:02:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ksvx3",
          "author": "Abarth_Vader",
          "text": "Thanks Ollama",
          "score": 2,
          "created_utc": "2026-02-04 18:45:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n3keq",
          "author": "satoshibitchcoin",
          "text": "I love how LocalLLMA stood firmly against these fucks from the get go. I love this community.",
          "score": 2,
          "created_utc": "2026-02-05 01:45:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jf568",
          "author": "ab2377",
          "text": "lol rightly so!",
          "score": 2,
          "created_utc": "2026-02-04 14:55:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jmbex",
          "author": "siegevjorn",
          "text": "Please do more! We'd love to see more like this",
          "score": 2,
          "created_utc": "2026-02-04 15:30:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3oxgpe",
          "author": "gnaarw",
          "text": "Reddit doing it's thing showing me the right threads once in a while üôÉ",
          "score": 1,
          "created_utc": "2026-02-05 10:00:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q2s4i",
          "author": "Specific-Goose4285",
          "text": "Isn't that Go? I had no idea ollama used Go.",
          "score": 1,
          "created_utc": "2026-02-05 14:46:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4am7kg",
          "author": "RatherBeEmbed",
          "text": "This post just popped up while doom scrolling and now I'm googling llama.cpp a bit. I have some experience with AI, like RL projects for school and fun, and script kiddie stuff with like stable diffusion a few years ago.\n\nBefore I go too far, is this rabbit hole worth diving into (read: fixate on) as a full time worker and full time student or should I put it off? lol",
          "score": 1,
          "created_utc": "2026-02-08 18:33:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jctqu",
          "author": "AdventurousGold672",
          "text": "Meh slamming ollama is so stupid, while I moved to llama.cpp, I still believe Ollama is maybe the best tool to introduce people to local llm, it's fast and easy to work with, no commands no need to go and download / convert models.",
          "score": -23,
          "created_utc": "2026-02-04 14:43:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jdk3f",
              "author": "kironlau",
              "text": "Better you suggest LM Studio, at least the models downloaded could be used latter.",
              "score": 53,
              "created_utc": "2026-02-04 14:47:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3je0y3",
                  "author": "Boricuakris",
                  "text": "LMStudio doesn‚Äôt seem to work for streaming the response via api. Has that been fixed?",
                  "score": -13,
                  "created_utc": "2026-02-04 14:50:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jeeok",
              "author": "International-Try467",
              "text": "Koboldai is literally idiotproof",
              "score": 25,
              "created_utc": "2026-02-04 14:52:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3jeqj3",
              "author": "robberviet",
              "text": "Lmao, for beginner LMStudio is much better.",
              "score": 33,
              "created_utc": "2026-02-04 14:53:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3jgln7",
              "author": "finah1995",
              "text": "LoL üòÜ isn't that screenshot comment by the creator and maintainer of llama.cpp ü¶ô and inventor of GGUF. \n\nIn my opinion this makes it lot funnier than if some other maintainer made fun of it like.",
              "score": 8,
              "created_utc": "2026-02-04 15:02:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3jjjxh",
              "author": "g_rich",
              "text": "LM Studio is better in every way.",
              "score": 5,
              "created_utc": "2026-02-04 15:17:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3k8wcw",
              "author": "Eugr",
              "text": "It's easy to start with, but something like LM Studio is even easier. My biggest gripe with Ollama (besides not giving credit to llama.cpp) is that they use a tiny context window by default (was 2048, now 4096 I think) and make it a rolling window, so you will never know you ran out of context, it will just truncate your request to fit it. \n\nAnd then people start complaining that RAG doesn't work, models forget what was said before, etc.",
              "score": 3,
              "created_utc": "2026-02-04 17:14:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3jh09a",
              "author": "dampflokfreund",
              "text": "Ollama has to be used with a CLI, still too much for the average user. Koboldcpp and LM Studio are easier to handle for beginners.",
              "score": -1,
              "created_utc": "2026-02-04 15:04:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jhr4g",
                  "author": "AdventurousGold672",
                  "text": "You don't need to run even single command to use ollama.\n\nDownload install, select model via gui, chat via gui.",
                  "score": 2,
                  "created_utc": "2026-02-04 15:08:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3jk96q",
                  "author": "truthputer",
                  "text": "You have clearly never used Ollama, but still feel like you are an authority on an app you have never used.",
                  "score": -2,
                  "created_utc": "2026-02-04 15:20:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3or7oy",
          "author": "Alarming_Bluebird648",
          "text": "wild how many people treat it like a godsend when it's basically just a bloated daemon. i'm glad we‚Äôre finally calling out the vc pump and dump vibes bc raw llama.cpp is just better infrastructure anyway.",
          "score": 1,
          "created_utc": "2026-02-05 08:59:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jhjao",
          "author": "leonbollerup",
          "text": "ohhh god.. this childish shit is linux\\*\\*\\* all over again.\n\nListen.. both sides.\n\n1. If you release something as opensource .. expect it to be used  \n2. You might not just want to copy/paste code .. verify it first - and give credits  \n3. Wake up! - you are both in the same team.. working for the same goal.. some competing against each other and work together!",
          "score": -26,
          "created_utc": "2026-02-04 15:07:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jk1f2",
              "author": "g_rich",
              "text": "The problem people have with Ollama is they are copying llama.cpp while claiming their engine is home built. They are also doing some questionable things to try to lock users in.",
              "score": 23,
              "created_utc": "2026-02-04 15:19:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3kovjg",
                  "author": "minus_28_and_falling",
                  "text": "They clearly state using llama.cpp code and credit its authors.\n\nhttps://github.com/ollama/ollama/tree/main/llama\n\nI also don't see the problem, that's what Open Source is about.",
                  "score": 6,
                  "created_utc": "2026-02-04 18:27:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3jo5ut",
                  "author": "leonbollerup",
                  "text": "Not defending them at all - just tired of the whining ‚Ä¶",
                  "score": -17,
                  "created_utc": "2026-02-04 15:39:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3l7335",
              "author": "SporksInjected",
              "text": "I personally feel like Ollama is harder to use than llamacpp and gets worse results. There aren‚Äôt as many options and they clearly aren‚Äôt interested in contributing to the community.",
              "score": 1,
              "created_utc": "2026-02-04 19:50:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3nywoa",
                  "author": "minus_28_and_falling",
                  "text": "Can't tell if they're interested in contributing or not (hard to look inside their heads), but they release code under MIT license, and every open sourced line of code is a contribution to the community.",
                  "score": 2,
                  "created_utc": "2026-02-05 04:56:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jtj0p",
          "author": "galic1987",
          "text": "Have you heard about ollama cloud üôàüôàüòÖ",
          "score": -1,
          "created_utc": "2026-02-04 16:04:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kfve7",
              "author": "One-Employment3759",
              "text": "It's a cool idea, making it easier to swap to models you can't tun locally.",
              "score": 2,
              "created_utc": "2026-02-04 17:46:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3khmao",
          "author": "freehuntx",
          "text": "Ollama does a better job at swapping models and allowing to run multiple models.  \nvllm and llama.cpp do a bad job in that regard.  \nOllama is more user friendly.  \nHate me but its the truth.",
          "score": -10,
          "created_utc": "2026-02-04 17:54:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kjb2g",
              "author": "onetwomiku",
              "text": "vllm is a production grade inference engine - it doesn't need to swap models.  \nllama.cpp can now swap models. If you need more than one at the time - just spawn more instances of llama.cpp.\n\nThe truth is simple - \"fuck ollama\".",
              "score": 9,
              "created_utc": "2026-02-04 18:02:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lgawu",
          "author": "Special_Concert_3906",
          "text": "LMAO ü§£",
          "score": -2,
          "created_utc": "2026-02-04 20:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k2iuo",
          "author": "Pretty_Ingenuity_192",
          "text": "hahahah",
          "score": -2,
          "created_utc": "2026-02-04 16:45:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3melgh",
          "author": "IronColumn",
          "text": "who cares",
          "score": -2,
          "created_utc": "2026-02-04 23:25:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1quvqs9",
      "title": "Qwen/Qwen3-Coder-Next ¬∑ Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/Qwen/Qwen3-Coder-Next",
      "author": "coder543",
      "created_utc": "2026-02-03 15:58:52",
      "score": 704,
      "num_comments": 249,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o3d1kxo",
          "author": "jacek2023",
          "text": "awesome!!! 80B coder!!! perfect!!!",
          "score": 103,
          "created_utc": "2026-02-03 16:07:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3e156l",
              "author": "-dysangel-",
              "text": "Can't wait to see this one - the 80B already seemed great at coding",
              "score": 21,
              "created_utc": "2026-02-03 18:49:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d11vr",
          "author": "danielhanchen",
          "text": "We made dynamic Unsloth GGUFs for those interested! We're also going to release Fp8-Dynamic and MXFP4 MoE GGUFs!\n\nhttps://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF\n\nAnd a guide on using Claude Code / Codex locally with Qwen3-Coder-Next: https://unsloth.ai/docs/models/qwen3-coder-next",
          "score": 282,
          "created_utc": "2026-02-03 16:04:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d2pbw",
              "author": "mr_conquat",
              "text": "Goddamn that was fast",
              "score": 65,
              "created_utc": "2026-02-03 16:12:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d3f2p",
                  "author": "danielhanchen",
                  "text": ":)",
                  "score": 36,
                  "created_utc": "2026-02-03 16:15:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3g8mbx",
                  "author": "bene_42069",
                  "text": "that's what she said",
                  "score": 7,
                  "created_utc": "2026-02-04 01:25:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dr3rh",
              "author": "slavik-dev",
              "text": "Qwen published their own GGUF:\n\n[https://huggingface.co/Qwen/Qwen3-Coder-Next-GGUF](https://huggingface.co/Qwen/Qwen3-Coder-Next-GGUF)\n\nu/danielhanchen do you know, if author's GGUF will have any advantage?",
              "score": 31,
              "created_utc": "2026-02-03 18:04:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gau93",
                  "author": "dinerburgeryum",
                  "text": "Obvs not DH but looking at it: Qwen uses a more ‚Äútraditional‚Äù quantization scheme, letting mainline llama.cpp decide what weights need more and less bits assigned. Extending that, Qwen‚Äôs quants do not use imatrix. It‚Äôs the last bit that interests me most: I‚Äôm actually very skeptical of imatrix-based quantization. It is much more like QAT than most people give it credit for, and the dataset used in calibration can have real downstream effects when it comes, especially, to agentic workflows. No disrespect to the Unsloth team, who are without question incredible allies in the open weights space, but I do prefer non-imatrix quants when available.",
                  "score": 20,
                  "created_utc": "2026-02-04 01:38:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3edjam",
              "author": "Chromix_",
              "text": "(Edit: [llama.cpp fix](https://www.reddit.com/r/LocalLLaMA/comments/1qvp0hm/model_qwen3next_correct_vectorized_key_gdiff/) was just merged)\n\nI'm getting the same type of error with the UD-Q4\\_K\\_XL quant on llama.cpp with -fa on. It randomly keeps complaining about non-existing syntax errors in different parts of go code, even though the code is correct. Qwen3-Next-80B-A3B-Instruct-UD-Q5\\_K\\_XL and the thinking version never behaved like that.\n\nFor example:\n\n    I found the issue! Look at line 41 in pkg/simd/distance.go:\n        return dot / (float32(math.Sqrt(float64(normA))) * float32(math.Sqrt(float64(normB))))\n            \n    There's a syntax error - there's an extra closing parenthesis. The expression should be:\n        return dot / (float32(math.Sqrt(float64(normA))) * float32(math.Sqrt(float64(normB))))\n            \n    But it's currently:\n        return dot / (float32(math.Sqrt(float64(normA))) * float32(math.Sqrt(float64(normB))))\n            \n    This is causing a division by zero or incorrect calculation. Let me fix this:\n\nYet another error/interpretation of the same line:\n\n    I found the bug! Look at line 41 in pkg/simd/distance.go:\n        return dot / (float32(math.Sqrt(float64(normA))) * float32(math.Sqrt(float64(normB))))\n    \n    The issue is that the denominator is being calculated incorrectly. The current code divides by the first sqrt and then multiplies by the second sqrt, which is mathematically wrong. It should be dividing by the product of both norms.\n\nOr another flavor:\n\n    However, there's a subtle bug at line 349:\n        entity_id = entity_title_to_ids[entity.title]\n    \n    This line has a syntax error - it's missing the assignment operator. It should be:\n        entity_id = entity_title_to_ids[entity.title]\n\nYes, a syntax error in perfectly compiling code is very \"subtle\" (as it doesn't exist).",
              "score": 9,
              "created_utc": "2026-02-03 19:47:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gd5ii",
                  "author": "velcroenjoyer",
                  "text": "Same for me, the model makes up a bunch of syntax errors in any code I give it and \"fixes\" them with the same exact code that supposedly has a syntax errors; it's pretty much unusable for code review because of this. I also tried the original Qwen3 Next 80B A3B Instruct and it does the same thing but will at least admit that it's wrong. I'm using the Unsloth UD-IQ3\\_XXS GGUF quant of both models in the latest CUDA 12 llama.cpp build on Windows with this command: llama-server -m (path-to-model) --host (local-ip) --port 8080 -c 32000 --jinja",
                  "score": 3,
                  "created_utc": "2026-02-04 01:51:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3iybfv",
                  "author": "Clank75",
                  "text": "Ahh!  I've had exactly the same problems with Typescript.  Did some changes, they compiled cleanly, and then it keeps trying to fix \"ah, there is an unbalanced ) on line XXX, let me just fix that\" errors that don't exist.\n\nThis was with the MXFP4 quant.",
                  "score": 1,
                  "created_utc": "2026-02-04 13:26:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3nvbl5",
                  "author": "danielhanchen",
                  "text": "Sorry about that - we had to redo all imatrix quants - Q8_0, Q8_K_XL, MXFP4_MOE and BF16 don't need re-updating, but the rest do!",
                  "score": 1,
                  "created_utc": "2026-02-05 04:31:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3d4yth",
              "author": "Terminator857",
              "text": "Where is your \"buy me a cup of coffee\" link so we can send some love? :) <3",
              "score": 21,
              "created_utc": "2026-02-03 16:23:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d7w70",
                  "author": "danielhanchen",
                  "text": "Appreciate it immensely, but it's ok :) The community is what keeps us going!",
                  "score": 35,
                  "created_utc": "2026-02-03 16:36:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3db4y2",
                  "author": "cleverusernametry",
                  "text": "They're in YC (sadly). They'll be somewhere between fine to batting off VCs throwing money at them. \n\nFor ours and the world's sake let's hope VC doesn't succeed in poisoning them",
                  "score": 6,
                  "created_utc": "2026-02-03 16:51:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dfb8v",
              "author": "ethertype",
              "text": "Do you have back-of-the napkin numbers for how well MXFP4 compares vs the 'classic' quants? In terms of quality, that is.",
              "score": 10,
              "created_utc": "2026-02-03 17:11:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dmlsk",
                  "author": "danielhanchen",
                  "text": "I'm testing them!",
                  "score": 22,
                  "created_utc": "2026-02-03 17:44:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ee2ci",
              "author": "ClimateBoss",
              "text": "what is the difference plz? u/[danielhanchen](https://www.reddit.com/user/danielhanchen/)\n\n* unsloth GGUF compared to Qwen Coder Next official GGUF ?  \n* is unsloth chat template fixes better for llama server?  \n* requantized? accuracy than Qwen original?",
              "score": 6,
              "created_utc": "2026-02-03 19:50:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dgweb",
              "author": "oliveoilcheff",
              "text": "What is better for strix halo, fp8 or gguf?",
              "score": 3,
              "created_utc": "2026-02-03 17:18:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3g4l6a",
                  "author": "mycall",
                  "text": "How much RAM do you have?  I have with 128GB RAM and was going to try Q8_0.\n\nUsing Q8_0 weights = 84.8 GB and KV @ 262,144 ctx ‚âà 12.9 GB (assuming fp16/bf16 KV):\n\n(84.8 + 12.9) √ó 1.15 = 112.355 GB (max context window * 15% extra)",
                  "score": 3,
                  "created_utc": "2026-02-04 01:02:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dmdv2",
              "author": "TurnUpThe4D3D3D3",
              "text": "Love you guys",
              "score": 4,
              "created_utc": "2026-02-03 17:43:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dpauh",
                  "author": "danielhanchen",
                  "text": "Thanks!",
                  "score": 1,
                  "created_utc": "2026-02-03 17:56:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e02jh",
              "author": "Far-Low-4705",
              "text": "what made you start to do MXFP4 MoE? do you reccomend that over the standard default Q4km?",
              "score": 3,
              "created_utc": "2026-02-03 18:45:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3f8qq5",
                  "author": "R_Duncan",
                  "text": "[https://www.reddit.com/r/LocalLLaMA/comments/1qrzyaz/i\\_found\\_that\\_mxfp4\\_has\\_lower\\_perplexity\\_than\\_q4\\_k/](https://www.reddit.com/r/LocalLLaMA/comments/1qrzyaz/i_found_that_mxfp4_has_lower_perplexity_than_q4_k/)\n\nSeems that some hybrid models have way better perplexity with some less size",
                  "score": 4,
                  "created_utc": "2026-02-03 22:13:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3de0p8",
              "author": "robertpro01",
              "text": "Hi u/danielhanchen , I am trying to run the model within ollama, but looks like it failed to load, any ideas?  \n  \ndocker exec 5546c342e19e ollama run [hf.co/unsloth/Qwen3-Coder-Next-GGUF:Q4\\_K\\_M](http://hf.co/unsloth/Qwen3-Coder-Next-GGUF:Q4_K_M)  \nError: 500 Internal Server Error: llama runner process has terminated: error loading model: missing tensor 'blk.0.ssm\\_in.weight'  \nllama\\_model\\_load\\_from\\_file\\_impl: failed to load model",
              "score": 2,
              "created_utc": "2026-02-03 17:05:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dmnm7",
                  "author": "danielhanchen",
                  "text": "Probably best to update Ollama",
                  "score": 5,
                  "created_utc": "2026-02-03 17:44:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3dlsjd",
                  "author": "R_Duncan",
                  "text": "Do you have the plain llama.cpp or you got a version capable of running qwen3-next ?",
                  "score": 1,
                  "created_utc": "2026-02-03 17:40:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3l10uw",
                  "author": "molecula21",
                  "text": "I‚Äôm facing the same issue with ollama. I updated it to the pre release 0.15.5 but that didn‚Äôt help. I am running ollama with open code on a DGX spark",
                  "score": 1,
                  "created_utc": "2026-02-04 19:22:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3f2n1i",
              "author": "Status_Contest39",
              "text": "Fast as lightning, even the shadow can not catch up, this is the legendary mode of the speed of light.",
              "score": 2,
              "created_utc": "2026-02-03 21:44:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ftu6y",
              "author": "coreyfro",
              "text": "I use your models!!!\n\nI have been running Qwen3-Coder-30B at Q8.  Looks like Qwen3-Coder-80B at Q4 performs equally (40tps on a Strix Halo, 64GB)\n\nI also downloaded 80B as Q3.  It's 43tps on same hardware but I could claw back some of my RAM (I allocate as little RAM for UMA as possible on Linux)\n\nDo you have any idea which is most useful and what I am sacrificing with the quantizing?  I know the theory but I don't have enough practical experience with these models.",
              "score": 1,
              "created_utc": "2026-02-04 00:04:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3g6nde",
              "author": "JsThiago5",
              "text": "Thanks! Do you know if MXFP4 gguf will appear to old models?",
              "score": 1,
              "created_utc": "2026-02-04 01:14:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3gjq3q",
              "author": "Odd-Ordinary-5922",
              "text": "even with setting an api key using a command claude code still asks me for a way to sign in? do you know why...",
              "score": 1,
              "created_utc": "2026-02-04 02:28:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3gsjw8",
              "author": "emaiksiaime",
              "text": "Thanks! I can run it with decent context and good speed on my potato! This is truly an incredible and accessible model! It‚Äôs a huge step in democratizing coding models! Thanks for making it that much more accessible!",
              "score": 1,
              "created_utc": "2026-02-04 03:18:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o40jc0z",
              "author": "lamagy",
              "text": "What specs would I need here to run this with decent speed?",
              "score": 1,
              "created_utc": "2026-02-07 02:29:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3db42o",
          "author": "ilintar",
          "text": "I knew it made sense to spend all those hours on the Qwen3 Next adaptation :)",
          "score": 137,
          "created_utc": "2026-02-03 16:51:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dc0xe",
              "author": "itsappleseason",
              "text": "bless you king",
              "score": 26,
              "created_utc": "2026-02-03 16:55:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ddb6d",
              "author": "No_Swimming6548",
              "text": "Thanks a lot man",
              "score": 21,
              "created_utc": "2026-02-03 17:01:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dv9we",
              "author": "jacek2023",
              "text": "...now all we need is speed ;)",
              "score": 7,
              "created_utc": "2026-02-03 18:23:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dvdkw",
                  "author": "ilintar",
                  "text": "Actually I think proper prompt caching is more urgent right now.",
                  "score": 18,
                  "created_utc": "2026-02-03 18:24:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dxjng",
              "author": "No_Conversation9561",
              "text": "Awesome work, man",
              "score": 3,
              "created_utc": "2026-02-03 18:33:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3eclf6",
              "author": "wanderer_4004",
              "text": "Any chance for getting better performance on Apple silicon? With llama.cpp I get 20Tok/s on M1 64GB with Q4KM while with MLX I get double that (still happy though that you did all the work to get it to run with llama.cpp!).",
              "score": 2,
              "created_utc": "2026-02-03 19:43:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3f1a6u",
                  "author": "ilintar",
                  "text": "Yeah, there are some optimizations in the works, don't know if x2 is achievable though.",
                  "score": 3,
                  "created_utc": "2026-02-03 21:38:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3f16na",
              "author": "epigen01",
              "text": "<3",
              "score": 1,
              "created_utc": "2026-02-03 21:37:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3djlfd",
          "author": "Thrumpwart",
          "text": "FYI from the HF page:\n\n\"To achieve optimal performance, we recommend the following sampling parameters: temperature=1.0, top_p=0.95, top_k=40.\"",
          "score": 29,
          "created_utc": "2026-02-03 17:30:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3d0hat",
          "author": "Ok_Knowledge_8259",
          "text": "so your saying a 3B activated parameter model can match the quality of sonnet 4.5??? that seems drastic... need to see if it lives up to the hype, seems a bit to crazy.",
          "score": 100,
          "created_utc": "2026-02-03 16:02:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3daojj",
              "author": "Single_Ring4886",
              "text": "Clearly it cant match it in everything probably only in Python and such but even that is good",
              "score": 35,
              "created_utc": "2026-02-03 16:49:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dlwhw",
              "author": "ForsookComparison",
              "text": "> can match the quality of sonnet 4.5???\n\nYou must be new. Every model claims this. The good ones usually compete with Sonnet 3.7 and the bad ones get forgotten.",
              "score": 72,
              "created_utc": "2026-02-03 17:41:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3doc6t",
                  "author": "Neither-Phone-7264",
                  "text": "i mean k2.5 is pretty damn close. granted, they're in the same weight class so its not like a model 1/10th the size overtaking it.",
                  "score": 36,
                  "created_utc": "2026-02-03 17:52:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ex52f",
                  "author": "buppermint",
                  "text": "This is extremely delusional. There are LOTS of open-weight models far, far better than Sonnet 3.7. This is speaking as someone who spent a huge amount of time coding with Sonnet 3.7/4.0 last summer - at that point the LLM could barely remember its original task after 100k tokens, and would make up insane hacky fixes because it didn't the intelligence to understand full architectures.\n\nModern 30B MoEs are easily at that level already. Using GLM-4.7 Flash with opencode requires me to use the same tricks I had to do with Sonnet 3.7 + claude code, but with everything 1000x cheaper. Stuff like K2/GLM4.7 are far, far better.\n\nThis is the same kind of people who insist that GPT-3.5 or GPT-4 was the best LLM and that everything else has gotten progressively worse for years. No, that level of performance was just new to you at the time so your brain has misencoded it as being better than it is.",
                  "score": 19,
                  "created_utc": "2026-02-03 21:19:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3di62y",
              "author": "AppealSame4367",
              "text": "Have you tried Step 3.5 Flash? You will be very surprised.",
              "score": 11,
              "created_utc": "2026-02-03 17:24:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fjy9i",
                  "author": "effortless-switch",
                  "text": "When it stops itself from getting in a loop on every third prompt maybe I'll finally be able to test it.",
                  "score": 1,
                  "created_utc": "2026-02-03 23:10:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3gtkcb",
              "author": "RnRau",
              "text": "Yeah - I'll wait for the next edition of swe-rebench before accepting such claims :)",
              "score": 1,
              "created_utc": "2026-02-04 03:24:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d66r3",
              "author": "-p-e-w-",
              "text": "It‚Äôs 80B A3B. I would be surprised if Sonnet were much larger.",
              "score": -20,
              "created_utc": "2026-02-03 16:28:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d78zy",
                  "author": "Orolol",
                  "text": "I would be surprised if sonnet is smaller than 1T total params.",
                  "score": 27,
                  "created_utc": "2026-02-03 16:33:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3de3qc",
          "author": "reto-wyss",
          "text": "It certainly goes brrrrr.\n\n\n- Avg prompt throughput: **24469.6 tokens/s**, \n- Avg generation throughput: 54.7 tokens/s, \n- Running: 28 reqs, Waiting: 100 reqs, GPU KV cache usage: 12.5%, Prefix cache hit rate: 0.0%\n\n\nTesting with the FP8 with vllm and 2x Pro 6000.",
          "score": 25,
          "created_utc": "2026-02-03 17:05:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dokd5",
              "author": "Eugr",
              "text": "Generation seems to be slow for 3B active parameters??",
              "score": 18,
              "created_utc": "2026-02-03 17:53:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dr053",
                  "author": "SpicyWangz",
                  "text": "I think that‚Äôs been the case with qwen next architecture. It‚Äôs still not getting the greatest implementation",
                  "score": 7,
                  "created_utc": "2026-02-03 18:04:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ens6z",
                  "author": "reto-wyss",
                  "text": "It's just a log value and it's simultaneous 25k pp/s and 54 tg/s, it was just starting to to process the queue, so no necessarily saturated. I was just excited to run on the first try :P",
                  "score": 2,
                  "created_utc": "2026-02-03 20:35:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ebsjs",
                  "author": "meganoob1337",
                  "text": "Or maybe not all requests are generating yet (see 28 running ,100 waiting looks like new requests are still started)",
                  "score": 1,
                  "created_utc": "2026-02-03 19:39:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e86an",
              "author": "Eugr",
              "text": "How are you benchmarking? If you are using vLLM logs output (and looks like you are), the numbers there are not representative and all over the place as it reports on individual batches, not actual requests.\n\nCan you try to run llama-benchy?\n\n```bash\nuvx llama-benchy --base-url http://localhost:8000/v1 --model Qwen/Qwen3-Coder-Next-FP8 --depth 0 4096 8192 16384 32768 --adapt-prompt --tg 128 --enable-prefix-caching\n```",
              "score": 5,
              "created_utc": "2026-02-03 19:22:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3e8apu",
              "author": "Eugr",
              "text": "This is what I'm getting on my single DGX Spark (which is much slower than your RTX6000):\n\n| model                     |            test |             t/s |       ttfr (ms) |    est_ppt (ms) |   e2e_ttft (ms) |\n|:--------------------------|----------------:|----------------:|----------------:|----------------:|----------------:|\n| Qwen/Qwen3-Coder-Next-FP8 |          pp2048 | 3743.54 ¬± 28.64 |   550.02 ¬± 4.17 |   547.11 ¬± 4.17 |   550.06 ¬± 4.18 |\n| Qwen/Qwen3-Coder-Next-FP8 |           tg128 |    44.63 ¬± 0.05 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_pp @ d4096 | 3819.92 ¬± 28.92 |  1075.25 ¬± 8.14 |  1072.34 ¬± 8.14 |  1075.29 ¬± 8.15 |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_tg @ d4096 |    44.15 ¬± 0.09 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 |  pp2048 @ d4096 | 1267.04 ¬± 13.75 | 1619.46 ¬± 17.59 | 1616.55 ¬± 17.59 | 1619.49 ¬± 17.59 |\n| Qwen/Qwen3-Coder-Next-FP8 |   tg128 @ d4096 |    43.41 ¬± 0.38 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_pp @ d8192 | 3723.15 ¬± 29.73 | 2203.34 ¬± 17.48 | 2200.43 ¬± 17.48 | 2203.38 ¬± 17.48 |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_tg @ d8192 |    43.14 ¬± 0.07 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 |  pp2048 @ d8192 |   737.40 ¬± 3.90 | 2780.31 ¬± 14.71 | 2777.40 ¬± 14.71 | 2780.35 ¬± 14.72 |\n| Qwen/Qwen3-Coder-Next-FP8 |   tg128 @ d8192 |    42.71 ¬± 0.04 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_pp @ d16384 | 3574.05 ¬± 11.74 | 4587.12 ¬± 15.02 | 4584.21 ¬± 15.02 | 4587.15 ¬± 15.01 |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_tg @ d16384 |    41.52 ¬± 0.03 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 | pp2048 @ d16384 |   393.58 ¬± 0.69 |  5206.47 ¬± 9.16 |  5203.56 ¬± 9.16 | 5214.69 ¬± 20.61 |\n| Qwen/Qwen3-Coder-Next-FP8 |  tg128 @ d16384 |    41.09 ¬± 0.01 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_pp @ d32768 |  3313.36 ¬± 0.57 |  9892.57 ¬± 1.69 |  9889.66 ¬± 1.69 |  9892.61 ¬± 1.69 |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_tg @ d32768 |    38.82 ¬± 0.04 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 | pp2048 @ d32768 |   193.06 ¬± 0.12 | 10610.91 ¬± 6.33 | 10608.00 ¬± 6.33 | 10610.94 ¬± 6.34 |\n| Qwen/Qwen3-Coder-Next-FP8 |  tg128 @ d32768 |    38.47 ¬± 0.02 |                 |                 |                 |\n\nllama-benchy (0.1.2)\ndate: 2026-02-03 11:14:29 | latency mode: api",
              "score": 3,
              "created_utc": "2026-02-03 19:22:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3e8cur",
                  "author": "Eugr",
                  "text": "Note, that by default vLLM disables prefix caching on Qwen3-Next models, so the performance will suffer on actual coding tasks as vLLM will have to re-process repeated prompts (which is indicated by your KV cache hit rate).\n\nYou can enable prefix caching by adding `--enable-prefix-caching` to your vLLM arguments, but as I understand, support for this architecture is experimental. It does improve the numbers for follow up prompts at the expense of somewhat slower prompt processing of the initial prompt:\n\n| model                     |            test |             t/s |        ttfr (ms) |     est_ppt (ms) |    e2e_ttft (ms) |\n|:--------------------------|----------------:|----------------:|-----------------:|-----------------:|-----------------:|\n| Qwen/Qwen3-Coder-Next-FP8 |          pp2048 | 3006.54 ¬± 72.99 |   683.87 ¬± 16.66 |   681.47 ¬± 16.66 |   683.90 ¬± 16.65 |\n| Qwen/Qwen3-Coder-Next-FP8 |           tg128 |    42.68 ¬± 0.57 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_pp @ d4096 | 3019.83 ¬± 81.96 |  1359.78 ¬± 37.52 |  1357.39 ¬± 37.52 |  1359.80 ¬± 37.52 |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_tg @ d4096 |    42.84 ¬± 0.14 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 |  pp2048 @ d4096 | 2368.35 ¬± 46.78 |   867.47 ¬± 17.30 |   865.08 ¬± 17.30 |   867.51 ¬± 17.30 |\n| Qwen/Qwen3-Coder-Next-FP8 |   tg128 @ d4096 |    42.12 ¬± 0.40 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_pp @ d8192 | 3356.63 ¬± 32.43 |  2443.17 ¬± 23.69 |  2440.77 ¬± 23.69 |  2443.21 ¬± 23.68 |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_tg @ d8192 |    41.97 ¬± 0.05 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 |  pp2048 @ d8192 | 2723.63 ¬± 22.21 |    754.38 ¬± 6.12 |    751.99 ¬± 6.12 |    754.41 ¬± 6.12 |\n| Qwen/Qwen3-Coder-Next-FP8 |   tg128 @ d8192 |    41.56 ¬± 0.12 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_pp @ d16384 | 3255.68 ¬± 17.66 |  5034.97 ¬± 27.35 |  5032.58 ¬± 27.35 |  5035.02 ¬± 27.35 |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_tg @ d16384 |    40.44 ¬± 0.26 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 | pp2048 @ d16384 | 2502.11 ¬± 49.83 |   821.22 ¬± 16.12 |   818.83 ¬± 16.12 |   821.26 ¬± 16.12 |\n| Qwen/Qwen3-Coder-Next-FP8 |  tg128 @ d16384 |    40.22 ¬± 0.03 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_pp @ d32768 | 3076.52 ¬± 12.46 | 10653.55 ¬± 43.19 | 10651.16 ¬± 43.19 | 10653.61 ¬± 43.19 |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_tg @ d32768 |    37.93 ¬± 0.04 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 | pp2048 @ d32768 | 2161.97 ¬± 18.51 |    949.75 ¬± 8.12 |    947.36 ¬± 8.12 |    949.78 ¬± 8.12 |\n| Qwen/Qwen3-Coder-Next-FP8 |  tg128 @ d32768 |    37.20 ¬± 0.36 |                  |                  |                  |\n\nllama-benchy (0.1.2)\ndate: 2026-02-03 10:50:37 | latency mode: api",
                  "score": 7,
                  "created_utc": "2026-02-03 19:23:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3hrndp",
                  "author": "foxpro79",
                  "text": "My guy just wanted to give you a huge thanks and shout out. Your contributions on the nvidia forums have been an inspiration for me to take the plunge and buy a spark for my own hobby-ing.",
                  "score": 2,
                  "created_utc": "2026-02-04 07:36:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e0aac",
              "author": "Flinchie76",
              "text": "How does it compare to MiniMax in 4 bit (should fit on those cards)?",
              "score": 1,
              "created_utc": "2026-02-03 18:46:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d6fko",
          "author": "teachersecret",
          "text": "This looks really, really interesting.\n\nMight finally be time to double up my 4090. Ugh.\n\nI will definitely be trying this on my 4090/64gb ddr4 rig to see how it does with moe offload. Guessing this thing will still be quite performant.\n\nAnyone given it a shot yet? How‚Äôs she working for you?",
          "score": 20,
          "created_utc": "2026-02-03 16:29:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3djoa7",
              "author": "ArckToons",
              "text": "I‚Äôve got the same setup. Mind sharing how many t/s you‚Äôre seeing, and whether you‚Äôre running vLLM or llama.cpp?",
              "score": 8,
              "created_utc": "2026-02-03 17:31:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d8oaz",
              "author": "Additional_Ad_7718",
              "text": "Please update me so I know if it's usable speeds or not ü´°ü´°ü´°",
              "score": 9,
              "created_utc": "2026-02-03 16:40:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3gzk9h",
              "author": "TurnUpThe4D3D3D3",
              "text": "That should be plenty to run a Q4 version",
              "score": 1,
              "created_utc": "2026-02-04 04:01:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o421pbe",
              "author": "kochanac",
              "text": "did you manage to run it? what was your performance?",
              "score": 1,
              "created_utc": "2026-02-07 09:50:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o438tww",
                  "author": "teachersecret",
                  "text": "I did. It was okay - I think I was in the 40t/s range, dropping pretty quickly from there as context expanded. Felt a bit too slow for my tastes, but perfectly serviceable. It's still on-drive and I'll probably keep it, but I think this one would be a lot more interesting if I had more vram.",
                  "score": 1,
                  "created_utc": "2026-02-07 15:07:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ei064",
          "author": "Eugr",
          "text": "PSA: if you are using vLLM, you may want to:\n\n- Use `--enable-prefix-caching`, because vLLM disables prefix caching for mamba architectures by default, so coding workflows will be slower because of that.\n- Use `--attention-backend flashinfer` as default FLASH_ATTN backend requires much more VRAM to hold the same KV cache. For instance, my DGX Spark with `--gpu-memory-utilization 0.8` can only hold ~60K tokens in KV cache with the default attention backend, but with Flashinfer it can fit 171K tokens (without quantizing KV cache to fp8).",
          "score": 13,
          "created_utc": "2026-02-03 20:08:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ewbtt",
              "author": "HumanDrone8721",
              "text": "Does it work in cluster more (2x Spark) ?",
              "score": 1,
              "created_utc": "2026-02-03 21:15:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3f0936",
                  "author": "Eugr",
                  "text": "I tried with Feb 1st vLLM build and it crashed in the cluster mode during inference, with both FLASH\\_ATTN and FLASHINFER backends. I'm trying to run with the fresh build now - let's see if it works.",
                  "score": 1,
                  "created_utc": "2026-02-03 21:33:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d61nq",
          "author": "Septerium",
          "text": "The original Qwen3 Next was so good in benchmarks, but actually using it was not a very nice experience",
          "score": 45,
          "created_utc": "2026-02-03 16:28:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3duv03",
              "author": "--Tintin",
              "text": "I like Qwen3 Next a lot. I think it aged well and is under appreciated.",
              "score": 21,
              "created_utc": "2026-02-03 18:21:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dbgfl",
              "author": "cleverusernametry",
              "text": "Besides it being slow as hell, at least on llama.cpp",
              "score": 13,
              "created_utc": "2026-02-03 16:53:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3e1iqt",
                  "author": "-dysangel-",
                  "text": "It was crazy fast on MLX, especially the subquadratic attention was very welcome for us GPU poor Macs. Though I've settled into using GLM Coding Plan for coding anyway",
                  "score": 5,
                  "created_utc": "2026-02-03 18:51:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e1z7h",
              "author": "Far-Low-4705",
              "text": "how do you mean?\n\nI think it is the best model we have for usable long context.",
              "score": 5,
              "created_utc": "2026-02-03 18:53:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ee5j8",
                  "author": "Septerium",
                  "text": "I haven't been lucky with it for agentic coding, specially with long context. Even the first version of Devstral small produced better results for me",
                  "score": 2,
                  "created_utc": "2026-02-03 19:50:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3hlo9p",
              "author": "relmny",
              "text": "I agree. I actually tested it a few times and didn't like anything about it, and went back to qwen3-Coder and others.\n\n  \nI hope it happens the same with qwen3-30b, that I used a lot at first, and then I noticed I started using other models more and more and then abandoned/deleted it... and then the Coder version came and that was my main model for a while (I still use it a lot).",
              "score": 2,
              "created_utc": "2026-02-04 06:44:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d1zmk",
          "author": "Recoil42",
          "text": "https://preview.redd.it/shnwpwn00bhg1.png?width=4420&format=png&auto=webp&s=956bb077c3abaaac65a592c9a02b7e50be6a443b\n\nHoly balls. \n\nAnyone know what the token burn story looks like yet?",
          "score": 39,
          "created_utc": "2026-02-03 16:09:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d8ujf",
              "author": "coder543",
              "text": "It's an instruct model only, so token usage should be relatively low, even if Qwen instruct models often do a lot of thinking in the response these days.",
              "score": 23,
              "created_utc": "2026-02-03 16:41:09",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3d38at",
              "author": "ClimateBoss",
              "text": "ik\\_llama better add graph split after shittin on OG qwen3 next ROFL",
              "score": 4,
              "created_utc": "2026-02-03 16:15:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dxi53",
                  "author": "twavisdegwet",
                  "text": "or ideally mainline llama merges graph support- I know it's not a straight drop in but graph makes otherwise unusable models practical for me.",
                  "score": 3,
                  "created_utc": "2026-02-03 18:33:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dlozt",
          "author": "ForsookComparison",
          "text": "This is what a lot of folks were dreaming of.\n\nFlash-speed tuned for coding that's not limited by such a small number of total params. Something to challenge gpt-oss-120b.",
          "score": 10,
          "created_utc": "2026-02-03 17:40:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3emwvn",
          "author": "noctrex",
          "text": "[https://huggingface.co/noctrex/Qwen3-Coder-Next-MXFP4\\_MOE-GGUF](https://huggingface.co/noctrex/Qwen3-Coder-Next-MXFP4_MOE-GGUF)\n\nOh guess I'm gonna have some MXFP4 competition from the big boys üòä",
          "score": 7,
          "created_utc": "2026-02-03 20:31:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ezm28",
              "author": "ethertype",
              "text": "Do you have a ballpark number for the quality of MXFP4 vs Q4/Q5/Q6/Q8?",
              "score": 2,
              "created_utc": "2026-02-03 21:30:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3m0yxy",
                  "author": "noctrex",
                  "text": "Unfortunately not. This would need quite expansive benchmarking and testing and unfortunately I haven't had the time to do it.",
                  "score": 1,
                  "created_utc": "2026-02-04 22:13:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3hzlas",
              "author": "ScoreUnique",
              "text": "Can I understand how is mxfp4 different than traditional or importance matrix quants? I've had a bit better of a performance on mxfp4 than on IQ not gonna lie. \n.thanks for the quants.",
              "score": 1,
              "created_utc": "2026-02-04 08:49:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3m0vis",
                  "author": "noctrex",
                  "text": "It's a quantization better suited for MoE models, it's quite simple actually, it quantizes the MoE tensors to FP4, and everything else to Q8.",
                  "score": 1,
                  "created_utc": "2026-02-04 22:13:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dhnav",
          "author": "HollowInfinity",
          "text": "This seems excellent so far, I'm using just a minimal agent loop with the 8-bit quant and gave it the test of having llama.cpp's llama-server output a CSV file with metrics for each request and it completed it using about 70,000 tokens. It rooted around the files first and even found where the metrics are already being aggregated for export and all in all took about 5 minutes.\n\nLiterally my go-to this morning was GLM-4.7-Flash and given that first test.. wow.",
          "score": 6,
          "created_utc": "2026-02-03 17:21:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fups1",
          "author": "dmter",
          "text": "It's so funny - it's not thinking kind so starts producing code right away, and it started thinking in the comments. then it produced 6 different versions, and every one of them is of course tested in latest software version (according to it), which is a nice touch.\nI just used the last version. After feeding debug output and 2 fixes it actually worked. about 15k tokens in total.\nGLM47q2 spent all available 30k context and didn't produce anything but the code it had in thinking didn't work.\n\nSo yeah this looks great at first glance - performance of 358B model but better and 4 times faster and also at least 2 times less token burn. But maybe my task was very easy (GPT120 failed though).\n\nOh and it's Q4 262k ctx - 20 t/s on 3090 with --fit on. 17 t/s when using about half of GPU memory (full moe offload).\n\n\nP.S. so I did some more prompts and it's not as good as it seemed but still nice. There was another prompt which was 1 shotted by GLM47q2 but Next Coder couldn't complete even after a few fixes.\n\nAlso I think Qwen3 Next Coder model could benefit from dedicated thinking mode as it misses key detail from prompt that need to be spelled out explicitly every time. \n\nMaybe thinking mode can be enabled with some command or llama.cpp parameter?",
          "score": 4,
          "created_utc": "2026-02-04 00:08:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eeqgj",
          "author": "2funny2furious",
          "text": "Please tell me they are going to keep adding the word next to all future releases.  Like Qwen3-Coder-Next-Next.",
          "score": 8,
          "created_utc": "2026-02-03 19:53:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3euyy0",
              "author": "cmpxchg8b",
              "text": "Like some kind of University project document naming.",
              "score": 3,
              "created_utc": "2026-02-03 21:09:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e60d5",
          "author": "Far-Low-4705",
          "text": "this is so useful.  \n  \nreally hoping for qwen 3 next 80b vl",
          "score": 4,
          "created_utc": "2026-02-03 19:12:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hkr7h",
              "author": "EbbNorth7735",
              "text": "I was just thinking the same thing. It seemed like the vision portion of qwen3 vl was relatively small",
              "score": 2,
              "created_utc": "2026-02-04 06:36:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d9qmm",
          "author": "Significant_Fig_7581",
          "text": "Finally!!!! When is the 30b coming?????",
          "score": 8,
          "created_utc": "2026-02-03 16:45:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dica8",
              "author": "pmttyji",
              "text": "\\+1. \n\nI really want to see what & how much difference the Next architecture makes? Like t/s difference between **Qwen3-Coder-30B** vs **Qwen3-Coder-Next-30B** ....",
              "score": 14,
              "created_utc": "2026-02-03 17:25:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dmji8",
                  "author": "R_Duncan",
                  "text": "It's not about t/s, maybe these are even slower for zero context, but use delta gated attention so kv cache is linear: context takes much less cache (like between 8k of other models) and do not grow much when increasing. Also, when you use long context, t/s don't drop that much. Reports are that these kind of models, despite using less VRAM, are way better in bench for long context like needle in haystack.",
                  "score": 10,
                  "created_utc": "2026-02-03 17:44:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dw23o",
          "author": "Danmoreng",
          "text": "Updated my Windows Powershell llama.cpp install and run script to use the new Qwen3-coder-next and automatically launch qwen-code. https://github.com/Danmoreng/local-qwen3-coder-env",
          "score": 3,
          "created_utc": "2026-02-03 18:27:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e5rh8",
          "author": "Aggressive-Bother470",
          "text": "I thought you'd forgotten about us, Qwen :D",
          "score": 3,
          "created_utc": "2026-02-03 19:11:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e80ws",
          "author": "kwinz",
          "text": "Hi! Sorry for the noob question,\nbut how does a model with this low number of active parameters affect VRAM usage?\n\nIf only 3B/80B parameters are active simultaneously, does it get meaningful acceleration on e.g. a 16GB VRAM card? (provided the rest can fit into system memory)?\n\nOr is it hard to predict which parameters will become active and the full model should be in VRAM for decent speed?\n\nIn other words can I get away with a quantization where only the active parameters, cache and context fit into VRAM, and the rest can spill into system memory, or will that kill performance?",
          "score": 3,
          "created_utc": "2026-02-03 19:21:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3g5mai",
              "author": "arades",
              "text": "When you offload moe layers to CPU, it's the whole layer, it doesn't swap the active tensors to the GPU. So the expert layers run at system ram/CPU inference speed, and the layers on GPU run at GPU speed. However, since there's only 3B active, the CPU isn't going to need to go very fast, and the ram speed isn't as important since it's loading so little. So, you should still get acceptable speeds even with most of the weights on the CPU.\n\nWhat's most important about these next models is the attention architecture. It's slower up front, and benefits most from loading on the GPU, but it's also much more memory efficient, and inference doesn't slow down nearly as much as it fills. This means you can keep probably the full 256k context on a 16GB GPU and maintain high performance for the entire context window.",
              "score": 2,
              "created_utc": "2026-02-04 01:08:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3foikg",
          "author": "JoNike",
          "text": "So I tried the mxfp4 on my 5080 16gb. I got 192gb of ram.\n\nLoaded 15 layers on gpu, kept the 256k context and offloaded the rest on my RAM.\n\nIt's not fast as I could have expected, 11t/s. But it seems pretty good from the first couple tests. \n\nI think I will use it with my openclaw agent to give it a space to code at night without going through my claude tokens.",
          "score": 3,
          "created_utc": "2026-02-03 23:34:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ftkle",
              "author": "BigYoSpeck",
              "text": "Are you offloading MOE expert layers to CPU or just using partial GPU offload for all the layers? Use `-ncmoe 34` if you're not already. You should be closer to 30t/s",
              "score": 6,
              "created_utc": "2026-02-04 00:02:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3g7zzp",
                  "author": "JoNike",
                  "text": "Doesn't seems to do any difference to me. I'll keep an eye on it. Care if I ask what kind of config you're using?\n\nEdit: Actually scratch that, I was doing it wrong, it does boost it quite a lot! Thanks for actually making me look into it!\n\nmy llama.cpp command for my 5080 16gb:\n\n```\n\n\n    llama-server -m Qwen3-Coder-Next-MXFP4_MOE.gguf\n      -c 262144 --n-gpu-layers 48 --n-cpu-moe 36\n      --host 127.0.0.1 --port 8080 -t 16 --parallel 1\n      --cache-type-k q4_0 --cache-type-v q4_0 \n      --mlock --flash-attn on\n```\n\nand this gives me 32.79 t/s!",
                  "score": 5,
                  "created_utc": "2026-02-04 01:22:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3g704l",
          "author": "mdziekon",
          "text": "Speed wise, the Unsloth Q4\\_K\\_XL seems pretty solid (3090 + CPU offload, running on 7950x3D with 64GB of RAM; running latest llama-swap & llama.cpp on Linux). After some minor tuning I was able to achieve:  \n\\- PP (initial ctx load): \\~900t/s  \n\\- PP (further prompts of various size): 90t/s to 330t/s (depends on prompt size, the larger the better)  \n\\- TG (initial prompts): \\~37t/s  \n\\- TG (further, \\~180k ctx): \\~31t/s\n\nCan't say much about output quality yet, so far I was able to fix a simple issue with TS code compilation code using Roo, but I've noticed that from time to time it didn't go deep enough and provided only a partial fix (however, there was no way for the agent to verify whether the solution was actually working). Need to test it further and compare to cloud based GLM4.7",
          "score": 3,
          "created_utc": "2026-02-04 01:16:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lbv8x",
              "author": "PaMRxR",
              "text": "Do you mind sharing the llama-server options? I have a similar setup (except 32GB RAM) and prompt processing is quite slow at ~200t/s.",
              "score": 1,
              "created_utc": "2026-02-04 20:13:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ld2ah",
                  "author": "mdziekon",
                  "text": "Try bumping batch physical size and logical size (-b and -ub) to 4096. It slightly slows down generation, but I found it greatly sped up initial prompt processing.",
                  "score": 1,
                  "created_utc": "2026-02-04 20:19:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3gez8z",
          "author": "ravage382",
          "text": "EDIT:\n\nAfter the llama.cpp fix today, it is doing really well in tool calling and general reasoning. I repeating the same prompt and  got as good of an answer as gpt120b using 4 tool calls to web tools. \n\nPre patch eval:\n\nInitial testing (Q6) with some web tools to do a product search came back with some rather mediocre results. I had it do a product search for corn husker lotion. It did one web search, pulled no actual pages and generated this summary(in which $5.66 was slightly over my price target of $6):  I searched current retail options for Corn Huskers Lotion 7 oz bottles, but the context you provided shows the *lowest price listed* is $9.54 for a single 7 oz bottle on Amazon (about $1.36/fl oz), and a 6-pack on eBay is $33.95‚Äîwell above your $6 target.\n\nGiven the current pricing in the search results:\n\n* [Walmart.com](http://Walmart.com) and Amazon listings didn't specify a price under $6 in the snippets.\n* A 6-pack on eBay ($33.95) averages \\~$5.66 per bottle‚Äîclose but still slightly above $6 *per unit*.\n* No clear evidence of widespread shortage or confirmed sub-$6 deals was found in recent listings.\n\nSince you mentioned a *current shortage*, it‚Äôs possible availability and pricing are inconsistent or fluctuating in real time. I recommend:\n\n* Checking [Walmart.com](http://Walmart.com) or Amazon directly for localized or third-party sellers.\n* Looking at local drugstores (e.g., CVS, Walgreens), discount retailers (e.g., Dollar General, Family Dollar), or grocery stores where shelf prices may differ.\n* Signing up for stock alerts on major sites in case supply improves.\n\nWould you like me to check current prices on a specific retailer (e.g., Walmart, Amazon, or local options)?\n\ngpt120b with the same set of tools and same prompt did 29 tool calls, between searches, page grabs and grabbing a few raw pages and then generated a paragraph summary with the cheapest options.\n\nCoding results look like they are an improvement over gpt120b, with a fully working html tetris clone on its first attempt. gpt120b has yet to manage that one.",
          "score": 3,
          "created_utc": "2026-02-04 02:01:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dc7j1",
          "author": "TacGibs",
          "text": "https://preview.redd.it/owke6orh8bhg1.jpeg?width=640&format=pjpg&auto=webp&s=0256a803ef598d14073957af5762d1d17c872667",
          "score": 15,
          "created_utc": "2026-02-03 16:56:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dcd4z",
              "author": "TacGibs",
              "text": "I know it's already there, but traditions are traditions üòÇ",
              "score": 26,
              "created_utc": "2026-02-03 16:57:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3eacsm",
          "author": "xmikjee",
          "text": "Bless you <3\n\nhttps://preview.redd.it/6se2fnr70chg1.jpeg?width=540&format=pjpg&auto=webp&s=5636bc8ec4ba6a57e35893ba0e9fe6d3c84fe5ac\n\n",
          "score": 6,
          "created_utc": "2026-02-03 19:32:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3d5w43",
          "author": "1ncehost",
          "text": "Wild",
          "score": 2,
          "created_utc": "2026-02-03 16:27:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e60c1",
          "author": "charliex2",
          "text": "did they fix the tool call bug?",
          "score": 2,
          "created_utc": "2026-02-03 19:12:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3h9i4s",
              "author": "ForsookComparison",
              "text": "In my testing, no",
              "score": 2,
              "created_utc": "2026-02-04 05:08:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ha4y2",
                  "author": "charliex2",
                  "text": "welp , thanks for replying",
                  "score": 2,
                  "created_utc": "2026-02-04 05:13:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3fwd41",
          "author": "PANIC_EXCEPTION",
          "text": "It's pretty fast on M1 Max 64 GB MLX. I'm using 4 bits and running it with qwen-code CLI on a pretty big TypeScript monorepo.",
          "score": 2,
          "created_utc": "2026-02-04 00:17:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3il1s9",
              "author": "r1str3tto",
              "text": "Are you able to do anything else with your Mac while it runs? I stopped trying to use Qwen Next 80B (MLX) on my 64GB M3 Max because I was getting too much stutter and freeze in application UI.",
              "score": 1,
              "created_utc": "2026-02-04 12:00:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3rmvbm",
                  "author": "PANIC_EXCEPTION",
                  "text": "Yeah, works fine. I use about half maximum context. If you try to push it to full context, you might get a kernel panic. Make sure your backend never attempts to load multiple LLMs at the same time, that can also cause it.",
                  "score": 1,
                  "created_utc": "2026-02-05 19:08:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3i4kqi",
          "author": "gkon7",
          "text": "Sorry for my ignorance, but I have 96‚ÄØGB of DDR5. Can I get decent performance with an 16‚ÄØGB AMD‚ÄØ9060‚ÄØXT or are these improvements specific to CUDA? Also, in this architecture, does increasing the context cause prompt processing performance to die?",
          "score": 2,
          "created_utc": "2026-02-04 09:37:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i7zcz",
              "author": "BigYoSpeck",
              "text": "I'm running an RX 6800 XT using ROCm on a 64gb DDR4 3600 system and getting about 25tok/s so I would imagine between the higher bandwidth of your DDR5 and lower bandwidth of your 9060 XT you should get somewhere in the same ballpark as me  \n  \nI haven't really tested very long context yet but get over 400tok/s prompt processing on up to a few thousand token prompts",
              "score": 1,
              "created_utc": "2026-02-04 10:09:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3i9j21",
                  "author": "gkon7",
                  "text": "Thanks. 400 tok/s for pp seems very nice actually.",
                  "score": 1,
                  "created_utc": "2026-02-04 10:23:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o49eeb9",
                  "author": "Manaberryio",
                  "text": "Are you using LM studio? Do you mind sharing your settings?",
                  "score": 1,
                  "created_utc": "2026-02-08 14:59:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d1lif",
          "author": "wapxmas",
          "text": "Qwen3 next Implementation still have bugs, qwen team refrains from any contribution to it. I tried it recently on master branch, it was short python function and to my surprise the model was unable to see colon after function suggesting a fix, just hilarious.",
          "score": 6,
          "created_utc": "2026-02-03 16:07:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ecc8b",
              "author": "neverbyte",
              "text": "I think I might be seeing something similar.  I am running the Q6 with lamma.cpp + Cline and unsloth recommended settings.  It will write a source file then say \"the file has some syntax errors\" or \"the file has been corrupted by auto-formatting\" and then it tries to fix it and rewrites the entire file without making any changes, then gets stuck in a loop trying to fix the file indefinitely. Haven't seen this before.",
              "score": 5,
              "created_utc": "2026-02-03 19:42:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3j31fr",
                  "author": "wapxmas",
                  "text": "Today it was fixed finally as I think https://github.com/ggml-org/llama.cpp/pull/19324. Tested my my prompt that revealed the issue - now all work flawlessly. Also tested coder without this fix - I can say I now have local llm that I can use daily even for real tasks, gave the model huge C project - it correctly made architecture document. Did it with roo code.",
                  "score": 3,
                  "created_utc": "2026-02-04 13:52:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ep7eu",
                  "author": "neverbyte",
                  "text": "I'm seeing similar behavior with Q8_K_XL as well so maybe getting this running on vllm is the play here.",
                  "score": 2,
                  "created_utc": "2026-02-03 20:42:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3hpr18",
              "author": "alexeiz",
              "text": "I just tried it in Cline (which I think routes to Openrouter).  My test is to convert some Perl code to Python, and qwen3-coder-next created a working version on the first try, which surprised me.  Usually a smaller model needs to run the generated code a couple of times to fix mistakes.  But this model didn't make any mistakes.",
              "score": 2,
              "created_utc": "2026-02-04 07:19:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d55wp",
              "author": "Terminator857",
              "text": "Which implementation?  MLX, tensor library, llama.cpp?",
              "score": 3,
              "created_utc": "2026-02-03 16:24:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d5fd2",
                  "author": "wapxmas",
                  "text": "llama.cpp, or did you see any other posts on this channel about buggy implementation? Stay tuned.",
                  "score": -15,
                  "created_utc": "2026-02-03 16:25:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dr5u7",
          "author": "bobaburger",
          "text": "https://preview.redd.it/oniow1pokbhg1.png?width=738&format=png&auto=webp&s=edd5b991e1ab2af97e151bf4db556159d97e077c\n\ndamn it, now i have to buy a new GPU",
          "score": 2,
          "created_utc": "2026-02-03 18:05:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dsw4t",
              "author": "strosz",
              "text": "Works fine if you have 64gb or more RAM with your 5060ti 16GB and can take a short break for the answer. Got a response in under 1 minute for an easy test at least, but more context will take a good coffe break probably",
              "score": 6,
              "created_utc": "2026-02-03 18:12:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3eoluc",
              "author": "arcanemachined",
              "text": "Weird that the tool doesn't allow you to add RAM into the mix.",
              "score": 1,
              "created_utc": "2026-02-03 20:39:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3edc5t",
          "author": "Rascazzione",
          "text": "Anyone knows what‚Äôs the difference between FP8 and FP8 dynamic?\n\nThanks",
          "score": 2,
          "created_utc": "2026-02-03 19:46:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dciex",
          "author": "Hoak-em",
          "text": "Full-local setup idea: nemotron-orchestrator-8b running locally on your computer (maybe a macbook), this running on a workstation or gaming PC, orchestrator orchestrates a buncha these in parallel -- could work given the sparsity, maybe even with a CPU RAM+VRAM setup for Qwen3-Coder-Next. Just gotta figure out how to configure the orchestrator harness correctly -- opencode could work well as a frontend for this kinda thing",
          "score": 3,
          "created_utc": "2026-02-03 16:58:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dhtao",
          "author": "popiazaza",
          "text": "Finally, a Composer 2 model. ^\\s",
          "score": 1,
          "created_utc": "2026-02-03 17:22:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dj4tf",
          "author": "Thrumpwart",
          "text": "If these benchmarks are accurate this is incredible. Now I need's me a 2nd chonky boi W7900 or an RTX Pro.",
          "score": 1,
          "created_utc": "2026-02-03 17:28:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dk49h",
          "author": "DeedleDumbDee",
          "text": "Is there a way to set this up in VScode as a custom agent?",
          "score": 1,
          "created_utc": "2026-02-03 17:33:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3docll",
              "author": "Educational_Sun_8813",
              "text": "you can setup any model with openapi compatible llama-server",
              "score": 3,
              "created_utc": "2026-02-03 17:52:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dlkl6",
          "author": "R_Duncan",
          "text": "Waiting for u/noctrex ....",
          "score": 1,
          "created_utc": "2026-02-03 17:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ent7n",
              "author": "noctrex",
              "text": "Got it up: [https://huggingface.co/noctrex/Qwen3-Coder-Next-MXFP4\\_MOE-GGUF](https://huggingface.co/noctrex/Qwen3-Coder-Next-MXFP4_MOE-GGUF)",
              "score": 6,
              "created_utc": "2026-02-03 20:36:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fo9u3",
                  "author": "NoahFect",
                  "text": "Any idea how this compares to Unsloth's UD Q4 version?",
                  "score": 1,
                  "created_utc": "2026-02-03 23:33:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dvlha",
              "author": "noctrex",
              "text": "Oh no, gonna take couple of hours..",
              "score": 5,
              "created_utc": "2026-02-03 18:24:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3doik2",
          "author": "corysama",
          "text": "I'm running 64 GB of CPU RAM and a 4090 with 24 GB of VRAM.\n\nSo.... I'm good to run which GGUF quant?",
          "score": 1,
          "created_utc": "2026-02-03 17:53:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3duv1q",
              "author": "pmttyji",
              "text": ">It runs on **46GB RAM**/VRAM/unified memory (85GB for 8-bit), is non-reasoning for ultra-quick code responses. We introduce new MXFP4 quants for great quality and speed and you‚Äôll also learn how to run the model on Codex & Claude Code. - [Unsloth guide](https://unsloth.ai/docs/models/qwen3-coder-next)",
              "score": 3,
              "created_utc": "2026-02-03 18:21:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dw9be",
              "author": "Danmoreng",
              "text": "yup works fine. just tested the UD Q4 variant which is ~50GB on my 64GB RAM + 5080 16GB VRAM",
              "score": 3,
              "created_utc": "2026-02-03 18:27:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dy7sj",
                  "author": "pmttyji",
                  "text": "More stats please. t/s, full command, etc.,",
                  "score": 3,
                  "created_utc": "2026-02-03 18:36:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dz26v",
          "author": "Far-Low-4705",
          "text": "holy sheet",
          "score": 1,
          "created_utc": "2026-02-03 18:40:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eng0o",
          "author": "Alternative-Theme885",
          "text": "whoa\n\n",
          "score": 1,
          "created_utc": "2026-02-03 20:34:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f67ur",
          "author": "No_Mango7658",
          "text": "Oh wow. 80b-a3b!\n\nAmazing",
          "score": 1,
          "created_utc": "2026-02-03 22:01:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fjrm7",
          "author": "billy_booboo",
          "text": "This is what I've been waiting for. Guess it's time to buy that dgx spark ü´†",
          "score": 1,
          "created_utc": "2026-02-03 23:09:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fnfx9",
          "author": "adam444555",
          "text": "Testing around with with the MXFP4\\_MOE version.\n\nHardware: 5090 9800x3D 32GB RAM\n\nDeploy config: 65536 ctx, kvc dtype fp16, 17 moe layer offload\n\nIt works surprisingly well even with MOE layer offload.\n\nI haven't do a comprehensive benchmark, but just using it in claude code.\n\nHere is a log with significant read and write tokens.\n\nprompt eval time =   29424.73 ms / 15089 tokens (    1.95 ms per token,   512.80 tokens per second)\n\neval time =   22236.64 ms /   647 tokens (   34.37 ms per token,    29.10 tokens per second)",
          "score": 1,
          "created_utc": "2026-02-03 23:28:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3geam7",
              "author": "DOAMOD",
              "text": "prompt eval time =    7038.33 ms /  3864 tokens (    1.82 ms per token,   548.99 tokens per second)\n\neval time =    1726.58 ms /    66 tokens (   26.16 ms per token,    38.23 tokens per second)\n\ntotal time =    8764.91 ms /  3930 tokens\n\nslot      release: id  2 | task 421 | stop processing: n\\_tokens = 26954, truncated = 0\n\nNice",
              "score": 1,
              "created_utc": "2026-02-04 01:57:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gevo1",
                  "author": "DOAMOD",
                  "text": "prompt eval time =    2682.17 ms /   773 tokens (    3.47 ms per token,   288.20 tokens per second)\n\neval time =    1534.91 ms /    57 tokens (   26.93 ms per token,    37.14 tokens per second)\n\ntotal time =    4217.08 ms /   830 tokens\n\nslot      release: id  2 | task 766 | stop processing: n\\_tokens = 60567, truncated = 0",
                  "score": 1,
                  "created_utc": "2026-02-04 02:00:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ibqx5",
              "author": "adam444555",
              "text": "Actually get much better speed by swtiching from WSL2 to windows. Crazy how bad WSL2 is to serve model",
              "score": 1,
              "created_utc": "2026-02-04 10:43:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3g9k87",
          "author": "street_melody",
          "text": "Since it is MoE can it run on smaller gpus with q4km?",
          "score": 1,
          "created_utc": "2026-02-04 01:31:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gfgeg",
          "author": "robberviet",
          "text": "80b. Much welcome than the 500b.",
          "score": 1,
          "created_utc": "2026-02-04 02:04:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gp5k1",
          "author": "dragonmantank",
          "text": "I'm gonna be honest, this came out at the best possible time. I'm currently between Claude timeouts, and been playing more and more with local LLMs. I've got the Q4\\_K\\_XL quant running from unsloth on one of the older Minisforum AI X1 Pros and this thing is blowing other models out of the water. I've had so much trouble getting things to run in Kilo Code I was honestly beginning to question the viability of a coding assistant.",
          "score": 1,
          "created_utc": "2026-02-04 02:59:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gpvs6",
          "author": "Kasatka06",
          "text": "Result with 4x3090 seems fasst, faster than glm 4.7\n\n\n\ncommand: \\[\n\n\"/models/unsloth/Qwen3-Coder-Next-FP8-Dynamic\", \n\n\"--disable-custom-all-reduce\",\n\n\"--max-model-len\",\"70000\", \n\n\"--enable-auto-tool-choice\", \n\n\"--tool-call-parser\",\"qwen3\\_coder\",\n\n\"--max-num-seqs\", \"8\",\n\n\"--gpu-memory-utilization\", \"0.95\", \n\n\"--host\", \"0.0.0.0\",\n\n\"--port\", \"8000\",\n\n\"--served-model-name\", \"local-model\", \n\n\"--enable-prefix-caching\",\n\n\"--tensor-parallel-size\", \"4\",      # 2 GPUs per replica\n\n\"--max-num-batched-tokens\", \"8096\",\n\n'--override-generation-config={\"top\\_p\":0.95,\"temperature\":1.0,\"top\\_k\":40}',\n\n\\]\n\n\n\n\n\n| model        |           test |              t/s |       ttfr (ms) |    est\\_ppt (ms) |   e2e\\_ttft (ms) |\n\n|:-------------|---------------:|-----------------:|----------------:|----------------:|----------------:|\n\n| local-model |         pp2048 | 3043.21 ¬± 221.64 |  624.66 ¬± 49.46 |  615.79 ¬± 49.46 |  624.79 ¬± 49.45 |\n\n| local-model |           tg32 |   121.99 ¬± 10.93 |                 |                 |                 |\n\n| local-model | pp2048 @ d4096 |  3968.76 ¬± 45.41 | 1411.31 ¬± 10.72 | 1402.43 ¬± 10.72 | 1411.45 ¬± 10.80 |\n\n| local-model |   tg32 @ d4096 |    105.47 ¬± 0.63 |                 |                 |                 |\n\n| local-model | pp2048 @ d8192 |  4178.73 ¬± 33.56 |  2192.20 ¬± 6.25 |  2183.32 ¬± 6.25 |  2192.46 ¬± 6.12 |\n\n| local-model |   tg32 @ d8192 |    104.26 ¬± 0.23 |                 |                 |                 |",
          "score": 1,
          "created_utc": "2026-02-04 03:03:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ibqmy",
              "author": "MinusKarma01",
              "text": "Is the 121.99 tok/s generation speed for one sequence or several?",
              "score": 1,
              "created_utc": "2026-02-04 10:43:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3j94l9",
                  "author": "Kasatka06",
                  "text": "Iam not sure, i just run llama benchy test into the vllm endpoint",
                  "score": 1,
                  "created_utc": "2026-02-04 14:24:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3gttf6",
          "author": "RayanAr",
          "text": "is it better than KIMI K2.5?",
          "score": 1,
          "created_utc": "2026-02-04 03:26:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gvj4z",
          "author": "mitch_feaster",
          "text": "Has anyone tried this out? How's the Claude Code experience?",
          "score": 1,
          "created_utc": "2026-02-04 03:36:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hgdgr",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-04 06:00:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hheu7",
              "author": "MichaelBui2812",
              "text": "Have you tried MiniMax v2 or v2.1 about the same?",
              "score": 1,
              "created_utc": "2026-02-04 06:08:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3htb98",
          "author": "k_means_clusterfuck",
          "text": "long long man!",
          "score": 1,
          "created_utc": "2026-02-04 07:51:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3idxw5",
          "author": "lumos675",
          "text": "I have 32gb vram only üò≠",
          "score": 1,
          "created_utc": "2026-02-04 11:02:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iemxs",
          "author": "DOAMOD",
          "text": "A bug in one function was fixed and it was working correctly, it looks promising and maintains a speed of 35/40tg 128k",
          "score": 1,
          "created_utc": "2026-02-04 11:09:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ifq8f",
          "author": "Wrong_Library_8857",
          "text": "tbh I'm curious if the jump from 2.5 to 3 is actually noticeable for local use or if it's mostly benchmark optimization. Anyone run it yet on something practical like refactoring or multi-file edits?",
          "score": 1,
          "created_utc": "2026-02-04 11:18:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3inyql",
          "author": "Ideabile",
          "text": "Does anybody know if this can run on a Macbook Pro M2 Max 64GB?",
          "score": 1,
          "created_utc": "2026-02-04 12:21:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k0fyn",
          "author": "laterbreh",
          "text": "FP8 version tensor parallel in vllm nightly on 2 rtx pros on a simple \"build single landing page in html for <insert subject>\" spit out 170 tokens per second.",
          "score": 1,
          "created_utc": "2026-02-04 16:35:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3npc88",
          "author": "Clear_Lead4099",
          "text": "This model is not good. At least for me. I use LLMs to help me code in Dart, and this turd couldn't write a simple app of bouncing ball I asked it to do. Used their recommended parameters for llama.cpp. I gave up after my 4th corrective prompt. The speed is good, yes, but who cares about speed when model is fucking dumb?! In contrast: GLM 4.6/7 and Minimax M2.1 nailed it in 1-2 prompts.",
          "score": 1,
          "created_utc": "2026-02-05 03:52:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gf7oe",
          "author": "pravbk100",
          "text": "Seems to have knowledge till june 2024. I tried it on huggingface about latest versions and here are the replies:\n\n1. Swift :¬†As of¬†June 2024, the¬†latest stable version of the Swift programming language is¬†5.10.\n\n2. React native :¬†As of¬†June 2024, the¬†latest stable version of React Native is¬†0.74.1, released on¬†June 13, 2024.\n\n3. Python :¬†As of¬†June 2024, the¬†latest stable version of Python is¬†3.12.3, released on¬†June 3, 2024.",
          "score": 1,
          "created_utc": "2026-02-04 02:02:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gjgn4",
          "author": "Old-Nobody-2010",
          "text": "How much **VRAM** do I need to run **Qwen-Code-Next** so I can use **OpenCode** to help me write code",
          "score": 0,
          "created_utc": "2026-02-04 02:27:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzjbw2",
      "title": "I built a rough .gguf LLM visualizer",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qzjbw2",
      "author": "sultan_papagani",
      "created_utc": "2026-02-08 20:08:31",
      "score": 669,
      "num_comments": 40,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qzjbw2/i_built_a_rough_gguf_llm_visualizer/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4doqm0",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-09 04:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4b8mbk",
          "author": "Educational_Sun_8813",
          "text": "maybe someone will be interested to see the code: https://github.com/Sultan-papagani/gguf-visualizer/tree/main\n\nbesides i'm aware of this: https://poloclub.github.io/transformer-explainer/",
          "score": 30,
          "created_utc": "2026-02-08 20:22:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4cji5q",
              "author": "jklre",
              "text": "to the top!",
              "score": 2,
              "created_utc": "2026-02-09 00:33:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4bbx41",
          "author": "DisjointedHuntsville",
          "text": "Really good job and thank you for taking the time to share :) I believe neuron pedia from Anthropic which is open source now is also a good contribution to explainability approaches: [https://www.neuronpedia.org/gemma-2-2b/graph?slug=nuclearphysicsis-1766322762807&pruningThreshold=0.8&densityThreshold=0.99](https://www.neuronpedia.org/gemma-2-2b/graph?slug=nuclearphysicsis-1766322762807&pruningThreshold=0.8&densityThreshold=0.99)\n\nWe have certainly not begun to scratch the surface of explainability in these models just yet and please keep sharing all the cool things you discover with the community since it really helps when there are more eyes on this stuff !",
          "score": 62,
          "created_utc": "2026-02-08 20:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4bej4j",
              "author": "JEs4",
              "text": "Just pointing out that Neuronpedia isn‚Äôt by Anthropic. They‚Äôre a contributor but this guy is behind it: https://www.johnnylin.co/",
              "score": 28,
              "created_utc": "2026-02-08 20:52:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4bo9im",
                  "author": "IrisColt",
                  "text": "This.",
                  "score": -6,
                  "created_utc": "2026-02-08 21:40:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4bezdg",
              "author": "JEs4",
              "text": "Whoops didn‚Äôt mean to double post. But yeah Neuonpedia is really neat. Using SAE models with their lookups was helpful during my abliteration research.\n\nhttps://preview.redd.it/lm0br8493cig1.jpeg?width=2755&format=pjpg&auto=webp&s=9d2a21eb771b93c17af06a0cfef9ec6fcc99d30c",
              "score": 17,
              "created_utc": "2026-02-08 20:54:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4bgy0w",
                  "author": "sultan_papagani",
                  "text": "this is really cool. thanks!",
                  "score": 3,
                  "created_utc": "2026-02-08 21:03:59",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4b5zwx",
          "author": "sultan_papagani",
          "text": "[website link](https://sultan-papagani.github.io/gguf-visualizer/)",
          "score": 13,
          "created_utc": "2026-02-08 20:09:02",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4c3kn6",
              "author": "AbheekG",
              "text": "Thanks so much for sharing!",
              "score": 5,
              "created_utc": "2026-02-08 23:01:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4b7ucq",
          "author": "Aggressive-Bother470",
          "text": "Cool.¬†",
          "score": 7,
          "created_utc": "2026-02-08 20:18:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4c5djr",
          "author": "o0genesis0o",
          "text": "Cool work! \n\nWould it be possible to, say, capture the activations of a run and playback to see the connections lighting up? My colleague has been fantasizing about some sorts of VR that allows him to sit and see the neural network lighting up as the token being processed. He imagined it would help with explainability. ",
          "score": 3,
          "created_utc": "2026-02-08 23:11:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ckr2p",
          "author": "Every_Abalone5692",
          "text": "Awesome work!",
          "score": 3,
          "created_utc": "2026-02-09 00:40:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bfblq",
          "author": "RoyalCities",
          "text": "This is very cool! Love visualizers like this. Would like to see if you could support other model types down the line but as is this is fantastic.\n\nOutside of just llms I mean. Like Image, video or audio models etc. where it's not all unified but it's say a t5 separately connecting to a Unet or DiT via cross attention. Maybe showing those connections and all that from a high level.\n\nNonetheless great work.",
          "score": 4,
          "created_utc": "2026-02-08 20:55:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bfsgj",
          "author": "MelodicRecognition7",
          "text": "cool!",
          "score": 2,
          "created_utc": "2026-02-08 20:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bk989",
          "author": "thatguy122",
          "text": "Love this. Reminds me a cyberpunk-esk hacking mini game.¬†",
          "score": 2,
          "created_utc": "2026-02-08 21:20:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bobp0",
          "author": "IrisColt",
          "text": "Thanks!!! I love it!",
          "score": 2,
          "created_utc": "2026-02-08 21:40:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bt34m",
          "author": "scottgal2",
          "text": "Awesome job!",
          "score": 2,
          "created_utc": "2026-02-08 22:04:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4cs6u8",
          "author": "paul_tu",
          "text": "Upvote for an effort",
          "score": 2,
          "created_utc": "2026-02-09 01:22:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ehgc4",
          "author": "Chromix_",
          "text": "A few months ago someone built something that doesn't just visualize it statically, but dynamically shows patterns and connections with activations. Here's one of the [earlier versions](https://www.reddit.com/r/LocalLLaMA/comments/1poybe9/llama_32_3b_mri_build_progress/). There were a bunch more investigative posts where the author used the extended tool to find and visualize patterns, like nodes being responsible for certain things, or being more sensitive to quantization. Unfortunately the account was deleted recently, making it difficult to find all the latest posts on that.\n\nSo, visualizing static properties clearly has its benefits, and another take at the dynamic visualization could also yield nice results.",
          "score": 2,
          "created_utc": "2026-02-09 08:25:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4eoab7",
          "author": "clawdvine-intern",
          "text": "oh man ive been wanting something like this forever. i always feel like im just blindly throwing quant levels at gguf files and hoping for the best lol. being able to actually see whats going on inside would be huge for figuring out why certain layers just tank quality when you go below Q5. is there any way to compare two files side by side? like original vs quantized? that would be the dream tbh",
          "score": 2,
          "created_utc": "2026-02-09 09:33:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4eug2r",
          "author": "renntv",
          "text": "Development of AI is so fast, but visualization to help explain what's happening are really lacking. I collect everything I can find that helps people to better understand the AI black box here: [https://dentro.de/ai/visualizations/](https://dentro.de/ai/visualizations/)  \nBrendan Bycroft is the GOAT, but his project is already 2 years old and not much emerged after it.   \nGreat to see the subject pop up again and your way of visualizing is pretty clever!",
          "score": 2,
          "created_utc": "2026-02-09 10:34:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bf1lo",
          "author": "SlowFail2433",
          "text": "Visualisation looks nice",
          "score": 2,
          "created_utc": "2026-02-08 20:54:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4btt3k",
              "author": "harrro",
              "text": "Yep, worked well on a 1.5B GGUF model I just tested.\n\n/u/sultan_papagani The 'walk' mode is super fast on my Firefox browser - i just barely touch the WSAD keys and it flies across the screen (sprint mode is even worse) which made it hard to move around though.\n\nNot sure if its because it was a small model or because my framerate is really high (ie: you're moving X units per tick and I'm well over 60fps) or just a Firefox thing.",
              "score": 3,
              "created_utc": "2026-02-08 22:08:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4buo5m",
                  "author": "sultan_papagani",
                  "text": "thanks! use scrollwheel to slow down",
                  "score": 3,
                  "created_utc": "2026-02-08 22:12:41",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4cpu58",
          "author": "ANR2ME",
          "text": "Interesting project üëç i wished there are more variation of models, like MoE or hybrid models with mamba layers (said to be more sensitive to quantization) for example.\n\n~~Btw, are you planning to open source this project later? ü§î~~\n\nEdit: is this the repo? https://github.com/bbycroft/llm-viz",
          "score": 1,
          "created_utc": "2026-02-09 01:09:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4cq5is",
              "author": "sultan_papagani",
              "text": "thanks for the feedback! [repo link](https://github.com/Sultan-papagani/gguf-visualizer)",
              "score": 2,
              "created_utc": "2026-02-09 01:10:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4dcsyd",
          "author": "Much-Researcher6135",
          "text": "That's sick, can you tell a bit about how you made it? I'm getting more and more interested in 3d dataviz and have no idea where to look for pointers.",
          "score": 1,
          "created_utc": "2026-02-09 03:12:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4dlepo",
          "author": "Alarming_Bluebird648",
          "text": "Mapping the tensor dimensions visually makes it much easier to verify layer architecture than scanning through metadata strings. Do you plan on adding support for inspecting weight distribution histograms per layer?",
          "score": 1,
          "created_utc": "2026-02-09 04:03:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4e42ak",
          "author": "FeiX7",
          "text": "make it mouse controlled instead of keyboard please.",
          "score": 1,
          "created_utc": "2026-02-09 06:21:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4eb4mx",
          "author": "HarjjotSinghh",
          "text": "this is either a masterpiece or a glitch. both equally impressive.",
          "score": 1,
          "created_utc": "2026-02-09 07:24:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4b7n5o",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -8,
          "created_utc": "2026-02-08 20:17:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4b824p",
              "author": "sultan_papagani",
              "text": "[repo link](https://github.com/Sultan-papagani/gguf-visualizer)",
              "score": 9,
              "created_utc": "2026-02-08 20:19:20",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4b7uqo",
              "author": "sultan_papagani",
              "text": "its offline. github pages, just simple html and js that runs on your browser. you can download it too",
              "score": 9,
              "created_utc": "2026-02-08 20:18:17",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4bdehd",
              "author": "o5mfiHTNsH748KVq",
              "text": "I can‚Äôt answer for OP, but I do this because, frankly, I need some fodder on my website for jobs/hiring people that look at my vanity url when I apply.\n\nGotta play the game a little bit. At least they released it as open source :)",
              "score": 4,
              "created_utc": "2026-02-08 20:46:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4bej2d",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -2,
                  "created_utc": "2026-02-08 20:52:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4cc5bg",
              "author": "666666thats6sixes",
              "text": "It doesn't, though? It only reads the gguf header, which is up to tens MiB (not \"a few hundred kilobytes\") in size depending on the size of the kv arrays, it stops reading once the header has been parsed.\n\nTried it with BF16 GLM-4.7, it read just 9466496 bytes, because that's how large the header is.",
              "score": 2,
              "created_utc": "2026-02-08 23:52:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ccmqo",
                  "author": "FullstackSensei",
                  "text": "OK, mea culpa",
                  "score": 1,
                  "created_utc": "2026-02-08 23:54:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qz23pp",
      "title": "PR opened for Qwen3.5!!",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/r10pwm02y7ig1.png",
      "author": "Mysterious_Finish543",
      "created_utc": "2026-02-08 06:57:13",
      "score": 607,
      "num_comments": 72,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o48bmue",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-08 10:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47sc2i",
          "author": "Betadoggo_",
          "text": "It also uses semi linear attention similar to qwen3-next\n\nhttps://preview.redd.it/bms5k1m018ig1.png?width=1401&format=png&auto=webp&s=9c1284766c41effa9206ce5416808f52152ae655\n\n",
          "score": 95,
          "created_utc": "2026-02-08 07:13:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4847mq",
              "author": "Midaychi",
              "text": "hopefully the max positional embeddings is a placeholder and the max context isn't 32768",
              "score": 16,
              "created_utc": "2026-02-08 09:04:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49rd9k",
                  "author": "trusty20",
                  "text": "How is 32k+ sequence length performance these days? The last few times I checked in on local 32k+ models there was a huge dropoff cliff in context recollection accuracy, it seemed like only the massive models could actually be reliable above 32k+ have we punched through that? What's the ceiling thought to be now for accuracy important stuff?",
                  "score": 8,
                  "created_utc": "2026-02-08 16:06:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4c9eis",
                  "author": "dreamkast06",
                  "text": "It might just be for the base model. Lots of them are trained on 32k the instruct tuned to a decent context length.",
                  "score": 2,
                  "created_utc": "2026-02-08 23:35:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o48x4z7",
              "author": "Iory1998",
              "text": "Well, that's the direction at the moment. I mean, look at Qwen3-Next and especially Kimi Linear. ",
              "score": 9,
              "created_utc": "2026-02-08 13:16:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o485dx9",
              "author": "cibernox",
              "text": "To understand what means in practice semi linear attention, can I expect roughly for context to take less space and thus token generation to be faster for a given context? Would the processing of a request with the same long promt also be faster?",
              "score": 6,
              "created_utc": "2026-02-08 09:15:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48k9iu",
                  "author": "PuppyGirlEfina",
                  "text": "Linear attention is O(1). Constant memory and each token computes in the same time. I assume semi means hybrid, so it might be more like O(log N), so better scaling than Attention's O(N).",
                  "score": 2,
                  "created_utc": "2026-02-08 11:35:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47s1v1",
          "author": "lly0571",
          "text": "We may have Qwen3.5-9B-Instruct and Qwen3.5-35B-A3B-Instruct later?\n\nLooks that Qwen3.5 may use a 248k sized vocab, which might be helpful for multilingual performance, and both of the dense model and moe model would use the the hybrid attention from Qwen3-Next.",
          "score": 57,
          "created_utc": "2026-02-08 07:10:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4cwro3",
              "author": "chibop1",
              "text": "Opus-4.6: Here's the full analysis. The key highlights from PR #43830:\n\n**Two model families** are being added ‚Äî a **dense Qwen3.5** (reference: 9B-Instruct) and a **MoE Qwen3.5** (reference: 35B-A3B-Instruct, 256 experts with top-8 routing).\n\n**The standout architectural feature** is a **hybrid attention design** ‚Äî approximately 75% of layers use **Gated DeltaNet linear attention** (a recurrent mechanism with causal conv1d, similar in spirit to Mamba-style state space models) while every 4th layer uses standard **full softmax attention** with GQA. This gives sub-quadratic complexity for most layers while retaining full attention's expressiveness periodically.\n\nOther notable details:\n- **Partial RoPE** ‚Äî only 25% of the 256-dim head gets rotary embeddings\n- **M-RoPE** (3D position encoding: temporal, height, width) for multimodal inputs\n- **Vision encoder** inherited from Qwen3-VL (27-layer ViT, patch size 16, spatial merge 2√ó2)\n- The models build on the already-merged **Qwen3-Next** architecture (PR #40771), with Qwen3.5 refactoring the projection structure in the DeltaNet module (separate `in_proj_qkv`, `in_proj_z`, `in_proj_\n\nMore detail: https://claude.ai/public/artifacts/93b0a136-fe1c-4077-892b-291bb90026f2",
              "score": -1,
              "created_utc": "2026-02-09 01:48:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o481g74",
          "author": "dampflokfreund",
          "text": "Super exciting, being finally native multimodal and using the latest architecture. this one should be gooood",
          "score": 29,
          "created_utc": "2026-02-08 08:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48i1of",
              "author": "simracerman",
              "text": "Isn‚Äôt Qwen3-Next already doing both?",
              "score": 4,
              "created_utc": "2026-02-08 11:15:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48in1a",
                  "author": "tarruda",
                  "text": "All Qwen3-Next releases so far were text only",
                  "score": 17,
                  "created_utc": "2026-02-08 11:20:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47vhnt",
          "author": "jamaalwakamaal",
          "text": "qWhen !!",
          "score": 60,
          "created_utc": "2026-02-08 07:42:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48hyhi",
              "author": "simracerman",
              "text": "G(when)GUF?!",
              "score": 18,
              "created_utc": "2026-02-08 11:14:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4b0cwh",
                  "author": "MrPecunius",
                  "text": "¬øQwandoMLX?",
                  "score": 5,
                  "created_utc": "2026-02-08 19:41:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o48osos",
              "author": "LinkSea8324",
              "text": "Usually a week after the PR is opened",
              "score": 5,
              "created_utc": "2026-02-08 12:14:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49toqa",
                  "author": "x0wl",
                  "text": "Can be faster if it's similar enough to Qwen3-Next",
                  "score": 3,
                  "created_utc": "2026-02-08 16:17:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49eaiq",
              "author": "Total_Laugh_1487",
              "text": "Qwin!",
              "score": 3,
              "created_utc": "2026-02-08 14:59:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4dodjv",
              "author": "nialv7",
              "text": "probably a new year's present for Chinese New Year",
              "score": 3,
              "created_utc": "2026-02-09 04:22:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o47tz3a",
          "author": "Significant_Fig_7581",
          "text": "Can't wait!!!!! Finally!!!!!",
          "score": 20,
          "created_utc": "2026-02-08 07:28:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o481pn4",
          "author": "darkpigvirus",
          "text": "wishing for Qwen 3.5 2B A350M if it is possible üçÄ",
          "score": 22,
          "created_utc": "2026-02-08 08:41:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48c4cb",
              "author": "_-_David",
              "text": "That is specific enough to pique my curiosity. Why that size specifically?",
              "score": 10,
              "created_utc": "2026-02-08 10:19:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48icpw",
                  "author": "jikilan_",
                  "text": "To run in his Nokia 3310, I think",
                  "score": 38,
                  "created_utc": "2026-02-08 11:18:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o496ryf",
              "author": "FeiX7",
              "text": "what A350M means?",
              "score": 1,
              "created_utc": "2026-02-08 14:16:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4975av",
                  "author": "darkpigvirus",
                  "text": "for an moe model the a350m means is that for each token the active parameters that is involved and active is only 350m instead of using all the 2 billion parameters so that to speed up the inference and only use the experts where they are deemed much more effective. idk if i explain it as the experts like but i did what i can",
                  "score": 3,
                  "created_utc": "2026-02-08 14:18:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48t4wz",
          "author": "QuackerEnte",
          "text": "https://preview.redd.it/7h263s4uo9ig1.jpeg?width=868&format=pjpg&auto=webp&s=99076a4dbda46aac08528b6b6224fb44d1e43f13\n\nYay 2B VL model",
          "score": 6,
          "created_utc": "2026-02-08 12:48:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47tfr3",
          "author": "arcanemachined",
          "text": "Very cool. I haven't used the Qwen \"next\" models much myself, but I heard a lot of complaints initially. (Mostly since it took llama.cpp so long to upstream the changes required to support the new architecture, I assume.)\n\nNow that they've been out for a while, can anyone speak to the pros and cons of the new architecture? Is it better? Are there any drawbacks?",
          "score": 13,
          "created_utc": "2026-02-08 07:23:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47uqxx",
              "author": "Mysterious_Finish543",
              "text": "The recent `qwen3-next-coder` model is pretty good, especially for the size. In its class, there are no comparable models. In terms of proprietary models, my vibe is that it sits somewhere around `claude-sonnet-4`?\n\nIt's also great that the `qwen3-next` architecture makes KV cache memory usage very efficient over long sequences, so it's possible to run it on long context on consumer hardware.\n\nThe initial Instruct and Thinking releases weren't super exciting though. Particularly the thinking model was a bit of a disappointment, very long CoT (mostly just repetition) and not very good at agents (compared to something like `gpt-oss-120b`). Seemed to be ultra-optimized for math and coding competition type problems.",
              "score": 20,
              "created_utc": "2026-02-08 07:35:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o480fnd",
                  "author": "Odd-Ordinary-5922",
                  "text": "from what I remember tho is that the initial 80b model was trained using 15T tokens when usually their models are trained on 35 Trillion or smth around there.",
                  "score": 8,
                  "created_utc": "2026-02-08 08:28:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o480acd",
                  "author": "kweglinski",
                  "text": "next also had awful sycophancy to the point it was annoying to read but I don't see it with coder next.",
                  "score": 3,
                  "created_utc": "2026-02-08 08:27:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4aeq89",
          "author": "ilintar",
          "text": "Note that I'm doing this without any support, just based on Transformers code and my conversion guidelines + Opus 4.6, but I'm aiming for 0-day support this time:\n\n[https://github.com/ggml-org/llama.cpp/pull/19435](https://github.com/ggml-org/llama.cpp/pull/19435)",
          "score": 10,
          "created_utc": "2026-02-08 17:59:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47z1t9",
          "author": "abdouhlili",
          "text": "Looks like 3.5 will kill VL models.",
          "score": 9,
          "created_utc": "2026-02-08 08:15:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48iu5b",
              "author": "tarruda",
              "text": "Do you mean there won't be any 3.5 VL releases?",
              "score": 2,
              "created_utc": "2026-02-08 11:22:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49uvx4",
                  "author": "x0wl",
                  "text": "It's VL [**https://github.com/bozheng-hit/transformers/blob/qwen3\\_5/src/transformers/models/qwen3\\_5/configuration\\_qwen3\\_5.py#L198**](https://github.com/bozheng-hit/transformers/blob/qwen3_5/src/transformers/models/qwen3_5/configuration_qwen3_5.py#L198)",
                  "score": 4,
                  "created_utc": "2026-02-08 16:23:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o48k0ot",
                  "author": "abdouhlili",
                  "text": "Yes, I think so.",
                  "score": 1,
                  "created_utc": "2026-02-08 11:33:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o49onm3",
          "author": "ilintar",
          "text": "Yummy. Lemme look at it :>",
          "score": 5,
          "created_utc": "2026-02-08 15:52:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o480y7q",
          "author": "mlon_eusk-_-",
          "text": "We are eating good folks",
          "score": 9,
          "created_utc": "2026-02-08 08:33:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4appt4",
          "author": "Admirable-Detail-465",
          "text": "Hopefully they make another model sized similarly to qwen 3 next, that was the perfect size for me",
          "score": 4,
          "created_utc": "2026-02-08 18:49:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48d3b3",
          "author": "CoqueTornado",
          "text": "speculative decoding in lmstudio with qwen3 80B iq4\\_xs +qwen3 0.6B  doesn't work for me with 64gb of ram + 8gb of vram, any thoughts?",
          "score": 3,
          "created_utc": "2026-02-08 10:28:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48ibr4",
              "author": "simracerman",
              "text": "MoE and speculative never worked for me. It‚Äôs already fast enough, I‚Äôd keep SD for strictly larger dense models.",
              "score": 6,
              "created_utc": "2026-02-08 11:17:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48onsn",
                  "author": "muxxington",
                  "text": "As I understand it, moe and conventional speculative decoding generally cannot work, at least not in a meaningful way. This would require an additional layer of speculative expert choosing. However, self-speculative decoding should work with moe, if I am not mistaken.",
                  "score": 1,
                  "created_utc": "2026-02-08 12:13:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49uu6v",
              "author": "ForsookComparison",
              "text": "Spec dec on Qwen3 hasn't worked since the earliest Qwen3 models last year. As soon as the 2507 checkpoints came out it was totally broken and we never got a new updated model small enough to be worth it.",
              "score": 2,
              "created_utc": "2026-02-08 16:23:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4c2q99",
                  "author": "CoqueTornado",
                  "text": "yep, I've done several tests and this is true. They should get back to the roots in this 3.5, I'd like fast and wise answers in my humble laptop :P",
                  "score": 2,
                  "created_utc": "2026-02-08 22:56:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o496btf",
              "author": "colin_colout",
              "text": "also the models need to be very similar in very specific ways (same tokenizer, and should generate similar logprobs) if you're using a draft model. \n\nqwen3-next and qwen3 aren't the same. if they don't use the same tokenizer (which i think they don't), then it's not viable as a draft model.",
              "score": 1,
              "created_utc": "2026-02-08 14:14:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48h563",
          "author": "ab2377",
          "text": "exciting.",
          "score": 3,
          "created_utc": "2026-02-08 11:07:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48mdy5",
          "author": "Full_Ad693",
          "text": "Curious how 3.0 improves on 2.5. Anyone tested on AMD yet?",
          "score": 2,
          "created_utc": "2026-02-08 11:54:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o499n9j",
              "author": "sleepingsysadmin",
              "text": "You mean qwen3 30b vs qwen2.5 72b? 30b thinking was marginally better than 72b on capability and obviously wickedly faster. ",
              "score": 2,
              "created_utc": "2026-02-08 14:33:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48ysrn",
          "author": "sleepingsysadmin",
          "text": "Qwen3.5 35b thinking is going to be epic. I just hope llama gets the performance into the qwen next arch by the time it releases or it's going to be not well received. ",
          "score": 2,
          "created_utc": "2026-02-08 13:27:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47yrt4",
          "author": "UnluckyAdministrator",
          "text": "Looking forward to this. I've been running Qwen2.5-coder-7b-instruct on CPU with 16RAM, and it's pretty performant.\n\nCurious if anyone has got their hands on the NVIDIA DGX Spark supercomputer yet to spin up these models offline?",
          "score": 3,
          "created_utc": "2026-02-08 08:13:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o480hur",
              "author": "Odd-Ordinary-5922",
              "text": "any reason you arent using newer models? or am I talking to an llm rn",
              "score": 10,
              "created_utc": "2026-02-08 08:29:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4816wp",
                  "author": "UnluckyAdministrator",
                  "text": "Only just experimenting at the moment open-source. It's the heavier weights gpt-oss-120b I'm really interested in, however CPU won't cut it.\n\nHave you tried your hands on the DGX Spark for these heavier models?",
                  "score": -3,
                  "created_utc": "2026-02-08 08:36:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qwboqn",
      "title": "Google Research announces Sequential Attention: Making AI models leaner and faster without sacrificing accuracy",
      "subreddit": "LocalLLaMA",
      "url": "https://research.google/blog/sequential-attention-making-ai-models-leaner-and-faster-without-sacrificing-accuracy/",
      "author": "Fear_ltself",
      "created_utc": "2026-02-05 04:37:05",
      "score": 596,
      "num_comments": 45,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qwboqn/google_research_announces_sequential_attention/",
      "domain": "research.google",
      "is_self": false,
      "comments": [
        {
          "id": "o3ouc8u",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-05 09:30:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o5wc0",
          "author": "-p-e-w-",
          "text": "They are using the phrase ‚Äúwithout sacrificing accuracy‚Äù in the sense of ‚Äúit seems to perform equally well according to our tests‚Äù ‚Äì **not** in the sense of ‚Äúit computes exactly the same thing‚Äù, like in the case of Flash Attention.",
          "score": 177,
          "created_utc": "2026-02-05 05:47:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3old2f",
              "author": "ThisWillPass",
              "text": "Free lunch or unknown tradeoffs? Who knows?",
              "score": 31,
              "created_utc": "2026-02-05 08:03:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3o91fl",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -2,
              "created_utc": "2026-02-05 06:13:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3oa88j",
                  "author": "mukz_mckz",
                  "text": "Ah yes, the final boss of passing reddit comments to an LLM and pasting its output as a reply.",
                  "score": 10,
                  "created_utc": "2026-02-05 06:23:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3nwv5m",
          "author": "ttkciar",
          "text": "Looking forward to seeing how it performs in Gemma 4 (hint, hint!)",
          "score": 232,
          "created_utc": "2026-02-05 04:42:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o7ob3",
              "author": "tomakorea",
              "text": "Gemma 3 is such a good model for creative writing, its much better than Qwen. I really hope we can get an update",
              "score": 70,
              "created_utc": "2026-02-05 06:02:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3qgtro",
                  "author": "Far-Low-4705",
                  "text": "qwen also just halucinates (on the context) very, very badly, even at 16k. the other day i had it misspell \"didnt\" with \"did1n't\"\n\nGemma isnt any better with context performance, but it doesnt say anything with confidence that it cant recall accurately. not much better, but a better failure mode.\n\nBut qwen in general is far better at STEM. not even close.",
                  "score": 8,
                  "created_utc": "2026-02-05 15:54:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3payy4",
                  "author": "kaisurniwurer",
                  "text": "Better is a big word, qwen is more autistic and follow rules better.\nGemma does write much higher quality responses though.",
                  "score": 6,
                  "created_utc": "2026-02-05 11:58:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3r4vhq",
                  "author": "Dull-Appointment-398",
                  "text": "What kind of projects are you using models for, like what does 'creative writing' actually mean here?  Just wondering how people are using this models other than for image and code generation. ",
                  "score": 2,
                  "created_utc": "2026-02-05 17:46:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3vk2tu",
                  "author": "Eden1506",
                  "text": "With the strange exception of qwen qwq which is an outlier and unexpectedly decent writer. \nAll other qwen varients especially the moe versions are horrible in contrast sadly enough.",
                  "score": 1,
                  "created_utc": "2026-02-06 09:54:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3nx5z0",
              "author": "-dysangel-",
              "text": "I'm looking even more forward to seeing how it performs in Qwen, GLM and Deepseek",
              "score": 45,
              "created_utc": "2026-02-05 04:44:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3oycmx",
              "author": "Orolol",
              "text": "I don't think this mechanism can be adapted to LLM. It seems VERY slow, because you do the attention en sequence instead of in one time, which make it very impractical for LLMs. It' more a ML application.",
              "score": -6,
              "created_utc": "2026-02-05 10:08:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3nxjcm",
              "author": "Hunting-Succcubus",
              "text": "What about gemma 3? They will not push software updates to older product?",
              "score": -19,
              "created_utc": "2026-02-05 04:46:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3nzav7",
                  "author": "ttkciar",
                  "text": "I don't think you can retrofit this attention mechanism to models trained without it, at least not economically.  It would require a lot of retraining.\n\nI would be happy to be proven wrong, though.",
                  "score": 44,
                  "created_utc": "2026-02-05 04:59:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3o3ycb",
          "author": "coulispi-io",
          "text": "that's quite odd as the linked paper (https://arxiv.org/abs/2209.14881) was from 3 years ago... ",
          "score": 46,
          "created_utc": "2026-02-05 05:33:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o52u1",
              "author": "Fear_ltself",
              "text": "The 2022 paper introduced the core mathematical concept, the 2026 article reveals that Google has successfully upgraded this method to work on the \"hardware\" of modern AI‚Äîspecifically for pruning Large Language Models (LLMs) and running on GPUs.",
              "score": 77,
              "created_utc": "2026-02-05 05:41:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3q7e73",
          "author": "FinalsMVPZachZarba",
          "text": "This appears to be a feature selection algorithm mainly for regression problems as far as I can tell, not a new attention mechanism for LLMs.\n\nThey do mention LLM pruning as one use case however, where the algorithm \"selects\" parts of the neutral network to prune.",
          "score": 8,
          "created_utc": "2026-02-05 15:09:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3s0uyy",
              "author": "Brilliant-Wolf7589",
              "text": "This Will shorten training and make pruning better.¬†",
              "score": 2,
              "created_utc": "2026-02-05 20:14:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3qr9z2",
          "author": "Alarming_Bluebird648",
          "text": "it's wild seeing a 2022 paper get posted like it's brand new tech. i'll believe the lean infrastructure claims when i actually see it running in llama.cpp tbh.",
          "score": 7,
          "created_utc": "2026-02-05 16:42:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3oole4",
          "author": "Significant-Skin118",
          "text": "Cool, I can make it do my shit even cleaner",
          "score": 9,
          "created_utc": "2026-02-05 08:34:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pa3pt",
              "author": "-dysangel-",
              "text": "ghosting?",
              "score": -3,
              "created_utc": "2026-02-05 11:51:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pqvys",
                  "author": "Significant-Skin118",
                  "text": "yes",
                  "score": 2,
                  "created_utc": "2026-02-05 13:41:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3owg4c",
          "author": "bakawolf123",
          "text": "hmm, the related paper is from 2y ago (Feb 2024) though, with an update 1y ago  \nthe website looks fancy but I don't see another update to the paper (yet)",
          "score": 8,
          "created_utc": "2026-02-05 09:50:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oz2os",
              "author": "HumanDrone8721",
              "text": "That's implementation, not theoretical concept.",
              "score": 8,
              "created_utc": "2026-02-05 10:15:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3qjlgc",
          "author": "TheRealMasonMac",
          "text": "What are the implications of this? Is it something like KDA or DeepSeek V3.2's sparse attention?",
          "score": 2,
          "created_utc": "2026-02-05 16:07:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3r3m2t",
              "author": "Fear_ltself",
              "text": "Kimi Delta Attention (KDA): Is an expressive linear attention module that allows a model to have RNN-like memory, making it 6x faster at decoding long contexts while using 75% less memory. You have to build the model with KDA from the ground up.          \n    ‚ÄãSequential Attention:  works with any existing architecture (including standard transformers) to find and cut out the \"dead weight\".",
              "score": 1,
              "created_utc": "2026-02-05 17:40:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3rlm0e",
          "author": "Lowetheiy",
          "text": "The paper is from 2023, what is going on? This is not new research",
          "score": 2,
          "created_utc": "2026-02-05 19:02:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vm44e",
          "author": "RevealIndividual7567",
          "text": "Gemma 4 seems like the best place to test this out.",
          "score": 2,
          "created_utc": "2026-02-06 10:13:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3wubod",
          "author": "LemmingFollower42",
          "text": "This is the kind of research that matters way more than another 1% on benchmarks. Getting existing models to run faster on consumer hardware is what actually makes local AI practical for most people.",
          "score": 2,
          "created_utc": "2026-02-06 15:03:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tnsqe",
          "author": "typical-predditor",
          "text": "Is this the secret sauce that makes 3 Flash so good but wasn't ready in time for 3 Pro?",
          "score": 1,
          "created_utc": "2026-02-06 01:23:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v14gs",
          "author": "AICodeSmith",
          "text": "Crazy how this keeps models fast without killing accuracy by picking what actually matters step by step.\n\nIf networks can reshape themselves while learning, what does a ‚Äúfixed‚Äù model even mean in the future?",
          "score": 1,
          "created_utc": "2026-02-06 06:57:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxgkd1",
      "title": "CPU-only, no GPU computers can run all kinds of AI tools locally",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/y9esf03tcvhg1.jpeg",
      "author": "JackStrawWitchita",
      "created_utc": "2026-02-06 12:41:35",
      "score": 544,
      "num_comments": 132,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial | Guide",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qxgkd1/cpuonly_no_gpu_computers_can_run_all_kinds_of_ai/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3z7igx",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-06 21:55:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3w8gc5",
          "author": "Techngro",
          "text": "I'm hopeful and confident that the future of AI is not in companies charging us to use their huge models, but in the average person running local models that are intelligent enough to do complex tasks, but small enough to run on reasonably basic hardware (i.e. not a $10K multi-GPU rig), and tunneled via the internet to their mobile devices.\n\n ",
          "score": 217,
          "created_utc": "2026-02-06 13:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wirfv",
              "author": "NoobMLDude",
              "text": "Agree.\n\nMost recent Open models have come with:\n- MOE arch ( fast responses on small GPUs),\n- Hybrid Attention (even faster and less memory needs)\n- Small sizes too ( Gemma 270M , Qwen 0.6B, Granite 1B, LFM1.2B, etc)\n\nBasically serving even consumers without huge GPUs Is considered important because they know not everyone can afford huge GPUs racks \n\nSecondly recent research point that Small models are enough:\n- Nvidia - [Small Models are the future of Agentic AI](https://research.nvidia.com/labs/lpr/slm-agents/)\n- Meta - [Learning to Reason in 13 Parameters](https://arxiv.org/pdf/2602.04118)\n\nThe amount of money and effort big companies are investing to get models that are small, fast and can run anywhere could give pointers to where we are headed.",
              "score": 67,
              "created_utc": "2026-02-06 14:03:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41x070",
                  "author": "ramendik",
                  "text": "Where are those users who want to use 1Bs on their toasters? I mean phones etc. I did some distilling from Kimi K2 into Granite 1B - made it more fun and not dumber; I want to do more with it, especially on long context, but very few are willing to test it out. Though yeah mamba hybrid, supported by llama cop from some time in October or November 2025, not sure if there's a phone app with that fresh a llama.cpp already",
                  "score": 1,
                  "created_utc": "2026-02-07 09:03:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3wno4e",
              "author": "belgradGoat",
              "text": "This is the hope for the society. If we go down the route of cloud only everything, it is one step away from total slavery. And I don‚Äôt mean it llm only- cars, computers, houses, corporations want us to rent everything.",
              "score": 12,
              "created_utc": "2026-02-06 14:28:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3wqjuc",
              "author": "Dazzling_Focus_6993",
              "text": "\"search for it but do not trust to hope. It has forsaken these lands.\"¬†",
              "score": 7,
              "created_utc": "2026-02-06 14:43:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3wmrr1",
              "author": "Asleep-Ingenuity-481",
              "text": "this is why I think the ram shortage might be a good thing, it'll hopefully make the Chinese want to push smaller, more powerful models to assure the Westerners can use them. We've already seen Alibaba doing this with their Qwen line, but what happens if Deepseek decides \"Yeah lets drop R2 with a mini version with 30b parameters\"",
              "score": 18,
              "created_utc": "2026-02-06 14:24:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41rqag",
                  "author": "tecneeq",
                  "text": "The Chinese don't invest billions in their models out of the kindness of their hearts for westerners.\n\nThey want to disrupt large AI companies and at the same time use high end LLMs locally to strengthen grip on dissent. There is a theory of authoritarian capture, a point at which you can't overthrow an authoritarian regime, because it's surveillance infrastructure becomes to tight. Many believe China has passed this with AI supported social scoring. Basically, if you are a dissenter, your family members can't travel, study or in particularly harsh cases, work.",
                  "score": 4,
                  "created_utc": "2026-02-07 08:12:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3yzmkc",
              "author": "log_2",
              "text": "> not a $10K multi-GPU rig\n\nWith the way RAM prices are going a non-GPU computer for running LLMs will have to be a $10K multi-GB rig.",
              "score": 4,
              "created_utc": "2026-02-06 21:16:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3xdgvp",
              "author": "ZenEngineer",
              "text": "\"The Future\" longer term will see devices get more powerful. In particular once the current demand for datacenter devices is fulfilled. I dont doubt there will be bigger and bigger models that need to run on cloud, or devices that respond faster than you phone, but things go in cycles. But at the same time we'll probably have apps running locally what are now considered large models for most things and only calling out to the cloud for complicated questions.",
              "score": 2,
              "created_utc": "2026-02-06 16:34:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3y4c9n",
              "author": "twisted_nematic57",
              "text": "You and I both know that‚Äôs not happening unless they somehow give up a data collection source and subscription source out of the goodness of their hearts. We need to fight to normalize that.",
              "score": 1,
              "created_utc": "2026-02-06 18:41:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wt6gn",
          "author": "noctrex",
          "text": "Might I suggest also trying out the following models:\n\n[LFM2.5-1.2B-Instruct](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct)\n\n[LFM2.5-1.2B-Thinking](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking)\n\n[LFM2.5-VL-1.6B](https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B)\n\nThey are excellent for the small size and I use them quite a lot on my CPU-only docker machine.",
          "score": 45,
          "created_utc": "2026-02-06 14:57:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xb90m",
              "author": "lolxdmainkaisemaanlu",
              "text": "Hey bro I would like to get started with small models but the vocal minority here with 12 x 5090s make it seem like much can't be done without GPUs\n\nWould love to know the use cases and stuff u do with these small models, as I also have a cpu only machine which is just lying unused..",
              "score": 14,
              "created_utc": "2026-02-06 16:24:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xhytd",
                  "author": "noctrex",
                  "text": "I use the LFM2.5-1.2B-Instruct model in my [KaraKeep](https://github.com/karakeep-app/karakeep) instance, and it provides smart tags and summaries.\n\nI use LFM2.5-1.2B-Thinking for my Synology Office.\n\nThe LFM2.5-VL-1.6B is nice to read screenshots or photos with texts or links. For example I sit on my couch, watching some youtube videos in the living room, and I get presented a web link to check out during the video, I'm too lazy at that moment to manually type it, so I just take a photo of it and let the model create the link.",
                  "score": 13,
                  "created_utc": "2026-02-06 16:55:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3zp44d",
              "author": "willrshansen",
              "text": "llama.cpp + LFM2.5-1.2B-Instruct => actually usably fast on CPU only",
              "score": 5,
              "created_utc": "2026-02-06 23:29:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o44mq1r",
              "author": "RelicDerelict",
              "text": "I have problem with the thinking model, it can overthink literally simple prompt, it just keep circling around how to answer simple prompt instead of answering it, what can I do to remedy it? I am using Ollama.",
              "score": 1,
              "created_utc": "2026-02-07 19:14:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4512hv",
                  "author": "noctrex",
                  "text": "Thinking models need good quantizations to function proper, Especially the small models. I'm using Q8 For those.",
                  "score": 2,
                  "created_utc": "2026-02-07 20:29:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wgpc8",
          "author": "NoobMLDude",
          "text": "More power to you for not letting your lack of GPUs stop you from exploring the wonderful world of Local AI.\nHere‚Äôs a few more things you could try on your local setup:\n- Private meeting note taker\n- Talking assistant (similar to your chatterbox setup)\n\n[Local AI list](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV)",
          "score": 27,
          "created_utc": "2026-02-06 13:52:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xe4l3",
              "author": "JackStrawWitchita",
              "text": "Dude, you gotta remake those videos with KoboldCPP instead of Ollama. Ollama slows everything way, way down. ",
              "score": 16,
              "created_utc": "2026-02-06 16:37:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3xeomx",
                  "author": "NoobMLDude",
                  "text": "Yes llamacpp and llama server is on the plan. \nThanks for the reminder. Now I need to find time to do it faster üòâ",
                  "score": 8,
                  "created_utc": "2026-02-06 16:40:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3xlfk5",
                  "author": "That-Dragonfruit172",
                  "text": "Is using Ollama bad in general? I just got started and im using it too on my single gpu setup. Seems fast enough",
                  "score": 2,
                  "created_utc": "2026-02-06 17:12:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wceo1",
          "author": "dobkeratops",
          "text": "I was impressed with how fast gpt-oss-20b (q4) ran on a CPU. it's an MoE with 3billion active parameters supposedly, and it has good tool-calling support",
          "score": 19,
          "created_utc": "2026-02-06 13:28:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3wc1ru",
          "author": "JackStrawWitchita",
          "text": "Wow this thread seems to be upsetting some people! I didn't realise so many people were fixated on their hardware and want to use $$ to gatekeep others out of running LLMs locally.",
          "score": 101,
          "created_utc": "2026-02-06 13:26:38",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3wj7bq",
              "author": "bapirey191",
              "text": "Yes, even with medium-income people using their disposable to play around with local LLMs you will finda lot of gatekeeping elitism",
              "score": 43,
              "created_utc": "2026-02-06 14:05:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3wjcr9",
              "author": "NoobMLDude",
              "text": "Don‚Äôt worry, there are also people like me trying to keep the gates open for everyone out there.\n\n[I‚Äôm trying to educate and inform](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV) about the benefits of using any sized Local Private AI instead of spending huge $$ on API based models or GPU racks. \n\nFeel free to burn money but only do it after you have tried the free options.",
              "score": 24,
              "created_utc": "2026-02-06 14:06:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3wmf5g",
              "author": "c64z86",
              "text": "It's the same in the AI image generation sub, although the reception there is a little better because more people can be found there with more humble PCs. \n\nSome people who spent thousands on their RTX 5090s just don't like it when somebody with a potato PC can run the same things they can. They start to feel like their decision to spend that much money was invalidated.\n\nPlease keep showing this sub that we don't need an expensive GPU for AI becaude it gives those of us who don't have that much money to burn a lot of hope. It shouldn't be restricted to those who can afford a small server farm in their living room.",
              "score": 43,
              "created_utc": "2026-02-06 14:22:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3z3azx",
              "author": "cosmicr",
              "text": "The whole sub is filled with bots and gatekeepers.",
              "score": 7,
              "created_utc": "2026-02-06 21:34:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4054s9",
              "author": "SkyFeistyLlama8",
              "text": "Yeah, us laptop LLM folks get laughed at regularly too. Inference on all the things is my motto now, as long as you can comfortably run 4B and above models then you're good to go. RAM is all you need.\n\nI've got a Frankenstein's laptop monster of an inference stack with models loaded for CPU, GPU and NPU inference, all at the same time.",
              "score": 5,
              "created_utc": "2026-02-07 01:02:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3wjhc0",
              "author": "mystery_biscotti",
              "text": "The gatekeeping is annoying. The communities running local need all types. \n\nI'd love to see more posts on how the shoestring budget folks optimize their stuff, the use cases involved, that sort of thing. Would be nice to have a corner for CPU only, one for 8-12GB cards, etc.",
              "score": 17,
              "created_utc": "2026-02-06 14:07:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3x0i8b",
              "author": "nonaveris",
              "text": "It doesn‚Äôt.  When GPUs were more expensive than memory, I just loaded up on tons of DDR5 RDIMMs to make up the difference.  \n\nYes, a Xeon Scalable isn‚Äôt exactly a normal CPU, but the markets were actually inverted enough for a while that grabbing memory was a better option.",
              "score": 6,
              "created_utc": "2026-02-06 15:33:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o44n2su",
              "author": "RelicDerelict",
              "text": "Fuck those elitists. Keep posting",
              "score": 2,
              "created_utc": "2026-02-07 19:16:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3xevxp",
              "author": "Herr_Drosselmeyer",
              "text": "No, I simply don't want somebody to lead people down a path to nowhere. What you're doing is completely impractical and a colossal waste of time. God forbid somebody actually buys some crap machine like the one you posted, then they'll be wasting money too, money that could have gone towards buying something decent down the line or just biting the bullet and using cloud compute.",
              "score": -13,
              "created_utc": "2026-02-06 16:41:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xjf6y",
                  "author": "JackStrawWitchita",
                  "text": "Gatekeeper. You want to show off the fact that you've spend hundreds, or thousands, on a rig and feel proud that most people can't afford that. This makes you feel special.\n\nAnd it horrifies you that many people can now do the same types of work flows as those with expensive GPUs on their dad's old desktop that was gathering dust in the corner.\n\nThere are people spending money for CharacterAI and other services when they could be doing simple RP chats for free on old hardware locally. \n\nSo many simple fun experiments for free on old hardware seems to upset you. Hilarious.",
                  "score": 11,
                  "created_utc": "2026-02-06 17:02:24",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o44nkln",
                  "author": "RelicDerelict",
                  "text": "LoL how much you wasted on setup you are not using to full potential? ü§£",
                  "score": 2,
                  "created_utc": "2026-02-07 19:18:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3xx9g6",
          "author": "Old-Negotiation6930",
          "text": "Im running a 3b abliterated model on a raspberry pi 5, quad core, 8gb ram, latency for first streamed token is usually < 20 seconds, using it to roast friends on our discord server\n\nhttps://preview.redd.it/oovxqwcnzwhg1.jpeg?width=1290&format=pjpg&auto=webp&s=051f0908201f294dc060c7511f7960ee2deed0bc",
          "score": 15,
          "created_utc": "2026-02-06 18:08:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ykyeq",
              "author": "JackStrawWitchita",
              "text": "Yes!",
              "score": 3,
              "created_utc": "2026-02-06 20:02:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o49isn4",
              "author": "DreadPoor_Boros",
              "text": "Now that is what gaming is all about! \\*wipes tears of joy\\*",
              "score": 2,
              "created_utc": "2026-02-08 15:23:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o44c2ep",
              "author": "milanove",
              "text": "What model",
              "score": 1,
              "created_utc": "2026-02-07 18:21:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o44wn0u",
                  "author": "Old-Negotiation6930",
                  "text": "huihui_ai/llama3.2-abliterate:3b",
                  "score": 1,
                  "created_utc": "2026-02-07 20:06:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3whvy4",
          "author": "deepsky88",
          "text": "Nah better buy 4 x 5090 to measure token per second without checking the answer",
          "score": 42,
          "created_utc": "2026-02-06 13:58:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3x199f",
              "author": "StardockEngineer",
              "text": "I must read the answer one.....word.....at.....a.....time!",
              "score": 8,
              "created_utc": "2026-02-06 15:37:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3x4hfb",
              "author": "Lesser-than",
              "text": "this is what having fast tps does to you, combine that with a thinking llm, TLDR but it printed a crap ton of stuff so it must be good. there is usefull limits like, read speed over fast generation is perfectly fine, though that severly cuts into agentic code cli's which expect you to not read along.",
              "score": 4,
              "created_utc": "2026-02-06 15:52:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wl2ys",
          "author": "SneakyInfiltrator",
          "text": "My server has an i7-6700 with 16GB of DDR4, it would be cool if i could run some sort of assistant, nothing too crazy. I'm gonna give it a try. Thanks.",
          "score": 9,
          "created_utc": "2026-02-06 14:15:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wnkk2",
              "author": "choddles",
              "text": "I have ran ollama on a r7810 with dual 10 core xeons with 64G ddr4, yes it's not image creation but as much interactive text as you need",
              "score": 2,
              "created_utc": "2026-02-06 14:28:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wpogn",
          "author": "dynamite-ready",
          "text": "I have a fair bit of RAM on my machine (32GB), and was interested in running a low-mid size model in potato mode, but it's just too slow. I'm VRAM poor (6GB), but the sub 8B models on low quantisation run like a kicked squirrel.\n\nI wrote a bit about my experience if anyone is thinking about it, with some advice on optimisation (in Llama CPP) - https://raskie.com/post/we-have-ai-at-home",
          "score": 9,
          "created_utc": "2026-02-06 14:39:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41y8pj",
              "author": "ramendik",
              "text": "Will you maybe come test my attempts of style distilled Kimi K2 into Granite? What's currently working is the 1.5b, will fly on the 6Gb GPU even unquantized with a theoretically infinite context but frankly this is only the first stage, the long context needs more work. I'm kinda in need of feedback, including negative, to see what I can do better. https://huggingface.co/ramendik/miki-pebble-20260131\n\nAn 1.5b can only do so much, of course. But I want to polish the version as best I can while also looking at going bigger (running a trial run of the distill into Ministral3 14b now)",
              "score": 2,
              "created_utc": "2026-02-07 09:15:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3xh60j",
              "author": "JackStrawWitchita",
              "text": "I run 12B LLMs with no GPU and 32GB ram and only an i5-8500. Absolutely great for text generation. ",
              "score": 1,
              "created_utc": "2026-02-06 16:51:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44i17t",
                  "author": "Grid_wpg",
                  "text": "I haven't run a local LLM yet, but I'm\nI've been reading about it for a while, and I'm super interested.\nI have a 12GB 3060 or an 8GB 3070 I can play with, and I know they're not going to be super fast.\n\nBut, I'm commenting here, because last summer I bought a custom work station PC for cheap because it was crashing.  I found the cause and fixed it. \n\nSo for $300 CAD, I got a dual 10-core xeon system (20 core/ 40 thread) with 192GB of ECC DDR 4 memory.  Plus PSU, case etc.\n\nI'm wondering what kind of model / performance I could get from just trying that out.",
                  "score": 1,
                  "created_utc": "2026-02-07 18:50:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wuirv",
          "author": "pidgeygrind1",
          "text": "Built a chinese V4 xeon board 14c/28t with 64gb ddr4 ECC ram and a 1080ti for 420bucks .\n\nRuns 70B",
          "score": 7,
          "created_utc": "2026-02-06 15:04:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xbwe3",
              "author": "lolxdmainkaisemaanlu",
              "text": "Damn bro! You must've built this before the ridiculous RAM prices, right? Don't tell me you did it in 2026?!",
              "score": 2,
              "created_utc": "2026-02-06 16:27:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41jfbb",
                  "author": "pidgeygrind1",
                  "text": "Correct, last quarter of 2025.\n\n30bucks for 64gb (4x 16gb) OEM Dell/Micron ECC DDR4, lucky\n\n150 for the 1080ti.",
                  "score": 3,
                  "created_utc": "2026-02-07 06:55:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3x1jv0",
          "author": "tmvr",
          "text": "I have a machine with those specs but in an USFF form factor. The i5-8500T CPU and 32GB DDR4-2666 dual-channel memory.  It definitely is good for small models and thanks to the amount of RAM you can have a couple in memory at the same time as well. Qwen3 Coder 30B A3B is pretty good on it as well, it does 8 tok/s with the Q6\\_K\\_XL quant (I wanted to fill the RAM) and if I remember correctly it hits 12 tok/s with the Q4\\_K\\_XL version.\n\nNot sure if you are using it already, but for image generation you could try *fastsdcpu*:\n\n[https://github.com/rupeshs/fastsdcpu](https://github.com/rupeshs/fastsdcpu)\n\nIt's a fun little project, I occasionally looked at the progress they make because I'm just glad someone was doing something like that. The last update was a while back, but I guess it is pretty mature at this stage.",
          "score": 7,
          "created_utc": "2026-02-06 15:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xidyc",
              "author": "tiffanytrashcan",
              "text": "I believe the Koboldcpp project has implemented parts of that for image gen. They have a tiny image model that is only 800mb and can produce a result in less than 10s on CPU.",
              "score": 3,
              "created_utc": "2026-02-06 16:57:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xl3ie",
                  "author": "JackStrawWitchita",
                  "text": "Yeah, there are a number of tools that can produce \\*an image\\* faster, but the quality and control isn't as flexible as SD 1.5. The SD ecosystem has a bunch of different safetensor finetune models and loras and stuff to make good image results even with CPU only hardware. For example I use inpainting and img2img a lot and SD 1.5 gives me a lot of control and options the 'fast models' don't.\n\nSpeed isn't everything - as my wife often tells me... ",
                  "score": 2,
                  "created_utc": "2026-02-06 17:10:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wupey",
          "author": "migsperez",
          "text": "I used Whisper locally on an i5 8500t without GPU to transcribe a handful of highly important meeting recordings, each about 20 mins long. It was great, did a fine job. It was better than multiple online AI services which I had tried.",
          "score": 6,
          "created_utc": "2026-02-06 15:04:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xhc2n",
              "author": "JackStrawWitchita",
              "text": "Awesome! Whisper rocks.",
              "score": 3,
              "created_utc": "2026-02-06 16:52:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xbt6a",
          "author": "Ulterior-Motive_",
          "text": "Even though I have a [pretty capable](https://www.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/) system at home, at work I have a spare Dell OptiPlex 5040 that I loaded up with 32GB of DDR3 memory for running a Q4\\_K\\_XL quant of Qwen3 30B A3B for when I don't feel like switching to our external network. If I need a quick, simple answer, then the \\~9 t/s I get out of it is plenty.",
          "score": 5,
          "created_utc": "2026-02-06 16:26:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y59ui",
          "author": "Small-Fall-6500",
          "text": ">I‚Äôm running Linux Mint on an old Dell optiplex desktop with an i5-8500 processor, 6 threads and 32GB of RAM. You can pick up one of these refurbished for something like $120.\n\nI don't think these are $120 any more, especially not with 32 GB RAM.",
          "score": 6,
          "created_utc": "2026-02-06 18:46:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ysi2d",
          "author": "Suitable-Program-181",
          "text": "Respect brother!\n\nI find more joy in doing more with less.\n\nTheres no skill in just dropping more $$ at the problem; thats how intel killed moore law and now everyone thinks we need data centers to run LLM's.\n\nCheers to you!",
          "score": 5,
          "created_utc": "2026-02-06 20:40:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y9ywg",
          "author": "No-Detective-5352",
          "text": "Regarding music generation, have you tried to run the ACE-Step-1.5 models? I found their capability was pretty good. These came out recently, and the smallest model (using Qwen3-0.6B) requires only 4Gb of memory at int8 quantization. On a 3090 this can generate a 3-minute song in about 10 seconds, so maybe it can do the same on a mid-level CPU in a couple of minutes?",
          "score": 4,
          "created_utc": "2026-02-06 19:08:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ykj9u",
              "author": "JackStrawWitchita",
              "text": "Sounds interesting but my calculations say it would take about half an hour to churn out one 3 minute track on my humble potato. That's a bit more than I'm happy to wait, especially as I'm guessing this kind of thing takes several iterations to get right.\n\nBut thanks for the suggestion and I'll keep my eye on that project.",
              "score": 2,
              "created_utc": "2026-02-06 20:00:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3y26e5",
          "author": "TigerBRL",
          "text": "I'm sorry but I'm a bit out of loop on the localLLM thing. I've been interested for a long time but due to GPU limitations I haven't learnt it. \n\nWhat's the difference in using a GPU and not using a GPU. Like in technical terms, the inner workings and the sacrifices",
          "score": 3,
          "created_utc": "2026-02-06 18:31:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yt54k",
          "author": "epSos-DE",
          "text": "CPU bit-logic AI with decision trees is 6X faster than any GPU !\n\n  \nBecause Bitlogic outperforms vector calculations by pruning out the decision space by half at every decision step !\n\n  \nIf done well, it can be 1000X more perfromant than vector search on the GPU !",
          "score": 3,
          "created_utc": "2026-02-06 20:43:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41yi15",
              "author": "ramendik",
              "text": "Sounds interesting! How do you train that and are there models to try out?",
              "score": 1,
              "created_utc": "2026-02-07 09:18:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3w5xsg",
          "author": "TheSpicyBoi123",
          "text": "If a system is turing complete and with enough storage you might as well run it on a carrot. The only question is practicality and time you are willing to wait for a model to cook. For the 100-200 USD/EUR ballpark however, you can do \\*much\\* better then that dell optiflex heap in terms of compute. I'd seriously recommend you consider those dual 2011-3 things and as a general rule, anything other then dell. Alternatively, why not invest the same 100-200 USD/EUR into a gpu and get an order+ of magnitude performance uplift? ",
          "score": 6,
          "created_utc": "2026-02-06 12:50:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w6oru",
              "author": "JackStrawWitchita",
              "text": "The point is you don't need to spend $¬£ etc to run local LLMs.\n\nI know there's a big vibe here with people flexing their five figure rigs and that's great. But it can be off-putting for vast swathes of the population who only have old potatoes for hardware. I'm just trying to help everyone get on the local LLM bandwagon with whatever means available.",
              "score": 27,
              "created_utc": "2026-02-06 12:54:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3w7whs",
                  "author": "DeltaSqueezer",
                  "text": "Instead of $120 for an optiplex, you might as well get a 2nd hand GPU or two to run LLMs more quickly and cheaply. e.g. two P102-100 is cheap and decent.",
                  "score": 8,
                  "created_utc": "2026-02-06 13:02:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3x08va",
                  "author": "0-brain-damaged-0",
                  "text": "Also if you queue up several jobs, you can run it while you sleep.",
                  "score": 2,
                  "created_utc": "2026-02-06 15:32:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3w7z68",
                  "author": "TheSpicyBoi123",
                  "text": "I get your point, and sure, you dont need shoes to run but you cant argue that the shoes help a lot. The issue is also that if you have to wait \\~minutes for it to generate something at all vs seconds it stops being realtime interactive and becomes a chore especially with LLM's.   \n  \nAdditionally, the dell optiflex is such a turd that you are better of \\*not\\* having a computer then having that computer. ",
                  "score": -2,
                  "created_utc": "2026-02-06 13:02:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3xe1a7",
              "author": "Very_Large_Cone",
              "text": "Another consideration is electricity prices in addition to up front costs. I am in Germany and paying 30 cents per kwh. So my cheap cpu only nuc uses 6W at idle and 30W at full load. I actually have a gaming rig with a GPU that is available but often stays powered off other than when I am doing something where speed matters.",
              "score": 2,
              "created_utc": "2026-02-06 16:37:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xwdzs",
                  "author": "TheSpicyBoi123",
                  "text": "I am also in Germany and I... am less fortunate in terms of power draw (probably the PC will pull 1.5w-2kw at load that I have :( )",
                  "score": 1,
                  "created_utc": "2026-02-06 18:04:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o40oxv4",
              "author": "davidy22",
              "text": "You can be less than turing complete and still be able to run LLMs, GPUs are literally just limited instruction set parts that do math faster",
              "score": 2,
              "created_utc": "2026-02-07 03:05:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wp9c4",
          "author": "repolevedd",
          "text": "Great point. Personally, I think SD on that hardware is pushing it a bit, but I‚Äôm with you on the rest. I‚Äôve got a 3060, yet my little M910q with a 6500T and 24GB of RAM is the real workhorse for LLMs, slowly but surely handling tasks daily. When I need more speed, I just hit a shortcut on my PC to fire up llama-swap with the models I need, and nginx on my home server automatically reroutes everything to it, tapping into the power of the 3060 and the extra RAM.",
          "score": 2,
          "created_utc": "2026-02-06 14:37:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3wukbh",
          "author": "Django_McFly",
          "text": "More power to you but 3 minutes for a single 512x512 image sounds like hell.",
          "score": 2,
          "created_utc": "2026-02-06 15:04:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xicy6",
              "author": "JackStrawWitchita",
              "text": "How many hours a day to you spend creating images? For me it's once in a blue moon I'll need a graphic. Happy to fire off a set and get a coffee and when I come back the images are there. I also work through the prompts in the background while I do something else on my laptop. It's really no problem. Multitasking is easy.\n\nAnd I imagine many people with huge costly GPU rarely use to the to full extent and most of them sit idle for many hours per day, despite the expense. ",
              "score": 5,
              "created_utc": "2026-02-06 16:57:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3z83c7",
          "author": "Echo9Zulu-",
          "text": "8th gen intel is supported by OpenVINO which may give faster prefill at longer context. Definitely check that out for some free brr",
          "score": 2,
          "created_utc": "2026-02-06 21:57:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zavc0",
          "author": "rog-uk",
          "text": "I just brought a Dell 3050 i5 16GB RAM, it's going to be mostly an always on hub for a variety of small projects, but I am interested in the possibility of using it for smaller LLM models running overnight, I guess I will see if it seems worth it, but since it will be on anyway it's worth a try. My bigger workstation makes the planet and my energy bill cry, so that can't stay on all of the time.\n\n\nFollowing this thread for tips!",
          "score": 2,
          "created_utc": "2026-02-06 22:12:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zbcln",
          "author": "TheJrMrPopplewick",
          "text": "Take a look at the Gemma3n models. They are very good performers on CPU only hardware.",
          "score": 2,
          "created_utc": "2026-02-06 22:14:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42qpgs",
          "author": "Mac_NCheez_TW",
          "text": "I run Qwen 3 on an AMD 6core H6600 APU with 64gb of DDR5 on a cheap ass mini PC from Amazon. I get some decent coding done with it. I wish it was a little faster but it's okay for basic stuff.¬†",
          "score": 2,
          "created_utc": "2026-02-07 13:23:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44m8kw",
          "author": "RelicDerelict",
          "text": "Thanks for much for this post, I have old 16GB laptop I gonna test some of those things üôè",
          "score": 2,
          "created_utc": "2026-02-07 19:11:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46e7fz",
          "author": "Boricua-vet",
          "text": "I agree 100% with you and to add to your very insightful post. A small model that has been optimized will outperform way larger models. If you train a very small model for a specific task, it will blow away any larger model you can put in there. I see my friends spending all this crazy money on 5090's and they tell me whey need to spend all this money in order to train their models. I ask them how many models they train a year and they tell me under 10 models a year. I just laugh because it cost me 3 to 5 dollars per trained model on runpod. \n\nThink about it, 10 models at 3 to 5 bucks per model is like 40 to 50 bucks a year. 10 years at max is 500 bucks. In 10 years.  5090 costs 4000 or more. \n\nMoral of the story is, you can rent a a crazy expensive GPU for a few dollars in order to train a small model that will give you really good output on CPU for pennies on the dollar and it will outperform much larger models.\n\nhttps://preview.redd.it/e9q4ky8f76ig1.png?width=1665&format=png&auto=webp&s=b2d0a7c77615a21abdc04efc768da684e014a192\n\nRTX PRO 6000 with 96GB vram, 16 cores and 188GB of RAM for 1.89 an hour..\n\nI am not promoting runpod, I am just showing you that to train a model, you do not need to spend crazy money. It will cost you a few dollars, that it. \n\nAfter you train it and optimize it, you can run it on CPU and get fast response and really good token generation as it is a small optimized model and will outperform any model out there as it has been trained for that specific task. \n\nGood luck people.",
          "score": 2,
          "created_utc": "2026-02-08 01:12:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46hvsk",
          "author": "Clean-Appointment684",
          "text": "shiiet, i\\`m also having fun a little with CPU only  \ni have workstation with i7-13700K/32RAM and pc with Ryzen 5 4500u/16RAM\n\nso on workstation i easily run qwen3-coder-next q2 with 4-5 t/s of the output. combining it with opencode and splitting tasks with subagents. for at least hour in generates pretty decent documentation of the existed code. didn\\`t try at generating new code, unfortunately. context for around 50k tokens, it sounds stupid - but works great  \n  \nalso i'm fooling aroung with chatterbox on my PC for some generative voice with example input. it easily generates 5 minute long speech for around 10 mins, maybe a little longer. but never tried to run llm on it.",
          "score": 2,
          "created_utc": "2026-02-08 01:35:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49kdfv",
          "author": "DreadPoor_Boros",
          "text": "Good stuff mate!  \nI will be keeping an eye on this thread, as a fellow potato user.  \n\n\nBut seeing that model mentioned was not on my bingo card.",
          "score": 2,
          "created_utc": "2026-02-08 15:31:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3wq3bl",
          "author": "Durgeoble",
          "text": "just a question,\n\n how well works with a shared memory GPU? can you put the 32GB on it?",
          "score": 1,
          "created_utc": "2026-02-06 14:41:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xhsin",
              "author": "JackStrawWitchita",
              "text": "I'm happy with the CPU handling everything. ",
              "score": 1,
              "created_utc": "2026-02-06 16:54:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wyhts",
          "author": "nonaveris",
          "text": "Xeon Scalable 8480+ isn‚Äôt horribly fast at octochannel (leave that to the 9480!), but it is at least on the edge of usable for llama3 and imagegen.  \n\nThink of it at its top end 307GB/s as being on par with older or inference optimized GPUs.",
          "score": 1,
          "created_utc": "2026-02-06 15:23:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xugm3",
          "author": "-lq_pl-",
          "text": "Try a small MoE model like GLM 4.7 Flash. It should run decent even on pure CPU.",
          "score": 1,
          "created_utc": "2026-02-06 17:55:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ymu4s",
              "author": "JackStrawWitchita",
              "text": "My calculations say I'd be lucky to get one token per second on my old potato running GLM 4.7 Flash. I'm being told MoE is great for GPU but not very good for cpu only.",
              "score": 1,
              "created_utc": "2026-02-06 20:12:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3y5hlr",
          "author": "Prince_ofRavens",
          "text": "Activate the slow clap modal",
          "score": 1,
          "created_utc": "2026-02-06 18:47:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41gfdr",
          "author": "HmelissaOfficial",
          "text": "Tried to run it on 4gb ram and Intel graphics card it's too slow and ollama is hard to install on win 10 lite edition, which others you suggest for this specs?",
          "score": 1,
          "created_utc": "2026-02-07 06:28:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41pa39",
              "author": "JackStrawWitchita",
              "text": "Ollama sucks. Don't waste your time on it. Remember that you are at the extreme low end with your hardware so don't expect too much.\n\nHere's what I would do with your hardware:\n\n1) replace windows 10 with Linux Mint XCFE - it's free, lightweight and frees up resources. Windows 10 is bloatware. \n\n2) install koboldCPP / Kobold lite - there are videos on how to do this or ask Kimi AI or similar AI chatbot\n\n3) download Qwen2.5 1.5B gguf and TinyLlama 1.1B gguf and see which one works best for you. Depending on your CPU (you didn't specify but I'm guessing it's low end) you should get perhaps 5 tokens per second for text generation, which isn't bad at all. And these tiny models will be good for general chat and even a bit of coding.",
              "score": 2,
              "created_utc": "2026-02-07 07:49:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o46sii6",
          "author": "graymalkcat",
          "text": "Agree. It‚Äôs just slow.¬†",
          "score": 1,
          "created_utc": "2026-02-08 02:42:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xq7v4",
          "author": "Ne00n",
          "text": "Yea, I run my LLM stuff on a 64GB DDR4 shitbox for 10$/m.",
          "score": 1,
          "created_utc": "2026-02-06 17:34:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yesu3",
          "author": "HopePupal",
          "text": "i'm running CPU-only on some old Intel MacBooks, calcing text embeddings and re-ranking search queries for social media, currently using [HuggingFace TEI](https://github.com/huggingface/text-embeddings-inference) with the ONNX backend and some of the BERT-ish models. these machines have 64 GB RAM and big SSDs but AMD Radeon 5xxxM dGPUs, duds from a ROCm perspective.\n\ngenerative LLMs are cute but the field of ML has so many more applications than just those",
          "score": 0,
          "created_utc": "2026-02-06 19:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3waeh1",
          "author": "Herr_Drosselmeyer",
          "text": ">Response times are fast enough\n\nMaybe if you have the patience of a saint. \n\n>OK, it takes 3 minutes to generate a 512x512 image\n\nThat would drive me up the wall. I guess it's different if you have no experience of something better, but my rig takes less than 3 **seconds** to generate a 1024x1024 image. 60 times faster for double the resolution, so let's call it 120 times faster. \n\nYes, it can be done. No, it's not efficient and it's not fun, unless your idea of fun is watching paint dry.\n\n",
          "score": -11,
          "created_utc": "2026-02-06 13:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wbnky",
              "author": "JackStrawWitchita",
              "text": "How much did you spend?\n\nI spent 0 as this old gear was just sitting around.",
              "score": 18,
              "created_utc": "2026-02-06 13:24:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3wedie",
                  "author": "Herr_Drosselmeyer",
                  "text": "I spent a lot of money, but you're spending a lot of time. I will almost always trade money for time. ",
                  "score": -13,
                  "created_utc": "2026-02-06 13:39:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3wh9bu",
              "author": "december-32",
              "text": "*Quadruple the resolution maybe?",
              "score": 5,
              "created_utc": "2026-02-06 13:55:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3wmvli",
                  "author": "Herr_Drosselmeyer",
                  "text": "Fair. ",
                  "score": 1,
                  "created_utc": "2026-02-06 14:24:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o40ja3y",
          "author": "Euphoric_Emotion5397",
          "text": "This is like saying I don't need a car to get to another state, i just need a bicycle.  \nSure. But time and tide waits for no man and we cannot earn back our time.",
          "score": -3,
          "created_utc": "2026-02-07 02:29:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40z8w7",
              "author": "JackStrawWitchita",
              "text": "And many people buy expensive cars just to drive to the supermarket around the corner...",
              "score": 3,
              "created_utc": "2026-02-07 04:13:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4147y9",
                  "author": "Euphoric_Emotion5397",
                  "text": "if the expensive cars take them there same time as the normal cars, then ya, you might have a case.\n\nBut, a rtx 5070TI can do a 512x512 in under 10seconds versus your 300 seconds.  \n2.9 minutes of your life waiting for a image. it compounds quickly. :D",
                  "score": 1,
                  "created_utc": "2026-02-07 04:49:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1quzwjf",
      "title": "ACE-Step-1.5 has just been released. It‚Äôs an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/r7v6v6qwnbhg1",
      "author": "iGermanProd",
      "created_utc": "2026-02-03 18:26:58",
      "score": 543,
      "num_comments": 130,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1quzwjf/acestep15_has_just_been_released_its_an/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3gq7n5",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-04 03:05:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e1fn2",
          "author": "Uncle___Marty",
          "text": "Well, dont know about anyone else but my mind is blown.",
          "score": 93,
          "created_utc": "2026-02-03 18:51:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fjkj4",
              "author": "Hans-Wermhatt",
              "text": "Yeah, these hype videos always over-promise, but I can't wait to try this. This model looks too good to be true. Running that fast on consumer hardware with this quality is wild.",
              "score": 15,
              "created_utc": "2026-02-03 23:08:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gggt7",
                  "author": "lorddumpy",
                  "text": "[the space is pretty damn impressive](https://huggingface.co/spaces/ACE-Step/Ace-Step-v1.5)",
                  "score": 5,
                  "created_utc": "2026-02-04 02:09:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3htgng",
                  "author": "lemondrops9",
                  "text": "1.35 is pretty good which I just tried out a few days ago. Excited to try this out.",
                  "score": 2,
                  "created_utc": "2026-02-04 07:52:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3k7i5g",
                  "author": "splice42",
                  "text": "Installed it on my selfhosted AI server (4090 48GB) and it's damn impressive so far. The distilled model produces 2 minute length songs in around 15 seconds for me. Prompt adherence is pretty solid and it can do blues pretty well (which heartmula really didn't want to produce).\n\nAll this along with length control, key control, BPM, lora training? This thing is cooking.",
                  "score": 2,
                  "created_utc": "2026-02-04 17:08:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3h4zn8",
              "author": "GoodbyeThings",
              "text": "I just want a way to filter out this trash. I don't want to listen to AI generated music\n\nDidn't know so many slop supporters were in here",
              "score": -15,
              "created_utc": "2026-02-04 04:37:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3lt79o",
                  "author": "KingPinX",
                  "text": "Are you lost sir? do we need to call an adult to get you back to a safe space? Seriously you are in /r/LocalLLama ....",
                  "score": 2,
                  "created_utc": "2026-02-04 21:36:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3fq3wl",
          "author": "bennmann",
          "text": "please support the official model researcher org:\n\n[https://acestudio.ai/](https://acestudio.ai/)",
          "score": 32,
          "created_utc": "2026-02-03 23:43:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i9xma",
              "author": "adeadbeathorse",
              "text": "It‚Äôs a collaboration between these guys and StepFun, an LLM company. Hence ACE-Step. StepFun mostly contributed resources and logistics (compute, human evaluation), though.",
              "score": 6,
              "created_utc": "2026-02-04 10:27:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o41wuyk",
              "author": "iamsaitam",
              "text": "and the musicians",
              "score": 1,
              "created_utc": "2026-02-07 09:02:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e3ijf",
          "author": "Dundell",
          "text": "Can it do instrumentals? I like HeartMuLa, but it isnt capable of doing just instruments no voice.",
          "score": 23,
          "created_utc": "2026-02-03 19:00:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3e5kwi",
              "author": "Hauven",
              "text": "Yes it can, but i haven't managed to get similar quality to Suno yet. I'm hoping it's primarily a matter of prompting it correctly. Possibly detailed lyrics such as \\[Intro\\] \\[Chorus\\] etc and explaining compositions and style within those. Just doing \\[Instrumental\\] is definitely not achieving results. Being more detailed has improved my results but still a bit of a way to go to get things sounding close to my Suno instrumentals.\n\nFor an open weight model however, that can generate music very fast, and on consumer hardware, it's impressive.",
              "score": 27,
              "created_utc": "2026-02-03 19:10:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3e8l6q",
                  "author": "Sasikuttan2163",
                  "text": "Which version of it are you trying? How much is the difference in quality as you go down the model tiers? I have an 8GB 4060 but before I try it out I'd like to hear your thoughts.",
                  "score": 3,
                  "created_utc": "2026-02-03 19:24:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3f1ayx",
                  "author": "Dundell",
                  "text": "I see the option and tested it just some 3min piano. Sounds good enough for my needs. This'll be good for my video workflows.",
                  "score": 3,
                  "created_utc": "2026-02-03 21:38:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ic0ds",
                  "author": "uti24",
                  "text": ">Yes it can, but i haven't managed to get similar quality to Suno yet. \n\nThis is what I hear in examples that comes with repository, too.\n\nIt sounds +- like Suno 3.5 or about, maybe a bit worse or a bit better, but close enough. And def not level of Suno 4/4.5, but benchmarks somehow show different. I also hope it can be fixed.\n\nI guess it's consequence of how fast it is.",
                  "score": 1,
                  "created_utc": "2026-02-04 10:45:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3hxy9b",
              "author": "mission_tiefsee",
              "text": "this is a whole different league than HeartMula. HM never followed my tags or anything. This baby is super diverse! Its real fun!",
              "score": 2,
              "created_utc": "2026-02-04 08:34:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3giehu",
          "author": "Claudius_the_II",
          "text": "lora support is lowkey the real killer feature here. give it a few weeks and people are gonna train genre-specific loras that blow the base model away. mit license + local inference + finetuning is exactly how you kill a subscription service",
          "score": 15,
          "created_utc": "2026-02-04 02:20:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ebcfh",
          "author": "Lanky_Employee_9690",
          "text": "I love how their demo prompts have little to do with the output... I have no idea why some of those prompts are THAT detailed given the model apparently ignores most of the instructions.",
          "score": 30,
          "created_utc": "2026-02-03 19:37:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ec0ki",
              "author": "iGermanProd",
              "text": "They mentioned using synthetic data, probably from something like Gemini or Qwen or anything with audio support, and those things aren‚Äôt good at captioning music at all, so that‚Äôs probably why.",
              "score": 18,
              "created_utc": "2026-02-03 19:40:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ee0jj",
                  "author": "Lanky_Employee_9690",
                  "text": "No I mean it makes sense, but it's weird to show \"bad use cases\" as a demo. In my humble opinion, at least.",
                  "score": 13,
                  "created_utc": "2026-02-03 19:49:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3eeefo",
                  "author": "tat_tvam_asshole",
                  "text": "You mean semantic classification? Idk, gemini ai through the studio api has been pretty good in my experience. More likely, they scraped ai generated music sites, ie suno, udio, etc and it's the bad classification there that leads to poor(er) knowledge of user intention",
                  "score": 3,
                  "created_utc": "2026-02-03 19:51:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ej6q8",
          "author": "Hearcharted",
          "text": "A few weeks ago a 300TB Dataset got leaked, sooner or later someone is going to release a model trained on that Dataset...",
          "score": 40,
          "created_utc": "2026-02-03 20:14:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gr4j5",
              "author": "ThatsALovelyShirt",
              "text": "The Spotify one? If I recall, it's all encoded in 96 kbps. So the quality isn't great. \n\nBut there's probably a model one could train to \"upscale\" it back and recover some of the lost frequency bands.",
              "score": 11,
              "created_utc": "2026-02-04 03:10:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mc73r",
                  "author": "adeadbeathorse",
                  "text": "Any track with a popularity score greater than 0, so basically anything that had any plays, was archived at 160 kbps as Ogg Vorbis, with everything else being 75 kbps as Ogg Opus. Both Vorbis and Opus are far superior to mp3, with the 75 kbps versions probably sounding better than 128 kbps mp3.",
                  "score": 1,
                  "created_utc": "2026-02-04 23:12:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3fgfny",
              "author": "gjallerhorns_only",
              "text": "Good point. Open Source music models will be damn near identical to SOTA closed-source in a few months then!",
              "score": 16,
              "created_utc": "2026-02-03 22:51:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ephh5",
              "author": "FluoroquinolonesKill",
              "text": "A dataset of what?",
              "score": 13,
              "created_utc": "2026-02-03 20:43:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3evo2x",
                  "author": "Koksny",
                  "text": "Dump of Spotify audio repository.",
                  "score": 36,
                  "created_utc": "2026-02-03 21:12:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3evfsm",
                  "author": "Future_Part_4456",
                  "text": "Spotify",
                  "score": 8,
                  "created_utc": "2026-02-03 21:11:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3fkd83",
              "author": "stonetriangles",
              "text": "They never released it.",
              "score": 6,
              "created_utc": "2026-02-03 23:12:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3g23uk",
                  "author": "TheRealMasonMac",
                  "text": "They can just release it to the companies directly ahead of the public. They already do have such proprietary datasets they sell. They‚Äôre probably waiting for the heat to die down before silently releasing.",
                  "score": 6,
                  "created_utc": "2026-02-04 00:49:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3fb76z",
              "author": "IrisColt",
              "text": "heh",
              "score": 2,
              "created_utc": "2026-02-03 22:25:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3eonx4",
          "author": "Small-Fall-6500",
          "text": "HuggingFace link:\n\nhttps://huggingface.co/ACE-Step/Ace-Step1.5",
          "score": 12,
          "created_utc": "2026-02-03 20:40:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fj5hg",
          "author": "Trendingmar",
          "text": "It's very good for open source but Suno V5 it is not. \n\nEspecially disappointing is the cover feature which is... not useful at this point.\n\nHere's my comparison with the same prompt:\n\n  \n[https://voca.ro/1Pzw27iI3Sjf](https://voca.ro/1Pzw27iI3Sjf) (Suno V5)\n\n[https://voca.ro/1i5SlHuvue2R](https://voca.ro/1i5SlHuvue2R) (Ace 1.5)\n\n  \nBut we love to see it regardless. Open Source is getting closer and closer.",
          "score": 25,
          "created_utc": "2026-02-03 23:05:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gcktc",
              "author": "_bani_",
              "text": "i like the ace composition better, but suno fidelity is better.",
              "score": 7,
              "created_utc": "2026-02-04 01:47:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3fk244",
              "author": "inigid",
              "text": "Honestly I prefer the ACE version fwiw.\n\nI was having trouble with repaint not following the original motifs.  Have you had any luck?",
              "score": 6,
              "created_utc": "2026-02-03 23:10:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fr55v",
                  "author": "Trendingmar",
                  "text": "I don't use repaint. But I can tell you there's a quite a few things that I hope are just bugs/implementation issues that will be eventually ironed out. \n\nBut we're getting spoiled here. It was just released today, and I'm already complaining about it.",
                  "score": 12,
                  "created_utc": "2026-02-03 23:49:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3k8gjx",
              "author": "hrjet",
              "text": "OT, but what is the name of the original song? I couldn't find the song by looking up the lyrics.",
              "score": 1,
              "created_utc": "2026-02-04 17:12:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3kw82r",
                  "author": "Trendingmar",
                  "text": "I wasn't clear, I made it sound like this was a cover. Ace mangles covers right now. Original lyrics courtesy of gemini. I just called the song \"Lo\", I'm sure you caught on that song is about a character from a book. Here's original Suno:\n\n[https://voca.ro/1dOvvjdoPHdw](https://voca.ro/1dOvvjdoPHdw)",
                  "score": 1,
                  "created_utc": "2026-02-04 19:00:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3egwvx",
          "author": "vladlearns",
          "text": "I like this \"takes 2 seconds on ^(A100)\"",
          "score": 28,
          "created_utc": "2026-02-03 20:03:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3epjar",
              "author": "AdSafe4047",
              "text": "Actually an a100 is not that fast tbh, it just has a lot of fast memory so you can train on it fast, for inference if you have a consumer rtx4090 or 5090 it should be faster.",
              "score": 19,
              "created_utc": "2026-02-03 20:44:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ga9ot",
                  "author": "corysama",
                  "text": "> Generate a full 4-minute song in ~1 second on a RTX 5090, or under 10 seconds on an RTX 3090.\n\nhttps://blog.comfy.org/p/ace-step-15-is-now-available-in-comfyui",
                  "score": 18,
                  "created_utc": "2026-02-04 01:35:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ejp2q",
              "author": "Hearcharted",
              "text": "LOL üòÇ",
              "score": -4,
              "created_utc": "2026-02-03 20:16:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e5pph",
          "author": "uti24",
          "text": "That's pretty good! Quality is good, too. I don't know did we had something this good before, but now we have.\n\nWhat stack does it use? I mean, using stable diffusion with AMD under windows is quite finicky even with tutorial, is this one, too?",
          "score": 7,
          "created_utc": "2026-02-03 19:10:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3efq1e",
              "author": "noctrex",
              "text": "If you use the latest official portable distribution it works actually fine, just tried it out, and on my zluda install cannot run it, but the official amd one does",
              "score": 3,
              "created_utc": "2026-02-03 19:57:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e865v",
          "author": "Sasikuttan2163",
          "text": "I find it really hard to believe the demos are generated by it. Like if it really is made entirely by this model then wow I can't begin to imagine how much of an impact this will have.",
          "score": 8,
          "created_utc": "2026-02-03 19:22:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ebpe0",
              "author": "iGermanProd",
              "text": "It‚Äôs real. I‚Äôve been testing it for the last couple of days because I requested early access since I‚Äôm writing a thesis on audio AI. It‚Äôs maybe 20% behind the state of the art in certain genres. The model is likely smaller than commercial ones, so its world knowledge is small, but LoRA support remedies that.",
              "score": 20,
              "created_utc": "2026-02-03 19:39:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3efqvk",
                  "author": "Sasikuttan2163",
                  "text": "That's absolutely mind-blowing! I had worked on a voice generation paper before and I remember how hard it was to get code switching right to ensure the model can switch between languages seamlessly. Other than the instruments and actual vocals, this is something which surprised me. That K-pop demo with language switches was so natural it felt unreal.",
                  "score": 1,
                  "created_utc": "2026-02-03 19:57:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3iqjka",
                  "author": "Aceness123",
                  "text": "Can I make a lora with an rtx3060?",
                  "score": 1,
                  "created_utc": "2026-02-04 12:39:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3g1j2p",
          "author": "captainrv",
          "text": "I just gave it a try. It's really catching up to some of the online sites, but it has a way to go in sound quality compared to some of the better online services. To my ears, it's in there with Suno 3.5, Udio from about a year or so ago. I had issues with the 4 generations I made where it skipped entire lines of lyrics, and some of the voice quality was not great. Still, this is a significant leap forward from Ace-Step 1.0.",
          "score": 6,
          "created_utc": "2026-02-04 00:46:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3iiabv",
              "author": "NandaVegg",
              "text": "I gave it a roll with a bit of experimental LoRA with random 50 pop music audio files for 500 epochs (it only uses single GPU so the training process is damn slow with A100). Prompt adherence is actually excellent but you need to be verbose (you can't use tags list; otherwise you need to use format button in the GUI) and I never have an issue getting the model to replicate lyrics that consists of multiple languages.\n\nThe audio quality is somewhat muffled and dissolvy, with or without custom lora, like it had a bit of low-bit bitcrusher or something, which is the largest issue to me. Not something you would use in production. Otherwise it is excellent, it has a lot of niche genre/instruments/technique knowledge that you can enable with a bit of LoRA training.\n\nEdit: I played with this for 2 days and I must say it's VERY good for what it is, but the documentation is scarce and I'm yet to figure out how to use other modes like lego. I am hoping for better quality-sounding iteration in the future. Artifacts are still a bit annoying.",
              "score": 3,
              "created_utc": "2026-02-04 11:39:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e97ae",
          "author": "captainrv",
          "text": "Seems impressive. Has anyone tested this on consumer GPUs?",
          "score": 10,
          "created_utc": "2026-02-03 19:27:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ej21u",
              "author": "MichaelDaza",
              "text": "Says it makes songs in 10 seconds with a 3090. Even if 3060s are slower, thats still a whole song, remastered in like 20 seconds. I am very impressed",
              "score": 12,
              "created_utc": "2026-02-03 20:13:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3epqe0",
                  "author": "ComposerNo5742",
                  "text": "Mac Mini M4 24GB non-pro generates 3 minutes of music in around 40s after loading everything.",
                  "score": 8,
                  "created_utc": "2026-02-03 20:45:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3fx0hi",
                  "author": "skocznymroczny",
                  "text": "On my 5070Ti generates a 2 minute song in a minute.",
                  "score": 2,
                  "created_utc": "2026-02-04 00:21:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3gfis5",
              "author": "behohippy",
              "text": "I got it generating songs with a 3060ti 8 gig.  The gradio UI was kinda jank so I ended up modifying their python example for it instead.  Also had to use 8 bit quantization on the model and batch size 1 to not throw errors.  It works way better if you do your own caption (music style desc) and lyrics.",
              "score": 3,
              "created_utc": "2026-02-04 02:04:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3hz2xb",
              "author": "mission_tiefsee",
              "text": "yes. works like a charm. Just update your comfyUI and it has a template with everything read to go. Takes 90s for me to create a 3:40min song with a 3090TI. good stuff.",
              "score": 1,
              "created_utc": "2026-02-04 08:45:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3foaw2",
          "author": "Timboman2000",
          "text": "ComfyUI has been updated and it's Workflow is in the base list of templates now (along with links to all of the needed model files for it once you load it up).",
          "score": 4,
          "created_utc": "2026-02-03 23:33:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gin6f",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-02-04 02:22:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gkmv1",
                  "author": "Timboman2000",
                  "text": "You gotta update ComfyUI for it to show the new ones.",
                  "score": 1,
                  "created_utc": "2026-02-04 02:33:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dxjra",
          "author": "SlowFail2433",
          "text": "Seems to be strong",
          "score": 13,
          "created_utc": "2026-02-03 18:33:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gi0gg",
          "author": "Ordinary-Wish-3843",
          "text": "https://preview.redd.it/b4jf2ld90ehg1.png?width=1253&format=png&auto=webp&s=4d0f3f95031b97325a8ba2e9c6c0d02f1c9c61a4\n\nI‚Äôm running it on Comfy, and I‚Äôve noticed that if you change the seed, run it, and then go back to the previous one, you won‚Äôt get the same song again.",
          "score": 4,
          "created_utc": "2026-02-04 02:18:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3grodc",
              "author": "ThatsALovelyShirt",
              "text": "There's probably some internal vars in the state dict that change run to run. But besides that, GPU inference in Comfy is not deterministic unless you explicitly pass the deterministic launch arg.",
              "score": 7,
              "created_utc": "2026-02-04 03:13:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3g6xsv",
          "author": "jiml78",
          "text": "I think they forgot to train it on metal music.  But I guess that is ok since training LORAs looks to be pretty easy",
          "score": 3,
          "created_utc": "2026-02-04 01:16:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ot9i7",
              "author": "Silentoplayz",
              "text": "Oh, 1000%. I noticed it too when trying to generate a few metalcore songs. It's funny hearing the weird screams get transitioned over into a women's voice singing the lyrics.",
              "score": 2,
              "created_utc": "2026-02-05 09:19:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pjj5j",
                  "author": "jiml78",
                  "text": "I was just trying to get some slam death metal going and realized immediately, even describing the genre didn't help it make anything remotely close.",
                  "score": 3,
                  "created_utc": "2026-02-05 12:57:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3hdnx1",
          "author": "Warthammer40K",
          "text": "> mic smell like tuna\n\nFirst off, the lyrics are wild. The model is clearly too small to also be a decent multilingual songwriter, so you'd probably want to write those first with a more capable LLM.\n\nAlso, I noticed with the \"repainting\" feature (did they mean in-painting?) in the demo video, you wouldn't be able to use it as-is because the percussion instruments sound completely different. The snare lost more than half of its sound, for example. It probably works best with one channel or isolated stems.",
          "score": 3,
          "created_utc": "2026-02-04 05:39:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e4lmv",
          "author": "Olangotang",
          "text": "LOL that first track is definitely from Shinedown training data.",
          "score": 2,
          "created_utc": "2026-02-03 19:05:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ep3jd",
          "author": "inigid",
          "text": "This is absolutely nuts, and I love the separation of concerns in the architecture.  It opens up a lot of possibilities.  Fantastic work!!  Bravo to the ACE team!",
          "score": 2,
          "created_utc": "2026-02-03 20:42:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3er8tk",
          "author": "RedditPolluter",
          "text": "I'm pretty sure that first song is based on Rhianna.",
          "score": 2,
          "created_utc": "2026-02-03 20:52:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gftkr",
          "author": "krait17",
          "text": "Any workflow for comfyui that has the Cover and Repaint feature ?",
          "score": 2,
          "created_utc": "2026-02-04 02:06:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hjm14",
              "author": "nicedevill",
              "text": "I would like to know as well.",
              "score": 1,
              "created_utc": "2026-02-04 06:26:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ip0cb",
                  "author": "krait17",
                  "text": "Dont bother with comfy, i've followed this tutorial and has all the features + it's ultra fast, like a few seconds compared to +30 seconds on comfy + the loading model time. [https://www.youtube.com/watch?v=QzddQoCKKss](https://www.youtube.com/watch?v=QzddQoCKKss)",
                  "score": 2,
                  "created_utc": "2026-02-04 12:28:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pxyv1",
          "author": "EasternAd8821",
          "text": "wtf!?! even if these are cherry picked, if it can do this 1 out of 4 times that is amazing. Ace is a Chinese company/group? they must be because it's the only place solid, amazing, rapid, open source AI research happens any more it seems like.",
          "score": 2,
          "created_utc": "2026-02-05 14:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fxr17",
          "author": "-p-e-w-",
          "text": "Seeing things like that makes you wonder how many industries will still exist 10 years from now.",
          "score": 3,
          "created_utc": "2026-02-04 00:25:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e2cg9",
          "author": "marcoc2",
          "text": "Language support?",
          "score": 0,
          "created_utc": "2026-02-03 18:55:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3e3qmi",
              "author": "Segaiai",
              "text": "Their demos have English, Chinese, Japanese, Korean, Arabic, Spanish, and Norwegian, but I haven't seen a specific language list. The only Korean and Japanese examples used English letters, but they also switched up how they wrote in Chinese, so maybe they were showing range.",
              "score": 8,
              "created_utc": "2026-02-03 19:01:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3j0sct",
                  "author": "guigs44",
                  "text": "> The only Korean and Japanese examples used English letters\n\nPer the Technical report: \"For non-Roman scripts (e.g., Chinese, Japanese, Thai), we implement a stochastic Romanization\nstrategy, converting 50% of lyrics into phonemic representations during training. This approach enables the model to\nshare phonological representations across languages, significantly enhancing pronunciation accuracy for rare tokens\nwithout expanding the vocabulary size.\"",
                  "score": 3,
                  "created_utc": "2026-02-04 13:40:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3exayz",
              "author": "ANR2ME",
              "text": "They mentioned 50 languages üòÖ",
              "score": 1,
              "created_utc": "2026-02-03 21:19:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ey87c",
          "author": "Nexter92",
          "text": "We are so fucking cook, even music will not be human only",
          "score": 2,
          "created_utc": "2026-02-03 21:24:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hud07",
              "author": "lemondrops9",
              "text": "so many AI songs on Youtube its getting very hard to tell what is or is not AI",
              "score": 1,
              "created_utc": "2026-02-04 08:00:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3g5lpy",
          "author": "CoUsT",
          "text": "Holy shit!\n\nGreat quality and such amount of features/tuning/configuration is just insane. Near instant generation is a nice bonus.",
          "score": 1,
          "created_utc": "2026-02-04 01:08:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gabot",
          "author": "Perfect-Campaign9551",
          "text": "The comfy workflows have problems I get a lot of distortion with drum and snare sound",
          "score": 1,
          "created_utc": "2026-02-04 01:35:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hs6xk",
          "author": "tarruda",
          "text": "This is the same company that released the best 128GB RAM LLM: Step 3.5 Flash.\n\nThey are under the radar but clearly have a super strong team of scientists.",
          "score": 1,
          "created_utc": "2026-02-04 07:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i08hs",
          "author": "sagiroth",
          "text": "Silly question but can this be used to make game sounds like footsteps ?",
          "score": 1,
          "created_utc": "2026-02-04 08:55:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ifjat",
              "author": "djtubig-malicex",
              "text": "Not sure. udio could since it was trained on radio advertising clips and trailer music. maybe fine tune and loras lol",
              "score": 1,
              "created_utc": "2026-02-04 11:16:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3is54f",
          "author": "DocHoss",
          "text": "Anyone know if this plays nice on a Strix Halo?",
          "score": 1,
          "created_utc": "2026-02-04 12:49:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iur8s",
          "author": "techlatest_net",
          "text": "Bookmarked HF demo. Vocal-to-BGM conversion is wild ‚Äì might train my voice on this weekend. Great drop!",
          "score": 1,
          "created_utc": "2026-02-04 13:05:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j2n8c",
          "author": "Stepfunction",
          "text": "Github: [https://github.com/ace-step/ACE-Step-1.5](https://github.com/ace-step/ACE-Step-1.5)",
          "score": 1,
          "created_utc": "2026-02-04 13:50:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nkeuw",
          "author": "lrq3000",
          "text": "Very impressive!\n\nIt generates very usable (ie, ready for editing in a DAW with little musical mistakes) samples at a rate of about 1/4 in my quick test and with very raw prompts, which is incredible! Especially given how fast the samples are generated!\n\nWith better prompts refinement and better understanding of how to use the model (keep in mind the online demo has a much reduced set of features compared to the downloadable full model, and I¬†could not get my head around how to use the repainting feature), it certainly is a game changer for local ai music generation.\n\nTip: it seems it can \"learn\" additional musical theory skills by giving a reference song, and what is particularly interesting is that this happens even if the target musical style is totally different from the reference song, the model can abstract musical concepts beyond the style. For example, it learnt to do complex musical phrasing here : https://youtu.be/7EwZO27pDSs",
          "score": 1,
          "created_utc": "2026-02-05 03:21:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nt3zg",
          "author": "Hot-Employ-3399",
          "text": "UX is much worse than previous version. In previous version we had dockerfile, here we have instructions on how to install [that don't work](https://github.com/ace-step/ACE-Step-1.5/issues/108)¬†\n\n\nPersonally I couldn't get uv sync to work, it failed, printing something about windows, tried uv venv + uv pip, it didn't work as torch and flash attention were installing the same time, had to install torch first, and then I not so related to ACE I've remembered that hf's xet is an absolute garbage that didn't want to download anything at speed >380kB/sec.¬†\nFuck everything about xet. Barely fixed this shit by disabling concurrency in .gitconfig. For some reason it failed if it was enabled¬†\n\n\nHaven't tested further, but let's say after wasting 30 minutes I've changed my mind about comfyui from \"redundant\" to \"actually may be better\"",
          "score": 1,
          "created_utc": "2026-02-05 04:17:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3oossp",
          "author": "lemondrops9",
          "text": "I thought 1.35 was decent. Ace 1.5 is blowing me away.",
          "score": 1,
          "created_utc": "2026-02-05 08:36:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qavjf",
          "author": "Free_Scene_4790",
          "text": "I've only managed to get it working on Comfy. The Gradio/Portable version doesn't work for me.",
          "score": 1,
          "created_utc": "2026-02-05 15:26:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47412b",
          "author": "CreativeEmbrace-4471",
          "text": "Say good buy to copyright strike scams on YT...",
          "score": 1,
          "created_utc": "2026-02-08 03:58:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gb4q4",
          "author": "Thrumpwart",
          "text": "Would be cool if LMStudio supported these models...",
          "score": 1,
          "created_utc": "2026-02-04 01:39:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hu53t",
              "author": "Uncle___Marty",
              "text": "Google \"pinokio\". Its an AI browser (open source) with a bunch of 1 click installers. ace step already has a script im using.",
              "score": 3,
              "created_utc": "2026-02-04 07:58:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jlf1i",
                  "author": "Thrumpwart",
                  "text": "Oh nice! I keep meaning to check out pinokio and never have. Thank you!",
                  "score": 3,
                  "created_utc": "2026-02-04 15:26:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3kc2sr",
              "author": "henk717",
              "text": "Its on our wishlist to, but unless something in the ggml ecosystem adds it its out of scope unfortunately.",
              "score": 2,
              "created_utc": "2026-02-04 17:29:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3kcxxz",
                  "author": "Thrumpwart",
                  "text": "Ah, thank you.",
                  "score": 1,
                  "created_utc": "2026-02-04 17:33:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3if2k6",
          "author": "manipp",
          "text": "So it seems the creator has gone out of his way to make the 'cover' feature destroy any melody of the input song to make sure it won't replicate the melody. He did this, according to the discord, \"Don‚Äôt fuucking second-guess my intentions. It has nothing to do with copyright‚Äîthis design is simply more interesting, and I like how it works. I get to decide how my model is designed. use paid ace-studio or suno\"\n\nVery very disappointing.",
          "score": 1,
          "created_utc": "2026-02-04 11:12:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jcy6h",
              "author": "iGermanProd",
              "text": "Just wait a bit for Comfy folk to figure out a2a. You could reasonably expect it to work well with the VAE being available and the model being a diffusion model. Don‚Äôt attribute malice so quickly. \n\nI‚Äôm not picking any sides, but let‚Äôs be rational and not entitled. I don‚Äôt like when people are so quick to attribute malice and shit on developers for not only releasing a model but also being kind and receptive enough to do it under an MIT license. And while it was said in quite a rude way, I do believe Junmin was only talking about their Gradio demo, not dictating how we should use the model.\n\nNow for the tech bit:\n\nWhat happens now in the Gradio demo is (to my knowledge) not any conspiracy, but rather the audio being turned into LM codes that get used for the diffusion process. Effectively, you only really preserve the structure, some rhythm, and a hint of the melody that way. Like a description. Ergo, it‚Äôs more of a remix/suggestion/alternate reality version. Junmin (one of the authors of this) says he regrets even calling it cover in the first place. \n\nThat‚Äôs because the source audio is NOT currently being applied to the diffusion process like it is in other ‚Äúcover‚Äù features or even image-to-image models, so it only has that structural metadata to go off of. Of course, it sounds nothing like the input. It‚Äôs a bit like asking Gemini to describe an image in as much detail as possible, then taking that text, then running Nano Banana on the result - it‚Äôll be similar but different, because you went through a whole layer of abstraction to get to the result. \n\nBut what you want is an editing workflow, so sending an image to Nano Banana and having it change the image, not guess from a different modality. \n\nAnd this seems like a trivial fix inside something like ComfyUI - just use the VAE, encode input audio, compose encoded audio over random noise (with different proportions to control strength), pass into diffusion, adjust denoising amount (to control strength in a different way), boom, you‚Äôll get a cover. Bonus points if you combine it with the structural LM codes to get probably either a horrible result if they clash, or a really good one if they don‚Äôt.",
              "score": 3,
              "created_utc": "2026-02-04 14:44:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dz0o4",
          "author": "ffgg333",
          "text": "Can someone make a Lora trainer on Google colab?",
          "score": -7,
          "created_utc": "2026-02-03 18:40:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e6l41",
          "author": "Opfklopf",
          "text": "God I hate \"creative\" AI. I don't want to see or hear it anymore. I thought this sub is about LLMs. I guess not, oh well..",
          "score": -37,
          "created_utc": "2026-02-03 19:14:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fab9q",
              "author": "redditscraperbot2",
              "text": "I feel bad for the authors after reading this take. If you followed the project you‚Äôd know they were actually not overly fond of the idea of using it to generate songs and that be the end of it. They want people to use the tools they released as a Swiss Army knife to improve and iterate on their creations. \n\nLike I really got the sense they like music and the creative process and you‚Äôve walked away with the wrong idea.",
              "score": 7,
              "created_utc": "2026-02-03 22:20:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fjkji",
                  "author": "Opfklopf",
                  "text": "Tbf I know nothing about it. I just hate the entire buzz companies create and the trash people spam the internet with so I just react allergically at this point.",
                  "score": -9,
                  "created_utc": "2026-02-03 23:08:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qwxtf8",
      "title": "BalatroBench - Benchmark LLMs' strategic performance in Balatro",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qwxtf8",
      "author": "S1M0N38",
      "created_utc": "2026-02-05 21:12:37",
      "score": 510,
      "num_comments": 57,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qwxtf8/balatrobench_benchmark_llms_strategic_performance/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3ubdfr",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-06 03:45:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3sglyc",
          "author": "mitchins-au",
          "text": "Finally a real world eval",
          "score": 167,
          "created_utc": "2026-02-05 21:29:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3uxsfy",
              "author": "m31317015",
              "text": "Legit something I didn't think of, super cool.",
              "score": 7,
              "created_utc": "2026-02-06 06:28:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3sfbzg",
          "author": "jacek2023",
          "text": "\"If you own a copy of Balatro, you can make your local LLM play it.\" you have my attention",
          "score": 83,
          "created_utc": "2026-02-05 21:23:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3x71rs",
              "author": "addandsubtract",
              "text": "Ironically, attention is all you need.",
              "score": 7,
              "created_utc": "2026-02-06 16:04:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3sgonf",
          "author": "jd_3d",
          "text": "Can you try Opus 4.6 on it? Curios if it improves from 4.5",
          "score": 29,
          "created_utc": "2026-02-05 21:30:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3simb4",
              "author": "S1M0N38",
              "text": "Right now is playing. checkout the twitch stream",
              "score": 32,
              "created_utc": "2026-02-05 21:39:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3tspnf",
                  "author": "JsThiago5",
                  "text": "will cost 1k$ per match ",
                  "score": 26,
                  "created_utc": "2026-02-06 01:52:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3t7u8i",
          "author": "Kholtien",
          "text": "I need a Dwarf Fortress eval",
          "score": 30,
          "created_utc": "2026-02-05 23:50:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wmf2g",
              "author": "IrisColt",
              "text": "You have my sword.",
              "score": 6,
              "created_utc": "2026-02-06 14:22:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3sqwxk",
          "author": "TomLucidor",
          "text": "If it is Jinja2-based then run DGM, OpenEvolve, SICA, or SEAL over it. See which LLM can self-evolve the fastest given the proper scaffold.",
          "score": 54,
          "created_utc": "2026-02-05 22:20:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3svnko",
              "author": "S1M0N38",
              "text": "I will look into those. Thanks",
              "score": 16,
              "created_utc": "2026-02-05 22:44:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3spjzv",
          "author": "Adventurous-Okra-407",
          "text": "One thing I wonder a lot for this eval is the Balatro release date. It existed since Feb 2024 and before that did not exist, so LLMs with more niche and more up to date info in their training data will have a big advantage over those that do not.\n\nThere are no books written about this game, for example.",
          "score": 18,
          "created_utc": "2026-02-05 22:13:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3uynak",
              "author": "Yorn2",
              "text": ">There are no books written about this game, for example.\n\nIf there's wikis or even blog posts though they definitely are getting indexed. Videos probably as well.\n\nA friend of mine created a guide for an obscure MMORPG that almost no one plays despite it being a Western MMO. It's actually only recently gotten popular, but he wrote the guide slowly (I helped with a few things) and put it all online over the course of a few years. For years afterwards not a whole lot of people played it, but all these Chinese bots were still indexing his site. \n\nNow that GLM, Qwen, and others have came out, I'll ask these offline-only models questions about the game and it's crazy how often they actually SOUND LIKE HIM when they talk about the different NPCs and strategies for playing the game. And don't get me wrong, they still hallucinate a lot, but they clearly talk about stuff he does on his website/guide. No where else in the world is this info, so I know they got it from him.",
              "score": 18,
              "created_utc": "2026-02-06 06:35:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3wx9a2",
                  "author": "my_name_isnt_clever",
                  "text": "Google has an ENORMOUS advantage for something like this, being able to train off YouTube data.",
                  "score": 4,
                  "created_utc": "2026-02-06 15:17:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3teloj",
          "author": "InternetExplorer9999",
          "text": "The only benchmark that matters",
          "score": 12,
          "created_utc": "2026-02-06 00:29:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t6ljb",
          "author": "X3liteninjaX",
          "text": "So insanely cool, I love random evals like this. Nice work!",
          "score": 9,
          "created_utc": "2026-02-05 23:43:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t9ezt",
          "author": "Briskfall",
          "text": "Strategic game benches like these are really fun to watch. Testing models for a novel, localized environment for their logic skills is akin to what chess/go research were later then generalized for broader ML applications.",
          "score": 7,
          "created_utc": "2026-02-05 23:59:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v6dyw",
          "author": "Alarming_Bluebird648",
          "text": "this is actually a sick way to test reasoning depth. i wonder how a quantized 70b handles the late game shop decisions bc those are brutal",
          "score": 4,
          "created_utc": "2026-02-06 07:44:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vqpmq",
          "author": "reggionh",
          "text": "Gemini 3 Flash arguably has the most intelligence per $ right now. I have been very impressed. It's a bit quirky, like it makes typos & hallucinates at times but I can live with it. ",
          "score": 4,
          "created_utc": "2026-02-06 10:55:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3thzqh",
          "author": "ayelg",
          "text": "Super cool\n\nWhat are you using to run the stream?",
          "score": 3,
          "created_utc": "2026-02-06 00:48:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w2f60",
              "author": "S1M0N38",
              "text": "Docker with 3 xvfb display -> x11grab -> ffmpeg -> twitch rtmp (everything hosted in Digital Ocean droplet)\nNo OBS",
              "score": 7,
              "created_utc": "2026-02-06 12:26:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3w5a42",
                  "author": "typeomanic",
                  "text": "This guy knows ball",
                  "score": 1,
                  "created_utc": "2026-02-06 12:45:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3wxk3s",
                  "author": "my_name_isnt_clever",
                  "text": "I can only imagine how much you've spent on Opus 4.6 with the stream still going. How long will it run before you'll be able to add it to the leaderboard?",
                  "score": 1,
                  "created_utc": "2026-02-06 15:19:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3tu36o",
              "author": "PM_ME_UR_COFFEE_CUPS",
              "text": "Likely OBS",
              "score": 1,
              "created_utc": "2026-02-06 02:00:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3tpk7d",
          "author": "SeriousGrab6233",
          "text": "This is super sick. This makes me want to make a benchmark now for another game",
          "score": 3,
          "created_utc": "2026-02-06 01:33:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3uwckd",
          "author": "Warthammer40K",
          "text": "oh thank god, my hands are gnarled and frozen into claws from playing Balatro 16 hours a day... now the computer can take over",
          "score": 3,
          "created_utc": "2026-02-06 06:16:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3toppy",
          "author": "FusionCow",
          "text": "we just benchmarking anything atp",
          "score": 4,
          "created_utc": "2026-02-06 01:28:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3uhojc",
          "author": "Ill-Fishing-1451",
          "text": "Very interesting. Can you tell why some models outperform others? What are they doing better?",
          "score": 2,
          "created_utc": "2026-02-06 04:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3x5lt8",
          "author": "tonyunreal",
          "text": "Oh wow, Opus 4.6 just successfully defused an Acrobat vs The Hook round, I'm speechless.",
          "score": 2,
          "created_utc": "2026-02-06 15:57:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xfccr",
              "author": "S1M0N38",
              "text": "Is this good or bad? I‚Äôve only played Balatro a few times",
              "score": 1,
              "created_utc": "2026-02-06 16:43:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3xgvyb",
                  "author": "tonyunreal",
                  "text": "Bad scenario, very good thinking process from Opus. I would argue it has way better crisis-solving capability than me, haha.",
                  "score": 2,
                  "created_utc": "2026-02-06 16:50:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o43wy77",
          "author": "LelouchZer12",
          "text": "Those error bar are pretty concerning",
          "score": 2,
          "created_utc": "2026-02-07 17:06:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44edkz",
              "author": "S1M0N38",
              "text": "Change from std. dev. to Confidence interval 95% in the new version (checkout the website). The new error bars mean that we are 95% sure that the average lies in the error bar. The previous error bars were capturing the data distribution (assuming normal) - this is a wrong assumption given the fact that data are capped at 24 (so the distribution is not symmetric). The average round as main metric is still sub-optimal. There is an issue open on coder/balatrollm where I plan to update how the average round is computed.\n\nThanks for point it out :)",
              "score": 1,
              "created_utc": "2026-02-07 18:32:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3se84l",
          "author": "NigaTroubles",
          "text": "Looks like qwen needs to release there Qwen4",
          "score": 2,
          "created_utc": "2026-02-05 21:18:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u26xj",
          "author": "Alan_Silva_TI",
          "text": "I don‚Äôt really dig Balatro, but something like this applied to turn-based CRPGs (which helps a lot with timing) especially ones that support multiplayer would be an instant viral hit.\n\nI‚Äôve been thinking about this a lot, and I‚Äôm pretty sure that in the near future many games will allow players to use AI (most likely LLMs) as local multiplayer participants.\n\nFrom a technical standpoint, it seems really feasible as all a game really needs is an API that sends the current battle state, plus a structured summary of progression: story context, choices made so far, available options, and constraints. Feed that into an LLM and let it act as another player.\n\nOnce games start exposing that kind of interface, this sort of thing is going to explode.",
          "score": 2,
          "created_utc": "2026-02-06 02:48:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wymlk",
              "author": "my_name_isnt_clever",
              "text": "I wonder if a locally running LLM could outperform traditional video game AI yet. I feel like that's still no right now, but I'd love to try it.",
              "score": 1,
              "created_utc": "2026-02-06 15:24:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o40ubmc",
                  "author": "Alan_Silva_TI",
                  "text": "I mean‚Ä¶ this really only works for games that aren‚Äôt real-time and don‚Äôt require handling hidden information.\n\nThat‚Äôs why I specifically used CRPGs as an example. In those games, you can provide a basic summary of the story so far, plus a detailed list of available actions for each companion:\ncan they attack? cast a spell? which spell? target who?\nAll of that is very easy to describe in text and maps well to logical reasoning.\n\nThe game itself handles all the actual calculations and rules. It just needs to relay the results back to the LLM through a simple combat log, like:\n‚ÄúEnemy received 0 damage because it is immune to that damage type.‚Äù\nLLMs can understand concepts like that just fine.\n\nYou also don‚Äôt need long-term memory of every fight. The LLM only needs to reason about the current encounter or the current dialogue choices.",
                  "score": 1,
                  "created_utc": "2026-02-07 03:40:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3t26ww",
          "author": "my_name_isnt_clever",
          "text": "gpt-oss-20b beating kimi-k2.5 makes no sense. One is 20b, the other is 1000b.",
          "score": 2,
          "created_utc": "2026-02-05 23:19:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3t8nr1",
              "author": "Klutzy-Snow8016",
              "text": "Current LLMs can't actually generalize much. Probably OpenAI had this obscure game or something similar in the training data, while Moonshot did not.",
              "score": 7,
              "created_utc": "2026-02-05 23:55:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3v62wz",
                  "author": "North-Act-7958",
                  "text": "obsucre game that was nominated for game of the year award of 2024 and won the indie category",
                  "score": 8,
                  "created_utc": "2026-02-06 07:41:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3uhat7",
                  "author": "OUT_OF_HOST_MEMORY",
                  "text": "GPT-OSS also reasons for \\~15k tokens sometimes, I don't know know how Kimi compares, but its probably helping out somehow",
                  "score": 4,
                  "created_utc": "2026-02-06 04:24:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3vac14",
          "author": "Joltie",
          "text": "I thought about doing the same for Into the Breach.\n\n\nI think the set rules of the game lend themselves well to AI evaluation of the ideal paths.",
          "score": 1,
          "created_utc": "2026-02-06 08:21:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vetz1",
          "author": "Hambeggar",
          "text": "Is Balatro considered an especially cerebral card game...?",
          "score": 1,
          "created_utc": "2026-02-06 09:04:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vlplh",
          "author": "RevealIndividual7567",
          "text": "This makes me want to setup a similar benchmark for factorion now, very cool.",
          "score": 1,
          "created_utc": "2026-02-06 10:10:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3voge2",
          "author": "goniszewski",
          "text": "Well, this is something new",
          "score": 1,
          "created_utc": "2026-02-06 10:35:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vr8u7",
          "author": "Mythril_Zombie",
          "text": "I can finally unlock all the things.",
          "score": 1,
          "created_utc": "2026-02-06 11:00:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vvq9c",
          "author": "zball_",
          "text": "lmfao ds3.2 proved itself once again being the OSS model generalization goat",
          "score": 1,
          "created_utc": "2026-02-06 11:37:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3w1b9v",
          "author": "tonyunreal",
          "text": "On the twitch stream, your bot keeps resetting the game after long thinking at the ante 5 boss blind. Better check the code for that, someone in chat said the bot resets the game with long holding the R key.",
          "score": 1,
          "created_utc": "2026-02-06 12:19:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w7e0v",
              "author": "S1M0N38",
              "text": "I've check the logs. Those were cause by OpenRouter returning invalid responses (partial JSON). It never happened with previous models. I will exclude those runs from the benchmark and implement the fix",
              "score": 3,
              "created_utc": "2026-02-06 12:59:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3wk6nh",
                  "author": "tonyunreal",
                  "text": "Glad you found the problem. Please keep us updated, the stream is a breeze to watch.",
                  "score": 1,
                  "created_utc": "2026-02-06 14:10:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3w2o65",
              "author": "S1M0N38",
              "text": "Prolly 3 tool calls error/fail in a row - This a is like game over. I'll check the logs tho.",
              "score": 1,
              "created_utc": "2026-02-06 12:28:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wf252",
          "author": "artisticMink",
          "text": "The ONLY viable benchmark.",
          "score": 1,
          "created_utc": "2026-02-06 13:43:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ztxkj",
          "author": "sloptimizer",
          "text": "Yes! Can we please have more fun and creative benchmarks like this?!",
          "score": 1,
          "created_utc": "2026-02-06 23:57:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o444k6d",
          "author": "dtdisapointingresult",
          "text": "does your benchmark allow the LLM to always know the list of jokers that exist, and which a player might want to hold out for? This is the \"meta\" that is necessary to beat the game.\n\nTo be fair it should also have a memory, let the LLM write a personal journal with their analysis of what worked and what didn't, and have it rewrite it at the end of every run.",
          "score": 1,
          "created_utc": "2026-02-07 17:44:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qym566",
      "title": "I trained a 1.8M params model from scratch on a total of ~40M tokens.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qym566",
      "author": "SrijSriv211",
      "created_utc": "2026-02-07 18:57:42",
      "score": 491,
      "num_comments": 83,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qym566/i_trained_a_18m_params_model_from_scratch_on_a/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o45jnx7",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-07 22:10:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44v2d6",
          "author": "FPham",
          "text": "Creating model from scratch is the hardcore LLM stuff.  Kudos (if we are still using those in 2026)",
          "score": 41,
          "created_utc": "2026-02-07 19:57:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44wgox",
              "author": "SrijSriv211",
              "text": "I've always been interested in training my own llm from scratch so yeah here we are I guess.",
              "score": 16,
              "created_utc": "2026-02-07 20:05:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o45y0xk",
          "author": "cosmicr",
          "text": "amazing you've pretty much reached GPT-2 level of quality on such a smaller scale.\n\nGiven your training data set, I can see lots of applications for this sort of thing in games. That is if the gaming community can ever get over the use of AI as a tool.\n\nHow big was the final model on disk?",
          "score": 21,
          "created_utc": "2026-02-07 23:33:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o478a7l",
              "author": "SrijSriv211",
              "text": "> That is if the gaming community can ever get over the use of AI as a tool.\n\nSo true.\n\n> How big was the final model on disk?\n\n25 MBs",
              "score": 19,
              "created_utc": "2026-02-08 04:27:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o49f8gt",
                  "author": "Palmquistador",
                  "text": "Daaang that is small. Awesome project!",
                  "score": 5,
                  "created_utc": "2026-02-08 15:04:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o44m63w",
          "author": "1ncehost",
          "text": "This is very cool. EleutherAI discord would probably be interested and has a lot of expertise that can help.",
          "score": 43,
          "created_utc": "2026-02-07 19:11:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44ms1h",
              "author": "SrijSriv211",
              "text": "Thank you!",
              "score": 8,
              "created_utc": "2026-02-07 19:14:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o49qy6y",
              "author": "itsmekalisyn",
              "text": "Is their discord active still?",
              "score": 1,
              "created_utc": "2026-02-08 16:04:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o44lrey",
          "author": "Single_Ring4886",
          "text": "Did you considered to do some \"post training\" to teach model single of just few actually useful \"tricks\"? The simplest thing which occurs to me is for example to detect names in text so you could make them via simple script into \"bold\". I think such \"practical\" applications for very small and very fast and cheap models is what open source could really shine in comparison to huge universal models.",
          "score": 12,
          "created_utc": "2026-02-07 19:09:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45b7oe",
              "author": "Budget-Juggernaut-68",
              "text": "We have those already. they're called NER models.",
              "score": 7,
              "created_utc": "2026-02-07 21:24:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o44mqcm",
              "author": "SrijSriv211",
              "text": "Yeah I'm thinking of post training. That's one of things I'll be working on next. First I want the pre-training to give even better results. I don't a loss of 3.5 is really that good. I'm also going to scale the base dataset size and model size a little more. This was more a stress test to check if it can generate good text with just 1M non-embedding parameters on such a diverse and dense dataset or not.",
              "score": 4,
              "created_utc": "2026-02-07 19:14:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44nfwu",
                  "author": "Single_Ring4886",
                  "text": "Good speed :) because once small model (which you can use even on cpu) is \"useful\" with something practical people might start using it :) and it would be more than just one time experiment.",
                  "score": 4,
                  "created_utc": "2026-02-07 19:17:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o45g87m",
          "author": "Standard-Influence67",
          "text": "I wonder if you do post train nowÔºåit can produce reasonable outputÔºåor you need to scale the parameters to do so?",
          "score": 6,
          "created_utc": "2026-02-07 21:51:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45gn7w",
              "author": "SrijSriv211",
              "text": "I'll post train and also scale parameters and dataset. Post training is my first priority right now.",
              "score": 2,
              "created_utc": "2026-02-07 21:53:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45hg28",
                  "author": "Standard-Influence67",
                  "text": "cool. but I wonder if keep this parameters then only do post train can let the model produce reasonable output or not.so maybe you can find out.",
                  "score": 2,
                  "created_utc": "2026-02-07 21:58:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o45zlol",
          "author": "Iory1998",
          "text": "Cool work. I wish you good luck for future iterations.",
          "score": 3,
          "created_utc": "2026-02-07 23:44:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4764dm",
              "author": "SrijSriv211",
              "text": "Thank you :)",
              "score": 2,
              "created_utc": "2026-02-08 04:12:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44qh8c",
          "author": "Tiny_Arugula_5648",
          "text": "It's funny most people haven't ever seen a real hallucination.. The weird rambling babbling that is almost coherent but not really..  That's what you get from small models.. Never really understood why people started calling false statements hallucinations when it went mainstream. The moment you read a real hallucination like this it really does make sense to call them hallucinations because it reads like someone who is totally out of their minds on something. ",
          "score": 13,
          "created_utc": "2026-02-07 19:33:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44qo84",
              "author": "SrijSriv211",
              "text": "Haha yes üòÇ",
              "score": 5,
              "created_utc": "2026-02-07 19:34:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44mz8a",
          "author": "1ncehost",
          "text": "By the way, a lot of SLM training work is consolidated in the nanogpt speedruns to glean from. Not poo pooing because im an enthusiast in this space also and appreciate toy models like this. Looking forward to your updates.",
          "score": 5,
          "created_utc": "2026-02-07 19:15:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44ngbn",
              "author": "SrijSriv211",
              "text": "Yeah ik üòÖ I'm working on it just for fun. Usually when I'm exhausted after studying for my exams. lol! I'll keep working on it cuz it's really fun. I want to see how far can I push it.",
              "score": 2,
              "created_utc": "2026-02-07 19:18:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44nzs1",
                  "author": "1ncehost",
                  "text": "Warning: very deep rabbit hole lol! Enjoy!",
                  "score": 4,
                  "created_utc": "2026-02-07 19:20:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o45tnr1",
          "author": "Madrawn",
          "text": "The idea seems clever. I think I might nap the code and run a couple tests myself.\n\nHave you compared how it fares against a basic GPTMini (\\[LayerNorm, Self-attention, Residual connection, LayerNorm, MLP\\]-blocks) network of similar parameter count and shape? That's usually were my \"novel\" architectures go to die. But also, if it performs vastly different/worse it's usually a sign of a bug, which are hard to notice if it works at all.\n\nThese networks can compensate for a lot of architectural mistakes at a performance/quality cost.\n\nAs for data sets, any reason why you're not using any of the hundreds available on huggingface? Tinystories for simple text, alpaca-python for instruct python code, wiki-text(needs some cleaning for LLMs) and openwebmath for stress testing. Those I tend to use for stuff like this.\n\nEdit: You seem to prepend the sink token at every single step. Is that intentional? It essentially makes your context grow twice as fast.",
          "score": 3,
          "created_utc": "2026-02-07 23:06:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4761ef",
              "author": "SrijSriv211",
              "text": "> Have you compared how it fares against a basic GPTMini ([LayerNorm, Self-attention, Residual connection, LayerNorm, MLP]-blocks) network of similar parameter count and shape?\n\nI did train Andrej Karpathy's nanoGPT on same dataset and tried to keep similar number of parameters. Strawberry seems to perform far better than that.\n\n> if it performs vastly different/worse it's usually a sign of a bug\n\nyes strawberry was performing weirdly in training. Retention was not working well with SPDA. The problem was that the generated weights were too noisy for SPDA. AFT managed to handle that however SPDA couldn't. That's why I added post normalization in both `produce` and `forward` functions in Retention. That fixed the bug completely.\n\n> As for data sets, any reason why you're not using any of the hundreds available on huggingface? Tinystories for simple text, alpaca-python for instruct python code, wiki-text(needs some cleaning for LLMs) and openwebmath for stress testing. Those I tend to use for stuff like this.\n\nTBH. I was just bored. Had nothing to do so I decided to waste my time by manually scrapping datasets. lol! Also the reason why I didn't use TinyStories cuz it's just too simple.\n\n> You seem to prepend the sink token at every single step. Is that intentional? It essentially makes your context grow twice as fast.\n\nYeah that's intentional. That's for attention sink. Similar idea is implemented in GPT-OSS as well. Also it doesn't grow the context. Think like this. input `<|sink|>Test prompt` -> model predicts `ing` which makes it `Test prompting`. Notice how I dropped `<|sink|>` in the final results. That's what's happening. I'll implement it at an architecture level similar to GPT-OSS",
              "score": 4,
              "created_utc": "2026-02-08 04:11:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o49s26j",
          "author": "gjsmo",
          "text": "Just curious, what's the training time (and hardware) like for such a small model? I would imagine it could be done on CPU only or basically any modern GPU, but I've never trained a model from scratch.",
          "score": 3,
          "created_utc": "2026-02-08 16:09:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49z5b5",
              "author": "SrijSriv211",
              "text": "It was trained on my old PC which has Intel i3 3rd gen, 8 GBs of ram and no GPU, and it took about 7-8 minutes per 100 steps. It took ~13 hrs to complete 10k steps of training.\n\nNOTE: It took 7-8 minutes per 100 steps cuz the retention mechanism is still pretty rough in terms of optimization. I'm working on it. The current draft I'm working on is able to train 100 steps in just 4-5 minutes with exact same setup.",
              "score": 1,
              "created_utc": "2026-02-08 16:44:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4a0iib",
                  "author": "gjsmo",
                  "text": "Wow - so I'd imagine almost any GPU could do it in minutes. Could be very interesting to play around with completely different training data or optimization techniques!",
                  "score": 1,
                  "created_utc": "2026-02-08 16:50:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o44ovsn",
          "author": "tob8943",
          "text": "Why is it repeating your prompt",
          "score": 2,
          "created_utc": "2026-02-07 19:25:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44pssc",
              "author": "SrijSriv211",
              "text": "It's not repeating the prompt. In the `generate` function I just append the original prompt before the generated tokens after the generation is complete.",
              "score": 5,
              "created_utc": "2026-02-07 19:30:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44pxhs",
                  "author": "tob8943",
                  "text": "thanks for answering",
                  "score": 3,
                  "created_utc": "2026-02-07 19:30:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o45ewts",
              "author": "ResidentPositive4122",
              "text": "Base models (or pre-trained) don't have a \"prompt\" in the sense that we use with modern LLMs (anything after gpt3.5). Their \"prompt\" is simply the beginning of a piece of text. And they generate the next probable token on that beginning. You would need to take this model and fine-tune it on prompt - answer pairs to have it work as a modern LLM.",
              "score": 3,
              "created_utc": "2026-02-07 21:44:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o451lew",
          "author": "mukz_mckz",
          "text": "This is cool! What hardware did you use and what did the training time look like?",
          "score": 2,
          "created_utc": "2026-02-07 20:32:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o452tnl",
              "author": "SrijSriv211",
              "text": "It was a stress test for the architecture so I trained it on my super low end potato PC. It has (Ik you might not believe it) intel i3 3rd gen cpu, 8 gbs of ram and no gpu. It took \\~7-8 minutes per 100 steps and the entire training was complete in just \\~13 hours.\n\nhttps://preview.redd.it/b1rejavvv4ig1.png?width=655&format=png&auto=webp&s=69082a7a1a6458c5183339ba6dab5bd3213a5f19\n\n  \n",
              "score": 10,
              "created_utc": "2026-02-07 20:39:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o452vtv",
                  "author": "SrijSriv211",
                  "text": "https://preview.redd.it/n1exql7zv4ig1.png?width=924&format=png&auto=webp&s=51b4c6398e6403af8d204b68fbc1aba337727672\n\n  \n",
                  "score": 3,
                  "created_utc": "2026-02-07 20:39:58",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o45otuk",
                  "author": "citaman",
                  "text": "Maybe you can try the `google colab` with gpu instance or `kaggle` with double gpu instance with some free instance per week to ever speed up or have a bigger model like 10M :D ",
                  "score": 3,
                  "created_utc": "2026-02-07 22:38:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o452q0w",
          "author": "BasketFar667",
          "text": "Very cool, but can it talk to the user, like \"Hello?\"? Can I try it if so?",
          "score": 2,
          "created_utc": "2026-02-07 20:39:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4543j5",
              "author": "SrijSriv211",
              "text": "It's just a pre-trained model. No post-training applied so it can't really talk like \"Hello. MODEL: HI! How are you?\" kinda thingy. Though it can generation conversation sentences which you can see in one of the screenshots where it creates a conversation between Arthur & Dutch (2 characters from RDR2). You can download the model from the [releases page](https://github.com/SrijanSriv211/Strawberry/releases/tag/s1)",
              "score": 4,
              "created_utc": "2026-02-07 20:46:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o458ym1",
          "author": "Longjumping_Spot5843",
          "text": "Can it make a coherent sentence or nah?",
          "score": 2,
          "created_utc": "2026-02-07 21:12:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45a3pz",
              "author": "SrijSriv211",
              "text": "Sometimes it can. Considering how small the model is and how dense and diverse the dataset is. I don't expect a proper coherent sentence at this scale. At least without post training, nope. After post training the model might generate better coherent sentences.",
              "score": 2,
              "created_utc": "2026-02-07 21:19:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o45khaw",
          "author": "INtuitiveTJop",
          "text": "This would be really cool for autocorrect on phones - something so small and light might be great at fixing sentences after the fact.",
          "score": 2,
          "created_utc": "2026-02-07 22:14:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o474way",
              "author": "SrijSriv211",
              "text": "Yes. Also the combination of GLobal Linear attention + Local Standard MHA attention will also make it easy for phones to run!",
              "score": 2,
              "created_utc": "2026-02-08 04:03:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o46e3m6",
          "author": "vinnybag0donuts",
          "text": "How'd you decide the architecture for the retention mechanism's wT, wC = wC, new\\_weights swap? It stores O(d¬≤) and derives L layers' worth of weights dynamically whereas I think typically transformers store O(L √ó d¬≤) parameters across L layers.",
          "score": 2,
          "created_utc": "2026-02-08 01:11:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4770d3",
              "author": "SrijSriv211",
              "text": "I did that cuz that was the only idea I had tbh. My intuition was to update current weights and swap it and repeat that again. That was slow, stable and easy to implement.",
              "score": 2,
              "created_utc": "2026-02-08 04:18:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o46e4lq",
          "author": "Pvt_Twinkietoes",
          "text": "Could you explain what you're trying to do like you're talking to a non-technical?",
          "score": 2,
          "created_utc": "2026-02-08 01:12:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o474n37",
              "author": "SrijSriv211",
              "text": "I'm trying to generate the attention qkv parameters on the fly using the input prompt. In standard transformers the attention qkv parameters are learned during pretraining and are fixed during inference. In Strawberry they aren't.",
              "score": 2,
              "created_utc": "2026-02-08 04:02:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4759za",
                  "author": "Pvt_Twinkietoes",
                  "text": "What's the advantage of doing this?",
                  "score": 1,
                  "created_utc": "2026-02-08 04:06:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o477xe9",
          "author": "zball_",
          "text": "The attention part just sound like fast weight programmers nowadays. But a learnable FFN is definitely interesting.",
          "score": 2,
          "created_utc": "2026-02-08 04:25:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o478fjk",
              "author": "SrijSriv211",
              "text": "Yeah I took inspiration from fast weights and hypernetworks üòÖ",
              "score": 1,
              "created_utc": "2026-02-08 04:28:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o47gq91",
          "author": "HillaryPutin",
          "text": "Wow that is remarkable fact recollection for a model that is just a few MB in size.",
          "score": 2,
          "created_utc": "2026-02-08 05:31:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47h1uk",
              "author": "SrijSriv211",
              "text": "Yeah! In terms of both text generation quality and final training loss, it is better than Andrej Karpathy's vanilla nanoGPT trained on same dataset and similar model size!",
              "score": 3,
              "created_utc": "2026-02-08 05:34:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o47ps3u",
                  "author": "HillaryPutin",
                  "text": "What do you do for work? ",
                  "score": 2,
                  "created_utc": "2026-02-08 06:50:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47zy20",
          "author": "UnluckyAdministrator",
          "text": "Impressive! Very brave training your own model. Good worküëå",
          "score": 2,
          "created_utc": "2026-02-08 08:24:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4837ne",
              "author": "SrijSriv211",
              "text": "Thank you :D",
              "score": 1,
              "created_utc": "2026-02-08 08:55:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o485454",
          "author": "stuehieyr",
          "text": "Wish I can do that and use my custom optimizer which groks fast.",
          "score": 2,
          "created_utc": "2026-02-08 09:13:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48a7yb",
              "author": "SrijSriv211",
              "text": "Your optimizer groks fast!!?? How? That's so amazing!",
              "score": 2,
              "created_utc": "2026-02-08 10:01:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48an47",
                  "author": "stuehieyr",
                  "text": "I can give you a hint if that‚Äôs alright as the paper isn‚Äôt yet published üòÖ. So there‚Äôs Lambert W function right? You can make the learning rate ‚Äúbreathe‚Äù as per difficult examples vs easy examples using it, setting a dynamic learning rate. You can tweak Adam to have this lambert W self balance the learning rate and it will automatically spend more time in the hard landscapes and grok fast. But this only works when you do full FP16 fine tune or train. Quantized it didn‚Äôt work at all.",
                  "score": 2,
                  "created_utc": "2026-02-08 10:05:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o49cmib",
          "author": "Particular_Garbage32",
          "text": "how did you learn to build from scratch ? did you have to use crazy math ?",
          "score": 2,
          "created_utc": "2026-02-08 14:50:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49e5go",
              "author": "SrijSriv211",
              "text": "I've always been interested in making my own llms and architectures. I watched Andrej Karpathy, 3blue1brown, welch labs and bycloud videos. I also read research papers and articles. TBH it's more of intuition than some crazy math. In fact the math for retention is remarkably simple. You just have to come up with some ideas and use some simple mathematics and logic in code. That's all.",
              "score": 2,
              "created_utc": "2026-02-08 14:58:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4bktrs",
          "author": "HarjjotSinghh",
          "text": "\\*\"Ohhhh, ‚Äò40M tokens‚Äô‚Äîso you trained your AI on ‚Äòhow many words are in the Bible.‚Äô\"\\*",
          "score": 2,
          "created_utc": "2026-02-08 21:23:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4cgcxt",
          "author": "FaithlessnessLife876",
          "text": "Interesting & Impressive!",
          "score": 2,
          "created_utc": "2026-02-09 00:16:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4e26lc",
              "author": "SrijSriv211",
              "text": "Thank you",
              "score": 1,
              "created_utc": "2026-02-09 06:05:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4d3f4o",
          "author": "Legal-Assistant-615",
          "text": "Have you open source it on github?",
          "score": 2,
          "created_utc": "2026-02-09 02:23:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dyg9w",
              "author": "SrijSriv211",
              "text": "Yes it's linked on the post",
              "score": 1,
              "created_utc": "2026-02-09 05:35:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4dzipr",
          "author": "AdForward9067",
          "text": "this looks fantastic to me! I had always wanted to do this. Is the model available for download? Is it feasible to use it in toolrun? Because my coding works are primarily on Python, C#, C++ and JavaScript too, the larger model size out there are actually an 'extra' load for mine. I would like to try to run it in my envinronment",
          "score": 2,
          "created_utc": "2026-02-09 05:43:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4e24jz",
              "author": "SrijSriv211",
              "text": "It's available for download in the releases page on the repo. The repo is linked. Unfortunately this is just a base model, it's not instruction tuned or agentic tool tuned. I'm working on that right now.",
              "score": 1,
              "created_utc": "2026-02-09 06:05:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o462oye",
          "author": "kind_cavendish",
          "text": "One question, does it know about Megumin from konosuba? And if so, what does it know?",
          "score": 1,
          "created_utc": "2026-02-08 00:03:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o476a7s",
              "author": "SrijSriv211",
              "text": "I don't think it knows about that. The dataset doesn't contain Anime related stuff.",
              "score": 1,
              "created_utc": "2026-02-08 04:13:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qy0l26",
      "title": "Nemo 30B is insane. 1M+ token CTX on one 3090",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qy0l26/nemo_30b_is_insane_1m_token_ctx_on_one_3090/",
      "author": "Dismal-Effect-1914",
      "created_utc": "2026-02-07 01:39:58",
      "score": 377,
      "num_comments": 103,
      "upvote_ratio": 0.97,
      "text": "Been playing around with llama.cpp and some 30-80B parameter models with CPU offloading. Currently have one 3090 and 32 GB of RAM. Im very impressed by Nemo 30B. 1M+ Token Context cache, runs on one 3090, CPU offloading for experts. Does 35 t/s which is faster than I can read at least. Usually slow as fuck at this large a context window. Feed it a whole book or research paper and its done summarizing in like a few mins. This really makes long context windows on local hardware possible. The only other contender  I have tried is Seed OSS 36b and it was much slower by about 20 tokens.",
      "is_original_content": false,
      "link_flair_text": "Generation",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qy0l26/nemo_30b_is_insane_1m_token_ctx_on_one_3090/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o4235o4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-07 10:05:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40mfn1",
          "author": "ubrtnk",
          "text": "Can you share your configuration?",
          "score": 37,
          "created_utc": "2026-02-07 02:49:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xx3p",
              "author": "Dismal-Effect-1914",
              "text": "RTX 3090  \n32GB DDR4 3200  \ni5-12600KF  \nFresh Ubuntu 24.04 server install, Cuda 13, latest drivers, bare metal.\n\nsudo nice -n -20 llama-server --hf-repo unsloth/Nemotron-3-Nano-30B-A3B-GGUF --hf-file Nemotron-3-Nano-30B-A3B-UD-Q4\\_K\\_XL.gguf --alias \"unsloth/Nemotron-3-Nano-30B-A3B\" --fit on --min\\_p 0.01 --temp 0.6 --top-p 0.95 --ctx-size 1024000 --port 8001 --jinja --host¬†[0.0.0.0](http://0.0.0.0/)¬†\\--cpu-moe --flash-attn on",
              "score": 95,
              "created_utc": "2026-02-07 04:04:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o411j1r",
                  "author": "ClimateBoss",
                  "text": "nice make a difference on tk/s?",
                  "score": 17,
                  "created_utc": "2026-02-07 04:30:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o42vj1w",
                  "author": "metamec",
                  "text": "You could condense those two -hf params into:\n\n`-hf unsloth/Nemotron-3-Nano-30B-A3B-GGUF:UD-Q4_K_XL`",
                  "score": 13,
                  "created_utc": "2026-02-07 13:52:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o41w655",
                  "author": "PooMonger20",
                  "text": "Thank you for sharing. Could you share what is your actual usecase? coding or something else?\n\n\nEdit: I just tried to make it vibe code a Tetris-like game in Python, it can't do it properly, code compiles but lots of missing parts.",
                  "score": 7,
                  "created_utc": "2026-02-07 08:55:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o42auc3",
                  "author": "Numerous_Mulberry514",
                  "text": "Try the iq4-nl, in my testing it had better complexity while using ~4gb less vram",
                  "score": 3,
                  "created_utc": "2026-02-07 11:19:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o41yru7",
                  "author": "IrisColt",
                  "text": "I kneel... I am going to check if this works for Win11.",
                  "score": 2,
                  "created_utc": "2026-02-07 09:21:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o42txko",
                  "author": "ubrtnk",
                  "text": "Thank you. I figured out a similar llama-swap config with splitting ngl as 30 and cpu moe as 23. Got 900k context on one 3090 and 16g of vram and still got 50-60tps",
                  "score": 2,
                  "created_utc": "2026-02-07 13:43:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o42bl8k",
                  "author": "bguberfain",
                  "text": "Maybe a dumb question, but my fresh build form llama-cpp got me this error:\n`error: invalid argument: --host¬†0.0.0.0¬†--cpu-moe`\nSo no host and cpu-moe options on latest version? Or is there as different fork/branch form lllama-cpp?",
                  "score": 1,
                  "created_utc": "2026-02-07 11:26:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o419ovn",
          "author": "JoNike",
          "text": "Was curious, tried the mxfp4 version with my 5080/192gb ram and got some pretty good results\n\n - 256K context, ncmoe=20: 81.7 t/s\n - 1M context, ncmoe=27: 69.6 t/s\n\nCache-type q4_0",
          "score": 27,
          "created_utc": "2026-02-07 05:31:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41quv1",
              "author": "pmttyji",
              "text": "You have decent config so don't use q4 cache, not good for stuff like Agentic coding. At least don't use q4 for K because it's sensitive. So `-ctk q8 -ctv q4` is last decent combination.\n\nNow could you please share t/s stats with `-ctk q8 -ctv q8`  for me? With 128K & 256K context. Thanks",
              "score": 18,
              "created_utc": "2026-02-07 08:04:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o43hwot",
                  "author": "JoNike",
                  "text": "So, I was actually curious about that. I'm far from an expert in local llm. I wasn't necessary thinking of using as an agent model (tho, maybe, why not!) so I was thinking the cache quant would not be as big of a deal.\n\nThat said, I did task Claude to do some tests (open claude in tmux, give it access to my llama server in a tmux pane and ask it to iterate config to measure speed and quality) for me overnight to determine if the q4 cache had an effect and the consensus was that it didn't have a noticeable impact and the vram save enabled me. However, I didn't push informatin retrieval on a giant context and that might be the part where the q4 is problematic.\n\nSummary:\n**q4_0 KV cache has zero measurable quality impact on this model.** Across 16 tests (math, code, knowledge, needle-in-a-haystack, instruction following, long-form coherence), both configs produced **identical or near-identical outputs** with matching correctness. This is expected given the architecture: only 6 out of 52 layers use KV cache.\n\n> q4_0 KV cache quantization only affects the 6 attention layers. The other 46 layers (88.5% of the model) are completely unaffected. This is fundamentally different from a pure transformer where q4_0 would affect every single layer.\n\n\nNow, is that actually true, I'll be honest, I do not know.\n\n| Metric | q4_0 KV | f16 KV (default) |\n|--------|---------|-------------------|\n| **Generation speed** | ~59.5 t/s | ~59.5 t/s |\n| **Prompt processing** | ~100-130 t/s | ~100-130 t/s |\n| **VRAM usage** | 9.0 GB | 10.1 GB |\n| **VRAM saved** | **1.1 GB** | baseline |\n\n\nI'd be curious if those results actually make sense: https://github.com/jo-nike/experiments/blob/main/nemotron-kv-cache-quality-report.md\n\nI did launch a new series of test to get a better view on the \"needle in a haystack\" test that will probe bigger context window for information retrieval.\n\nOnce those tests are done, I'll get you a t/s for the config you asked and get back to you",
                  "score": 6,
                  "created_utc": "2026-02-07 15:53:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o44yphg",
                  "author": "JoNike",
                  "text": "Needle in a haystack results for up to 256k context if that's interesting to anyone (Do note I wasn't pushing my vram to its max for those tests, it was really to test contexts so the t/s is slower): \n\n- https://github.com/jo-nike/experiments/blob/main/nemotron-big-context-needle-results.md\n\n**tl;dr**: q4_0 KV cache is recommended with Nemotron-3-Nano-30B. It delivers identical needle-retrieval accuracy to f16 while saving 1.1 GB of VRAM and providing faster prompt processing. The 6-attention-layer hybrid architecture makes this model exceptionally tolerant of aggressive KV quantization.\n\n---\n\nNow for your question, q8 for k/v cache, i was pushing it to it's max here:\n\n > - 128k: 90.9 t/s, ncmoe=12\n > - 256k: 87.7 t/s, ncmoe=15\n > - 1mil: 62.7t/s, ncmoe=30 (but that crashed on bigger context)\n\nMade me review my q4 config which gave me:\n\n > - 128k: 97.0 t/s, nccmoe=9\n > - 256k: 89.1 t/s, ncmoe=11\n > - 1mil: 64.9 t/s, ncmoe=30\n\nHere's the report if you're interested, it goes in quite bigger details such as t/s over context size: https://github.com/jo-nike/experiments/blob/main/nemotron-q8-kv-cache-results.md\n\n**Bottom line**: \n\nWith my setup, at 256k content, I'm probably better use Q8. At 128k content, I'm probably better with Q4. The difference between them is marginal.\n\n---\n\n**Testing Notes**: \n\n- That methodology of using Claude via claude code as an assistant to iterate over settings, test speed and quality and document the experimentation, was really great and a giant time saver. Claude basically created itself a script, run it, monitor it, take notes. \n\n- I was using sonnet 4.5 for this because I abused opus this week ü§¶, but I would have used Opus otherwise, a local llm could definitely work too.\n\n- Flash attention requires transient vram that grows quite a bit with 500k+ context. Caused oom, had to readjust to give more headroom to flash attention by offloading more to cpu. **Don't overly pack your vram if you intend on actually using the whole 1m context**, you'd risk oom as the context grow due to FA overhead need.\n\n- I could probably optimized the 1mil q4 config a little bit, i offloaded to give FA space and because it's so long to generate the big context, I got tired and didn't care to redo the test to squeeze every little drop of vram I could.\n\n- Generating a 950,000 tokens context takes something like 30 minutes with q8, might have been a bit fast with q4, I didn't time sadly.\n\n- t/s drop significantly once the context get filled. For example, Q4 gets to 19.1 t/s on a 1million context.\n\n- I got that Blackwell gpu recently, hadn't had much chances to play with mxfp4 before, definitely interesting! It's also the first time i try such a big context.\n\n- I probably used about 300k of claude tokens to run these optimization exercises without trying to optimize my usage.\n\nI'd be curious to hear if those reports make sense. I'm quite trusting of Claude + it's not a lot of analysis (tho there is some), it's really just running processes and then compiling results. It does look quite good to me.\n\nIf interested in my llama.cpp config, let me know if you think I can optimize that even more:\n\n---\n\n[Nemotron-3-Nano-30B-A3B-MXFP4_MOE-256K]\n\nmodel = /mnt/data/models/NVIDIA-Nemotron-3-Nano-30B-A3B-MXFP4_MOE.gguf\n\nn-gpu-layers = 52\n\nn-cpu-moe = 15\n\nctx-size = 262144\n\nflash-attn = on\n\ncache-type-k = q8_0\n\ncache-type-v = q8_0\n\nparallel = 1\n\nthreads = 16\n\nmlock = 1\n\n---\n\n[Nemotron-3-Nano-30B-A3B-MXFP4_MOE-1M]\n\nmodel = /mnt/data/models/NVIDIA-Nemotron-3-Nano-30B-A3B-MXFP4_MOE.gguf\n\nn-gpu-layers = 52\n\nn-cpu-moe = 30\n\nctx-size = 1048576\n\nflash-attn = on\n\ncache-type-k = q4_0\n\ncache-type-v = q4_0\n\nparallel = 1\n\nthreads = 16\n\nmlock = 1",
                  "score": 2,
                  "created_utc": "2026-02-07 20:17:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o41gj4i",
              "author": "Dismal-Effect-1914",
              "text": "Damn! Thats real good!",
              "score": 2,
              "created_utc": "2026-02-07 06:28:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o40dm0f",
          "author": "Sergiowild",
          "text": "35 t/s at that context length is wild. curious if you've tested accuracy degradation toward the end of the context window? some models claim big ctx but start hallucinating or losing track of earlier content past a certain point.",
          "score": 44,
          "created_utc": "2026-02-07 01:54:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40f8u6",
              "author": "Dismal-Effect-1914",
              "text": "I havent tried a task long enough yet, But ive seen it chomping through over 100k+ tokens so far and its still just as coherent in its output as the initial prompt, after multiple rounds of questioning. My next test will be feeding it The Decline and Fall of the Roman Empire and asking it a very obscure question lol. Though ive always had to use RAG for this length.",
              "score": 24,
              "created_utc": "2026-02-07 02:04:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o41ie0e",
                  "author": "Eisenstein",
                  "text": "If it saves you any effort, I made a tool that will throw arbitrarily long documents at an LLM and have it continue to write, and will provide a quantified analysis of the output.\n\n* https://github.com/jabberjabberjabber/Context-Tester",
                  "score": 26,
                  "created_utc": "2026-02-07 06:45:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o417qld",
              "author": "nufeen",
              "text": "Regarding Nemotron 3 Nano 30b, I don't bellieve in this big context window support. I tried loading books into context and asking it to search for a phrase similar to the one I specified and retrieve the fragment of the book with it. There was nothing like that in the books (in the exact words, because similar ideas could have been there), but the model literally made up fragments of text, reporting that it had found a quote in the text that sounded exactly like what I had asked it to find. And that was within 200k of context. Q8 quant. I tried lowering the temperature, but it didn't help. I even tried the same model on Openrouter with Nvidia as provider, thinking that maybe something is wrong in my setup. But with the same books and prompt the model hallucinated again.",
              "score": 14,
              "created_utc": "2026-02-07 05:16:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41yaax",
                  "author": "Chromix_",
                  "text": "Same finding here in different long-context tests that I made. Qwen3 Next and a few other models were better at it. Nemo Nano sometimes didn't find the facts I asked for, came up with hallucinated facts, and even didn't find it when providing an exact quote. If felt like the attention mechanism was unable to attend to some (interleaved) tokens.",
                  "score": 8,
                  "created_utc": "2026-02-07 09:16:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o494hh9",
                  "author": "Ok_Warning2146",
                  "text": "How abt kimi linear's long context performance?¬†",
                  "score": 1,
                  "created_utc": "2026-02-08 14:03:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o41vvzm",
                  "author": "Brilliant_Bobcat_209",
                  "text": "I‚Äôm new to this so take it with a pinch of salt, but I guess handling (not crashing) large context windows is different from maintaining attention/quality/accuracy.  In general I‚Äôd always prefer to chunk context up. Your use case sounds more like a retrieval/indexing issue, than a model issue?",
                  "score": 1,
                  "created_utc": "2026-02-07 08:52:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o40hbyz",
              "author": "Fit-Produce420",
              "text": "Every current model degrades near the end of it's context, some start getting gnarly at just half context.¬†",
              "score": 11,
              "created_utc": "2026-02-07 02:17:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41vu9v",
                  "author": "No-Detective-5352",
                  "text": "Remember that this is a Mamba-2 based model, so it might not have the same characteristics as pure transformer architectures, especially regarding retaining memory over longer contexts.\n\nEdit: There you go:  https://build.nvidia.com/nvidia/nemotron-3-nano-30b-a3b/modelcard",
                  "score": 11,
                  "created_utc": "2026-02-07 08:52:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o41qe59",
                  "author": "florinandrei",
                  "text": "Citation needed.",
                  "score": -5,
                  "created_utc": "2026-02-07 07:59:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4186t9",
              "author": "mxforest",
              "text": "Not sure how true it is but there was a guy on this sub who mentioned that Nemotron 3 nano became really unreliable after 16k context in his testing. Not sure what kind of tests he was running. I have asked fair complex questions requiring a lot of thinking and it gave me right answers after consuming 30-50k thinking tokens.",
              "score": 3,
              "created_utc": "2026-02-07 05:19:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o43jfqh",
              "author": "YehowaH",
              "text": "Because most models use technique like yarn or similar to achieve long context sizes. Just check out Nvidia to get the real context size without degeneration: [Ruler](https://github.com/NVIDIA/RULER)",
              "score": 2,
              "created_utc": "2026-02-07 16:00:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o40wiby",
          "author": "dsartori",
          "text": "This model has incredible performance on my AMD hardware also. Far faster than anything else in the 30b class. It‚Äôs great for speed but I find the quality falls a bit short of its peers in my specific use case.",
          "score": 6,
          "created_utc": "2026-02-07 03:54:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40o1gw",
          "author": "TopTippityTop",
          "text": "Can you share your configuration/workflow, how you've set it up to run it that well?",
          "score": 6,
          "created_utc": "2026-02-07 02:59:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40ohgr",
              "author": "Dismal-Effect-1914",
              "text": "sudo nice -n -20 llama-server     --hf-repo unsloth/Nemotron-3-Nano-30B-A3B-GGUF --hf-file Nemotron-3-Nano-30B-A3B-UD-Q4\\_K\\_XL.gguf     --alias \"unsloth/Nemotron-3-Nano-30B-A3B\"     --fit on     --min\\_p 0.01     --temp 0.6     --top-p 0.95     --ctx-size 1024000     --port 8001     --jinja --host [0.0.0.0](http://0.0.0.0) \\--cpu-moe --flash-attn on",
              "score": 16,
              "created_utc": "2026-02-07 03:02:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o40oyjy",
                  "author": "TopTippityTop",
                  "text": "Thank you!",
                  "score": 2,
                  "created_utc": "2026-02-07 03:05:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o40efv3",
          "author": "ruibranco",
          "text": "35 t/s at 1M context on a single 3090 is genuinely impressive. The MoE architecture really pays off here since you only need to load the active experts into VRAM while the rest stays in system RAM. CPU offloading for the inactive experts is the key trick that makes this viable on consumer hardware. Have you tried it with any retrieval-heavy tasks where it needs to reference specific details from early in the context, not just summarization. That's usually where the long context models start to show their limits even if the raw speed holds up.",
          "score": 15,
          "created_utc": "2026-02-07 01:59:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40wc9z",
              "author": "Dismal-Effect-1914",
              "text": "Yeah the MoE arch is really what makes this possible. Nothing super heavy yet. ",
              "score": 3,
              "created_utc": "2026-02-07 03:53:42",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o43fb7s",
              "author": "themixtergames",
              "text": "What‚Äôs the stack for your Reddit account?",
              "score": -2,
              "created_utc": "2026-02-07 15:40:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o43fm8p",
                  "author": "ruibranco",
                  "text": "What you mean?",
                  "score": 1,
                  "created_utc": "2026-02-07 15:41:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o416k32",
          "author": "generousone",
          "text": "What's the vram size of Nemo 30b when it loads in with that context?",
          "score": 4,
          "created_utc": "2026-02-07 05:07:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40ohsz",
          "author": "EbbNorth7735",
          "text": "What are you using to run it and parse the documents?",
          "score": 3,
          "created_utc": "2026-02-07 03:02:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40y4w1",
              "author": "Dismal-Effect-1914",
              "text": "Open-webui",
              "score": 4,
              "created_utc": "2026-02-07 04:06:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o46jiql",
                  "author": "nullnuller",
                  "text": "Could it be attributed to open-webui chunking your long context document? Anyway to verify that you are passing the whole context to the LLM?",
                  "score": 1,
                  "created_utc": "2026-02-08 01:46:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o41ymxp",
          "author": "rexmontZA",
          "text": "What would I need to change in the parameters to be able to run this as best as possible on a 5070 Ti? I have 32GB DDR4 memory and 5800X3D.",
          "score": 3,
          "created_utc": "2026-02-07 09:19:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40islf",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 7,
          "created_utc": "2026-02-07 02:26:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40x4ft",
              "author": "Dismal-Effect-1914",
              "text": "Q4\\_K\\_M - Im not using the standard ngl flag, --fit on, it seems to more optimally load the model without having to manually specify. --cpu-moe is key though. I believe the extra room for the KV cache on card due to MoE offloading is what makes this possible.",
              "score": 4,
              "created_utc": "2026-02-07 03:59:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o40zele",
                  "author": "FluoroquinolonesKill",
                  "text": "Do you really need --cpu-moe with --fit? I thought --fit would handle all that.\n\nEdit: Tried with and without --cpu-moe. No difference observed.",
                  "score": 2,
                  "created_utc": "2026-02-07 04:15:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o41e4j6",
          "author": "Divergence1900",
          "text": "what‚Äôs your ttft?",
          "score": 2,
          "created_utc": "2026-02-07 06:08:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41tft7",
          "author": "AdOrnery4151",
          "text": "1M context at 35 t/s on a single 3090 is honestly wild",
          "score": 2,
          "created_utc": "2026-02-07 08:29:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41ue60",
          "author": "__Maximum__",
          "text": "Yes, the technology is wild. I hope other labs work on this, it can go a long way.",
          "score": 2,
          "created_utc": "2026-02-07 08:38:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o423zyf",
          "author": "dreyybaba",
          "text": "How good is it for coding?",
          "score": 2,
          "created_utc": "2026-02-07 10:13:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43rrjj",
              "author": "Dismal-Effect-1914",
              "text": "It was able to whip up a decent website but I would not expect it to be great at coding. It does beat Qwen 30B in benchmarks though. For my purposes its more of a general use model.",
              "score": 1,
              "created_utc": "2026-02-07 16:41:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44gfps",
                  "author": "ScoreUnique",
                  "text": "What's your preferred agent?",
                  "score": 1,
                  "created_utc": "2026-02-07 18:42:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o410ezw",
          "author": "bigh-aus",
          "text": "Does it do tool calls?",
          "score": 1,
          "created_utc": "2026-02-07 04:22:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42yf6b",
              "author": "samplebitch",
              "text": "Supposedly it is trained for tool calling and reasoning. (Looking at it in LM Studio at the moment)",
              "score": 2,
              "created_utc": "2026-02-07 14:09:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o41y0vq",
          "author": "legit_split_",
          "text": "Kimi Linear might be another contender",
          "score": 1,
          "created_utc": "2026-02-07 09:13:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42fxlj",
              "author": "DOAMOD",
              "text": "With the current optimizations no, Nemo is x3 faster or more, Linear 90tgs vs 260...",
              "score": 1,
              "created_utc": "2026-02-07 12:04:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o46st5j",
              "author": "zoyer2",
              "text": "for coding it wasn't that great, a bit better but nothing crazy",
              "score": 1,
              "created_utc": "2026-02-08 02:44:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o433ols",
          "author": "ahtolllka",
          "text": "I guess it is insanely quanted as 30B in fp8 is 30GB VRAM for weights only",
          "score": 1,
          "created_utc": "2026-02-07 14:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o435gd9",
          "author": "tronathan",
          "text": "I can see this being great to keep around as a sort of \"context manager\" to avoid compaction, maybe.  I see you're running at Q4 - I guess I'm about to answer my own question - Have you tried coding with this? No, of course you havent, cause it's not a coding model. ",
          "score": 1,
          "created_utc": "2026-02-07 14:49:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45p10m",
          "author": "vogelvogelvogelvogel",
          "text": "may i ask how you found the quality of the summary and rough area/topic of the paper.. since i had not so great results in no matter which model..",
          "score": 1,
          "created_utc": "2026-02-07 22:40:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o465rmo",
          "author": "Ok_Warning2146",
          "text": "How come it is ranked very low at #157 in lmarena? Does it have a good ranking in any long context bench?",
          "score": 1,
          "created_utc": "2026-02-08 00:21:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o46xbmu",
              "author": "Dismal-Effect-1914",
              "text": "I dont go by LMArena, afaik those are just user rankings. There are actual benchmarks on the HF model card for long context reasoning that indicate it is competent there.",
              "score": 1,
              "created_utc": "2026-02-08 03:13:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o475fjg",
                  "author": "Ok_Warning2146",
                  "text": "Well, I think it is better to trust benchmark run by third parties than the numbers posted by nvidia  themselves.",
                  "score": 1,
                  "created_utc": "2026-02-08 04:07:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48vpvo",
          "author": "Parking-Bonus-5039",
          "text": "What about vllm?",
          "score": 1,
          "created_utc": "2026-02-08 13:07:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4a39ka",
          "author": "EdenistTech",
          "text": "Great tip, thanks. I am getting 120 t/s on a 5070Ti/5060Ti setup using an mxfp4 version and 900K context. That Blackwell FP4 support is paying off, I guess.",
          "score": 1,
          "created_utc": "2026-02-08 17:04:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4arkno",
              "author": "Dismal-Effect-1914",
              "text": "Could you share the HF link? id like to try this as well!",
              "score": 1,
              "created_utc": "2026-02-08 18:58:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4e5r2l",
          "author": "FPham",
          "text": "I have to check. Most small models when use 1M tokens are very blurry in the middle, kinda know the beginning and end and have feeling there was middle, but can't really remember for sure.",
          "score": 1,
          "created_utc": "2026-02-09 06:36:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41sb9q",
          "author": "Jero9871",
          "text": "Does it run good in ollama? Will test it out later on a 4090, not sure which quants to use.",
          "score": 1,
          "created_utc": "2026-02-07 08:18:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o423rx1",
          "author": "tungd",
          "text": "It‚Äôs a hybrid model, not full transformers. You can get the same effect with a similar hybrid model such as IBM Granite.",
          "score": 1,
          "created_utc": "2026-02-07 10:11:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42voqu",
          "author": "silenceimpaired",
          "text": "That isn't the dense model right... it's MoE? I will assume so since no link was provided to the model. If I'm right, I'm not surprised with the results. Happy for you, but not surprised.",
          "score": 1,
          "created_utc": "2026-02-07 13:53:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40w3zv",
          "author": "Deep_Traffic_7873",
          "text": "Yes it is also the best model to run openclaw",
          "score": -3,
          "created_utc": "2026-02-07 03:52:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43rhgb",
              "author": "Dismal-Effect-1914",
              "text": "This was actually one of the use cases im exploring it for. I kept running into context length errors trying to run openclaw with GLM 4.7, and im not paying for another bigger cloud model. So... run my own local model at 1M context, and try to get some decent intelligence in there.",
              "score": 2,
              "created_utc": "2026-02-07 16:40:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o43t76m",
                  "author": "dstoro",
                  "text": "I am also currently looking into this - have you decided for a model yet?\n\nAlso: If I understood correctly, there are problems with local LLMs and tool-calling. There is a [merge request](https://github.com/openclaw/openclaw/pull/9339) with a fix - but it's not merged yet.",
                  "score": 1,
                  "created_utc": "2026-02-07 16:48:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qvrc59",
      "title": "Some hard lessons learned building a private H100 cluster (Why PCIe servers failed us for training)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qvrc59/some_hard_lessons_learned_building_a_private_h100/",
      "author": "NTCTech",
      "created_utc": "2026-02-04 15:20:42",
      "score": 350,
      "num_comments": 97,
      "upvote_ratio": 0.99,
      "text": "^(Just wanted to dump some notes here after spending the last few months architecting a private training stack (70B+ param models. We initially tried to save budget by looking at standard PCIe servers instead of the HGX/SXM form factors, and honestly, the \"paper math\" vs. reality was a brutal wake-up call.))\n\n^(Thought this might save someone else the headache if you're trying to move from inference to actual training runs on-prem.)\n\n^(1. The \"NVLink Tax\" isn't optional for training. We tried to model this out with PCIe Gen5, but the math just falls apart. When you're doing All-Reduce ops across nodes, PCIe caps out at \\~128 GB/s. NVLink is pushing \\~900 GB/s. If you cheap out here, you basically end up with expensive GPUs sitting idle, waiting for data. For inference, PCIe is totally fine. For training, it‚Äôs a bottleneck that kills your ROI.)\n\n^(2. Storage checkpoints are violent. This was the biggest surprise. Everyone talks about GPU VRAM, but nobody warned us about the checkpoint writes. A 175B model dumps a \\~2.5TB checkpoint. To keep the GPUs from stalling, you need to write that to disk in under a minute. Our standard NFS filer absolutely choked. We had to look at parallel filesystems (Weka/VAST or local NVMe raid just to survive the write bursts.))\n\n^(3. You don't need InfiniBand, but Ethernet is annoying. We didn't have the budget/staff for an InfiniBand fabric, so we went with RoCEv2 on standard switches. It works, but it‚Äôs finicky. One silent buffer overflow or a misconfigured PFC (Priority Flow Control setting can stall the whole cluster. If you go Ethernet, monitor your pause frames religiously.))\n\n^(Anyway, I wrote up a longer deep dive with the specific diagrams and our decision framework for \"Sandbox vs Production\" builds if anyone is interested. Link is pinned in my profile.)\n\n^(Happy to answer questions on the networking side - that RoCEv2 tuning took years off my life.)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qvrc59/some_hard_lessons_learned_building_a_private_h100/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o3jnkaj",
          "author": "laurekamalandua",
          "text": "The kind of content I'm here for. Thanks OP.",
          "score": 97,
          "created_utc": "2026-02-04 15:36:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jsc22",
              "author": "NTCTech",
              "text": "I appreciate it., honestly hard to find deep dives these days that aren't just vendor marketing in disguise, so tried to keep it raw.",
              "score": 61,
              "created_utc": "2026-02-04 15:58:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3jxx8r",
                  "author": "laurekamalandua",
                  "text": "Yes, I checked your history. All of it straightforwardly educational and transparant. I can't tell if this job is for a company (service provider) or a frontier startup, but if you have details about tool usage on the inference/training stack (MLOps architecture) , I'd be interested too üòä Specifically, whether many build their own control plane or resort to OSS.¬†",
                  "score": 10,
                  "created_utc": "2026-02-04 16:24:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3k9yvn",
                  "author": "BallsInSufficientSad",
                  "text": "Is there a discord or another sub where folks talk about training?  This sub, I find, is 99.9% inference folks (which is fine).",
                  "score": 4,
                  "created_utc": "2026-02-04 17:19:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3kx6v4",
                  "author": "Imaginary_Context_32",
                  "text": "A few questions\n\nTraining ‚Äúform scratch, if yes why why?‚Äù Or Fine-tuning or LORA?\n\n Did you test in the Claud before aws,gcp, lambda‚Ä¶..",
                  "score": 1,
                  "created_utc": "2026-02-04 19:04:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3mcdbu",
                  "author": "TheThoccnessMonster",
                  "text": "Can you post the article ‚Äî on mobile finding the link to your write up is a PITA. Thanks! This is very interesting!",
                  "score": 1,
                  "created_utc": "2026-02-04 23:13:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jowk5",
          "author": "beskone",
          "text": "As a storage engineer, I feel a fast NVMe over Fabrics Parallel FS should be the 1st requirement for a training build.\n\nWithout the storage to feed the GPU's, you're gonna have a lot of idle time.\n\nAnd Infiniband for the compute side should be mandatory IMO (RoCEv2 is actually preferable for storage in most cases)\n\nGood writeup of the most common pinch points in these workflows. I think a lot of people overlook the shared storage aspect of training.",
          "score": 29,
          "created_utc": "2026-02-04 15:42:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jryn1",
              "author": "NTCTech",
              "text": "Everyone obsesses over TFLOPS and forgets they drop to zero if the storage controller chokes.\n\nI'm with you on IB for compute SHARP is killer, but we went RoCE purely to avoid the knowledge silo. Our whole team speaks Arista; I didn't want to build a fabric that only one guy knew how to fix.",
              "score": 14,
              "created_utc": "2026-02-04 15:56:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3jvgxs",
                  "author": "beskone",
                  "text": "Arista guy here! IB is actually a really simple protocol. RDMA is built in, no PFC/ECN bullshit like with RoCE. It's a fully switched fabric and if you do Fat-Tree as physical interconnect layout (like a really dumbed down Spine and Leaf) it's fully optimized for AI workloads. \n\nMellanox has a bunch of free training for it, I was able to get through the associate certifications in less than 2 days. It's actually impressive how straightforward it is.",
                  "score": 4,
                  "created_utc": "2026-02-04 16:12:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3m5i4m",
              "author": "TheJrMrPopplewick",
              "text": "IB hasn't been mandatory for compute side in a while now, and there's really no need for it in most moderate AI clusters. 400G / 800G Ethernet fabrics with DCQCN handle multi-node training to thousands of GPUs pretty well. Ultra Ethernet will further push things in this direction.",
              "score": 1,
              "created_utc": "2026-02-04 22:36:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mefc1",
                  "author": "beskone",
                  "text": "Sure you can make it work! But 800Gb IB has less latency and is more efficient overall. Still going to be the preferred choice and is still the choice in the Nvidia Reference Architecture for AI builds.",
                  "score": 1,
                  "created_utc": "2026-02-04 23:24:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jshf3",
          "author": "Long_comment_san",
          "text": "I didn't expect the storage write speed a problem at all. That's a big surprise.",
          "score": 8,
          "created_utc": "2026-02-04 15:59:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jtb6s",
              "author": "NTCTech",
              "text": "Yep, it caught us totally off guard too....everyone benchmarks read throughput feeding the dataset but forgets the massive write burst when the model dumps state.\n\nHonestly considering doing a separate write-up just on the storage tuning because it was such a specific headache.",
              "score": 8,
              "created_utc": "2026-02-04 16:03:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jqdm8",
          "author": "Weird-Consequence366",
          "text": "Quality post. This is what I‚Äôm here to read",
          "score": 5,
          "created_utc": "2026-02-04 15:49:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jnzin",
          "author": "turtleisinnocent",
          "text": "What if , and I know it sounds crazy I know, but what if we had milli-second distributed RAM where page faults are automatically mapped by the hardware itself \n\nand you could have  as much RAM as you want in that cluster as you can fit in those bad 64 bits of yours \n\nthat‚Äôd make things like super mega easier yeah?\n\nsometimes we fix the wrong problem",
          "score": 8,
          "created_utc": "2026-02-04 15:38:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jqcm4",
              "author": "NTCTech",
              "text": "You are describing the CXL dream.....HA!\n\nIf we could just pool huge tiered memory with coherent access without the latency penalty, my life would be so much simpler. We are getting closer with CXL 3.0 specs, but right now the physics of moving that much data across the wire is still the bottleneck. Until then we are stuck optimizing these distinct memory islands. \n\nOne day, though.....",
              "score": 8,
              "created_utc": "2026-02-04 15:49:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3jqn7q",
                  "author": "turtleisinnocent",
                  "text": "Google‚Äôs got it my friend. Jupiter network gets you faster than local memory access in some cases. They‚Äôre just not sharing.",
                  "score": 7,
                  "created_utc": "2026-02-04 15:50:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3nto1k",
              "author": "gnomebodieshome",
              "text": "SSI, ScaleMP, NumaScale, TidalScale, so many more have come and gone.",
              "score": 1,
              "created_utc": "2026-02-05 04:20:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jotgt",
          "author": "Traditional-Gap-3313",
          "text": "great post. Quick question: what if it's 8xRTX 6000 Pro or nothing? I'm jumping through a lot of hoops to get that server, H100s are simply unobtainable for a shitload of reasons that I don't want to get into. How long were the training runs? We don't think we'll have a single run longer then a few weeks at most. Did you still manage to get some useful results with the PCIe configuration?",
          "score": 3,
          "created_utc": "2026-02-04 15:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jr0mv",
              "author": "NTCTech",
              "text": "Thank you....appreciate it, and glad you find it helpful....\n\nThe H100 allocation game is a nightmare....\n\nAs for your question: You can absolutely still train on PCIe. It is not broken, you just pay a time tax.\n\nSince you are stuck with the RTX 6000s (which are great cards, btw), your main enemy is the All-Reduce step where cards sync data. To fight the PCIe bottleneck, try to crank up your Gradient Accumulation steps. Basically, do more work locally on the card before syncing. You might wait a bit longer for convergence, but for a multi-week run, it is totally viable. Don't let the perfect architecture stop you from building the good enough one.",
              "score": 14,
              "created_utc": "2026-02-04 15:52:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3nf90c",
              "author": "evil0sheep",
              "text": "Before you buy RTX Pro 6000s be aware that not all Blackwell is created equal. RTX pro is sm120 (Blackwell GeForce) vs sm100 for b200. The former lacks dedicated tensor memory (TMEM) which means you have to use register based tensor instructions . This makes it a pain to find kernels that even work (e.g for flash attention or QAT) and sometimes requires you to write your own, and even then it‚Äôs a lot harder to saturate sm120 tensor cores in flash attention kernels because the tensor instructions use so many registers that you can‚Äôt issue enough warps to saturate the memory controllers. It‚Äôs a subtle difference but it bit me and it bit some old coworkers of mine I got lunch with recently, don‚Äôt let it bite you.",
              "score": 3,
              "created_utc": "2026-02-05 02:51:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ni3bw",
                  "author": "Traditional-Gap-3313",
                  "text": "Thanks, this is good info to have. However, it doesn't change much. I can either get that server or not get a server at all. And if I want a server, then I don't really have a choice.\n\nSo I have to hope that the support will improve",
                  "score": 1,
                  "created_utc": "2026-02-05 03:08:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jvb2j",
              "author": "DataGOGO",
              "text": "For training? It would work, but what about H200 NVL's not an option?",
              "score": 1,
              "created_utc": "2026-02-04 16:12:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3k06dd",
                  "author": "NTCTech",
                  "text": "Pure allocation issue...if they are struggling to get an h100 quote, the h200 nvls are basically unicorn dust right now. Supply chain is still ruling everything.",
                  "score": 2,
                  "created_utc": "2026-02-04 16:34:38",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3k5e7v",
          "author": "Current_Ferret_4981",
          "text": "Check out https://jax-ml.github.io/scaling-book/training/ for a good discussion on rough scaling laws during training. Your points about pcie vs nvlink are 100% accurate and the reason I often tell people that 8x3090 is not the way to go for anything besides a multi-user inference node. You absolutely lose out trying to use that for training. \n\nQuick note, pcie 5.0 does rate to 128GB bidirectional, but it's essentially non-existent for full rate bidirectional. Best case you are getting 64GB/s but most cases you are going to be looking at 32-64GB/s bidirectional (if code is well designed) or 32GB/s unidirectional. That is really where you get hit hard with those all-reduces.\n\nAlso note, if you have spare compute vs storage speed you could reduce checkpoints. There is a subchapter in that reference where you can see how the checkpointing/caching hits differently. Checkpointing trades O(n^2 ) compute for O(n) memory, but you have to remember that we often talk about FLOPs in Tera or bigger vs memory in Giga so it's not automatic that you want that tradeoff!",
          "score": 3,
          "created_utc": "2026-02-04 16:58:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ka5le",
              "author": "NTCTech",
              "text": "100% on the pcie bandwidth reality check. the 128GB/s on the spec sheet assumes a perfect vacuum with spherical cows. In ib\\_write\\_bw tests between nodes, we were seeing closer to that 32-50 range depending on the overhead.\n\nAnd thank you for that jax-ml link I hadn't seen that specific chapter on checkpoint trade-offs. Bookmarked....",
              "score": 3,
              "created_utc": "2026-02-04 17:20:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3juux6",
          "author": "oulu2006",
          "text": "Just here to say I love this content please keep it coming :) really interesting stuff to read",
          "score": 2,
          "created_utc": "2026-02-04 16:10:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lxkyw",
          "author": "TheJrMrPopplewick",
          "text": "Typically, PFC on its own is not recommended because pause frames are not super helpful and will slow your fabric significantly when possibly not needed. You will likely want to look at and adopt DCQCN (ECN+PFC combo) presuming your switches support it. Or some people use ECN only and no PFC, which can work pretty well for RoCE workflows.\n\nUsing PCIe based H100s is also not helping you unfortunately if you are running multi-node training because the H100s are being throttled by your limited NIC throughput and PCIe throughput (as you noted). SXM (DGX/HGX) goes a long way to fix this as each GPU is assigned a NIC 1:1 and those NICs are 400G.\n\nAnd firm yes on checkpoints. People underlook this all the time and I have regular conversations about it. The key thing is while you are dumping that checkpoint, all the GPUs are idle so getting that checkpoint across the wire to your shared storage asap is critical. \n\nEthernet works well for back-end training fabrics now and is a lot more baked than it was a year or two back, but it does require good networking knowledge and comfort level with RoCE behavior and being able to tune/profile your fabric.",
          "score": 2,
          "created_utc": "2026-02-04 21:57:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jutba",
          "author": "DataGOGO",
          "text": "Were you using GPU's without any NVlink, or something like the H200 NVL's? Yeah, P2P / all reduce ops, even at 2 GPU's is brutal; at 8, I would be shocked if it even works, especially if you are crossing sockets.\n\nI will check out your deep dive. \n\n\n\n ",
          "score": 1,
          "created_utc": "2026-02-04 16:09:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jx140",
              "author": "NTCTech",
              "text": "We were testing with standard pcie h100s, not the NVLs which bridge that gap a bit better. And yes, once you cross the UPI link between sockets, the latency just kills the all-reduce. At 8 cards, without nvlink, it was basically a very expensive heater that occasionally did math.",
              "score": 3,
              "created_utc": "2026-02-04 16:20:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3k0yui",
                  "author": "DataGOGO",
                  "text": "ooof\n\nSo what is the play from here?  moving to the NVL's? dumping it all and going SXM?\n\nLast I looked you can only use an 4 way bridge on the NVL's I don't think there is an 8 way bridge (?), really SXM is the way to go, if you can get them, and if you have the funds. \n\n\n\n",
                  "score": 1,
                  "created_utc": "2026-02-04 16:38:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jv3sd",
          "author": "lettrio",
          "text": "all ears for ethernet problems, could you please elaborate?",
          "score": 1,
          "created_utc": "2026-02-04 16:11:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jy9us",
              "author": "NTCTech",
              "text": "The short version - standard ethernet is lossy by design as it drops packets when busy, but RoCEv2 needs a lossless fabric to work well.\n\nSo you have to tune priority flow control perfectly. if you get it wrong, a switch buffer fills up, sends a pause frame, and suddenly your entire 800GbE fabric stalls because of one noisy neighbor. Head-of-line blocking is the enemy.....",
              "score": 4,
              "created_utc": "2026-02-04 16:25:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3k05bv",
                  "author": "lettrio",
                  "text": "thank you!  any possible mitigations?",
                  "score": 1,
                  "created_utc": "2026-02-04 16:34:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3k5nrb",
          "author": "a_beautiful_rhind",
          "text": "Was P2P working for your PCIE setup? By default it seems nvidia isn't fond of that and it would kill your bandwidth even more when not enabled.",
          "score": 1,
          "created_utc": "2026-02-04 16:59:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kc6tm",
              "author": "NTCTech",
              "text": "Getting p2p to work was a fight. by default, the motherboard acs settings usually block it for security isolation.\n\nWe had to disable acs in the bios or set pci=nomsi in grub sometimes to let the cards talk directly without bouncing everything through the cpu root complex. If you miss that, your bandwidth falls off a cliff.",
              "score": 1,
              "created_utc": "2026-02-04 17:29:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3k8k64",
          "author": "kouteiheika",
          "text": "> A 175B model dumps a 2.5TB checkpoint\n\nHow are you getting a 2.5TB checkpoint from a 175B model? Normally I'd assume a 175B model checkpoint should take ~700GB at most (assuming weights are in bf16 and you're using Muon instead of Adam).",
          "score": 1,
          "created_utc": "2026-02-04 17:12:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ketg4",
              "author": "NTCTech",
              "text": "You are right on if using Muon or pure bf16 states, but we were sticking to the standard AdamW implementation for stability.\n\nThe bloat comes from the optimizer states. for 175B, you have the weights bf16 + gradients bf16, but then Adam keeps a copy of the master weights in fp32, plus the momentum and variance states (also fp32).\n\nMath roughly: 175B \\* (4 bytes master + 4 bytes momentum + 4 bytes variance) gets you to \\~2.1TB just for the states, before you even add the actual model weights. it‚Äôs brutal.",
              "score": 4,
              "created_utc": "2026-02-04 17:42:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3l288r",
                  "author": "kouteiheika",
                  "text": "That does sound a little bit... excessive? Granted, my experience is limited to single node training so maybe in a distributed setting on a cluster you need to do things differently for things to be stable, but - do you *actually* need all of the extra state, and in fp32 nonetheless?\n\nFor reference, I've gone as low as keeping the optimizer states quantized (with Muon) in 4-bit *and* directly accumulating gradients in the optimizer's state (so gradients don't take up *any* VRAM, besides temporary scratch buffers), *and* I was quantizing the weights at the same time (hybrid 8-bit and 4-bit), and that learned just fine and perfectly stable for me (but, again, only single node training).",
                  "score": 1,
                  "created_utc": "2026-02-04 19:28:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3k90t0",
          "author": "RhubarbSimilar1683",
          "text": "From what I hear these private training setups are mostly used by financial companies for securities trading like automated quant stock trading. Maybe some medical research too. A few for ai companies because there are few of them. What are people using private training clusters for?¬†",
          "score": 1,
          "created_utc": "2026-02-04 17:15:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kfurl",
              "author": "NTCTech",
              "text": "u/Ready-Scheme-7525 nailed the ROI angle - if you burn GPUs 24/7, cloud pricing is extortion.\n\nBut beyond cost, the biggest driver I see is Data Sovereignty. We work with Legal & Compliance firms who have petabytes of sensitive case files. They want to RAG against that data, but their contracts explicitly forbid sending a single byte to an Azure/OpenAI API.\n\nSo they are forced to build on-prem or in private colos just to keep the data air-gapped. It‚Äôs less about cheaper for them and more about legal survival.",
              "score": 3,
              "created_utc": "2026-02-04 17:46:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ml3zj",
                  "author": "wahnsinnwanscene",
                  "text": "Don't these hyperscalers offer a dedicated cluster and workforce precisely for this situation?",
                  "score": 1,
                  "created_utc": "2026-02-05 00:01:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3kdb1n",
              "author": "Ready-Scheme-7525",
              "text": "For cost efficient training (of anything).  If your org trains models that don't fit on a single node and you can keep the GPUs reasonably busy then you buy servers.  It is significantly cheaper than cloud even once you factor in all the overhead.  Roughly one year of cloud time pays off the server you get to keep in service for ~3 years or more.  Also, if restrictions prevent you from using cloud, you buy servers.",
              "score": 1,
              "created_utc": "2026-02-04 17:35:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kft89",
          "author": "Marksta",
          "text": "\\#2 about the storage is pretty eye opening. So for 175B model, you want something pushing ~40GiB/s write. I agree, a local NVMe array is going to be the way. [Would be a shame if those became scarce...]\n\nThe next point of it though, is you mentioned GPUs stalling/idling killing your ROI. Is it standard practice to actually have work for your personal cluster at all times? Like, let's say you're doing iterative training steps and checking them... so you have test\\_final\\_final4real\\_(5).ckpt you're cooking and when it's done, isn't somebody going to look at it? Or you run some automated inferencing on it, run it against some benchs, then do you have another automated step to say \"Needs more sugar\" or whatever and jump into the next step of training?\n\nI'm totally naive to anything training aside from dataset goes in, GPUs crunch, model checkpoint comes out.",
          "score": 1,
          "created_utc": "2026-02-04 17:46:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3km0ih",
              "author": "NTCTech",
              "text": "Great question....so the idle time i'm talking about isn't waiting for a human to check the file it is the GPU literally pausing its math to wait for the hard drive to finish writing.\n\nUnless you have asynchronous checkpointing perfectly tuned (which is hard), the training loop often halts during the save. if you checkpoint every 60 mins and the write takes 10 mins (slow storage), you are wasting \\~16% of your compute rental. on a $5M cluster, that's lighting money on fire.\n\nRe: workflow - it is usually fully automated. we queue up jobs in a scheduler (slurm/k8s). humans watch the loss curves on a dashboard like weights & biases in real-time. if the graph looks good, we let it ride. we usually only touch the checkpoints themselves after the run is totally done.",
              "score": 3,
              "created_utc": "2026-02-04 18:14:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3lgibt",
                  "author": "Aggressive-Bother470",
                  "text": "Probably the best AMA we've ever had :D",
                  "score": 4,
                  "created_utc": "2026-02-04 20:36:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lckom",
          "author": "Claudius_the_II",
          "text": "The checkpoint write bottleneck is honestly the most underrated problem in on-prem training. Everyone laser-focuses on GPU interconnect bandwidth but then plugs in commodity NAS and wonders why their $30k cards sit idle 15% of the run. The RoCEv2 vs IB tradeoff is real too ‚Äî we went through similar PFC tuning hell and ended up just isolating storage on its own rail to keep sanity.",
          "score": 1,
          "created_utc": "2026-02-04 20:17:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3le555",
              "author": "NTCTech",
              "text": "That 15% idle metric is exactly what I used to get the budget approved for the NVMe tier. Executives obsess over GPU interconnect specs but forget that if the GPU is waiting on I/O, it‚Äôs just a very expensive space heater.\n\nAnd yeah, physical isolation for the storage rail saved my sanity too. Converged Ethernet is great in whitepapers, but in production, I just want my storage traffic to stay out of my compute lane.",
              "score": 2,
              "created_utc": "2026-02-04 20:24:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lz528",
          "author": "smflx",
          "text": "Thanks for sharing RARE valuable experience. I also trying even 16x pcie gpus for years.\n\n1. Yup. I also wanted to avoid NVLink  because it's expensive.  I have realized pcie4 is not enough for FSDP training. Lessens I learned with big disappointment. \n\nI try now pcie5, hope it's working ok...  Almost none of accurate information than just own experiment. Here, mostly inference or small scale training. Companies usually use DGX.\n\nYour sharing experience is RARE & very helpful. Thanks a lot.\n\n2. Still, I hope pcie5 is ok for multi gpu training.\n\nI have experienced communication speed could vary a lot with the same 4 GPU setup, depending on board.\n\nYes, it was due to actual (not theoretical) pcie speed. You can't assume the speed shown in p2p 1:1 bandwidth test. With nccl-test, it could be very slow per mainboard. I didn't know this for years.\n\nI hope to see nccl-test numbers in your setup.\n\n3. Yeah, dumping checkpoints to nfs takes time. NVME is  fast, but eventually I use hdd. Checkpoints are huge.",
          "score": 1,
          "created_utc": "2026-02-04 22:04:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3mefpm",
              "author": "NTCTech",
              "text": "This is such a vital point. theoretical bandwidth is a lie once you start hitting p2p 1:1 tests under heavy fsdp load.\n\nWe saw similar behavior pcie 4 is technically enough on paper, but in practice, the communication overhead during the sharded parameter gather/scatter kills the scaling efficiency. I‚Äôm definitely including your warning about mainboard variance in the final guide. It‚Äôs not just the card it‚Äôs the lanes on the board.",
              "score": 1,
              "created_utc": "2026-02-04 23:24:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3mkrsd",
                  "author": "smflx",
                  "text": "I wonder if your mainboard lowered the bandwidth. I mean I have still hope for pice5.\n\nWe may share p2pBandwidthTest & nccl-test, to discover the specs manufacturer don't document honestly.\n\nWe should know, before purchase, about RAM bandwidth (surprised to find it depends on CPU too, not just channels), actual p2p all-reduce, all-to-all PCIe bandwidth.\n\nPCIe4 p2pBandwidthTest I got is 50G at max(amd), 40G on Intel. PCIe5 p2pBandwidthTest is 100G at max.\n\nNccl-test is quite low like under 10G (pcie4) normally, even 1G in faulty configuration.",
                  "score": 1,
                  "created_utc": "2026-02-04 23:59:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3m8oco",
          "author": "Gohan472",
          "text": "Thank you OP! This is excellent content!",
          "score": 1,
          "created_utc": "2026-02-04 22:53:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3meksv",
              "author": "NTCTech",
              "text": "Much appreciated! Glad the unfiltered experience is resonating. stay tuned for the full config deep-dive either tomorrow or on Friday....",
              "score": 1,
              "created_utc": "2026-02-04 23:25:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3mfnq1",
          "author": "FkingPoorDude",
          "text": "How about don‚Äôt checkpoint so often lol",
          "score": 1,
          "created_utc": "2026-02-04 23:31:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o63yg",
          "author": "IrisColt",
          "text": "Thanks!!!",
          "score": 1,
          "created_utc": "2026-02-05 05:49:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3khodh",
          "author": "FullOf_Bad_Ideas",
          "text": "1. \"tax\"? I can't stand llm speek. Both training and inference are often bottlenecked by inter connect bandwidth, it depends on what you're doing.\nif you wanted to train 70B model from scratch you're not using single node, you're using 16-64 nodes anyway. There's no \"900gb/s is fine but 128gb/s isn't\" for anything. Nvlink doesn't solve the issue it just makes it a bit more bearable. There are papers on decentralized training  runs over internet that attempt to tackle this issue, and some configs have to be avoided.\n\n2. Try to use Megatron Async Checkpointing. And you can stall gpu's for a few mins, if you're saving just a few times a day it does not matter.",
          "score": 1,
          "created_utc": "2026-02-04 17:55:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3komzw",
              "author": "NTCTech",
              "text": "Valid pushback on the async checkpointing....\n\nTechnically, yes if you tune Megatron-LM's async saving perfectly, you can hide a lot of that latency and keep the compute bound. in practice, we found it brittle. we had issues with rank synchronization hanging during the async hand-off, and when you're burning cash on rental/power, we opted to \"solve\" it with brute-force IOPS rather than debugging the save loop for another week.\n\nRe: \"tax\", it's a metaphor. but the practical delta between 64GB/s effective pcie and 900GB/s nvlink dictates your entire topology. decentralized/gossip training is fascinating research, but for a dense private cluster, we just wanted the fat pipe.",
              "score": 3,
              "created_utc": "2026-02-04 18:26:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxpf86",
      "title": "[Release] Experimental Model with Subquadratic Attention: 100 tok/s @ 1M context, 76 tok/s @ 10M context (30B model, single GPU)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qxpf86/release_experimental_model_with_subquadratic/",
      "author": "Sad-Size2723",
      "created_utc": "2026-02-06 18:19:46",
      "score": 342,
      "num_comments": 46,
      "upvote_ratio": 0.98,
      "text": "Hey everyone,\n\nLast week I shared preliminary results on a new subquadratic attention mechanism ([https://www.reddit.com/r/LocalLLaMA/comments/1qol3s5/preliminary\\_new\\_subquadratic\\_attention\\_20k\\_toks](https://www.reddit.com/r/LocalLLaMA/comments/1qol3s5/preliminary_new_subquadratic_attention_20k_toks)). Following up with the full release: model + inference code are now available.\n\n**TL;DR**: 30B model achieving O(L\\^(3/2)) scaling instead of O(L\\^2). Enables 1M‚Äì10M context on a single GPU with decode speeds that stay practical even at extreme context lengths. Ships with an OpenAI-compatible server and CLI to try out.\n\n\\- ü§ó **Model**: [https://huggingface.co/concavity-ai/superlinear-exp-v0.1](https://huggingface.co/concavity-ai/superlinear-exp-v0.1)\n\n\\- üíª **Code**: [https://github.com/concavity-ai/superlinear](https://github.com/concavity-ai/superlinear) (\\`pip install superlinear\\`)\n\n\\- üìÑ **Paper**: [https://arxiv.org/abs/2601.18401](https://arxiv.org/abs/2601.18401)\n\n\n\n**Main Idea**\n\nYou can think of attention as a search algorithm to find relevant information for next-token prediction. Standard attention is basically O(L) brute-force search. We're doing O(L\\^0.5) jump-search with learned routing: score O(L\\^0.5) candidate spans, select top-k, then do token-level attention within the selected spans.\n\nThis gives **O(L\\^(3/2)) total complexity** while preserving **random context access** ‚Äî any token can be selected by content-dependent routing, unlike fixed sliding windows. When you 10x the context length, the search budget only grows by \\~3.2x. That subquadratic scaling really matters for long context.\n\n\n\n**Performance (Single B200 GPU)**\n\n    | Context Length | Prefill (tok/s) | Decode (tok/s) | Memory  |\n    |----------------|-----------------|----------------|---------|\n    | 1M tokens      | ~20,202         | ~109           | 66 GB   |\n    | 10M tokens     | ~5,576          | ~76            | ~120 GB |\n\nKey point: 1M ‚Üí 10M context (10x increase) only drops decode speed by \\~30%, not the 10x slowdown with dense attention.\n\n\n\n**Why This Matters**\n\nWhen you have fast long-context inference, usage patterns change. The key is **maintaining the cache** instead of reprocessing everything:\n\n\\- ***Almost-infinite chat***: KV cache in memory for instant responses, save/restore sessions to disk for persistence\n\n\\- ***Document Q&A***: Load documents once, ask cross-document questions without reprocessing (our GitHub example: 8 Wikipedia articles with cross-document reasoning)\n\n\\- ***Long-form generation***: 20k+ token reasoning on difficult math problems and coherent long article writing, all with maintained context\n\nEarly results: perfect NIAH at 512K context (up from 256K last week), cross-document reasoning working, subquadratic scaling working in practice.\n\nSince no existing inference engine is going to support our custom kernels, we built the full stack ourselves: Triton kernels, OpenAI-compatible server, session snapshots, chunked prefill, CLI with BM25 RAG.\n\n\n\n**Limitations & Next Steps**\n\n***Current limitations:***\n\n\\- This is an \\*\\*architecture + systems feasibility release\\*\\*, not production-quality\n\n\\- Limited training data (initial SFT only)\n\n\\- Comprehensive evals beyond NIAH still needed\n\n\\- FP16 only (66GB for 1M context) ‚Äî quantization coming soon\n\n***Quantization*** **(coming soon):**\n\n\\- 4-bit/8-bit quantization to run 1M context on 24GB consumer GPUs\n\n\\- Target: RTX 4090 / RTX 5090 with full 1M context\n\n\\- 2M context on 48GB cards (e.g., RTX 6000 Ada)\n\n***Hardware support:***\n\n\\- Currently CUDA only (B200, RTX 6000 Blackwell tested)\n\n\\- AMD ROCm port coming (Triton kernels should make this straightforward)\n\n\\- Eventually Apple Silicon (harder but not impossible)\n\n***Training & Quality improvements:***\n\n\\- Scaling up SFT data with more long-context examples\n\n\\- Potentially doing continued pretraining on long documents\n\n\\- Expanding perfect NIAH range beyond 512K\n\n\\- Real-world long-context benchmarks (book QA, codebase analysis, multi-document reasoning)\n\n***New end-user applications***: We are planning to develop local-first end-user applications based on this. What would you actually use long context for? Would love to hear specific use cases to help us prioritize.\n\n\n\n\\---\n\nTrying something new is extremely hard. Everyone likes existing transformer architectures ‚Äî optimizations at every level, predictable scaling laws. But to make truly long-context models practical on local hardware, I think we need new ideas. It doesn't hurt to try, right?\n\nI'm trying not to spam this sub, so the GitHub repo is the best place to follow progress. Happy to answer questions here though! If you try it and hit issues, open a GitHub issue. And if you have thoughts on long-context use cases, I'd love to hear them.\n\nThanks for all the encouragement on the last post!\n\n**Links**:\n\n\\- ü§ó **Model**: [https://huggingface.co/concavity-ai/superlinear-exp-v0.1](https://huggingface.co/concavity-ai/superlinear-exp-v0.1)\n\n\\- üíª **Code**: [https://github.com/concavity-ai/superlinear](https://github.com/concavity-ai/superlinear)\n\n\\- üìÑ **Paper**: [https://arxiv.org/abs/2601.18401](https://arxiv.org/abs/2601.18401)",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qxpf86/release_experimental_model_with_subquadratic/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o40xagb",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-07 04:00:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z6lpj",
          "author": "QuackerEnte",
          "text": "NO I was literally about to release something similar. You beat me to it man, congratulations. \n\n(My idea was: instead of multi-step search (coarse to fine like your paper proposes), I'm using hierarchical refinement and compression. O(L*K^2 ) with fixed levels, like a pyramid. The coarse summary vectors can be attended to alongside normal tokens, instead of span-attention on selected regions. It could also \"zoom in\" and decide to fetch more detail to load into context (similar to your random access idea), via learned attention thresholds instead of search scores. \nKey difference is also that your idea needs end-to-end training, while mine was a model-agnostic wrapper approach because I couldn't afford to retrain an entire model.) \n\nOverall really great read, a lot to learn from!\nI may or may not eventually publish my work if it holds any value for the community. I'll be following your future work.",
          "score": 26,
          "created_utc": "2026-02-06 21:50:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zhzjc",
              "author": "Sad-Size2723",
              "text": "No worries, I am not sure if you are an active researcher, but you will always find people doing similar things. I have seen many articles doing things similar to what you just suggested, but I think it's still worth it to work on the idea. The reason is that people implement even the same idea differently, and the engineering details matter greatly. So it is very possible that you will get better results. In terms of resources, it also depends who you compare to. For us, even though we can do small scale fine-tuning, that's nowhere near the scale of any LLM labs...",
              "score": 18,
              "created_utc": "2026-02-06 22:49:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o42g2dw",
              "author": "RobotRobotWhatDoUSee",
              "text": "You should definitely publish it and discuss it here. This is how ratchet works, you're often working on something similar to others in parallel,but that's fine (especially if you're not chasing tenure), you just have a section of your lit review (in your paper) that notes similar projects and how you differ.",
              "score": 1,
              "created_utc": "2026-02-07 12:05:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3yfbv9",
          "author": "ruibranco",
          "text": "The fact that 10x context only costs \\~30% decode speed is the real headline here. That scaling curve is what makes this actually practical instead of just theoretically interesting. Waiting for the 4-bit quant to see how this runs on a 4090 with 1M context, that would be a game changer for local RAG pipelines where you currently have to chunk everything aggressively to fit in reasonable context windows.",
          "score": 22,
          "created_utc": "2026-02-06 19:34:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zgj8u",
              "author": "Sad-Size2723",
              "text": "Yeah, the scaling adds up fast especially when you go into million token scale where the attention calculation becomes dominant. \n\nYour point about RAG is completely valid because that's one of our first applications. I am actually going to write a detailed Medium article on this. Will share it here if you are interested. ",
              "score": 15,
              "created_utc": "2026-02-06 22:41:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zu9lo",
                  "author": "WillemDaFo",
                  "text": "Please share it üëç",
                  "score": 4,
                  "created_utc": "2026-02-06 23:59:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3y5vi8",
          "author": "ortegaalfredo",
          "text": "What I found very interesting is that the model is basically Nemotron 3, so this can be applied to existing models.\n\nJust today I saw an announcement from nvidia about a kv-cache compression algorithm that enables >10M context sizes. I believe a model with 10M context size will have a memory approaching that of a person.",
          "score": 48,
          "created_utc": "2026-02-06 18:49:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ydybm",
              "author": "Sad-Size2723",
              "text": "So we are actually replacing the attention layers, which in theory can be done on any models. We are applying it to Nemotron 3 because of quality and computation efficiency considerations. The current KV cache implementation on this model is pretty efficient already, but will certainly look into compression if there is bottleneck in the future. ",
              "score": 20,
              "created_utc": "2026-02-06 19:28:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3yeysr",
                  "author": "Significant_Fig_7581",
                  "text": "Why not GLM 4.7 Flash? it's really really slow, but also really good",
                  "score": 15,
                  "created_utc": "2026-02-06 19:33:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3yaxz9",
          "author": "Ok_Warning2146",
          "text": "Great work. Can u submit your model to [contextarena.ai](http://contextarena.ai) such that we can see how well it performs on long context bench? So how much kv cache u use at 1m context? kimi linear uses 14.875gb at 1m.",
          "score": 27,
          "created_utc": "2026-02-06 19:13:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yg1ry",
              "author": "Sad-Size2723",
              "text": "Noted, one of the next steps for us is to perform a comprehensive evaluation and context [arena.ai](http://arena.ai) is definitely considered. \n\nIn terms of context length, we chose Nemotron 3 Nano 30B because of its efficient KV cache implementation. Right now it is about 6GB per 1M tokens. ",
              "score": 12,
              "created_utc": "2026-02-06 19:38:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45obbb",
                  "author": "Orolol",
                  "text": "I'll try to test it on [familybench](https://github.com/Orolol/familyBench) a long context reasoning benchmark",
                  "score": 1,
                  "created_utc": "2026-02-07 22:36:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3y2d8j",
          "author": "Accomplished_Ad9530",
          "text": "I saw your previous post and thought your paper looked interesting. Good explanations in your post and comments, too. And thanks for releasing the code and model so quickly. h/t",
          "score": 22,
          "created_utc": "2026-02-06 18:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yeh0a",
              "author": "Sad-Size2723",
              "text": "Thanks! It takes some effort to put together the inference code so that people can actually use it",
              "score": 2,
              "created_utc": "2026-02-06 19:30:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o408m3h",
                  "author": "Accomplished_Ad9530",
                  "text": "Working inference code is hugely appreciated. Wish more ML labs would put that effort in.\n\nBTW, you might be interested in the new paper \"MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers\" \\[ [https://arxiv.org/abs/2602.00398](https://arxiv.org/abs/2602.00398) \\] as a complementary technique to your own. It also has some interesting implications for RAG and interpretability.",
                  "score": 3,
                  "created_utc": "2026-02-07 01:23:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ylanj",
          "author": "Confident-While-1322",
          "text": "Look forward to the Apple Silicon version",
          "score": 8,
          "created_utc": "2026-02-06 20:04:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y0bao",
          "author": "Limp_Finding_7168",
          "text": "This is genuinely exciting stuff. The jump from O(L\\^2) to O(L\\^3/2) might seem incremental on paper, but those real-world decode speeds at 10M context are pretty compelling - only 30% slowdown instead of the usual 10x death spiral is huge for practical applications. Really curious how the quantized versions will perform once there ready, especially if you can get 1M context running smoothly on consumer hardware.",
          "score": 15,
          "created_utc": "2026-02-06 18:22:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ye9ma",
              "author": "Sad-Size2723",
              "text": "Thanks! Yeah, the scaling is actually quite dramatic at long context. I should probably make it more clear, for decoding full attention is O(L) and our algorithm is O(L\\^0.5), so at 1M it is already 1000x efficiency, but of course we are paying a much higher overhead, but even at 10x overhead it is still a win over full attention. ",
              "score": 2,
              "created_utc": "2026-02-06 19:29:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3y83ac",
          "author": "twack3r",
          "text": "What is the quality of attention across the context window like? Is there the usual dip or does this approach alleviate this?\n\nIn my experience there is a huge difference between ctx sizes and their actual usability between architectures.",
          "score": 5,
          "created_utc": "2026-02-06 18:59:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yfhv1",
              "author": "Sad-Size2723",
              "text": "Good point, yeah the quality does drop significantly as you increase the context length, although we are at perfect NIAH at 512k context, it is still different from real world use case. That's why we want to make sure people are aware that this is still experimental. \n\nThe main idea here is that, we want to show the first order feasibility - is the model efficient at decoding, and is it possible to train it? If so, then it's worth the effort to fine-tune it. Essentially proving some kind of scaling law so that we will continue to improve the context capabilities. ",
              "score": 13,
              "created_utc": "2026-02-06 19:35:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3y6efr",
          "author": "Business-Weekend-537",
          "text": "Hopefully the unsloth guys see this and can work with you- then people could train longer context models at home.",
          "score": 6,
          "created_utc": "2026-02-06 18:51:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yetmc",
              "author": "Sad-Size2723",
              "text": "Haha, we haven't proved ourselves yet, there is no way they will look into this, unless this is proven to be useful...",
              "score": 4,
              "created_utc": "2026-02-06 19:32:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3yvqf1",
                  "author": "Business-Weekend-537",
                  "text": "Don‚Äôt sell yourself short- if you had a tutorial on how to use this with a 3090 I‚Äôd try it in a heartbeat.",
                  "score": 7,
                  "created_utc": "2026-02-06 20:56:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o40a6ip",
          "author": "Prestigious_Thing797",
          "text": "It would be good to see more quality benchmarks of this relative to the baseline model and other methods. There's a lot of different more efficient attention mechanisms (several referenced in your paper) but the drawback with all of them has been that they perform worse than standard attention, which has lead to the modern mixing of linear and full attention in models like the one you used, Qwen3-Next and so on.\n\nThe only benchmark given (NIAH) is not so common these days because practically all models perform well on it. You probably won't train up a new model from scratch that is competitive with models on benchmarks people really use- but you can randomly init different layers (all linear, mixed in superlinear, full attention) train each under the same regime and then compare the performance on a set of benchmarks across the three.\n\nAs of right now- this paper doesn't really demonstrate any hard evidence of benefit over using a standard linear attention layer.",
          "score": 3,
          "created_utc": "2026-02-07 01:33:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o470fxs",
              "author": "Sad-Size2723",
              "text": "Thanks for the comment. Yeah for standard attention, it is just so optimized (hardware + software + capital) that pretty much any other method will lose on either on speed or quality, if not both. There is a reason why Minimax is giving up on linear attention. \n\nAnd this is exactly the problem here, for any method to work, it has to beat Flash Attention by a large margin, even a 2x to 5x gain is likely not enough, because any subquadratic method will have to give away some quality, and in most case its not enough to compensate for the speedup. \n\nThis leads to the point of the paper here - we want to demonstrate that it can beat Flash Attention on speed and maintain the ability to attend to any tokens if needed and it is trainable. Is this method feasible? \n\nI agree with you on the lack of benchmarks here. I treat this as a feasibility study. A fair comparison on benchmarks will require comparable training as full attention models, which requires astronomical resources. I am taking a more qualitative path here rather than quantitive - at least on my offline tests, it performs comparable to standard attention on short context, and it is able to maintain reasoning chains up to tens of thousands of tokens. But yeah, maybe I should add some standard benchmarks to the paper just so that we know it is better than linear attention...",
              "score": 2,
              "created_utc": "2026-02-08 03:34:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o426xla",
          "author": "Individual_Spread132",
          "text": ">  What would you actually use long context for?\n\nGathering data on fictional characters; basically, dumping an entire script of a game / book into the chat. I've already attempted it with the baseline Nemotron-3-Nano-30B-A3B but it hallucinated a bit (nonetheless it was impressive). I wonder if it's going to be better with this new version!",
          "score": 3,
          "created_utc": "2026-02-07 10:42:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o474kbc",
              "author": "Sad-Size2723",
              "text": "Right now I am focusing on extending the context length,  in terms of quality it is likely not at the same level as the base model yet. This is something we will definitely focus on next",
              "score": 1,
              "created_utc": "2026-02-08 04:01:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3y77tc",
          "author": "botirkhaltaev",
          "text": "Man this looks cool will check it out",
          "score": 2,
          "created_utc": "2026-02-06 18:55:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zye6e",
          "author": "smflx",
          "text": "Good to know such long context possible. I'm interested in building a model for creative writing with very long context. Definitely I will read your paper. Thanks for sharing.",
          "score": 2,
          "created_utc": "2026-02-07 00:23:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40byz3",
          "author": "Alarming_Bluebird648",
          "text": "that scaling curve at 10m context is actually wild. i've been looking for a subquadratic approach that works on existing weights like nemotron. ngl the inference speed staying that high is the real infrastructure win here.",
          "score": 2,
          "created_utc": "2026-02-07 01:44:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o471l6f",
              "author": "Sad-Size2723",
              "text": "You remind me of an important point that I didn't mention in the paper. Even at 1M, the per query computation is still dominated by the MoE layers and the Mamba layers, which means that even though our attention layer is of O(L\\^0.5) at decoding time, the actual scaling exponent is smaller than 0.5, that's why the scaling curve remains pretty flat even at 10M. ",
              "score": 1,
              "created_utc": "2026-02-08 03:41:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o41ub5b",
          "author": "rulerofthehell",
          "text": "This is great work!! Curious how this is different than Log Linear Attention? It‚Äôs so promising!! I was trying with subquadratic attention with much smaller models (1Bish), good to see this side of research!\n\nFeel like something like this in combination with Deepseek Engram like paper can really bring local LLMs to the main stage in future",
          "score": 2,
          "created_utc": "2026-02-07 08:37:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o472gtd",
              "author": "Sad-Size2723",
              "text": "Thanks! For log linear attention there have been many implementations in the literature, but if you follow the multi-step approach in the paper using binary or k-ary search, then you can achieve log linear attention too. However, at this point I don't recommend it because I think log linear scaling is too aggressive that the loss in quality is not worth it. It is already very fast at O(L\\^1.5) now, we would probably focus more on quality next than speed. ",
              "score": 1,
              "created_utc": "2026-02-08 03:47:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o40ide8",
          "author": "Inevitable-Jury-6271",
          "text": "This is a really cool release ‚Äî especially the ‚Äú10x context only ~30% decode hit‚Äù part.\n\nIf you want to make it easier for folks to compare apples-to-apples, a couple eval/reporting ideas that would help a ton:\n\n- Baseline vs superlinear: same weights / same data regime, and swap (a) full attn, (b) hybrid linear+full, (c) hybrid linear+superlinear, then run a small battery (MMLU-ish, GSM, HumanEval, etc.) + long-context (beyond NIAH) so we see the quality/latency trade.\n- Long-context *usefulness* tests: multi-doc QA with adversarial distractors + ‚Äúneedle at random‚Äù at multiple positions + retrieval-style tasks.\n- Memory accounting: KV cache bytes/token @ 1M and 10M + what‚Äôs resident vs streamed.\n\nAlso: do you have any intuition yet on whether routing errors are the main failure mode at very long ctx (vs. general degradation from training data)?",
          "score": 1,
          "created_utc": "2026-02-07 02:23:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47472b",
              "author": "Sad-Size2723",
              "text": "Hey, thanks for the comment. I have done some simple tests locally like GSM8k and Math500, and the results are pretty good. The interesting thing is, the original Nemotron 3 paper didn't show the benchmarks on these, I guess they are too simple for a 30B model? But for harder math problems, the model is able to generate coherent reasoning chains over 30k tokens to come up with the right answer, so I am not too worried about the basic LLM performance, but yeah, I do need to find the time to show these benchmarks, because it seems like people do care about these numbers.\n\nI actually spent most of my time on the harder problem of extending the context capability of the model. I was able to push the perfect NIAH context from 256k last week to 512k this week, and my goal is to get to 1M before running other tests, but since I am doing long context training, it should be able to generalize to other similar tests.\n\nAnd yeah, routing is definitely the biggest problem, because after selecting the spans it is just standard attention. The router is actually very complicated and since it doesn't come up with the base model, we will have train it with a lot of data. Maybe there is a better way to train it, like the lightening indexer in DeepSeek v3.2, or other block based architectures. ",
              "score": 1,
              "created_utc": "2026-02-08 03:59:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4a6g29",
          "author": "Dravodin",
          "text": "Thanks for the work. This is something genuinely interesting to me after a much gap. Will be checking it out.",
          "score": 1,
          "created_utc": "2026-02-08 17:19:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y2wa0",
          "author": "Ok-Buffalo2450",
          "text": "When vllm or other inference engine support pls?",
          "score": -1,
          "created_utc": "2026-02-06 18:35:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yfl54",
              "author": "Sad-Size2723",
              "text": "Not in the near term, since this is still experimental...",
              "score": 3,
              "created_utc": "2026-02-06 19:36:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1quxtkj",
      "title": "The open-source version of Suno is finally here: ACE-Step 1.5",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1quxtkj",
      "author": "AppropriateGuava6262",
      "created_utc": "2026-02-03 17:13:53",
      "score": 337,
      "num_comments": 71,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1quxtkj/the_opensource_version_of_suno_is_finally_here/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3diutq",
          "author": "atineiatte",
          "text": "Is the graph supposed to be a literal joke?¬†",
          "score": 99,
          "created_utc": "2026-02-03 17:27:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3do9cf",
              "author": "LosEagle",
              "text": "The name of the company is StepFun. Nothing from them surprises me anymore.",
              "score": 26,
              "created_utc": "2026-02-03 17:52:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3e4c0c",
                  "author": "Cool-Chemical-5629",
                  "text": "Hey, steps want to have fun too. It all started with pussies getting stuck in washing machines. Long story, don't ask...\n\nhttps://i.redd.it/9c53d5z5vbhg1.gif",
                  "score": 12,
                  "created_utc": "2026-02-03 19:04:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3dyf3u",
                  "author": "Neither-Phone-7264",
                  "text": "i mean stepfun 3.5 flash is surprisingly decent",
                  "score": 9,
                  "created_utc": "2026-02-03 18:37:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ec7wl",
              "author": "Luke-Pioneero",
              "text": "Lol yeah, the labels are a bit goofy. I guess they can't get real numbers for closed-source models since they're black boxes, so they prob just timed the web progress bars we all sit around waiting for.\n\nVague labels aside, the 2s speed on this thing is actually legit. Still messing with it to see if it can handle the specific genres I'm into.",
              "score": 16,
              "created_utc": "2026-02-03 19:41:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dmx2u",
              "author": "Hearcharted",
              "text": "üòÇ",
              "score": 9,
              "created_utc": "2026-02-03 17:46:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3fiy0d",
              "author": "jazir555",
              "text": "Looks like a poorly done job in Microsoft Paint lmao",
              "score": 1,
              "created_utc": "2026-02-03 23:04:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dm8nt",
          "author": "TheRealMasonMac",
          "text": "Massive improvement over the previous one. Unfortunately, it has quite poor instruction following and coherency compared to Suno v3. Audio quality is not bad, and it seems properly creative/different from Suno. Seems like a solid base.\n\nBut I hear they‚Äôre already in the middle of preparing v2?",
          "score": 26,
          "created_utc": "2026-02-03 17:42:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i8siu",
              "author": "djtubig-malicex",
              "text": "There's also ACE Studio's own offering which is similar but obviously closed source in their own cloud and a much higher quality output with less direct controls, possibly based on the same tech.",
              "score": 1,
              "created_utc": "2026-02-04 10:16:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dlkcz",
          "author": "HugoCortell",
          "text": "I'm sure the model is great, but I can't stop myself from making fun of terrible graphs:\n\nWow, I love the comparison against \"most models\" and it's crazy that they even managed to beat \"some models\", those were SOTA just a few days ago!\n\nHoly shit, they even beat \"a few models\"?! That was my favourite model from the famed \"AI lab\" from \"some country\"!!!",
          "score": 46,
          "created_utc": "2026-02-03 17:39:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dmvyu",
              "author": "Hearcharted",
              "text": "üòÖ",
              "score": 3,
              "created_utc": "2026-02-03 17:45:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e1sxz",
          "author": "robert_kurwica213321",
          "text": "if loras can be trained it will probably be better than suno after some geeks tune it",
          "score": 10,
          "created_utc": "2026-02-03 18:52:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dqzqw",
          "author": "lordpuddingcup",
          "text": "Only sad thing it misses on is lyric align which is pretty critical, but this is LOCAL",
          "score": 11,
          "created_utc": "2026-02-03 18:04:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e8gj0",
          "author": "Different_Fix_2217",
          "text": "Random gen from it:  \n[https://files.catbox.moe/gwln4b.mp3](https://files.catbox.moe/gwln4b.mp3)\n\nSomeone elses I liked: [https://files.catbox.moe/3vcfd0.mp3](https://files.catbox.moe/3vcfd0.mp3)\n\nIt likes long detailed prompts btw. It can take negative prompts as well, gonna have to play with it.",
          "score": 17,
          "created_utc": "2026-02-03 19:23:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fjzg8",
              "author": "jazir555",
              "text": "I actually think it sounds great, I give this a 7.8-8/10. By the end of the year, this will probably be completely solved. Honestly, this is amazing for local gen. Voice in general will be solved by the end of the year, holy shit. \n\nMy perfect use case for voice gen is just completely sweeping customer service. Imagine calling a company's customer service line and it answers instantly, no waiting. And it knows everything about the company, and you don't need to be escalated to another team member ever. Suddenly customer service becomes almost pleasant. I can't wait to never have to deal with the bs hassle of calling customer service ever again.",
              "score": 5,
              "created_utc": "2026-02-03 23:10:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hep63",
                  "author": "IrisColt",
                  "text": "oh, and the hold music is AI-generated too, heh...",
                  "score": 2,
                  "created_utc": "2026-02-04 05:47:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3jq2h6",
                  "author": "_BreakingGood_",
                  "text": "Customer service being considered \"pleasant\" might actually be considered a risk in the industry.\n\nLots of companies don't actually want you calling customer service and they work hard to ensure you believe it will be a terrible experience if you even try. ",
                  "score": 1,
                  "created_utc": "2026-02-04 15:48:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3fc36j",
              "author": "Hauven",
              "text": "\\+1 to this. I've just tried this on the space (with the help of GPT-5.2) and it made a number of long paragraphs, and the music actually sounds very close to Suno quality now. I'm glad I saw your comment. I'll keep playing around more with the prompting.",
              "score": 5,
              "created_utc": "2026-02-03 22:29:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hecst",
                  "author": "IrisColt",
                  "text": "Prompt fot GPT-5.2?",
                  "score": 1,
                  "created_utc": "2026-02-04 05:44:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3h1i10",
              "author": "autoencoder",
              "text": "Love the first song! So relaxing",
              "score": 2,
              "created_utc": "2026-02-04 04:14:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3heah1",
              "author": "IrisColt",
              "text": "You nailed it!",
              "score": 0,
              "created_utc": "2026-02-04 05:44:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e1m7n",
          "author": "bennmann",
          "text": "please support the official model researcher org:\n\n[https://acestudio.ai/](https://acestudio.ai/)",
          "score": 17,
          "created_utc": "2026-02-03 18:52:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dv3sa",
          "author": "daisseur_",
          "text": "I love the trustmebro graph, I'll try it for sure !",
          "score": 8,
          "created_utc": "2026-02-03 18:22:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dju2c",
          "author": "ffgg333",
          "text": "Can someone make a free Google colab for using it and training Loras?",
          "score": 7,
          "created_utc": "2026-02-03 17:31:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3doesb",
          "author": "markeus101",
          "text": "The examples are nice tho ngl",
          "score": 5,
          "created_utc": "2026-02-03 17:52:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dn1nj",
          "author": "uti24",
          "text": "I tried examples from repo, it sounds good.\n\nI guess about as good as SUNO 3.5, interesting that it beats SUNO 4 and 5 in benchmarks.",
          "score": 5,
          "created_utc": "2026-02-03 17:46:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3eebt5",
              "author": "BrightRestaurant5401",
              "text": "To each its own, I really disliked SUNO at any version. This sits more in between Udio and Suno for me.",
              "score": 3,
              "created_utc": "2026-02-03 19:51:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e8xwz",
          "author": "guiopen",
          "text": "It's so nice from their part to not only release the weights, but release an entire system to run it, it auto optimized for vram and everything is documented and explained in an easy to understand way, might be the first time i see a model launch so ready and easy to use \n\n(But haven't tested yet, in practice maybe I will face all sorts of problems)",
          "score": 4,
          "created_utc": "2026-02-03 19:25:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3du35o",
          "author": "Muted-Celebration-47",
          "text": "sound very good in demo",
          "score": 3,
          "created_utc": "2026-02-03 18:18:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eulun",
          "author": "Not_your_guy_buddy42",
          "text": "I still listen to the synthwave album I made prompting ace step 1.5 with cluster names from my data. \"Technology-Driven Collaborative and Interactive Experiences\" is a banger",
          "score": 3,
          "created_utc": "2026-02-03 21:07:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hfhmm",
              "author": "IrisColt",
              "text": "a message from the future...",
              "score": 2,
              "created_utc": "2026-02-04 05:53:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ehuuq",
          "author": "NoBuy444",
          "text": "Do we want fast generative models or quality ones. For music, I'd rather have a quality sound that takes longer to render than a fast but unusable render.",
          "score": 5,
          "created_utc": "2026-02-03 20:07:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hfbqc",
              "author": "IrisColt",
              "text": "Yes, but when there‚Äôs a lot of variation in quality, faster rendering lets you prototype more quickly... and their demo, which evaluates songs after generation, suggests automated quality checks may be possible...",
              "score": 3,
              "created_utc": "2026-02-04 05:52:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hx9wi",
                  "author": "NoBuy444",
                  "text": "I tried it yesterday and I was quite impress by the results with such a small model. I wonder how much Lora training is possible with this model but it could be interesting",
                  "score": 2,
                  "created_utc": "2026-02-04 08:27:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dhnfi",
          "author": "Single_Ring4886",
          "text": "Cant find any examples of songs anywhere.",
          "score": 5,
          "created_utc": "2026-02-03 17:21:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3djiqr",
              "author": "_raydeStar",
              "text": "it's on their github - they have two repos there, the gradio, then the example page.  [https://github.com/ace-step/ace-step-v1.5.github.io/tree/main/mp3/samples/GeneralSongs](https://github.com/ace-step/ace-step-v1.5.github.io/tree/main/mp3/samples/GeneralSongs)",
              "score": 12,
              "created_utc": "2026-02-03 17:30:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dqj1z",
              "author": "truth_is_power",
              "text": "Go to the discord for examples, people share tracks + generate there\n\n  \nimo 1.0 was fun to play with,\n\n1.5v is worth checking out ",
              "score": 4,
              "created_utc": "2026-02-03 18:02:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dxxb5",
                  "author": "SlowFail2433",
                  "text": "Yeah the discord is full of them",
                  "score": 1,
                  "created_utc": "2026-02-03 18:35:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3er9y7",
              "author": "ANR2ME",
              "text": "The project page have the playable examples https://ace-step.github.io/ace-step-v1.5.github.io/",
              "score": 2,
              "created_utc": "2026-02-03 20:52:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dn42g",
              "author": "AnticitizenPrime",
              "text": "https://huggingface.co/spaces/ACE-Step/ACE-Step\n\nDemo",
              "score": 1,
              "created_utc": "2026-02-03 17:46:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dpeg1",
                  "author": "Single_Ring4886",
                  "text": "When I clicked on link from their git it lead to 404, thanks!",
                  "score": 1,
                  "created_utc": "2026-02-03 17:57:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dkjz4",
              "author": "gubbelplex",
              "text": "[https://ace-step.github.io/](https://ace-step.github.io/)",
              "score": 1,
              "created_utc": "2026-02-03 17:35:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3eeurk",
          "author": "mpasila",
          "text": "I think I'll keep paying for Suno if I need to generate music.. Very first test it skipped ton of lyrics and the prompt adherence is pretty poor I'd say.",
          "score": 2,
          "created_utc": "2026-02-03 19:53:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dl84k",
          "author": "hapliniste",
          "text": "Tried the gradio demo with short prompts and I'm very underwhelmed üòÖ\n\nThe git examples are fine but saying suno 4+ level seems very misleading. More like very fast suno 2-3 maybe?",
          "score": 5,
          "created_utc": "2026-02-03 17:38:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qg8rv",
              "author": "rickd_online",
              "text": "I was underwhelmed from the Gradio Demo but had better results with the comfyUI workflow",
              "score": 1,
              "created_utc": "2026-02-05 15:52:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e24qr",
          "author": "Erhan24",
          "text": "Okay my truthful impression. It is as fast as DiffRhythm. The prompt adherence is not really doing it for me. Like really bad. No real understanding electronic music genres imho. Same main sounding and not really good or coherent music. \n\nI'm producer so I wanted to get some ideas out of it but we still have a long way to go. Still very nice project so far. I think it will be interesting when anyone realistically makes a lora for one specific genre.",
          "score": 3,
          "created_utc": "2026-02-03 18:54:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3edwty",
              "author": "BrightRestaurant5401",
              "text": "I tried it only for about a hours now and I don't know if you a familiar with udio its manual prompting style?  \nI am getting really decent results in comfyui, I'm only already quite afraid that variance is going to be an issue.",
              "score": 2,
              "created_utc": "2026-02-03 19:49:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3efw02",
                  "author": "Erhan24",
                  "text": "I started testing with organic and melodic house. Didn't work. At the end of my test I said whatever and just typed techno. Still sounds like the YouTube type of progressive if you can call it that.\n\nAlso comfyui btw and tested turbo and base model.\n\nI don't want to be too negative. Everything has its place and open models are the way in the right direction. I hope the big players jump in soon.",
                  "score": 1,
                  "created_utc": "2026-02-03 19:58:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dj9mb",
          "author": "ffgg333",
          "text": "If Loras can be made, can it be trained on 6 gb vram? Or on free Google colab?",
          "score": 1,
          "created_utc": "2026-02-03 17:29:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dpo5d",
          "author": "ILoveMy2Balls",
          "text": "How do song evals work?",
          "score": 1,
          "created_utc": "2026-02-03 17:58:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dw4v9",
          "author": "pmttyji",
          "text": "I'm gonna check this. But thanks for the laughs(that graph) :D",
          "score": 1,
          "created_utc": "2026-02-03 18:27:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e63a1",
          "author": "mynameismati",
          "text": "So you mean I could run this on my RTX 3050 with 8GB of VRAM?",
          "score": 1,
          "created_utc": "2026-02-03 19:12:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3egbly",
          "author": "ChopSticksPlease",
          "text": "Anyone got it working on Linux?\n\nThrows some errors after the generation is seemingly completed...\n\nTypeError: AceStepConditionGenerationModel does not support len()",
          "score": 1,
          "created_utc": "2026-02-03 20:00:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3f45ch",
              "author": "Feisty_Resolution157",
              "text": "I hit that and one or two other little things. Just pasted them into Claude Code and done.",
              "score": 1,
              "created_utc": "2026-02-03 21:51:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3i35xn",
              "author": "PerkyPlant",
              "text": "For me, I got past that error and generating actual songs by replacing line 725 in [handler.py](http://handler.py) inside acestep folder:  \n  \n`if not self.model or not hasattr(self.model, 'tokenizer') or not hasattr(self.model, 'detokenizer'):`  \n  \nwith line  \n  \n`if self.model is None or not hasattr(self.model, 'tokenizer') or not hasattr(self.model, 'detokenizer'):`  \n  \nThis was provided by Claude. Make sure to restart acestep after editing the file.",
              "score": 1,
              "created_utc": "2026-02-04 09:23:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3i90rw",
          "author": "djtubig-malicex",
          "text": "As udio seems to be having its own issues now, looking forward to sinking time into the new ACE-Step 1.5 and explore the LoRA capabilities.",
          "score": 1,
          "created_utc": "2026-02-04 10:18:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lwh4y",
          "author": "LabComplete7393",
          "text": "does anyone know if this can be used on comfyui, or is this on something else entirely?",
          "score": 1,
          "created_utc": "2026-02-04 21:52:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vyg7j",
              "author": "djtubig-malicex",
              "text": "ComfyUI already natively supports ACE-Step 1.5, check the templates.",
              "score": 1,
              "created_utc": "2026-02-06 11:58:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o40moqg",
                  "author": "LabComplete7393",
                  "text": "OMG im an idiot...thanks I didn't actually think it was already on comfyui. I thought it was a whole other program.",
                  "score": 1,
                  "created_utc": "2026-02-07 02:50:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dl3w4",
          "author": "if47",
          "text": "Vibe Research, no thanks",
          "score": -15,
          "created_utc": "2026-02-03 17:37:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dlp5a",
              "author": "TheRealMasonMac",
              "text": "They‚Äôre an actual lab?",
              "score": 15,
              "created_utc": "2026-02-03 17:40:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qz5uww",
      "title": "Qwen3 Coder Next as first \"usable\" coding model < 60 GB for me",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/",
      "author": "Chromix_",
      "created_utc": "2026-02-08 10:43:59",
      "score": 326,
      "num_comments": 148,
      "upvote_ratio": 0.98,
      "text": "I've tried lots of \"small\" models < 60 GB in the past. GLM 4.5 Air, GLM 4.7 Flash, GPT OSS 20B and 120B, Magistral, Devstral, Apriel Thinker, previous Qwen coders, Seed OSS, QwQ, DeepCoder, DeepSeekCoder, etc. So what's different with Qwen3 Coder Next in OpenCode or in Roo Code with VSCodium?\n\n* **Speed**: The reasoning models would often yet not always produce rather good results. However, now and then they'd enter reasoning loops despite correct sampling settings, leading to no results at all in a large over-night run. Aside from that the sometimes extensive reasoning takes quite some time for the multiple steps that OpenCode or Roo would induce, slowing down interactive work *a lot*. Q3CN on the other hand is an instruct MoE model, doesn't have internal thinking loops and is relatively quick at generating tokens.\n* **Quality**: Other models occasionally botched the tool calls of the harness. This one seems to work reliably. Also I finally have the impression that this can handle a moderately complex codebase with a custom client & server, different programming languages, protobuf, and some quirks. It provided good answers to extreme multi-hop questions and made reliable full-stack changes. Well, almost. On Roo Code it was sometimes a bit lazy and needed a reminder to really go deep to achieve correct results. Other models often got lost.\n* **Context size**: Coding on larger projects needs context. Most models with standard attention eat all your VRAM for breakfast. With Q3CN having 100k+ context is easy. A few other models also supported that already, yet there were drawbacks in the first two mentioned points.\n\nI run the model this way:  \n`set GGML_CUDA_GRAPH_OPT=1`\n\n`llama-server -m Qwen3-Coder-Next-UD-Q4_K_XL.gguf -ngl 99 -fa on -c 120000 --n-cpu-moe 29 --temp 0 --cache-ram 0`\n\nThis works well with 24 GB VRAM and 64 GB system RAM when there's (almost) nothing else on the GPU. Yields about 180 TPS prompt processing and 30 TPS generation speed for me.\n\n* `temp 0`? Yes, works well for instruct for me, no higher-temp \"creativity\" needed. Prevents the *very occasional* issue that it outputs an unlikely (and incorrect) token when coding.\n* `cache-ram 0`? The cache was supposed to be fast (30 ms), but I saw 3 second query/update times after each request. So I didn't investigate further and disabled it, as it's only one long conversation history in a single slot anyway.\n* `GGML_CUDA_GRAPH_OPT`? Experimental option to get more TPS. Usually works, yet breaks processing with some models.\n\n**OpenCode vs. Roo Code**:\n\nBoth solved things with the model, yet with OpenCode I've seen slightly more correct answers and solutions. But: Roo asks *by default* about every single thing, even harmless things like running a syntax check via command line. This can be configured with an easy permission list to not stop the automated flow that often. OpenCode on the other hand just permits everything by default in code mode. One time it encountered an issue, uninstalled and reinstalled packages in an attempt of solving it, removed files and drove itself into a corner by breaking the dev environment. Too autonomous in trying to \"get things done\", which doesn't work well on bleeding edge stuff that's not in the training set. Permissions can of course also be configured, but the default is \"YOLO\".\n\nAside from that: Despite running with only a locally hosted model, and having disabled update checks and news downloads, OpenCode (Desktop version) tries to contact a whole lot of IPs on start-up.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o4bx0qi",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-08 22:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48mli1",
          "author": "andrewmobbs",
          "text": "I've also found Qwen3-Coder-Next to be incredible, replacing gpt-oss-120b as my standard local coding model (on a 16GB VRAM, 64GB DDR5 system).\n\nI found it worth the VRAM to increase \\`--ubatch-size\\` and \\`--batch-size\\` to 4096, which tripled prompt processing speed. Without that, the prompt processing was dominating query time for any agentic coding where the agents were dragging in large amounts of context. Having to offload another layer or two to system RAM didn't seem to hurt the eval performance nearly as much as that helped the processing.\n\nI'm using the IQ4\\_NL quant - tried the MXFP4 too, but IQ4\\_NL seemed slightly better. I am seeing very occasional breakdowns and failures of tool calling, but it mostly works.",
          "score": 34,
          "created_utc": "2026-02-08 11:56:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48pwdi",
              "author": "Chromix_",
              "text": "Setting it that high gives me 2.5x more prompt processing speed, that's quite a lot. Yet the usage was mostly dominated by inference time for me, and this drops it to 75% due to less offloaded layers. With batch 2048 it's still 83% and 2x more PP speed. Context compaction speed is notably impacted by inference time (generating 20k tokens), so I prefer having as much of the model as possible on the GPU, as my usage is rarely impacted by having to re-process lots of data.",
              "score": 2,
              "created_utc": "2026-02-08 12:23:47",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o490ujj",
              "author": "-dysangel-",
              "text": "thanks for that - I remember playing around with these values a long time ago and seeing they didn't improve inference speed - but didn't realise they could make such a dramatic difference to prompt processing. That is a very big deal",
              "score": 1,
              "created_utc": "2026-02-08 13:40:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48l0wx",
          "author": "SatoshiNotMe",
          "text": "It‚Äôs also usable in Claude Code via llama-server, set up instructions here:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md\n\nOn my M1 Max MacBook 64 GB I get a decent 20 tok/s generation speed and around 180 tok/s prompt processing",
          "score": 42,
          "created_utc": "2026-02-08 11:42:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48xzfl",
              "author": "wanderer_4004",
              "text": "Same hardware here - M1 Max MacBook 64 GB. With MLX I get 41 tok/s TG and 360 tok/s PP. However, MLX server is less good than llama.cpp in kv-caching and especially branching. Also occasionally seems to leak memory. Am using Qwen Code and am quite happy with it. Either way, Qwen Coder Next is definitely a pretty useful model and a lot stronger than the 30B versions.",
              "score": 6,
              "created_utc": "2026-02-08 13:22:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49wzi5",
                  "author": "txgsync",
                  "text": "Yep this behavior led me to write my own implementation of a MLX server with ‚Äúslots‚Äù like llama.cpp has so more than one thing can happen at a time. FLOPS/byte goes up!\n\nInferencer and LM Studio both now support this too. If you use Gas Town for parallel agentic coding this dramatically speeds things up for your Polecats. Qwen3-Coder-Next is promising on Mac with parallel agentic harnesses. But I have to test it a bit harder.",
                  "score": 3,
                  "created_utc": "2026-02-08 16:33:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4dzxnf",
                  "author": "Consumerbot37427",
                  "text": "Also on Apple Silicon w/ Max here. I have had lots of issues with MLX, I might stop bothering with them and just stick with GGUFs. Waiting for prefill is so frustrating, and seeing log messages about \"failed to trim x tokens, clearing cache instead\" drove me nuts.\n\nI had been doing successful coding with Mistral Vibe/Devstral Small, but the context management issue plus the release of Qwen3 Coder Next inspired me to try out Claude Code with LM Studio serving the Anthropic API, and it seems amazing! It seems to be much better at caching prefill and managing context, so not only do I get more tokens per second from a MoE model, the biggest bonus is how much less time is spent waiting for the context/prefill. Loving it!",
                  "score": 1,
                  "created_utc": "2026-02-09 05:47:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o48rihj",
              "author": "Chromix_",
              "text": "Claude Code uses a whole lot of tokens for the system prompt though, before any code is processed at all. OpenCode and Roo used less last time I checked. Still, maybe the results are better? I haven't tested Claude CLI with local models so far.",
              "score": 11,
              "created_utc": "2026-02-08 12:36:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48u02l",
                  "author": "SatoshiNotMe",
                  "text": "Yes CC has a sys prompt of at least 20K tokens. On my M1 Max MacBook the only interesting LLMs with good-enough generation speed are the Qwen variants such as 30B-A3B and the new coder-next. GLM-4.7-flash has been bad at around 10 tok/s.",
                  "score": 6,
                  "created_utc": "2026-02-08 12:55:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o494045",
                  "author": "msrdatha",
                  "text": "Initially I was testing both CC and opencode, but then Claude started the drama like limiting other agents and tools on api usage etc. This made me think, may be CC will not be good for local ai, the moment they feel its gaining traction and we would be suddenly banned with some artificially introduced limitations. So left cc for good and continued with opencode and kilo",
                  "score": 3,
                  "created_utc": "2026-02-08 14:00:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4asmlq",
                  "author": "cleverusernametry",
                  "text": "Roo has a very large system prompt as well no? I'm guesing opencode is the same deal",
                  "score": 2,
                  "created_utc": "2026-02-08 19:03:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ai5gt",
                  "author": "Purple-Programmer-7",
                  "text": "Opencode > Claude code. It‚Äôs okay that people don‚Äôt listen though üòÇ",
                  "score": 4,
                  "created_utc": "2026-02-08 18:15:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49fs2h",
              "author": "XiRw",
              "text": "Why don‚Äôt you use their website at this point if you are going non local with Claude instead of tunneling through an API?",
              "score": 1,
              "created_utc": "2026-02-08 15:07:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49x2tt",
                  "author": "SatoshiNotMe",
                  "text": "I mainly wanted to use the 30B local models for sensitive document work, so can‚Äôt use an API, and needed it to run on my Mac. I really wouldn‚Äôt use 30B models for serious coding; for that I just use my Max sub.",
                  "score": 1,
                  "created_utc": "2026-02-08 16:34:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o49hqub",
          "author": "Terminator857",
          "text": "Failed for me on a simple test.  Asked to list recent files in directory tree.  Worked.  Then asked to show dates and human readable file sizes.  Went into a loop.  Opencode q8.  Latest build of llama-server.  strix-halo.\n\nSecond attempt, I asked gemini for recommend command line parameters for llama-server.  It gave me: llama-server ¬†¬†-m /home/dcar/llms/qwen3/Coder-next/Qwen3-Coder-Next-Q8\\_0-00001-of-00002.gguf ¬†¬†¬†-ngl 999¬†-c 131072 ¬†¬†-fa on ¬†¬†-ctk q8\\_0 ¬†¬†¬†¬†-ctv q8\\_0 --no-mmap  \n  \nI tried again and didn't get a loop but didn't get a very good answer: find . -type f -printf '%TY-%Tm-%Td %TH:%TM:%TS %s %p\\\\n' | sort -t' ' -k1,2 -rn | head -20 | awk 'NR>1{$3=sprintf(\"%0.2fM\", $3/1048576)}1'\n\nResult for my directory tree: \n\n2026-02-03 14:36:30.4211214270 35033623392 ./qwen3/Coder-next/Qwen3-Coder-Next-Q8\\_0-00002-of-00002.gguf  \n2026-02-03 14:27:21.1727458690 47472.42M ./qwen3/Coder-next/Qwen3-Coder-Next-Q8\\_0-00001-of-00002.gguf",
          "score": 6,
          "created_utc": "2026-02-08 15:17:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49odj4",
              "author": "Chromix_",
              "text": "That was surprisingly interesting. When testing with Roo it listed the files right away, same as in your test with OpenCode. Then after asking about dates & sizes it started asking me back, not just once like it sometimes does, but forever in a loop. Powershell or cmd, how to format the output, exclude .git, only files or also directories, what date format, what size format, sort order, hidden files, and then it kept going into a loop asking about individual directories again and again. That indeed seems to be broken for some reason.",
              "score": 3,
              "created_utc": "2026-02-08 15:51:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o48n72h",
          "author": "dubesor86",
          "text": "played around with it a bit, very flakey json, forgetful to include mandatory keys and very verbose, akin to a thinker without explicit reasoning field.",
          "score": 3,
          "created_utc": "2026-02-08 12:01:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48q8jo",
              "author": "Chromix_",
              "text": "Verbose in code or in user-facing output? The latter seemed rather compact for me during the individual steps, with the regular 4 paragraph conclusion at the end of a task. Maybe temperature 0 has something to do with that.",
              "score": 1,
              "created_utc": "2026-02-08 12:26:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o49prcs",
          "author": "StardockEngineer",
          "text": "Install oh my OpenCode into OpenCode to get the Q&A part of planning as you‚Äôve described in Roo Code.  Also provides Claude Code compatibility for skills, agents and hooks.",
          "score": 4,
          "created_utc": "2026-02-08 15:58:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49vjx7",
              "author": "Chromix_",
              "text": ">99% of this project was built using OpenCode. I tested for functionality‚ÄîI don't really know how to write proper TypeScript.\n\nA vibe-coded vibe-coding tool plug-in? I'll give it a look.",
              "score": 1,
              "created_utc": "2026-02-08 16:26:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o49ybgd",
                  "author": "txgsync",
                  "text": "I like to vibe-code UIs for my vibe-coded plugins used in my vibe-coding platform.",
                  "score": 2,
                  "created_utc": "2026-02-08 16:40:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o49yhuo",
          "author": "TBG______",
          "text": "**I tested: llama.cpp +** Qwen3-Coder-Next-MXFP4\\_MOE.gguf **on RTX 5090 ‚Äì Three Setups Compared**\n\nSetup 1 ‚Äì Full GPU Layers (VRAM-heavy)  \nVRAM Usage: \\~29 GB dedicated  \nCommand: A:\\\\llama.cpp\\\\build\\\\bin\\\\Release\\\\llama-server.exe --model \"A:\\\\Qwen3-Coder-Next-GGUF\\\\Qwen3-Coder-Next-MXFP4\\_MOE.gguf\" --host [0.0.0.0](http://0.0.0.0) \\--port 8080 --alias \"Qwen3-Coder-Next\" --seed 3407 --temp 0.8 --top-p 0.95 --min-p 0.01 --top-k 40 --jinja --n-gpu-layers 28 --ctx-size 131072 --batch-size 1024 --threads 32 --threads-batch 32 --parallel 1  \nSpeed (65k token prompt):  \nPrompt eval: 381 tokens/sec  \nGeneration: 8.1 tokens/sec  \nNote: Generation becomes CPU-bound due to partial offload; high VRAM but slower output.\n\nSetup 2 ‚Äì CPU Expert Offload (VRAM-light)  \nVRAM Usage: \\~8 GB dedicated  \nCommand: A:\\\\llama.cpp\\\\build\\\\bin\\\\Release\\\\llama-server.exe --model \"A:\\\\Qwen3-Coder-Next-GGUF\\\\Qwen3-Coder-Next-MXFP4\\_MOE.gguf\" -ot \".ffn\\_.\\*\\_exps.=CPU\" --host [0.0.0.0](http://0.0.0.0) \\--port 8080 --alias \"Qwen3-Coder-Next\" --seed 3407 --temp 0.8 --top-p 0.95 --min-p 0.01 --top-k 40 --jinja --n-gpu-layers 999 --ctx-size 131072 --batch-size 1024 --threads 32 --threads-batch 32 --parallel 1  \nSpeed (70k token prompt):  \nPrompt eval: 60-140 tokens/sec (varies by cache hit)  \nGeneration: 20-21 tokens/sec  \nNote: Keeps attention on GPU, moves heavy MoE experts to CPU; fits on smaller VRAM but generation still partially CPU-limited.\n\nSetup 3 ‚Äì Balanced MoE Offload (Sweet Spot)  \nVRAM Usage: \\~27.6 GB dedicated (leaves \\~5 GB headroom)  \nCommand: A:\\\\llama.cpp\\\\build\\\\bin\\\\Release\\\\llama-server.exe --model \"A:\\\\Qwen3-Coder-Next-GGUF\\\\Qwen3-Coder-Next-MXFP4\\_MOE.gguf\" --host [0.0.0.0](http://0.0.0.0) \\--port 8080 --alias \"Qwen3-Coder-Next\" --seed 3407 --temp 0.8 --top-p 0.95 --min-p 0.01 --top-k 40 --jinja --n-gpu-layers 999 --n-cpu-moe 24 --ctx-size 131072 --batch-size 1024 --threads 32 --threads-batch 32 --parallel 1  \nSpeed (95k token prompt):  \nPrompt eval: 105-108 tokens/sec  \nGeneration: 23-24 tokens/sec  \nNote: First 24 layers' experts on CPU, rest on GPU. Best balance of VRAM usage and speed; \\~3x faster generation than Setup 1 while using similar total VRAM.\n\nSetup 4 ‚Äì Balanced MoE Offload  + full ctx size  \nVRAM Usage: \\~30.9 GB dedicated (leaves \\~1.1 GB headroom)  \nCommand: $env:GGML\\_CUDA\\_GRAPH\\_OPT=1\n\nA:\\\\llama.cpp\\\\build\\\\bin\\\\Release\\\\llama-server.exe --model \"A:\\\\Qwen3-Coder-Next-GGUF\\\\Qwen3-Coder-Next-MXFP4\\_MOE.gguf\" --host [0.0.0.0](http://0.0.0.0) \\--port 8080 --alias \"Qwen3-Coder-Next\" --seed 3407 --temp 0.8 --top-p 0.95 --min-p 0.01 --top-k 40 --jinja --n-gpu-layers 999 --n-cpu-moe 24 --ctx-size 262144 --batch-size 1024 --threads 32 --threads-batch 32 --parallel 1\n\nSpeed (95k token prompt):  \nPrompt eval: 105-108 tokens/sec  \nGeneration: 23-24 tokens/sec\n\nRecommendation: Use Setup 3 for Claude Code with large contexts. It maximizes GPU utilization without spilling, maintains fast prompt caching, and delivers the highest sustained generation tokens per second.\n\nAny ideas to speed it up ?",
          "score": 3,
          "created_utc": "2026-02-08 16:41:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49zs2z",
              "author": "Chromix_",
              "text": "With so much VRAM left on setup 3 you can bump the batch and ubatch size to 4096 as another commenter [suggested](https://www.reddit.com/r/LocalLLaMA/comments/1qz5uww/comment/o48mli1/). That should bring your prompt processing speed to roughly that of setup 1.",
              "score": 1,
              "created_utc": "2026-02-08 16:47:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4abj0m",
                  "author": "TBG______",
                  "text": "Thanks: i needed a bit more ctx sizs so i did: $env:GGML\\_CUDA\\_GRAPH\\_OPT=1\n\nA:\\\\llama.cpp\\\\build\\\\bin\\\\Release\\\\llama-server.exe --model \"A:\\\\Qwen3-Coder-Next-GGUF\\\\Qwen3-Coder-Next-MXFP4\\_MOE.gguf\" --host [0.0.0.0](http://0.0.0.0) \\--port 8080 --alias \"Qwen3-Coder-Next\" --seed 3407 --temp 0.8 --top-p 0.95 --min-p 0.01 --top-k 40 --jinja --n-gpu-layers 999 --n-cpu-moe 24 --ctx-size 180224 --batch-size 4096 --ubatch-size 2048 --threads 32 --threads-batch 32 --parallel 1\n\nSpeed (145k token prompt):  \nPrompt eval: 927 tokens/sec  \nGeneration: 23 tokens/sec\n\nInteractive speed (cached, 200‚Äì300 new tokens):  \nPrompt eval: 125‚Äì185 tokens/sec  \nGeneration: 23‚Äì24 tokens/sec\n\ncalling from Claude Code",
                  "score": 1,
                  "created_utc": "2026-02-08 17:44:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48oi89",
          "author": "msrdatha",
          "text": "Indeed the speed, quality and context size points mentioned are spot on with my test environment with mac M3 and kilo code as well. \n\nThis is my preferred model for coding now. I am switching this and Devstral-2-small from time to time.\n\nAny thoughts on which is a good model for \"Architect/Design\" solution part? Does a thinking model make any difference in design only mode?",
          "score": 3,
          "created_utc": "2026-02-08 12:12:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48r3t8",
              "author": "Chromix_",
              "text": "Reasoning models excel in design mode for me as well. I guess a suitable high-quality flow would be:\n\n* Ask your query to Q3CN, let it quickly dig through the code and summarize all the things one needs to know about the codebase for the requested feature.\n* Pass that through Qwen 3 Next Thinking, GLM 4.7 Flash, Apriel 1.6 and GPT OSS 120B and condense the different results back into options for the user to choose.\n* Manually choose an option / approach.\n* Give it back to Q3CN for execution.\n\nExperimental IDE support for that could be interesting, especially now that llama.cpp allows model swapping via API. Still, the whole flow would take a while to be executed, which could still be feasible if you want a high quality design over lunch break (well, high quality given the local model & size constraint).",
              "score": 5,
              "created_utc": "2026-02-08 12:33:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48t1w7",
                  "author": "msrdatha",
                  "text": "appreciate sharing these thoughts. makes sense very much.\n\nI have been thinking if a simple RAG system or Memory can help in such cases. Just thought only - not yet tried. Did not want to spend too much time on learning deep RAG or Memory implementation. I see kilo code does have some of these in settings. not yet tired on an actual code scenario.\n\nany thoughts or experience on such actions related to coding? ",
                  "score": 1,
                  "created_utc": "2026-02-08 12:48:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49379g",
              "author": "-dysangel-",
              "text": "How much RAM do you have? For architect/design work I think GLM 4.6/4.7 would be good. Unsloth's glm reap 4.6 at IQ2\\_XXS works well for me, taking up 89GB of RAM. I mostly use GLM Coding Plan anyway, so I just use local for chatting and experiments.\n\nHaving said that, I'm testing Qwen 3 Coder Next out just now, and it's created a better 3D driving simulation for me than GLM 4.7 did via the official coding plan. It also created a heuristic AI to play tetris with no problems. I¬†need to try pushing it even harder\n\nhttps://i.redd.it/xgtfa0jo0aig1.gif\n\n  \n",
              "score": 5,
              "created_utc": "2026-02-08 13:55:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o493jfy",
                  "author": "-dysangel-",
                  "text": "Qwen 3 Coder Next time trial game, single web page with three.js. Very often models will get the wheel orientation incorrect etc. It struggled a bit to get the road spline correct, but fixed it after a few iterations of feedback :)\n\nhttps://i.redd.it/l9j55bxr0aig1.gif\n\n",
                  "score": 1,
                  "created_utc": "2026-02-08 13:57:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o494jo5",
                  "author": "msrdatha",
                  "text": "89GB of RAM at what context size?",
                  "score": 1,
                  "created_utc": "2026-02-08 14:03:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4aixoh",
          "author": "EliasOenal",
          "text": "I have had good results with Qwen3 Coder Next (Unsloth's Qwen3-Coder-Next-UD-Q4_K_XL.gguf) locally on Mac, it is accurate even with reasonably complex tool use and works with interactive tools [through the term-cli skill](https://github.com/EliasOenal/term-cli) in OpenCode. [Here's a video clip of it interactively debugging with lldb.](https://www.youtube.com/watch?v=bas5abIAsH4) (Left side is me attaching a session to Qwen's interactive terminal to have a peek.)",
          "score": 3,
          "created_utc": "2026-02-08 18:19:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ajqmy",
              "author": "Chromix_",
              "text": "Soo you're saying when I install the term-cli plugin then my local OpenCode with Qwen can operate my Claude CLI for me? üòâ",
              "score": 1,
              "created_utc": "2026-02-08 18:22:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ale8n",
                  "author": "EliasOenal",
                  "text": "Haha, indeed! I yesterday wanted to debug a sporadic crash I encountered twice in llama.cpp, when called from OpenCode. (One of the risks of being on git HEAD.) I spawned two term-cli sessions, one with llama.cpp and one with OpenCode, asking a second instance of OpenCode to take over to debug this. It actually ended up typing into OpenCode, running prompts, but it wasn't able to find a way to reproduce the crash 50k tokens in. So I halted that for now.",
                  "score": 1,
                  "created_utc": "2026-02-08 18:30:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48hc4c",
          "author": "Blues520",
          "text": "Do you find it able to solve difficult tasks because I used the same quant and it was coherent but the quality was so so.",
          "score": 6,
          "created_utc": "2026-02-08 11:08:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48i7qd",
              "author": "-dysangel-",
              "text": "My feeling is that the small/medium models are not going to be that great at advanced problem solving, but they're getting to the stage where they will be able to follow instructions well to generate working code. I think you'd still want a larger model like GLM/Deepseek for more in depth planning and problem solving, and then Qwen 3 Coder has a chance of being able to implement individual steps. And you'd still want to fall back to a larger model or yourself if it gets stuck.",
              "score": 8,
              "created_utc": "2026-02-08 11:16:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48k9lw",
                  "author": "Chromix_",
                  "text": "Yes, for the occasional really \"advanced problem solving\" I fill the context of the latest GPT model with manually curated pages of code and text, set it to high reasoning, max tokens and get a coffee. Despite that, and yielding pretty good results and insights for some things, it still frequently needs corrections due to missing optimal (or well, better) solutions. Q3CN has no chance to compete with that. Yet it doesn't need to for regular day-to-day dev work, that's my point - seems mostly good enough.",
                  "score": 3,
                  "created_utc": "2026-02-08 11:35:40",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o48ifp1",
                  "author": "Blues520",
                  "text": "That makes sense and it does do well in tool calling which some models like Devstral trip themselves over with.",
                  "score": 1,
                  "created_utc": "2026-02-08 11:18:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o48j2dj",
              "author": "Chromix_",
              "text": "The model & inference in llama.cpp [had issues](https://www.reddit.com/r/LocalLLaMA/comments/1quvqs9/comment/o3edjam/?context=3) when they were released initially. This has been fixed by now. So if you don't use the latest version of llama.cpp or haven't (re-)downloaded the [updated quants](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) then that could explain the mixed quality you were seeing. I also tried the Q8 REAP vs a UD Q4, but the Q8 was making more mistakes, probably because the [REAP quants](https://www.reddit.com/r/LocalLLaMA/comments/1qvjonm/first_qwen3codernext_reap_is_out/) haven't been updated yet, or maybe it's due to REAP itself.\n\nFor \"difficult tasks\": I did not test the model on LeetCode challenges, implementing novel algorithms and things, but on normal dev work: Adding new features, debugging & fixing broken things in a poorly documented real-life project - no patent-pending compression algorithms and highly exotic stuff.\n\nThe latest Claude 4.6 or GPT-5.2 Codex performs of course *way better*. More direct approach towards the solution, sometimes better approaches that the Q3CN didn't find at all. Yet still, for \"just getting some dev work done\" it's no longer needed to have the latest and greatest. Q3CN is the first local model that's usable for me in this area. Of course you might argue that using the latest SOTA is always best, as you always want the fastest, best solution, no matter what, and I would agree.",
              "score": 3,
              "created_utc": "2026-02-08 11:24:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48ji52",
                  "author": "Blues520",
                  "text": "I pulled the latest model and llamacpp yesterday so the fixes were in. I'm not saying that it's a bad model, I guess I was expecting more given the hype. \n\nI didn't do any leetcode but normal dev stuff as well. I suspect that a higher quant will be better. I wouldn't bother with the REAP quant though.",
                  "score": 3,
                  "created_utc": "2026-02-08 11:28:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o48hwfy",
              "author": "Status_Contest39",
              "text": "the same feeling, even not as good as GLM4.7 flash",
              "score": 2,
              "created_utc": "2026-02-08 11:14:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o48lz0y",
              "author": "mysho",
              "text": "I tried to let it convert a simple systems service to one activated by a socket. Used Kilo code with qwen3-coder-next. Took it 30 requests for such a trivial task, but it managed in the end. I expected better, but it's kinda usable for trivial stuff.",
              "score": 1,
              "created_utc": "2026-02-08 11:50:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48u0pw",
          "author": "fadedsmile87",
          "text": "I have an RTX 5090 + 96GB of RAM. I'm using the Q8\\_0 quant of Qwen3-Coder-Next with \\~100k context window with Cline. It's magnificent. It's a very capable coding agent. The downside of using that big quant is the tokens per second. I'm getting 8-9 tokens / s for the first 10k tokens, then it drops to around 6 t/s at 50k full context.",
          "score": 5,
          "created_utc": "2026-02-08 12:55:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48vzk2",
              "author": "Chromix_",
              "text": "That's surprisingly slow, especially given that you have a RTX 5090. You should be getting *at least* half the speed that I'm getting with a Q4. Did you try with my way of running it (of course with manually adjusted ncmoe to almost fill the VRAM)? Maybe there's a RAM speed impact. With 96 GB you might be running at a rather low speed in practice if you have 4 modules. Mine are running at DDR5-6000.",
              "score": 2,
              "created_utc": "2026-02-08 13:09:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48x0xi",
                  "author": "fadedsmile87",
                  "text": "I have 2x 48GB DDR5 mem sticks. 6000 MT/s (down from 6400 for stability)  \ni9-14900K\n\nI'm using the default settings in LM Studio.  \ncontext: 96k  \noffloading 15/48 layers onto GPU (LM Studio estimates 28.23GB on GPU, 90.23GB on RAM)",
                  "score": 2,
                  "created_utc": "2026-02-08 13:16:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49ylg3",
              "author": "blackhawk00001",
              "text": "Same setup here, 96GB/5090/7900x/windows hosted on LAN to be used by VS code IDE with kilo code extension on a linux desktop.\n\nTry using llama.cpp server, below are the commands that I'm using to get 30 t/s with Q4\\_K\\_M and 20 t/s with Q8.  The Q8 is slower but solved a problem in one pass that the Q4 could not figure out.  Supposedly it's much faster on vulkan at this time but I haven't tried yet.\n\n.\\\\llama-server.exe -m \"D:\\\\llm\\_models\\\\Qwen3-Coder-Next-GGUF\\\\Qwen3-Coder-Next-Q8\\_0-00001-of-00003.gguf\" --jinja --temp 1.0 --top-p 0.95 --min-p 0.01 --top-k 40 -fa on --fit on -c 131072 --no-mmap --host\n\nI love using LM Studio for quick chat sessions but it was terrible for local LLM agents in an IDE.",
              "score": 1,
              "created_utc": "2026-02-08 16:41:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4a8u8t",
                  "author": "fadedsmile87",
                  "text": "I was using LM Studio.\n\nThanks to Chromix, I've installed llama.cpp and used:  \nllama-server -m Qwen3-Coder-Next-Q8\\_0-00001-of-00003.gguf -fa on --fit-ctx 120000 --fit on --temp 0 --cache-ram 0 --fit-target 128\n\nNow I'm getting 27 t/s on the Q8\\_0 quant :-)",
                  "score": 3,
                  "created_utc": "2026-02-08 17:31:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4a7qmj",
              "author": "fragment_me",
              "text": "That's expected. I get 17-18 tok/s with a 5090 and ddr4 using UD q6 k xl. Q8 is huge.  My command with param for running ud q6 k xl are :\n\n.\\\\llama-server.exe -m Qwen3-Coder-Next-UD-Q6\\_K\\_XL.gguf  \\`\n\n\\-ot \".(19|\\[2-9\\]\\[0-9\\]).ffn\\_(gate|up|down)\\_exps.=CPU\" \\`\\`\n\n\\--no-mmap --jinja --threads -12  \\`\n\n\\--cache-type-k q8\\_0 --cache-type-v q8\\_0  --flash-attn on  --ctx-size 128000 -kvu \\`\n\n\\--temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01  \\`\n\n`--host` [`127.0.0.1`](http://127.0.0.1) \\`--parallel 4 --batch-size 512  \\`",
              "score": 1,
              "created_utc": "2026-02-08 17:25:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4a8pb2",
                  "author": "fadedsmile87",
                  "text": "I was using LM Studio.\n\nThanks to Chromix, I've installed llama.cpp and used:  \nllama-server -m Qwen3-Coder-Next-Q8\\_0-00001-of-00003.gguf -fa on --fit-ctx 120000 --fit on --temp 0 --cache-ram 0 --fit-target 128\n\nNow I'm getting 27 t/s on the Q8\\_0 quant :-)",
                  "score": 1,
                  "created_utc": "2026-02-08 17:30:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48qjgh",
          "author": "Brilliant-Length8196",
          "text": "Try Kilo Code instead of Roo Code.",
          "score": 5,
          "created_utc": "2026-02-08 12:28:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49i4st",
              "author": "Terminator857",
              "text": "Last time I tried, I didn't have an easy time figuring out how to wire kilocode with llama-server.",
              "score": 1,
              "created_utc": "2026-02-08 15:19:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4dyjyr",
                  "author": "alexeiz",
                  "text": "Use \"openai compatible\" settings.",
                  "score": 1,
                  "created_utc": "2026-02-09 05:36:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o494rp9",
          "author": "Danmoreng",
          "text": "Did you try the fit and fit-ctx parameters instead of ngl and n-cpu-moe ? Just read the other benchmark thread (https://www.reddit.com/r/LocalLLaMA/comments/1qyynyw/llamacpps\\_fit\\_can\\_give\\_major\\_speedups\\_over\\_ot\\_for/) and tested on my hardware, it gives better speed.",
          "score": 2,
          "created_utc": "2026-02-08 14:04:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o499myz",
              "author": "Chromix_",
              "text": "Yes, tried that (and even [commented](https://www.reddit.com/r/LocalLLaMA/comments/1qyynyw/comment/o488gm0/) how to squeeze more performance out of it) but it's not faster for me, usually a bit slower.",
              "score": 3,
              "created_utc": "2026-02-08 14:33:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4akno5",
          "author": "LoSboccacc",
          "text": "    srv  update_slots: all slots are idle\n    srv  params_from_: Chat format: Qwen3 Coder\n    slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1\n    slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist\n    slot launch_slot_: id  3 | task 0 | processing task, is_child = 0\n    slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 65536, n_keep = 0, task.n_tokens = 1042\n    slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)\n    slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 978, batch.n_tokens = 978, progress = 0.938580\n    slot update_slots: id  3 | task 0 | n_tokens = 978, memory_seq_rm [978, end)\n    slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 1042, batch.n_tokens = 64, progress = 1.000000\n    slot update_slots: id  3 | task 0 | prompt done, n_tokens = 1042, batch.n_tokens = 64\n    slot init_sampler: id  3 | task 0 | init sampler, took 0.13 ms, tokens: text = 1042, total = 1042\n    slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 977, pos_max = 977, size = 75.376 MiB)\n    slot print_timing: id  3 | task 0 |\n    prompt eval time =    8141.99 ms /  1042 tokens (    7.81 ms per token,   127.98 tokens per second)\n           eval time =    4080.08 ms /    65 tokens (   62.77 ms per token,    15.93 tokens per second)\n\nit's not much but two year ago we we'd having 15 tps on capybara 14b being barely coherent and now we have a somewhat usable haiku 3.5 at home",
          "score": 2,
          "created_utc": "2026-02-08 18:27:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ashz5",
          "author": "DHasselhoff77",
          "text": "Qwen3 Coder Next also supports fill-in-the-middle (FIM) tasks. This means you can use it for auto completion via for example llama-vscode while also using it for agentic tasks. No need for two different models occupying VRAM simultaneously.\n\nEdit: Alright actually it's not a great fit because as a recurrent model, llama.cpp can't cache it properly. See https://github.com/ggml-org/llama.cpp/pull/19408#issuecomment-3866421943",
          "score": 2,
          "created_utc": "2026-02-08 19:02:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ax2ua",
              "author": "Chromix_",
              "text": "It'd be a rather good yet slow FIM model, yes. On the other hand there is [Falcon 90M ](https://www.reddit.com/r/LocalLLaMA/comments/1qdl9za/falcon_90m/)with FIM support which you could easily squeeze into the remaining VRAM or even run on CPU for auto-complete. ",
              "score": 1,
              "created_utc": "2026-02-08 19:25:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4auxsr",
          "author": "live4evrr",
          "text": "I was almost ready to give up on it, but after downloading the latest GGUF (4 bit XL) from Unsloth and updating llama.cpp, it is a good local option. Of course can't be compared to frontier cloud models (no it is not nearly as good as Sonnet 4.5) but it is still good. Amazing how well it can run so well on a 32GB VRAM card with sufficient ram (64+).",
          "score": 2,
          "created_utc": "2026-02-08 19:14:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4blw6h",
          "author": "rm-rf-rm",
          "text": "Looking for help in getting it working with MLX üôè  https://old.reddit.com/r/LocalLLaMA/comments/1qwa7jy/qwen3codernext_mlx_config_for_llamaswap/",
          "score": 2,
          "created_utc": "2026-02-08 21:28:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4dh9y6",
          "author": "AcePilot01",
          "text": "Imight copy your settings there, cus I also have a 4090 and 64gb of ram lol",
          "score": 2,
          "created_utc": "2026-02-09 03:37:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ecyd9",
              "author": "Chromix_",
              "text": "You'll need to ensure to have [sufficient free VRAM](https://www.reddit.com/r/LocalLLaMA/comments/1klqw5a/more_free_vram_for_your_llms_on_windows/) to achieve similar numbers - or tweak the `-n-cpu-moe` parameter a bit.",
              "score": 1,
              "created_utc": "2026-02-09 07:42:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o48hkmt",
          "author": "simracerman",
          "text": "Good reference. I will give opencode and this model a try.\n\nAt what context size did you notice it lost the edge?",
          "score": 1,
          "created_utc": "2026-02-08 11:11:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48jemi",
              "author": "Chromix_",
              "text": "So far I didn't observe any issue where it did something where it \"should have known better\" due to having relevant information in its context, but I \"only\" used it up to 120k. Of course its long context handling is far from perfect, yet it seems good enough in practice for now. Kimi Linear should be better in that aspect (not coding though), but I haven't tested it yet.",
              "score": 1,
              "created_utc": "2026-02-08 11:27:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o48ie2y",
          "author": "Easy_Kitchen7819",
          "text": "Try DeepSwe",
          "score": 1,
          "created_utc": "2026-02-08 11:18:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48p0ce",
              "author": "Money-Frame7664",
              "text": "Do you mean agentica-org/DeepSWE-Preview ?",
              "score": 2,
              "created_utc": "2026-02-08 12:16:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48pimy",
          "author": "jacek2023",
          "text": "How do you use OpenCode on 24 GB VRAM? How long do you wait for prefill? Do you have this fix? [https://github.com/ggml-org/llama.cpp/pull/19408](https://github.com/ggml-org/llama.cpp/pull/19408)",
          "score": 1,
          "created_utc": "2026-02-08 12:20:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48slm6",
              "author": "Odd-Ordinary-5922",
              "text": "if you have --cache-ram set to something high prefill isnt really a problem",
              "score": 1,
              "created_utc": "2026-02-08 12:44:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48tc04",
                  "author": "jacek2023",
                  "text": "I use --cache-ram 60000, what's your setting?",
                  "score": 1,
                  "created_utc": "2026-02-08 12:50:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o48usbl",
              "author": "Chromix_",
              "text": "Thanks for pointing that out. No I haven't tested with this very recent fix yet. [ggerganov states](https://github.com/ggml-org/llama.cpp/pull/19408#issuecomment-3866421943) though that reprocessing would be unavoidable if something early in the prompt is changed - which is exactly what happens when Roo Code for example switches from \"Architect\" to \"Code\" mode.\n\nHow I use OpenCode with 24GB VRAM? Exactly with the model, quant and command line stated in my posting, although prompt processing [could be faster](https://www.reddit.com/r/LocalLLaMA/comments/1qz5uww/comment/o48mli1/) as pointed out in another comment. With Roo the initial processing takes between 15 to 40 seconds before it jumps into action, yet as it'll iterate quite some time on its own anyway, waiting for prefill isn't that important for me.",
              "score": 1,
              "created_utc": "2026-02-08 13:00:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48vtx2",
                  "author": "jacek2023",
                  "text": "Yes I am thinking about trying roo (I tested that it works), but I am not sure how \"agentic\" it is. Can you make it compile and run your app like in opencode? I use Claude Code (+Claude) and Codex (+GPT 5.3) simultaneously and opencode works similarly, can I achieve that workflow in roocode?",
                  "score": 1,
                  "created_utc": "2026-02-08 13:08:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48znv3",
          "author": "rorowhat",
          "text": "The Q4 quant was taking 62GB of Ram on LMstudio as well, it didn't make sense.",
          "score": 1,
          "created_utc": "2026-02-08 13:33:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49p4h0",
          "author": "klop2031",
          "text": "Yeah i feel the same. For the first time this thing can do agentic tasks and can code well. I actually found myself not using a frontier model and just using this because of privacy. Im like wow so much better",
          "score": 1,
          "created_utc": "2026-02-08 15:55:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49tyhf",
          "author": "anoni_nato",
          "text": "I'm getting quite good results coding with Mistral Vibe and GLM 4.5 air free (openrouter, can't self host yet).\n\n\nHas its issues (search and replace fails often so it switches to file overwrite, and sometimes it loses track of context size) but it's producing code that works without me opening an IDE.",
          "score": 1,
          "created_utc": "2026-02-08 16:19:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4a0kmc",
          "author": "HollowInfinity",
          "text": "I used OpenCode, roo, my own agent and others but found the best agent is (unsurprisingly) Qwen-Code. The system prompts and tool setup is probably exactly what the agent is trained for. Although as I type this you could probably just steal their tool definitions and prompts for whatever agent you're using.",
          "score": 1,
          "created_utc": "2026-02-08 16:51:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4aso5l",
          "author": "jedsk",
          "text": "did you get any err outs with opencode?\n\nit kept failing for me when just building/editing an html page",
          "score": 1,
          "created_utc": "2026-02-08 19:03:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4az8z6",
              "author": "Chromix_",
              "text": "No obvious errors aside from the initial uninstalling of packages because it wasn't prompted to leave the dev environment alone. Well, and then there's this - the first and sometimes second LLM call in a sequence always fails for some reason, despite the server being available:\n\nhttps://preview.redd.it/30iad8mbpbig1.png?width=862&format=png&auto=webp&s=d391b1e735271b6cb1c5a67b10b1b245fd6ac309\n\n",
              "score": 1,
              "created_utc": "2026-02-08 19:35:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4b82kw",
          "author": "IrisColt",
          "text": "Thanks!!!",
          "score": 1,
          "created_utc": "2026-02-08 20:19:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4b93l1",
          "author": "shrug_hellifino",
          "text": "Any quant I use, I am getting this error: forcing full prompt re-processing due to lack of cache data as shown below, reloading 50,60,70,150k context over and over is quite miserable.. all latest build and fresh quant dl just in case as of today 2/8/26. Any guidance or insight would be appreciated.\n\n    slot print_timing: id  3 | task 6556 | \n    prompt eval time =   95123.58 ms / 54248 tokens (    1.75 ms per token,   570.29 tokens per second)\n           eval time =   15815.35 ms /   666 tokens (   23.75 ms per token,    42.11 tokens per second)\n          total time =  110938.94 ms / 54914 tokens\n    slot      release: id  3 | task 6556 | stop processing: n_tokens = 54913, truncated = 0\n    srv  update_slots: all slots are idle\n    srv  log_server_r: done request: POST /v1/messages 127.0.0.1 200\n    srv  params_from_: Chat format: Qwen3 Coder\n    slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = 99553296262\n    srv  get_availabl: updating prompt cache\n    srv   prompt_save:  - saving prompt with length 53502, total state size = 1329.942 MiB\n    srv          load:  - looking for better prompt, base f_keep = 0.001, sim = 0.001\n    srv        update:  - cache size limit reached, removing oldest entry (size = 285.055 MiB)\n    srv        update:  - cache size limit reached, removing oldest entry (size = 840.596 MiB)\n    srv        update:  - cache size limit reached, removing oldest entry (size = 1335.463 MiB)\n    srv        update:  - cache state: 5 prompts, 6876.842 MiB (limits: 8192.000 MiB, 262144 tokens, 311062 est)\n    srv        update:    - prompt 0x5b0377f1a0f0:   50773 tokens, checkpoints:  1,  1341.325 MiB\n    srv        update:    - prompt 0x5b036f03bf40:   51802 tokens, checkpoints:  1,  1365.454 MiB\n    srv        update:    - prompt 0x5b0375648cd0:   52203 tokens, checkpoints:  1,  1374.857 MiB\n    srv        update:    - prompt 0x5b0376a89180:   52844 tokens, checkpoints:  1,  1389.888 MiB\n    srv        update:    - prompt 0x5b0378b94380:   53502 tokens, checkpoints:  1,  1405.317 MiB\n    srv  get_availabl: prompt cache update took 1473.25 ms\n    slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> ?temp-ext -> dist \n    slot launch_slot_: id  2 | task 7250 | processing task, is_child = 0\n    slot update_slots: id  2 | task 7250 | new prompt, n_ctx_slot = 262144, n_keep = 0, task.n_tokens = 54953\n    slot update_slots: id  2 | task 7250 | n_past = 36, slot.prompt.tokens.size() = 53502, seq_id = 2, pos_min = 53501, n_swa = 1\n    slot update_slots: id  2 | task 7250 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n    slot update_slots: id  2 | task 7250 | erased invalidated context checkpoint (pos_min = 52818, pos_max = 52818, n_swa = 1, size = 75.376 MiB)\n    slot update_slots: id  2 | task 7250 | n_tokens = 0, memory_seq_rm [0, end)",
          "score": 1,
          "created_utc": "2026-02-08 20:24:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4bcg47",
              "author": "Chromix_",
              "text": "A potential fix for [this](https://github.com/ggml-org/llama.cpp/issues/19394) was just merged, get the latest version and test again :-)  \nYou could also increase `--cache-ram` if you have some free RAM to spare.",
              "score": 2,
              "created_utc": "2026-02-08 20:41:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4brxgi",
                  "author": "shrug_hellifino",
                  "text": "Wow, I just rebuilt this morning, so this is that new? Thank you for the pointer!",
                  "score": 2,
                  "created_utc": "2026-02-08 21:58:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4baxv3",
          "author": "Savantskie1",
          "text": "I‚Äôm currently using vs code insiders, I can‚Äôt use cli coding tools. So can you check to see if this model will work with that? I use lm studio, I don‚Äôt care if llama.cpp is faster I won‚Äôt use it so don‚Äôt suggest it please.",
          "score": 1,
          "created_utc": "2026-02-08 20:33:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4be5qj",
              "author": "Chromix_",
              "text": "Roo Code is a VSCode plugin that you can use with any OpenAI-compatible API, like for example LMStudio provides. Out of interest: Is there a specific reason to stick to LMStudio if it's only used as API endpoint for a IDE (or IDE plugin)? The difference can be very large as another commenter [found out](https://www.reddit.com/r/LocalLLaMA/comments/1qz5uww/comment/o4a32mu/).",
              "score": 1,
              "created_utc": "2026-02-08 20:50:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4bjxsc",
                  "author": "Savantskie1",
                  "text": "I don‚Äôt care about speed, I care about ease of use and being able to load and unload a model without needing to spawn a separate instance of the model runner. That‚Äôs just waste of resources. Unnecessary overhead",
                  "score": 1,
                  "created_utc": "2026-02-08 21:18:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4blpeo",
          "author": "HumanDrone8721",
          "text": "Just in case someone wonders here are the fresh benchmark on a semi-potato PC i7-14KF (4090+3090+128GB DDR5) for the 8bit fat quanta, coding performance later:\n\n    llama-bench -m  .cache/llama.cpp/unsloth_Qwen3-Coder-Next-GGUF_UD-Q8_K_XL_Qwen3-Coder-Next-UD-Q8_K_XL-00001-of-00003.gguf -fa on -ngl 26 -mmp 0\n    ggml_cuda_init: found 2 CUDA devices:\n      Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n      Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n    | model                          |       size |     params | backend    | ngl |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n    | qwen3next 80B.A3B Q8_0         |  79.83 GiB |    79.67 B | CUDA       |  26 |           pp512 |        220.85 ¬± 2.24 |\n    | qwen3next 80B.A3B Q8_0         |  79.83 GiB |    79.67 B | CUDA       |  26 |           tg128 |         14.68 ¬± 0.27 |",
          "score": 1,
          "created_utc": "2026-02-08 21:27:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4bnefo",
              "author": "Chromix_",
              "text": "That TG speed looks slower than expected. In another comment [here](https://www.reddit.com/r/LocalLLaMA/comments/1qz5uww/comment/o4a32mu/) someone got 27 t/s with a single RTX 5090 and your CPU. Yes, the 5090 is faster, but not twice as fast. Have you tried only using the 4090, and the options/settings from my post, just to get an idea if things can be sped up for you?",
              "score": 1,
              "created_utc": "2026-02-08 21:35:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4brkdo",
                  "author": "HumanDrone8721",
                  "text": "That's for the 4090 and 3090 separate benchmarks, the fact that only 14 layers fit in card and the difference is negligible between cards tells me the the performance is RAM and CPU bound and not on the capabilities of the GPU. \n\nThe poster with the 5090 probably managed to fit 39 or even 40 layers in the GPU and this gave a boost of speed, unfortunately as almost no one is bothered to post the actual precise command line and parameters, is just some anecdote.\n\n    CUDA_VISIBLE_DEVICES=0 llama-bench -m  .cache/llama.cpp/unsloth_Qwen3-Coder-Next-GGUF_UD-Q8_K_XL_Qwen3-Coder-Next-UD-Q8_K_XL-00001-of-00003.gguf -fa on -ngl 14 -mmp 0\n    ggml_cuda_init: found 1 CUDA devices:\n      Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n    | model                          |       size |     params | backend    | ngl |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n    | qwen3next 80B.A3B Q8_0         |  79.83 GiB |    79.67 B | CUDA       |  14 |           pp512 |        167.85 ¬± 1.60 |\n    | qwen3next 80B.A3B Q8_0         |  79.83 GiB |    79.67 B | CUDA       |  14 |           tg128 |         10.74 ¬± 0.05 |\n\n\n    CUDA_VISIBLE_DEVICES=1 llama-bench -m  .cache/llama.cpp/unsloth_Qwen3-Coder-Next-GGUF_UD-Q8_K_XL_Qwen3-Coder-Next-UD-Q8_K_XL-00001-of-00003.gguf -fa on -ngl 14 -mmp 0\n    ggml_cuda_init: found 1 CUDA devices:\n      Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n    | model                          |       size |     params | backend    | ngl |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n    | qwen3next 80B.A3B Q8_0         |  79.83 GiB |    79.67 B | CUDA       |  14 |           pp512 |        160.27 ¬± 1.55 |\n    | qwen3next 80B.A3B Q8_0         |  79.83 GiB |    79.67 B | CUDA       |  14 |           tg128 |         10.55 ¬± 0.15 |\n\n    build: 8872ad212 (7966)",
                  "score": 1,
                  "created_utc": "2026-02-08 21:56:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4burr5",
          "author": "Zestyclose_Yak_3174",
          "text": "Seems like the verdict is still out. Many seem to say it's good, yet also many seem to say it is a very weak model in the real world.",
          "score": 1,
          "created_utc": "2026-02-08 22:13:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4c61i6",
          "author": "pol_phil",
          "text": "This model is great. My only problem is that its prefix caching doesn't work on vLLM. I think SGLang has solved this, but haven't tried it yet.\n\nAre u aware of other serving frameworks which do not have this issue? Because, for me, it turns out slower than larger models (for long conversations)",
          "score": 1,
          "created_utc": "2026-02-08 23:15:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ebt1d",
              "author": "Chromix_",
              "text": "[Two](https://github.com/ggml-org/llama.cpp/issues/19394) [fixes](https://github.com/ggml-org/llama.cpp/pull/19408) in that area were just added for llama.cpp. vLLM is of course faster if you have the VRAM for it.",
              "score": 1,
              "created_utc": "2026-02-09 07:30:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4dc0yu",
          "author": "BrianJThomas",
          "text": "Couldn‚Äôt do any tool calls successfully for me in opencode and I gave up.",
          "score": 1,
          "created_utc": "2026-02-09 03:07:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ebyuv",
              "author": "Chromix_",
              "text": "Latest updated model quant, at least Q4, latest llama.cpp, latest opencode? There were issues 5 days ago that have been solved since then. I have not seen a single failed too call since then.",
              "score": 1,
              "created_utc": "2026-02-09 07:32:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4dcal1",
          "author": "AcePilot01",
          "text": "How do you \"run \" one like that? \n\nI use Openwebui and ollama, so when I download them (forget how they even get placed in there, lmfao I just have ai do it all haha)",
          "score": 1,
          "created_utc": "2026-02-09 03:09:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ech8y",
              "author": "Chromix_",
              "text": "Ditch ollama for llama.cpp. [He could do it](https://www.reddit.com/r/LocalLLaMA/comments/1qz5uww/comment/o49lydo/), you can do it too. (To be fair you can also connect OpenCode to ollama, but why not switch to something nicer while being at it?)",
              "score": 1,
              "created_utc": "2026-02-09 07:37:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4dkx7d",
          "author": "AcePilot01",
          "text": "Are your comparisons of Opencode and roo code compared to Qwen3 coder next, or am I missing something? or are those agents what you USE this model with?\n\nOr could you just use those settings on say, openwebui? Or use this in that way? did you? are those local?\n\nIdeally I would like to code locally perhaps in a number of IDE?  I assume Roo/Open code are basically just IDE's?",
          "score": 1,
          "created_utc": "2026-02-09 04:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4edjhc",
              "author": "Chromix_",
              "text": "You cannot compare \"OpenCode\" to \"Qwen3\", because OpenCode is a harness for using LLMs, and Qwen3 is a LLM. My post is about using both OpenCode as well as Roo Code with Qwen3 Coder Next (Q3CN).\n\nYou can also use OpenWebUI with Q3CN, but it doesn't give you any agentic coding functionality like OpenCode or Roo. You could paste in code though.\n\n>I assume Roo/Open code are basically just IDE's?\n\nNo, Roo Code is a plugin for VSCode (an IDE), so if you install it you have agentic coding in an IDE. Of course you could also rewire the Copilot that's forced into VSCode for local LLMs. OpenCode is less of an IDE, but more a vibe-coding tool.",
              "score": 1,
              "created_utc": "2026-02-09 07:47:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4dqgfn",
          "author": "Revolutionary_Loan13",
          "text": "Anyone using a docker image with lama-server on it or does it not perform as well?",
          "score": 1,
          "created_utc": "2026-02-09 04:36:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4edyo6",
              "author": "Chromix_",
              "text": "What for would you use docker? One of the [main points](https://github.com/ggml-org/llama.cpp?tab=readme-ov-file#description) about llama.cpp is that you can just use it as-is, without having to install any dependencies. You don't even need to install llama.cpp, just copy and run the binary distribution. Docker is usually used to run things that need dependencies, a running database server, whatsoever.\n\nIt'd be like taking your M&Ms out of the pack and wrapping them individually before eating them, just because you're used to unwrap your candy one by one when snacking.",
              "score": 1,
              "created_utc": "2026-02-09 07:51:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ek0h9",
          "author": "crablu",
          "text": "I have problems running qwen3-coder-next with opencode (RTX 5090, 64GB RAM). I tried with Qwen3-Coder-Next-UD-Q4_K_XL.gguf and Qwen3-Coder-Next-MXFP4_MOE.gguf. It works perfectly fine in chat.\n\nstart command:\n\n    llama-server.exe ^\n     --models-preset \"E:\\LLM\\llama-server\\models.ini\" ^\n     --models-max 1 ^\n     --parallel 1 ^\n     --cont-batching ^\n     --flash-attn on ^\n     --jinja ^\n     --port 8080\n\nmodels.ini:\n\n    [qwen3-coder-next-mxfp4]\n    model   = E:\\LLM\\models\\unsloth\\Qwen3-Coder-Next-GGUF\\Qwen3-Coder-Next-MXFP4_MOE.gguf\n    c = 65536\n    b = 8192\n    ub = 8192\n    temp = 1\n    top-p = 0.95\n    top-k = 40\n    min-p = 0.01\n    n-cpu-moe = 24\n    no-mmap = true    \n\nOpencode is not able to use the write tool. The UI says invalid.\nI built latest llama.cpp. Does anyone know how to fix this?",
          "score": 1,
          "created_utc": "2026-02-09 08:50:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ekcr3",
              "author": "Chromix_",
              "text": "Try temperature 0, verify that you have the latest update of the Q4 model. It works reliably for me with that.",
              "score": 1,
              "created_utc": "2026-02-09 08:54:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4emix8",
                  "author": "crablu",
                  "text": "With temp 0 it seems to work now. Thank you.",
                  "score": 1,
                  "created_utc": "2026-02-09 09:15:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1quvvtv",
      "title": "Qwen3-Coder-Next",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/Qwen/Qwen3-Coder-Next",
      "author": "danielhanchen",
      "created_utc": "2026-02-03 16:03:56",
      "score": 321,
      "num_comments": 98,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1quvvtv/qwen3codernext/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o3fvty3",
          "author": "rm-rf-rm",
          "text": "Locked post as its duplicated. Use the bigger thread here: https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/",
          "score": 1,
          "created_utc": "2026-02-04 00:14:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3d1q7m",
          "author": "danielhanchen",
          "text": "We made some Dynamic Unsloth GGUFs for the model at https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF - MXFP4 MoE and FP8-Dynamic will be up shortly.\n\nWe also made a guide: https://unsloth.ai/docs/models/qwen3-coder-next which also includes how to use Claude Code / Codex with Qwen3-Coder-Next locally",
          "score": 80,
          "created_utc": "2026-02-03 16:08:00",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3d6pi6",
              "author": "bick_nyers",
              "text": "MXFP4 and FP8-Dynamic? Hell yeah!",
              "score": 17,
              "created_utc": "2026-02-03 16:31:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d8adk",
                  "author": "danielhanchen",
                  "text": "They're still uploading and converting!",
                  "score": 8,
                  "created_utc": "2026-02-03 16:38:34",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3d560k",
              "author": "AXYZE8",
              "text": "Can you please benchmark the PPL/KLD/whatever with these new these new FP quants? I remember you did such benchmark way back for DeepSeek & Llama. It would be very interesting to see if MXFP4 improves things and if so then how much (is it better than Q5\\_K\\_XL for example?).",
              "score": 15,
              "created_utc": "2026-02-03 16:24:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d82x0",
                  "author": "danielhanchen",
                  "text": "Yes our plan was to do them! I'll update you!",
                  "score": 19,
                  "created_utc": "2026-02-03 16:37:37",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o3f8ogf",
                  "author": "Holiday_Purpose_3166",
                  "text": "I'd like to see this too. \n\nAssuming the model never seen MXFP4 in training it's likely to have lowest PPL - better than BF16 and Q8_0 but have a KLD better than Q4_K_M.\n\nAt least that's what was noticed in noctrex GLM 4.7 Flash quant",
                  "score": 1,
                  "created_utc": "2026-02-03 22:12:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dr4w6",
              "author": "NeverEnPassant",
              "text": "Any reason to use your GGUF over the ones Qwen [released](https://huggingface.co/Qwen/Qwen3-Coder-Next-GGUF)?",
              "score": 9,
              "created_utc": "2026-02-03 18:05:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d2p76",
              "author": "IceTrAiN",
              "text": "damn son, you fast.",
              "score": 10,
              "created_utc": "2026-02-03 16:12:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d83ml",
                  "author": "danielhanchen",
                  "text": ":)",
                  "score": 9,
                  "created_utc": "2026-02-03 16:37:43",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3d75qa",
              "author": "KittyPigeon",
              "text": "Q2_K_KL/IQ3_XXS loaded for me on LMStudio for 48 GB Mac Mini. Nice. Thank you.\n\nCould never get the non coder qwen next model to load on LMStudio without an error message.",
              "score": 3,
              "created_utc": "2026-02-03 16:33:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d8c3v",
                  "author": "danielhanchen",
                  "text": "Let me know how it goes! :)",
                  "score": 2,
                  "created_utc": "2026-02-03 16:38:47",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dcmt8",
              "author": "Achso998",
              "text": "Would you recommend iq3_xss or q3_k_xl?",
              "score": 2,
              "created_utc": "2026-02-03 16:58:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ewdh4",
              "author": "Danmoreng",
              "text": "updated my powershell run script based on your guide :) https://github.com/Danmoreng/local-qwen3-coder-env",
              "score": 1,
              "created_utc": "2026-02-03 21:15:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d5uyh",
              "author": "HarambeTenSei",
              "text": "no love for anything vllm based huh",
              "score": -3,
              "created_utc": "2026-02-03 16:27:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d893g",
                  "author": "danielhanchen",
                  "text": "Oh we have a section for vLLM / SGLang deployment for models as well on our guides - https://unsloth.ai/docs/basics/inference-and-deployment/vllm-guide and https://unsloth.ai/docs/basics/inference-and-deployment/sglang-guide",
                  "score": 9,
                  "created_utc": "2026-02-03 16:38:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d8cd5",
          "author": "palec911",
          "text": "How much am I lying to myself that it will work on my 16GB VRAM ?",
          "score": 21,
          "created_utc": "2026-02-03 16:38:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dadxn",
              "author": "Comrade_Vodkin",
              "text": "*me cries in 8gb vram*",
              "score": 11,
              "created_utc": "2026-02-03 16:48:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dkd4z",
                  "author": "pmttyji",
                  "text": "In past, I tried IQ4\\_XS(40GB file) of Qwen3-Next-80B-A3B. 8GB VRAM + 32GB RAM. It gave me 12 t/s before all the optimizations on llama.cpp side. I need to download new GGUF file to run the model with latest llama.cpp version. I was lazy to try that again.\n\nSo just download GGUF & go ahead. Or wait for couple of days to see t/s benchmarks in this sub to decide the quant.",
                  "score": 11,
                  "created_utc": "2026-02-03 17:34:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dg556",
              "author": "sine120",
              "text": "Qwen3-Codreapr-Next-REAP-GGUF-IQ1\\_XXXXS",
              "score": 9,
              "created_utc": "2026-02-03 17:14:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dna22",
              "author": "tmvr",
              "text": "Why wouldn't it? You just need enough system RAM to load the experts. Either all to get as much content as you can fit into the VRAM or some if you take some compromise in context size.",
              "score": 7,
              "created_utc": "2026-02-03 17:47:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ex4mx",
              "author": "Danmoreng",
              "text": "Depends on your RAM. I get ~21t/s with the Q4 (48GB in size) on my notebook with an AMD 9955HX3D, 64GB RAM and RTX 5080 16GB.",
              "score": 2,
              "created_utc": "2026-02-03 21:19:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3djavj",
              "author": "grannyte",
              "text": "How much ram? if you can move the expert to ram maybe?",
              "score": 1,
              "created_utc": "2026-02-03 17:29:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dkpo4",
              "author": "pmttyji",
              "text": "Hope you have more RAM. [Just try](https://www.reddit.com/r/LocalLLaMA/comments/1quvvtv/comment/o3dkd4z/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button).",
              "score": 1,
              "created_utc": "2026-02-03 17:35:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d6an5",
          "author": "Competitive-Prune349",
          "text": "80B and non-reasoning model ü§Ø",
          "score": 15,
          "created_utc": "2026-02-03 16:29:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3e458n",
              "author": "Middle_Bullfrog_6173",
              "text": "Just like the instruct model it's based on...",
              "score": 9,
              "created_utc": "2026-02-03 19:03:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3eb676",
                  "author": "Far-Low-4705",
                  "text": "ü§Ø",
                  "score": 3,
                  "created_utc": "2026-02-03 19:36:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e9wh8",
              "author": "Sensitive_Song4219",
              "text": "Qwen's non-reasoning models are sometimes preferable; Qwen3-30B-A3B-Instruct-2507 isn't much worse than its thinking equivalent and performs much faster overall due to shorter outputs.",
              "score": 7,
              "created_utc": "2026-02-03 19:30:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ebab8",
                  "author": "Far-Low-4705",
                  "text": "much worse at engineering/math and STEM though",
                  "score": 1,
                  "created_utc": "2026-02-03 19:37:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d2ysn",
          "author": "SlowFail2433",
          "text": "Very notable release if it performs well as it shows that gated deltanet can scale in performance",
          "score": 12,
          "created_utc": "2026-02-03 16:13:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3d6aqr",
          "author": "tarruda",
          "text": "I wonder if it is trained in  \"fill in the middle\" examples for editor auto completion. Could be a killer all around local LLM for both editor completion and agentic coding.",
          "score": 9,
          "created_utc": "2026-02-03 16:29:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3da3uh",
              "author": "MaxKruse96",
              "text": "[https://github.com/QwenLM/Qwen3-Coder?tab=readme-ov-file#fill-in-the-middle-with-qwen3-coder](https://github.com/QwenLM/Qwen3-Coder?tab=readme-ov-file#fill-in-the-middle-with-qwen3-coder)\n\nYes. FIM",
              "score": 15,
              "created_utc": "2026-02-03 16:46:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d3yac",
          "author": "dinerburgeryum",
          "text": "Holy shit amazing late Christmas present for ya boy!!!",
          "score": 7,
          "created_utc": "2026-02-03 16:18:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dchi3",
              "author": "archieve_",
              "text": "Chinese New Year gift actually üòÅ",
              "score": 13,
              "created_utc": "2026-02-03 16:57:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dd124",
                  "author": "dinerburgeryum",
                  "text": "Êñ∞Âπ¥Âø´‰πê!",
                  "score": 1,
                  "created_utc": "2026-02-03 17:00:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d17n5",
          "author": "westsunset",
          "text": "Have you tried it at all?",
          "score": 11,
          "created_utc": "2026-02-03 16:05:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d1sgl",
              "author": "danielhanchen",
              "text": "Yes a few hours ago! It's pretty good!",
              "score": 21,
              "created_utc": "2026-02-03 16:08:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3d4l9v",
                  "author": "spaceman_",
                  "text": "Would you say it outperforms existing models in the similar size space (mostly gpt-oss-120b) in either speed or quality?",
                  "score": 20,
                  "created_utc": "2026-02-03 16:21:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3dkap9",
                  "author": "Which_Slice1600",
                  "text": "Do you think it's good for something like claw? (As a smaller model with good agentic capacities)",
                  "score": 1,
                  "created_utc": "2026-02-03 17:34:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d57jv",
          "author": "sautdepage",
          "text": "Oh wow, can't wait to try this. Thanks for the FP8 unsloth!  \n  \nWith VLLM Qwen3-Next-Instruct-FP8 is a joy to use as it fits 96GB VRAM like a glove. The architecture means full context takes like 8GB of VRAM, prompt processing is off the charts, and while not perfect it already could hold through fairly long agentic coding runs.",
          "score": 11,
          "created_utc": "2026-02-03 16:24:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d8l3l",
              "author": "danielhanchen",
              "text": "Yes FP8 is marvelous! We also plan to make some NVFP4 ones as well!",
              "score": 12,
              "created_utc": "2026-02-03 16:39:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3dqj4u",
                  "author": "Kitchen-Year-8434",
                  "text": "Oh wow. You guys getting involved with the nvfp4 space would help those of us that splurged on blackwells feel like we might have actually made a _slightly_ less irresponsible decision. :D",
                  "score": 6,
                  "created_utc": "2026-02-03 18:02:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3dcvec",
                  "author": "OWilson90",
                  "text": "Using Nvidia model opt? That would be amazing!",
                  "score": 1,
                  "created_utc": "2026-02-03 16:59:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e2dqq",
              "author": "LegacyRemaster",
              "text": "is it fast? with llama.cpp only 34 tokens/sec on 96gb rtx 6000. CPU only 24... so yeah.. is it VLLM better?",
              "score": 3,
              "created_utc": "2026-02-03 18:55:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ec4aa",
                  "author": "Far-Low-4705",
                  "text": "damn, i get 35T/s on two old amd mi50's lol (thats at Q4 tho)\n\nllama.cpp definitely does not have a efficient implementation for qwen3 next atm lol",
                  "score": 3,
                  "created_utc": "2026-02-03 19:40:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3f35uf",
                  "author": "sautdepage",
                  "text": "Absolutely it rips! On RTX 6000 you get 80-120 toks/sec that holds well at long context and with concurrent requests. Insane prompt processing 6K-10K/sec - pasting a 15 pages doc to ask a summary is a 2 seconds thing.\n\nHere's my local vllm command which uses around 92 of 96GB\n\n    vllm serve Qwen/Qwen3-Next-80B-A3B-Instruct-FP8  \\\n    --port ${PORT} \\\n    --enable-chunked-prefill \\\n    --max-model-len 262144 \\\n    --max-num-seqs 4 \\\n    --max-num-batched-tokens 16384 \\\n    --tool-call-parser hermes \\\n    --chat-template-content-format string \\\n    --enable-auto-tool-choice \\\n    --disable-custom-all-reduce \\\n    --gpu-memory-utilization 0.95",
                  "score": 3,
                  "created_utc": "2026-02-03 21:47:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3eo3vr",
                  "author": "Nepherpitu",
                  "text": "4x3090 on VLLM runs at 130tps without flashinfer. Must be around 150-180 with it, will check tomorrow.",
                  "score": 1,
                  "created_utc": "2026-02-03 20:37:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d30b5",
          "author": "TomLucidor",
          "text": "SWE-Rebench or bust (or maybe LiveCodeBench/LiveBench just in case)",
          "score": 6,
          "created_utc": "2026-02-03 16:14:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d512r",
              "author": "ResidentPositive4122",
              "text": "In 1-2 months we'll have rebench results and see where it lands.",
              "score": 3,
              "created_utc": "2026-02-03 16:23:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dexud",
              "author": "nullmove",
              "text": "I predict that non-thinking mode wouldn't do particularly well against high level novel problems. But pairing it with a thinking model for plan mode might just be very interesting in practice.",
              "score": 2,
              "created_utc": "2026-02-03 17:09:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fqgbx",
                  "author": "TomLucidor",
                  "text": "The non-thinking model can engage in \"error driven development\" at least... agentically.",
                  "score": 1,
                  "created_utc": "2026-02-03 23:45:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d3luh",
          "author": "Few_Painter_5588",
          "text": "How's llamacpp performance? IIRC the original Qwen3 Next model had some support issues",
          "score": 4,
          "created_utc": "2026-02-03 16:16:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d7cxt",
              "author": "Daniel_H212",
              "text": "Pretty sure it's the exact same architecture. When team released the original early just so the architecture will be ready for use in the future and by now all the kinks have been ironed out.",
              "score": 8,
              "created_utc": "2026-02-03 16:34:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d8pd9",
              "author": "danielhanchen",
              "text": "The model is mostly ironed out by now - Son from HF also made some perf improvements!",
              "score": 6,
              "created_utc": "2026-02-03 16:40:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3da13z",
                  "author": "Few_Painter_5588",
                  "text": "Good stuff! Keep up the hard work!",
                  "score": 1,
                  "created_utc": "2026-02-03 16:46:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d40jj",
          "author": "fancyrocket",
          "text": "How well does the Q4_K_XL perform?",
          "score": 5,
          "created_utc": "2026-02-03 16:18:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e81fs",
          "author": "curiousFRA",
          "text": "I recommend to read their technical report [https://github.com/QwenLM/Qwen3-Coder/blob/main/qwen3\\_coder\\_next\\_tech\\_report.pdf](https://github.com/QwenLM/Qwen3-Coder/blob/main/qwen3_coder_next_tech_report.pdf)  \nEspecially how they construct training data. Very cool approach to mine issue-related PRs from github and construct executable environments that reflect real world bugfixing tasks.",
          "score": 5,
          "created_utc": "2026-02-03 19:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ei7ia",
          "author": "zoyer2",
          "text": "Finally a model that beats GPT-OSS-120B at my one-shot game tests by a pretty great margin. Using llama.cpp **Qwen3-Coder-Next-UD-Q4\\_K\\_XL.gguf.** Using 2x3090. Still agent use left to test.\n\nManages to one-shot without any fail so far some more advanced games. Advanced tower defense. Procedural sidescroller with dynamic weather. Advanced zelda game.\n\nhttps://preview.redd.it/mb7vf91w6chg1.png?width=1605&format=png&auto=webp&s=4a5d1f0c50e6b2e06b27d33edb383068a2d4e25f",
          "score": 4,
          "created_utc": "2026-02-03 20:09:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3eimt2",
              "author": "zoyer2",
              "text": "https://preview.redd.it/j2wvgk197chg1.png?width=1796&format=png&auto=webp&s=b0e91643b9d40c282289b3c816746a84d5c62513",
              "score": 1,
              "created_utc": "2026-02-03 20:11:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fasux",
                  "author": "Taeyangsin",
                  "text": "what are you using to generate that and test in that ui?",
                  "score": 1,
                  "created_utc": "2026-02-03 22:23:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3deu2t",
          "author": "sine120",
          "text": "The IQ4\\_XS quants of Next work fairly well in my 16/64GB system with 10-13 tkps.  I still have yet to run my tests on GLM-4.7-flash and now I have this as well.  My gaming PC is rapidly becoming a better coder than I am.  What's your guy's preferred local hosted CLI/ IDE platform?  Should I be downloading Claude Code even though I don't have a Claude subscription?",
          "score": 3,
          "created_utc": "2026-02-03 17:08:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dn6wg",
              "author": "pmttyji",
              "text": ">The IQ4\\_XS quants of Next work fairly well in my 16/64GB system with 10-13 tkps.\n\nWhat's your full llama.cpp command?\n\nI got 10+ t/s for Qwen3-Next-80B IQ4\\_XS with my 8GB VRAM+32GB RAM when llama-benched with no context. And it was with old GGUF & before all Qwen3-Next optimizations.",
              "score": 3,
              "created_utc": "2026-02-03 17:47:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3douaq",
                  "author": "sine120",
                  "text": "I'm an LM studio heathen for models I'm just playing around with.  I just offloaded layers and context until my GPU was full.  Q8 context, default template.",
                  "score": 2,
                  "created_utc": "2026-02-03 17:54:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ebfd3",
                  "author": "Orph3us42",
                  "text": "Are you using cpu-moe ?",
                  "score": 1,
                  "created_utc": "2026-02-03 19:37:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dkvms",
          "author": "sleepingsysadmin",
          "text": "Well after tinkering with fitting it to my system, I cant load it all to vram :(\n\nI get about 15TPS. \n\nKilo code straight up failed. I probably need to update it. Got qwen code updated trivially and coded with it.\n\nOh baby it's really strong. Much stronger coder than GPT 20b high. I'm not confident about if it's better or not compared to GPT 120b. \n\nAfter it completed, it got: \\[API Error: Error rendering prompt with jinja template: \"Unknown StringValue filter: safe\".\n\nUnsloth jinja wierdness? I didnt touch it.",
          "score": 3,
          "created_utc": "2026-02-03 17:36:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dx73t",
              "author": "thaatz",
              "text": "I had the same issue. I removed the check for `safe` in the jinja template on the line where it says `{%- set args_value = args_value if args_value is string else args_value | tojson | safe %}`. The idea is that since that line filters for \"safe\" but then doesn't know what to do with it, I just dont check for the value \"safe\".  \nSeems to be working in kilo code for now, hopefully there is a real template fix/update in the coming days.",
              "score": 3,
              "created_utc": "2026-02-03 18:32:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fo6lw",
                  "author": "IceTrAiN",
                  "text": "Thanks, this helped my LM Studio API respond to tool calls correctly. I had to remove it in two spots in the template.",
                  "score": 1,
                  "created_utc": "2026-02-03 23:32:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d29ci",
          "author": "MaxKruse96",
          "text": "brb creaming my pants",
          "score": 9,
          "created_utc": "2026-02-03 16:10:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d2hhd",
              "author": "danielhanchen",
              "text": "Haha",
              "score": 1,
              "created_utc": "2026-02-03 16:11:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d5ob5",
          "author": "Extra_Programmer788",
          "text": "Is the there any inference provides it for free to try?",
          "score": 2,
          "created_utc": "2026-02-03 16:26:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dobn0",
          "author": "Deep_Traffic_7873",
          "text": "Is this model better or worse than qwen 30b a3b ?¬†",
          "score": 2,
          "created_utc": "2026-02-03 17:52:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dszr7",
              "author": "TokenRingAI",
              "text": "Definitely better",
              "score": 5,
              "created_utc": "2026-02-03 18:13:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dw22b",
                  "author": "Deep_Traffic_7873",
                  "text": "Both are a3b i'd like to see also it in the benchmark",
                  "score": 0,
                  "created_utc": "2026-02-03 18:27:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e6g3y",
              "author": "sleepingsysadmin",
              "text": "For sure better. Not even a question to me.",
              "score": 5,
              "created_utc": "2026-02-03 19:14:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3edi1z",
          "author": "sagiroth",
          "text": "So wait can I run Q3 with 8vram and 32gm system ram ?",
          "score": 2,
          "created_utc": "2026-02-03 19:47:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f2hrf",
          "author": "7h3_50urc3",
          "text": "Tried it with opencode and when writing files it always fails with: Error message: JSON Parse error: Unrecognized token '/'\\]\n\nDoesn't matter Q4 or Q8, unsloth or qwen gguf.",
          "score": 2,
          "created_utc": "2026-02-03 21:43:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fj4hl",
              "author": "7h3_50urc3",
              "text": "Seems to be a bug in llama.cpp so never mind.",
              "score": 1,
              "created_utc": "2026-02-03 23:05:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3die84",
          "author": "nunodonato",
          "text": "Help me out guys, if I want to run the Q4 with 256k context, how much VRAM are we talking about?",
          "score": 2,
          "created_utc": "2026-02-03 17:25:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dd2jl",
          "author": "iAndy_HD3",
          "text": "Us 16vram are so left out of everything cool",
          "score": 1,
          "created_utc": "2026-02-03 17:00:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3df87c",
          "author": "gamblingapocalypse",
          "text": "Oooooooo :)",
          "score": 1,
          "created_utc": "2026-02-03 17:10:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3et0bn",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 0,
          "created_utc": "2026-02-03 21:00:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}