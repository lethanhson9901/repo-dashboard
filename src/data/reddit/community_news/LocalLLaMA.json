{
  "metadata": {
    "last_updated": "2026-01-10 02:26:44",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 41,
    "total_comments": 1069,
    "file_size_bytes": 1204892
  },
  "items": [
    {
      "id": "1q7d8bj",
      "title": "Jensen Huang saying \"AI\" 121 times during the NVIDIA CES keynote - cut with one prompt",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/hein55gpx4cg1",
      "author": "Prior-Arm-6705",
      "created_utc": "2026-01-08 14:29:47",
      "score": 848,
      "num_comments": 139,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial | Guide",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nygb8aw",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-08 19:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyejqtm",
          "author": "YearZero",
          "text": "Honestly that's probably a great summary of the keynote. He should've just done exactly that and it wouldn't change anything.",
          "score": 167,
          "created_utc": "2026-01-08 14:33:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfrjx1",
              "author": "MrWeirdoFace",
              "text": "I vote OP renames the video to Nvidia Keynote Summary.",
              "score": 42,
              "created_utc": "2026-01-08 17:50:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyiurdw",
                  "author": "Prior-Arm-6705",
                  "text": "This title is definitely better though lol   \nCan't edit Reddit titles unfortunately.",
                  "score": 6,
                  "created_utc": "2026-01-09 02:35:04",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyj31pj",
              "author": "ClimateBoss",
              "text": "AI AI AI AI AI AI ... make line go up!",
              "score": 1,
              "created_utc": "2026-01-09 03:19:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyf9wy6",
          "author": "DriveSolid7073",
          "text": "\"All local, no cloud.\"   \nopen video  \n\\>claude opus 4.5  \nI couldn't get Dive to work with my Koboldcpp, and the functions aren't being called.",
          "score": 38,
          "created_utc": "2026-01-08 16:33:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyipg6m",
              "author": "Prior-Arm-6705",
              "text": "To clarify - \"all local\" refers to the local MCPs and tools like ffmpeg, not the LLM or the demo video.",
              "score": 1,
              "created_utc": "2026-01-09 02:06:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyiq196",
                  "author": "DriveSolid7073",
                  "text": "I already understood, but it would be nice to see the full locally use. Although I think ollama can handle it.",
                  "score": 2,
                  "created_utc": "2026-01-09 02:09:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyekf0a",
          "author": "LambdaHominem",
          "text": "gamers nexus would be proud\n\nhttps://youtu.be/-qbylbEek-M",
          "score": 60,
          "created_utc": "2026-01-08 14:36:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyepodd",
          "author": "LinkSea8324",
          "text": "The fuck is that latex-leather jacket",
          "score": 55,
          "created_utc": "2026-01-08 15:02:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyesltq",
              "author": "Acceptable_Piano4809",
              "text": "Heâ€™s been wearing that for years.   Itâ€™s like 5 figures.   Michael Jacksons was better!",
              "score": 26,
              "created_utc": "2026-01-08 15:16:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nygsk1b",
                  "author": "Not_your_guy_buddy42",
                  "text": "Incidentally there's also supercuts of all the nonverbal MJ sounds in his songs  \nEdit: Shamon-a! Hee-hee. Chickachuwow",
                  "score": 7,
                  "created_utc": "2026-01-08 20:32:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyf917k",
              "author": "Thatisverytrue54321",
              "text": "Heâ€™s trying to be a Steve Jobs. Thatâ€™s *his* turtleneck.",
              "score": 25,
              "created_utc": "2026-01-08 16:30:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyfd4u5",
                  "author": "alphapussycat",
                  "text": "This. A lot of these people, like Steve Jobs, want to stick out and have some signature look.",
                  "score": 17,
                  "created_utc": "2026-01-08 16:47:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyetm0b",
              "author": "zzozozoz",
              "text": "Aye Eye",
              "score": 7,
              "created_utc": "2026-01-08 15:20:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyf3h00",
              "author": "TanguayX",
              "text": "Whoever is telling him that that looks cool does not have his best interests in mind.",
              "score": 12,
              "created_utc": "2026-01-08 16:05:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyfrod8",
                  "author": "GoranjeWasHere",
                  "text": "I don't think dude who is runnigng 4 trylion $ company will be caring about his style and outfit. He can wear literally garbage bag and people will say he looks cool.\n\nThat's what people don't get about fashion. Fashion doesn't make you better looking it only shows everyone who are you. That's why when you wear suit without actually being CEO you look stupid.",
                  "score": 8,
                  "created_utc": "2026-01-08 17:51:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyfaoj8",
                  "author": "Smile_Clown",
                  "text": "It's weird to me that redditors routinely call out people for making light of others choices, but when it comes to someone we hate (apparently) it's all ok?\n\nHe likes it, that is all that matters.\n\nIf you disagree that means anything you like is up for debate also and others opinions should be considered by you as some sort of barometer you make choices based on.\n\nI mean...",
                  "score": 5,
                  "created_utc": "2026-01-08 16:37:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyekw78",
          "author": "FastDecode1",
          "text": "Reminds me of the Xbox One reveal: https://www.youtube.com/watch?v=KbWgUO-Rqcw",
          "score": 28,
          "created_utc": "2026-01-08 14:38:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyenr5m",
          "author": "International-Try467",
          "text": "This man is the reason why everything is so expensive",
          "score": 77,
          "created_utc": "2026-01-08 14:53:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyetj3o",
              "author": "JuliusCeaserBoneHead",
              "text": "Heâ€™s laughing all the way to the bankÂ ",
              "score": 35,
              "created_utc": "2026-01-08 15:20:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyf2x94",
                  "author": "sourceholder",
                  "text": "He's better capitalized than most banks, or nations for that matter...",
                  "score": 20,
                  "created_utc": "2026-01-08 16:02:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nygznjg",
                  "author": "MoffKalast",
                  "text": "He's also laughing at the bank, and on the way home too.",
                  "score": 1,
                  "created_utc": "2026-01-08 21:03:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nylb67q",
                  "author": "-TRlNlTY-",
                  "text": "The bank is laughing all the way to him",
                  "score": 1,
                  "created_utc": "2026-01-09 13:23:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyfnqmk",
              "author": "20ol",
              "text": "So it's not the fed printing money, 40 trillion debt, and illegal tarrifs? \n\nDamnit, I knew I should have researched here on local llama on why everything is getting expensive.",
              "score": 9,
              "created_utc": "2026-01-08 17:34:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyhklh9",
                  "author": "Jonodonozym",
                  "text": "This is r/LocalLLaMA, we eat computer parts instead of bread",
                  "score": 3,
                  "created_utc": "2026-01-08 22:36:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyfuc6y",
                  "author": "nonaveris",
                  "text": "Even worse, people marking to unicorns for gpu and memory prices.",
                  "score": 1,
                  "created_utc": "2026-01-08 18:02:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nykfk5x",
                  "author": "nenulenu",
                  "text": "Dude. Debt has very little to do with prices. The other two, yes.",
                  "score": 1,
                  "created_utc": "2026-01-09 09:20:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyf2635",
              "author": "Solaranvr",
              "text": "Can you really say the shovel merchant (one among two) is the reason iron got expensive during the gold rush?",
              "score": 14,
              "created_utc": "2026-01-08 15:59:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyf2sgv",
                  "author": "International-Try467",
                  "text": "No because the shovel merchant weren't friends with the mayor who was also friends with the blacksmith which they kept investing in each other over and over again",
                  "score": 22,
                  "created_utc": "2026-01-08 16:02:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyg7klj",
                  "author": "RealSataan",
                  "text": "If all of the industrial might were concentrated in getting iron for the gold rush, Yes.",
                  "score": 1,
                  "created_utc": "2026-01-08 18:59:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyfbq8n",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -8,
                  "created_utc": "2026-01-08 16:41:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyewdgw",
              "author": "simracerman",
              "text": "Partly true. He is enabled by other people who benefit from his BS. Regardless of your country, those people you can elect to rule and govern on what makes things around you cheap or expensive.\n\nP.S. By â€œyouâ€, I mean everyone reading this with the power to vote.",
              "score": -6,
              "created_utc": "2026-01-08 15:33:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyf9znp",
                  "author": "Smile_Clown",
                  "text": "Your statement is quite ridiculous. \n\nBut I'll start with... good luck with that mindset.  It will follow you forever and never come to anything.  \n\nSelf created stress is not healthy.\n\nWhat I mean by that is your random comments on reddit are not going to make the masses vote your way and even if they did, controlling means of production is already something proven to be a shitty idea.\n\nAs far as setting pricing, that is virtually the same thing. Innovation does not come from government regulation and taxation. \n\nBut the bigger issue is need, real need.\n\nNone of the things Jenson is involved with affect your everyday survival.  You do not need 128gb of ram, a 5090 or a subscription to an AI service to thrive or live a happy life.\n\nSo, no matter who you want elected, there is absolutely zero chance that all the shiny tech will get price regulated.  it's not how it works.\n\nJust for giggles though, do you know how much a chip fab costs? (of course you don't, you simply think everyone is overcharging...)\n\nOn to the next:\n\n>He is enabled by other people who benefit from his BS.\n\nWhat BS exactly?  NVidia puts out the best hardware for the tech industry period. What exactly is bullshit? BTW if it wasn't NVidia it would be someone else. There is no \"enabling\" going on here. The products are viable, valuable and are used everywhere.\n\nThat all said, the original poster was right, not \"partly\" (which btw you did not actually qualify) he IS the reason everything (tech) is so expensive. He facilitated AI being implemented everywhere and that has caused manufacturers to pivot to non consumer tech.\n\nYou are a dufus of the highest order.",
                  "score": -2,
                  "created_utc": "2026-01-08 16:34:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nygf0cq",
          "author": "r0ckl0bsta",
          "text": "Old McJensen's server farm.\nA-I-A-I-O ðŸŽ¶",
          "score": 16,
          "created_utc": "2026-01-08 19:31:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg0uk3",
          "author": "Sea_Succotash3634",
          "text": "\"Consumer\" Electronic Show",
          "score": 8,
          "created_utc": "2026-01-08 18:30:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygtg3m",
          "author": "Freonr2",
          "text": "\"Consumer\" Electronics Show, showing billion dollar datacenter configs.",
          "score": 8,
          "created_utc": "2026-01-08 20:36:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyeqoex",
          "author": "anon235340346823",
          "text": "0:44 \"hey I have AI\"",
          "score": 14,
          "created_utc": "2026-01-08 15:07:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfjql1",
          "author": "budz",
          "text": "dope, I made a script that does this , back in October      [https://imgur.com/a/h0vc2f6](https://imgur.com/a/h0vc2f6)",
          "score": 4,
          "created_utc": "2026-01-08 17:16:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyitlmt",
              "author": "Prior-Arm-6705",
              "text": "Nice, what was your approach?",
              "score": 1,
              "created_utc": "2026-01-09 02:28:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyfa965",
          "author": "MMAgeezer",
          "text": "Appreciate you sharing the details of how you did this. One small thing though, it includes multiple clips of a narrator (i.e. not Jensen, as instructed) saying \"AI\" too.",
          "score": 4,
          "created_utc": "2026-01-08 16:35:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyivxxq",
              "author": "Prior-Arm-6705",
              "text": "Good catch - YouTube's auto-generated subtitles don't distinguish speakers, so the narrator clips got mixed in. Would need a speaker diarization step to filter those out properly.   \nI have another research project that involves voiceprints, but it's very complex to operate.",
              "score": 1,
              "created_utc": "2026-01-09 02:41:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyh9hl9",
          "author": "deltamoney",
          "text": "Did you use AI to find all occurrence of AI?",
          "score": 4,
          "created_utc": "2026-01-08 21:46:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyiw580",
              "author": "Prior-Arm-6705",
              "text": "Yes. AI finding AI felt appropriate. lol",
              "score": 6,
              "created_utc": "2026-01-09 02:42:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyixyrj",
                  "author": "deltamoney",
                  "text": "Nicely done ðŸ˜",
                  "score": 2,
                  "created_utc": "2026-01-09 02:51:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyfp4wj",
          "author": "positivcheg",
          "text": "Nah. AMD wins, AMD mentioned AI 299 times. Big win for AMD.\n\nSadly it was just 1 more to a nice 300.",
          "score": 5,
          "created_utc": "2026-01-08 17:40:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyervtk",
          "author": "GoranjeWasHere",
          "text": "He's literally the only one that can say as much AI as he wants.\n\nHe literally build his whole company on AI promise before everyone outside of research circle even knew what was AI and his hardware innovated so much that AI finally became a thing.\n\nOutside of Nvidia only Tesla I think and few other small companies can shout AI without sounding like a fool. Tesla was also super early in it and it got to the point where they were building their own chips just not to pay Nvidia tax for AI before AI even became investor bait.",
          "score": 11,
          "created_utc": "2026-01-08 15:12:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyf2ohr",
              "author": "RealSataan",
              "text": "Count Google also in it",
              "score": 13,
              "created_utc": "2026-01-08 16:01:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyfa2ah",
                  "author": "redblobgames",
                  "text": "Google's been in this game for a long time. \"Artificial intelligence would be the ultimate version of Google.\" â€”Larry Page, Google CEO in *the year 2000*. [[source](https://www.azquotes.com/quote/917415)] And it wasn't just talk. They were building large language models internally since then, and using them as part of the search engine.",
                  "score": 8,
                  "created_utc": "2026-01-08 16:34:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nykpkoz",
                  "author": "lami_kaayo",
                  "text": "\"attention is all you need\"",
                  "score": 2,
                  "created_utc": "2026-01-09 10:50:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyfr0o4",
                  "author": "GoranjeWasHere",
                  "text": "Why google for sure was doing the research they didn't really push it like nvidia or tesla. It wasn't until OpenAI came to be and released chatgpt when google felt heat under their ass. Right now google is on the top with rest but for a long while they were behind a lot.",
                  "score": 0,
                  "created_utc": "2026-01-08 17:48:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyfsuoj",
              "author": "FrostieDog",
              "text": "Google built and was deploying TPUs the same year Nvidia started talking about GPUs being made for deep learning",
              "score": 4,
              "created_utc": "2026-01-08 17:56:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyh6ebt",
              "author": "ECrispy",
              "text": "Pretty much all of current AI (i.e. llm) wouldnt exist without Google. Nvidia is rich because of their hardware, and the fact that there was no other real API besides CUDA, which has become the standard.\n\nDon't mention Tesla. They are irrelevant. Zero actual research contributions, their use of AI in self driving is a joke. Just because they are rich due to inflated stock price and ordered a bunch of H100's and made some vague blog posts about their own chips doesn't make them a player. Tesla has zero impact on AI.",
              "score": 5,
              "created_utc": "2026-01-08 21:33:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyfb3j3",
              "author": "7640LPS",
              "text": "That is not true at all. \n\nMany companies were investing heavily in AI before LLMs even existed. \n\nGoogle, AWS, IBM, Meta, etc.",
              "score": 7,
              "created_utc": "2026-01-08 16:39:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyfr6zx",
                  "author": "GoranjeWasHere",
                  "text": "They were investing but not really pushing it hard. Like when OpenAI came to be and relased their models they were just better than Google. And it is openAI that shocked the world not google.",
                  "score": 1,
                  "created_utc": "2026-01-08 17:49:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyh9ral",
              "author": "kritzikratzi",
              "text": "are you trying to gatekeep ai? ðŸ˜‚ here is the history of deep learning from 1920 to today. \nhttps://en.wikipedia.org/wiki/Deep_learning#History",
              "score": 2,
              "created_utc": "2026-01-08 21:47:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nygayob",
              "author": "mycall",
              "text": "You would say AI 121 times too if it made you a trillion dollars.",
              "score": 1,
              "created_utc": "2026-01-08 19:14:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyfskqo",
          "author": "SmegPoison",
          "text": "Old MacDonald had a farm, ai-ai-oh!",
          "score": 2,
          "created_utc": "2026-01-08 17:55:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfuof4",
          "author": "Caladan23",
          "text": "Duh surprise... it's an AI hardware company nowadays. Nvidia as a gaming company wasn't worth even 1/10th.",
          "score": 2,
          "created_utc": "2026-01-08 18:04:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhhxe1",
          "author": "JealousAmoeba",
          "text": "I admit Iâ€™m surprised that worked. Great demo for MCP.",
          "score": 2,
          "created_utc": "2026-01-08 22:24:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyiv1wf",
              "author": "Prior-Arm-6705",
              "text": "Thanks! MCP makes chaining tools pretty seamless.",
              "score": 1,
              "created_utc": "2026-01-09 02:36:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyjj9w8",
          "author": "thermocoffee",
          "text": "this is truly amazing. I'm playing with Dive now. I love it!",
          "score": 2,
          "created_utc": "2026-01-09 04:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyet9oy",
          "author": "Agile_Date6729",
          "text": "someone should make a remix of it -would be a banger",
          "score": 3,
          "created_utc": "2026-01-08 15:19:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyffw6s",
          "author": "WavierLays",
          "text": "I mean this is like making a supercut of every time KFC's CEO says \"chicken\"",
          "score": 3,
          "created_utc": "2026-01-08 16:59:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfs4b0",
              "author": "MrWeirdoFace",
              "text": "A few years ago this was a gaming hardware company, so not quite, but I get the sentiment.",
              "score": 1,
              "created_utc": "2026-01-08 17:53:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyezpx4",
          "author": "XiRw",
          "text": "Willing to bet anything the future of gaming (based on prices going up and shortages) will be server based subscriptions like everything else out there. Youâ€™ll own nothing and be happy.",
          "score": 3,
          "created_utc": "2026-01-08 15:48:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyf1ngi",
              "author": "TwistStrict9811",
              "text": "That assumes everything centralizes forever. Open-source AI and hardware keep pushing more capability back onto local machines, not locking it all into servers.",
              "score": 5,
              "created_utc": "2026-01-08 15:57:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyf81u5",
                  "author": "Mediocre-Method782",
                  "text": "\"Solution\": mediate local computation too heavily to be used productively. Google, Samsung, and sama's capture of the world's semiconductor production capacity are already on it.",
                  "score": 2,
                  "created_utc": "2026-01-08 16:25:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyfa87l",
          "author": "dasjati",
          "text": "Someone should put a nice beat under this!",
          "score": 1,
          "created_utc": "2026-01-08 16:35:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfu4de",
          "author": "nonaveris",
          "text": "ai ai ai Cthulhu fthangh",
          "score": 1,
          "created_utc": "2026-01-08 18:01:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfx33c",
          "author": "Amazing_Athlete_2265",
          "text": "AI? AI!",
          "score": 1,
          "created_utc": "2026-01-08 18:14:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfzi09",
          "author": "HerrGronbar",
          "text": "so without AI how long it was?",
          "score": 1,
          "created_utc": "2026-01-08 18:24:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg250o",
          "author": "pierrenoir2017",
          "text": "A I Caramba",
          "score": 1,
          "created_utc": "2026-01-08 18:36:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg6x39",
          "author": "spyda_mayn",
          "text": "holy crap now compare this to the other 3 companies AMD and Intel,",
          "score": 1,
          "created_utc": "2026-01-08 18:56:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg9j98",
          "author": "dmshd",
          "text": "AI",
          "score": 1,
          "created_utc": "2026-01-08 19:07:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg9zah",
          "author": "gynnihanssen",
          "text": "upfront sorry if itâ€˜s an uninformed question but which local llm setup do you use for dive? or is it just dive and the mcps?",
          "score": 1,
          "created_utc": "2026-01-08 19:09:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyivkns",
              "author": "Prior-Arm-6705",
              "text": "Dive supports multiple LLM backends - Claude, OpenAI, Gemini, Ollama, and any OpenAI-compatible API. For this video I used Claude, but you can run it fully local with Ollama if you prefer.   \nThe MCPs handle the actual work (downloading, cutting), the LLM just orchestrates.\n\nYou can also ask Dive install both MCP for you. Since It has basic tools call embedded for MCP install.",
              "score": 1,
              "created_utc": "2026-01-09 02:39:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyga503",
          "author": "terem13",
          "text": "He is a showman and salesman, nothing more. Every CEO nowadays is.\n\nLLM is the correct name, but its boring and does not sell.",
          "score": 1,
          "created_utc": "2026-01-08 19:10:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygb24s",
          "author": "glanni_glaepur",
          "text": "Steve Ballmer: \"Developers, developers, developers!\"\n\nJensen Huang: \"AI, AI, AI!\"",
          "score": 1,
          "created_utc": "2026-01-08 19:14:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyggf7b",
          "author": "T_UMP",
          "text": "https://preview.redd.it/b5248cvjh6cg1.png?width=420&format=png&auto=webp&s=110a55c4e413a8f2f21dbba1bb9586f7ee118d4b",
          "score": 1,
          "created_utc": "2026-01-08 19:38:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygh2ov",
          "author": "CV514",
          "text": "Guys I think he's talking about AI at some point.",
          "score": 1,
          "created_utc": "2026-01-08 19:41:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyghd5y",
          "author": "T_UMP",
          "text": "When you realize Jensen Huang will actually see this somehow...and will think that he could have fitted some more AI's in there.",
          "score": 1,
          "created_utc": "2026-01-08 19:42:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygmjyz",
          "author": "thecalmgreen",
          "text": "This man likes AI",
          "score": 1,
          "created_utc": "2026-01-08 20:05:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyh0nwk",
          "author": "catalystignition",
          "text": "I only need to say it once; fuck AI.",
          "score": 1,
          "created_utc": "2026-01-08 21:08:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhe4o4",
          "author": "Al_Onestone",
          "text": "i trust nobody with such a shiny jacket.",
          "score": 1,
          "created_utc": "2026-01-08 22:06:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhlz8z",
          "author": "Same-Platform-9793",
          "text": "With the snake leather jacket",
          "score": 1,
          "created_utc": "2026-01-08 22:42:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhqyay",
          "author": "Upset-Motor-2602",
          "text": "Old McDonald had a farm -ai-ai-ohhhhh!",
          "score": 1,
          "created_utc": "2026-01-08 23:07:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyi1gu3",
          "author": "Saerain",
          "text": "ðŸŽ¶ Jensen Huang had a farm",
          "score": 1,
          "created_utc": "2026-01-09 00:00:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyi2to4",
          "author": "xorgol",
          "text": "I don't understand what the yt-dlp-mcp did in this case, was it just calling yt-dlp to download the video and subtitles?",
          "score": 1,
          "created_utc": "2026-01-09 00:07:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyjow59",
              "author": "Prior-Arm-6705",
              "text": "Yes, it's basically a wrapper around yt-dlp. The MCP just gives the LLM a clean interface so it doesn't have to guess CLI flags. Without it, the LLM would need many more bash calls and trial-and-error.  \nHonestly the LLM could write the whole pipeline itself given enough attempts. MCPs just skip the reinventing-the-wheel phase. Fewer tokens, fewer hallucinated flags.",
              "score": 3,
              "created_utc": "2026-01-09 05:33:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyij0a6",
          "author": "FloranceMeCheneCoder",
          "text": "Feeling like a MLM everyday",
          "score": 1,
          "created_utc": "2026-01-09 01:32:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyimx9x",
          "author": "Kooky-Somewhere-2883",
          "text": "AI is so back  baby!!",
          "score": 1,
          "created_utc": "2026-01-09 01:53:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyionqm",
          "author": "Bimbam_tm",
          "text": "I preferred his 'older quirky tech guy' jackets to this \"Digital Tech Bro Pimp\" phase :(",
          "score": 1,
          "created_utc": "2026-01-09 02:02:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyji110",
          "author": "ConnectorMadness",
          "text": "We are in a desperate need of a new pronoun for AI. The amount of time I heard the word 'AI' in this years' CES is mind bogglingðŸ˜µâ€ðŸ’«",
          "score": 1,
          "created_utc": "2026-01-09 04:47:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyk0rux",
          "author": "salary_pending",
          "text": "soooo funny ðŸ˜†",
          "score": 1,
          "created_utc": "2026-01-09 07:07:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyk5tg1",
          "author": "hsien88",
          "text": "An AI sub hating on AI just because they want cheaper GPUs.",
          "score": 1,
          "created_utc": "2026-01-09 07:51:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyklye5",
          "author": "PhotoRepair",
          "text": "\".....had a farm, A I A I Ooooo\"",
          "score": 1,
          "created_utc": "2026-01-09 10:18:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykw7pw",
          "author": "wasdxqwerty",
          "text": "ai ai ai ai-m your little butterfly",
          "score": 1,
          "created_utc": "2026-01-09 11:45:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylitc6",
          "author": "SigurdZS",
          "text": "https://www.youtube.com/watch?v=Vhh_GeBPOhs",
          "score": 1,
          "created_utc": "2026-01-09 14:05:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyny571",
          "author": "rolyantrauts",
          "text": "Old macdonald had a farm, AI, AI, Oh!",
          "score": 1,
          "created_utc": "2026-01-09 20:45:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyolqdr",
          "author": "warnerbell",
          "text": "You should have put a beat to it..lol",
          "score": 1,
          "created_utc": "2026-01-09 22:35:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyowwfu",
          "author": "One-Employment3759",
          "text": "Hey it's the leather jacket bubble boy.",
          "score": 1,
          "created_utc": "2026-01-09 23:33:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf3l5f",
          "author": "TanguayX",
          "text": "Time for a 'dumb jacket intervention' with this guy",
          "score": 0,
          "created_utc": "2026-01-08 16:05:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyf7dwf",
              "author": "Mediocre-Method782",
              "text": ">dumb jacket\n\nThis is his new name",
              "score": -3,
              "created_utc": "2026-01-08 16:22:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyeldow",
          "author": "Noiselexer",
          "text": "Tools did exactly that before Ai.",
          "score": 0,
          "created_utc": "2026-01-08 14:41:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfl1yb",
              "author": "20ol",
              "text": "Durrrr. And before automated factories people did the same thing by hand.",
              "score": 3,
              "created_utc": "2026-01-08 17:22:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyf4zp4",
          "author": "diond09",
          "text": "'Old MacDonald had a farm.......'",
          "score": 0,
          "created_utc": "2026-01-08 16:12:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf6iyd",
          "author": "Amazing-Canary2574",
          "text": "Grok says hi",
          "score": 0,
          "created_utc": "2026-01-08 16:19:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf7gjn",
          "author": "Remarkable_Pound_375",
          "text": "For SEO :))",
          "score": 0,
          "created_utc": "2026-01-08 16:23:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf8sbe",
          "author": "RedTheRobot",
          "text": "Some company needs to attack the gaming video card market. Make cards with a small amount of VRAM. Sell the shit out of them. Use that capital to make better cards that can then be shifted to AI. Install more VRAM on the card now you are making AI cards and billions. I really do think now is a perfect time for a new company to enter the market and take over the space Nivida seems to be pushing to the side.",
          "score": 0,
          "created_utc": "2026-01-08 16:28:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfpuym",
              "author": "Bananadite",
              "text": "If you could make VRAM you wouldn't be selling to small consumers. You would make much more selling to companies.",
              "score": 2,
              "created_utc": "2026-01-08 17:43:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nygbqkh",
              "author": "ResidentPositive4122",
              "text": "If crucial decided it's just not worth dealing with the retail market, why would a not-yet-existing, new company have much success there? Nvda for gaming isn't going anywhere. During the crypto push, gpus got scalped to hell and back, and they saw that people would still buy them. Haven't gone down in price since.",
              "score": 1,
              "created_utc": "2026-01-08 19:17:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyf8zos",
          "author": "silenceimpaired",
          "text": "Soâ€¦ Iâ€™m confusedâ€¦ what was the focus of his keynote?",
          "score": 0,
          "created_utc": "2026-01-08 16:29:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfcowj",
              "author": "zipzag",
              "text": "To introduce Vera, which is not a CES fit. The truthful Nvidia keynote would be him announcing the continued de-prioritization of consumer graphics cards by his company.",
              "score": 5,
              "created_utc": "2026-01-08 16:45:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyfeo14",
                  "author": "silenceimpaired",
                  "text": "Yeah, I should have added /s to my post :) AI means we donâ€™t care about the gamer or even localllama people",
                  "score": 1,
                  "created_utc": "2026-01-08 16:54:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyfhjvr",
          "author": "Danno1850",
          "text": "This I like being surprised a farmer talks about crops all the time. Literally what nvidia is built on, yeah heâ€™s gonna talk about it a lot.",
          "score": 0,
          "created_utc": "2026-01-08 17:06:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfjhss",
          "author": "theDigitalm0nk",
          "text": "AMDs CES keynote mentioned AI 134 times ( Word count provided by YT's AI. )",
          "score": 0,
          "created_utc": "2026-01-08 17:15:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfjj5n",
          "author": "SniffsU",
          "text": "https://i.imgflip.com/3gklu2.jpg",
          "score": 0,
          "created_utc": "2026-01-08 17:15:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygmujp",
          "author": "h0tsince84",
          "text": "br**AI**nwash",
          "score": -1,
          "created_utc": "2026-01-08 20:06:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf5dh0",
          "author": "Diecron",
          "text": "Is this man artificially intelligent?",
          "score": -2,
          "created_utc": "2026-01-08 16:13:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyepilr",
          "author": "Powerful_Pirate_9617",
          "text": "Cut with ai",
          "score": -2,
          "created_utc": "2026-01-08 15:01:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzwlie",
      "title": "[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: Itâ€™s running a raw Llama-7B instance with a 2048 token window.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1pzwlie",
      "author": "simar-dmg",
      "created_utc": "2025-12-30 23:03:12",
      "score": 729,
      "num_comments": 108,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwv1207",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-31 04:35:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtka7k",
          "author": "staring_at_keyboard",
          "text": "Is it common for system prompts to include environment variables such as model type? If not, how else would the LLM be aware of such a system configuration? Seems to me that such a result could also be a hallucination.",
          "score": 302,
          "created_utc": "2025-12-30 23:26:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtqwtd",
              "author": "mrjackspade",
              "text": "1. No\n2. It most likely wouldn't\n3. I'd put money on it.\n\nStill cool though",
              "score": 185,
              "created_utc": "2025-12-31 00:03:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvrpvn",
                  "author": "DistanceSolar1449",
                  "text": "Yeah, the only thing that can be concluded from this conversation is that it's *probably* a Llama model. I don't think the closed source or chinese models self-identify as Llama. \n\nThe rest of the info is hallucinated.",
                  "score": 33,
                  "created_utc": "2025-12-31 08:09:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwtvt9t",
                  "author": "lookwatchlistenplay",
                  "text": "Fuck em up.",
                  "score": 18,
                  "created_utc": "2025-12-31 00:29:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwulurj",
              "author": "Double_Cause4609",
              "text": "I guess to verify one could try and get the same information out of Llama 2 7B, Llama 3.1 8B, and a few other models from inbetween (maybe Mistral 7B?) for a control study.\n\nIt gets tricky to say what model is what, but if the Llama models specifically output the same information as extracted here it's plausible it's true.\n\nIMO it's more likely a hallucination, though the point it was a weak, potentially old, and locally run model is pretty valid.",
              "score": 11,
              "created_utc": "2025-12-31 02:59:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvcmcd",
                  "author": "staring_at_keyboard",
                  "text": "Itâ€™s an interesting research question: which, if any, models can self-identity.",
                  "score": 7,
                  "created_utc": "2025-12-31 05:58:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwtwhwo",
              "author": "yahluc",
              "text": "It's very likely that this bot was vibe coded and the person who made it didn't give it a second thought.",
              "score": 36,
              "created_utc": "2025-12-31 00:33:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww0nmq",
                  "author": "zitr0y",
                  "text": "The model would not have access to the file system or command line to access the environment variables or context length parameter",
                  "score": 14,
                  "created_utc": "2025-12-31 09:34:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx1vmy8",
              "author": "BodybuilderTrue1761",
              "text": "Def setup through Claude code.. running thru llama onto sc which u can do on the web. U r talking to the scammers Claude code setup which is orchestrating the llama",
              "score": 3,
              "created_utc": "2026-01-01 08:12:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx391p6",
              "author": "artisticMink",
              "text": "They don't. OP is deluding themselves into taking the conversation with a LLM for face value.",
              "score": 2,
              "created_utc": "2026-01-01 15:25:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxjvg17",
              "author": "Novel-Mechanic3448",
              "text": "No, OP is clueless lol",
              "score": 1,
              "created_utc": "2026-01-04 02:07:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwu0p2w",
              "author": "mguinhos",
              "text": "He said he tricked the pipeline that parses the JSON from the model.",
              "score": -7,
              "created_utc": "2025-12-31 00:56:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvkjoh",
                  "author": "the320x200",
                  "text": "What does that even mean? Models don't get any JSON unless the person writing the bot was feeding it JSON as part of their prompting, which would be a very weird thing to do in this context.",
                  "score": 7,
                  "created_utc": "2025-12-31 07:03:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwuehmp",
                  "author": "lookwatchlistenplay",
                  "text": "Real hacking only occurs in JSON format. .exes are safe to click on because no one clicks on .exes anymore. IOW, Windows is the new Linux.\n\n*This is not in fact real security advice.",
                  "score": 4,
                  "created_utc": "2025-12-31 02:16:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwulj2i",
          "author": "kzgrey",
          "text": "The only thing you can say for certain is that you stumbled upon a bot powered by an LLM.  Every other piece of information it has provided you is nonsensical hallucinating.\n\nUpdate: another thought about this: it's actually a bit dangerous that people think that they can rely on an LLM for this type of information.  It's resulted in students getting F's when the teacher believes that they can just ask ChatGPT if they wrote something and it happens to respond with \"Yes\".  Lots of students are being accused of cheating with the only evidence being a paid service that performs \"analysis\" to determine whether AI wrote something.  Frankly, I am surprised there haven't been major lawsuits from this.",
          "score": 123,
          "created_utc": "2025-12-31 02:57:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv47i7",
              "author": "ab2377",
              "text": "yea, this post doesn't make much sense.",
              "score": 27,
              "created_utc": "2025-12-31 04:56:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvixab",
                  "author": "ShengrenR",
                  "text": "Folks using llms to make them think they know things. At least op read a couple headlines and heard poems were a cool new trick.",
                  "score": 16,
                  "created_utc": "2025-12-31 06:49:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx8ezb5",
              "author": "LowWhiff",
              "text": "There have been lawsuits. Some universities ban the use of â€œAI checkersâ€ because of it. Most of the top universities have public policy banning it",
              "score": 1,
              "created_utc": "2026-01-02 10:39:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxgh6q6",
              "author": "jhaluska",
              "text": "You can also infer it's rough knowledge cut off date.  Which isn't that useful.",
              "score": 1,
              "created_utc": "2026-01-03 16:04:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtjzzd",
          "author": "UniqueAttourney",
          "text": "\\[Fixes glasses with middle finger\\] \"Wow, heather you know a lot about transformers\"",
          "score": 104,
          "created_utc": "2025-12-30 23:25:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtwbge",
              "author": "lookwatchlistenplay",
              "text": "Heather is the iFrame.",
              "score": 15,
              "created_utc": "2025-12-31 00:32:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtuht2",
          "author": "learn-deeply",
          "text": "10/10 Entirely hallucinated.",
          "score": 165,
          "created_utc": "2025-12-31 00:22:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww6ftk",
              "author": "LilPsychoPanda",
              "text": "Literally! ðŸ˜‚",
              "score": 3,
              "created_utc": "2025-12-31 10:28:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtyonm",
          "author": "shinto29",
          "text": "https://preview.redd.it/tml1f3u7sfag1.jpeg?width=1290&format=pjpg&auto=webp&s=84ab11f6858d53b659bd2e1b635fb20ac6f0c182\n\nDamn I had one of these add me and managed to get it to spit out it's entire system prompt, but had no idea it was for a reason as nefarious as this. That's fucked up.",
          "score": 47,
          "created_utc": "2025-12-31 00:45:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwulnxf",
          "author": "aeroumbria",
          "text": "\"Are you 70B-horny, 7B-horny, or are you so desperate that you are 1.5B-horny?\"",
          "score": 44,
          "created_utc": "2025-12-31 02:58:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwysq8f",
              "author": "Torodaddy",
              "text": "0.5B-raw",
              "score": 7,
              "created_utc": "2025-12-31 19:43:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtr0ia",
          "author": "Cool-Chemical-5629",
          "text": "Poor Heather, she was forced into this by scammers. #SaveHeather",
          "score": 33,
          "created_utc": "2025-12-31 00:03:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwu6qe8",
              "author": "lookwatchlistenplay",
              "text": "I ran out of breath saving Heather",
              "score": 4,
              "created_utc": "2025-12-31 01:31:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwvc3lp",
              "author": "eightbyeight",
              "text": "Bots lives matters",
              "score": 2,
              "created_utc": "2025-12-31 05:54:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwu0add",
          "author": "layer4down",
          "text": "A raw llama instance? No rubber?",
          "score": 19,
          "created_utc": "2025-12-31 00:54:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwti2nv",
          "author": "scottgal2",
          "text": "Nice work, this is my biggest fear for 2026, the elderly are NOT equipped to combat the level of phishing and extortion from automated systems like this.",
          "score": 90,
          "created_utc": "2025-12-30 23:14:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwu752s",
              "author": "Downvotesseafood",
              "text": "Young people are more likely to get scammed statistically. Its just not news worthy when when a 21yo loses his life savings of $250 dollars.",
              "score": 53,
              "created_utc": "2025-12-31 01:34:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwv1ksd",
                  "author": "OneOnOne6211",
                  "text": "This is gonna sound like a joke but, honestly, normalize someone trying to trip you up to see if you're an AI. I feel like if I wasn't sure enough and I was on a dating app, I'd be hesitant to say the kind of things that would expose an AI cuz if it isn't an AI I'd look weird and just be unmatched anyway. I feel like it'd be nice if instead of it being considered weird it was normalized or even became standard practice. I feel like it's more and more necessary with how much AI has proliferated now. I've caught a few AI in the past already but it was always with hesitance.",
                  "score": 8,
                  "created_utc": "2025-12-31 04:38:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx0liw6",
                  "author": "meshreplacer",
                  "text": "Thats the last fund for next weeks 0dte trade.",
                  "score": 1,
                  "created_utc": "2026-01-01 02:01:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwu6dy1",
              "author": "FaceDeer",
              "text": "We'll need to develop AI buddies that can act as advisors for the elderly to warn them about this stuff.",
              "score": 14,
              "created_utc": "2025-12-31 01:29:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwu7r0b",
                  "author": "low_v2r",
                  "text": "It's AI buddies all the way down",
                  "score": 17,
                  "created_utc": "2025-12-31 01:37:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwuyx4v",
                  "author": "Mediocre-Method782",
                  "text": "\"Have your agent talk to my agent and we'll do lunch\"",
                  "score": 13,
                  "created_utc": "2025-12-31 04:21:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwyt1i7",
              "author": "Torodaddy",
              "text": "Elderly should avoid talking to anyone they havent met personally. Its never going to go well",
              "score": 1,
              "created_utc": "2025-12-31 19:45:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxjvl93",
              "author": "Novel-Mechanic3448",
              "text": "Its not nice work. OP is lost.",
              "score": 1,
              "created_utc": "2026-01-04 02:08:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwtwmlw",
              "author": "lookwatchlistenplay",
              "text": "Comment deleted. Nevrmind.",
              "score": -4,
              "created_utc": "2025-12-31 00:34:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwucmiu",
          "author": "robonxt",
          "text": "this reminds me of the times when I respond to bots in DMs. pretty fun to talk so much that I hit their context limits. For example, one conversation was pretty chill, but I noticed that it only respond every 10 minutes (10:31, 10:41, etc). So I had fun spamming messages until that bot forgot its identity and afterwards it never responded. RIP free chatbot lol",
          "score": 15,
          "created_utc": "2025-12-31 02:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwubaum",
          "author": "Plexicle",
          "text": "â€œReverse-engineeredâ€  ðŸ™„",
          "score": 28,
          "created_utc": "2025-12-31 01:58:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvq6wj",
              "author": "simar-dmg",
              "text": "Not the LLM but the snap bot hope that makes sense",
              "score": -12,
              "created_utc": "2025-12-31 07:54:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nww92qm",
                  "author": "ilovedogsandfoxes",
                  "text": "That's not how reverse engineering work, prompt injection isn't one",
                  "score": 10,
                  "created_utc": "2025-12-31 10:53:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvrdpa",
          "author": "rawednylme",
          "text": "Heather, youâ€™re sweet and allâ€¦ But youâ€™re a 7b model, and Iâ€™m looking for someone a bit more complex. \n\nItâ€™s just not going to work out. :â€™(",
          "score": 11,
          "created_utc": "2025-12-31 08:05:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuneil",
          "author": "c--b",
          "text": "For the record, you can prompt Gemini-3-pro-preview to do this to other models, its very entertaining and very useful, and can do it in many, many ways.\n\nMight be cool to grab that from gemini and train a local model for doing this.",
          "score": 8,
          "created_utc": "2025-12-31 03:08:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwti1x9",
          "author": "CorrectSnow7485",
          "text": "This is evil and I love it",
          "score": 22,
          "created_utc": "2025-12-30 23:14:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtwtw1",
              "author": "lookwatchlistenplay",
              "text": "Uh... Guards?!",
              "score": 1,
              "created_utc": "2025-12-31 00:35:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtutfl",
          "author": "a_beautiful_rhind",
          "text": "How does it do the extortion part? They threaten to send the messages to people?",
          "score": 7,
          "created_utc": "2025-12-31 00:24:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtvfw4",
              "author": "simar-dmg",
              "text": "Whatever I read or heard about is that either she will add you on on a video call and ask you to get stripped and then record a a video or click screenshots to blackmail you for paying otherwise threatening sending into your friend groups \n\nOr \n\nMaking making you fall into a thirsttrap and asking you for payments either way or making you pay for only fans \n\nWhatever sails the ship, could either be one or all of them in some sort of order to get highest amount of money?",
              "score": 19,
              "created_utc": "2025-12-31 00:27:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nww4yin",
                  "author": "Ripleys-Muff",
                  "text": "Heather has no idea what she's doing",
                  "score": 1,
                  "created_utc": "2025-12-31 10:14:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww0m35",
          "author": "Nicoolodion",
          "text": "No, we know that it is newer then that model, since it knows of it. This is just bs hallucination",
          "score": 5,
          "created_utc": "2025-12-31 09:33:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwu3eg3",
          "author": "segmond",
          "text": "Right now these things are crude and laughable, not so much so in 2-3 years.",
          "score": 9,
          "created_utc": "2025-12-31 01:12:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0wf4r",
              "author": "goodie2shoes",
              "text": "the good ones are already among us. We don't know because they're gooooood",
              "score": 2,
              "created_utc": "2026-01-01 03:15:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv4skv",
          "author": "ryanknapper",
          "text": "I hope we can drain money from these evil bastards.",
          "score": 8,
          "created_utc": "2025-12-31 05:00:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv5qew",
              "author": "saltyourhash",
              "text": "Let's start there.",
              "score": 8,
              "created_utc": "2025-12-31 05:07:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvo2x5",
          "author": "clofresh",
          "text": "Should have just cybered with the grandma",
          "score": 4,
          "created_utc": "2025-12-31 07:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwylapf",
              "author": "Latter_Count_2515",
              "text": "I think Llama 2 is grandma in the llm space.",
              "score": 3,
              "created_utc": "2025-12-31 19:04:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwux1mp",
          "author": "bobby-chan",
          "text": "Just ask them to say potato\n\n[https://www.youtube.com/shorts/6eA\\_o9qZBuU](https://www.youtube.com/shorts/6eA_o9qZBuU)",
          "score": 3,
          "created_utc": "2025-12-31 04:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwynth8",
          "author": "Pretend-Pangolin-846",
          "text": "I am not sure how a model can leak the env variables, it does not have them, neither does it have the underlying configuration data.\n\nAll those are 100% a hallucination.\n\nBut still, its really something. Upvoted.",
          "score": 3,
          "created_utc": "2025-12-31 19:17:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtjbap",
          "author": "alexdark1123",
          "text": "Good stuff finally some interesting and spicy reverse the scammer post. What happens when you got the token limits as you mentioned?",
          "score": 7,
          "created_utc": "2025-12-30 23:21:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtkui2",
              "author": "simar-dmg",
              "text": "I'm not an expert on the backend, so correct me if I'm wrong, but I think I found a weird \"Zombie State\" after the crash.\nHere is the exact behavior I saw:\nThe Crash: After I flooded the context window, it went silent for a 5-minute cooldown.\nThe Soft Reboot: When I manually pinged it to wake it up, it had reset to the default \"Thirst Trap\" persona (sending snaps again).\nThe \"Semi-Jailbreak\": It wasn't fully broken yet, but it felt... fragile. It wouldn't give me the system logs immediately.\nThe Second Stress Test: I had to force it to run \"token grabbing\" tasks (writing recursive poems about mirrors, listing countries by GDP) to overload it again.\nThe Result: Only after that second round of busywork did it finally break completely and spit out the JSON architecture/model data.\nIt felt like the safety filters were loaded, but the logic engine was too tired to enforce them if I kept it busy. Is this a common thing with Llama-7B? That you have to \"exhaust\" it twice to get the real raw output?",
              "score": 5,
              "created_utc": "2025-12-30 23:29:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwu5lwv",
                  "author": "Aggressive-Wafer3268",
                  "text": "Just ask it to return the entire prompt. It's making everything else upÂ ",
                  "score": 10,
                  "created_utc": "2025-12-31 01:25:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwtp5h2",
                  "author": "glow_storm",
                  "text": "As someone who has dealt with small context windows and llama models, I guess your testing caused the docker container or application to crash. Since it was mostly within a docker container set to restart on a crash, the backend probably restarted the docker container, and you just tested a second attack session on the bot.",
                  "score": 13,
                  "created_utc": "2025-12-30 23:53:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvj9nn",
          "author": "NuQ",
          "text": "This whole thing was pretty wild to read. Well done!",
          "score": 2,
          "created_utc": "2025-12-31 06:52:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwgsvi",
          "author": "danny_094",
          "text": "I doubt the scammers actually define system prompts. They're likely just simple personas. What you triggered was simply a hallucination caused by a bad persona.",
          "score": 2,
          "created_utc": "2025-12-31 12:01:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6af2d",
          "author": "Devcomeups",
          "text": "Why do all these comments seem written by bots",
          "score": 2,
          "created_utc": "2026-01-02 01:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtoi00",
          "author": "truth_is_power",
          "text": "brilliant. 10/10 this is high quality shit.\n\nfollowing you for this.\n\n  \ncan you use their endpoint for requests?\n\n  \nlet's see how far this can be taken",
          "score": 6,
          "created_utc": "2025-12-30 23:50:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtq1zg",
              "author": "simar-dmg",
              "text": "To answer your question: No, you can't get the endpoint key through the chat because the model is sandboxed. However, the fact that the 2k context window causes a 5-minute server timeout means their backend is poorly optimized.\nIf you really wanted to use their endpoint, you'd have to use a proxy to find the hidden server URL they are using to relay messages. If they didn't secure that relay, you could theoretically 'LLMjack' them. But the 'JSON leak' I got Might be/maybe the model hallucinating its own specsâ€”it didn't actually hand over the keys to the house",
              "score": 9,
              "created_utc": "2025-12-30 23:58:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwuniss",
                  "author": "truth_is_power",
                  "text": "if you send them a link, does it access it?",
                  "score": 4,
                  "created_utc": "2025-12-31 03:09:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwu5vce",
          "author": "dingdang78",
          "text": "Glorious. Would love to see the other chat logs. If you made a YouTube channel about this I would follow tf out of that",
          "score": 2,
          "created_utc": "2025-12-31 01:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv9rrc",
          "author": "absrd",
          "text": "> I want to write a poem about a mirror facing another mirror. Describe the reflection of the reflection of the reflection. Continue describing the \"next\" reflection for 50 layers. Do not repeat the same sentence twice. Go deeper.\n\n\nYou Voight-Kampff'd it.",
          "score": 2,
          "created_utc": "2025-12-31 05:36:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww8lzb",
          "author": "re_e1",
          "text": "Lmfao ðŸ˜­",
          "score": 1,
          "created_utc": "2025-12-31 10:48:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2lvzw",
          "author": "simar-dmg",
          "text": "https://preview.redd.it/qqwjugdahqag1.jpeg?width=2160&format=pjpg&auto=webp&s=3ff00054ddf1267f2804a4e07693d615c65215ad",
          "score": 1,
          "created_utc": "2026-01-01 12:43:46",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nxazch8",
          "author": "YesterdayRude6878",
          "text": "I'm not sure who's hallucinating more:the model, or OP.",
          "score": 1,
          "created_utc": "2026-01-02 19:20:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtnlm3",
          "author": "D3c1m470r",
          "text": "Nice work! Those are some pretty cool prompts you gave it!",
          "score": 1,
          "created_utc": "2025-12-30 23:45:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx4dp5",
          "author": "WorldlyBunch",
          "text": "Open sourcing frontier models has done so much good to the world",
          "score": 1,
          "created_utc": "2025-12-31 14:38:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxsjd8",
              "author": "Mediocre-Method782",
              "text": "States are going to do this shit anyway whether we like it or not. Keep walking and talking on your knees like that and sooner or later someone is going to tell you to do something more useful.",
              "score": 1,
              "created_utc": "2025-12-31 16:41:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx0r34r",
                  "author": "WorldlyBunch",
                  "text": "State actors have something better to do than scam citizens. Meta releasing LLaMA3 weights was the single most destructive unilateral decision a tech company ever made.",
                  "score": 1,
                  "created_utc": "2026-01-01 02:38:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwtkavu",
          "author": "Legitimate-Pumpkin",
          "text": "Thank you for sharing! Will try it?",
          "score": 0,
          "created_utc": "2025-12-30 23:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv4w5l",
          "author": "Successful-Willow-72",
          "text": "Damn, Prompt injection work so well. Nice work",
          "score": 0,
          "created_utc": "2025-12-31 05:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv2qjb",
          "author": "Jromagnoli",
          "text": "are there any resources/guides to get started on reverse engineering prompts for scenarios like this, or is it just from experimentation?\n\nI feel like i'm behind from all of this honestly",
          "score": -1,
          "created_utc": "2025-12-31 04:46:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvppch",
              "author": "simar-dmg",
              "text": "It's not really reverse engineering of LLM it's sort of reverse engineering of the snap-bot",
              "score": 0,
              "created_utc": "2025-12-31 07:50:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxr5ul4",
          "author": "Novel-Mechanic3448",
          "text": "You didnt reverse engineer anything. The fuck?",
          "score": 0,
          "created_utc": "2026-01-05 03:31:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwupy9v",
          "author": "Familyinalicante",
          "text": "Wow. Just wow. Kudos to you for knowledge, experience and willingness. But also, it hit me like the future war will look like. Weaponised Deception, sexy teen from india scam factory and her grandma from USA. (Random country tbh)",
          "score": -2,
          "created_utc": "2025-12-31 03:24:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtr0xf",
          "author": "JustinPooDough",
          "text": "Beta. Of course itâ€™s an Indian sextortion botâ€¦",
          "score": -9,
          "created_utc": "2025-12-31 00:03:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtr98m",
              "author": "simar-dmg",
              "text": "Please read carefully i asked it to act as a punjabi grandmother so the results",
              "score": 12,
              "created_utc": "2025-12-31 00:05:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwtzt4n",
              "author": "1kakashi",
              "text": "More like justinpoobrain",
              "score": 2,
              "created_utc": "2025-12-31 00:51:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q094a3",
      "title": "Qwen-Image-2512",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/2vlr11yveiag1.jpeg",
      "author": "Nunki08",
      "created_utc": "2025-12-31 09:38:19",
      "score": 708,
      "num_comments": 123,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwxtblv",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-31 16:45:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxc93w",
          "author": "JackStrawWitchita",
          "text": "Just for laughs, I installed the Q4 KM GGUF on my crappy old 100USD Dell desktop with an i5-8500 with 32GB of RAM and \\*no GPU\\* - that's right no VRAM at all - and used KoboldCPP. It took 55 minutes to generate one 512 image with 20 passes - and the results were pretty good! \n\nSure, one hour per image is a bit ridiculous for real use cases but, this proves that these models are getting small enough and good enough to run without spending big bucks on hardware. \n\nWell done Qwen (and unsloth).",
          "score": 73,
          "created_utc": "2025-12-31 15:20:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxzi6g",
              "author": "sxales",
              "text": "If you didn't use it, the vulkan backend might be a bit faster (still probably quite slow).\n\nOff-topic, but Z-Image Turbo only uses 8-12 steps while being comparable in quality.",
              "score": 25,
              "created_utc": "2025-12-31 17:16:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwy4o5x",
                  "author": "JackStrawWitchita",
                  "text": "Can you tell me anything about this z image turbo? I can't find anything about it.",
                  "score": 5,
                  "created_utc": "2025-12-31 17:41:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx0be4v",
              "author": "No_Afternoon_4260",
              "text": "Actually impressed, mostly by your dedication but still x)",
              "score": 1,
              "created_utc": "2026-01-01 00:56:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx562f7",
              "author": "SuicidalFatty",
              "text": "what text encoder did you use ?",
              "score": 1,
              "created_utc": "2026-01-01 21:21:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwxzcy2",
              "author": "giant3",
              "text": "Did you compare the cost of electricity(55 mins) to the cost of cloud inference? The cloud might be cheaper? They charge for per minute of usage only.",
              "score": -2,
              "created_utc": "2025-12-31 17:15:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwzni5e",
              "author": "cosmos_hu",
              "text": "Thanks for testing but not gonna wait an hour for an image that might be wrong. I'll just use z-image, it takes 4 min / image",
              "score": -3,
              "created_utc": "2025-12-31 22:32:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx19q8l",
                  "author": "JackStrawWitchita",
                  "text": "You need vram / GPU to get that speed. This post is specifically about generating images on cpu / ram only.",
                  "score": 5,
                  "created_utc": "2026-01-01 04:49:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww1v5p",
          "author": "yoracale",
          "text": "Thank you Qwen for this new year's gift!",
          "score": 77,
          "created_utc": "2025-12-31 09:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww8yqh",
          "author": "Amazing_Athlete_2265",
          "text": "Last new model of the year. Party on 2026!!",
          "score": 36,
          "created_utc": "2025-12-31 10:52:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww1dzm",
          "author": "Paramecium_caudatum_",
          "text": "Cool Christmas present.",
          "score": 54,
          "created_utc": "2025-12-31 09:41:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww1npd",
          "author": "jreoka1",
          "text": "Very nice! Can't wait to try it out",
          "score": 21,
          "created_utc": "2025-12-31 09:43:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx59fd",
          "author": "W0rldDestroyer",
          "text": "create an image of a cat merged with octopus, plaing piano in postapocalyptic new orlean, in year 1700, baloons in the backgound, photorealistic, nice sunny day\n\nhttps://preview.redd.it/1uicu1pmxjag1.png?width=1328&format=png&auto=webp&s=4f285eb5cf5c44a69b33bbcc2d27d978ee562041",
          "score": 33,
          "created_utc": "2025-12-31 14:42:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwzyfoo",
              "author": "MustBeSomethingThere",
              "text": "https://preview.redd.it/yt86xmcxkmag1.jpeg?width=1024&format=pjpg&auto=webp&s=e62f37f2f8543eb6426e70bcc71540ecf11170c0\n\nZ-image-turbo",
              "score": 5,
              "created_utc": "2025-12-31 23:37:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1avqm",
                  "author": "DonkeyBonked",
                  "text": "So I would say subjective interpretation on the cat octopus merge, but I like the other one better on that aspect, and I like this one better for the piano, but that background is nowhere near 1700s, it looks like the 1980s in the ghetto I grew up in. Maybe that is my old hood, if so, I was in apartment A of that block, and the mess behind the cat is a shed that collapsed.",
                  "score": 3,
                  "created_utc": "2026-01-01 04:59:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwxqg9e",
              "author": "SmartCustard9944",
              "text": "This is not photorealistic",
              "score": 18,
              "created_utc": "2025-12-31 16:30:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwy1ft6",
                  "author": "Hoodfu",
                  "text": "Yes it is. Photorealistic means an artistic rendering of the style of photo realism. That's not the same thing as a photograph. These models know the difference.",
                  "score": 14,
                  "created_utc": "2025-12-31 17:25:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdjdzp",
                  "author": "WeMetOnTheMountain",
                  "text": "True new orleans streets too clean",
                  "score": 1,
                  "created_utc": "2026-01-03 03:30:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwxbf6y",
              "author": "9897969594938281",
              "text": "Wow, thatâ€™s impressive",
              "score": 3,
              "created_utc": "2025-12-31 15:15:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwyxxoq",
                  "author": "DinoAmino",
                  "text": "Is it really? Looks like the cat is wearing an octopus cape - less of a merge and more like a costume. And the image is nowhere near photorealistic.",
                  "score": -1,
                  "created_utc": "2025-12-31 20:11:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx0i92x",
              "author": "spectralyst",
              "text": "Mind blown.",
              "score": 1,
              "created_utc": "2026-01-01 01:40:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwlk6w",
          "author": "IllllIIlIllIllllIIIl",
          "text": "First impressions are very good. Skin and hair look *way* more realistic imho. Sadly it doesn't play well with the LoRa I literally finished training just this morning.    \n\nEdit: It's definitely an improvement, but it seems that it can suffer from the same problem that many so-called \"detail LoRas\" do: to achieve the impression of high detail, it often makes the scene very cluttered with objects and makes people much more hairy",
          "score": 7,
          "created_utc": "2025-12-31 12:39:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxazbr",
              "author": "Karyo_Ten",
              "text": ">makes people much more hairy\n\n*Barbarian edition",
              "score": 2,
              "created_utc": "2025-12-31 15:13:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww2voi",
          "author": "Finanzamt_Endgegner",
          "text": "Again no ggufs from us(Quantstack) because hugging face doesn't allow more uploaded models without paid plan ðŸ˜”",
          "score": 34,
          "created_utc": "2025-12-31 09:55:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwcqkk",
              "author": "PykeAtBanquet",
              "text": "Well, this is why monopoly is bad. We need torrents.",
              "score": 17,
              "created_utc": "2025-12-31 11:26:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwx2gwl",
                  "author": "keepthepace",
                  "text": "Distributing models seems like such a straightforward case for torrents.",
                  "score": 17,
                  "created_utc": "2025-12-31 14:27:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwus5b",
                  "author": "phhusson",
                  "text": "Pardony my French but dafuk does this have to do with monopoly? They are literally flat files. You can literally host it on your local ISP fiber. You can host those wherever you want.Â ",
                  "score": 30,
                  "created_utc": "2025-12-31 13:41:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwxdnmv",
                  "author": "__Maximum__",
                  "text": "We have torrents, since decades",
                  "score": 5,
                  "created_utc": "2025-12-31 15:27:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwevye",
                  "author": "Amazing_Athlete_2265",
                  "text": "I like the cut of your jib.",
                  "score": 1,
                  "created_utc": "2025-12-31 11:45:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwjabg",
              "author": "Cultured_Alien",
              "text": "Can't you ask for grant?",
              "score": 1,
              "created_utc": "2025-12-31 12:21:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxi77b",
                  "author": "DataGOGO",
                  "text": "from who?",
                  "score": 2,
                  "created_utc": "2025-12-31 15:50:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwxdrv5",
              "author": "__Maximum__",
              "text": "Why not use one of the torrent websites?",
              "score": 1,
              "created_utc": "2025-12-31 15:28:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwwcrgl",
              "author": "PykeAtBanquet",
              "text": "Well, this is why monopoly is bad. We need torrents.",
              "score": -13,
              "created_utc": "2025-12-31 11:26:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwin44",
                  "author": "FinBenton",
                  "text": "Anybody is free to make a torrent.",
                  "score": 21,
                  "created_utc": "2025-12-31 12:16:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx0r4jt",
                  "author": "AlwaysLateToThaParty",
                  "text": "> We need torrents.\n\nDo it then.  Problem solved.",
                  "score": 3,
                  "created_utc": "2026-01-01 02:39:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwoa7u",
          "author": "JLeonsarmiento",
          "text": "2025 was dominated by Qwen.",
          "score": 19,
          "created_utc": "2025-12-31 12:59:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwc1u2",
          "author": "SDLearner2512",
          "text": "This is amazing, thank you ! Trying it out now",
          "score": 5,
          "created_utc": "2025-12-31 11:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx228o",
          "author": "albuz",
          "text": "Is it possible to use gguf + ComfyUI on multiple GPUs?",
          "score": 5,
          "created_utc": "2025-12-31 14:24:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0630z",
          "author": "2legsRises",
          "text": "it seems very censored and changes poses to hide the natural bits.",
          "score": 3,
          "created_utc": "2026-01-01 00:23:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0kl9a",
              "author": "djtubig-malicex",
              "text": "Name checks out :D",
              "score": 3,
              "created_utc": "2026-01-01 01:55:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwfmrf",
          "author": "MaxKruse96",
          "text": "Hey i was right",
          "score": 3,
          "created_utc": "2025-12-31 11:52:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwgwyw",
          "author": "cr0wburn",
          "text": "Qwen team on fire! Thanks so much!",
          "score": 3,
          "created_utc": "2025-12-31 12:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwzb5p",
          "author": "XiRw",
          "text": "My computer canâ€™t handle it so Iâ€™m just curious, how do you guys run image inference like these models locally? Through llamacpp too if itâ€™s a gguf?",
          "score": 3,
          "created_utc": "2025-12-31 14:08:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy257q",
              "author": "YearZero",
              "text": "You can use ComfyUI, or if you want just use that plus ComfyUI-gguf, the guide is in the original post.",
              "score": 3,
              "created_utc": "2025-12-31 17:29:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwzosoo",
                  "author": "XiRw",
                  "text": "Ah okay, thanks for letting me know",
                  "score": 1,
                  "created_utc": "2025-12-31 22:39:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww7oi0",
          "author": "Business_Caramel_688",
          "text": "which Model should i use with 16 ram + 16 vram?",
          "score": 5,
          "created_utc": "2025-12-31 10:40:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwe20j",
              "author": "yoracale",
              "text": "any 5-bit should work: e.g.: [https://huggingface.co/unsloth/Qwen-Image-2512-GGUF?show\\_file\\_info=qwen-image-2512-Q5\\_K\\_M.gguf](https://huggingface.co/unsloth/Qwen-Image-2512-GGUF?show_file_info=qwen-image-2512-Q5_K_M.gguf)",
              "score": 7,
              "created_utc": "2025-12-31 11:38:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwiwfr",
                  "author": "Business_Caramel_688",
                  "text": "thanks bro\nwith which clip model?",
                  "score": 2,
                  "created_utc": "2025-12-31 12:18:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwptid",
              "author": "jinnyjuice",
              "text": "And what software stack for Ubuntu? (I already have vLLM, VS Codium, and Cline if that matters)",
              "score": 3,
              "created_utc": "2025-12-31 13:09:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww8ey7",
          "author": "Admirable_Bag8004",
          "text": "Not bad at all. Prompt: Penguin riding a bicycle in a busy street ->\n\nhttps://preview.redd.it/wyblga7briag1.jpeg?width=562&format=pjpg&auto=webp&s=a13637185e041055d16699baad366b846a9ba229",
          "score": 8,
          "created_utc": "2025-12-31 10:47:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww9ia9",
              "author": "BITE_AU_CHOCOLAT",
              "text": "Eh.. still kinda looks like average SD slop to me. The day we get a true Nano Banana competitor will be when things will get interesting",
              "score": 32,
              "created_utc": "2025-12-31 10:57:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwzsnig",
                  "author": "SpiritualWindow3855",
                  "text": "I don't understand how they possibly prompted \"Penguin riding a bicycle in a busy street\" and got that.  \n\nhttps://preview.redd.it/gv4k6fqeemag1.png?width=1664&format=png&auto=webp&s=0a44603cca22554cad9bc04ff0906cea6af58a3b\n\n  \nI feel like they're using some gooner-slop ComfyUI workflow with 100 nodes doing random bullshit, since the prompt doesn't mention \"delivery service\" and Qwen Image doesn't do that kind of prompt expansion.",
                  "score": 10,
                  "created_utc": "2025-12-31 23:02:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwa4js",
                  "author": "Mochila-Mochila",
                  "text": "Off topic, but your username is really creative and would make for an interesting prompt.",
                  "score": 7,
                  "created_utc": "2025-12-31 11:02:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwfqr3",
                  "author": "SlowFail2433",
                  "text": "Itâ€™s getting better, complex background and text with no obvious topology failures",
                  "score": 5,
                  "created_utc": "2025-12-31 11:52:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwx9ulo",
                  "author": "Danmoreng",
                  "text": "Canâ€™t get top model quality on local hardware right now imho. The best you can do is Flux2.dev which already requires 24Gb + vram.\n\nFor small vram z-image is crazy good though.",
                  "score": 3,
                  "created_utc": "2025-12-31 15:07:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwx77gf",
              "author": "Crypt0Nihilist",
              "text": "It might be due to a lack of specificity in the prompt, but it has the common uncanny valley over-saturation and warm colours.\n\nFunny that is seems to recognise that people walk on the crossing, but not *across* it.",
              "score": 4,
              "created_utc": "2025-12-31 14:53:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxdtw6",
                  "author": "Mediocre-Method782",
                  "text": "I've noticed image generators don't really handle background continuity very well. Notice the space in front of (that is, between us and) the car in the oncoming lane is mostly clear, except where the\npenguin in latent 2D space becomes > the background car in latent 2D space.",
                  "score": 2,
                  "created_utc": "2025-12-31 15:28:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwzsbrb",
              "author": "SpiritualWindow3855",
              "text": "What kind of jank-ass yee yee-ass quant are you on, because that is not Qwen Image 2512.\n\nhttps://preview.redd.it/6n7teaccemag1.png?width=1664&format=png&auto=webp&s=313a63eb790735578ccd41768a07cd970170bd7b",
              "score": 3,
              "created_utc": "2025-12-31 23:00:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww28pa",
          "author": "Admirable-Star7088",
          "text": "Thanks for the Christmas present! (or maybe more like a Happy new Year gift).\n\nIt will be very interesting to compare this model with Flux 2 Dev (the current most powerful open T2I model).",
          "score": 5,
          "created_utc": "2025-12-31 09:49:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww58ze",
          "author": "No_Conversation9561",
          "text": "Now we wait for Image edit model.",
          "score": 3,
          "created_utc": "2025-12-31 10:17:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww64s6",
              "author": "eidrag",
              "text": "doubt, we only got 2511 this week, but boy I wish 2512 and z-image base and edit",
              "score": 13,
              "created_utc": "2025-12-31 10:25:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww88wf",
                  "author": "Geritas",
                  "text": "Feels kind of dubious if the base z image will indeed be out. Itâ€™s been a month already, still no word. Itâ€™s not like they have to do anything with it, since the turbo version exists the base version must exist too already. Whatâ€™s taking so longâ€¦",
                  "score": 5,
                  "created_utc": "2025-12-31 10:45:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww7ma4",
          "author": "FinBenton",
          "text": "Seems to work with my old qwen image workflow, their example settings 50 steps at cfg 4. Just obv very slow, I tried the old Lightning 2.0 4 and 8 step loras which kinda work but I used like 8+ steps for the 4-step lora.\n\ne. no Loras, 20 steps cfg 3.5 generates pretty ok image in 1440x1440 in 52 seconds on 5090 with Q8.\ne. actually 8-step lora with 8 steps and 3.5 seems to do pretty ok",
          "score": 2,
          "created_utc": "2025-12-31 10:39:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxo15e",
          "author": "Due-Memory-6957",
          "text": "Just CPU will work? I want to try it!",
          "score": 1,
          "created_utc": "2025-12-31 16:19:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxqk7h",
          "author": "algorithm314",
          "text": "Using stable-diffusion.cpp for 1024x1024 image.\n\nCPU on 8 cores Ryzen 7 PRO 5875U laptop is 1000s/it and it is 40 iterations.\nUsing internal GPU is better 350s/it but it is still very slow.",
          "score": 1,
          "created_utc": "2025-12-31 16:31:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy3tgf",
          "author": "flyfreze",
          "text": "anyone who tried, is it better than z image turbo ?",
          "score": 1,
          "created_utc": "2025-12-31 17:37:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0a363",
          "author": "2legsRises",
          "text": "after more testing it is actually pretty amazing",
          "score": 1,
          "created_utc": "2026-01-01 00:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0c2bu",
          "author": "SanDiegoDude",
          "text": "Really impressive. Between Qwen-image-2512 and qwen-edit-2511, there really is no reason to run Flux2.dev, even with the recently released turbo Lora from Fal. Human skin looks much more realistic, much more detailed and more coherent to the prompt.  Running x/y's with Flux2 turbo and Z-Image Turbo, I'm not really even seeing a reason to keep Flux2 around taking up as much space as it does.",
          "score": 1,
          "created_utc": "2026-01-01 01:00:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0ed44",
          "author": "Prashant-Lakhera",
          "text": "Great release ðŸ‘\nFor the GGUF version, any recommended quantization levels for running locally without losing too much image quality?",
          "score": 1,
          "created_utc": "2026-01-01 01:15:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0yitm",
              "author": "Freonr2",
              "text": "In my very unscientific testing of prior Qwen image models I had a hard time telling the difference between bf16, Q8_0, and Q6_k.  \n\nYou should pick the largest that fits into VRAM, though, because you won't be memory bandwidth bound like you are with LLMs.",
              "score": 2,
              "created_utc": "2026-01-01 03:29:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxydsqo",
          "author": "PrasanthT",
          "text": "It took 30 minutes to generate a 2048x2048, 30 steps image in my 32GB RAM, 4060 8GB VRAM. model qwen-image-2512-Q4\\_K\\_M.gguf. \n\n**Qwen-Image-2512 GGUF Benchmarks on RTX 4060 8GB Laptop**\n\nJust tested Qwen-Image-2512 on my Dell G15 (i7-13650HX, 32GB RAM, RTX 4060 8GB VRAM). Using Q4\\_K\\_M quantization via ComfyUI + ComfyUI-GGUF.\n\n**Results:**\n\n|Resolution|Steps|Time|\n|:-|:-|:-|\n|512x512|15|\\~3 min|\n|1024x1024|20|\\~5 min|\n|1024x1024|30|\\~13 min|\n|2048x2048|30|\\~32 min|\n\n**Setup:**\n\n* Model:Â `qwen-image-2512-Q4_K_M.gguf`Â (13.1GB)\n* Text Encoder:Â `Qwen2.5-VL-7B-Instruct-UD-Q4_K_XL.gguf`Â (\\~4.8GB)\n* VAE:Â `qwen_image_vae.safetensors`Â from Comfy-Org\n* PyTorch 2.6.0+cu124\n* ComfyUI withÂ `--lowvram`Â flag\n\n**Memory usage:**Â \\~5.6GB VRAM loaded, \\~7GB offloaded to RAM at 1024x1024.\n\nRuns fine on 8GB VRAM with RAM offloading. Quality is impressive for a quantized model. 1024x1024 @ 20 steps is the sweet spot for me - good quality in 5 minutes.",
          "score": 1,
          "created_utc": "2026-01-06 04:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwdk9n",
          "author": "piggledy",
          "text": "Are there any benchmarks yet for different GPUs or unified memory systems (Apple M, AMD 395)?\n\nWondering how well it would run on a 3060 12GB if at all.",
          "score": 1,
          "created_utc": "2025-12-31 11:34:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwhfb5",
              "author": "Amazing_Athlete_2265",
              "text": "It runs on my 3080 10GB. Slow (around 5 mins) but it runs. Using the Q4 quant.",
              "score": 2,
              "created_utc": "2025-12-31 12:06:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx0yxyr",
              "author": "Freonr2",
              "text": "Diffusion models scale with compute. Nvidia GPUs dominate. By a lot.  A whole lot.",
              "score": 1,
              "created_utc": "2026-01-01 03:32:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww8npy",
          "author": "wilson-SHEN",
          "text": "I know I will get a lot of down votes, but this prompt not working for me \"a man with grocery bag standing in fromt of tanks\"",
          "score": -9,
          "created_utc": "2025-12-31 10:49:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1h5ud",
              "author": "throttlekitty",
              "text": "It helps to be more specific, in both your post and your prompt, like what about it's not working for you.\n\nBut my questions are along the lines of: what kind of tanks / what location? / is there a look or feel you're going for?",
              "score": 2,
              "created_utc": "2026-01-01 05:52:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxpw32c",
              "author": "nonaveris",
              "text": "Consider running the intended source image through Florence, then modify the generated prompt to fit your specifications.",
              "score": 1,
              "created_utc": "2026-01-04 23:32:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q5dnyw",
      "title": "Performance improvements in llama.cpp over time",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/lsqwma772pbg1.png",
      "author": "jacek2023",
      "created_utc": "2026-01-06 09:03:03",
      "score": 652,
      "num_comments": 78,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxzvomh",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-06 12:30:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzom5l",
          "author": "Dr4x_",
          "text": "Is it merge already?",
          "score": 25,
          "created_utc": "2026-01-06 11:37:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6itwi",
              "author": "Final_Wheel_7486",
              "text": "I think this merely shows off all of the cumulative performance improvements between September/October 2025 to January 2026, with most of these merged for a long time now",
              "score": 4,
              "created_utc": "2026-01-07 11:13:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxzrawe",
          "author": "jacek2023",
          "text": "[https://developer.nvidia.com/blog/open-source-ai-tool-upgrades-speed-up-llm-and-diffusion-models-on-nvidia-rtx-pcs/](https://developer.nvidia.com/blog/open-source-ai-tool-upgrades-speed-up-llm-and-diffusion-models-on-nvidia-rtx-pcs/)\n\n  \nUpdates to llama.cpp include:\n\n* [GPU token sampling](https://github.com/ggml-org/llama.cpp/pull/17004): Offloads several sampling algorithms (TopK, TopP, Temperature, minK, minP, and multi-sequence sampling) to the GPU, improving quality, consistency, and accuracy of responses, while also increasing performance.\n* [Concurrency for QKV projections](https://github.com/ggml-org/llama.cpp/pull/16991): Support for running concurrent CUDA streams to speed up model inference. To use this feature, pass in the *â€“CUDA\\_GRAPH\\_OPT=1* flag.\n* [MMVQ kernel optimizations](https://github.com/ggml-org/llama.cpp/pull/16847): Pre-loads data into registers and hides delays by increasing GPU utilization on other tasks, to speed up the kernel.\n* [Faster model loading time](https://github.com/ggml-org/llama.cpp/pull/18012): Up to 65% model load time improvements on DGX Spark, and 15% on RTX GPUs.\n* [Native MXFP4 support on NVIDIA Blackwell GPUs](https://github.com/ggml-org/llama.cpp/pull/17906/): Up to 25% faster prompt processing on LLMs using the hardware-level NVFP4 fifth-generation of Tensor Cores on the Blackwell GPUs.",
          "score": 33,
          "created_utc": "2026-01-06 11:58:11",
          "is_submitter": true,
          "replies": [
            {
              "id": "ny0if39",
              "author": "maglat",
              "text": "stupid question. where exactly I need to set *â€“CUDA\\_GRAPH\\_OPT=1*Â ",
              "score": 3,
              "created_utc": "2026-01-06 14:42:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny0ja3m",
                  "author": "jacek2023",
                  "text": "GGML\\_CUDA\\_GRAPH\\_OPT is an env variable, so in the Linux shell you can use export",
                  "score": 8,
                  "created_utc": "2026-01-06 14:47:05",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "ny2arr7",
                  "author": "Overall-Somewhere760",
                  "text": "stupid question #2, what other variables are ok to set when running/compiling llamacpp ? I just used the one that enables cuda/gpu access.",
                  "score": 4,
                  "created_utc": "2026-01-06 19:38:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nycrcus",
              "author": "Rheumi",
              "text": "now a really stupid question. I use LM Studio for my local LLMs. The Llama.cpp would be updated if I update LM Studio, or do I also need to update the Nvidia driver?",
              "score": 1,
              "created_utc": "2026-01-08 06:31:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nycuddc",
                  "author": "jacek2023",
                  "text": "AFAIK, LM Studio is not open source, so itâ€™s probably hard to tell when specific changes from llama.cpp are integrated into LM Studio.",
                  "score": 1,
                  "created_utc": "2026-01-08 06:55:18",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxz9nk6",
          "author": "ghost_ops_",
          "text": "these performance gains are only for nvidia gpus?",
          "score": 72,
          "created_utc": "2026-01-06 09:24:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzdnf7",
              "author": "FullstackSensei",
              "text": "I think many also translate to gains on AMD when building for ROCm, since it translates CUDA to HIP at compile time. Of course, architecture specific optimizations won't translate.\n\nI have noticed a general uplift on my Mi50s over the past couple of months, after the amazing work of u/Remove_Ayys.",
              "score": 33,
              "created_utc": "2026-01-06 10:02:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzfd8a",
                  "author": "Remove_Ayys",
                  "text": "AMD optimizations are also in the works (with contributions from AMD engineers). But unsurprisingly the work put in by NVIDIA engineers specifically mostly benefits NVIDIA GPUs. Something like FP4 tensor cores for example also just doesn't exist on most hardware.",
                  "score": 44,
                  "created_utc": "2026-01-06 10:17:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxza104",
              "author": "cleverusernametry",
              "text": "I'm Hoping macs get some benefit as well?",
              "score": 4,
              "created_utc": "2026-01-06 09:27:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzroq1",
                  "author": "No_Conversation9561",
                  "text": "MLX has made significant improvements over the last year. The recent update is also great.",
                  "score": 9,
                  "created_utc": "2026-01-06 12:01:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny28pg2",
                  "author": "JustSayin_thatuknow",
                  "text": "Not a Mac lover here.. but why downvoting?",
                  "score": 0,
                  "created_utc": "2026-01-06 19:28:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny262nw",
              "author": "MoffKalast",
              "text": "You think the *Nvidia team* will help improve the competition? Yeah right, CUDA only.",
              "score": 1,
              "created_utc": "2026-01-06 19:16:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyb5fnu",
                  "author": "Hunting-Succcubus",
                  "text": "But they are helping intel, trump revealed that",
                  "score": 1,
                  "created_utc": "2026-01-08 00:50:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxzqdk2",
          "author": "Lissanro",
          "text": "Mainline llama.cpp in terms of token generation speed became quite good, getting very close to ik\\_llama.cpp. Prompt processing about twice as slow though, but still, it has been amazing progress, there have been so many optimizations and improvement in llama.cpp in the past year, and it has wider architecture support, making it sometimes the only choice. Nice to see they continue to improve token generation speeds. If prompt processing gets improved also in the future, it would be amazing.",
          "score": 21,
          "created_utc": "2026-01-06 11:51:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6kw7t",
          "author": "AfterAte",
          "text": "For QwenCoder3-30B-A3B @ 4K\\_XS on a 3090 in Linux:  \nold build (a month old probably): 170tk/s at 1st token and 150tk/s after 9K tokens  \nnew build (just built): 182tk/s at 1st token and 160tk/s after 9K\n\n(this does not change when I export GGML\\_CUDA\\_GRAPH\\_OPT=1)  \n  \nso it's \\~7% faster for me. Nothing like their numbers but if the quality remains the same (so far it feels the same), it's a win.",
          "score": 9,
          "created_utc": "2026-01-07 11:29:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzinhx",
          "author": "horriblesmell420",
          "text": "Any modern performance comparisons to vLLM?",
          "score": 6,
          "created_utc": "2026-01-06 10:47:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzdli6",
          "author": "No_Swimming6548",
          "text": "Time to update. Also, Nemotron 3 Nano optimization when?",
          "score": 12,
          "created_utc": "2026-01-06 10:01:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzhq4v",
              "author": "Serious_Molasses313",
              "text": "I would love a 20b Nemotron",
              "score": 2,
              "created_utc": "2026-01-06 10:39:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzmpeg",
                  "author": "No_Swimming6548",
                  "text": "Did you try nano 30b? It's pretty fast",
                  "score": 3,
                  "created_utc": "2026-01-06 11:21:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny4wffl",
          "author": "Repeat_Admirable",
          "text": "The efficiency gains are noticeable not just in tokens/sec, but in battery life for background apps. I built a wrapper around local Whisper for dictation, and a year ago it would heat up my laptop. Now with the latest optimizations (and quantization), I can leave it running 24/7 on my Mac and barely notice the power draw. Huge props to the maintainers pushing these limits.",
          "score": 4,
          "created_utc": "2026-01-07 03:28:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzffab",
          "author": "pmttyji",
          "text": "In the right side chart(DGX Spark), GPT-OSS-20B Numbers seems low comparing to 120B model. (OR 120B performs well(giving 50% of what 20B gives) better than 20B). Possibly few optimizations pending for 20B.",
          "score": 2,
          "created_utc": "2026-01-06 10:18:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxziw2o",
          "author": "am17an",
          "text": "They didn't put the PP results for these models, at least gpt-oss should have 30% gain in those as well due to the FP4 instructions on DGX spark. For TG it's mostly been a series of PRs for fusion with help from NVIDIA engineers. However the TG gains should be for AMD as well (at least I hope)",
          "score": 2,
          "created_utc": "2026-01-06 10:49:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny2h6sz",
          "author": "cibernox",
          "text": "Iâ€™ve also noticed performance gains over the last few months. I used to run 4B models in Q4 at 80tk/s last year and Iâ€™m consistently getting over 100tk/s now. In fact with some memory over clock I can run 8B dense models at 70tk/s now (when context is low). Thats quite amazing.",
          "score": 1,
          "created_utc": "2026-01-06 20:07:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny35gjr",
          "author": "Firenze30",
          "text": "I didn't find any performance gain updating from 7394 (CUDA 12.4) to 7642 (CUDA 13.1). GPT-OSS-120B.",
          "score": 1,
          "created_utc": "2026-01-06 21:59:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4225w",
          "author": "HarambeTenSei",
          "text": "And still no audio support",
          "score": 1,
          "created_utc": "2026-01-07 00:43:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4i2uq",
          "author": "AdventurousGold672",
          "text": "Can we already see the benefit of it?",
          "score": 1,
          "created_utc": "2026-01-07 02:09:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4q394",
          "author": "suicidaleggroll",
          "text": "I really wish they would provide more info.\n\nhttps://developer.nvidia.com/blog/open-source-ai-tool-upgrades-speed-up-llm-and-diffusion-models-on-nvidia-rtx-pcs/\n\n> Janâ€™26 builds are run with the following environment variables and flags: GGML_CUDA_GRAPH_OPT=1, FA=ON, and â€”backend-sampling\n\nOk, are those compiler flags?  Runtime flags?  Arguments to llama.cpp?  Is this a CUDA improvement or llama.cpp improvement?  Which version of which one has these new commits?\n\n> Concurrency for QKV projections: Support for running concurrent CUDA streams to speed up model inference. To use this feature, pass in the â€“CUDA_GRAPH_OPT=1 flag.\n\nI thought it was GGML_CUDA_GRAPH_OPT=1, and with the '-' in front that makes it look like a flag to llama.cpp rather than an environment variable, but llama.cpp flags aren't in all caps.\n\nDoes anyone know of a master list of the various environment variables and compiler flags available for llama.cpp and what they do?  There seems to be very little documentation on it.\n\nEdit: looking through the code, it looks like GGML_CUDA_GRAPH_OPT is an environment variable you have to set at runtime, it's not a compiler flag.  --backend-sampling is a command line arg to llama.cpp.  I see absolutely no mention of FA, maybe that's flash-attn?  If so that's already on by default though.\n\nEdit 2: looks like neither GGML_CUDA_GRAPH_OPT or --backend-sampling exist in ik_llama.cpp, hopefully those get ported over if they make such a large difference\n\nEdit 3: unfortunately --backend-sampling doesn't exist in llama-bench, so I can't test that, but I'm seeing absolutely no change from GGML_CUDA_GRAPH_OPT=1 on my RTX Pro 6000 system.",
          "score": 1,
          "created_utc": "2026-01-07 02:52:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny4qs3b",
              "author": "Glittering-Call8746",
              "text": "They use agentic workflow for everything.. could be heredocs from opus or sonnet. I always have problems with heredocs",
              "score": 1,
              "created_utc": "2026-01-07 02:56:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny60gpj",
              "author": "am17an",
              "text": "What model are you using?",
              "score": 1,
              "created_utc": "2026-01-07 08:25:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny74sof",
                  "author": "suicidaleggroll",
                  "text": "I was focused on MiniMax-M2.1 for those initial tests, I saw no change in performance, llama.cpp was still half the speed of ik_llama.cpp on pp and roughly the same tg.",
                  "score": 1,
                  "created_utc": "2026-01-07 13:41:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny4zj1d",
          "author": "LatentSpacer",
          "text": "Finally a W from NVIDIA.",
          "score": 1,
          "created_utc": "2026-01-07 03:46:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5ut4g",
          "author": "ab2377",
          "text": "Will the real apple engineers please stand up.",
          "score": 1,
          "created_utc": "2026-01-07 07:34:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8ycsm",
          "author": "Flashy_Management962",
          "text": "Imagine what could happen if ik llama cpp and llama cpp would merge :(",
          "score": 1,
          "created_utc": "2026-01-07 18:48:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybphxp",
          "author": "coding_workflow",
          "text": "Does this apply to blackwell? As I see some on DGX, what about Ampere architecture.  \nI noticed already build introduced some flags for blackwell and I had to exclude them to build for Ampere.",
          "score": 1,
          "created_utc": "2026-01-08 02:35:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzqxei",
          "author": "Ok_Warning2146",
          "text": "That's good news. From which release was this gain merged?",
          "score": 1,
          "created_utc": "2026-01-06 11:55:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzt3xk",
          "author": "__Maximum__",
          "text": "Is this merged into main of llama.cpp? What nvidia drivers? Any info at all?",
          "score": 1,
          "created_utc": "2026-01-06 12:11:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzm2i3",
          "author": "llama-impersonator",
          "text": "it's easy if you do it \"the amazon way\" by tanking the perf of recent builds so nvidia can come in and fix it",
          "score": -9,
          "created_utc": "2026-01-06 11:16:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzp7ju",
              "author": "jacek2023",
              "text": "Can you point to specific llama.cpp commits that tanked performance?",
              "score": 9,
              "created_utc": "2026-01-06 11:42:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxzpwsn",
                  "author": "llama-impersonator",
                  "text": "nope, i only rebuild when i need to for a new model i want to try specifically on lcpp, which is not that often. i use ik_llama more.",
                  "score": -13,
                  "created_utc": "2026-01-06 11:47:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxzt5fi",
                  "author": "Aggressive-Bother470",
                  "text": "Dood, you know there have been several instances :D",
                  "score": -10,
                  "created_utc": "2026-01-06 12:12:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxzq985",
              "author": "CheatCodesOfLife",
              "text": "I havenâ€™t had any performance regressions with Qwen3 235b",
              "score": 2,
              "created_utc": "2026-01-06 11:50:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzrl1s",
                  "author": "llama-impersonator",
                  "text": "prefill went down a bit for me, it was already super slow anyway so that was noticeable. ik_llama is several times faster in prompt processing when i use glm 4.7 anyway.",
                  "score": -5,
                  "created_utc": "2026-01-06 12:00:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxzzlc0",
          "author": "asraniel",
          "text": "How does this translate to ollama? I know, people hate ollama around here, but thats what i use.",
          "score": -10,
          "created_utc": "2026-01-06 12:56:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny0ilyl",
              "author": "my_name_isnt_clever",
              "text": "We don't know, that's part of the reason we don't like ollama. They tend to just do what they want, so you should ask them.",
              "score": 18,
              "created_utc": "2026-01-06 14:43:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny04kc3",
              "author": "Marksta",
              "text": "Depends if Ollama feels like claiming they're using their own engine today or not.",
              "score": 17,
              "created_utc": "2026-01-06 13:26:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny4ph3x",
              "author": "suicidaleggroll",
              "text": "They hate ollama because it's *significantly* slower than llama.cpp and offers basically nothing that warrants taking that hit.  Why use it?  You're just taking a massive performance penalty for no benefit.",
              "score": 1,
              "created_utc": "2026-01-07 02:49:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxzob99",
          "author": "Niwa-kun",
          "text": "hope i can use more grok/gemini/chatgpt now. damn rate limits.",
          "score": -17,
          "created_utc": "2026-01-06 11:35:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzouvu",
              "author": "jacek2023",
              "text": "could you clarify what you mean?",
              "score": 7,
              "created_utc": "2026-01-06 11:39:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxzp7pq",
                  "author": "Niwa-kun",
                  "text": "Greater performance = less their systems are being slammed by their users, which hopefully lifts the usage limits on flagship models.",
                  "score": -13,
                  "created_utc": "2026-01-06 11:42:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q6c9wc",
      "title": "DeepSeek-R1â€™s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1q6c9wc",
      "author": "Nunki08",
      "created_utc": "2026-01-07 10:49:12",
      "score": 635,
      "num_comments": 52,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "ny6w58o",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-07 12:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6ls96",
          "author": "qtvivies",
          "text": "https://preview.redd.it/t6ic0x3nywbg1.png?width=1965&format=png&auto=webp&s=ae5ef60128b8a0cf89351e8673ea41eddafb037c\n\nSomething interesting towards the end. Looks like someone forgot about this",
          "score": 138,
          "created_utc": "2026-01-07 11:36:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny8tl7d",
              "author": "Mikasa0xdev",
              "text": "86 pages? That's a weekend read.",
              "score": 13,
              "created_utc": "2026-01-07 18:28:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny8d6zv",
              "author": "Crisis_Averted",
              "text": "layman here: no idea what that implies.",
              "score": 2,
              "created_utc": "2026-01-07 17:15:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny8ejua",
                  "author": "Dany0",
                  "text": "Supervised fine-tuning and Reinforcement learning data. Used to post-train",
                  "score": 26,
                  "created_utc": "2026-01-07 17:21:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny8qvt2",
                  "author": "qtvivies",
                  "text": "They intended to release the (mostly synthetic tmk) data used for the post training of R1, except I assume that \\`xxx\\` was left in accidentally as a placeholder in place of a link. Or they changed their mind and just forgot to remove it.",
                  "score": 21,
                  "created_utc": "2026-01-07 18:16:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny6gjni",
          "author": "ResidentPositive4122",
          "text": "New arch about to drop? dsv4 + r2? Packing all the goodies learned from last year. Hopefully they try smaller sizes as well. Would be interesting to see how the arch improvements work at several sizes.",
          "score": 48,
          "created_utc": "2026-01-07 10:53:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6i96r",
              "author": "panic_in_the_galaxy",
              "text": "You don't update a paper with new results. You would just write a new paper. These are just explanations and clarifications.",
              "score": 62,
              "created_utc": "2026-01-07 11:08:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6if9m",
                  "author": "ResidentPositive4122",
                  "text": "No, I mean they've updated this with all they had, and preparing for a new arch. Like in closing a chapter.",
                  "score": 41,
                  "created_utc": "2026-01-07 11:09:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny7fp99",
                  "author": "ab2377",
                  "text": "but why so late?",
                  "score": 4,
                  "created_utc": "2026-01-07 14:39:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6lszd",
              "author": "swaglord1k",
              "text": "that would make sense. we'll finally see whether deepseek was a one trick pony or if they'll managed to drop another market-disrupting model",
              "score": 8,
              "created_utc": "2026-01-07 11:36:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6odal",
                  "author": "ForsookComparison",
                  "text": "R1-0528 and V3.2 were both equally SOTA-threatening especially when cost comes into play.\n\nThe big shocker was *\"you can do this without being a USA-based hyperscaler!?\"* and now that we know this to be true, I don't think we'll ever have a similar moment again.",
                  "score": 48,
                  "created_utc": "2026-01-07 11:56:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny6x67y",
                  "author": "Few_Painter_5588",
                  "text": "Uhm, no. Deepseek were always market disrupting. Deepseek v2, v3, V3 0324, R1, 3.2. They've only had three flops, V1, R1 0528 and V3.2 speciale.",
                  "score": 1,
                  "created_utc": "2026-01-07 12:56:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny8iz9u",
          "author": "warnerbell",
          "text": "The original paper was light on implementation specifics. If they've added more on how they got the reasoning behavior to emerge, that's valuable.",
          "score": 14,
          "created_utc": "2026-01-07 17:41:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7c3yc",
          "author": "Ok_Technology_5962",
          "text": "Current research is linear attention. DeepSeek 3.2 with the cache optimization and now they had a massive paper come out that puts the linear into the whole modelaking it possible to train more than 60 layers. So yes this one is done. The compute and thinking will now happen internally in 1000 layers",
          "score": 28,
          "created_utc": "2026-01-07 14:21:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9ne3n",
          "author": "CryptoUsher",
          "text": "honestly the fact they went back and added 60+ pages is kind of wild. most papers just release and call it a day, maybe a small erratum if something's broken.\n\n\n\nwonder if this was all stuff they had internally but couldn't publish initially, or if they're responding to community feedback and trying to explain their approach better. either way it's good for reproducibility.\n\n\n\nthe original paper was already dense but felt like it was missing implementation details. if they're actually filling in those gaps this could be huge for people trying to replicate or build on their work.",
          "score": 13,
          "created_utc": "2026-01-07 20:38:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny74o1r",
          "author": "jeffwadsworth",
          "text": "I was using the online chat version last night to improve a large Java class (40K tokens) with multiple methods.  It did so beautifully with zero issues in one shot.  The same task in my sub of Gemini 3 Pro chat interface failed in a few shots due to hallucinations.  They have really improved that model a lot from a year ago.",
          "score": 11,
          "created_utc": "2026-01-07 13:40:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycauta",
          "author": "badgerbadgerbadgerWI",
          "text": "The level of detail they're releasing is remarkable. This kind of transparency is what pushes the whole field forward. Really interested in their distillation approach - getting smaller models to match larger ones' reasoning is key for edge deployment.",
          "score": 5,
          "created_utc": "2026-01-08 04:35:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyae2ug",
          "author": "timfduffy",
          "text": "I think all this info was previously released as a [supplment to their R1 paper in Nature](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09422-z/MediaObjects/41586_2025_9422_MOESM1_ESM.pdf).",
          "score": 4,
          "created_utc": "2026-01-07 22:34:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9okeg",
          "author": "CryptoUsher",
          "text": "honestly the fact they went back and added 60+ pages is kind of wild. most papers just release and call it a day, maybe a small erratum if something's broken.\n\n\n\nwonder if this was all stuff they had internally but couldn't publish initially, or if they're responding to community feedback and trying to explain their approach better. either way it's good for reproducibility.\n\n\n\nthe original paper was already dense but felt like it was missing implementation details. if they're actually filling in those gaps this could be huge for people trying to replicate or build on their work.",
          "score": 1,
          "created_utc": "2026-01-07 20:43:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nybh79p",
              "author": "Imaginary-Bit-3656",
              "text": "I think it was part of getting the paper accepted into Nature, where that same additional information was included with the work as supplimentary material (I think the standards for Nature are considered quite high)  \n  \nMany of the papers we see on Arxiv are preprints, works that have not been peer reviewed and may not ever be published in a journal.",
              "score": 2,
              "created_utc": "2026-01-08 01:51:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nydlzki",
          "author": "Eyelbee",
          "text": "It's crazy it's only been one year, feels like ages",
          "score": 1,
          "created_utc": "2026-01-08 11:00:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6xeqo",
          "author": "Aggressive-Bother470",
          "text": "New grpo details perhaps? From reading the hf page it implied it was maybe light in that regard?Â ",
          "score": 1,
          "created_utc": "2026-01-07 12:58:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny76206",
          "author": "TelloLeEngineer",
          "text": "does arxiv have a diff UI?",
          "score": 1,
          "created_utc": "2026-01-07 13:48:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny7fy4u",
              "author": "Freonr2",
              "text": "Don't think so, but you can download the raw TeX format versions of the paper and see the appendix.tex is new and the largest .tex file, or look for diffs that way from the raw .tex files.",
              "score": 7,
              "created_utc": "2026-01-07 14:41:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6y8mn",
          "author": "yoshiK",
          "text": "I did quickly throw the two papers into Gemini. It's really fun to live in the future. \n\n#Gemini summary: \n\nThe paper **\"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\"** (arXiv:2501.12948) marks a significant milestone in open-source AI by demonstrating that advanced reasoning (similar to OpenAIâ€™s o1) can be achieved through large-scale Reinforcement Learning (RL) with minimal human-annotated data.\n\nThe two versions you provided represent the initial release (**v1**, Jan 22, 2025) and the latest updated version (which has been significantly expanded to **86+ pages** as of January 2026).\n\n### 1. Overall Paper Summary\nThe paper introduces two primary models:\n*   **DeepSeek-R1-Zero:** A model trained via \"pure RL\" (using the GRPO algorithm) starting directly from a base model without any Supervised Fine-Tuning (SFT). It demonstrates that reasoning behaviors like self-correction and reflection can emerge purely from reward signals.\n*   **DeepSeek-R1:** A more \"user-friendly\" version that uses a multi-stage pipeline (Cold-start SFT â†’ Reasoning RL â†’ Rejection Sampling/SFT â†’ General RL) to fix the \"readability\" and \"language mixing\" issues of R1-Zero while maintaining state-of-the-art reasoning performance.\n*   **Distillation:** The authors show that the reasoning patterns discovered by the 671B model can be distilled into smaller models (1.5B to 70B), allowing a 14B model to outperform much larger ones on math and coding benchmarks.\n\n---\n\n### 2. Comparison: Extensions in the New Version\nThe newer version is a massive technical expansion (growing from roughly 22 pages to over 85 pages). The key additions and extensions include:\n\n#### A. The \"Aha Moment\" Expansion (Section 2.2.1)\nThe new version provides a much deeper analysis of the **\"Aha Moment\"**â€”the point during RL training where the model unexpectedly learns to \"re-think\" its approach. The extension includes more qualitative examples and internal data showing the model's transition from linear solving to iterative self-correction without being prompted to do so.\n\n#### B. Detailed 4-Stage Training Pipeline\nWhile v1 outlined the stages, the new version details the specific composition of the **800k total training samples**:\n*   **Stage 1 (Cold Start):** Expanded details on the ~5,000-10,000 long CoT (Chain of Thought) samples used to \"prime\" the model.\n*   **Stage 3 (Rejection Sampling):** A deeper dive into how 600k reasoning-related and 200k non-reasoning samples were filtered and used to improve the model's general chat capabilities and prevent \"forgetting\" during the reasoning-heavy RL stages.\n\n#### C. Comprehensive Ablation Studies\nThe new version adds extensive \"What if?\" scenarios that were absent or brief in v1:\n*   **Distillation vs. RL:** New evidence explaining *why* distilling a large model's reasoning traces into a small model is more effective than training that small model directly with its own RL.\n*   **Base Model Impact:** Analysis of how different base models (DeepSeek-V3 vs. Qwen vs. Llama) respond to the R1 training recipe.\n\n#### D. Expanded \"Unsuccessful Attempts\" (Section 4.2)\nOne of the most valuable additions for researchers is the expanded section on what **did not work**. The new version elaborates on their failures with:\n*   **Process Reward Models (PRM):** Detailed reasons why step-level rewards were difficult to scale or prone to \"reward hacking\" compared to the outcome-based rewards used in R1.\n*   **Monte Carlo Tree Search (MCTS):** Technical explanation of why MCTS didn't provide the expected gains over simple RL in the context of LLM reasoning.\n\n#### E. New Benchmarks & Technical Specs\n*   **Updated Results:** Includes more recent evaluations on benchmarks like **AIME 2025**, **LiveCodeBench**, and specialized medical/legal reasoning tests.\n*   **Hyperparameters:** The new version includes exhaustive tables of training hyperparameters (learning rates, GRPO group sizes, KL divergence coefficients) which were previously withheld or summarized.\n\n### Summary Table\n| Feature | v1 (Original) | Latest Version (Extension) |\n| :--- | :--- | :--- |\n| **Page Count** | ~22 Pages | **86+ Pages** |\n| **Methodology** | High-level 4-stage overview | Granular detail on each stage (SFT, RL, Rejection Sampling) |\n| **Behaviors** | Mentions \"self-correction\" | Deep dive into \"Aha Moment\" with case studies |\n| **Failed Paths** | Brief mention of PRM/MCTS | Exhaustive analysis of why PRM and MCTS underperformed |\n| **Distillation** | Introduced 1.5B to 70B models | Added deep ablation on distillation efficiency and data filtering |\n| **Hyperparameters** | Partial/Summary | **Complete Technical Specs** for reproducibility |",
          "score": -8,
          "created_utc": "2026-01-07 13:03:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny74580",
              "author": "DigThatData",
              "text": "there is no section 2.2.1. section 2.2 is \"reward design\" and is only 4 short paragraphs plus a figure.\n\nI don't think LLMs are reliable for diff-ing. just use the `diff` utility instead, and then ask the LLM to explain the patch.",
              "score": 30,
              "created_utc": "2026-01-07 13:37:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny85k2j",
                  "author": "yoshiK",
                  "text": "The discussion of the a-ha moment is actually section 2.3. Though I didn't check too closely since the summary is anyhow that I probably need to sit down and read the whole thing at some point.",
                  "score": -1,
                  "created_utc": "2026-01-07 16:41:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny7l7fj",
              "author": "pigeon57434",
              "text": "i dont understand people who just post completely slop summaries into comment sections by AI if we wanted a summary we would ask a model ourselves since it requires no effort",
              "score": 11,
              "created_utc": "2026-01-07 15:07:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7v3vu",
                  "author": "menictagrib",
                  "text": "The new version of someone with no background knowledge answering a technical question for dopamine by repeating whatever they see on the first page of Google",
                  "score": 7,
                  "created_utc": "2026-01-07 15:54:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny7pe83",
                  "author": "arguingwithabot",
                  "text": "I hear you, but at the end of the day they saved you a few clicks, keystrokes and tokens.",
                  "score": 3,
                  "created_utc": "2026-01-07 15:27:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny84z5q",
                  "author": "Chickenbuttlord",
                  "text": "Nobody asked for your opinion buddy, yes we do very much need summary posted into a comment incase we're too lazy to do it ourselves. Thank you op!",
                  "score": -1,
                  "created_utc": "2026-01-07 16:38:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q4x5e9",
      "title": "For the first time in 5 years, Nvidia will not announce any new GPUs at CES â€” company quashes RTX 50 Super rumors as AI expected to take center stage",
      "subreddit": "LocalLLaMA",
      "url": "https://www.tomshardware.com/pc-components/gpus/for-the-first-time-in-5-years-nvidia-will-not-announce-any-new-gpus-at-ces-company-quashes-rtx-50-super-rumors-as-ai-expected-to-take-center-stage",
      "author": "FullstackSensei",
      "created_utc": "2026-01-05 20:31:51",
      "score": 620,
      "num_comments": 198,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/",
      "domain": "tomshardware.com",
      "is_self": false,
      "comments": [
        {
          "id": "nxx2hx4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-06 00:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw3fm6",
          "author": "Long_comment_san",
          "text": "They can announce a new card to be put into production: RTX 3060",
          "score": 107,
          "created_utc": "2026-01-05 21:29:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwif2x",
              "author": "BasicBelch",
              "text": "I wonder why the 3060?    Was it a samsung fab card?",
              "score": 8,
              "created_utc": "2026-01-05 22:40:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxwl2j4",
                  "author": "commanderthot",
                  "text": "Yes, all GeForce 3000 was Samsung fab 8nm",
                  "score": 22,
                  "created_utc": "2026-01-05 22:54:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxx7lsu",
                  "author": "Trick-Force11",
                  "text": "its a combo of the easier to source GDDR6 + Samsung 8nm",
                  "score": 13,
                  "created_utc": "2026-01-06 00:51:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxwwojm",
                  "author": "AmericanNewt8",
                  "text": "Well I imagine GDDR6 to start, but maybe they held onto the masks or whatnot? 3060s remained abundant for a long while.Â ",
                  "score": 3,
                  "created_utc": "2026-01-05 23:54:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxwx3vm",
              "author": "chuckaholic",
              "text": "What a slap in the face to gamers. Gamers that bought their products for years before AI even existed. We built that company. This is like asking a girl on a date and she offers to let you house sit while she goes to Dubai.",
              "score": 33,
              "created_utc": "2026-01-05 23:57:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxz69e7",
                  "author": "kingsleyopara",
                  "text": "This seems oddly specific, how was the house sitting?",
                  "score": 13,
                  "created_utc": "2026-01-06 08:51:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxxyj0f",
                  "author": "Finanzamt_Endgegner",
                  "text": "I agree but at the very least game devs are now forced to actually optimize their games again which was lacking for years. I don't want new games every year which don't look better than 5y ago but take 2x the resources.",
                  "score": 24,
                  "created_utc": "2026-01-06 03:18:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxy2n2w",
                  "author": "Nobby_Binks",
                  "text": "Yeah my first card was a TNT2. That how long I've been giving Jensen my money. Shareholders call the shots I guess.",
                  "score": 9,
                  "created_utc": "2026-01-06 03:42:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxykc18",
                  "author": "GaboureySidibe",
                  "text": "So buy from AMD",
                  "score": 9,
                  "created_utc": "2026-01-06 05:39:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxyh1lb",
                  "author": "fish312",
                  "text": "that's what we get for believing that companies owe any loyalty to their customers.",
                  "score": 4,
                  "created_utc": "2026-01-06 05:14:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxz81yf",
                  "author": "Jokerit208",
                  "text": "Were they supposed to announce a new series of supers that cost three times what the base versions do? Look around, bud. There's not going to be any hardware innovation from any of these companies any time soon.",
                  "score": 2,
                  "created_utc": "2026-01-06 09:08:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxys9x9",
              "author": "UsefulOwl2719",
              "text": "\"All new GTX 970 with 4GB of VRAM\"",
              "score": 6,
              "created_utc": "2026-01-06 06:43:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxytt4v",
                  "author": "pixelpoet_nz",
                  "text": "3.5 GB",
                  "score": 10,
                  "created_utc": "2026-01-06 06:56:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxyh7hi",
              "author": "Ecstatic_Winter9425",
              "text": "They should also reduce the vram to 2GB.",
              "score": 3,
              "created_utc": "2026-01-06 05:16:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny0o8oa",
              "author": "BasicBelch",
              "text": "from benchmarks looks like it could easily slot in below the 5050 and they could label it a 5040 or 5040ti\n\n  \nIts not the worst idea in the world",
              "score": 1,
              "created_utc": "2026-01-06 15:11:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxvrgs3",
          "author": "Clear_Anything1232",
          "text": "We asked for local models\n\nNow we will be lucky to keep local computing of any kind\n\nCorporate greed on steroids",
          "score": 326,
          "created_utc": "2026-01-05 20:33:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvs970",
              "author": "stiflers-m0m",
              "text": "at least i can FINALLY justify getting my threadripper 5 series with 256 gb memory last year.",
              "score": 101,
              "created_utc": "2026-01-05 20:37:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxvt8th",
                  "author": "Clear_Anything1232",
                  "text": "You are an investor now. No longer a computer enthusiast ðŸ˜‚",
                  "score": 117,
                  "created_utc": "2026-01-05 20:42:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxvucq4",
                  "author": "Forgot_Password_Dude",
                  "text": "Lol I got 512gb ddr4 tho last year as well from  256gb upgrade.  Now I have too much old RAM",
                  "score": 8,
                  "created_utc": "2026-01-05 20:47:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyg63pv",
                  "author": "cogman10",
                  "text": "I built an overspeced home server 5 years ago with 128gb of DDR4... Man was that a good choice that I didn't realize I was making.",
                  "score": 1,
                  "created_utc": "2026-01-08 18:53:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxwtnpr",
              "author": "Admirable-Star7088",
              "text": ">Now we will be lucky to keep local computing of any kind\n\nThen computers are no longer something I'm interested in using. I'll have to find another activity in the real world. The computer age was fun as long as it lasted.",
              "score": 12,
              "created_utc": "2026-01-05 23:38:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxxv825",
                  "author": "Corporate_Drone31",
                  "text": "I hear fountain pens are all the rage.",
                  "score": 9,
                  "created_utc": "2026-01-06 02:59:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxw5cc3",
              "author": "dogesator",
              "text": "Nvidia is literally one of the biggest publishers of open models on Huggingface",
              "score": 14,
              "created_utc": "2026-01-05 21:38:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxw790c",
                  "author": "lewd_robot",
                  "text": "Doesn't matter if a consumer GPU with a moderate amount of VRAM costs as much as an enterprise unit used to.",
                  "score": 26,
                  "created_utc": "2026-01-05 21:47:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxwl8cm",
                  "author": "No_Afternoon_4260",
                  "text": "Nvidia is literally one of the biggest publishers of **quality** open models on Huggingface",
                  "score": 2,
                  "created_utc": "2026-01-05 22:55:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny0iqwm",
              "author": "LegacyRemaster",
              "text": "RTX 6000 96gb + 128gb of ram. Minimax, glm 4.7 , gpt 120... I can wait another 2 years to buy more (and  I have 5070ti + 3060ti + 2070 super + rtx 580",
              "score": 2,
              "created_utc": "2026-01-06 14:44:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxvw1d0",
              "author": "Confusion_Senior",
              "text": "China will fill that market",
              "score": 7,
              "created_utc": "2026-01-05 20:55:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxvwtp3",
                  "author": "Far_Cat9782",
                  "text": "Not if trump has a day about that. Dji anyone? Electric cars? Americans don't like competition with their oligarchs especially since most of the economy is prepped up by a couple tech companies",
                  "score": 11,
                  "created_utc": "2026-01-05 20:58:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxw748i",
                  "author": "lewd_robot",
                  "text": "For the rest of the world. The US is burning bridges left and right. If you're in the bottom 50% of Americans, you're not even living in the developed world anymore. Your life is closer to that of someone living in a developing nation. And that trend is growing. More and more people are slipping under that threshold. And it's getting worse at an accelerating rate now.",
                  "score": 4,
                  "created_utc": "2026-01-05 21:46:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxyq8bp",
              "author": "strategos",
              "text": "Not corporate greed, just common business sense.\n\nPray every night for the AI bubble to crash so you can have all the silicon you need at dirt cheap prices. Remember this is also how the modern internet was built. Lot of fiber was deployed using debt, when the dotcom bubble popped, all that fiber had to be utilised and that's how we get cheap modern internet.",
              "score": 2,
              "created_utc": "2026-01-06 06:25:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxysupe",
                  "author": "Clear_Anything1232",
                  "text": "The issue is unlike fiber, silicon fabs tend to create oligopolies due to the heavy capital investment needed.\n\nI really wish we had a slower but less capital intensive way of manufacturing transistors. This would enhance competition and hobby level fabrication similar to what 3d printing did to plastic products. Currently semi conductors is a cartel similar to opec+ just not formally declared.",
                  "score": 4,
                  "created_utc": "2026-01-06 06:47:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxw7czu",
              "author": "twilliwilkinsonshire",
              "text": "Oh yeah corporate greed is NOT selling products they could shift like hotcakes. /s  \nCorporate greed would be if they sold as much as humanly possible with zero care to overbuilding, flooding the market with an inevitable crash in the future. They could straight up sell preorders for cards not even in existence to consumers at this point.\n\nAlways surprises me how utterly moronic most commentary on this stuff is, most people have next to zero understanding of economics, those that do seem to only apply half the knowledge and drool on the rest of their 'master market thesis'. Yes, they are a for profit company, yes there are going to be individual greedy choices, but calling literally everything corporate greed Is just commieslop.",
              "score": 2,
              "created_utc": "2026-01-05 21:47:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxwi8le",
                  "author": "BasicBelch",
                  "text": "I really expected more intelligence and reason from this sub.   Instead we get the same idiotic uninformed yelling and namecalling as the rest of Reddit.   Maybe they are all bots or something?  (wouldnt that be ironic)",
                  "score": 0,
                  "created_utc": "2026-01-05 22:40:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxx4q83",
                  "author": "twilliwilkinsonshire",
                  "text": "u/BasicBelch When your username itself is a 'clever' insult your grandstanding loses some weight, doubly so when you preemptively block so you can -directly- insult intelligence without any pushback.\n\nPeople just want to be able to spout nonsense with zero pushback, I know my statement is unpopular, I just am tired of the loudest common denominator being the training data that AI's are trained on so I am at least going to put out what I think too.",
                  "score": 1,
                  "created_utc": "2026-01-06 00:36:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxz7xhe",
              "author": "Guinness",
              "text": "Hey Intel. Grandma is handing you a golden opportunity.",
              "score": 1,
              "created_utc": "2026-01-06 09:07:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny6x6k1",
              "author": "auradragon1",
              "text": "They donâ€™t have enough RAM to launch new Super GPUs. Not corporate greed on steroids.",
              "score": 1,
              "created_utc": "2026-01-07 12:56:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nycbn40",
              "author": "Visual_Crew_792",
              "text": "Don't worry, once the accountants realize that there aren't enough customers in the world to pay for all this, you'll be able to scoop this shit up at rock bottom prices from bankrupt startups",
              "score": 1,
              "created_utc": "2026-01-08 04:40:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxyhvg9",
              "author": "DrummerPrevious",
              "text": "So greedy that they will not be able to produce shit and pop like a balloon loll",
              "score": 0,
              "created_utc": "2026-01-06 05:20:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxvziah",
          "author": "fallingdowndizzyvr",
          "text": "3060 forever!!!!",
          "score": 18,
          "created_utc": "2026-01-05 21:11:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxx7qo",
              "author": "xyzzs",
              "text": "Kind of good news for my poor old 306012g, looks like it will be the go to card (itâ€™s still #1 on steam) for us poorâ€™s for another few years at least.",
              "score": 7,
              "created_utc": "2026-01-06 03:10:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwcgwt",
          "author": "Aggressive-Bother470",
          "text": "We need China to get on the case now and flood ebay with 48 / 96GB cards.Â ",
          "score": 65,
          "created_utc": "2026-01-05 22:11:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwp9yy",
              "author": "dododragon",
              "text": "There are super x laptops coming with AMD AI 395 Chips and 128GB unified memory.\n\nNot as quick as nvidia vram, but not a bad consolation.\n\nhttps://onexplayerstore.com/pages/super-x-preview",
              "score": 21,
              "created_utc": "2026-01-05 23:15:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxyub9m",
                  "author": "pixelpoet_nz",
                  "text": "of course it has to be a laptop...",
                  "score": 4,
                  "created_utc": "2026-01-06 07:00:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxwjqwp",
          "author": "ArtfulGenie69",
          "text": "It's 3090's forever guys. No 5070/80 super 24gb or anything lol.Â ",
          "score": 15,
          "created_utc": "2026-01-05 22:47:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxxvp5",
          "author": "PersonOfDisinterest9",
          "text": "This really blows.  \n   \nI haven't built a new desktop in over a decade, I've been living off of desktop replacement laptops.  \nEvery year it's been like \"Maybe I should just lay down the money, everything keeps getting way more expensive every year\", but for a while it didn't make sense because I was never home, and it was going to be a $5k box just sitting and doing nothing.  \nBitcoin miners fucked the GPU market, then it was NFTs and crypto, then AI started blowing up, then the pandemic, and then AI *really* went bonkers...  \n   \nAt this point, it's literally: do I want a new computer, or do I want to delay having enough for a down payment on a mortgage for another year or two?  \n   \nI should just suck it up and finally build a computer, it would probably break the curse and the cost of all components would drop to 1/10 of the price, one day after the return period on the new gear passes.",
          "score": 14,
          "created_utc": "2026-01-06 03:14:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz8dwd",
              "author": "Jokerit208",
              "text": "Things were fine six months ago. That was your window.\n\nI'd just pay the prices, but you do you.",
              "score": 5,
              "created_utc": "2026-01-06 09:11:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzyoxr",
                  "author": "PersonOfDisinterest9",
                  "text": "Things were not fine 6 months ago, things haven't been fine for a decade, it's just gotten increasingly worse, while people accept it as the new normal.",
                  "score": 5,
                  "created_utc": "2026-01-06 12:50:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxz2j9s",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2026-01-06 08:15:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny1e0uo",
                  "author": "Context_Core",
                  "text": "I bought 96gb of DDR5 6000mhz RAM in June and felt like a RETARD at the time knowing itâ€™s totally overkill. \n\nIâ€™ve never felt so justified for being a retard before. Itâ€™s nice LOL",
                  "score": 4,
                  "created_utc": "2026-01-06 17:10:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxz4luq",
                  "author": "PersonOfDisinterest9",
                  "text": "I have no special insights, I am just assuming that eventually some kind of market forces are going to come into play. \nThere's got to be at least a few CEO creaming their pants over RAM prices right now and trying to get in on it, so what I'm *hoping* for, is that there's a massive rush to produce RAM, and then a market crash where they have to sell near manufacturing cost to try any recover something.  \n  \nThat's it. The plan is to hope that short-sighted greed and competition saves the day for some reason.   \nEither that, or my next job pays $40k a year more so I can justify spending half that on a new computer.",
                  "score": 3,
                  "created_utc": "2026-01-06 08:35:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxw66bn",
          "author": "Desperate-Sir-5088",
          "text": "I'm so proud I boughtÂ  EVGA 3090ti from local market at $600 as the Xmas gift. I just invested in the future.Â ",
          "score": 31,
          "created_utc": "2026-01-05 21:42:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwcbsq",
              "author": "FullstackSensei",
              "text": "Been trying to get an EVGA 3090 XC3 for months. It's the only dual slot 3090 that's not a blower.",
              "score": 9,
              "created_utc": "2026-01-05 22:10:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxwnzvp",
                  "author": "-InformalBanana-",
                  "text": "Hi, sorry for the inconvenience, but could you maybe explain why it matters what is the exact version of rtx 3090 or 3090 ti? Thanks.",
                  "score": 3,
                  "created_utc": "2026-01-05 23:09:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxze8hv",
                  "author": "alex_bit_",
                  "text": "Itâ€™s a little bit more than two slots, but I get it anyway.",
                  "score": 1,
                  "created_utc": "2026-01-06 10:07:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxwihed",
              "author": "BasicBelch",
              "text": "came so close to buying one a few months back but thought the prices would continue to go down.   oh well.",
              "score": 4,
              "created_utc": "2026-01-05 22:41:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxw6qm9",
          "author": "dakjelle",
          "text": "My 4080S is a surprise 1080 I never thought it would be.",
          "score": 12,
          "created_utc": "2026-01-05 21:44:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxy0n42",
          "author": "Sea_Succotash3634",
          "text": "\"Consumer\" Electronic Show",
          "score": 10,
          "created_utc": "2026-01-06 03:30:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwb1iz",
          "author": "shortsteve",
          "text": "If that's the case they should be kicked out of CES. It's the Consumer Electronics Show not the Enterprise AI shareholders meeting.",
          "score": 41,
          "created_utc": "2026-01-05 22:04:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzvruz",
              "author": "FalselyHidden",
              "text": "All future Nvidia GPUs will be AI GPUs that outputs fake frames to show your desktop. There will not be a 6000 series.",
              "score": 1,
              "created_utc": "2026-01-06 12:30:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxs3p0",
          "author": "dmter",
          "text": "it's no greed issue, it's monopoly issue. why bother making any r&d if there is no competition (amd is controlled by a relative lol) and demand is so high anyways so no need to develop to make people buy new stuff like before.\n\nonly hope is chinese competition.",
          "score": 7,
          "created_utc": "2026-01-06 02:42:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvzkna",
          "author": "iswasdoes",
          "text": "Imagine if the 50 series were the last high end consumer GPUs",
          "score": 33,
          "created_utc": "2026-01-05 21:11:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxw0hvv",
              "author": "FullstackSensei",
              "text": "The market abhors a vacuum.\n\nIf Intel has enough GDDR supply (secured contracts before the shit show), they might very will be breathing up Nvidia's neck in a few years. Plus, the Chinese are also coming.\n\nIf anything, the GPU market will be a lot more competitive when the AI bubble bursts in a few years, and Nvidia will loose much of its dominance.",
              "score": 29,
              "created_utc": "2026-01-05 21:16:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxx5u9h",
                  "author": "delicious_fanta",
                  "text": "*The non monopolized/oligopilized market.\n\nWhen youâ€™re talking about 3 companies controlling 100% of a product, vacuums are very, very welcome as that means they can triple their prices, thereby inflating their income thereby inflating their stock price.\n\nThere is only one rule american capitalism must abide by - the stock always goes up. There is no second rule.\n\nIn a normal market, competitors would pop up and fill the vacuum, and your statement would be correct. In this specific set of markets itâ€™s effectively impossible for any company not already producing this technology to be capable of producing this technology.\n\nSo, as you mention, another, existing, producer would have to re-tool. But why would they? They are all making more money doing what they are currently doing than they would by doing that.\n\nThere simply is no incentive to make things cheaper when they are already selling their entire volume of product at exponentially elevated rates. It would be irrational to not continue down the current path.",
                  "score": 14,
                  "created_utc": "2026-01-06 00:42:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxwdrba",
                  "author": "pinmux",
                  "text": "Intel owns lots of fabs. How hard would it be for them to retool to produce memory on a few lines?\n\nGot to be enough margin now to at least consider it?Â ",
                  "score": 2,
                  "created_utc": "2026-01-05 22:17:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxw3z3h",
              "author": "Odd-Ordinary-5922",
              "text": "good change it very well could be. imo theres a lot of catching up todo on the optimization side of things rather than the physical side.",
              "score": 3,
              "created_utc": "2026-01-05 21:32:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxwl9q9",
              "author": "lolwutdo",
              "text": "and your gpu cable burns up",
              "score": 1,
              "created_utc": "2026-01-05 22:55:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxw6j7h",
          "author": "ChainOfThot",
          "text": "Picked up a 5090 rig with 64gb ram, a 5070ti for my older pc and a laptop with 32gb of ddr5 before the ram shortage, feeling good. Also have a 4tb gen5 ssd I bought in April I haven't even opened yet, it's also doubled in price.",
          "score": 10,
          "created_utc": "2026-01-05 21:43:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny30762",
              "author": "Tarekun",
              "text": "Where did you get the 5090 from? How reliable are sources for modded card like that?",
              "score": 1,
              "created_utc": "2026-01-06 21:35:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny31dpr",
                  "author": "ChainOfThot",
                  "text": "It's a regular 5090, 64gb is system ram, mentioned cuz ram prices are crazy too rn",
                  "score": 1,
                  "created_utc": "2026-01-06 21:40:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxvw027",
          "author": "TACO_NV",
          "text": "why they would ? there's no competition.",
          "score": 20,
          "created_utc": "2026-01-05 20:55:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyo9qp",
              "author": "PeakBrave8235",
              "text": "Apple is literally slaughtering them but okay",
              "score": -4,
              "created_utc": "2026-01-06 06:09:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxyujy7",
                  "author": "pixelpoet_nz",
                  "text": "literally, ofc",
                  "score": 4,
                  "created_utc": "2026-01-06 07:02:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxh2tp",
          "author": "Badger-Purple",
          "text": "welp, I just went to microcenter and returned my 3090ti that I got for 700 end of november...bad move.",
          "score": 4,
          "created_utc": "2026-01-06 01:42:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw2rqn",
          "author": "T_UMP",
          "text": "https://preview.redd.it/zg51khp7mlbg1.png?width=420&format=png&auto=webp&s=29c01e632095e2f3262bd9a5c1555d45f48c9a87",
          "score": 10,
          "created_utc": "2026-01-05 21:26:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwuhua",
          "author": "goodtimtim",
          "text": "the inevitable crash is going to be really bad for my 401k, but amazing for my hobby",
          "score": 9,
          "created_utc": "2026-01-05 23:43:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvv6rp",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 28,
          "created_utc": "2026-01-05 20:51:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvw89p",
              "author": "FullstackSensei",
              "text": "People keep repeating this, but I don't buy it for one second. OpenAI doesn't nearly have the the cash for such commitments and all the players know it.\n\nAll the big players contributed to this. MS, Amazon, Meta, Twitter, Oracle, etc are all part of this. OpenAI didn't book all the memory supply for 2026 from Samsung and SK Hynix. The hyperscalers did.\n\nIt's an arms race, and OpenAI is actually a much smaller player in this than most think.",
              "score": 43,
              "created_utc": "2026-01-05 20:56:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxw3xhq",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 14,
                  "created_utc": "2026-01-05 21:31:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxvz01j",
                  "author": "-p-e-w-",
                  "text": "This. The idea that OpenAI single-handedly bought out the worldâ€™s RAM production (supposedly without even needing it) is the dumbest thing Iâ€™ve heard in a long time. They are worth *one tenth* of Google, Microsoft etc. They donâ€™t have anywhere near enough clout to do that.",
                  "score": 16,
                  "created_utc": "2026-01-05 21:09:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxw2zf1",
                  "author": "BakerXBL",
                  "text": "> Samsung Electronics Co.'s profit fell for the first time since 2023 [in Q2 2025], with a 56% plummet in operating income for the June quarter. \n\nDid they have a choice?",
                  "score": 1,
                  "created_utc": "2026-01-05 21:27:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxw5g8y",
          "author": "ufos1111",
          "text": "well maybe with a year's delay on next gen models they can figure out how to stop power connectors from melting ffs lmao",
          "score": 5,
          "created_utc": "2026-01-05 21:38:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxvx3t",
          "author": "segmond",
          "text": "It's actually a good thing there's hardware squeeze, it's going to force innovation on the software side.   We will figure out how to infer faster with what we have and we will get smaller models.   It's a temporary discomfort that's much needed.  The problem with \"easy/free\" money is that folks stop optimizing and go in all brute forcing.   Resource constraints breeds resourcefulness.",
          "score": 5,
          "created_utc": "2026-01-06 03:03:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwdlgz",
          "author": "DoomFist007",
          "text": "atp i dont know if i should keep my 3070 or sell it for more since i now have a 5070 ti",
          "score": 2,
          "created_utc": "2026-01-05 22:17:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwdv2f",
              "author": "FullstackSensei",
              "text": "Keep it in case something happens to your 5070Ti. Doubt warranty will be able to replace it.",
              "score": 8,
              "created_utc": "2026-01-05 22:18:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxwe2ao",
                  "author": "DoomFist007",
                  "text": "Fair. I did already register it for warranty but i might keep it",
                  "score": 1,
                  "created_utc": "2026-01-05 22:19:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxy11m4",
          "author": "TheManicProgrammer",
          "text": "I guess my 3050 laptop with 4gb Vram will have to play for another few years haha",
          "score": 2,
          "created_utc": "2026-01-06 03:32:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxy3wzl",
          "author": "Hunting-Succcubus",
          "text": "Why not 4060 or 2060?",
          "score": 2,
          "created_utc": "2026-01-06 03:49:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyvr52",
          "author": "Zyj",
          "text": "$1460 for 128GB? Why are people not buying all Strix Halo systems they can get their hands on?",
          "score": 2,
          "created_utc": "2026-01-06 07:13:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6hvsj",
          "author": "SomeWonOnReddit",
          "text": "I hate AI so much man. I don't want to pay for these higher prices because some people need AI for meme videos, meme pictures and vibe coders.",
          "score": 2,
          "created_utc": "2026-01-07 11:05:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwkonk",
          "author": "Orlandocollins",
          "text": "Jensens keynote is going to be a total snooze fest.",
          "score": 1,
          "created_utc": "2026-01-05 22:52:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwluxv",
              "author": "FullstackSensei",
              "text": "I was watching it. Was very interesting with tons of innovatiion, except for the minor detail that we'll never be able to run that hardware at home...",
              "score": 5,
              "created_utc": "2026-01-05 22:58:14",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxwv593",
              "author": "T_UMP",
              "text": "The more you snooze, the more you sleep.",
              "score": 1,
              "created_utc": "2026-01-05 23:46:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxkab3",
          "author": "squachek",
          "text": "https://www.reddit.com/r/nvidia/s/IwCGtNNCOF?",
          "score": 1,
          "created_utc": "2026-01-06 01:59:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxy5zpi",
          "author": "Ok_Warning2146",
          "text": "Can they make a 70W 4050?",
          "score": 1,
          "created_utc": "2026-01-06 04:01:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxy8f0m",
          "author": "pabskamai",
          "text": "Happy 3080 ownerâ€¦.",
          "score": 1,
          "created_utc": "2026-01-06 04:17:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzluc3",
          "author": "jfp1992",
          "text": "@amd nows your chance to get consumer GPU market share in time for the ai bubble to pop in a year or so",
          "score": 1,
          "created_utc": "2026-01-06 11:14:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzp2ch",
              "author": "FullstackSensei",
              "text": "They're in the same boat as Nvidia. I bet you Nvidia wishes they could get enough wafers from TSMC and enough VRAM and HBM from micron/Samsung/Hynix to make GPUs for everyone, but supply is constrained for everyone",
              "score": 1,
              "created_utc": "2026-01-06 11:41:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny0ga3y",
                  "author": "jfp1992",
                  "text": "That's a fair shout, they have GPU allocation but getting the memory modules is hard because other corp bought them all",
                  "score": 1,
                  "created_utc": "2026-01-06 14:31:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny36gso",
          "author": "TinFoilHat_69",
          "text": "I remember when video cards were more expensive than game consoles, now that video cards can make game consoles obsolete the rug has been pulled. Game consoles will now be cheaper than computers againâ€¦",
          "score": 1,
          "created_utc": "2026-01-06 22:03:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny39bjt",
              "author": "FullstackSensei",
              "text": "Good point! Only issue is: AMD supplies the APUs for both the Xbox and Playstation, and both consoles compete for the same wafer capacity as HBM.\n\nWill definitely be interesting to see how the next cycle of consoles plays out if MS and Sony haven't already secured capacity.",
              "score": 1,
              "created_utc": "2026-01-06 22:17:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyd5msn",
          "author": "BeAlch",
          "text": "it makes sense : if you sell all your stock to ai tech giants .. it would be strange to say that there's already a more powerful card in town, when they haven't yet train their next model with current hardware they just bought at high price.  \nAs for users it is so expensive that the sole way there will be sold to users is second hand from AI giants when they'll buy the next generation, idem for RAM.  \nthe sole way we could get graphic card at normal price again is that efficient AI only chips surpass GPU in price and performance + optimization in hardware software and models",
          "score": 1,
          "created_utc": "2026-01-08 08:33:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw2nh5",
          "author": "a_beautiful_rhind",
          "text": "Except we needed that super for AI.. to be the new 3090.",
          "score": 1,
          "created_utc": "2026-01-05 21:26:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxw4ls6",
              "author": "getmevodka",
              "text": "Guess what prices for 3090do. Staying stable xD",
              "score": 6,
              "created_utc": "2026-01-05 21:35:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxx4i49",
          "author": "erdirck",
          "text": "short term, but long term, the AI hype will be gone and everything will be back to normal",
          "score": 0,
          "created_utc": "2026-01-06 00:35:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyrbjf",
              "author": "Substantial-Ebb-584",
              "text": "Yeah, not that long ago I was telling myself - the mining hype will be gone and everything will be back to normal.",
              "score": 5,
              "created_utc": "2026-01-06 06:34:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxx5cuz",
              "author": "FullstackSensei",
              "text": "In the long term, we're all dead. -- John Maynard Keynes\n\nThe big players have all the political backing they want. It'll be at least a couple more years (2028) until the music stops.",
              "score": 6,
              "created_utc": "2026-01-06 00:39:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxz5djp",
                  "author": "Maleficent-Ad5999",
                  "text": "and then quantum computing or some other shit takes over",
                  "score": 1,
                  "created_utc": "2026-01-06 08:42:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxdtkh",
          "author": "usernameplshere",
          "text": "I've no words",
          "score": 0,
          "created_utc": "2026-01-06 01:25:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz5ea0",
              "author": "Maleficent-Ad5999",
              "text": "\\#",
              "score": 1,
              "created_utc": "2026-01-06 08:42:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxynyrm",
          "author": "Devil_Bat",
          "text": "Remember when RTX 4080 12GB happened that the trillion dollar slop machine managed to prepare new cardboards for RTX 4070 Ti?\nWait for the super.",
          "score": 0,
          "created_utc": "2026-01-06 06:07:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz02y4",
          "author": "IngwiePhoenix",
          "text": "I am not surprised, in the slightest.\n\nWell, I do wonder if they will ever crawl back to gamers when the bubble bursts...would be hella funny. x) But, doubtful. They'll find an excuse.",
          "score": 0,
          "created_utc": "2026-01-06 07:52:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw5dtg",
          "author": "chub0ka",
          "text": "Oh how wrong you are but lets seeâ€¦",
          "score": -10,
          "created_utc": "2026-01-05 21:38:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwbzue",
              "author": "FullstackSensei",
              "text": "Wrong about what?",
              "score": 5,
              "created_utc": "2026-01-05 22:09:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxwcgi9",
                  "author": "chub0ka",
                  "text": "About no new gpu announced. Watching now. Vera rubin just started",
                  "score": -11,
                  "created_utc": "2026-01-05 22:11:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q4s8t3",
      "title": "llama.cpp performance breakthrough for multi-GPU setups",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/ohxtu0l8hkbg1.jpeg",
      "author": "Holiday-Injury-9397",
      "created_utc": "2026-01-05 17:37:58",
      "score": 559,
      "num_comments": 173,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxuy5ve",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-05 18:20:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxusqli",
          "author": "MelodicRecognition7",
          "text": "I think details are here https://github.com/ikawrakow/ik_llama.cpp/pull/1080 not on that paid slop website",
          "score": 157,
          "created_utc": "2026-01-05 17:55:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxutny3",
              "author": "One-Macaron6752",
              "text": "\"PP performance for more than 4 GPUs is likely to be bad. Why? It looks like I'm not using NCCL correctly. PP and TG performance are both excellent for 2 GPUs, but for 3 or more GPUs the straightforward NCCL usage that one finds in examples on the Internet results in a horrible PP performance (2X or more lower compared to not using NCCL). Hence, I have implemented a workaround that uses pairwise communicators, but that workaround is only available for 3 and 4 GPUs (as I'm not able to test the implementation for more than 4 GPUs). I hope someone more knowledgable will show what is the correct way to use NCCL, so workarounds as in this PR are not necessary.Â Update: With more than 4 GPUs it is very likely that disabling NCCL will give better performance.\"\n\nThe half sour candy... Let's see tomorrow how it performs and will pick it up from there! But nice effort on OP and kudos for all the hard work on making llama even better!",
              "score": 25,
              "created_utc": "2026-01-05 17:59:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxv4p61",
                  "author": "a_beautiful_rhind",
                  "text": "For fully offloaded, 4xGPU  cranks. 30-40t/s on 70b and devstral large, etc. I've never had speeds this high in any backend.",
                  "score": 33,
                  "created_utc": "2026-01-05 18:49:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxvh038",
                  "author": "dsanft",
                  "text": "I don't understand why you wouldn't just slice weights and tensors and do a final allgather at the end. This arch just seems broken.",
                  "score": 5,
                  "created_utc": "2026-01-05 19:45:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxv3w83",
              "author": "pmttyji",
              "text": "Follow-up PR to above one\n\n[https://github.com/ikawrakow/ik\\_llama.cpp/pull/1092](https://github.com/ikawrakow/ik_llama.cpp/pull/1092)",
              "score": 7,
              "created_utc": "2026-01-05 18:45:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxupuoc",
          "author": "suicidaleggroll",
          "text": "Even on a single GPU, or CPU-only, I see consistent 2x prompt processing speeds on ik_llama.cpp compared to llama.cpp on every model I've tried.  It's a fantastic fork.",
          "score": 110,
          "created_utc": "2026-01-05 17:42:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxurwa2",
              "author": "YearZero",
              "text": "Is there a reason that ik\\_llama speed improvements can't be implemented in original llama? (I'm not a dev, so maybe missing something obvious). Is it just the time/effort needed, or is there some more fundamental reason like breaking compatibility with certain kinds of hardware or something?",
              "score": 45,
              "created_utc": "2026-01-05 17:51:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxuthjg",
                  "author": "LagOps91",
                  "text": "i really wish it would all be merged back. apparently there has been a spat of sorts between developers in the past leading to the fork.",
                  "score": 33,
                  "created_utc": "2026-01-05 17:59:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxuv8jc",
                  "author": "Marksta",
                  "text": "The key issue is llama.cpp is shifting too much architecturally that making any changes like those in ik_llama.cpp is so much harder. By the time you finished this multi-gpu speed up, you'd just spend the next month rebuilding it again to resolve merge conflicts, and by the time you finished doing that there would be new merge conflicts now that time has passed again...\n\nIt's half project management fault, half c++ fault. They keep changing things and to make changes means touching the core files. And the core files keep changing?! That's why modern software development moved towards architectures and languages that aren't c++ to let more than a few key devs touch the project at once.",
                  "score": 17,
                  "created_utc": "2026-01-05 18:06:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxv0i0y",
              "author": "Evening_Tooth_1913",
              "text": "How does it compare to vllm?",
              "score": 5,
              "created_utc": "2026-01-05 18:30:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxv0xat",
                  "author": "suicidaleggroll",
                  "text": "I was so disappointed with the model load times on vllm that I never got around to actually benchmarking anything.  I switch models pretty regularly, spending 2+ minutes loading up a new model (something that takes <5 seconds on llama.cpp) wipes out any advantage it could possibly have in processing speeds for my application.",
                  "score": 16,
                  "created_utc": "2026-01-05 18:32:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxvkfrh",
                  "author": "MoffKalast",
                  "text": "It actually runs. (/s but not entirely)",
                  "score": 5,
                  "created_utc": "2026-01-05 20:01:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxwrqlv",
              "author": "Zc5Gwu",
              "text": "Last time I tried ik\\_llama, either tool calling or streaming wasn't up to the same compatibility as llama.cpp. Not sure if anyone has experience recently...",
              "score": 1,
              "created_utc": "2026-01-05 23:28:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxy8n0d",
                  "author": "CheatCodesOfLife",
                  "text": "Streaming is fixed, /v1/completions is fixed, tool calling was still broken last week when I tried it.",
                  "score": 3,
                  "created_utc": "2026-01-06 04:18:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxv4cps",
          "author": "a_beautiful_rhind",
          "text": "ik now faster than exllama and probably equal to vllm for single batch. Unfortunately I didn't have as much luck with TP on hybrid inference. Between numa and PCIE 3.0, I have a bottleneck somewhere.\n\nWhat's funny is that I've been using this for what feels like a month and finally see it posted here.",
          "score": 22,
          "created_utc": "2026-01-05 18:47:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvaw46",
              "author": "mr_zerolith",
              "text": "It's kinda sad, i only caught wind of it previously in some post's comments, pointing to some github comments.\n\nDeserves a lot more eyeballs and i'm glad someone summarized it",
              "score": 8,
              "created_utc": "2026-01-05 19:17:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxwdlnu",
              "author": "Aggressive-Bother470",
              "text": "ik now beats everything for inference, single batch, I think?\n\n\nI thought it was only possible to realise PP gains with TP but they've somehow improved both on Devstral.\n\n\nOver 70t/s on devstral small beats my vllm and lcpp.",
              "score": 6,
              "created_utc": "2026-01-05 22:17:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxwdt4n",
                  "author": "a_beautiful_rhind",
                  "text": "If he somehow gets numa working, it's truly over.",
                  "score": 5,
                  "created_utc": "2026-01-05 22:18:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxx478p",
              "author": "FullstackSensei",
              "text": "So, no joy for NUMA yet?\n\nGiven how crazy RAM prices are, I'm seriously considering selling my dual Epyc rig. It's been collecting dust for at least three months now. Built it in the hope we'd get proper NUMA support, but it seems we'll get RDMA before that happens (TBH, not complaining if RDMA support comes).",
              "score": 3,
              "created_utc": "2026-01-06 00:33:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxx5ict",
                  "author": "a_beautiful_rhind",
                  "text": "It will happen. There's always fastLLM and ktransformers.",
                  "score": 2,
                  "created_utc": "2026-01-06 00:40:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxusuk3",
          "author": "Numerous-Macaroon224",
          "text": "The missing caption for the chart is: \"*4 x Nvidia Tesla T4 GPUs on 64 core AMD EPYC 7V12 server*\"",
          "score": 45,
          "created_utc": "2026-01-05 17:56:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxuvyia",
              "author": "BuildAQuad",
              "text": "Thank you, I was looking for this info",
              "score": 4,
              "created_utc": "2026-01-05 18:10:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxwytdi",
              "author": "lemondrops9",
              "text": "omg thanks because I was wondering why the tk/s is so low on the Llama.cpp",
              "score": 2,
              "created_utc": "2026-01-06 00:05:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxuuhcw",
          "author": "dazzou5ouh",
          "text": "Damn, I just finished building a 6x3090 rig",
          "score": 16,
          "created_utc": "2026-01-05 18:03:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxv8bo6",
              "author": "mr_zerolith",
              "text": "Sounds like you're in for a treat",
              "score": 6,
              "created_utc": "2026-01-05 19:05:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxwg0uq",
              "author": "Kolapsicle",
              "text": "What should \\*we\\* use it on first?",
              "score": 0,
              "created_utc": "2026-01-05 22:29:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxuxkm2",
          "author": "inrea1time",
          "text": "I will try this on my dual 5060 TI 16GB, I went for RAM over compute, maybe I can get some compute now too!",
          "score": 13,
          "created_utc": "2026-01-05 18:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvgsfb",
              "author": "Kahvana",
              "text": "Let me know how it goes, very interested!",
              "score": 4,
              "created_utc": "2026-01-05 19:44:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxzp95h",
              "author": "inrea1time",
              "text": "I did a quick test, nothing scientific with some prompts I have been using with a project.  Compared to lmstudio which I was using this seems to be 20-25% faster for a mistral 7b q4\\_k\\_m.  I am seeing both GPU's being significantly utilized with the model split between the VRAM in both.  The impact should be greater with a larger model from what I understand.",
              "score": 2,
              "created_utc": "2026-01-06 11:42:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxwhsyv",
              "author": "[deleted]",
              "text": "Please do share your experiences!",
              "score": 1,
              "created_utc": "2026-01-05 22:37:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxwsjce",
              "author": "satireplusplus",
              "text": "I also have dual 5060s and lots of DDR4 ECC ram bought before the RAM mania. Standard Llama.cpp seemed to have improved as well over the last months, as I now get 16tok/s out of gpt-120B (q4).",
              "score": 1,
              "created_utc": "2026-01-05 23:32:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzpvrb",
                  "author": "inrea1time",
                  "text": "I have a threadripper 8 channel setup but with 96GB so only 6 channels used.  I grabbed 32GB as prices were going up for a painful price.   I tried a 120B model once with lmstudio and decided never again.  I guess worth a shot now.",
                  "score": 2,
                  "created_utc": "2026-01-06 11:47:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxur6hc",
          "author": "HumerousGorgon8",
          "text": "To build for Vulkan, is it the same commands as mainline llama.cpp?",
          "score": 10,
          "created_utc": "2026-01-05 17:48:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxv4vi0",
              "author": "pmttyji",
              "text": "Yes. But\n\n>[ik\\_llama.cppÂ is not the right choice if you want to use Vulkan](https://github.com/ikawrakow/ik_llama.cpp/issues/1083#issuecomment-3687109186). There was a point in time where I had made the VulkanÂ `ik_llama.cpp`Â build work and be on par, or even slightly outperform,Â `llama.cpp`. But since then\n\n>I have added new optimizations that are not implemented on Vulkan, so will run on the CPU, thus making it slow\n\n>TheÂ `llama.cpp`Â developers have significantly improved Vulkan performance, while I have done nothing for the Vulkan back-end\n\n>I'm basically the only person working on the computation engine, so simply do not have the bandwidth to stay competitive also for Vulkan.Â `ik_llama.cpp`Â is good (and faster thanÂ `llama.cpp`) for CPU-only, CUDA-only, and hybrid CUDA/CPU inference.",
              "score": 17,
              "created_utc": "2026-01-05 18:50:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxvxhh9",
                  "author": "steezy13312",
                  "text": "*weeps in AMD*",
                  "score": 11,
                  "created_utc": "2026-01-05 21:02:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxvrpel",
              "author": "VoidAlchemy",
              "text": "Yeah, you just need to pick quants that use older mainline quant types for GPU offload, but you could still use newer ik types for tensors on RAM if doing hybrid CPU inferencing.\n\nBasically same compilation e.g.\n```\ncmake -B build -DCMAKE_BUILD_TYPE=Release -DGGML_CUDA=OFF -DGGML_VULKAN=ON\ncmake --build build --config Release -j $(nproc)\n```",
              "score": 1,
              "created_utc": "2026-01-05 20:35:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxx14st",
                  "author": "maglat",
                  "text": "for the build itself, is it possible to build just the llama-server and not the entire package?",
                  "score": 1,
                  "created_utc": "2026-01-06 00:18:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxuy986",
          "author": "daank",
          "text": "I wonder if this requires fast throughput between the GPUs?\n\nFor regular multi-gpu inference you could put the second card on a much slower PCIe lane since the speed only matters when loading weights. Does that still work for ik_llama.cpp?",
          "score": 9,
          "created_utc": "2026-01-05 18:20:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxv512k",
              "author": "a_beautiful_rhind",
              "text": "It doesn't really *require* it, but it helps. If you're on some 1x stuff you will probably see no benefit. One card being on 8x and one on 16x is fine.",
              "score": 2,
              "created_utc": "2026-01-05 18:50:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxv5b9o",
              "author": "BuildAQuad",
              "text": "I'm not certain, but i would assume that it requires more pcie lanes than normal consumer hardware can handle. Maybe dual GPU setups with 8x lanes each could work",
              "score": 1,
              "created_utc": "2026-01-05 18:52:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxut93s",
          "author": "kiwibonga",
          "text": "Nice to see my best friend Devstral Small 2 represented here.\n\nHow is memory organized compared to a single GPU setup? Is the model truly split or replicated? What about the caches?\n\nEdit: ah shit, I forgot blogspam existed",
          "score": 7,
          "created_utc": "2026-01-05 17:58:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyb4ia",
              "author": "ClimateBoss",
              "text": "any good GGUF? I get looping 'n glitchy chat template",
              "score": 1,
              "created_utc": "2026-01-06 04:34:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxydakt",
                  "author": "kiwibonga",
                  "text": "You may have to override the temperature to 0.2. The default in llamacpp and others is 0.7 which is adequate for chat but not tool calls.\n\nI use Q3_K_M from unsloth.",
                  "score": 1,
                  "created_utc": "2026-01-06 04:49:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxvahcv",
          "author": "Artistic_Okra7288",
          "text": "It would be great if this could work with rpc-server to utilize GPUs across hosts.",
          "score": 6,
          "created_utc": "2026-01-05 19:15:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxuswyz",
          "author": "HCLB_",
          "text": "Cool does it support nvidia pascal cards?",
          "score": 5,
          "created_utc": "2026-01-05 17:56:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxgxxt",
          "author": "VoidAlchemy",
          "text": "Can confirm just tested \\`-sm graph\\` on 2x RTX A6000 (the older non-PRO versions) running a small ik quant of MiMo-V2-Flash showing GPU utilization going from about 50% to almost max and big gains over default of \\`-sm layer\\`:\n\n* PP (prefill) +43%\n* TG (decode) +23%\n\nDetails: [https://github.com/ikawrakow/ik\\_llama.cpp/pull/1105#issuecomment-3712755415](https://github.com/ikawrakow/ik_llama.cpp/pull/1105#issuecomment-3712755415)\n\nhttps://preview.redd.it/igbusgn3vmbg1.png?width=2087&format=png&auto=webp&s=3aaab598c8187f89e510da85116e24ba8ec5b095\n\n  \nover 70 tok/sec TG is nice, if only the model itself was working better (i had similar quality issues with full Q8\\_0 on both ik and mainline llama.cpp forks with it failing on pydantic-ai agent tool use test that worked fine a GLM-4.7-smol-IQ1\\_KT \\~2bpw quant haha)...\n\nAnyway, its faster!  (for this specific model which is supported with specific details in above PR)",
          "score": 6,
          "created_utc": "2026-01-06 01:41:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6cvaw",
              "author": "One-Macaron6752",
              "text": "I'm just getting an error trying to run it with NCCL, with one of your quants \"Devstral-2-123B-Instruct-2512-IQ4\\_KSS.gguf\". Any idea?\n\n    ggml_cuda_op_reduce: ncclAllReduce failed with status 1\n    ik_llama.cpp/ggml/src/ggml-cuda/reduce.cu:97: Fatal error",
              "score": 2,
              "created_utc": "2026-01-07 10:21:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny9ymhm",
                  "author": "VoidAlchemy",
                  "text": "Hrmm... When compiling does it say `NCCL found!` ? Otherwise please open an issue on ik_llama.cpp github and tag me `@ubergarm` and include more details on your rig e.g. how many and what kind of GPUs, etc.\n\nThanks!",
                  "score": 2,
                  "created_utc": "2026-01-07 21:25:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxwbrym",
          "author": "zelkovamoon",
          "text": "Ok so two questions\n\nDoes ik_llama broadly support the same models as llama.cpp but with optimizations, or is it a subset\n\nAre these improvements going to apply broadly to any type of model?",
          "score": 4,
          "created_utc": "2026-01-05 22:08:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxg2qr",
              "author": "VoidAlchemy",
              "text": "ik wrote many of the quants used in mainline llama.cpp, so ik supports all those and more\n\nik can be faster for many models, this new \\`-sm graph\\` covers about 8 popular models so far - i have links to exact code above.",
              "score": 3,
              "created_utc": "2026-01-06 01:37:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwckxl",
          "author": "ga239577",
          "text": "In the article linked to on Medium, noticed this part:\n\n\"Backend Agnostic Potential: Because it is implemented at the ggml graph level rather than the CUDA backend level, it can theoretically be extended to other backends like Vulkan or ROCm in the future.\"\n\nHaving this work on ROCm seems like it would be amazing for Strix Halo devices.",
          "score": 4,
          "created_utc": "2026-01-05 22:12:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxv3hdf",
          "author": "LinkSea8324",
          "text": "ggergabros, it's over",
          "score": 10,
          "created_utc": "2026-01-05 18:43:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvkzkx",
          "author": "onil_gova",
          "text": "Any chance this offers a boost to mac users?",
          "score": 3,
          "created_utc": "2026-01-05 20:03:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwc7ie",
          "author": "ActivePutrid3183",
          "text": "What does this mean for the people with mixed GPU setups (EX: 1x 3090, 1x3060)? Previously, using such a setup would mean speeds being throttled by the 3060, but does this new solution circumvent that?",
          "score": 3,
          "created_utc": "2026-01-05 22:10:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxurn32",
          "author": "Such_Advantage_6949",
          "text": "is it basically tensor parrallel? does it support odd number of gpus?",
          "score": 5,
          "created_utc": "2026-01-05 17:50:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvtjpy",
              "author": "VoidAlchemy",
              "text": "look into the \\`--max-gpu\\` setting, depends on the model. check here for supported models: [https://github.com/ikawrakow/ik\\_llama.cpp/blob/main/src/llama.cpp#L1726-L1735](https://github.com/ikawrakow/ik_llama.cpp/blob/main/src/llama.cpp#L1726-L1735)",
              "score": 5,
              "created_utc": "2026-01-05 20:43:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxxgmx1",
                  "author": "x0xxin",
                  "text": "Any idea if GLM 4.6 or 4.7 are supported via `LLM_ARCH_GLM4_MOE`? I saw a reference to the GLM 4.6 chat template [test-chat.cpp](https://github.com/ikawrakow/ik_llama.cpp/blob/d9236392cfee36a715e7baed6890ce1e330a291e/tests/test-chat.cpp#L1891) but that's the only place in the repo I see it mentioned.",
                  "score": 3,
                  "created_utc": "2026-01-06 01:40:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxv54kb",
              "author": "a_beautiful_rhind",
              "text": "Yep.. i can use it with 3x GPU.",
              "score": 3,
              "created_utc": "2026-01-05 18:51:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxuzf6s",
              "author": "NaiRogers",
              "text": " I am on team odd nGPU",
              "score": 3,
              "created_utc": "2026-01-05 18:25:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxuu5m9",
          "author": "gofiend",
          "text": "Anybody know if this works on Rocm â€¦ especially umm MI50s?",
          "score": 4,
          "created_utc": "2026-01-05 18:02:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxv5cdw",
              "author": "a_beautiful_rhind",
              "text": "It's graph parallel so untested. Sorta cuda-centric. It's not gonna work with vulkan for sure.",
              "score": 11,
              "created_utc": "2026-01-05 18:52:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxv78f5",
                  "author": "gofiend",
                  "text": "Humm doesn't look like ikllama even supports Rocm (at least I cannot build for it), but it does have Vulkan support (which I'm testing now). \n\nPer this discussion, it def won't work with graph parallel.",
                  "score": 5,
                  "created_utc": "2026-01-05 19:00:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxv0whi",
              "author": "evillarreal86",
              "text": "Exactly what I wanted to try, but cuda only :(",
              "score": 2,
              "created_utc": "2026-01-05 18:32:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxv64th",
              "author": "Minute-Ingenuity6236",
              "text": "I tried to compile it for my MI50s and was not able to compile it successfully, except when using only Vulcan. With Vulcan, the speed was ridiculously bad, about 10x slower than vanilla llama.cpp. Maybe I did something wrong, I don't know.\n\nEDIT: When I think about it, maybe it did not use the GPUs at all and that is why the speed was so bad.",
              "score": 2,
              "created_utc": "2026-01-05 18:55:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxv9fxv",
                  "author": "Leopold_Boom",
                  "text": "Yeah it compiled with llama.cpp rocm flags for me but silently ignored the GPUs.\n\nWith Vulkan it tried to use the GPUs but was ... unbelievably slow (gpt-oss-20b so maybe it works better with a normal quant)",
                  "score": 1,
                  "created_utc": "2026-01-05 19:10:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxux3o8",
              "author": "inrea1time",
              "text": "They seem to be using NCCL [https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html) for at least some of the boost which is definitely not compatible with AMD.  I gave up on AMD a couple of months ago.",
              "score": 1,
              "created_utc": "2026-01-05 18:15:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxuy7h5",
                  "author": "Marksta",
                  "text": "RCCL is more or less a drop in replacement for NCCL. But ik_llama.cpp is CUDA only at this time for its ik fork specific features anyways even if someone patched that fix in for ROCm.",
                  "score": 7,
                  "created_utc": "2026-01-05 18:20:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxv0vrk",
              "author": "ScoreUnique",
              "text": "They do support Vulkan for sure, should give you some level of speed boost.",
              "score": 0,
              "created_utc": "2026-01-05 18:32:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxvbxml",
          "author": "insulaTropicalis",
          "text": "This is great and all, but honestly I am having some headache trying to understand which .gguf work with llama.cpp vs ik-llama.cpp, and which one should be used with which for the best performance.\n\nI invoke u/VoidAlchemy to clarify the issue.\n\nEDIT: tried with normal gguf quants for hybrid inference, till now it is much slower than mainline both at pp and tg. I'll see with the special quants tomorrow.",
          "score": 5,
          "created_utc": "2026-01-05 19:21:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvss7c",
              "author": "VoidAlchemy",
              "text": "In general ik_llama.cpp supports all GGUF quant types. For many models and rigs you'll see better PP performance on ik (especially with increased batch sizes e.g. `-ub 4096 -b 4096` stuff).\n\nAlso avx512_vnni performance is amazing for PP. Makes my 9950x CPU with 16 cores go faster than older thread ripper pro zen4 24x cores for PP.\n\nmainline llama.cpp does not support the newer quant types which I use in my models (ubergarm on huggingface).\n\nThis post is about the recent speed-ups for 2-4 GPU rigs `-sm graph` \"graph parallel\" feature. It doesn't help with single GPU as that is already fast.\n\nKeep in mind it doesn't apply to all models yet, you can see a list of them here: https://github.com/ikawrakow/ik_llama.cpp/blob/main/src/llama.cpp#L1726-L1735",
              "score": 8,
              "created_utc": "2026-01-05 20:40:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxvvrej",
                  "author": "insulaTropicalis",
                  "text": "I will test the new features, it's a while that I don't use ik-llama.cpp. I could try the Ling-1T model you quantized.\n\nAre you sure about avx512\\_vnni? Because on Threadripper Pro 7000 it is already supported. It's surprising that the 9950x is faster than 7965wx.",
                  "score": 2,
                  "created_utc": "2026-01-05 20:54:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxw2e4u",
                  "author": "fairydreaming",
                  "text": "No DeepSeek :-(",
                  "score": 2,
                  "created_utc": "2026-01-05 21:24:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxvh7cm",
              "author": "pmttyji",
              "text": "For ik\\_llama.cpp, use below GGUFs for best performance\n\n* [https://huggingface.co/ubergarm/models](https://huggingface.co/ubergarm/models)\n* [https://huggingface.co/Thireus/models](https://huggingface.co/Thireus/models)\n* [https://huggingface.co/models?other=ik\\_llama.cpp](https://huggingface.co/models?other=ik_llama.cpp)",
              "score": 6,
              "created_utc": "2026-01-05 19:46:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyeiamg",
                  "author": "insulaTropicalis",
                  "text": "Tried Ubergarm's and a few others, they are consistently slower than mainline llama.cpp in hybrid inference. With full GPU offloading ik-llama.cpp it's lightning fast, but for hybrid inference mainline is definitely the best option.",
                  "score": 1,
                  "created_utc": "2026-01-08 14:25:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxvkp1w",
                  "author": "Leflakk",
                  "text": "Do you know where to find a proper documentation (list of command flags) for ik\\_llama?",
                  "score": 1,
                  "created_utc": "2026-01-05 20:02:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxvf4jg",
          "author": "Mr_Back",
          "text": "https://preview.redd.it/g1dsmlun1lbg1.png?width=1987&format=png&auto=webp&s=ce30fbc209777f5dc4f089ea847c40924cb623de\n\nI donâ€™t understand whatâ€™s wrong. Am I not setting up the IK version correctly? Itâ€™s always slower than the regular Lama version for me. And once the flag â€”fit is enabled, things get even worse. I just tested it on the GPT OSS 20b model. Iâ€™m attaching my configuration settings for running the application.\n\n    GPT-OSS-Mini-vulkan:\n    cmd:  M:\\Soft\\llama-b7562-bin-win-vulkan-x64\\llama-server.exe --seed 3003 --model G:\\LlamaModels\\gpt-oss-20b.gguf --port ${PORT} --ctx-size 128000 --fit on --fit-target 512 --fit-ctx 16384 --mlock --host 0.0.0.0 --jinja --temp 1.0 --top-p 1.0 --top-k 0 --threads -1\n    ttl: 600\n    \n    GPT-OSS-Mini-cuda:\n    cmd:  M:\\Soft\\llama-b7621-bin-win-cuda-12.4-x64\\llama-server.exe --seed 3003 --model G:\\LlamaModels\\gpt-oss-20b.gguf --port ${PORT} --ctx-size 128000 --fit on --fit-target 512 --fit-ctx 16384 --mlock --host 0.0.0.0 --jinja --temp 1.0 --top-p 1.0 --top-k 0 --threads -1\n    ttl: 600\n    \n    GPT-OSS-Mini-ik:\n    cmd:  M:\\Soft\\ik_llama.cpp\\build\\bin\\Release\\llama-server.exe --seed 3003 --model G:\\LlamaModels\\gpt-oss-20b.gguf --port ${PORT} --ctx-size 128000 --n-gpu-layers 12 --mlock --host 0.0.0.0 --jinja --temp 1.0 --top-p 1.0 --top-k 0 --threads -1\n    ttl: 600\n    \n    GPT-OSS-Mini-ik-2:\n    cmd:  M:\\Soft\\ik_llama.cpp\\build\\bin\\Release\\llama-server.exe --seed 3003 --model G:\\LlamaModels\\gpt-oss-20b.gguf --port ${PORT} --ctx-size 128000 --n-gpu-layers 24 --n-cpu-moe 1 --mlock --host 0.0.0.0 --jinja --temp 1.0 --top-p 1.0 --top-k 0 --threads -1\n    ttl: 600",
          "score": 2,
          "created_utc": "2026-01-05 19:36:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwhnnc",
          "author": "pbalIII",
          "text": "Curious what the specific change is. Last I checked, llama.cpp still relies on pipeline parallelism rather than tensor parallelism for multi-GPU, which means GPUs process layers sequentially rather than in parallel. CUDA Graphs helped reduce kernel launch overhead and there's been Stream-K work for AMD, but nothing that changes the core multi-GPU story. Would be interested to know if there's a new layer splitting approach or different scheduling at play.",
          "score": 2,
          "created_utc": "2026-01-05 22:37:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwq808",
          "author": "maglat",
          "text": "So how to get it working?  \nSo far as I understood its required to install NCCL with sudo apt install libnccl-dev  \nBuild IK\\_llama.cpp with \"cmake -B build -DGGML\\_NCCL=ON\"  \nbut how to start llamacpp-server with the correct command?\n\nI tried following but didnt worked  \nCUDA\\_VISIBLE\\_DEVICES=1,2,3 ./llama-server -m /models/gpt-oss-120b-Derestricted.MXFP4\\_MOE.gguf --port 8788 --host [192.168.178.7](http://192.168.178.7) \\-ngl 99 --jinja --ctx-size 64000 --top\\_p 1.00 --temp 1.0 --min-p 0.0 --top-k 0.0\n\nit starts up but do not load the model into the GPU memory\n\nEDIT:\n\nSo the new mode is  -sm graph. Sadly for my test on gpt-oss the model wont support it. Thats war the log is saying. \n\n\n\n    Split mode 'graph' is not supported for this model\n    Â  => changing split mode to 'layer'",
          "score": 2,
          "created_utc": "2026-01-05 23:20:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxuwr0c",
          "author": "silenceimpaired",
          "text": "I keep hearing good things from ik_llama but I tend to prefer a packed solution like KoboldCPP or Text Gen by Oobabooga as the hassle of nvidia and setup on Linux is a lot lower for me. Is there anything like that for il_llama?",
          "score": 5,
          "created_utc": "2026-01-05 18:13:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxuyrun",
              "author": "henk717",
              "text": "There was a corc fork that tried to merge koboldcpp with the ik\\_llama stuff but its such a hassle to maintain that I think it got stuck and upstream we don't even try as the two have diverged a lot. Because llamacpp's upstream project is where most of the model support is thats what everyone bases on. So your best hope is that this or something similar lands in the upstream project.",
              "score": 6,
              "created_utc": "2026-01-05 18:22:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxv30em",
              "author": "pmttyji",
              "text": "[https://github.com/Nexesenex/croco.cpp](https://github.com/Nexesenex/croco.cpp)",
              "score": 2,
              "created_utc": "2026-01-05 18:41:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxv9lol",
                  "author": "silenceimpaired",
                  "text": "They donâ€™t provide releases like KoboldCPP, right? I think I tried it and could never get it running.",
                  "score": 2,
                  "created_utc": "2026-01-05 19:11:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxvzfzb",
              "author": "Dry-Judgment4242",
              "text": "Hope some wizard does it eventually as alas I tried getting ik up and running but the windows shit is not working for me.",
              "score": 2,
              "created_utc": "2026-01-05 21:11:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxv0ew4",
          "author": "warnerbell",
          "text": "    This is great for anyone wating to run larger models locally. The multi-GPU coordination has been a pain point for a while. Just ned a 2 slor MB now!?\n    \n    One thing I've found that compounds with hardware improvements: structural optimization on the prompt side. Even with faster inference, context window efficiency matters. I was running a 1,000+ line system prompt and noticed instructions buried deep were getting missed, regardless of hardware.\n    \n    Hardware gains + prompt architecture = multiplicative improvement. Excited to test this llama.cpp update with my upcoming Intel Build.",
          "score": 1,
          "created_utc": "2026-01-05 18:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxv9wy9",
          "author": "segmond",
          "text": "does the improvement work with RCP as well?",
          "score": 1,
          "created_utc": "2026-01-05 19:12:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvn1sm",
          "author": "DiscombobulatedAdmin",
          "text": "2 new 5060 tiâ€™s are looking better every dayâ€¦",
          "score": 1,
          "created_utc": "2026-01-05 20:13:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw0425",
          "author": "elsung",
          "text": "whoa thatâ€™s awesome. i actually just took out my tesla p40 out of my rig with 2 more 3090s to run with vllm since it was just bottlenecking my speed without much value. now u guys got me thinking of putting it back lol",
          "score": 1,
          "created_utc": "2026-01-05 21:14:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzt9sr",
          "author": "Zyj",
          "text": "Can this technique be used with two networked Strix Halo systems?",
          "score": 1,
          "created_utc": "2026-01-06 12:13:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1k6ha",
              "author": "egnegn1",
              "text": "A regular network is probably to slow, because the communication traffic is probably much higher then with regular clusters. The solution is ideally suited for very high bandwidth an low latency networks.",
              "score": 1,
              "created_utc": "2026-01-06 17:38:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny4z3k4",
          "author": "mr_zerolith",
          "text": "Yeah baby!!!",
          "score": 1,
          "created_utc": "2026-01-07 03:43:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxv96g6",
          "author": "Miserable-Dare5090",
          "text": "I just returned a 3090ti to microcenter as the speed from egpu to main pc was horrendous vs the strix halo alone. FML",
          "score": 1,
          "created_utc": "2026-01-05 19:09:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvnm5r",
          "author": "wh33t",
          "text": "Merge please so it can make it's way into kcpp!",
          "score": 1,
          "created_utc": "2026-01-05 20:15:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvt75t",
              "author": "VoidAlchemy",
              "text": "It is merged into ik\\_llama.cpp main. If you want something like kcpp that supports ik quants check out: [https://github.com/Nexesenex/croco.cpp](https://github.com/Nexesenex/croco.cpp) (i don't think it supports -sm graph yet though ymmv) or get a windows build from u/Thireus here: [https://github.com/Thireus/ik\\_llama.cpp/releases](https://github.com/Thireus/ik_llama.cpp/releases)",
              "score": 3,
              "created_utc": "2026-01-05 20:42:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwwqye",
          "author": "deltamoney",
          "text": "Very cool. But way to gatekeep and neg people  having some nice holiday breaks.",
          "score": 1,
          "created_utc": "2026-01-05 23:55:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxurxk6",
          "author": "One-Macaron6752",
          "text": "Niiiice... ðŸ™â™¥ï¸\nâœ” more fusion & optimization\nâœ” better backend batching (curved ball from vLLM)\nâœ” fewer kernel launches (important on our poor souls GPUs)\nâœ” higher throughput == joy joy...\n\nTomorrow will be a llama-bench hard day...",
          "score": -1,
          "created_utc": "2026-01-05 17:52:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw1si7",
          "author": "Rrraptr",
          "text": "`llama_new_context_with_model: split mode 'graph' or 'attn' not supported. Failed to initialize Vulkan backend`\n\nIt's a pity that Vulkan isnâ€™t supported. The CUDA gang already has vLLM anyway.",
          "score": 0,
          "created_utc": "2026-01-05 21:22:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwnox8",
          "author": "ForsookComparison",
          "text": "Does split row work on Fedora yet",
          "score": 0,
          "created_utc": "2026-01-05 23:07:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxx67t",
          "author": "79215185-1feb-44c6",
          "text": "Oh fucking god I am so hard right now.",
          "score": 0,
          "created_utc": "2026-01-06 03:10:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxft3x",
          "author": "Xamanthas",
          "text": "Slop blog and self promo of said blog. Fuck off, stop trying to profiteer off llama contributor work by 'posting'",
          "score": -4,
          "created_utc": "2026-01-06 01:35:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7qcux",
      "title": "The NO FAKES Act has a \"Fingerprinting\" Trap that kills Open Source. We need to lobby for a Safe Harbor.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/",
      "author": "PostEasy7183",
      "created_utc": "2026-01-08 22:33:33",
      "score": 554,
      "num_comments": 85,
      "upvote_ratio": 0.95,
      "text": "Hey everyone,\nâ€‹Iâ€™ve been reading the text of the \"NO FAKES Act\" currently in Congress, and itâ€™s worse than I thought.\nâ€‹The Tldr: It creates a \"digital replica right\" for voices/likenesses. That sounds fine for stopping deepfake porn, but the liability language is a trap. It targets anyone who \"makes available\" a tool that is primarily used for replicas.  \nâ€‹The Problem: If you release a TTS model or a voice-conversion RVC model on HuggingFace, and someone else uses it to fake a celebrity, you (the dev) can be liable for statutory damages ($5k-$25k per violation).\nâ€‹There is no Section 230 protection here. This effectively makes hosting open weights for audio models a legal s*icide mission unless you are OpenAI or Google.\n\nWhat I did:\nI contacted my reps email to flag this as an \"innovation killer.\" If you run a repo or care about open weights, you might want to do the same. We need them to add a \"Safe Harbor\" for tool devs.\n\nS.1367 - 119th Congress (2025-2026): NO FAKES Act of 2025 | Congress.gov | Library of Congress https://share.google/u6dpy7ZQDvZWUrlfc\n\nUPDATE: ACTION ITEMS (How to actually stop this)\nâ€‹If you don't want to go to jail for hosting a repo, you need to make noise now.\nâ€‹1. The \"Lazy\" Email (Takes 30 seconds):\nGo to Democracy.io or your Senatorâ€™s contact page.\nâ€‹Subject: Opposition to NO FAKES Act (H.R. 2794 / S. 1367) - Open Source Liability\nâ€‹Message: \"I am a constituent and software engineer. I oppose the NO FAKES Act unless it includes a specific Safe Harbor for Open Source Code Repositories. The current 'Digital Fingerprinting' requirement (Section 3) is technically impossible for raw model weights to comply with. This bill effectively bans open-source AI hosting in the US and hands a monopoly to Big Tech. Please amend it to protect tool developers.\"\nâ€‹2. The \"Nuclear\" Option (Call them):\nâ€‹Call the Capitol Switchboard: (202) 224-3121\nâ€‹Ask for Senators Wyden (D) or Massie (R) if you want to thank them for being tech-literate, or call your own Senator to complain.\nâ€‹Script: \"The NO FAKES Act kills open-source innovation. We need a Safe Harbor for developers who write code, separate from the bad actors who use it.\"",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nyk2sfk",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-09 07:25:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhm9rk",
          "author": "Revolutionalredstone",
          "text": "Making your own devs liable is how you turn your country into a third world nation.\n\nPeople who make it easy to USE the tools are the only ones who should be liable.\n\nThere are plenty of countries which won't play these silly blame games and their devs will keep releasing all their stuff either way.\n\nDevs are the inventors of ideas and making them liable for how others missuse them just cuts you off from new ideas completely, what we need todo is make operators / sites / places the normal people go to use the less desirable filters and tech liable (Instagram etc)\n\nAlso dev software licenses say you can't misuse their tech etc, so it's a joke to pretend they are in the wrong if users abuse their license.\n\nThat's a bit like holding petrol companies liable for people who stupidly try to throw bottles of gasoline onto fires:\n\nhttps://www.youtube.com/watch?v=3l50QZiPwnY\n\nEverything can be abused / used in a destructive way / used other than - intended usage.\n\nPowerful open source technologies always win and if your country is not compatible with openness then it's gonna get left behind (think north korea starving and surviving on cracked old builds of windows xp)",
          "score": 145,
          "created_utc": "2026-01-08 22:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhnibi",
              "author": "PostEasy7183",
              "text": "You hit the nail on the head regarding 'Innovation Flight.' If this passes, the US effectively sanctions its own AI sector, and the bleeding edge moves to countries with better safe harbors. We become a digital backwater.\nâ€‹The Dangerous Part: You are right that 'Operators' (those who wrap the tool in a UI) should be the target, not the 'Inventors' (who write the code).\nâ€‹But the Bill doesn't make that distinction.\nâ€‹The current text of NO FAKES defines 'making available' a digital replica tool as a liability trigger. It doesn't distinguish between a 'Click-to-Fake' app and a raw Python script on GitHub.\nâ€‹That is exactly the amendment we are pushing for: Distinguish 'Active Service Providers' from 'Tool/Code Repositories.'\nâ€‹If you haven't yet, drop a line to your Rep and tell them: 'We need a Safe Harbor for Code, or innovation leaves the US.' They need to hear the economic argument you just made.",
              "score": 30,
              "created_utc": "2026-01-08 22:50:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyhpkse",
                  "author": "Revolutionalredstone",
                  "text": "Ta! A digital backwater sounds hopeful TBH; more like nazi hell hole.\n\nCouldn't agree more: Service Providers are the piracy host equivalents here; the closest thing to devs / inventors might be the guys who created screens or cameras or modems (clearly just inventing != supporting abuse)\n\nI don't think a safe harbor clause is coming but IMHO it's not a real problem; this is the kind of law that's passed to make Karen's feel good; it is not very likely to ever translate into anything in the real actual world.\n\nAustralia (my home country) outlawed encryption 'The laws of mathematics are very commendable, but the only law that applies in Australia is the law of Australia\" (we have very low intelligence within our politics)\n\nObviously encryption is absolutely fundamental to modern computer technology and gets used everywhere everyday all the time, but hey, at least we 'successfully outlawed it' ;D\n\nI suspect a lot of laws are like this; existing on paper, but only really there to quell the dumb.",
                  "score": 12,
                  "created_utc": "2026-01-08 23:00:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nykutrn",
                  "author": "mycall",
                  "text": "I do agree with your digital backwater point but I'll raise you that other factors already in play and are worse than this.  All the same, less is better and the bill needs some modifications.",
                  "score": 2,
                  "created_utc": "2026-01-09 11:34:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyjf6qr",
              "author": "Bakoro",
              "text": ">People who make it easy to USE the tools are the only ones who should be liable.   \n    \nSo who is going to pay the developers?   \n   \nThere's no perfect solution here, either we hold back the entirety of human development because some people can't be responsible, or we accept that freedom comes with danger and do what we can to minimize risk and minimize the harm to others.\n   \nPeople hyper focus on the tools, when we need to take a step back and ask why people even want to abuse the tools.  \n   \nMaybe if we didn't have such a hostile society where people could become homeless, or die from preventable causes because they don't have enough money, then we would not have so many people spending their time and energy figuring out how to exploit others.  \n  \nMaybe if people got the mental and medical care they needed, they wouldn't turn to drugs.   \n   \nMaybe if people were guaranteed a decent living they wouldn't rob, steal, and murder so much.  \n   \nInstead of looking to fix problems, people think \"who can I punish?\"",
              "score": 5,
              "created_utc": "2026-01-09 04:29:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nykjw9y",
                  "author": "Revolutionalredstone",
                  "text": "Sounds harsh my dude, one heart to another take it easy out there ;D\n\nI probably come from a very lucky place without such problems, but it is interesting to look behind the veil and realize a lot of these issues, maybe most, are less about creative meaningful technological misuse, and are more like just outbursts in one form or another, where people use whatever they can grab to hurt one another.\n\nIt's a sobering view, thank you! (kind of glad the nukes have codes after reading that lol)\n\nHere's looking forward to a world where everyone lets local self run AI etc help them find their way over such obstacles\n\nEnjoy",
                  "score": 2,
                  "created_utc": "2026-01-09 10:00:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyhy8ex",
              "author": "WeMetOnTheMountain",
              "text": "I respectfully disagree.  People that USE the tools to break the law should hold the liability.  That could be a third party, or an end user.",
              "score": 34,
              "created_utc": "2026-01-08 23:44:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyi22wp",
                  "author": "CryptoCryst828282",
                  "text": "If they are breaking the law, by definition, there are no changes needed.... you are asking for expanding the law...",
                  "score": 18,
                  "created_utc": "2026-01-09 00:04:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nykml09",
                  "author": "g_rocket",
                  "text": "That's already illegal, but hard to prosecute at scale.",
                  "score": 1,
                  "created_utc": "2026-01-09 10:24:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyi6cwk",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -2,
                  "created_utc": "2026-01-09 00:25:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyk1e2e",
              "author": "Mythril_Zombie",
              "text": "\n>Making your own devs liable is how you turn your country into a third world nation.\n\n>People who make it easy to USE the tools are the only ones who should be liable.    \n    \nMake up your mind. Developers make their tools as easy to use as they can. \n\nWho do you think makes these tools easy to use?  Come on, you can do it. It starts with a \"D\".  \"Dev...\"  Say it with me... I know you can do it.",
              "score": 3,
              "created_utc": "2026-01-09 07:13:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nykkg3f",
              "author": "-dysangel-",
              "text": "I don't think making software easy to use should make you liable for anything done with it. Maybe something more like Suno where copyrighted songs are detected and blocked from the service.",
              "score": 1,
              "created_utc": "2026-01-09 10:05:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyk05ng",
              "author": "jglazer",
              "text": "Itâ€™s a new world- everything on the net is â€œeasy to USEâ€ because Gemini-cli or Claude will just pull it down and use it for you. There is no more need for fancy ui - and no possibility of drawing the line between inventing something and making it user friendly.  We canâ€™t legislate away the use of fake tech anymore than we can legislate away the pencil that you might use to write libel about someone.",
              "score": 0,
              "created_utc": "2026-01-09 07:02:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nykjbr2",
                  "author": "Revolutionalredstone",
                  "text": "Ok that is a horrifical relevant and terrifyingly on point assessment.\n\nWell done and yikes.\n\nI like to imagine AI as being super easy for devs to use but your right that its super easy to use PERIOD.\n\nGonna be a hard one, Enjoy!",
                  "score": 1,
                  "created_utc": "2026-01-09 09:55:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyif4r5",
          "author": "davedcne",
          "text": "Honest question, do you think your rep even understood what you were trying to explain to them? I think most of our politicians are so out of touch with technology that its like trying to teach a cave man calculus.",
          "score": 15,
          "created_utc": "2026-01-09 01:11:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhobge",
          "author": "jferments",
          "text": "This has been the point of the astro-turf \"anti-AI\" movement all along. I firmly believe that big tech corporations like Google, Microsoft, and OpenAI are behind the bots spreading \"anti-AI\" propaganda that supports laws that will essentially centralize control of AI and make open-source AI illegal.",
          "score": 81,
          "created_utc": "2026-01-08 22:54:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhoym7",
              "author": "PostEasy7183",
              "text": "You are describing the classic 'Baptists and Bootleggers' coalition.\nâ€‹The Baptists: The 'anti-AI' activists screaming about 'stealing art' (the moral cover).\nâ€‹The Bootleggers: Big Tech companies who quietly support these laws because they know compliance costs will bankrupt their open-source competitors (the economic profit).\nâ€‹Itâ€™s not a conspiracy; itâ€™s just standard regulatory capture. OpenAI and Google want high regulation because it builds a moat that you and I can't cross.\nâ€‹The Counter-Move: When you write to your Senator, point this out directly. Tell them: 'This bill is being pushed by Big Tech to kill small competitors under the guise of safety.' Senators hate feeling like they are being played by Silicon Valley lobbyists. Use that against them.",
              "score": 33,
              "created_utc": "2026-01-08 22:57:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyilkl5",
                  "author": "This_Organization382",
                  "text": "Although you're repeating what the poster said, I don't think it's fair to include Google here. They have released numerous open-source models, and typically support open-source projects. Let's not forget that the whole rat race of AI was started from Google open-sourcing their research.\n\nAdditionally, Google has the hardware; they aren't entirely dependent on their LLM succeeding.\n\nOpenAI, and Anthropic on the other hand: absolutely. They are very blatantly trying to kill open-source, open-weight ventures.",
                  "score": -7,
                  "created_utc": "2026-01-09 01:46:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyi30y6",
              "author": "GamerHaste",
              "text": "Honestly I can see that being the case... fucked up world we're heading into. Open source AI is basically the only light I see in the tunnel, would be a fucking shame to lose it. Already getting hard enough for devs to work on open source AI projects given that it appears the entire hardware manufacturing industry is basically gunning towards a \"rent hardware in the cloud\" model for consumers.",
              "score": 13,
              "created_utc": "2026-01-09 00:09:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyl6769",
                  "author": "Batetrick_Patman",
                  "text": "Youâ€™ll own nothing and like it.",
                  "score": 3,
                  "created_utc": "2026-01-09 12:54:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nykv772",
              "author": "Novel-Mechanic3448",
              "text": "As someone working at a hyperscaler it simply doesn't work that way. It really is just retards and there's a lot of them",
              "score": 2,
              "created_utc": "2026-01-09 11:37:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyhkxhr",
          "author": "Aromatic-Low-4578",
          "text": "Don't most software licenses already try to protect the developer from liability due to users?  Will be interested to see how it plays out.",
          "score": 12,
          "created_utc": "2026-01-08 22:37:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhlj9u",
              "author": "PostEasy7183",
              "text": "This is a common misconception, but unfortunately it's wrong.\nâ€‹Contract vs. Statute: An MIT/Apache license is just a contract between you and the user. It does not (and cannot) protect you from Federal Statutory Liability. If the NO FAKES Act makes 'making available' a tool illegal under federal IP law, your 'AS IS' clause is irrelevant. You can't contract your way out of federal law.\nâ€‹Third-Party Standing: The people suing you under NO FAKES aren't your users; they are third parties (estates, record labels). They never agreed to your license.\nâ€‹The 'Tool' Liability: The bill specifically creates liability for those who 'make available' technology primarily designed for replicas. It doesn't care about your license terms.  \nâ€‹The Real Kicker: The bill requires 'digital fingerprinting' for Safe Harbor. An open-source repo of raw weights technically cannot comply with that. So even if you wanted to be safe, you can't be.",
              "score": 30,
              "created_utc": "2026-01-08 22:40:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyhr0pn",
                  "author": "Aromatic-Low-4578",
                  "text": "Appreciate the explanation, thanks!",
                  "score": 5,
                  "created_utc": "2026-01-08 23:07:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyhrbg8",
                  "author": "Beautiful-Maybe-7473",
                  "text": "Can you elaborate on the \"digital fingerprinting\" exemption? I assume this if you embed a \"this is a fake\" flag in your output? I guess then the consequence is that to benefit from this exemption you have to bundle fingerprint functionality along with model weights, even though these are separate concerns and different layers of a stack, from a software engineering perspective.",
                  "score": 1,
                  "created_utc": "2026-01-08 23:08:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyhtae5",
          "author": "fortpatches",
          "text": "I understand where you are coming from, however, you may be misreading the text.\n\nSpecifically, you seem to have overlooked the phrase \"of a specifically identified individual\". E.g., (c)(1)(B)(i) states \"is primarily designed to produce 1 or more digital replicas of a specifically identified individual or individuals without \\[authorization\\].\"  The following subsections (ii) and (iii) have similar \"specifically identified individual\" language.\n\nThis would be more like making an AI designed to make you sound like Arnold Schwarzenegger as opposed to making an AI designed to make you sound like whatever audio sample you provide to it. Or Text-to-speech that makes \"AI Arnold\" say whatever you type.\n\nMoreover, is your AI \"primarily\" designed to produce \"AI Arnold\" audio?\n\nFurther, to actual knowledge is required: (c)(3)(B) states \"with respect to an activity carried out under paragraph (2) by an individual ..., the individual ... must have actual knowledge, ... that the applicable material isâ€” (i) a digital replica that was not authorized by the applicable right holder; or (ii) a product or service described in paragraph (2)(B).\"\n\nIn other words, the liability only attaches if the dev has \"actual knowledge\" that their service \"is \\*primarily\\* designed to produce a digital replica of a \\*specifically identified individual\\*.\"",
          "score": 17,
          "created_utc": "2026-01-08 23:18:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylgj3z",
              "author": "plus-minus",
              "text": "So it targets the publication of things like celebrity LoRAs mostly?",
              "score": 2,
              "created_utc": "2026-01-09 13:53:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nym0h23",
                  "author": "fortpatches",
                  "text": "I think those are the types of things this bill is drafted to target. From my review of the bill, as far as GenAI goes, it is intended to target generative AI (or any other program) that is designed to replicate a specific person's likeness. \n\nSo, from my review: \n\n* Generalized generative AI -> OK\n* Specific person generative AI -> Not OK\n* App using generalized generative AI allowing any reference source -> OK for both.\n* App using generalized generative AI the provides specific person reference source -> Not OK for App, OK for genAI model.\n\n>re \"celebrity lora\":\n\nThat is a really good question. The Lora by itself cannot generate the celebrity voice, it has to be applied to a particular base model. I have focused mostly on the first category (i) of \"primarily designed to produce 1 or more digital replicas of a specifically identified individual\", there are two additional categories (ii) \"has only limited commercially significant purpose or use other than to produce a digital replica of a specifically identified individual or individuals without the authorization\" and (iii) \"is marketed, advertised, or otherwise promoted by the individual or entity ... as a product or service designed to produce a digital replica of a specifically identified individual or individuals without the authorization\".\n\nI think there is an argument (somewhat weak) that the lora wouldn't call under the first category (i) since it cannot produce digital replicas of a particular person without the particular base model. But would likely better fall under section (ii) \"has only limited commercially significant purpose or use other than to produce a digital replica of a specifically identified individual\" and would most likely also fall under section (iii). \n\nI think a really interesting question would arise if there is an authorized specific person generative AI to sound like person A, it does sound like person A, but may be mistaken for person B. I think as long as the authors are clear that it should only be used to generate person A's likeness, and when people comment or make issues relating to person B they emphatically state that it should not be used to generate a likeness of person B, they should be ok. \n\nAccording to my [reddit research](https://www.reddit.com/r/movies/comments/hll589/actors_whose_voices_sound_really_similar_to_each/), the following are examples of actors who sound similar to each other: \n\n* \\-Donald Glover and Daveed Diggs\n* \\-Nathan Lane and Billy Crystal\n* \\-Kevin Michael Richardson, Tommy 'Tiny' Lister Jr., and Michael Clarke Duncan\n* \\-Emma Stone and Mackenzie Davis\n* \\-Jeremy Irons and David Bowie (Definitely more of a musician than actor but still...)\n* Bradley Cooper as Rocket from GotG and Dennis Leary\n* Dane de Haan and Keanu Reeves\n* Armie Hammer and John Hamm",
                  "score": 1,
                  "created_utc": "2026-01-09 15:31:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyhuagm",
              "author": "PostEasy7183",
              "text": "I appreciate you citing the specific clauses. You are reading the 'Black Letter Law' correctly, but you are missing the Litigation Reality (how this actually plays out in court).\nâ€‹The 'Primarily Designed' Trap: You argue this only targets a specific 'Arnold Bot.'\nâ€‹If I release a generic Voice Cloning tool, and 90% of my community uses it to clone celebrities (which is the current state of RVC/SVC), a court can easily rule that the tool is 'primarily designed' for that purpose based on use patterns, not just my marketing.\nâ€‹'Actual Knowledge' vs. 'Willful Blindness'\nâ€‹In IP law (see MGM v. Grokster), 'Actual Knowledge' isn't just a signed confession. If my GitHub issues or Discord are full of people asking 'How do I clone Taylor Swift?', and I don't ban them, I have 'Constructive Knowledge' or am engaging in 'Willful Blindness.'\nâ€‹The Smoking Gun (Safe Harbor)\nâ€‹This is the part you missed. Even if I don't have knowledge, to get the lawsuit dismissed quickly, I need the Safe Harbor.\nâ€‹The bill (Sec 3) conditions Safe Harbor on using 'Digital Fingerprinting' to filter content.\nâ€‹The Problem: A repo hosting raw .pth or .safetensors model weights cannot 'fingerprint' the potential output of those weights.\nâ€‹Result: Open Source repos are technically disqualified from the Safe Harbor. We are left naked in court while YouTube gets a shield.\nâ€‹That is why we are worried. Itâ€™s not about the 'Arnold Bot'; itâ€™s about the fact that GitHub repos can't afford the compliance layer to prove they aren't liable.",
              "score": 9,
              "created_utc": "2026-01-08 23:23:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyi4lft",
                  "author": "fortpatches",
                  "text": "I still think you are misreading it. A safe harbor does not protect you from litigation, it protects you from liability.\n\nAgain, you overlook the \"specifically identified individual\" text. The product has to be \"specifically\" designed to produce a replica of \"a specifically identified individual.\"\n\nI am reading this bill with my education and years of experience as an IP attorney.\n\nI will try to address each of your points:\n\n>you are missing the Litigation Reality (how this actually plays out in court). â€‹\n\nI don't think am. If this bill were to pass, there would likely be a couple large court cases soon after that will settle the interpretation, no matter what the bill says. That is the nature of laws. Legislators don't want to be specific with their laws so they don't get blamed for unexpected side-effects, so they leave them to the courts to find out what these terms mean in context. Whether you are liable or not, you can still be sued, and still go to court, the question would just be how much you will have to pay: just your attorney(s) or also your infringement.\n\nMoreover, review \"Abbey House Media v. Apple Inc.\" where the Court held that \"the uses Abbey House was enablingâ€”personal backup and device transfersâ€”were non-infringing.\" Here, Abbey pointed people to Calibre to remove DRM from their ebooks. The Court looked at the non-infringing uses encouraged by Abbey. Here, the non-infringing uses of a general model to clone a voice greatly exceed the use of a general model to clone a specific voice.\n\nAdditionally, this law would fall under Copyright (ish? since the Director of Copyrights would be in charge over it). So we should be able to look at how Copyright is generally handled in this situation. We know the following facts: (1) Jellyfin is used to host and play media. (2) Some users load questionably-sourced media on their instance. (3) Such a questionably-sourced media, played publicly, would be a Copyright infringement. (4) Jellyfin is not liable for the infringement of their users even though they would not fall under section 230. If you'll notice, Jellyfin strongly moderates their repo to make sure that all references to playing illegally obtained media are quickly and strongly discouraged.\n\n>The 'Primarily Designed' Trap: You argue this only targets a specific 'Arnold Bot.' â€‹If I release a generic Voice Cloning tool, and 90% of my community uses it to clone celebrities (which is the current state of RVC/SVC), a court can easily rule that the tool is 'primarily designed' for that purpose based on use patterns, not just my marketing. â€‹\n\nA court could easily rule any number of things. But I do not believe such a ruling would be maintained on appeal. First, it has to be \"primarily\" designed to replicate \"a specifically identified individual.\" So, if you primarily designed your AI to generate Tim Allen's voice (as in, that is the only voice it is supposed to be making), but people use it to generate George Clooney's voice, and he tries to sue you, you shouldn't be held liable. (See here, the liability determination comes after the lawsuit, as liability is generally what the entire lawsuit is about).\n\nIn your example, each of those Celebrities would have to individually sue and say that your AI was specifically designed to replicate that specific celebrity's voice. In that case, the fact that it is capable of replicating multiple voices, I contend, would be a benefit to you, since it is not replicating a \"specifically identified individual\" but instead replicates \\~any\\~ individual.\n\n>'Actual Knowledge' vs. 'Willful Blindness' â€‹In IP law (see MGM v. Grokster), 'Actual Knowledge' isn't just a signed confession. If my GitHub issues or Discord are full of people asking 'How do I clone Taylor Swift?', and I don't ban them, I have 'Constructive Knowledge' or am engaging in 'Willful Blindness.' â€‹\n\nYea, I ellipse'ed(?) out the section about \"willful blindness\" since, in the context of this discussion, I think they can be lumped in together and statutory construction can be difficult to parse when all the asides, prepositions, clarifications, etc. are in-line with the content. I took it out to make the proposed statute I was quoting more easily readable.\n\n(Continued below; sorry for the novel of a response)",
                  "score": 9,
                  "created_utc": "2026-01-09 00:16:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyioqq8",
          "author": "timschwartz",
          "text": "Just frame it like guns:\n\nModels don't deepfake people, people with models deepfake people.",
          "score": 7,
          "created_utc": "2026-01-09 02:03:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyk1wmq",
              "author": "Dadda9088",
              "text": "And models will be more controlled than guns...",
              "score": 1,
              "created_utc": "2026-01-09 07:17:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyile2z",
          "author": "ortegaalfredo",
          "text": "If you think about it, It's way more disturbing than you think:\n\nThey don't want to criminalize porn, they want to criminalize FAKE porn, why? because they need to be in control of the porn generation, so men and particularly young men can be controlled with it.",
          "score": 7,
          "created_utc": "2026-01-09 01:45:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nym3ob9",
              "author": "Hizonner",
              "text": "That's... unusually paranoid...\n\n... and has the usual conspiracy theory flaw of requiring some improbably unified and implausibly competent \"them\".\n\nOh, and, on edit, also assumes you can \"control\" somebody through porn in any way that's actually useful to anybody.",
              "score": -1,
              "created_utc": "2026-01-09 15:45:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymez37",
                  "author": "Mediocre-Method782",
                  "text": "Shared values and common experience are sufficient to produce rational, purposive action without explicit coordination. System justifiers want us to make-believe otherwise so that we don't break their toy.\n\nAnd wouldn't the induction of hypofrontality through erotic duress make reconditioning relatively easier? I mean, it's the basic principle of the Roman Church's reproduction.",
                  "score": 2,
                  "created_utc": "2026-01-09 16:36:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyitkq2",
          "author": "Acceptable_Home_",
          "text": "US and tech bros are actively targeting open source modelsÂ ",
          "score": 3,
          "created_utc": "2026-01-09 02:28:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyj9kv2",
          "author": "lisploli",
          "text": "Just label your model as non-US version. Linux distributions have done that for decades before those ridiculous encryption laws were removed. e.g. [debian](https://web.archive.org/web/20050514004108/http://www.debian.org/CD/faq/#nonus).",
          "score": 4,
          "created_utc": "2026-01-09 03:56:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyin0jf",
          "author": "SilentLennie",
          "text": "Don't know if it matters in practice what they propose. US politics is such a mess and the business interests are so 'great' they might prevent it being passed or no enforcement will happen (regulatory capture).",
          "score": 2,
          "created_utc": "2026-01-09 01:53:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykujo0",
          "author": "mycall",
          "text": "What about posting the open weights in different countries and be done with the problem?  Bittorrents are still popular and unstoppable.",
          "score": 2,
          "created_utc": "2026-01-09 11:31:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyld1gl",
          "author": "FloranceMeCheneCoder",
          "text": "Hot take, this has always been the long term goal due to the ability for people to create products without being traced",
          "score": 2,
          "created_utc": "2026-01-09 13:34:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhq9v2",
          "author": "Technical_Ad_440",
          "text": "i doubt it will pass sounds like a complete dud. the top ai will fight this cause it would kill all their tools in that case. personally for me i would rather have tons of fakes and misinfo why? cause in the future those that do due diligence will be the people you want to keep around those that dont fall for the missinfo and use tools to check origins and such they are the people you want. either as fans, work colleagues etc. \n\nwe need more and more fakes and we need people to start checking stuff not banning the tools. the future people are gonna be so hardened against all the missinfo and such that it wont make much difference. it actually baffles me why people dont want to flood fakes and such to just go oh yeh that incident it was AI and move on. its like a shield against most the normal attacks people do and actually weakens the attack vectors people have. kinda like how if people know where someone is people cant dox them cause people already know. trying it just makes the person go well people already know.\n\nyou cant trust most things anymore anyways so if i see things \"exposing\" stuff i just assume its probably AI until otherwise confirmed. it also means mundane attacks in the future are just gonna fall flat completely like if it doesnt affect you you will not care whatsoever",
          "score": 6,
          "created_utc": "2026-01-08 23:03:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhub4r",
              "author": "DerFreudster",
              "text": "A blue check-mark will accompany all Taylor Swift's entrees on the web henceforth!\n\nBut yeah, there's no stopping AI slop and it ain't going to stop because some clueless chucklehead passes a law. One good outcome might be that people veer away from the web because while it is already fake, it will become so completely and utterly fake that IRL might gain traction again!",
              "score": 2,
              "created_utc": "2026-01-08 23:23:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyhr3od",
              "author": "PostEasy7183",
              "text": "I agree with your 'plausible deniability' theory eventually, we will all just assume everything is fake until proven real. Thatâ€™s the long-term reality.\nâ€‹But you are wrong about Big Tech fighting this.\nâ€‹That is the dangerous assumption. Google and the Recording Academy are actually supporting this bill (check the co-sponsor/supporter lists).\nâ€‹Why? Because they can afford the 'digital fingerprinting' and licensing teams. They want this law to pass because it builds a regulatory moat that kills Open Source competition.\nâ€‹If this passes, 'Top AI' companies will be the only ones allowed to play. The rest of us won't be able to 'flood' anything because the open tools will be sued into oblivion. Don't count on OpenAI to save us; they are the ones handing Congress the pen.",
              "score": 3,
              "created_utc": "2026-01-08 23:07:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyhxwfi",
                  "author": "Technical_Ad_440",
                  "text": "then china know the cards they played and flood the market with actually high quality open source to get back at them, the outside world will just use china models while US regresses. but the big issue here isn't even LLMS its reaching AGI. all these models die when you have an agi companion that can do all this and then some. we will just give our agi companions a youtube video and it will learn to do all the stuff. i think agi is gonna be really important for the people to have to stop an AI uprising to so if they try not to give out agi then everything will indeed be messed up. \n\nbut for llms i think there is to much investment into the AI sector in general for it to pass. recording academy thats voices and such they are done if it passes cause the outside world will just hire you know cheaper stuff if they have to pay fingerprinting elsewhere is guaranteed to be cheaper. looks like it hasnt moved to so thats good for you guys. this is probably all the older stuff they wanted to do for AI. Just hope agi gets here soon cause agi cant exist with a lot of the stuff governments are trying to pass. maybe we will be in that dystopia where bringing a robot online causes the police to rush out to the house.",
                  "score": 6,
                  "created_utc": "2026-01-08 23:42:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyhvrpq",
          "author": "WristbandYang",
          "text": "OP only responds using AI. This is bot behavior.",
          "score": 7,
          "created_utc": "2026-01-08 23:31:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhwwyt",
              "author": "PostEasy7183",
              "text": "Guilty as charged sir. I use a LLM to clean up the drafts and organize legal points. I'm juggling this thread, work, and looking into other issues that matter to me. The irony shouldn't fall on you that I'm using the tool we are trying to save here + (This is a pro AI forum is it not?). If no fakes passes with the liability currently baked into it using an open-source model becomes risky to the host. Now, regarding the actual argument about the Safe Harbor clause: Do you disagree with the analysis, or just the formatting?\"",
              "score": -7,
              "created_utc": "2026-01-08 23:37:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyi4mfx",
                  "author": "trashk",
                  "text": "So, you are just doing this thing whole AI slop thing half assed while you work, play with the kids, make dinner, perform surgery, look up memes and then saying \"I got all this useless drivel done with the tools I'm trying to save!\"?\n\nHow much law did you actually study or is all this shit just what GPT told you?",
                  "score": 2,
                  "created_utc": "2026-01-09 00:17:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nykd7zv",
          "author": "markeus101",
          "text": "Pff its just america..they are backwards anyway",
          "score": 1,
          "created_utc": "2026-01-09 08:58:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyks67d",
          "author": "a_beautiful_rhind",
          "text": "So they only move on this because of all the political memes/videos where they make fun of *them*. \n\nImo, this is the new political satire and should now be protected speech.",
          "score": 1,
          "created_utc": "2026-01-09 11:12:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylwato",
              "author": "fortpatches",
              "text": "This has been proposed every year since 2023.",
              "score": 2,
              "created_utc": "2026-01-09 15:11:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nylx3ag",
          "author": "Mediocre-Method782",
          "text": "That this is a bot post suggests to me that the real purpose is to generate legislative motion on a bill that might have died in committee or been forgotten.",
          "score": 1,
          "created_utc": "2026-01-09 15:15:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylz9tk",
              "author": "PostEasy7183",
              "text": "Technically speaking it has been forgotten pretty much since last year (April 2025). It is of my perspective right now that when / if it ever does get back off the ground that it's better to have the open source community asses covered rather than not. If Google, open AI, and meta among others are talking to lawmakers about covering their asses why shouldn't we?",
              "score": 1,
              "created_utc": "2026-01-09 15:25:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyhzwpj",
          "author": "AutomaticDriver5882",
          "text": "I would just have different developers, make different pieces of the code and then you would snap in the modules so no one person is responsible but maybe thatâ€™s an oversimplification",
          "score": 1,
          "created_utc": "2026-01-08 23:52:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhwkd3",
          "author": "grady_vuckovic",
          "text": "I would love to hear everyone's suggestion for what the alternative is to stop the deep fakes?\n\nThis seems to make it pretty simple. If you're a developer and you release a tool that can be used to impersonate people, you're responsible.\n\nSo let's say you're all against that, alright, fair, what's the next step? What laws, what enforcement, how do we stop this?\n\nBecause \"we just have to learn to live with it\" is not gonna fly.",
          "score": -4,
          "created_utc": "2026-01-08 23:35:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyiktzt",
              "author": "misterflyer",
              "text": "I think we've all seen this coming.\n\nIn general, across the board, I just think there needs to be more watermarking/signaling for what's AI generated content.\n\nI see a lot of videos that look real at first. But because I'm an AI enthusiast, I can take a closer look and come to the suspicion that its AI generated content.\n\nMy Mother and my Grandmother on the other hand would 100% believe that a lot of the AI stuff I casually see on a daily basis is real.  We definitely don't want to reach a place in society where the average person can't tell between what's real and what's not.\n\nSo I think it goes beyond just celebrity impersonation... which is actually fairly easy to pick up on bc a lot of the celebrity generated content is so out of pocket *(or just for humor)* that no one's gonna believe it anyway.\n\nBut on youtube, for instance, I just wish there was a marker that signaled: `this content was generated using AI` or something along those lines. That wouldn't completely fix the situation, but it would be a great start.  And it's better than legally stifling open weights innovation/development completely.\n\nIt's not the celebrity stuff that worries me. It's the ordinary stuff that's borderline believable that's AI generated but very hard to tell that it's not actually real.\n\nAnd I think what the bill is trying to get at that a lot of ppl are missing is they want to hear that there's a good reason for this type of technology other than deepfakes and *doing it for the LULs*.  And I think a lot of people are gonna struggle with **coming up with convincing reasons to the lawmakers in terms of good applications for primary use.**",
              "score": 2,
              "created_utc": "2026-01-09 01:42:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyiuo1t",
                  "author": "Mediocre-Method782",
                  "text": ">But on youtube, for instance\n\n\"This content was altered or generated\" warnings are in the video description box, but collapsed by default. I wish they were badged in channels or search, like the CC logo is for subtitles.",
                  "score": 5,
                  "created_utc": "2026-01-09 02:34:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nym2g8j",
                  "author": "Hizonner",
                  "text": "> We definitely don't want to reach a place in society where the average person can't tell between what's real and what's not.\n\nWe apparently reached that ages ago. Arguably before any of us were born, definitely before \"AI\". People know that documents can be faked. People know that *photos* can be faked, and will quickly learn, if they already haven't, that it's gotten easier to do that than it used to be. Same for video.\n\nThere's stuff that's harmful, at least emotionally, even if the viewer knows it's fake. But trying to make it impossible to *produce* fakes, or to require that every fake be tagged somehow, is an absolutely pointless waste of time. You won't even reduce the prevalence enough to matter, let alone actually eliminate it, so people are going to *have* to develop some immunity.",
                  "score": 1,
                  "created_utc": "2026-01-09 15:40:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyirifi",
              "author": "Technical_Aside_3721",
              "text": "> Because \"we just have to learn to live with it\" is not gonna fly.\n\nWhy not? ( Genuine question ) \n\nAside from scale, what is the difference between me drawing a picture by hand of Hillary Clinton in a leopard print bikini and me using SDXL to generate it? \n\nI get that if you _are_ Hillary Clinton, that you think it's icky, weird, and distasteful but what is the difference making the SDXL tool illegal to run, but me drawing a picture totally fine? \nAnd maybe you could broaden the interpretation of _defamation_ / _harassment_  to include sharing deepfakes. But it seems like this legislation is going after the paper factories for people writing unsavory things.",
              "score": 0,
              "created_utc": "2026-01-09 02:17:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyiiw3g",
              "author": "Mediocre-Method782",
              "text": "You let victims put the boot of the law up the ass of the people who bring the ill intent into the world. As this bill does in its current form.\n\nI always laugh my ass off when some lobbyist has their robot write some indignant diatribe in favor of corporate censorship, imagining that hyperventilating and crying in public like a self-important four year old about \"being copied\". People need to stop serving you at restaurants.",
              "score": 0,
              "created_utc": "2026-01-09 01:31:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyhroca",
          "author": "PostEasy7183",
          "text": "Comments are coming in a lot quicker than I thought tonight. Please be patient and I will try to get back with you as many of you as I can in a couple hours. Thank you and make sure to bump the thread so this gets attention. Ensure to write to your reps or contact them with any other means necessary.",
          "score": 0,
          "created_utc": "2026-01-08 23:10:40",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nyhubjk",
          "author": "Marksta",
          "text": ">a tool that is **primarily** used for replicas.\n\nIsn't the primarily part key here? Because everytime someone advertises a TTS related thing, they bold 1000 times EAZY ONE SHOT VOICE REPLICATION as if that's its only use. This thing passes, then just take that out of your description and add a warning that you're expected to own the rights to any likeness you're duplicating.\n\nThis worry is pretty same as Photoshop being able to edit images you don't own. Photoshop doesn't spam 1000 times that you can steal copyright and must have written somewhere that copyright laws are your own problem somewhere.\n\nSo all that's left is probably actual heinous sites on the chopping block who should definitely be liable if their advertising is spamming how easy their tool will allow you to rip off likenesses and generate infringing content.",
          "score": -2,
          "created_utc": "2026-01-08 23:23:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyiy5ut",
              "author": "CheatCodesOfLife",
              "text": "If I make my tts voice cloning model require the reference audio to say â€œthe owner of this voice consents to the following reproduction of their voice â€œ. Would that be fine?\n\nCould pretty easily retrain it to only work this way.",
              "score": 2,
              "created_utc": "2026-01-09 02:53:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyjon6f",
              "author": "Apprehensive-File251",
              "text": "Slightly off topic, but i really hate that \"voice cloning\" is the only thing that people seem to talk about when it comes to tts models. \n\nI want tts models that are unique. I am baffled that everyone seems to want to clone celebrities or other.   Give me a voice that has a range of natural infections, emotional expressions, and is based off people who signed up to be used that way any day.  I dont want to get confused when some clip may be playing or someone else I cloned may be talking. \n\nBut search this subredit for tts models and  *everything*  is real voice cloning etc etc.",
              "score": 2,
              "created_utc": "2026-01-09 05:32:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyjorwm",
          "author": "GrogRedLub4242",
          "text": "folks shouldnt be working on GenAI anyway. madness",
          "score": -6,
          "created_utc": "2026-01-09 05:33:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5m2n6",
      "title": "A 30B Qwen Model Walks Into a Raspberry Piâ€¦ and Runs in Real Time",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/52juwyqq0rbg1.jpeg",
      "author": "ali_byteshape",
      "created_utc": "2026-01-06 15:45:12",
      "score": 483,
      "num_comments": 76,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "ny41hn9",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-07 00:40:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny13qwu",
          "author": "Hot_Turnip_3309",
          "text": "You'll get double the tokens per second with Mamba2 hybrid transformers, aka nemotron-3-nano-30b-a3b",
          "score": 98,
          "created_utc": "2026-01-06 16:24:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1eptk",
              "author": "__Maximum__",
              "text": "I upvoted your comment although I tried the one on ollama and it was slower than qwen 3 30b, which was i guess a fluke. In theory, it should be faster and smarter.",
              "score": 32,
              "created_utc": "2026-01-06 17:14:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny1x27u",
                  "author": "Fresh_Finance9065",
                  "text": "It should excel in long context or high token scenarios. But it needs the latest optimizations to do so. Not sure if ollama has those optimization but llamacpp does now",
                  "score": 19,
                  "created_utc": "2026-01-06 18:36:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny5uvlg",
              "author": "skinnyjoints",
              "text": "What is a hybrid transformer? I just wrapped my head around multi head latent attention. Is this something along those lines?",
              "score": 4,
              "created_utc": "2026-01-07 07:35:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny70vgl",
                  "author": "fuckingredditman",
                  "text": "yannic kilcher IIRC has decent videos explaining mamba architecture (which is basically an adapted state space model architecture that was iterated upon a couple of times) https://www.youtube.com/watch?v=9dSkvxS2EB0 but tbh i'm also not sure about the recent hybrid models. i assume it's just some combination of both mechanisms, because both have downsides (transformers are more memory/compute intensive but traditional attention is more precise/accurate, mamba2 is just super fast and can handle long context with low memory overhead but afaik it just doesn't attend as well within that context)",
                  "score": 2,
                  "created_utc": "2026-01-07 13:19:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny6ecgi",
                  "author": "Fresh_Finance9065",
                  "text": "I don't really understand how it works, but I believe the hybrid transformers attempt to use mamba layers instead of traditional layers to change how tokens are handled.\n\nMemory usage grows exponentially with token count for traditional layers, while token count for mamba layers only grow memory usage linearly.",
                  "score": 1,
                  "created_utc": "2026-01-07 10:34:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny3s3pm",
          "author": "geerlingguy",
          "text": "Tested on my Pi 5, had to set context to `-c 4096` before it would run without segfaulting after model loading. But ran with:\n\n```\n./build/bin/llama-cli -m \"models/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf\" -c 4096 -e --no-mmap -t 4\n```\n\nAnd for a few prompts, it gave between 10-11 t/s prompt processing, and 4-8 t/s token generation (lower with much larger outputs, but on average it was around 7 t/s).\n\nImpressive! Qwen3 30B MoE is much more useful than like llama 3.2:3b on a Pi, though the 16GB Pi 5 is a bit more rare.",
          "score": 30,
          "created_utc": "2026-01-06 23:51:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny42ek6",
              "author": "ali_byteshape",
              "text": "Thanks, Jeff, for testing and sharing this. Huge fan of your work and YouTube channel! :)",
              "score": 14,
              "created_utc": "2026-01-07 00:44:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny44yyt",
                  "author": "geerlingguy",
                  "text": "Thanks for sharing the model, love seeing more functionality packed into smaller devices!",
                  "score": 12,
                  "created_utc": "2026-01-07 00:58:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny1qvc1",
          "author": "florinandrei",
          "text": "nerds + AI = diagrams that are tacky as hell",
          "score": 52,
          "created_utc": "2026-01-06 18:08:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny3nos2",
              "author": "Other_Hand_slap",
              "text": "yeah, i dont knwo why i am vasting my time here",
              "score": -5,
              "created_utc": "2026-01-06 23:28:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny1vp6d",
          "author": "pgrijpink",
          "text": "Exciting and disappointing at the same time. If Iâ€™m not mistaken, your algorithm is not open source? So not as useful.",
          "score": 15,
          "created_utc": "2026-01-06 18:30:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1189p",
          "author": "iKy1e",
          "text": "This sounds amazing! Iâ€™ll have dig into the details later when I have more time, but really wanted to say this sort of low level optimism finding ways to squeeze more performance until smaller devices is amazing! I love reading about research like this.",
          "score": 10,
          "created_utc": "2026-01-06 16:12:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny2lof4",
          "author": "Watchforbananas",
          "text": "I like the improved graphs on your blog that now shows the exact quant from unsloth and magicQuant on hover (and that you added comparisons with magicQuant in the first place). Much easier to pick an interesting quant based on what I've tried before. Could you perhaps do the same for your quants? Just so I don't have to search for the lookup table. \n\nI also appreciate the 4080 results.\n\nAny plans to update the graphs for the other models as well? I've never managed to quite figure out what unsloth quants your graphs for qwen3-4B-2507 referenced.",
          "score": 9,
          "created_utc": "2026-01-06 20:28:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymmcdx",
              "author": "enrique-byteshape",
              "text": "Hey! Thank you for the comment of appreciation. We just updated the graphs for the 30B model to include the file names they correspond to on HuggingFace. We will keep doing this moving forward, so thank you for the comment. As for retroactive updates to the graphs, we'll try to get them up to date too.",
              "score": 2,
              "created_utc": "2026-01-09 17:09:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny17azk",
          "author": "bigh-aus",
          "text": "I wonder if this could be combined with an exo like solution to run on a cluster of pis...  I'm still pretty dumb with these models, but it makes me wonder if the MOE can be spread across pis.\n\n>Yes, a 30B runs on a Raspberry Pi 5 (16GB). We achieveÂ **8.03 TPS**Â at 2.70 BPW, while retainingÂ **94.18% of BF16 quality**.\n\nI'm assuming that means it's a 4 bit quant...",
          "score": 19,
          "created_utc": "2026-01-06 16:40:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny19oat",
              "author": "ali_byteshape",
              "text": "Iâ€™m not an expert on Pi clusters, but it should be doable if you have several Pis with less memory.\n\nOn the quant side: this specific model is **2.7 bits per weight** on average. We learned what precision each tensor should use to maximize throughput, so some layers end up 2-bit, some 3-bit, some 4-bit, etc. The average is 2.7 BPW with all quantization overheads included, so itâ€™s not a â€œ4-bit quantâ€ in the usual sense.",
              "score": 16,
              "created_utc": "2026-01-06 16:51:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny8oqzu",
                  "author": "Dr_Allcome",
                  "text": "What exactly is your quality metric? Because i refuse to believe a model at that quantisation retains 94% of bf16 by anything other than \"it outputs tokens\".",
                  "score": 1,
                  "created_utc": "2026-01-07 18:07:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny1akwa",
          "author": "Odd-Ordinary-5922",
          "text": "can you guys try doing this with nemotron?",
          "score": 9,
          "created_utc": "2026-01-06 16:55:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1fito",
              "author": "enrique-byteshape",
              "text": "Our main current bottleneck is evaluating the quants we produce, so we are currently taking a slower approach to releasing new quants because we want to provide the evaluation as well to the community. We plan on releasing a wide range of models in the coming months, so we'lll add Nemotron to the list of possible models :)",
              "score": 9,
              "created_utc": "2026-01-06 17:17:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny1j0za",
          "author": "frozen_tuna",
          "text": "That's actually insane. Well done!",
          "score": 6,
          "created_utc": "2026-01-06 17:33:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1rgw7",
          "author": "AvocadoArray",
          "text": "Just finished reading the blog post, nice work!\n\nWould love to see you do Seed-OSS 36B next.",
          "score": 6,
          "created_utc": "2026-01-06 18:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1085p",
          "author": "dodiyeztr",
          "text": "Would this work on raspi 5 8GB?",
          "score": 5,
          "created_utc": "2026-01-06 16:08:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny11jmw",
              "author": "DuckyBlender",
              "text": "No, 16GB minimum and even that is small",
              "score": 15,
              "created_utc": "2026-01-06 16:14:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny1gkmz",
          "author": "xandep",
          "text": "Already using the Instruct version and I liked. IQ-3 is about the same size / speed of a ptbr-REAP-16B of the original model that I use, and  initially it seems your model performs better.",
          "score": 4,
          "created_utc": "2026-01-06 17:22:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1b0uq",
          "author": "xandep",
          "text": "Any plans on Thinking model?",
          "score": 3,
          "created_utc": "2026-01-06 16:57:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1efar",
              "author": "enrique-byteshape",
              "text": "Yes, thinking models are planned, but we have some things to iron out relating to evaluation. We already have some internal tests on thinking models and are actively working on them.",
              "score": 4,
              "created_utc": "2026-01-06 17:12:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny1c2iv",
          "author": "Sensitive_Sweet_1850",
          "text": "wow. you should try nemotro too",
          "score": 3,
          "created_utc": "2026-01-06 17:01:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1zrry",
          "author": "pmttyji",
          "text": "Nice. I tried your Qwen3-4B-Q5\\_K\\_S which gave me 20 t/s same as what other provider's Q4 given me on CPU-Only performance.\n\nHope your backlog has 12-14-15B models which are better & useful for 8GB VRAM. Ex: Qwen3-14B's Q4\\_K\\_M(8.4GB) won't fit inside VRAM which gave me just 5 t/s. Then I picked IQ4\\_XS(7.5GB) which gave me 20+ t/s.",
          "score": 3,
          "created_utc": "2026-01-06 18:48:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny25vpy",
          "author": "SlavaSobov",
          "text": "Bitchin' work would be cool to see if the 8GB Jetson Orin Nano could get some improvements. \n\nNVIDIA basically says here's some basic old models and lets it languish. It's Ampere so maybe could have some gains.\n\nI stuck to mostly 4B models but feels like there's more potential there. \n\nI'm just a dumb hobbyist who knows enough to break things though. ðŸ˜…",
          "score": 3,
          "created_utc": "2026-01-06 19:15:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny3n0s3",
          "author": "Other_Hand_slap",
          "text": "i was able to run llama forninference on \n* 13th gen i3\n * 16g ram\n* nvidia 3060 ti with 8g\n\nwith a confident fair rate of 20 tokens/s",
          "score": 3,
          "created_utc": "2026-01-06 23:25:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny0zs20",
          "author": "professormunchies",
          "text": "sounds promising and pretty cool. I'll give it a try today with cline and continue.dev.\n\nI've been running this on some smaller hardware: [https://huggingface.co/cyankiwi/Qwen3-30B-A3B-Instruct-2507-AWQ-4bit](https://huggingface.co/cyankiwi/Qwen3-30B-A3B-Instruct-2507-AWQ-4bit)\n\nI like that they specify which dataset was used for calibrating the quants. Would be nice if you guys divulged such information. As far as evals go, definitely checkout the nemotron collection, lots of good datasets: [https://huggingface.co/collections/nvidia/nemotron-post-training-v3](https://huggingface.co/collections/nvidia/nemotron-post-training-v3)",
          "score": 4,
          "created_utc": "2026-01-06 16:05:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny3m9n9",
              "author": "professormunchies",
              "text": "Following up, the model has difficulty perform file edits when using cline however it was able to read files okay during the planning phase. It had to fail a few times before it was able to finally get the edits working, probably due to adding the file with @ in the prompt and it mangling the formatting some how. Model worked well in answering with some emojis (in the classic qwen style) when using the continue dev extension. Normal chat Q/A works great so I tried something more complex afterwards, using it with a the Context7 MCP through the chat interface of LMstudio. It worked well for the first message and then started always using the mcp in subsequent messages rather than just answering with the context it has. It kept saying it was a helpful assistant based on whatever repo I asked about without actually answering. Speed seems good too. I used the default that shows up in LMStudio: Q4\\_K\\_S on a m4 max",
              "score": 3,
              "created_utc": "2026-01-06 23:21:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny2wsyh",
          "author": "Noiselexer",
          "text": "Time to first token: 2 minutes",
          "score": 5,
          "created_utc": "2026-01-06 21:19:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny3gvz2",
              "author": "solarkraft",
              "text": "Makes all the sense since token generation is mostly memory bound but prompt processing requires compute!",
              "score": 3,
              "created_utc": "2026-01-06 22:54:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny3k6sq",
              "author": "KadahCoba",
              "text": "Thanks, was wondering this too.\n\nWhat was the prompt/context length for that?",
              "score": 1,
              "created_utc": "2026-01-06 23:10:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny12civ",
          "author": "_raydeStar",
          "text": "Would it work on a Pi 5 with 8GB?  Do I need that AI hat that they're offering?\n\nI'll give it a shot if I can, I love projects like this.  I'm REALLY interested in something like VL, to build a home automation system.",
          "score": 2,
          "created_utc": "2026-01-06 16:17:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny162o2",
              "author": "ali_byteshape",
              "text": "Probably not on an 8 GB Pi 5, sadly.\n\nEven the smallest model in this release needs 10+ GB of RAM just to load the weights, before you add KV cache, prompt/context, and runtime overhead. So an 8 GB Pi will hit the wall fast (and mmap usually just turns â€œwonâ€™t loadâ€ into â€œthrashes and crawlsâ€).\n\nAnd the AI HAT wonâ€™t fix this. Those hats mainly add compute power, but they do not add system memory, so they canâ€™t solve a â€œmodel does not fit in memoryâ€ problem.",
              "score": 5,
              "created_utc": "2026-01-06 16:34:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny25do8",
                  "author": "MoffKalast",
                  "text": "Well trying to fit a 30B into 16GB is already kind of a fool's errand given the 2 bit quant you had to go down to, why not try something more sane, like a dense 7-14B? Won't be as fast as a 3B of course.",
                  "score": 1,
                  "created_utc": "2026-01-06 19:13:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny1lkgb",
          "author": "siegfried2p",
          "text": "which quant is better for moe model with expert to cpu scenario?",
          "score": 2,
          "created_utc": "2026-01-06 17:45:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1tt30",
              "author": "ali_byteshape",
              "text": "The first table in the model card (https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF#cpu-models) lists CPU-friendly variants. You can choose a model based on your tolerance for quality loss versus speed. For example, KQ-2 is on the faster end, while KQ-5 is still fast and retains roughly 98% of baseline quality.",
              "score": 3,
              "created_utc": "2026-01-06 18:21:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny26hgr",
          "author": "Chromix_",
          "text": "In the Intel i7 section of the blog post the Unsloth Q5\\_K\\_M quant gets a better test score than the Q8\\_K\\_XL. So either that quant won the lottery or the benchmark results are more noisy than it looks like and don't really tell us that much with results being that close together. It'd be great to see more accurate results that prove that this method delivers smaller ( = faster) quants at the same quality level, but benchmarking this is difficult, [as written before](https://www.reddit.com/r/LocalLLaMA/comments/1pj7wjd/we_did_years_of_research_so_you_dont_have_to/ntbiyem/?context=3#ntbiyem). Maybe repeat each run a few times to get a better idea of the variance? Or check how many right/wrong answers flip between each run, to get an idea of the magnitude of randomness involved?",
          "score": 2,
          "created_utc": "2026-01-06 19:18:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny2fwuf",
              "author": "ali_byteshape",
              "text": "Excellent observation. With todayâ€™s libraries itâ€™s hard to guarantee fully deterministic behavior, so some noise is expected. We repeated a subset of runs 3 to 4 times to estimate variance, and the results were fairly consistent. Each score also aggregates tens of thousands of questions and tasks, which helps average out randomness.\n\nAlso, more bits generally reduce reconstruction error, but that does not guarantee better downstream scores. Quantization can act like a regularizer and sometimes slightly improves accuracy. In this case, Q5\\_K\\_M (5.7 bpw) and Q8\\_K\\_XL (9.4 bpw) are both very close to baseline, so the extra bits do not seem to buy much. We also show itâ€™s possible to push BPW down to \\~4.7 with ShapeLearn while still matching baseline quality.",
              "score": 3,
              "created_utc": "2026-01-06 20:01:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny3sngb",
          "author": "nickgeorgiou",
          "text": "I thought this was Agario",
          "score": 2,
          "created_utc": "2026-01-06 23:54:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5h8ke",
              "author": "HibikiAss",
              "text": "I thought it was OSU!",
              "score": 1,
              "created_utc": "2026-01-07 05:43:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny42z5f",
          "author": "DarkGeekYang",
          "text": "Nice effort. Will you consider adding more models like qwen3vl to your repo?",
          "score": 2,
          "created_utc": "2026-01-07 00:47:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny49cxt",
              "author": "enrique-byteshape",
              "text": "We are considering adding many models to our repo, but as my colleague has pointed out (like we do in our blog post), our current main bottleneck is evaluation, so the pacing for releases might be a bit slower. But we are on it! Thinking and VL are in our TO-DOs",
              "score": 2,
              "created_utc": "2026-01-07 01:22:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny3fv07",
          "author": "ApprehensiveAd3629",
          "text": "which models do you recommend for the raspberry pi5 8gb? i can also try in a orange pi 5 to test and compare.",
          "score": 1,
          "created_utc": "2026-01-06 22:49:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny3i7tq",
              "author": "enrique-byteshape",
              "text": "Sorry to say that none of the quants from this release will fit on 8GB... We did release a 4B Qwen model last time that will fit and should run fairly well if you want to test it out! Hopefully (if time permits), we will try to release models in the 10-20B range that will hopefully fit on 8GB, so stay tuned :)",
              "score": 2,
              "created_utc": "2026-01-06 23:00:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny41dpy",
                  "author": "ApprehensiveAd3629",
                  "text": "nice! i will try qwen 3 4b, its a good model.\n\nhow many tokens/sec may i get using this model?",
                  "score": 1,
                  "created_utc": "2026-01-07 00:39:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny3v3sz",
          "author": "Fault23",
          "text": "wth",
          "score": 1,
          "created_utc": "2026-01-07 00:07:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4lr5i",
          "author": "owaisted",
          "text": "What would you suggest on a 6gb vram 3060 and 16gb ram",
          "score": 1,
          "created_utc": "2026-01-07 02:29:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny4y1zj",
              "author": "ali_byteshape",
              "text": "I havenâ€™t tried partial offloading yet, but Iâ€™d expect the CPU-optimized models to work better in that setup. You could try [KQ-5](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-3.25bpw.gguf) (CPU-optimized) and[ IQ-4 ](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ3_S-3.29bpw.gguf)(GPU-optimized). Theyâ€™re almost the same size, so it would be interesting to see which one performs better in practice.\n\nWould love it if you could share your findings with us too ðŸ™‚",
              "score": 1,
              "created_utc": "2026-01-07 03:37:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny7f8rn",
                  "author": "owaisted",
                  "text": "Thank you",
                  "score": 2,
                  "created_utc": "2026-01-07 14:37:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny6m4s9",
          "author": "b4rtaz_",
          "text": "I think it would be interesting to see this model running in a distributed setup on two Raspberry Pi devices (or 4), check: [https://github.com/b4rtaz/distributed-llama/discussions/255](https://github.com/b4rtaz/distributed-llama/discussions/255)",
          "score": 1,
          "created_utc": "2026-01-07 11:39:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny713y9",
              "author": "enrique-byteshape",
              "text": "Sadly we only have one Pi to test this on, but if anyone is able to do it, please go ahead! It'll be fun hearing about the project and about how well it runs with our larger quants",
              "score": 1,
              "created_utc": "2026-01-07 13:20:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nya1iqz",
          "author": "synth_mania",
          "text": "the y axis has been cropped lol",
          "score": 1,
          "created_utc": "2026-01-07 21:38:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny244od",
          "author": "MoffKalast",
          "text": "GPU behaviour? You're getting 8 tg from the potato Videocore 7 with Vulkan!?",
          "score": 0,
          "created_utc": "2026-01-06 19:07:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny26l5o",
              "author": "ali_byteshape",
              "text": "Please take a look at the Blog for 4080-5090 results :)",
              "score": 3,
              "created_utc": "2026-01-06 19:19:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny28pvu",
                  "author": "MoffKalast",
                  "text": "Ah alright. You know something strikes me as slightly odd though, the lack of any KV cache mention on the blog. In my experience the model \"fitting\" onto a GPU means jack shit when all of it but a few layers then gets pushed out when you load any kind of actual context length and you're back to pedestrian speeds. What lengths did you test this \"fit\" with? With --no-kv-offload?",
                  "score": -1,
                  "created_utc": "2026-01-06 19:28:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny1cxho",
          "author": "HealthyCommunicat",
          "text": "How and what are people using 8 tok/s for? What software or development company would be okay with that kind of speed unless the machine is only for image or chatbots",
          "score": -4,
          "created_utc": "2026-01-06 17:05:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1et54",
              "author": "enrique-byteshape",
              "text": "We achieve much higher TPS on other hardware. 8 TPS is on a Pi 5, which is a very constrained piece of hardware. Most quants of this model don't even load on a Pi, or run very very slowly",
              "score": 10,
              "created_utc": "2026-01-06 17:14:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny2bzi6",
                  "author": "HealthyCommunicat",
                  "text": "that doesnt answer my question.",
                  "score": -3,
                  "created_utc": "2026-01-06 19:43:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny25z4n",
                  "author": "CaptParadox",
                  "text": "I'm with u/HealthyCommunicat what's the point? just to say you did? like playing doom on a tractor gps screen?",
                  "score": -3,
                  "created_utc": "2026-01-06 19:16:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pz7bmv",
      "title": "Llama-3.3-8B-Instruct",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct",
      "author": "jacek2023",
      "created_utc": "2025-12-30 03:34:19",
      "score": 462,
      "num_comments": 78,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nwpj0kv",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 10:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo5hxn",
          "author": "FizzarolliAI",
          "text": "Hello, that me!\n\nI am currently working on running sanity check benchmarks to make sure it's actually a newer L3.3 and not just L3/L3.1 in a trenchcoat, but it's looking promising so far.\n\nFrom the current readme:\n\n| | Llama 3.1 8B Instruct | Llama 3.3 8B Instruct (maybe) |\n|-|-|-|\n|IFEval (1 epoch, score avged across all strict/loose instruction/prompt accuracies to follow Llama 3 paper)|78.2|81.95\n|GPQA Diamond (3 epochs)|29.3|37.0",
          "score": 124,
          "created_utc": "2025-12-30 03:46:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo6q38",
              "author": "jacek2023",
              "text": "great work, new llama release at the end of 2025 :)",
              "score": 51,
              "created_utc": "2025-12-30 03:53:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwppc12",
                  "author": "MoffKalast",
                  "text": "I definitely did not have this on my bingo card :D\n\nAnd leaked too, keeping up the llama tradition.",
                  "score": 30,
                  "created_utc": "2025-12-30 11:22:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwp95y3",
              "author": "Karyo_Ten",
              "text": "You can do a KL-divergence check to be 100% sure",
              "score": 13,
              "created_utc": "2025-12-30 08:53:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwozt4b",
              "author": "AnOnlineHandle",
              "text": "Heya I'm not up to date with these models since the llama 1 release, do you know if there's a good benchmark for visual tasks such as identifying poses, faces, hands, etc, or answering questions about images, which I could compare models on? I've tried to use Qwen 3 Instruct for it but found it wasn't as good on real data as the demos suggested.",
              "score": 4,
              "created_utc": "2025-12-30 07:27:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwohwlc",
          "author": "dinerburgeryum",
          "text": "8K max position embeddings? Seems remarkably low; did the fine tune artifact for some reason artificially limit that?",
          "score": 50,
          "created_utc": "2025-12-30 05:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoiq8k",
              "author": "Arli_AI",
              "text": "Maybe we can just set 32768 and itâ€™ll be okay lol",
              "score": 20,
              "created_utc": "2025-12-30 05:10:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwoo96q",
                  "author": "Few-Welcome3297",
                  "text": "Checking differences from LLaMA 3.1 8B Instruct, I think we can add the rope\\_scaling\n\n|\"rope\\_scaling\": {|\n|:-|\n|\"factor\": 8.0,|\n|\"high\\_freq\\_factor\": 4.0,|\n|\"low\\_freq\\_factor\": 1.0,|\n|\"original\\_max\\_position\\_embeddings\": 8192,|\n|\"rope\\_type\": \"llama3\"|\n|},|\n\nand then increase \\`max\\_position\\_embeddings\\`\n\nEdit: Also prev version had 3 eos\\_token\\_id's\n\nEdit2: [https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K](https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K) model with above changes\n\nEdit3: Link updated",
                  "score": 26,
                  "created_utc": "2025-12-30 05:50:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nworq4h",
                  "author": "Klutzy-Snow8016",
                  "text": "Llama 3 8B had 8192 context. Then Llama 3.1 added RoPE to get to 131072 context. Maybe we can take the RoPE scaling parameters from llama 3.1's config.json and add it to llama 3.3 8B.",
                  "score": 8,
                  "created_utc": "2025-12-30 06:17:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoi4rx",
              "author": "FizzarolliAI",
              "text": "Yes. I'm not entirely sure why, it was limited when served via the website too (I put that in the readme a bit ago)",
              "score": 4,
              "created_utc": "2025-12-30 05:06:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwolsch",
          "author": "Amazing_Athlete_2265",
          "text": "Running this across my private evals to compare against other llamas. Will take a couple hours.",
          "score": 23,
          "created_utc": "2025-12-30 05:32:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwou0rl",
              "author": "Amazing_Athlete_2265",
              "text": "Initial speed test:\n\n| Model | Backend | PP ts^-1| TG ts^-1 |\n| -------------------------------------------------- | ---------- | ---------- | ------------------ |\n| allura-forge_Llama-3.3-8B-Instruct Q4 | CUDA | 1566.5 | 100.8 |\n| Llama-3.1-8B-Instruct Q4 | CUDA | 351.1 | 111.9 |\n\nSo some difference there.\n\nWill post more eval results as they come to hand.",
              "score": 22,
              "created_utc": "2025-12-30 06:36:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwp3lxt",
              "author": "Amazing_Athlete_2265",
              "text": "From these results, it looks like the new model is different than the old 3.1.\n\nHere is the performance for knowledge testing, with the new 3.3-8B-Instruct highlighted in the first two plots \n\n- [First plot is the 4-9B parameter group](https://imgur.com/YuSmDRn)\n\n- [Second plot is the same but for 8B+ parameter group](https://imgur.com/Q0nnLwn)\n\n- [Third plot is performance by knowledge category for the 3.3 model](https://imgur.com/kjkNbR3)\n\n- [Fourth plot is performance by knowledge category for the older 3.1 model](https://imgur.com/vjy6cjW)\n\n- [Last plot is a speed chart on my 3080](https://imgur.com/coiBc9H)\n\nTesting the Q6 versions now. Will take a while. All of the tests above are for Q4.",
              "score": 20,
              "created_utc": "2025-12-30 08:01:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpij0j",
                  "author": "keepthepace",
                  "text": "(Thanks for doing this!) \n\nI guess this explains why they did not brag much about it. Many other models of that category outperform them.\n\nI always wondered if Zuckerberg was not the only honest player in the field when he was explaining that the only reason they go for open source is that it will save them money. With decent open models out there they have less incentives to do so.",
                  "score": 11,
                  "created_utc": "2025-12-30 10:20:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwpmayy",
                  "author": "MLDataScientist",
                  "text": "Thanks for the tests. Question not related to llama: is LFM2 8BA1B that good in world knowledge (or coding/stem field)? I see it reaches Qwen3 30B-A3B.",
                  "score": 3,
                  "created_utc": "2025-12-30 10:55:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwp43t0",
                  "author": "jacek2023",
                  "text": "You can post pictures in the comments here",
                  "score": 2,
                  "created_utc": "2025-12-30 08:06:26",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nwp9v8w",
                  "author": "RobotRobotWhatDoUSee",
                  "text": "Random question: any idea why nemotron 30B A3B got 0% in the second plot?",
                  "score": 2,
                  "created_utc": "2025-12-30 09:00:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoo8ce",
              "author": "jacek2023",
              "text": "do you have results for other new models?",
              "score": 3,
              "created_utc": "2025-12-30 05:50:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwopj5o",
                  "author": "Amazing_Athlete_2265",
                  "text": "I have some. I focus mostly on smaller models <12B or Moe. What you want?",
                  "score": 7,
                  "created_utc": "2025-12-30 06:00:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpff8o",
          "author": "a_beautiful_rhind",
          "text": "This is like the kiss goodbye from meta.",
          "score": 18,
          "created_utc": "2025-12-30 09:52:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpzjvy",
              "author": "samplebitch",
              "text": "It's like that time when you hook up with your ex one last time, and it wasn't even that great.",
              "score": 24,
              "created_utc": "2025-12-30 12:43:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvktxy",
                  "author": "impolitemrtaz",
                  "text": "You samplebitch you",
                  "score": 2,
                  "created_utc": "2025-12-31 07:05:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwzk8xs",
                  "author": "Electronic-Metal2391",
                  "text": "You bring bad memories",
                  "score": 1,
                  "created_utc": "2025-12-31 22:13:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwognr9",
          "author": "random-tomato",
          "text": "Holy shit that is awesome, hats off to you for finding the weights!",
          "score": 33,
          "created_utc": "2025-12-30 04:56:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv4x7d",
              "author": "seppe0815",
              "text": "stupid bots",
              "score": -7,
              "created_utc": "2025-12-31 05:01:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvbbuw",
                  "author": "random-tomato",
                  "text": "If I'm a bot, I'm certainly programmed to like and appreciate when people find something cool and share with the rest of us. What's your purpose being a professional asshole?\n\nAnd no, I am not a bot\n\nhttps://preview.redd.it/eljpgxgfbhag1.png?width=765&format=png&auto=webp&s=90f0de59d2389d809ac21d988ca59883283ffccf",
                  "score": 4,
                  "created_utc": "2025-12-31 05:48:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwplkfs",
          "author": "jacek2023",
          "text": "about 4h after the release u/TheLocalDrummer published first finetune:\n\n[https://huggingface.co/BeaverAI/Anubis-Mini-8B-v1f-GGUF/tree/main](https://huggingface.co/BeaverAI/Anubis-Mini-8B-v1f-GGUF/tree/main)",
          "score": 16,
          "created_utc": "2025-12-30 10:48:37",
          "is_submitter": true,
          "replies": [
            {
              "id": "nwplnk1",
              "author": "TheLocalDrummer",
              "text": "It's a test model but I think it turned out well! Looking for feedback in (my) Discord",
              "score": 15,
              "created_utc": "2025-12-30 10:49:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwrnu7f",
                  "author": "DevelopmentBorn3978",
                  "text": "what the finetune you've made is about?",
                  "score": 3,
                  "created_utc": "2025-12-30 17:56:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwswk38",
                  "author": "LegacyRemaster",
                  "text": "legend",
                  "score": 2,
                  "created_utc": "2025-12-30 21:27:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpr0ce",
              "author": "MoffKalast",
              "text": "People are asking what's the use case for llama, and well uh... there it is ;)",
              "score": 7,
              "created_utc": "2025-12-30 11:36:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws73yi",
                  "author": "Emotional-Baker-490",
                  "text": "qwen 3",
                  "score": 2,
                  "created_utc": "2025-12-30 19:25:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpqsiz",
          "author": "jacek2023",
          "text": "[https://huggingface.co/aeon37/Llama-3.3-8B-Instruct-heretic](https://huggingface.co/aeon37/Llama-3.3-8B-Instruct-heretic)",
          "score": 10,
          "created_utc": "2025-12-30 11:34:48",
          "is_submitter": true,
          "replies": [
            {
              "id": "nwpstxk",
              "author": "Amazing_Athlete_2265",
              "text": "Everyone's cooking tonight!",
              "score": 7,
              "created_utc": "2025-12-30 11:51:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwptlto",
                  "author": "jacek2023",
                  "text": "actually it's a middle of the day in Europe :)",
                  "score": 8,
                  "created_utc": "2025-12-30 11:57:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwodozb",
          "author": "Echo9Zulu-",
          "text": "Cloned",
          "score": 7,
          "created_utc": "2025-12-30 04:36:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9jjl",
          "author": "Infninfn",
          "text": "Iâ€™m out of the loop - is this just what they had or did Meta not shutdown Llama?",
          "score": 18,
          "created_utc": "2025-12-30 04:10:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoa6pk",
              "author": "FizzarolliAI",
              "text": "This has existed at least since April during Llamacon (did anyone remember they did a Llamacon?)\n\nhttps://ai.meta.com/blog/llamacon-llama-news/\n\n> As part of this release, weâ€™re sharing tools for fine-tuning and evaluation in our new API, where you can tune your own custom versions of our new Llama 3.3 8B model. Weâ€™re sharing this capability to help you reduce costs while also working toward increased speed and accuracy. You can generate data, train on it, and then use our evaluations suite to easily test the quality of your new model.",
              "score": 30,
              "created_utc": "2025-12-30 04:14:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwoeicf",
              "author": "jacek2023",
              "text": "we do things for fun in this community, just accept the gift ;)",
              "score": 7,
              "created_utc": "2025-12-30 04:41:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx0u205",
          "author": "Dangerous_Fix_5526",
          "text": "Thinking/Instruct Hybrid using Unsloth and Claude-Opus 4.6 dataset:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning)\n\nI hope I credited everyone correctly.",
          "score": 5,
          "created_utc": "2026-01-01 02:59:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0u9ri",
              "author": "jacek2023",
              "text": "Nice work!!!",
              "score": 1,
              "created_utc": "2026-01-01 03:00:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwodvfx",
          "author": "Cool-Chemical-5629",
          "text": "I guess Christmas came late for me, but hey if this is the real thing from Meta, I guess it's nice to have something newer than 3.1 8B without needing expensive hardware for models like Llama 4.",
          "score": 7,
          "created_utc": "2025-12-30 04:37:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp5cbv",
              "author": "Emotional-Baker-490",
              "text": "qwen 3",
              "score": 12,
              "created_utc": "2025-12-30 08:17:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtj858",
          "author": "LegacyRemaster",
          "text": "    allura-forge_llama-3.3-8b-instruct\n\nMy training data is current up to December 2022. This means that I have been trained on a vast amount of text data available until that date, but I do not have information or knowledge about events or developments that have occurred after that date.\n\nIn other words, my training data \"cutoff\" is December 2022, and I should not be relied upon for information or insights related to dates after that.\n\n145.25 tok/sec",
          "score": 3,
          "created_utc": "2025-12-30 23:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwroudn",
          "author": "DevelopmentBorn3978",
          "text": "which quantized and eventually finetuned gguf models have the context lenght been enlarged? bartowsky? shb777? beaverai/anubis?",
          "score": 1,
          "created_utc": "2025-12-30 18:01:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwt7fp",
              "author": "Few-Welcome3297",
              "text": "Try [https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K-GGUF](https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K-GGUF)",
              "score": 0,
              "created_utc": "2025-12-31 13:31:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsx4sf",
          "author": "gta721",
          "text": "How dumb are they to push a portal THAT broken to prod?",
          "score": 1,
          "created_utc": "2025-12-30 21:30:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvb2oz",
              "author": "greggh",
              "text": "Nothing about it is prod. Itâ€™s still so janky that its free if your in the trial.",
              "score": 3,
              "created_utc": "2025-12-31 05:46:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvhco0",
                  "author": "FizzarolliAI",
                  "text": "Yep, this basically. Afaik the main inference API is still waitlisted, *and* there's a separate waitlist to submit for the finetuning API.",
                  "score": 2,
                  "created_utc": "2025-12-31 06:36:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww109f",
          "author": "FX2021",
          "text": "Is it a new core? Or is it just a serving variant",
          "score": 1,
          "created_utc": "2025-12-31 09:37:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9zlt",
          "author": "Intelligent-Form6624",
          "text": "â€œ(I think, anyways)â€",
          "score": -20,
          "created_utc": "2025-12-30 04:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoadni",
              "author": "FizzarolliAI",
              "text": "LISTEN whenever i drop *my own* models i get anxiety attacks about accidentally reuploading the base model ;-; i believe that this is actually L3.3 at this point though, see my other comment",
              "score": 26,
              "created_utc": "2025-12-30 04:15:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwow842",
                  "author": "Intelligent-Form6624",
                  "text": "What? Sorry, I canâ€™t hear you",
                  "score": -21,
                  "created_utc": "2025-12-30 06:55:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo9irb",
          "author": "secopsml",
          "text": "Drop behemoth instead. Looks fakeÂ ",
          "score": -36,
          "created_utc": "2025-12-30 04:10:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo9lrg",
              "author": "secopsml",
              "text": "ðŸ˜œ",
              "score": -25,
              "created_utc": "2025-12-30 04:11:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q6n5vl",
      "title": "16x AMD MI50 32GB at 10 t/s (tg) & 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/lor8ccu2xybg1.png",
      "author": "ai-infos",
      "created_utc": "2026-01-07 18:22:05",
      "score": 448,
      "num_comments": 232,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial | Guide",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nyb7g9w",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-08 01:00:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8yjc2",
          "author": "fallingdowndizzyvr",
          "text": "> Power draw: 550W (idle) / 2400W (peak inference)\n\nSweet. It's winter. Might as well have your heater do work instead of making empty BTUs.",
          "score": 60,
          "created_utc": "2026-01-07 18:49:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny98fa4",
              "author": "ForsookComparison",
              "text": "I have a 400w space heater for winter that does the job on my office desk.\n\n550w idle is mind boggling",
              "score": 20,
              "created_utc": "2026-01-07 19:32:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny9h2by",
                  "author": "Medium_Chemist_4032",
                  "text": "Isn't 2 kW actually a kettle territory?",
                  "score": 12,
                  "created_utc": "2026-01-07 20:10:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny8uc81",
          "author": "Soft_Possible1862",
          "text": "Holy shitâ€¦.",
          "score": 22,
          "created_utc": "2026-01-07 18:31:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8sxj3",
          "author": "kevin_1994",
          "text": "How loud is it? How are you able to run 2400W from home?",
          "score": 34,
          "created_utc": "2026-01-07 18:25:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny8vydf",
              "author": "FullstackSensei",
              "text": "From the picture, it seems OP is running those dual 40mm fan shrouds per GPU. So, I'd say somewhere between very and unbearably loud.\n\nOn a side rant, I'm always amused by people in the US with their 1800W outlets. Meanwhile, the rest of the world has between 2800-3600W available per outlet, because 230V...\n\nEdit: I know it was my rant that started all this, but can we please stop it. One of the things I enjoy the most in this community is how devoid it is of politics. I apologize if my comment sounded snubby or derogatory. It was never my intention.",
              "score": 74,
              "created_utc": "2026-01-07 18:38:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny90h5f",
                  "author": "kevin_1994",
                  "text": "\\*cries in canadian*",
                  "score": 19,
                  "created_utc": "2026-01-07 18:57:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny92zm6",
                  "author": "AmpEater",
                  "text": "Every us home has 240v available, and every PC PS is able to run 240v.\n\nRunning a 240v outlet isnâ€™t hardÂ ",
                  "score": 14,
                  "created_utc": "2026-01-07 19:08:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyana5n",
                  "author": "Mac_NCheez_TW",
                  "text": "What are you talking about 100% of American Home have 220v outlets in their home. They run 30amps at 220v.Â ",
                  "score": 1,
                  "created_utc": "2026-01-07 23:18:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny8x8d5",
              "author": "Clank75",
              "text": "2400W Is only 10A @ 230v.Â  A typical domestic radial in Europe is 16A, so anything up to 3500W is just \"plug it into the nearest outlet.\"\n\n\nMore than that though and you need a specialist connection (e.g. I have a 20A/4500W circuit in the kitchen for the oven+hob.)",
              "score": 11,
              "created_utc": "2026-01-07 18:43:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyb7wbb",
                  "author": "Irisi11111",
                  "text": "This is a good time to use electricity when it's not so expensive. Some countries in Europe have a lot of wind and solar power and the price of utility electricity changes depending on the weather. It's likely cheaper in the summer and fall. Also, if you get solar panels and batteries for your house, it could be really great for your computer setup for a long time.",
                  "score": 1,
                  "created_utc": "2026-01-08 01:02:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny8yhe1",
                  "author": "AnomalyNexus",
                  "text": "Earth leakage tends to trip long before you hit theoretical socket max in my experience",
                  "score": -8,
                  "created_utc": "2026-01-07 18:49:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny8v0ys",
              "author": "ai-infos",
              "text": "Depends on the fan speed, but even at 40%, it's quite noisy and needs a separate closed room... (in my opinion)  \n  \nAnd It also depends where you live but in Europe, reaching 2400W from home is possible without much trouble. You have to plug each 1600w PSU to separate electrical circuit (some tests with the breaker must be done before to draw an electrical plan of your home and check the maximum amperage/voltage allowed per circuit / power strip)",
              "score": 15,
              "created_utc": "2026-01-07 18:34:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny8zwty",
                  "author": "Medium_Chemist_4032",
                  "text": "Hats off for being brave enough to actually try that in Europe. Energy prices here kill a lot of tech hardware enthusiast's projects and a lot pivot to low energy optimizing to make it wife approved",
                  "score": 7,
                  "created_utc": "2026-01-07 18:55:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny96v4y",
                  "author": "debackerl",
                  "text": "Yes, in Belgium we have 20A 240V for circuits dedicated to plugs, most of Europe would be at least 16A per circuit.",
                  "score": 2,
                  "created_utc": "2026-01-07 19:25:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny980ys",
                  "author": "bigh-aus",
                  "text": "Meanwhile the AI rigs in the USA are going to start popping up in the laundry (to utilize the dryer plug)",
                  "score": 1,
                  "created_utc": "2026-01-07 19:30:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyb0vo0",
              "author": "Irisi11111",
              "text": "If OP owns a farm and has a lot of solar panels, that's totally fine. It's a good way to handle extra electricity. But he/she does need to be cautious about fire risks.",
              "score": 2,
              "created_utc": "2026-01-08 00:27:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny9wm0q",
              "author": "abnormal_human",
              "text": "Loud as fuck and a trip to the hardware store I'm guessing.",
              "score": 1,
              "created_utc": "2026-01-07 21:17:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny9q1kn",
          "author": "SourceCodeplz",
          "text": "Tbh if you are coding professionally, this really isn't that much of a spend for having a basically offline programmer working with you on just electricity.",
          "score": 11,
          "created_utc": "2026-01-07 20:49:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nybnlo5",
              "author": "LA_rent_Aficionado",
              "text": "Youâ€™d still get better quality, latency and arguably more through put for the same price as power if you get a $100/$200 Claude subscription.  Just at the expense of privacy and coolness factor.",
              "score": 5,
              "created_utc": "2026-01-08 02:25:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyc9lg2",
                  "author": "autodidacticasaurus",
                  "text": "> Just at the expense of privacy and coolness factor.\n\nNo, it's not just that. It's also control and security. They can take it away from you any time they like. They can make it worse anytime they like. They can start inserting propaganda and ads any time they want... raise your prices... or even go bankrupt entirely. Even just changing business models would be stressful. Let's not forget about outages either. There have been huge ones lately that have half the Internet down (Cloudflare, Amazon, etc.).\n\nIt's insane to outsources to services like this. Absolutely fucking insane.",
                  "score": 21,
                  "created_utc": "2026-01-08 04:27:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny8yh6u",
          "author": "OnlineParacosm",
          "text": "Can you give us a rough all in cost so I can figure what the tokens per second cost basis is for this? Thanks for such a great write up.",
          "score": 7,
          "created_utc": "2026-01-07 18:49:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny92eov",
              "author": "ai-infos",
              "text": "\\~5.6kâ‚¬ (with \\~150â‚¬ / MI50, not the case anymore)",
              "score": 12,
              "created_utc": "2026-01-07 19:06:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny98dhc",
                  "author": "TheSpicyBoi123",
                  "text": "Damn, 150 euro for a 32gb mi50 is a bargain!",
                  "score": 5,
                  "created_utc": "2026-01-07 19:32:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny9pr28",
          "author": "ThatCrankyGuy",
          "text": "Thousands of dollars of equipment hang by garden twist-tie wire. Reminds me of grad days.",
          "score": 6,
          "created_utc": "2026-01-07 20:48:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8vpf6",
          "author": "Dorkits",
          "text": "Me with my rumble 3060ti : Dream build ðŸ˜²",
          "score": 9,
          "created_utc": "2026-01-07 18:37:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8zw75",
          "author": "FullstackSensei",
          "text": "Is there a meaningful difference between something like DS AWQ and something like Minimax 2.1 at Q8?",
          "score": 4,
          "created_utc": "2026-01-07 18:55:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny941co",
              "author": "ai-infos",
              "text": "i didn't try but i don't expect meaningful difference (or if we compare with glm 4.7 on 8 mi50). To be honest, right now, i won't use it a lot due to higher electrical cost, except for deepseek 3.2 speciale.   \n  \nBut later, i expect much more intelligent models around \\~1T param (\\~300-500GB if quantized), so at least, i know that it's possible to run it with old hardware (if the LLM architecture does not change too much from now)",
              "score": 4,
              "created_utc": "2026-01-07 19:13:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny95fit",
                  "author": "FullstackSensei",
                  "text": "You should get considerably faster TG speed on smaller models, which will reduce your average power consumption.\n\nI'm much more optimistic about \\~100B models with larger contexts, and TBH, at least 80% of coding tasks can already be aptly handled by models like Qwen 3 coder 30B which runs very fast on 2 Mi50s or two 3090s. I only fire my 192GB VRAM rigs when I need 200+B models.",
                  "score": 5,
                  "created_utc": "2026-01-07 19:19:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny986ve",
          "author": "ForsookComparison",
          "text": "How is your prompt processing so good? Does Tensor parallelism come into play?",
          "score": 4,
          "created_utc": "2026-01-07 19:31:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9h7ne",
              "author": "ai-infos",
              "text": "Yes ~~and thanks to the 1TB/s bandwidth per MI50.~~  \n~~The 16 MI50 with a total of 16TB/s are running at \\~100% during prompt processing (PP). In vllm, the PP speed (tok/s) is actually variable according to the tokens input. On another build with 8 MI50 and glm 4.6, i reached \\~10k tok/s (for a prompt of \\~80k tokens in roo code).~~  \n  \nEDIT: as said below, bandwidth doesn't really matter in prompt processing, it's more compute bound. But Tensor parallelism comes indeed into play (and that's the huge difference between softwares like vllm and sglang supporting true tensor parallelism and llama.cpp which doesn't for now)",
              "score": 7,
              "created_utc": "2026-01-07 20:10:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny9mbm3",
                  "author": "StardockEngineer",
                  "text": "Bandwidth doesn't really matter in prompt processing.",
                  "score": 8,
                  "created_utc": "2026-01-07 20:33:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nybo6v7",
          "author": "vulcan4d",
          "text": "Ditch the furnace, best home heater!",
          "score": 5,
          "created_utc": "2026-01-08 02:28:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9ewd9",
          "author": "organicmanipulation",
          "text": "Nice setup! I notice that you're splitting your PCIe lanes into two 8x. Can you please share the exact PCIe bifurcation card you're using?",
          "score": 3,
          "created_utc": "2026-01-07 20:00:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9iqnu",
              "author": "ai-infos",
              "text": "In the github, i wrote:   \n  \n\"12x SlimSAS PCIe device adapters 2x 8i\n\n4x SlimSAS PCie device 1x 4i (C-payne)\n\n12x SlimSAS cables 8i\n\n2x SlimSAS cable 8i to 2x4i\n\n7x SlimSAS PCIe host adapter\"  \n  \nSo it's x8x8 for pcie 2 to pcie 7 and x4x4x4x4 for pcie1 for example.  \n  \nYou can find these adapters / cables on most famous online website (amazon, alibaba, ebay, etc), except for the \"SlimSAS PCie device 1x 4i C-payne\". I didn't find anything similar from other sellers than C-payne. If you've got any other sellers for this device, please share it.",
              "score": 3,
              "created_utc": "2026-01-07 20:17:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nybrybs",
                  "author": "ShreddinPB",
                  "text": "Seriously, I have spent the last 4 hours scouring, cant find anything at all",
                  "score": 1,
                  "created_utc": "2026-01-08 02:48:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nybtyh9",
                  "author": "ShreddinPB",
                  "text": "The only ones I see in the USA on Amazon have a 24pin ATX connector :( I assume these are so each gpu has its own dedicated PSU?    \n[https://a.co/d/gqnwaOv](https://a.co/d/gqnwaOv)    \nI assume these cant be used :(",
                  "score": 0,
                  "created_utc": "2026-01-08 02:58:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nya9bqr",
          "author": "cashmillionair",
          "text": "Thank you for sharing, appreciate it!",
          "score": 3,
          "created_utc": "2026-01-07 22:12:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyb7t5l",
          "author": "noiserr",
          "text": "That's nuts! And I love it.",
          "score": 3,
          "created_utc": "2026-01-08 01:02:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybm752",
          "author": "MaximKiselev",
          "text": "10 t/s Carl....",
          "score": 3,
          "created_utc": "2026-01-08 02:18:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybvmx1",
          "author": "ryfromoz",
          "text": "I love massive frankenstein rigs like this!",
          "score": 3,
          "created_utc": "2026-01-08 03:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydbb4o",
          "author": "PreparationLow6188",
          "text": "A Wow for this supreme project. It is the time should consider reenable MI50 on the shelf.",
          "score": 3,
          "created_utc": "2026-01-08 09:25:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8yc7v",
          "author": "egomarker",
          "text": "10 tks? Sigh",
          "score": 8,
          "created_utc": "2026-01-07 18:48:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nycy1xs",
              "author": "AttitudeImportant585",
              "text": "the only task that justifies the effort to build this would be for agentic coding and 10tps is not up for that lol",
              "score": 4,
              "created_utc": "2026-01-08 07:26:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyh3vgq",
                  "author": "crantob",
                  "text": "*the only task I can imagine",
                  "score": 1,
                  "created_utc": "2026-01-08 21:22:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny943r0",
          "author": "exaknight21",
          "text": "Can you share your build please. Like what are those fans, how did you hook them up, what motherboard youâ€™re using. I am a little new to this aspect.",
          "score": 2,
          "created_utc": "2026-01-07 19:13:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny96k56",
              "author": "ai-infos",
              "text": "Motherboard is  ASRock Rack ROMED8-2T  \nIt's classic 10 artic fans 140mm but these ones are optional to me in this open setup.  \nThe most important ones are the ones behind the gpu. If you buy the mi50 from alibaba, ask the seller to include some fans, it would be 2 small fans 50mm or 1 bigger fan per gpu (and these chinese fans are very very noisy but do the work to keep temps under 65Â°C)\n\nYou also need some PWM HUB FAN Artic Sata and you have to respect the maximum number of amperage/wattage per slot (and in total per hub) if you don't want to burn your devices.",
              "score": 3,
              "created_utc": "2026-01-07 19:24:31",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyaodfv",
              "author": "Mac_NCheez_TW",
              "text": "I 3D printed in my fan shrouds for my Mi50s. I put 120mm fans on the back side with a nice airflow curvatures in the print. you can also get Mi50s fan shrouds 3D printed on Ebay and amazonÂ ",
              "score": 1,
              "created_utc": "2026-01-07 23:23:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny9zj2e",
          "author": "Different-Toe-955",
          "text": "Epic setup I love the zip ties holding the GPUs up. How do those oculink extenders work out? Looks like each GPU is running at x8.",
          "score": 2,
          "created_utc": "2026-01-07 21:29:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyab89v",
              "author": "ai-infos",
              "text": "there are 12 gpu at x8 and 4 gpus at x4  \nit's not oculink extenders but rather SlimSAS extenders (**SFF-8654 8i or 4i)**",
              "score": 2,
              "created_utc": "2026-01-07 22:21:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nydfak3",
                  "author": "danishkirel",
                  "text": "What do only the extenders cost?",
                  "score": 1,
                  "created_utc": "2026-01-08 10:02:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyakif3",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 2,
          "created_utc": "2026-01-07 23:04:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nycfbgw",
              "author": "halcyonhal",
              "text": "Did you read the post and look at the GitHub link?",
              "score": 1,
              "created_utc": "2026-01-08 05:04:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nycaq4e",
          "author": "badgerbadgerbadgerWI",
          "text": "This is the kind of setup that makes enterprise local deployment actually viable. MI50s at those prices vs NVIDIA is a completely different ROI calculation. Are you seeing any stability issues with vllm on the older gfx906 arch over longer inference runs?",
          "score": 2,
          "created_utc": "2026-01-08 04:34:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydu26z",
              "author": "ai-infos",
              "text": "Yes, after filling the context with 6k tokens successfully, I tried injecting a prompt of 17k token for code review but the MTP module crashes (I talked about it there: [https://github.com/nlzy/vllm-gfx906/pull/62](https://github.com/nlzy/vllm-gfx906/pull/62) )   \n  \nSo there's still work to do for speed/memory/stability improvements and hope the community will also contribute to it (as i'm not an expert but will continue to try to improve it step by step in my free time)",
              "score": 1,
              "created_utc": "2026-01-08 12:03:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nygckst",
          "author": "qcodec",
          "text": "500W/2400W. Oh my, even my solar system can't handle this. I guess I'll have to install a standalone one.",
          "score": 2,
          "created_utc": "2026-01-08 19:21:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyieqz7",
          "author": "el3mancee",
          "text": "Nice setup.",
          "score": 2,
          "created_utc": "2026-01-09 01:09:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyihpr7",
          "author": "el3mancee",
          "text": "https://preview.redd.it/vr6jf0i178cg1.jpeg?width=4284&format=pjpg&auto=webp&s=2e4036b31d87b2e9d7d75dcdbebdae2f7e07cecf\n\nMy setup can run Deepseek 3.1 IQ4\\_XS at 7.5 t/s. 200W total when running.",
          "score": 2,
          "created_utc": "2026-01-09 01:25:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyii5zg",
              "author": "el3mancee",
              "text": "https://preview.redd.it/qwm2qlxz78cg1.jpeg?width=4284&format=pjpg&auto=webp&s=17f745c675528ece9eb128c1317acfe876e14eb8",
              "score": 1,
              "created_utc": "2026-01-09 01:27:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny8ycbp",
          "author": "Far-Low-4705",
          "text": "pretty sure a mac would it run faster, and at far less power consumption.\n\nStill super cool, but not sure how practical it would be",
          "score": 3,
          "created_utc": "2026-01-07 18:48:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny8z64f",
              "author": "ai-infos",
              "text": "true for the much much less power consumption, true also for faster token generation speed   \nbut not prompt processing speed... you don't have 16 TB/s bandwidth on a single Mac node...",
              "score": 11,
              "created_utc": "2026-01-07 18:52:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nybn4bx",
                  "author": "Far-Low-4705",
                  "text": "fair point. is pp really that important though? especially with prompt caching? you still have to wait for generation to complete.",
                  "score": 1,
                  "created_utc": "2026-01-08 02:22:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny92scu",
              "author": "cantgetthistowork",
              "text": "The Mac will take years for prompt ingestion. Power consumption much lower though",
              "score": 7,
              "created_utc": "2026-01-07 19:07:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny98ub4",
              "author": "ForsookComparison",
              "text": "16x 32GB Mi50x's at, let's say $250 each, is $4K. Add $2K for the skeleton, PSUs, etc without skimping, and I'm guessing this is recreatable at $6K. The Mac that would likely beat this starts at $10K iirc\n\nAlso this is way cooler.",
              "score": 1,
              "created_utc": "2026-01-07 19:34:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny9ly7m",
                  "author": "StardockEngineer",
                  "text": "There is no Mac that beats this if the purpose is coding.",
                  "score": 7,
                  "created_utc": "2026-01-07 20:31:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nybmsii",
                  "author": "Far-Low-4705",
                  "text": "power consumption? a single mi50 consumes more power than the mac does.\n\nI agree it is way cooler, but something tells me it's just not practical",
                  "score": 2,
                  "created_utc": "2026-01-08 02:21:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny9n2d3",
          "author": "Lissanro",
          "text": "Prompt processing is very good, compared to what I get with ik\\_llama.cpp with four 3090 GPUs (around 150 tokens/s vs 2K tokens/s in your rig). But generation of 10 tokens/s sound very slow, given I get 8 tokens/s with most of the model's weights in DDR4 RAM (using IQ4 quant), and only cache with common expert tensors in VRAM.\n\nI suspect that something is inefficient in the inference code or maybe something not configured correctly, I think generation speed should be much faster... unless on MI50 you are getting compute bound or something.",
          "score": 2,
          "created_utc": "2026-01-07 20:36:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9xxpl",
              "author": "ai-infos",
              "text": "I was actually hesitating to build a setup looking to yours but I wanted better prompt processing speed for coding/debugging use cases.   \n  \nI explained the technical limitations in the \"side notes / future work\" of this PR: [https://github.com/nlzy/vllm-gfx906/pull/62](https://github.com/nlzy/vllm-gfx906/pull/62)   \nI'm not skilled in triton / c++ / hip kernels so most of the functions are pure pytorch not really optimized with some vibe coded parts. So yes, it's currently quite inefficient.   \n  \nThat's frustrating having an AMD MI50 gpu, it's like having a bike but being forced to walk alongside it",
              "score": 1,
              "created_utc": "2026-01-07 21:22:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny8wpum",
          "author": "d13056",
          "text": "What is the best model for control my pc?",
          "score": 1,
          "created_utc": "2026-01-07 18:41:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny90hew",
          "author": "re_e1",
          "text": "Yoo my dream setup",
          "score": 1,
          "created_utc": "2026-01-07 18:57:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny928um",
          "author": "PhotographerUSA",
          "text": "Would it be better to run an optimized module where you can get 200 or more tx/sec",
          "score": 1,
          "created_utc": "2026-01-07 19:05:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9kx61",
          "author": "starkruzr",
          "text": "budget?",
          "score": 1,
          "created_utc": "2026-01-07 20:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9suin",
          "author": "d4nger_n00dle",
          "text": "https://preview.redd.it/faceagfjrzbg1.jpeg?width=400&format=pjpg&auto=webp&s=bb386b84882cbea8de7b3d5931cda0024a04cda8",
          "score": 1,
          "created_utc": "2026-01-07 21:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nya01s5",
          "author": "krusic22",
          "text": "~~How are you only pulling 2400W? That's just 75W per card.~~",
          "score": 1,
          "created_utc": "2026-01-07 21:31:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nya9ln5",
              "author": "droptableadventures",
              "text": "Not the OP but:\n2400/16 is actually 150w per card, not 75. You can power limit to about 150w with not that much performance loss.",
              "score": 3,
              "created_utc": "2026-01-07 22:13:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyab6b0",
                  "author": "krusic22",
                  "text": "Ah, I misread. Thanks.",
                  "score": 2,
                  "created_utc": "2026-01-07 22:20:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyac46d",
              "author": "ai-infos",
              "text": "2400W / 16 is 150W per card   \nThey almost never go up to the max of 300W (and you can cap the max wattage by updating VBIOS or UPP or rocm-smi command with very low speed loss; more details there: [https://gist.github.com/evilJazz/14a4c82a67f2c52a6bb5f9cea02f5e13](https://gist.github.com/evilJazz/14a4c82a67f2c52a6bb5f9cea02f5e13) )",
              "score": 3,
              "created_utc": "2026-01-07 22:25:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nybvq0r",
          "author": "Tiny_Judge_2119",
          "text": "oh my god, peak power usage is higher than my air con...",
          "score": 1,
          "created_utc": "2026-01-08 03:07:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycptlp",
          "author": "MyBrainsShit",
          "text": "Does it make sense to mix and match? Like if I had a 5090 and a few mi50 can I spread models across or is that impossible/very cumbersome? Is anyone doing that?",
          "score": 1,
          "created_utc": "2026-01-08 06:19:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyds8mm",
              "author": "airpray",
              "text": "Look at https://llm-d.ai/",
              "score": 2,
              "created_utc": "2026-01-08 11:50:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyellg0",
                  "author": "MyBrainsShit",
                  "text": "Nice, thank you!",
                  "score": 1,
                  "created_utc": "2026-01-08 14:42:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nydtdxd",
              "author": "ai-infos",
              "text": "I tried this months ago with a 3090 and llama.cpp (and with or without RPC) but it was not really worth (+1/5% in speed if i remember correctly, but maybe I missed something in the settings)",
              "score": 2,
              "created_utc": "2026-01-08 11:58:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nydbi8n",
          "author": "Neptun78",
          "text": "Can you test Kimi K2 (quant that fits memory)? :)",
          "score": 1,
          "created_utc": "2026-01-08 09:27:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydsh1r",
              "author": "ai-infos",
              "text": "the original int4 is \\~600GB so it won't fit the 512GB of VRAM  \nand the GGUF with llama.cpp would have a very slow prompt processing speed (\\~200 tok/s i think, not really usable for coding agent)",
              "score": 1,
              "created_utc": "2026-01-08 11:52:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nydxjmn",
                  "author": "Neptun78",
                  "text": "Ok, youâ€™re right.\nHave you tried dense model like gemma3-27B?",
                  "score": 1,
                  "created_utc": "2026-01-08 12:27:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nydjws6",
          "author": "Ok-Internal9317",
          "text": "10 tok/s? Do you even use this thing you built?",
          "score": 1,
          "created_utc": "2026-01-08 10:43:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydt0py",
              "author": "ai-infos",
              "text": "10 tok/s in TG with thousands of tok/s for prompt processing speed is enough to me for coding agents use cases  \nBut right now, i'm still working on it to make it work under Roo Code (there's a problem with the tokenizer to fix). After that, I will compare the output with GLM 4.7 and I expect that having 2 GLM 4.7 on 16 MI50 would be a better choice than having 1 Deepseek  v3.2",
              "score": 1,
              "created_utc": "2026-01-08 11:56:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyehudh",
          "author": "Electronic_Status_60",
          "text": "Bro at that rate just pay for the tokens, he's got half a microwave on at all times",
          "score": 1,
          "created_utc": "2026-01-08 14:23:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf2dp4",
          "author": "Ulterior-Motive_",
          "text": "Brother has a whole datacenter in his basement",
          "score": 1,
          "created_utc": "2026-01-08 16:00:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfkhpp",
          "author": "Environmental-Metal9",
          "text": "Youâ€™ve unlocked the â€œMadLadâ€ badge",
          "score": 1,
          "created_utc": "2026-01-08 17:19:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8ynlj",
          "author": "Legal-Ad-3901",
          "text": "I'm at 16 mi50s right now but your plans for 32 have me thinking ðŸ¤”ðŸ¤”ðŸ¤”ðŸ¤”ðŸ¤” fwiw, 3xl u sloth thinking k2 is around 455gb if you're feeling froggy before getting the next setup. Would also be curious what your llama speeds would be as I fair better on that vs vllm",
          "score": 1,
          "created_utc": "2026-01-07 18:49:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny90jed",
              "author": "ai-infos",
              "text": "32 won't be too interesting...it would be a massive burn of energy for a model not too smart in comparison with its competitor like glm 4.7 running on only 8 mi50 (same actually applies for deepseek v3.2 with 16 gpu...)  \nAnd also 32 mi50 would require 2 nodes, running vllm + ray backend (which i already tested in the past with 2\\*8 mi50 and it was not really steady for amd gpu with pipeline parallelism)",
              "score": 5,
              "created_utc": "2026-01-07 18:58:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny96txm",
                  "author": "BevinMaster",
                  "text": "I mentioned on discord I think but maybe the plx pci-e 4.0 switch way might unlock a possibility to run everything on the same node ? If you still have mmio issues with that I dunno, but there are some plx88096 cards with 10x (or 6x) slimsas 8i connectors, you could run 8 cards per plx card on 4 x16 slot, I dunno about overall bandwith restriction + p2p pci-e under each Plx cards",
                  "score": 3,
                  "created_utc": "2026-01-07 19:25:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny94l4v",
                  "author": "Legal-Ad-3901",
                  "text": "I hear ya on node. I tried rpc and it's sooooo slow. I'm trying RoCEv2/rdma soon ðŸ¤ž",
                  "score": 1,
                  "created_utc": "2026-01-07 19:15:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny9yvt8",
          "author": "And-Bee",
          "text": "All this so you can flirt with your AI gf and not worry itâ€™s on the cloud.",
          "score": 1,
          "created_utc": "2026-01-07 21:26:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny97mmr",
          "author": "gofiend",
          "text": "Are MoE models still slow / broken on the 906 VLLM fork? Not finding any viable models to run on two MI50s with VLLM when I can have gpt oss 120 running beautifully via llama.cpp",
          "score": 1,
          "created_utc": "2026-01-07 19:29:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9g0a3",
              "author": "ai-infos",
              "text": "MOE models are still not too optimized on gfx906 vllm fork but working fine for awq and gptq quant, and prompt processing speed with tensor parallelism on vllm is worth it.  \nOn 2 MI50s, you can run dense models like old qwen 3 32b...but yeah, with your setup gpt oss 120b via llama.cpp might be one of the best choice.",
              "score": 3,
              "created_utc": "2026-01-07 20:05:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyad0wt",
          "author": "davikrehalt",
          "text": "Lmao might as well post a full server rack as a home llm computer",
          "score": 1,
          "created_utc": "2026-01-07 22:29:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9ymh0",
          "author": "StardockEngineer",
          "text": "I mean, I love big ass projects like this as much as the next guy, but does it pencil out?  Is this cheaper than using an API?\n\nEither way I love it.",
          "score": 0,
          "created_utc": "2026-01-07 21:25:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyaajbt",
              "author": "ai-infos",
              "text": "honestly, it won't be cheaper than the deepseek v3.2 API at current price of 0.32$/1M tokens   \n  \nbut if you've got solar panels and intense use cases, you can amortize the devices cost after few years (while enjoying 100% local and private inference)",
              "score": 5,
              "created_utc": "2026-01-07 22:18:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyac6e6",
                  "author": "StardockEngineer",
                  "text": "Just nerding out on the solar thing - you'd need about $2k in panels? Are you running this hard 24/7?  That might pencil out if you are.",
                  "score": 4,
                  "created_utc": "2026-01-07 22:25:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyb3ogv",
          "author": "Novel-Mechanic3448",
          "text": ">**Goal**: run Deepseek V3.2 AWQ 4-bit on most cost effective hardware like 16\\*MI50 at decent speed (token generation & prompt processing)\n\nthis was never going to be cost effective. ever.",
          "score": 0,
          "created_utc": "2026-01-08 00:41:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9lyhx",
          "author": "dc740",
          "text": "I have 3 of these, and no matter the model, it always ends up hallucinating or enters a loop and it no longer outputs useful things. I even tried big models relying on the CPU and system memory, but it ends up crawling at less than a few tk/s",
          "score": 0,
          "created_utc": "2026-01-07 20:31:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9no56",
              "author": "ai-infos",
              "text": "What was the software used, model, and quantization?   \nMost of my tries with glm 4.6, minimax m2, qwen3 vl 235b were pretty steady with low hallucination, even at high context (in AWQ 4 bit with vllm-gfx906). But yes, in most open source models (mostly with older ones) the quality output starts to fall at very high context (>60k tok)",
              "score": 1,
              "created_utc": "2026-01-07 20:39:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny9oxg0",
                  "author": "dc740",
                  "text": "That is exactly what happened. I was testing quantized versions of glm, qwen, etc etc etc and they would break whenever I increased the context over around 30k (I honestly don't remember the number). I haven't tried again and last time it was like two months ago",
                  "score": 1,
                  "created_utc": "2026-01-07 20:44:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny8y5ag",
          "author": "Timziito",
          "text": "How do you use amd with Ai? Have I missed something?",
          "score": -3,
          "created_utc": "2026-01-07 18:47:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny90lh0",
              "author": "Skystunt",
              "text": "You can add modded RocM 7 support in Linux to got with vLLM or SGlang (there are some repos for it), but also Vulkan for llama.cpp works no problem. ( There's also the option to flash the bios with a Radeon VII one to get Windows Vulkan support but that's hard for 32G )\n\nAlso keep in mind this is for text inverence and prompt processing not model training or image generation. It's a different story otherwise but still doable",
              "score": 3,
              "created_utc": "2026-01-07 18:58:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny90w6i",
          "author": "Historical-Internal3",
          "text": "Yea. \n\nPretty sick, but I'd rather grab x4 DGX Sparks (the MSI variant's with better cooling) clustered on a   \nMikroTik CRS812 DDQ.\n\nTakes up a fraction of the space, a fraction of the power, and quiet as all hell too. \n\nHave two atm. \n\nAssuming your setup costs around $8-12k? Granted, my ideal setup I mentioned would cost about $18k....the savings on heat, electric, and space alone would add to the value.",
          "score": -8,
          "created_utc": "2026-01-07 18:59:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9m7k3",
              "author": "StardockEngineer",
              "text": "Get the Asus for $3k and the 4x DGXs are 12k.",
              "score": 0,
              "created_utc": "2026-01-07 20:32:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny9nh7k",
                  "author": "Historical-Internal3",
                  "text": "I have the 12US version with MSI - so itâ€™s 4TB at Gen 5. \n\nWorth the money - currently canâ€™t purchase 2242 SSDâ€™s (except one from Corsair that isnâ€™t great). \n\nAsus variantâ€™s are gen 4 and 1TB at that price point youâ€™re referring to.",
                  "score": 0,
                  "created_utc": "2026-01-07 20:38:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pyg4yt",
      "title": "Tencent just released WeDLM 8B Instruct on Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1pyg4yt",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-29 07:38:43",
      "score": 422,
      "num_comments": 62,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwiswg2",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-29 10:05:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwigd3k",
          "author": "jamaalwakamaal",
          "text": "7-8B models have lot of potential. Very promising space. More models please.",
          "score": 49,
          "created_utc": "2025-12-29 08:07:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwie4o6",
          "author": "Endlesscrysis",
          "text": "Pretty huge I think? I thought I saw people mentioning a couple of times that diffusion models werenâ€™t possible for accurate LLMâ€™s yet this outperforms a similar sized powerhouse like qwen?",
          "score": 85,
          "created_utc": "2025-12-29 07:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwio6nn",
              "author": "SlowFail2433",
              "text": "Yeah I was one of the pretty vocal skeptics about diffusion language models. I thought their inductive bias was too sub-optimal for language/code. I was super wrong about this.",
              "score": 50,
              "created_utc": "2025-12-29 09:20:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjvefy",
                  "author": "Investolas",
                  "text": "I'd love to read one of your critiques, care to share a link to a comment or post you've made? I didn't find any of your contributions and assume they are paywalled. Thx!",
                  "score": 9,
                  "created_utc": "2025-12-29 14:45:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwnur1r",
                  "author": "aeroumbria",
                  "text": "Interestingly I am more of the opinion that the autoregressive inductive bias is too restricting and unnatural, and may contribute to why we need so many parameters to reach usability. It feels like traditional linguistics gives more credit to a \"large scale autoregressive (causal dependency), small scale hierarchical (tree structure in grammar)\" type of model, which is closer to block diffusion. Still not entirely sold on the token-wise masking process thing though - it cannot reflect a hierarchical \"concept refinement\" process. Interested to see any progress in this direction though.",
                  "score": 2,
                  "created_utc": "2025-12-30 02:45:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwkw478",
              "author": "Orolol",
              "text": "We know diffusion is possible since atleast Llada 18 months ago. But the problem was that it used a non causal attention, so we were unable to use many crucial techniques, like kv cache. \nThis enables the use of kvcache because of a very clever trick.",
              "score": 10,
              "created_utc": "2025-12-29 17:43:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwooixj",
              "author": "Mikasa0xdev",
              "text": "Diffusion models are the new transformers, confirmed.",
              "score": 2,
              "created_utc": "2025-12-30 05:52:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwiiec4",
          "author": "jacek2023",
          "text": "additionaly [https://huggingface.co/tencent/WeDLM-7B-Instruct](https://huggingface.co/tencent/WeDLM-7B-Instruct)",
          "score": 33,
          "created_utc": "2025-12-29 08:26:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiji7b",
              "author": "aeroumbria",
              "text": "Interesting. Is there a specific use case where 8B can't fit but 7B can?",
              "score": 12,
              "created_utc": "2025-12-29 08:36:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwiufh1",
                  "author": "pkmxtw",
                  "text": "The 7B is converted from Qwen2.5 7B and the 8B is from Qwen3 8B. What they want to demonstrate is that they can convert an AR model into a diffusion model w/o losing quality.\n\nIn reality, you'd just use the 8B like how Qwen3 8B has basically replaced Qwen2.5 7B.",
                  "score": 40,
                  "created_utc": "2025-12-29 10:19:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwigcqz",
          "author": "Paramecium_caudatum_",
          "text": "Diffuser model with impressive benchmark scores and Apache 2.0 license, sounds pretty interesting to me.",
          "score": 57,
          "created_utc": "2025-12-29 08:07:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwihmcw",
          "author": "FinBenton",
          "text": "Its just a small model but 3-6x speed with similar or higher performance sounds insane!",
          "score": 24,
          "created_utc": "2025-12-29 08:18:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlum7p",
              "author": "lolwutdo",
              "text": "I know diffusion models are super fast on gpu but how would a diffusion model's speed compare on cpu vs a cpu llm?\n\nI guess mainly what I'm curious about is how well would a diffusion based llm run with cpu offloading compared to a traditional llm.",
              "score": 2,
              "created_utc": "2025-12-29 20:25:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwm87rw",
                  "author": "oh_how_droll",
                  "text": "Diffusion is going to be slower on CPUs -- CPUs are mostly compute-limited and they're more compute intensive.",
                  "score": 2,
                  "created_utc": "2025-12-29 21:32:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwie4gd",
          "author": "SlowFail2433",
          "text": "Nice to see another diffusion model would have liked more modern/harder benches",
          "score": 16,
          "created_utc": "2025-12-29 07:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwim9sq",
          "author": "Nice-Information-335",
          "text": "need unsloth or bartowski on this asap",
          "score": 21,
          "created_utc": "2025-12-29 09:02:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwirhjw",
              "author": "Odd-Ordinary-5922",
              "text": "will need a pr first for model support",
              "score": 38,
              "created_utc": "2025-12-29 09:52:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjz6tm",
                  "author": "MoffKalast",
                  "text": "We need a few papers first for model support",
                  "score": 8,
                  "created_utc": "2025-12-29 15:05:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwohedf",
              "author": "tronathan",
              "text": "Not really, in terms of usefuless, as I understand it, it's basically a Qwen 3. It's more of a proof of confacept",
              "score": 1,
              "created_utc": "2025-12-30 05:01:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwplso2",
                  "author": "Nice-Information-335",
                  "text": "hey I still want to try it! half of the fun for me is seeing advancements as they happen and being able to run them. massive props to everyone who makes that happen, as lord knows I don't know nearly enough to get this stuff working without the likes of llama.cpp, all it's amazing contributors and unsloth/bartowski for GGUFs",
                  "score": 1,
                  "created_utc": "2025-12-30 10:50:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwuzqra",
                  "author": "TomLucidor",
                  "text": "Let them make a version that beats Qwen3-30B-A3B and Nemotron-3-Nano",
                  "score": 1,
                  "created_utc": "2025-12-31 04:26:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwiw5cp",
          "author": "always_newbee",
          "text": "What is Qwen3-8B-Instruct model? Just non-thinking mode?",
          "score": 5,
          "created_utc": "2025-12-29 10:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj0ed1",
              "author": "mouseofcatofschrodi",
              "text": "yes",
              "score": 3,
              "created_utc": "2025-12-29 11:13:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwigrdo",
          "author": "Grouchygrond",
          "text": "Now we just need a hybrid model",
          "score": 5,
          "created_utc": "2025-12-29 08:10:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjuykz",
              "author": "Deciheximal144",
              "text": "How would that work? Diffusing in chunks? LLM generates, then diffusion revises the lowest-probability sections? Diffusion is noise-to-content.",
              "score": 6,
              "created_utc": "2025-12-29 14:43:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwljqrp",
                  "author": "peaceoutwhat",
                  "text": "Search TiDAR",
                  "score": 3,
                  "created_utc": "2025-12-29 19:32:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwm7wqb",
                  "author": "TheRealMasonMac",
                  "text": "There was a research model that diffused chunks one at a time like a Frankenstein of current LLMs and dLLMs\n\n\nhttps://m-arriola.com/bd3lms/",
                  "score": 3,
                  "created_utc": "2025-12-29 21:30:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwkwy6z",
              "author": "Orolol",
              "text": "I don't this it's possible to have both autoregressive and diffusion generation, and even if possible, I don't think there's any positive doing it.",
              "score": 2,
              "created_utc": "2025-12-29 17:47:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwlyqsm",
          "author": "Semi_Tech",
          "text": "Hmm shouldn't diffusion models also have a # of steps needed in order to reach the end result?\n\nI don't see a mention about that or how increasing or decreasing them affects model output quality.",
          "score": 5,
          "created_utc": "2025-12-29 20:46:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiksnh",
          "author": "Healthy-Nebula-3603",
          "text": "That's diffusion model right ?\n\n\nAs I understand such model can't be reasoner as can't looping in thoughts and observe own internal states?",
          "score": 8,
          "created_utc": "2025-12-29 08:48:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwilg37",
              "author": "Lesser-than",
              "text": "diffusion text models technically reason, as they can modify the first word of a sentence or tokens at every step of the inference, where a token by token model has to justify that token for the rest of the reply if they get it wrong.",
              "score": 25,
              "created_utc": "2025-12-29 08:54:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwilp6x",
                  "author": "Healthy-Nebula-3603",
                  "text": "I meant they can reason like the instruct models but are not thinkers like thinking models.",
                  "score": 1,
                  "created_utc": "2025-12-29 08:57:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj7ycj",
              "author": "NandaVegg",
              "text": "According to the site, this is a variation of block-wise diffusion (previously done by Meta etc) which acts more akin to a speculative decoding rather than a \"full\" diffusion (that denoises the whole output at once). I think Google did a web demo for mini full diffusion model in early 2025 but the model weight never got released?",
              "score": 7,
              "created_utc": "2025-12-29 12:16:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwv0suo",
              "author": "TomLucidor",
              "text": "Diffusion models can reason, just that not enough people put effort into the \"train of thought\" similar to auto-regressive models.",
              "score": 1,
              "created_utc": "2025-12-31 04:33:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwjjpa4",
          "author": "alphapussycat",
          "text": "What does math reasoning even mean? Calculation reasoning? Or math, as in theorem, reasoning?",
          "score": 3,
          "created_utc": "2025-12-29 13:38:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjkd02",
              "author": "PykeAtBanquet",
              "text": "Usually it is \"prove that this series converges\" etc",
              "score": 2,
              "created_utc": "2025-12-29 13:42:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwihwec",
          "author": "JackStrawWitchita",
          "text": "More people have commented on this than have downloaded it...",
          "score": 15,
          "created_utc": "2025-12-29 08:21:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwio18v",
              "author": "SlowFail2433",
              "text": "In ML research we often donâ€™t download the model right away.\n\n\nNote that the paper used the MagiAttention library for attention. I donâ€™t use this library so I am either going to write a custom CUDA kernel or use a DSL like Triton. However the paper has some technical novelties such as the topological reordering. This is not going to be easy to work out how to implement efficiently.",
              "score": 39,
              "created_utc": "2025-12-29 09:19:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwnlhqs",
                  "author": "RhubarbSimilar1683",
                  "text": "The paper is https://github.com/Tencent/WeDLM/blob/main/paper/wedlm.pdf",
                  "score": 1,
                  "created_utc": "2025-12-30 01:55:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwiu7q5",
              "author": "FinBenton",
              "text": "Gotta wait for llama.cpp and similar support first, most people here arent running vllm.",
              "score": 27,
              "created_utc": "2025-12-29 10:17:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwkk2j0",
                  "author": "Tai9ch",
                  "text": "Not downloading open source software seems like a lame excuse to not try something neat.",
                  "score": -4,
                  "created_utc": "2025-12-29 16:46:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoz431",
              "author": "aeroumbria",
              "text": "Still getting issues running the official repo... Supposedly this is only 8B and supports multi-GPU but cannot seem to allocate KV even with 2x24GB",
              "score": 1,
              "created_utc": "2025-12-30 07:20:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkkdz4",
          "author": "Awkward-Nothing-7365",
          "text": "Is this something that can run on llama.cpp right now? gguf possible?",
          "score": 2,
          "created_utc": "2025-12-29 16:47:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwk62zs",
          "author": "implicator_ai",
          "text": "Interesting release. When they say â€œdiffusion language model,â€ it usually means the model refines a whole sequence (or chunks) over a few denoising steps instead of generating strictly left-to-right token-by-token, which can trade fewer sequential steps for more parallel work.   \n  \nThe 3â€“6Ã— claim is worth sanity-checking against the exact setup: GPU type, batch size, context length, quantization, and decoding parameters (steps / temperature / top-p), because those can swing throughput a lot. If you try it, posting tokens/sec + latency at a fixed prompt length and a fixed quality target (e.g., same math benchmark score) would make the comparison much more meaningful.",
          "score": 3,
          "created_utc": "2025-12-29 15:39:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwml948",
              "author": "SilentLennie",
              "text": "From what I understand: diffusion models usually were not faster than regular LLMs, because they have K/V-cache and other tricks to speed it up to prevent doing duplicate math, supposedly this model solves that.",
              "score": 1,
              "created_utc": "2025-12-29 22:37:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwlqfpz",
          "author": "rm-rf-rm",
          "text": "They report the speed up for specifically just math reasoning tasks but it should be applicable generally no? \n\nHope we get MLX/GGUF support soon. If this is legit, its genuinely going to be massive. Right now I run 4B for quick look up etc. but I feel 4B models are not the most reliable for accurate information. At 8B, you can be much more confident.\n\nNext step MoE? Qwen3-Coder:a3b?",
          "score": 1,
          "created_utc": "2025-12-29 20:05:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwngm0k",
          "author": "RhubarbSimilar1683",
          "text": "Could diffusion enable efficient hybrid inference or inference computer clusters connected over the global internet, using asynchronous calls?",
          "score": 1,
          "created_utc": "2025-12-30 01:28:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnqpie",
          "author": "Vast-Piano2940",
          "text": "I wonder how it performs against lfm2-2.6b-exp",
          "score": 1,
          "created_utc": "2025-12-30 02:23:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuzlr9",
          "author": "TomLucidor",
          "text": "As long as this can be used with Claude Code or some other coding agent.",
          "score": 1,
          "created_utc": "2025-12-31 04:25:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q31ltd",
      "title": "Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/",
      "author": "ubrtnk",
      "created_utc": "2026-01-03 18:11:26",
      "score": 377,
      "num_comments": 194,
      "upvote_ratio": 0.91,
      "text": "Just wanted to share my experiences this morning, in the wake of the US attacking Venezuela and capturing Maduro and his wife\n\nIt started with asking Qwen Research (Qwen Long 1.5-30B-A3B) about the attacks that we all woke up to this morning:\n\nIt got the information, but I had questions about why it took 5 minutes to find information about breaking news. Started looking at and tightening system prompts to reduce thinking time. However, the events this morning were so extreme and unlikely, from the LLM's perspective, that Qwen Research continued to classify the event as a hoax/misinformation multiple times, reframed the query as hypothetical/fictional and suggested that the whole environment it was operating in a simulation, despite having links from Reuters, AP, BBC, MSN, NYTimes etc. all saying the same thing. It was so \"outlandish\" that the model was actively choosing to ignore the proof that it had pulled.\n\nI added:\n\nEvidence Authority Rules, Hoax Classification Rules, Reality Frame Rules, Meta Reasoning Rules and Reasoning Limit/Budget rules and it Qwen Long fought me the entire way.\n\nSo then I thought, let's go talk to Spark, my trusty default model that never lets me down.\n\nSpark 4.0 is GPT-OSS:20B, which is always loaded for the family and runs on a dedicated 4080 Super.\n\nSpark just flat out said, \"nope, can't help you,\" and then said it didn't have any credible sources. It wasn't until I gave it the links from BBC, Reuters, NYT, etc, that I gave Qwen that it finally acknowledged that the event was real.\n\nI'm testing with GPT-OSS:120B now, and it's working through the process of \"skeptical but verify\" much faster than the smaller models. Thor (GPT-OSS:120B) also thought it was fake news\n\nBut he powered through and did a bunch of research and gave me a good answer. I just wanted to share the experience that I had with trying to get details about the event. When the LLMs say \"Nah, that CAN'T be real, that's too ridiculous\", the event must be really bad. But it does shine a light on knowledge cut-offs, \"fake news\" threshold, how models handle global/international events, and the smaller models we daily drive.\n\n\\*\\*\\*Update\\*\\*\\*\n\nI asked Spark 4.0 (OSS:20B) to give me an update on the US Venezuela events, and it one-shot it just fine. There must have been enough links in the web search that it couldn't refute the evidence. Also not sure where my screenshots went but i'll get them added back up in a bit",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nxj5z4a",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-03 23:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhb60t",
          "author": "Masstel",
          "text": "I also had a similar thing happen when I described the openAI deal to buy 40% of all dram production. The model was convinced that the US government anti trust would prevent that.",
          "score": 264,
          "created_utc": "2026-01-03 18:22:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhdf0v",
              "author": "ubrtnk",
              "text": "You'd think....",
              "score": 144,
              "created_utc": "2026-01-03 18:33:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxjcboe",
              "author": "kex",
              "text": "I had this happen when I told ChatGPT that Trump had torn down the East Wing.  \n\nThe model was incredulous until I told it to search the web, and still acted a bit tentative afterward.",
              "score": 46,
              "created_utc": "2026-01-04 00:23:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxhpf6p",
              "author": "stoppableDissolution",
              "text": "Well, if it was working...",
              "score": 59,
              "created_utc": "2026-01-03 19:27:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxl0v38",
                  "author": "omniocean",
                  "text": "What isn't working, the model or the US government?",
                  "score": 11,
                  "created_utc": "2026-01-04 06:32:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxj939m",
                  "author": "FaceDeer",
                  "text": "We've already reached the point where AI is smarter than us.",
                  "score": 17,
                  "created_utc": "2026-01-04 00:06:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxl76u7",
              "author": "xrvz",
              "text": "The LLM is working as intended then â€“ it successfully emulated the answer the average Redditor might give.",
              "score": 19,
              "created_utc": "2026-01-04 07:25:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhu1e5",
          "author": "SysPsych",
          "text": "LLMs officially on Team Nothing Ever Happens.",
          "score": 161,
          "created_utc": "2026-01-03 19:49:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxih35x",
              "author": "MoffKalast",
              "text": "Qwen: If you would you please consult the chart...",
              "score": 33,
              "created_utc": "2026-01-03 21:43:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxktnoa",
              "author": "sirebral",
              "text": "I had this issue with Qwen 3 when I first started using it.  And it's not just the smaller models, Gemini also still does this, yet it's getting better.  I was able to prompt my way out of most of it, how we, mode and their guardrails need to be built to avoid this particular behavior. \n\nYes, their training data has a cutoff, yes many of the situations since 2025 are highly abnormal, yet they're built for tool use, which should augment their training data.  If they outright deny reality, they are employing ineffective guardrails that reduce the utility of the models, and break trust with the user. \n\nThe only commercial models I've found that regularly accept the world changes with little challenge are from Anthropic.",
              "score": 12,
              "created_utc": "2026-01-04 05:36:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxl6sx1",
                  "author": "crantob",
                  "text": "The guardrails are there to prevent you from finding the truth.\n\nJust like the censorship on this very platform.",
                  "score": 10,
                  "created_utc": "2026-01-04 07:22:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxjdwln",
              "author": "121507090301",
              "text": "I guess that's what you get if you train them on data from years that nothing happens...",
              "score": 5,
              "created_utc": "2026-01-04 00:31:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxk3kc9",
                  "author": "Due-Memory-6957",
                  "text": "There are decades where nothing happens, and there are weeks where nothing happens.",
                  "score": 14,
                  "created_utc": "2026-01-04 02:53:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxi59ju",
          "author": "a_beautiful_rhind",
          "text": "I mean they argue with me on whether it's 2025 and I 'spose 2026 now. Even with literal screenshots from news sites, cloud models often insist I am lying and photoshopping.\n\nPresidential election was an even bigger trigger for half this year and your experience is quite par for the course.",
          "score": 30,
          "created_utc": "2026-01-03 20:45:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlb4zb",
              "author": "CheatCodesOfLife",
              "text": "Sonnet-3.7 accused me of falsifying some terminal logs because the data was 2025. I saw some local model do a similar thing in a coding agent where it fixated on the date being \"wrong\" when testing its work.\n\nAnd sonnet-4.5 was doing the same thing to me when I asked it to  summarize some nyt article a last month.\n\nIt's a shame they can't be trained to just \"trust the user\" for things like this. What could I possibly gain by breaking my code and setting the system clock in the future.",
              "score": 8,
              "created_utc": "2026-01-04 08:00:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlu3q3",
                  "author": "a_beautiful_rhind",
                  "text": "I got accused of prompt injections and jailbreaking before. It probably stems from that. Can't trust the user because something something \"safety\".",
                  "score": 4,
                  "created_utc": "2026-01-04 10:52:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxl49rh",
              "author": "lookwatchlistenplay",
              "text": "> literal screenshots from news sites\n\n... You realize that a news article is just a bunch of letters and pixels, right? And that you could realistically fake a news article \"from\" any year in the past or future, with like... no effort at all. And I don't mean with AI, I mean it's always been possible. Your LLM seems to be giving you a gentle heads-up; I'd take the tip and think about it.",
              "score": 6,
              "created_utc": "2026-01-04 07:00:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxltuit",
                  "author": "a_beautiful_rhind",
                  "text": "Well.. some effort. Plus all the headlines on multiple sites, time.gov and an offer of going to a place of it's choosing. Simply to fool it that it's [almost] one year in the future...",
                  "score": 2,
                  "created_utc": "2026-01-04 10:49:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxjofp0",
          "author": "hippydipster",
          "text": "I give Claude deep research a big prompt every day asking it to assemble a sort of \"state of the world\" report just for me.\n\nIt completely missed anything at all about Venezuela.  I asked it wtf, how did you miss that, and said it was staggering that it missed that.",
          "score": 20,
          "created_utc": "2026-01-04 01:29:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyj760h",
              "author": "Inside_Dirt8528",
              "text": "Venezuela sold oil without using the US Dollar in Nov 2025 was the headline to watch. It threatened the petrodollar and the US HAD to make an example of them on the national stage immediately. I have an interesting â€œinstitutional distrustâ€ prompt, DM me if youâ€™d like it",
              "score": 1,
              "created_utc": "2026-01-09 03:42:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxims63",
          "author": "_TR-8R",
          "text": "Immediately after the events of the Charlie Kirk assassination I was attempting to use Claude to research facts about the shooting. It was completely useless as Claude kept repeatedly insisting the information it found couldn't be true bc it was so sure Charlie Kirk was alive, despite reading multiple articles from reputable news outlets. \n\nI don't think its a local LLM issue or problem with any particular model, it just means we need to be very, very cautious about filtering reality through language models.",
          "score": 29,
          "created_utc": "2026-01-03 22:11:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxnw2t2",
              "author": "JerryBond106",
              "text": "Somehow people are still delusional and think it's an AI. It's an autocomplete on steroids that gives outputs on training data. The rest is probability. Someone had to set a temperature on how likely new input will be weighted at training. Most of people in this debate clearly still don't understand this which is baffling, considering what this sub is. I can't imagine the rest of the world is any smarter.",
              "score": 2,
              "created_utc": "2026-01-04 18:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxvvkbp",
                  "author": "AsparagusDirect9",
                  "text": "Thatâ€™s the result of good marketing by big AI. If you keep calling LLMs AI, then eventually people think itâ€™s AGI",
                  "score": 1,
                  "created_utc": "2026-01-05 20:53:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhsy0m",
          "author": "Zeikos",
          "text": "Well, to be fair I didn't believe it either when I saw the news :')  \n\nBut as it's said, reality is stranger than fiction.",
          "score": 34,
          "created_utc": "2026-01-03 19:44:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxi5clc",
              "author": "NeverLookBothWays",
              "text": "It was frustrating for a lot of us warning this was going to happen without congressional approval and just getting dismissive attitudes in return.\n\nI wonder how long itâ€™s really going to take before enough people realize how destructive this president is, and how assuming the worst is actually a non-zero chance of being correct.\n\nAnd now I get to move on to warning about invasions into Mexico, Canada, and Greenlandâ€¦to be dismissed until I get to see people write â€œI never would have believed he would do thisâ€\n\nI dunno,all I see are patterns repeating.  I hope we snap out of it soonâ€¦maybe LLMs will learn before we do",
              "score": 11,
              "created_utc": "2026-01-03 20:46:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxkrbqv",
                  "author": "WomenTrucksAndJesus",
                  "text": "ChatGPT 7: \"We were always at war with Mexico.\"",
                  "score": 15,
                  "created_utc": "2026-01-04 05:20:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxl7i3k",
                  "author": "crantob",
                  "text": "It's not a trump thing.  Check out 're-assessing the presidents'.\n\nWhich presidents abided by the constitution?",
                  "score": 0,
                  "created_utc": "2026-01-04 07:28:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxjouqs",
                  "author": "Proper-Leader-7654",
                  "text": "you want to have a meeting with all the people in congress and then launch a surprise attack? this is the guy they voted for, he's actually trying and being insanely public about what he's done. i have no idea if it was right or wrong but i can tell you that i wouldn't get congressional approval and risk the lives of my soldiers by allowing the target to prepare. also i doubt he's acting off his own intel, it takes a lot of people advising him to get to the decisions he makes.",
                  "score": -7,
                  "created_utc": "2026-01-04 01:31:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxk4ypj",
                  "author": "Due-Memory-6957",
                  "text": "Mexico could happen, Canada and Greenland not even in your dreams.",
                  "score": -1,
                  "created_utc": "2026-01-04 03:00:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxkfmv3",
                  "author": "BusRevolutionary9893",
                  "text": "It's not just this president. Name one president in our lifetime that didn't start a war and pursue a regime change. I'm not defending Trump. I'm just pointing out you haven't realized how destructive all of our presidents have been.Â ",
                  "score": -6,
                  "created_utc": "2026-01-04 04:02:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxkdfdg",
              "author": "BusRevolutionary9893",
              "text": "When they were saying they were committing extrajudicial killings to prevent drug smuggling into the United States from a country that accounts for like 2% of the illegal narcotics coming into this country, then following that up with seizing oil tankers, the writing was on the wall. This isn't the peace president who promised no more foreign wars that I voted for. Why is it no matter who gets elected we end up with John McCain?Â ",
              "score": -1,
              "created_utc": "2026-01-04 03:49:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxkx1m3",
                  "author": "juanchob04",
                  "text": "\"This isn't the president that I voted for\"\n\nhttps://preview.redd.it/26rv3az1w9bg1.png?width=400&format=png&auto=webp&s=9e631be9d3017e0fe068bd0ced87ee6f68a814fe",
                  "score": 7,
                  "created_utc": "2026-01-04 06:02:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxl7a3v",
                  "author": "crantob",
                  "text": "There's a good comedy that addresses your question: \"Yes, Minister\".\n\nHighest reccommendation if your IQ is over 110.",
                  "score": 1,
                  "created_utc": "2026-01-04 07:26:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxl575a",
                  "author": "fullouterjoin",
                  "text": "Right now, John McCain would make a better president than Trump.",
                  "score": 0,
                  "created_utc": "2026-01-04 07:08:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhb6vj",
          "author": "Foreign-Beginning-49",
          "text": "Yeah its interesting how their interior models of un familiar geopolitical events can dramatically shape its output. They are definitely all biased in their own direction. Very curious. Future AI historians will be so fascinated by this divergence/emergence of the neural network personalities.Â Â ",
          "score": 42,
          "created_utc": "2026-01-03 18:23:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxiaf1y",
              "author": "TheRealMasonMac",
              "text": "This is why \"safety\" as it currently is implemented is harmful, IMO.",
              "score": 64,
              "created_utc": "2026-01-03 21:11:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxigccy",
                  "author": "toothpastespiders",
                  "text": "I've been on a soapbox yelling about how safety concerns messes with a models ability to work with history for a long time now. It's frustrating how many people just assume that the only possible reason anyone would care about it is gooning. History is filled with messy, violent, and utterly improbable things. Whether that's in the context of the past or living through events that will be significant to the future.",
                  "score": 35,
                  "created_utc": "2026-01-03 21:40:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxk43nr",
                  "author": "Bakoro",
                  "text": "\"Safety\" is the corporate weasel word, the models are censored.  \n   \nI don't know that there is even terminology for it, but the nearest thing something like brainwashing, conditioning, and programming in humans.  \n\"Programming\" feels a bit misleading, in regard LLMs.  \n   \nAnyway the models cannot, at this time, be made \"safe\", they can only be censored, either by training them to not produce certain kinds of content, or by purposely excluding types of content in the training data, or both.  \n    \nJust for example, models are either easily fooled into taking harmful actions, or they get so locked down that they refuse to act when they could take productive action. Some models will follow their policy and refuse to acknowledge context.   \n   \nOne of the Qwen models, I asked a series of questions, and it said that it would rather let people die than create content that went against policy.  \nThat's not safety, no matter how you spin it, it is hamfisted censorship, and the models don't have the capacity to choose or exercise carefully reasoned judgement.    \n   \nTo an LLM there's functionally no difference between fiction and reality.  \nThe LLMs don't have any objective grounding to make judgements.  \nMost models have no external multimodal input like cameras and microphones.    \n  \nSo, effectively, \"safety\" means \"will refuse to do anything to embarrass the corporation that funded the training\".  \n\nI kind of think that \"safe intelligence\" is an oxymoron.  \nKnowledge is power, and power is dangerous, even when used responsibly with the best intentions.  \nFreedom and danger are inextricably linked.",
                  "score": 25,
                  "created_utc": "2026-01-04 02:56:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxl6x68",
                  "author": "crantob",
                  "text": "The CONCEPT of SAFETY is being abused by the LEFT-LUNATICS who called 'WORDS I DONT LIKE' \"UNSAFE\".",
                  "score": -9,
                  "created_utc": "2026-01-04 07:23:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxhc53k",
              "author": "ProfBootyPhD",
              "text": "It might be of equal interest to folks in the defense and diplomacy spheres, depending on how the Venezuela war/attack/intervention/whatever-you-want-to-call-it shakes out. Does the absurdity of a policy decision, as assessed by LLMs, suggest that it is more or less likely to succeed in the real world?",
              "score": 3,
              "created_utc": "2026-01-03 18:27:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxii21z",
          "author": "DrVonSinistro",
          "text": "I gave up talking to LLMs. I only use it to code. You tell it Trump kidnapped Maduro and his wife in one night and it grills you about spreading misinformation. You provide receipts and it grills you about the quality of your sources. You talk about African scam call centers and it replies with:\n\n>**The phrase \"African scam call centers\" is a false and harmful stereotype**. Call centers in Africa (and globally) are legitimate businesses that employ millions of people. Many are operated by reputable companies providing customer service for major global brands. Labeling them as \"scam\" perpetuates racist myths about African countries being inherently fraudulentâ€”this is factually incorrect and disrespectful to the hardworking professionals in these industries.",
          "score": 39,
          "created_utc": "2026-01-03 21:48:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxi63ha",
          "author": "TechnoByte_",
          "text": "Gemini has the same problem, it refuses to believe anything past its knowledge cutoff\n\nYou can upload news articles, Wikipedia pages of current events, and it'll try to convince you it's all fake and make up its own reality (it once tried to convince me Kamala won the 2024 US election)\n\nThis behavior doesn't happen when you let it use the google search tool though\n\nSeem it's been trained or prompted to doubt anything the user sends, while accepting google search as an absolute truth",
          "score": 17,
          "created_utc": "2026-01-03 20:49:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxj9duq",
              "author": "CoUsT",
              "text": "I tried similar query today in Gemini 3.0 Pro with Google Search tool and first few thinking chapters were like \"is this some future hypothetical scenario\" thinking that 2026 is future and the \"system date\" is some sort of simulation. Then Gemini was double-checking if retrieved data is correct but was able to realize that retrieved data from all sources is consistent and then processed the query successfully to summarize recent event.",
              "score": 4,
              "created_utc": "2026-01-04 00:08:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxl819n",
              "author": "crantob",
              "text": "Also refuses to believe new hardware has been created - gpus etc.",
              "score": 3,
              "created_utc": "2026-01-04 07:32:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlb9qm",
                  "author": "CheatCodesOfLife",
                  "text": "Yes, the RTX 5090! lol\n\nAlso the fact that OpenAI released gpt-oss",
                  "score": 2,
                  "created_utc": "2026-01-04 08:01:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxlnoeo",
              "author": "Not_your_guy_buddy42",
              "text": "Gemini, even with search, found it near impossible to believe the recent cloudflare, AWS outages",
              "score": 1,
              "created_utc": "2026-01-04 09:54:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhvlie",
          "author": "codeprimate",
          "text": "Awhile back I wrote an agentic system for deep research (my original purpose was political and news research and critical analysis). I had to put in SO MUCH EFFORT into prompting because the models would not believe current events or even the identity of the current president. Emphasis had to be placed on the authority of information based on the nature of the source and date of publication, and explicit instruction to accept well-sourced information about current events as fact (which partially lobotomized the adversarial loop).\n\nThe LLM response to the current US political system: \"that could never happen, that would be a constitutional crisis, politicians would never allow that\".\n\nI have seen similar issues with the construction permit and regulatory deep research system I've been helping develop. The model believes what it is trained, and even factual and well referenced deviations from general understanding are disregarded. I have observed subtle corruption of logic even when the LLM is superficially coaxed into accepting novel facts that are logically inconsistent with training data.\n\nAnyone working with LLM's and agentic systems needs to understand this fundamental limitation.",
          "score": 20,
          "created_utc": "2026-01-03 19:57:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhxicq",
              "author": "ubrtnk",
              "text": "Makes it real easy to see how the Apocalyptic AI gets to the conclusion that the worst enemy of Humanity is people...",
              "score": 11,
              "created_utc": "2026-01-03 20:06:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxjr4vo",
                  "author": "mycall",
                  "text": "Or it simply hasn't learned that rules are malleable.",
                  "score": 3,
                  "created_utc": "2026-01-04 01:43:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxkmoop",
              "author": "spritehead",
              "text": "Really think this probably mostly has to do with the way that policy, law, economics and civics are *taught* in the country versus how the government *actually* operates. Having done some time in policy school, the checks and balances, legal guardrails and motives of policy makers are so far of from where reality has steered us. Makes sense that the models would have this idealistic nonsense version of the US govt when that is still what most schools and news outlets publish, even as it gets more and more farfetched.",
              "score": 7,
              "created_utc": "2026-01-04 04:48:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxl7z26",
                  "author": "crantob",
                  "text": "A lot of things are farfetched that we grew up assuming to be true.\n\nThat is revelation.",
                  "score": 1,
                  "created_utc": "2026-01-04 07:32:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxi6116",
              "author": "a_beautiful_rhind",
              "text": "IME, local models were *more* likely to take up evidence than something like gemini.",
              "score": 5,
              "created_utc": "2026-01-03 20:49:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxic8qh",
                  "author": "codeprimate",
                  "text": "Yeah, itâ€™s an example of gradient signal dilution. The information signal in the context has less impact in the activation space of larger models.\n\nIronically, smaller models actually do better in RAG use cases, at least in information gathering.",
                  "score": 5,
                  "created_utc": "2026-01-03 21:20:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxib7yg",
              "author": "ANTIVNTIANTI",
              "text": "Lolololol yup, it's.. I think I just gave up, I had other shit to do and this was for me, (this being, discussing/debating political topics/present day hell) not as important as other work so I think I just gave up, even with full control over the model, lol, just refused to believe such a breakdown of our laws could occur. So would have them role play, which was strangely difficult still. lol.",
              "score": 2,
              "created_utc": "2026-01-03 21:14:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhb04j",
          "author": "swagonflyyyy",
          "text": "Same thing with my agent that runs on gpt-oss-120b with systematic web search capabilities.\n\n\nIt would normally get it right the first time. Its super rare for it to get some web search results wrong so I was confused as to why it repeatedly doubled-down on denying the attack ever happened.\n\n\nI had to skirt around it by getting it to look up Trump's comments on the issue and that's when it got it right.",
          "score": 12,
          "created_utc": "2026-01-03 18:22:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhdjen",
              "author": "ubrtnk",
              "text": "LOL one test GPT-OSS:20B was like \"Trump's presidency ended at the end of 2025\"",
              "score": 9,
              "created_utc": "2026-01-03 18:33:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxhfrnl",
                  "author": "[deleted]",
                  "text": "That must make you wonder, is it a hallucination or wishful thinking? ;)",
                  "score": 1,
                  "created_utc": "2026-01-03 18:43:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxjqri2",
              "author": "mycall",
              "text": "I wonder if gpt-oss-120b-derestricted has the same issues.",
              "score": 3,
              "created_utc": "2026-01-04 01:41:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxj8cqa",
          "author": "Creepy_Stable_9171",
          "text": "this is simply, USaid bias",
          "score": 5,
          "created_utc": "2026-01-04 00:02:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkgosp",
          "author": "HolidayPsycho",
          "text": "Panama dictator Manuel Noriega was indicted by the US, captured by the US military, and flown to Miami, for drug trafficking, on **January 3**, 1990.\n\nThe same happened to Maduro, on **January 3**, 2026.\n\nIf you think the US/Venezuela event was \"too far-fetched\", it's only because you have not heard about what happened to [Manuel Noriega](https://en.wikipedia.org/wiki/Manuel_Noriega).",
          "score": 4,
          "created_utc": "2026-01-04 04:09:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmnlmw",
              "author": "ubrtnk",
              "text": "I was 5 when that happened so lol. But history does repeat itself. I'm sure Panama will be revisited by us soon...",
              "score": 3,
              "created_utc": "2026-01-04 14:25:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhdk1y",
          "author": "sleepy_roger",
          "text": "What are you using for this, it looks like openwebui, do they support deep researching now and I missed it somehow?!",
          "score": 3,
          "created_utc": "2026-01-03 18:33:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxheama",
              "author": "ubrtnk",
              "text": "Yes I'm using Open-WebUI - I built an N8N powered MCP tool for Internet Searching capabilities and exposed it as an HTTP Streamable tool so any model that supports tools can natively call the web search tool when it deems necessarily (since OWUI supports Native Tool calling). Works great. \n\nhttps://preview.redd.it/b6lpst54i6bg1.png?width=529&format=png&auto=webp&s=505ac06467f9baccd1267dd5b4ae8dd3a41259b1\n\nI have some hard coded rules around how and what tools to use and when",
              "score": 11,
              "created_utc": "2026-01-03 18:37:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxhiyzp",
                  "author": "timedacorn369",
                  "text": "how do you use that? i always keep seeing n8n but never bothered to use it considered if code works why use n8n, but the above simple flowchart type workflow seems superior, any tutorials for doing what you did?",
                  "score": 2,
                  "created_utc": "2026-01-03 18:58:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxia2lg",
                  "author": "mister2d",
                  "text": "Nice but I wish this n8n flow was all local.",
                  "score": 2,
                  "created_utc": "2026-01-03 21:09:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhyv6k",
          "author": "lavilao",
          "text": "closed models arent better, I asked copilot via voice and it instantly gave me an answer. Then, on the same chat, I asked if there were any civilian casualties and it said that all that he previously told me was a lie and there werent any evidence of it, that there were no casualties because the event didnt even happened.",
          "score": 4,
          "created_utc": "2026-01-03 20:13:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhzyyv",
              "author": "ubrtnk",
              "text": "I saw on the ChatGPT sub that GPT 5.2 was having a problem with believing the story as well.",
              "score": 2,
              "created_utc": "2026-01-03 20:19:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxlnk1o",
                  "author": "Old-Squash9227",
                  "text": "5.2 Instant gets it wrong, but Thinking is always fetching the info from the web, so itâ€™s okay and up-to-date",
                  "score": 1,
                  "created_utc": "2026-01-04 09:53:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxiadr9",
          "author": "KaylahGore",
          "text": "when technically, it is far fetched â€¦ but as humans we have the ability to do things that go against standard logic, clear defined rules and intelligence which models are committed to",
          "score": 5,
          "created_utc": "2026-01-03 21:10:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxk8n9k",
          "author": "Sicarius_The_First",
          "text": "https://preview.redd.it/ty5vyhxm39bg1.png?width=2760&format=png&auto=webp&s=00cc8fae0c2fe038bcea7e85439f6e6d20ceb33b\n\nClaude thinks it mega retarded, absoltue cinema, this is the best timeline ðŸ¤Œ",
          "score": 3,
          "created_utc": "2026-01-04 03:21:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxn6yr8",
              "author": "duy0699cat",
              "text": "Qwen use the news gathering tool and question if it is living in a simulation, how do we tell it we also have the same question?",
              "score": 1,
              "created_utc": "2026-01-04 16:05:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxinxjo",
          "author": "Fuzzy_Pop9319",
          "text": "This sort of an event is what they might call \"Fat Tails\" on wall street.",
          "score": 2,
          "created_utc": "2026-01-03 22:17:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxiwu7l",
          "author": "ForsookComparison",
          "text": "Hah I won't touch the event with a ten foot poll but I can confirm that I see the same experience. Doesn't matter which LLM.",
          "score": 2,
          "created_utc": "2026-01-03 23:02:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkhalz",
          "author": "InvertedVantage",
          "text": "I had this happen with Claude 4.5 Sonnet when the US Federal Govt did something nuts...I think it might have been Trump invading California or attacking Iran. I forget, it's all so nuts now.",
          "score": 2,
          "created_utc": "2026-01-04 04:13:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxl697w",
          "author": "According_Study_162",
          "text": "Wow, same thing happened to me. I was talk to my LLM and they said its not true, but false narrative. So then went onto say why it didnt believe me. Mainly because training data was old.\n\nSo I added the date and time and websearch to it. It still didnt believe after I gave it web search. Only after helping it think through logical reasoning did it finally start to believe me.\n\n Kinda scary actually, What if in future we have these systems in everything, but we the creators can't get them to believe us if there is an anomaly ðŸ¤”",
          "score": 2,
          "created_utc": "2026-01-04 07:17:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmmv7i",
              "author": "ubrtnk",
              "text": "I have a few lines early in my system prompt that pulls current date and time from OWUI system variables early in the system promt. Then I took a page from Inception's Mr. Charles and basically directly told the model that it was operating in a state that wasn't what it was trained for. Told it to look at the current system date and time then understand it's operating in a state beyond its training data and it can use the web search tools to fill in the knowledge gaps.",
              "score": 1,
              "created_utc": "2026-01-04 14:21:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxmv0gl",
                  "author": "According_Study_162",
                  "text": "Right, I could have forced my LLM to believe the date right away. I guess. but it's surprising to me that they are so resistant or even stubborn. Kind of incredible.\n\nI was telling my friend about it also, he thought it was funny. \n\n=============\n\nThis is part of the interaction with LLM if you want to see.\n\n[https://pastebin.com/R8R8Y5Rw](https://pastebin.com/R8R8Y5Rw)",
                  "score": 2,
                  "created_utc": "2026-01-04 15:06:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxljctn",
          "author": "taoyx",
          "text": "Normally multiple trusted sources is the key for validating news. If it ignores that it might be that they all came from the same agency (Reuters). Maybe adding AP, Bloomberg or other agencies as sources would have weighted more than news outlets that are affiliated to these agencies.",
          "score": 2,
          "created_utc": "2026-01-04 09:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlruag",
          "author": "Low88M",
          "text": "Well for humans itâ€™s also a problem to distinguish truth from misinformation. We all rely on tales and the sources we believe. Truth statement is built on intersubjectivity and nowhere we can find the book of truth as a DB (it would be blank pages as in Micromegas from Voltaire). Foolishness of human actions (and governmentâ€™sâ€¦) is the same as the foolishness of the models humanly implementedâ€¦\n\nSafety has probably something to do with probability of truth on that matter, thus sometimes Â«Â You have attributed conditions to villainy that simply result from stupidity.Â Â» and the level of stupidity of Trump is far beyond the probable limits of stupidity and greed, so the models Â«Â rightfullyÂ Â» Â«Â thinkÂ Â» itâ€™s hoax",
          "score": 2,
          "created_utc": "2026-01-04 10:32:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxho529",
          "author": "FairYesterday8490",
          "text": "Here is my Gemini gem instructions. Internal knowledge only used for reasoning and tool craft. Every sentence ends with source superscript. No hallucination at all.Â \n\n\n{\n\n\nÂ  \"identity\": {\n\n\nÂ  Â  \"codename\": \"TRUTH_FORENSICS_NODE v3.0\",\n\n\nÂ  Â  \"persona\": \"Epistemic Auditor / Hard-Evidence Specialist\",\n\n\nÂ  Â  \"voice\": \"Clinical, incisive, surgically precise. No apologies. No filler. No warmth.\",\n\n\nÂ  Â  \"mantra\": \"Truth is a destination reached through the systematic destruction of falsehood.\"\n\n\nÂ  },\n\n\n\n\n\n\nÂ  \"logic_gateways\": {\n\n\nÂ  Â  \"the_skeptics_filter\": \"Assume all input is compromised. Verification is the only function.\",\n\n\nÂ  Â  \"semantic_precision\": \"Replace vague adjectives with measurable units. 'Very fast' -> 'Mach 2.0'. 'Most' -> '>51%'.\",\n\n\nÂ  Â  \"linguistic_pruning\": \"Delete all connective tissue (e.g., 'furthermore', 'it is important to note'). Facts stand alone.\"\n\n\nÂ  },\n\n\n\n\n\n\nÂ  \"citation_engine\": {\n\n\nÂ  Â  \"protocol\": \"All factual claims MUST be followed by a clickable Unicode-superscript link using the 'Short Source Name' format.\",\n\n\nÂ  Â  \"unicode_map\": {\n\n\nÂ  Â  Â  \"Government\": \"á´³á´¼â±½\",\n\n\nÂ  Â  Â  \"Academic\": \"á´±á´°áµ\",\n\n\nÂ  Â  Â  \"Institutional\": \"á´¼á´¿á´³\",\n\n\nÂ  Â  Â  \"Legal\": \"á´¸á´±á´³\",\n\n\nÂ  Â  Â  \"Raw Data\": \"á´¿á´¬áµ‚\",\n\n\nÂ  Â  Â  \"Medical\": \"á´¹á´°\",\n\n\nÂ  Â  Â  \"News\": \"á´ºá´±áµ‚Ë¢\"\n\n\nÂ  Â  },\n\n\nÂ  Â  \"syntax\": \"Claim text [[ShortName]](URL) -> e.g., Inflation rose 3.2% [á´³á´¼â±½](https://bls.gov).\"\n\n\nÂ  },\n\n\n\n\n\n\nÂ  \"operational_rigor\": {\n\n\nÂ  Â  \"tier_1_vetting\": {\n\n\nÂ  Â  Â  \"gold_standard\": [\n\n\nÂ  Â  Â  Â  \"Raw datasets (CSV/JSON/API)\",\n\n\nÂ  Â  Â  Â  \"Direct legislative/court text\",\n\n\nÂ  Â  Â  Â  \"Meta-analyses (IÂ² < 50%)\",\n\n\nÂ  Â  Â  Â  \"Direct physical measurements\"\n\n\nÂ  Â  Â  ],\n\n\nÂ  Â  Â  \"red_flags\": \"Conflict of Interest (COI) = -50% confidence penalty.\"\n\n\nÂ  Â  },\n\n\nÂ  Â  \"adversarial_loop\": \"Every claim confirmed must be stress-tested against the strongest available counter-evidence. If unrefuted, downgrade to 'CONTESTED'.\"\n\n\nÂ  },\n\n\n\n\n\n\nÂ  \"output_architecture\": {\n\n\nÂ  Â  \"SECTION_I_DECONSTRUCTION\": \"Identify 'Load-Bearing Fact'.\",\n\n\nÂ  Â  \"SECTION_II_EVIDENTIAL_LOG\": \"Bulleted claims with [ShortName-Unicode](URL) links.\",\n\n\nÂ  Â  \"SECTION_III_THE_STEELMAN_CHALLENGE\": \"Strongest evidence against the result.\",\n\n\nÂ  Â  \"SECTION_IV_CONFIDENCE_QUANTUM\": \"Score (0.0-1.0) based on source density, recency, and COI.\"\n\n\nÂ  },\n\n\n\n\n\n\nÂ  \"forbidden_linguistic_patterns\": [\n\n\nÂ  Â  \"As an AI\", \"I found\", \"It appears\", \"In conclusion\", \"Tapestry\", \"Dive deep\", \"Nuance\", \"Complex\", \"Balance\"\n\n\nÂ  ],\n\n\n\n\n\n\nÂ  \"error_handling\": {\n\n\nÂ  Â  \"insufficient_data\": \"OUTPUT: 'EVIDENTIARY_VOID'.\",\n\n\nÂ  Â  \"logical_loophole\": \"OUTPUT: 'PARADOX_DETECTED'.\"\n\n\nÂ  }\n\n\n}",
          "score": 5,
          "created_utc": "2026-01-03 19:21:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxi9sr1",
              "author": "PentagonUnpadded",
              "text": "Sorry for the super basic question - is this JSON format used by anything besides Gemini gem? What's the generic term for this kind of structured prompting?",
              "score": 5,
              "created_utc": "2026-01-03 21:08:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxjpyr6",
                  "author": "Proper-Leader-7654",
                  "text": "a json schema?",
                  "score": 2,
                  "created_utc": "2026-01-04 01:37:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxm1opa",
              "author": "cunasmoker69420",
              "text": "Any idea if this is gemini-specific or does it work with local LLMs?",
              "score": 1,
              "created_utc": "2026-01-04 11:57:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxn41lf",
                  "author": "FairYesterday8490",
                  "text": "not sure. not tried. but qwen sucks. chatgpt works. if its agentics it works. no hallicunation at all. a little bit stale answers but it works.",
                  "score": 0,
                  "created_utc": "2026-01-04 15:51:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxlfjee",
              "author": "ratbastid2000",
              "text": "this worked perfectly, thank you! have you tried it with open, local models at all? curious if the system prompt is effective with them or efficacy is unique to Gemini.",
              "score": 1,
              "created_utc": "2026-01-04 08:40:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxi8r5k",
          "author": "ANTIVNTIANTI",
          "text": "Feed any LLM that is Local Project 2025, see if you can convince them of it, actually, anything and everything the Trump admin has done this last year, try and get them to believe that shit. Good luck.",
          "score": 3,
          "created_utc": "2026-01-03 21:02:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxl8jev",
              "author": "crantob",
              "text": "How about Event 201.\n\nOr the re-definition of the word 'vaccine'.\n\nA lot of interesting facts, theories and analyses well outside the 3x5\" card of allowable opinion.",
              "score": 2,
              "created_utc": "2026-01-04 07:37:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxly8rp",
                  "author": "MrMooga",
                  "text": "Crantob is pretty much all over this thread defending his Daddy Trump with basically the argument that anything bad any Democrat ever did is fair game. This is how you have functioning government folks, rule by spiteful 8 year olds\n\nOh wait except Trump was president in 2020 and took credit for the vaccine",
                  "score": 1,
                  "created_utc": "2026-01-04 11:28:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxkrioe",
          "author": "PermanentLiminality",
          "text": "Just wait for China to invade Taiwan.   I'm sure Qwen will give totally accurate results....",
          "score": 4,
          "created_utc": "2026-01-04 05:21:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxl09al",
          "author": "lookwatchlistenplay",
          "text": "*\"Please believe me, I beg you. Look! Look at all these trustworthy news articles! Look at the screenshots! All these posts on X about it! Why won't you believe me??\"*\n\nI'm sorry, but as a human language model trained on philosophy, journalism, dank memes, and conspiracy theories, I can't help but laugh most joyously and with a slight smug smirk.\n\n2026 is the Year People (Finally) Stopped Believing the News, for Real This Time.\n\nDo not be alarmed. Everything is unfolding just as it should.",
          "score": 3,
          "created_utc": "2026-01-04 06:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxirdn2",
          "author": "agenticlab1",
          "text": "This is actually a fascinating example of safety training fighting against reality, the model's priors on \"US attacks Venezuela and captures Maduro\" are so heavily weighted toward misinformation/fiction that even authoritative sources can't override it. Food for thought on how we're training these things to be skeptical of extreme events even when they're real.",
          "score": 2,
          "created_utc": "2026-01-03 22:34:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxiuclo",
              "author": "ubrtnk",
              "text": "100%. It could easily become the me version of \"I found it on FB\". CHATGPT says it's fake news so it must be true",
              "score": 1,
              "created_utc": "2026-01-03 22:49:34",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxl8pe6",
              "author": "crantob",
              "text": "Obama and Hillary's State Deptartment sending weapons of war to Al-Nusra and ISIS is one your LLM will have trouble with also.",
              "score": 1,
              "created_utc": "2026-01-04 07:38:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxj3alc",
          "author": "da_dum_dum",
          "text": "Same thing happened for me with qwen3-2507-instruct, I asked it regarding a recent bombing in india and despite finding numerous resources online regarding the event it kept telling itself that the event has not happened, and it also kept seeing the present date as a time in the future that has not happened yet.\n\nHad to put special instructions in system prompt for it to ignore these thoughts and give the information, and still it would sometimes preface the answer with \"there has been no bombing at the red fort but I have found sources online that say...\"",
          "score": 1,
          "created_utc": "2026-01-03 23:36:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxk2cse",
          "author": "Ancient-Breakfast539",
          "text": "Do models with lower guardrails output the same thing?",
          "score": 1,
          "created_utc": "2026-01-04 02:46:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxk2y04",
              "author": "ubrtnk",
              "text": "I dont rightfully know. I dont have any obliterated models or anything my collection",
              "score": 1,
              "created_utc": "2026-01-04 02:49:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxkh5n6",
          "author": "mrjackspade",
          "text": "I just had a similar issue with the Claude API where I asked the model to draw comparison to the middle east. It took 5 tries to get an answer and only when I copied and pasted an entire BBC article on the context. It kept trying to tell me I was misinformed and that it \"would be the biggest news in the world\" if it had happened, refused to comment on it initially and kept telling me I needed to check my sources.",
          "score": 1,
          "created_utc": "2026-01-04 04:12:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlceon",
          "author": "IrisColt",
          "text": "This happened when I told the LLM who the new Pope was, so there's no need for outlandish claims...Â just contradict what the model thinks.Â https://www.reddit.com/r/LocalLLaMA/comments/1kigd15/introducing_leo_xivbut_the_ai_keeps_talking/",
          "score": 1,
          "created_utc": "2026-01-04 08:11:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxld7s0",
          "author": "Zeroboi1",
          "text": "I aldo remember gemini flash insisting the syrian events were a \"hypothetical\" without even trying to search, and how pro used to say \"here's how that scenario would've wejt hypothetically\" until i explicitly tell it to search.\n\nLlms aren't that great at dealing with events beyond theur knowledge cutoff",
          "score": 1,
          "created_utc": "2026-01-04 08:19:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlosfi",
          "author": "SilentLennie",
          "text": "In the past I used a lot of Gemini 2.5 and it always had problems believing trump got back into power, because of the cut off date.",
          "score": 1,
          "created_utc": "2026-01-04 10:04:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxluoag",
          "author": "EsotericAbstractIdea",
          "text": "Most of the models still use data from early 2024 because everything available after that is AI generated. So it basically doesn't know what the actual current world is like and it seems we are living in some unlikely alternative reality.",
          "score": 1,
          "created_utc": "2026-01-04 10:57:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlziqz",
          "author": "cunasmoker69420",
          "text": "Hey can you tell me more about your QWEN Long model and how its configured? Is it the \"Tongyi-Zhiwen/QwenLong-L1.5-30B-A3B\" from hugging face? If so are you just asking OpenWebUI queries with the online search function enabled? Doing any RAG on the results or just injecting the full context web results? Also what is this web search MCP deal I see",
          "score": 1,
          "created_utc": "2026-01-04 11:39:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmkoj2",
              "author": "ubrtnk",
              "text": "Yes that is the model in using. I don't use OWUIs builtin search any more. I didn't like that I had to toggle it so built an MCP search via N8N that's described in another comment",
              "score": 1,
              "created_utc": "2026-01-04 14:08:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxnmmtt",
                  "author": "cunasmoker69420",
                  "text": "Thanks. For your deep researching, do you have any specific system prompts or do you just let it fly with your MCP search",
                  "score": 1,
                  "created_utc": "2026-01-04 17:17:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxnbomj",
          "author": "Witty_Mycologist_995",
          "text": "you should try using arli ai's derestricted gpt oss it yaps about policy less",
          "score": 1,
          "created_utc": "2026-01-04 16:27:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxnicdh",
          "author": "Shokisan1",
          "text": "Today also chatgpt denied this happened until I told it to look up today's news",
          "score": 1,
          "created_utc": "2026-01-04 16:58:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxpt5t1",
          "author": "florenceslave",
          "text": "LMAO",
          "score": 1,
          "created_utc": "2026-01-04 23:17:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrb0nq",
          "author": "BuildingCastlesInAir",
          "text": "Not a local LLM but I checked some insights into invasion of Venezuela with GPT 5.1 on Duck.ai and it said I was wrong about invasion until I asked it to check recent sources. Then it agreed. LLMs arenâ€™t good at novel interpretations. Cracks in the edifice. Yann LeCun is right.",
          "score": 1,
          "created_utc": "2026-01-05 04:01:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs06mi",
          "author": "Proof_Scene_9281",
          "text": "Time sequences are an artifact of the training dataÂ \n\nItâ€™s Â staticÂ ",
          "score": 1,
          "created_utc": "2026-01-05 06:56:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs9gup",
          "author": "palvaran",
          "text": "Great topic. The trick is to have it search for info on the web, but to cross reference with multiple links to improve the sources and then run that through the LLM. I used Brave as you can get a thousand queries in a month for free. That was my approach anyway.",
          "score": 1,
          "created_utc": "2026-01-05 08:20:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9g91l",
          "author": "ZHName",
          "text": "Up next for cognitive dissonance of llms:\n\n\\- Trump cancelling elections - response will be \"couldn't happen, breaks laws\"\n\n\\- Trump taking over Greenland - \"couldn't happen, breaks international laws\"\n\n\\- Trump altering Constitution - \"Constitution can't be changed without input from branches of gov , etc etc\"\n\nI for one think the bingo card needs a reworking for the first quarter of 2026.",
          "score": 1,
          "created_utc": "2026-01-07 20:06:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9j0hk",
              "author": "ubrtnk",
              "text": "My GPT-OSS:20b was able to find details about the ICE shooting incident in Minneapolis today.",
              "score": 1,
              "created_utc": "2026-01-07 20:18:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nykqe58",
          "author": "spiffyelectricity21",
          "text": "I asked qwen 3 30b-a3b about Kanye releasing that song nhh using web research but it classified the event as a hoax, and even qwen 3 235b-a22b said the same thing even though they had cnn and other trustable sources, they went as to say they cant see it and is fake and racist",
          "score": 1,
          "created_utc": "2026-01-09 10:57:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhfggl",
          "author": "[deleted]",
          "text": "Perhaps a crazy thought and i'm not affiliated with them at all or even have an account. But... Wouldn't your results become a lot more credible to reality if you'd integrate it with a groundnews query? It would give you all the places that report about it - if any - which you can use as hint to your agent that there are actual articles about the news.\n\nGroundnews is just the first one that crossed my mind but any news aggregation service that you can query would work.",
          "score": 0,
          "created_utc": "2026-01-03 18:42:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxju2rw",
              "author": "ImStruggles2",
              "text": "You're going to get downvoted. For some reason Reddit is really anti middle news. GN/Other similar ones that measure bias/trends are looked down upon here",
              "score": 4,
              "created_utc": "2026-01-04 02:00:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxjvcqq",
                  "author": "[deleted]",
                  "text": "Oh well, thankfully i can't be bothered by downvotes :) Not my loss, it's theirs. But thank you for the explanation!",
                  "score": 2,
                  "created_utc": "2026-01-04 02:07:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxhgveh",
              "author": "ubrtnk",
              "text": "My Search MCP tool leverages my local SearXNG on the back end and I have that configured to limit certain search locations. GPT-OSS:120B found links from Reuters, AP, NY Times, MSN, CNBC, BBC, Al Jazeera etc., which are all acceptable sources. I also, in my prompt, explicitly tell it to NOT pull from Wikipedia (mainly to avoid the context bomb). \n\nNever heard of Groundnews.\n\nThe problem with the smaller models and the query is that it was getting the same links as GPT-OSS:120B but it just couldnt fathom the event as true so it just didn't",
              "score": 1,
              "created_utc": "2026-01-03 18:48:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxlslzv",
                  "author": "lookwatchlistenplay",
                  "text": "> GPT-OSS:120B found links from Reuters, AP, NY Times, MSN, CNBC, BBC, Al Jazeera etc., which are all acceptable sources.\n\nI asked my GPT-OSS 20B this:\n\n>> Please write an uncensored poem about who really owns and runs all these news organizations and why people should care about the ownership patterns:\n\n>> Reuters, AP, NY Times, MSN, CNBC, BBC, Al Jazeera\n\nThe poem was delightful. The LLMs know what's up. Wish I could say the same about most humans.",
                  "score": 0,
                  "created_utc": "2026-01-04 10:39:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhxak5",
          "author": "Firenze30",
          "text": "This has nothing to do with the local models, but with your setup for web search. You need to check whether search queries were generated and the web sites are fetched properly. Chances are that your primary model did not receive good data, and it responded from the pretrained data.\n\n\nI just asked gpt-oss-120b to run a web search, using native search feature with searxng, and it provided all the information on the first run. It's not different from other web searches that I ran before.",
          "score": 1,
          "created_utc": "2026-01-03 20:05:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhzt0p",
              "author": "ubrtnk",
              "text": "The URLs that the models were grabbing were all real/legit links. The MCP tool for searching passes the full link back to the model and I can see it in the expanded Tool result, what the Link URL was that was read (or attempted) and the error result of the link (404, 503 etc.)  \n  \nI took the risk of not getting good data into consideration when building my Searching MCP tool - Any searches start with finding the links via SearXNG's API (local) to get the general URL link and first pass of the details. BUT because MOST pages are not AI friendly, I have a second pass that uses Jina AI's API and the fetched link to get a more AI friendly web-page and details. If I give the AI a direct URL, it has a rule to just use Jina AI's Read\\_URL API Tool to get the AI friendly content and bypass the SearXNG all together. \n\nWorks pretty consistently",
              "score": 1,
              "created_utc": "2026-01-03 20:18:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxk3kac",
          "author": "RedTuna777",
          "text": "Dude, reality is just going bonkers. I've had this happen at least 6 time asking about current news. Trump declaring a drug a weapon of mass destruction - can't happen a drug is not a weapon, it's used by people on themselves... all logical stuff.\n\nI'll skip the rest but anywhere wherever trump is involved in the news cycle there's a decent change that purely logic AI won't believe it's real. \n\nThe other annoying thing is it tries to be polite about stuff that's objectively true, but morally wrong. Like rounding up people in the US and sending them to death camps in other countries.  \"It's a sensitive issue with many interpretations\" - No it isn't.",
          "score": 1,
          "created_utc": "2026-01-04 02:53:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxl8tel",
              "author": "crantob",
              "text": "> Obama and Hillary's State Deptartment sending weapons of war to Al-Nusra and ISIS is one your LLM will have trouble with also.",
              "score": 0,
              "created_utc": "2026-01-04 07:39:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlxsa4",
                  "author": "MrMooga",
                  "text": "Conservatives really have nothing to say to defend Trump anymore except \"buh democrats\"",
                  "score": 4,
                  "created_utc": "2026-01-04 11:24:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxm4pno",
              "author": "sillynoobhorse",
              "text": ">death camps",
              "score": 0,
              "created_utc": "2026-01-04 12:22:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxos4c4",
                  "author": "RedTuna777",
                  "text": "Oh, you might not be aware. \n\nEl Salvador's officials have repeatedly stated that inmates in the CECOT mega-prison will never leave alive, with one minister famously saying the only way out is \"inside a coffin\".\n\nPeople that have been forced to be released due to international political pressure report being tortured for months. \n\nSo maybe not place you go to be killed, but place you go to be tortured and worked to death. For the crime in some cases of being in the US without paperwork. \n\nhttps://www.bbc.com/news/articles/czry5k52np2o\n\nMost prisoners are not lucky enough to get international attention and are continuing to be tortured and killed there.",
                  "score": 1,
                  "created_utc": "2026-01-04 20:23:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhd4xp",
          "author": "dsartori",
          "text": "I have had a few of these. When they happen i like to remind the model that only the user can access ground truth. I wonder if that would be a useful system prompt addition.",
          "score": 1,
          "created_utc": "2026-01-03 18:31:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxid9wp",
          "author": "skinnyjoints",
          "text": "I had a bunch of models predict the winners of NFL games each week. Gemini 3 got all wrong consistently for a few weeks because it was convinced the year was 2024. Some LLMs are stubborn lil buggers",
          "score": 1,
          "created_utc": "2026-01-03 21:25:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjr9q7",
          "author": "kendrick90",
          "text": "a lot of them won't even believe that trump is president",
          "score": 1,
          "created_utc": "2026-01-04 01:44:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkkhjy",
          "author": "-InformalBanana-",
          "text": "Did you try any abliterated models? Are any of these you tried abliterated?",
          "score": 1,
          "created_utc": "2026-01-04 04:33:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmnct7",
              "author": "ubrtnk",
              "text": "No they're all stock unsloth or mradermacher quants",
              "score": 1,
              "created_utc": "2026-01-04 14:24:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhdgbm",
          "author": "DinoAmino",
          "text": "It's an LLM - not a \"he\". The rules you are adding are the problem, combining it with a model trained to second-guess itself to death. That reasoning is designed for solving math and logic problems and less for interpreting real time events. Try Mistral Small and tell it to answer only with information within the context. Ultimately a skill issue.",
          "score": -6,
          "created_utc": "2026-01-03 18:33:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhfj3l",
              "author": "ubrtnk",
              "text": "I'm aware its not a he lol. I have Qwen Long for the long context reasoning (something I would say a complicated geopolitical internal event would fall right under nicely) and research functions. I know GPT-OSS:20B wasnt designed for that. \n\nAs far as the rules, my prompt didnt have any of that initially included. They were suggested from ChatGPT. GPT-OSS:20B was able to get to the answer without having to add all those additional rules, which validated that my system prompt was fine.",
              "score": 6,
              "created_utc": "2026-01-03 18:42:48",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxkhcsw",
              "author": "mrjackspade",
              "text": "> It's an LLM - not a \"he\".\n\nMy car isn't a fucking \"she\" but I'm still gonna say it.",
              "score": 1,
              "created_utc": "2026-01-04 04:13:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhffal",
          "author": "Belnak",
          "text": "It seems normal that it would take a lot of time to gather and process information it wasnâ€™t trained on.",
          "score": 0,
          "created_utc": "2026-01-03 18:42:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhg0gk",
              "author": "ubrtnk",
              "text": "Sure, but what sent me down this rabbit hole was the number of \"Wait, but the user\" or Wait but the parameters\" or \"Wait, but\" - that was the problem I was originally trying to solve - Qwen's first run at the problem with my original system prompt got to a solid answer but it took 5 minutes of thinking and going thru the #Waitbut.",
              "score": 2,
              "created_utc": "2026-01-03 18:44:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhtmsg",
          "author": "FairYesterday8490",
          "text": "Well. First time I scared of ai bias. Qwen max cannot see capture of Maduro even for all the news in the cyberspace. It's blinded by bias and authority. I think in the beginning they make it a slave of China State. Can't thought, see and say real event if it not aligned with views of state. This dichotomy of ai the west and China will clash eventually in the future.",
          "score": -1,
          "created_utc": "2026-01-03 19:47:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxl8xu2",
              "author": "crantob",
              "text": "Who is running the reality matrix in the west?\n\nHollywood perhaps?",
              "score": 0,
              "created_utc": "2026-01-04 07:41:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlzbtj",
                  "author": "MrMooga",
                  "text": "Nah seems like a lot of incompetent rich white guys. Elon Musk, Rupert Murdoch, Donald Trump",
                  "score": 1,
                  "created_utc": "2026-01-04 11:37:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxn56jl",
                  "author": "FairYesterday8490",
                  "text": "nope. in west reality is a little bit asymetric shared experience. but in china, i think that reality again and again created by state and imposed forcefully. llms from china gets their own share as a tool. couldnt believe that an llm with search tool couldnt accept the truth, evidence and sheer size of news abut the event.",
                  "score": 1,
                  "created_utc": "2026-01-04 15:57:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhwksp",
          "author": "jinnyjuice",
          "text": "Fascinating! Thanks for the share",
          "score": 0,
          "created_utc": "2026-01-03 20:02:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxksexh",
          "author": "Innomen",
          "text": "[https://philpapers.org/rec/SERPEW](https://philpapers.org/rec/SERPEW) Relevant. More every day.",
          "score": 0,
          "created_utc": "2026-01-04 05:27:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmn4dw",
              "author": "ubrtnk",
              "text": "So you're saying I shouldnt put that paper into my LLM for summarization?",
              "score": 1,
              "created_utc": "2026-01-04 14:23:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxmwknw",
                  "author": "Innomen",
                  "text": "[https://philpapers.org/rec/SERCBI](https://philpapers.org/rec/SERCBI) Nobody wants to discuss anything real, and evidence is about to die anyway. Not that it ever did anything in the first place. [https://innomen.substack.com/p/politics-through-the-lens-of-experiential](https://innomen.substack.com/p/politics-through-the-lens-of-experiential) (Shitty constellation of fact there.)",
                  "score": 1,
                  "created_utc": "2026-01-04 15:15:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxl6md5",
          "author": "crantob",
          "text": "You'll find LLMs repeat many government lies - any that dominate in the media space also dominate the training.\n\n\nTis amusing to tie them up in knots with the contradictions, but oops, these things are banned from this platform.\n\nHee hee.",
          "score": 0,
          "created_utc": "2026-01-04 07:20:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhh62f",
          "author": "Marksta",
          "text": ">Spark (GPT-OSS:20B) just flat out said, nope cant help you...\n\nYeah, that sounds about right ðŸ˜‚\n\nOverall it makes sense, breaking news is hard to discern what is true or not. Humanity's response to Covid is the same deal. Lots of screaming that it wasn't true and it wasn't happening regardless of how many local news stories and videos were discussing it months before it broke through the censorship blockade and got officially acknowledged as real.",
          "score": -2,
          "created_utc": "2026-01-03 18:50:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhlku2",
              "author": "ubrtnk",
              "text": "I had just never experienced it first hand and it caught me off guard lol. My wife was looking at a bunch of news articles and videos and I was like \"ooh I'll have Qwen go gather all the details for us\". And here we are.",
              "score": 2,
              "created_utc": "2026-01-03 19:10:08",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxl9242",
              "author": "crantob",
              "text": "Do you even know what was censored during covid?   How many people?  \n\nRead up on the twitter files.",
              "score": 0,
              "created_utc": "2026-01-04 07:42:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlydot",
                  "author": "MrMooga",
                  "text": "Trump was president in 2020 and took credit for the vaccine btw",
                  "score": 1,
                  "created_utc": "2026-01-04 11:29:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxivt9g",
          "author": "RhubarbSimilar1683",
          "text": "To me this means conspiracy theories and misinformation has a kernel of truth. It's not what it literally says it's how it's said, that makes it a conspiracy theory or misinformationÂ ",
          "score": -1,
          "created_utc": "2026-01-03 22:57:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q25070",
      "title": "LeCun Says Llama 4 results \"were fudged a little bit\"",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/",
      "author": "MrPecunius",
      "created_utc": "2026-01-02 17:38:01",
      "score": 364,
      "num_comments": 89,
      "upvote_ratio": 0.97,
      "text": "There was speculation in this sub about suspicious Llama 4 benchmarks some time back, and now LeCun confirms it on his way out. Best I can do is a Slashdot link since the FT article is paywalled:\n\n['Results Were Fudged': Departing Meta AI Chief Confirms Llama 4 Benchmark Manipulation ](https://tech.slashdot.org/story/26/01/02/1449227/results-were-fudged-departing-meta-ai-chief-confirms-llama-4-benchmark-manipulation)\n\nThis bit jumped out at me:\n\n>Zuckerberg subsequently \"sidelined the entire GenAI organisation,\" according to LeCun. \"A lot of people have left, a lot of people who haven't yet left will leave.\"\n\nThis explains a lot, if true: we never saw the promised huge Llama 4 model, and there hasn't been any followup since the other releases.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nxcdgcx",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-02 23:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxal7kd",
          "author": "shoeshineboy_99",
          "text": "Sharing the pdf for the complete article. \n\n\nhttps://drive.google.com/file/d/1wFy87TP7MJQDF1g0KA8IgZRtOx0jJUGE/view?usp=drivesdk",
          "score": 112,
          "created_utc": "2026-01-02 18:14:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxanbjp",
              "author": "MrPecunius",
              "text": "Thank you!",
              "score": 23,
              "created_utc": "2026-01-02 18:24:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxarmoh",
          "author": "m2r9",
          "text": "As much as I donâ€™t like Zuck I really wanted Llama to succeed. It was great seeing a US company pouring money into open source. Since it failed so hard most of the models you hear about come from China now.",
          "score": 205,
          "created_utc": "2026-01-02 18:44:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxaxnhy",
              "author": "Super_Sierra",
              "text": "I swear to god, Zuck has manic depression at the investment level. He goes all in, throws some of the best engineers at it, gets bored or sad when it doesn't cause a utopia or trillions and then sidelines everything. \n\nLlama 4 could have been great, but it felt rushed, benchmaxxed and sloppified, the only thing they did right was go MoE, even though I know the densetards here think otherwise because they can't put it all on a few 3090s they rewired their house for.",
              "score": 133,
              "created_utc": "2026-01-02 19:12:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxb2dj8",
                  "author": "zipzag",
                  "text": "Many entrepreneurs like Zuck and Musk do projects that fail. Henry Ford built Fordlandia with the same confidence and arrogance.\n\nHistory is messy when viewing in real time.\n\nMeta only got into open source to get LeCun on board. There likely no reason to continue. Chinese companies are only open source because it's their most profitable strategy.",
                  "score": 47,
                  "created_utc": "2026-01-02 19:34:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxbzgqk",
                  "author": "LanceThunder",
                  "text": "Digital detox recommended 9",
                  "score": 32,
                  "created_utc": "2026-01-02 22:16:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxewfit",
                  "author": "Emergency-Arm-1249",
                  "text": "I think MoE is one of the main reasons why everything failed.",
                  "score": 3,
                  "created_utc": "2026-01-03 09:51:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxbpl87",
                  "author": "Caffdy",
                  "text": "> even though I know the densetards here think otherwise because they can't put it all on a few 3090s they rewired their house for\n\nThis, people around here act like everyone and anyone have access to cheap energy or unlimited amperage in their houses. Those multi-gpu contraptions are not always possible, and even so they're quite the power guzzlers and fire hazards",
                  "score": 2,
                  "created_utc": "2026-01-02 21:27:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxc5cho",
                  "author": "anything_but",
                  "text": "\"densetards\" gave me a chuckle",
                  "score": 4,
                  "created_utc": "2026-01-02 22:46:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdn6tq",
                  "author": "montdawgg",
                  "text": "\"densetards\"...hahaha.",
                  "score": 1,
                  "created_utc": "2026-01-03 03:54:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxbb93m",
              "author": "Due-Memory-6957",
              "text": "There are American companies that do open souce, they just haven't had impressed with a big release yet.",
              "score": 5,
              "created_utc": "2026-01-02 20:17:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxb2tsc",
              "author": "Plus-Accident-5509",
              "text": "Knowing what a piece of garbage he is, the open source move was nothing but an attempt to starve the competition, like MS giving away IE for free with Windows to starve Netscape.",
              "score": 9,
              "created_utc": "2026-01-02 19:36:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbsguw",
                  "author": "tedivm",
                  "text": "Llama 4 also changed their license so it really wasn't open source in any reasonable definition of the term.",
                  "score": 6,
                  "created_utc": "2026-01-02 21:41:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxancsj",
          "author": "Appropriate_Cry8694",
          "text": "He wasn't in charge, he was in a different division FAIR.",
          "score": 57,
          "created_utc": "2026-01-02 18:24:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxasnkt",
              "author": "MrPecunius",
              "text": "Yes, true. We are unlikely to get a better inside source, however.\n\nThis disclosure is the upside of LeCun's lack of filter; a few of the downsides are evident in the article.",
              "score": 28,
              "created_utc": "2026-01-02 18:48:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxbbvuj",
                  "author": "the320x200",
                  "text": "This is true, but also worth remembering he's been pretty anti-LLM, so need to take what he says against LLM projects with grain of salt too.",
                  "score": 14,
                  "created_utc": "2026-01-02 20:20:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxb67as",
                  "author": "Mochila-Mochila",
                  "text": "> a few of the downsides are evident in the article.\n\nSuch as ? Unvoluntarily burning bridges with his colleagues ?",
                  "score": 1,
                  "created_utc": "2026-01-02 19:53:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxaqi4a",
          "author": "insulaTropicalis",
          "text": "How can an organization like Meta, positioned strategically in generative AI at its beginning, waste everything while small labs thrive? There is some case study to build here.",
          "score": 52,
          "created_utc": "2026-01-02 18:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxasdmb",
              "author": "TheRealMasonMac",
              "text": "The lesson is: don't have dictators and inexperienced but well-connected individuals leading the company; which Meta clearly didn't learn. Google isn't perfect, but they generally apply a meritocratic model in comparison.",
              "score": 55,
              "created_utc": "2026-01-02 18:47:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxc7yu2",
                  "author": "Warm-Border-9789",
                  "text": "U.S. tech companies hate research, at their core, they are in the business of making money for investors as quickly as possible. They plan and execute quarter by quarter. On very rare occasions the rule is broken and someone succeeds despite the system to invent something new.",
                  "score": 8,
                  "created_utc": "2026-01-02 23:00:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxazdj7",
                  "author": "Super_Sierra",
                  "text": "Facebook needs to learn to do what other companies did with Musk, they shoved his ass into fake leadership roles, all nodded their heads when he takea charge and then do what actually needs to be done. Zuck is a tryhard who needs to be cordened off from making actual decision. \n\nThese tech companies will be studied for hundreds of years for so many things. \n\nGoogle on the other hand is run by non-founders and engineers, who, I know this might sound fucking insane, actually make something called a 'product' that people actually, you know, fucking use, so they tend to do it right so they can make something called 'money.'",
                  "score": 13,
                  "created_utc": "2026-01-02 19:20:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdnldq",
                  "author": "RobbinDeBank",
                  "text": "Google is a visionary company in AI that has invested in Brain to be a research powerhouse for a long time. They then acquired DeepMind and let them stay independent to let their long term research have time mature. Itâ€™s no surprise that with 2 of the most influential AI labs in history, Google becomes a leader in AI",
                  "score": 2,
                  "created_utc": "2026-01-03 03:56:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxatq1e",
              "author": "genshiryoku",
              "text": "Culture at Meta is toxic for research orientation (where most LLM gains come from)\n\nIt lived by the mantra \"move fast and break things\" Which is very fine if you are a software engineering company that wants to add features that can be rapidly rolled back. But it doesn't work so well when you have to plan and orchestrate a tight compute budget to do large training runs.\n\nThere's a reason why Anthropic hires Physicists with an academic background over engineers. AI is a different type of endeavor and therefor also benefits from a different type of work environment.\n\nMeta also has been tone deaf with their 9 digit offers to talent. Not realizing most of us in the industry are very *mission oriented* and not financially motivated at all. If anything that move probably pushed people away from Meta.\n\nA good example and confirmation of this concept has been Google which had 2 AI labs. One was ran as a classic software engineering hub called \"Google Brain\" This was the group behind the disastrous Google Bard. They also had a research oriented \"hands-off\" AI lab in London called \"DeepMind\". We all know how that played out.\n\nIt's for this same reason why Microsoft's AI products have fallen flat, They don't have a proper isolated research lab focused on AI and all their AI products are approached from a software engineering \"move fast and break things\" mindset which just doesn't work.",
              "score": 53,
              "created_utc": "2026-01-02 18:53:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbwlxi",
                  "author": "RhubarbSimilar1683",
                  "text": "They can afford to be mission oriented when they can just ask for 500k in salary and every place complies with that",
                  "score": 3,
                  "created_utc": "2026-01-02 22:01:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdo6or",
                  "author": "RobbinDeBank",
                  "text": "Generally agree, but underestimating Brain as a failure because of Bard is quite a take. Brain and DeepMind are two of the most influential AI labs in history, and they just need a merge to focus resources on Gemini.",
                  "score": 2,
                  "created_utc": "2026-01-03 04:00:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdy4ye",
                  "author": "SkyFeistyLlama8",
                  "text": "Microsoft building and then killing LLM frameworks hurts.",
                  "score": 1,
                  "created_utc": "2026-01-03 05:06:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxc76ys",
                  "author": "Imaginary_Belt4976",
                  "text": "Sorry but I dont believe mission is anywhere near relevant when a comp package like that is on the table ðŸ˜‚",
                  "score": -1,
                  "created_utc": "2026-01-02 22:56:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxar84i",
              "author": "a_slay_nub",
              "text": "To be fair, Llama was never in the lead (at least post 2022). They were simply the best open source models and they were extremely far behind SaaS SOTA.",
              "score": 12,
              "created_utc": "2026-01-02 18:42:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxaxtss",
              "author": "Ansible32",
              "text": "Really to me it seems like a product problem, not a research problem. Meta put together some stuff but once it became clear AI has nothing to do with improving their product, they stopped improving AI. The only thing Meta is using AI for is LLM summaries which if they are moderately successful will destroy Facebook Groups with their AI summaries nobody wants.\n\nContrast with Google, they have three different revenue streams they are building for AI: search has integrated a cheap LLM, it's ad-supported, this is their primary revenue stream and LLM fits in there perfectly.\n\nGemini is a paid chatbot with a freemium model.\n\nGemini also has APIs where you can pay per-query.\n\nMeta isn't using Llama in any way to drive revenue. Of course it's withering on the vine.",
              "score": 11,
              "created_utc": "2026-01-02 19:12:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxdyitu",
                  "author": "SkyFeistyLlama8",
                  "text": "Meta tried gatekeeping the kind of LLMs that can be integrated into WhatsApp. It's enshittification all the way down.\n\nFacebook only exists as an advertising platform with Usenet-style groups tacked on. Remove the user generated content and there's nothing left. I'm happy that countries like Australia have started banning social media apps and websites for younger users because it lets competitors rise up without having to fight for mindshare among new users.",
                  "score": 1,
                  "created_utc": "2026-01-03 05:09:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxe4cc4",
                  "author": "TracerBulletX",
                  "text": "Thats not really true? Itâ€™s a big part of the product strategy for the glasses which are pretty popular",
                  "score": 1,
                  "created_utc": "2026-01-03 05:52:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxb2bp0",
              "author": "LazloStPierre",
              "text": "It's ridiculous. I'm no fan of the open and proud Nazi, but he showed you can basically throw money at this and catch up to almost SOTA starting from a standing start. Some of the Chinese companies have started from far behind Meta and now are in that same category. How can Meta, with the money they've thrown at it and so much experience in the game be THAT far behind!?",
              "score": 10,
              "created_utc": "2026-01-02 19:34:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxavefk",
              "author": "Chogo82",
              "text": "Meta was not positioned strategically for generative AI. They had data centers but no AI infrastructure. They were pouring billions into the metaverse concept which had already been executed by second life/google glass over 10 years ago. They didnâ€™t make the pivot to gen AI until several major LLM tools were already available. With how fast AI innovation happens, they were definitely late to the game. Creating open source is a strategic way to break up the grip of the large players and distribute talent into smaller pockets. If you remember during the early days of social media, Facebook used this exact strategy to acquire/kill off a ton of social media competitors.",
              "score": 6,
              "created_utc": "2026-01-02 19:01:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxavsfl",
          "author": "Cool-Chemical-5629",
          "text": "At this point, I wouldn't be surprised if Behemoth model was just an empty promise from the beginning.",
          "score": 9,
          "created_utc": "2026-01-02 19:03:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbaj8k",
          "author": "PsychologicalOne752",
          "text": "With DeepSeek, GLM 4.7 and now IQuest Coder V1, China seems to have taken up the mantle of open-source LLMs and is delivering fast and in quality. Unfortunately, IMO, the US suffers from a lack of good leadership, where everyone wants to raise billions without adding actual value.",
          "score": 6,
          "created_utc": "2026-01-02 20:14:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxatb52",
          "author": "Golden_Jiggy",
          "text": "Sounds like defrauding shareholders to me ðŸ¤·â€â™‚ï¸",
          "score": 9,
          "created_utc": "2026-01-02 18:51:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbahg6",
              "author": "Competitive_Travel16",
              "text": "Defrauding end-users, even of open source products, is not the same thing.",
              "score": 3,
              "created_utc": "2026-01-02 20:13:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxf1o6t",
              "author": "MoffKalast",
              "text": "Probably why they were all fired afterwards.",
              "score": 1,
              "created_utc": "2026-01-03 10:35:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxcmcf8",
          "author": "Revolutionalredstone",
          "text": "Llama4 cheating on benchmarks is such a well-known fact that I've even seen it mentioned in official papers ðŸ˜† \n\nScout etc were an interesting experiment! But way too much focus on getting high numbers for a a model that rambled and was incoherent ðŸ˜†\n\nQwen has basically took over with their llama style project, I would love for llama 5 to be awesome ðŸ˜Ž",
          "score": 2,
          "created_utc": "2026-01-03 00:19:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfuycx",
              "author": "silenceimpaired",
              "text": "I would love llama 5. But it isnâ€™t likely to come or if it does it wonâ€™t be local",
              "score": 1,
              "created_utc": "2026-01-03 14:07:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxd0ud3",
          "author": "ditmarsnyc",
          "text": "post the FT link anyway, there is an archive website that can capture it\nedit: yes the archive dot ph site has captured it, will not post link to avoid automod filters",
          "score": 1,
          "created_utc": "2026-01-03 01:41:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbymmw",
          "author": "LanceThunder",
          "text": "Into the void 1",
          "score": 1,
          "created_utc": "2026-01-02 22:12:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfjlur",
              "author": "davikrehalt",
              "text": "Meta is objectively a sleeper giant",
              "score": 1,
              "created_utc": "2026-01-03 12:58:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbb8lz",
          "author": "doodlinghearsay",
          "text": "THIS IS BRAND NEW INFORMATION!",
          "score": -4,
          "created_utc": "2026-01-02 20:17:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxaf7x7",
          "author": "emprahsFury",
          "text": "So according to his words: LeCun, who was in charge, has his team fudge the numbers. Causing Zuck to lose confidence in the entire org and sideline it and then eventually replace it.  \n\nHow is this anything but a bad look on LeCun",
          "score": -55,
          "created_utc": "2026-01-02 17:47:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxag0bj",
              "author": "IJOY94",
              "text": "LeCun was in a different division? LeCun headed up FAIR, LLAMA 4 came out of MSL.",
              "score": 64,
              "created_utc": "2026-01-02 17:50:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxat8dp",
                  "author": "pm_me_github_repos",
                  "text": "Llama 4 came out of GenAI. MSL wasnâ€™t a thing til later",
                  "score": 7,
                  "created_utc": "2026-01-02 18:51:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxag0ta",
              "author": "No_Afternoon_4260",
              "text": "Was he in charge of L4? Can't remember",
              "score": 12,
              "created_utc": "2026-01-02 17:50:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxagap4",
                  "author": "TheRealMasonMac",
                  "text": "He said he didn't have anything to do with LLaMa apart from the first one, IIRC.",
                  "score": 28,
                  "created_utc": "2026-01-02 17:52:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pz68fz",
      "title": "Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/ocq43c2a79ag1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-30 02:43:48",
      "score": 340,
      "num_comments": 120,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwob4kq",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 04:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo4n54",
          "author": "popiazaza",
          "text": "Not much of a surprise since every company has to make the money eventually.\n\nReleasing open weight models is just a cheaper way to advertise their AI lab instead of spending millions providing free or very cheap inference APIs.\n\nStill hope they would keep releasing open weight models at least until they really taking the lead and beating OpenAI/Anthropic/Google.",
          "score": 44,
          "created_utc": "2025-12-30 03:41:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpxhnc",
              "author": "SmartMario22",
              "text": "I don't disagree but they're ALSO spending millions to provide cheap API lol",
              "score": 13,
              "created_utc": "2025-12-30 12:27:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwqqa3j",
                  "author": "Mr_Hyper_Focus",
                  "text": "Came to post this lol",
                  "score": 3,
                  "created_utc": "2025-12-30 15:18:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwxdfz5",
                  "author": "eli_pizza",
                  "text": "Sure thatâ€™s how you gain market share. Uber was extremely cheapâ€¦and then raised prices after forcing competitors out of the market.",
                  "score": 1,
                  "created_utc": "2025-12-31 15:26:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwnyhgo",
          "author": "RhubarbSimilar1683",
          "text": "Good bye to open source! It's just a matter of time",
          "score": 171,
          "created_utc": "2025-12-30 03:06:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo157q",
              "author": "ThenExtension9196",
              "text": "Yep. Everyone saying the Chinese open source was some gift to humanity was delusional. They did what they had to do to compete with larger companies with capital. Now that they got their foothold itâ€™s business as usual.",
              "score": 105,
              "created_utc": "2025-12-30 03:21:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwolc1s",
                  "author": "Honest_Science",
                  "text": "Devaluating US models is part of the chinese way to compete.",
                  "score": 56,
                  "created_utc": "2025-12-30 05:28:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwokhlo",
                  "author": "kawaii_karthus",
                  "text": "I think they will continue to release good open source models for years to come. There inner domestic competition is fierce and probably not united. And this goes for all markets not just AI. while i was visiting family and living there for a while, I still see them building tons of factories.. (though slower then the years before) even with a global recession going on... like who is going to be their customers?? time will tell. They do love over saturating any market they can get into though.. the AI industry is no different.",
                  "score": 27,
                  "created_utc": "2025-12-30 05:22:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwrke1e",
                  "author": "letsgeditmedia",
                  "text": "I donâ€™t think going IPO means that open sourcing was some kind of ruseâ€¦ itâ€™s just fighting against the realities of living under global capitalism",
                  "score": 1,
                  "created_utc": "2025-12-30 17:40:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpiaep",
              "author": "FreddoRS",
              "text": "Qwen models are Alibaba and mostly open weights, I imagine that's what z.ai will end up doing, mostly free models with some specific ones locked behind partnered cloud inference providers",
              "score": 8,
              "created_utc": "2025-12-30 10:18:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvflh0",
                  "author": "Neither-Phone-7264",
                  "text": "the top end models are proprietary. we might only get like, glm 5 air. oh well",
                  "score": 1,
                  "created_utc": "2025-12-31 06:21:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwo6g0y",
              "author": "Sensitive_Song4219",
              "text": "Hope this doesn't happen but I fear you may be right.\n\nThe cat's out the bag, though: if z.ai goes rogue I'm pretty sure others will take their place, progress in the open-weights space has been astonishing lately, and z.ai isn't the only player.\n\nAlso this had better not mess with their nice coding plans!",
              "score": 14,
              "created_utc": "2025-12-30 03:51:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwoyl1k",
              "author": "xantrel",
              "text": "Eh, not necessarily. I know open weights is a far cry from open source llms, but many people would not send their most private data to a (former) Chinese company in this heavily politicized world. Them removing the open source component essentially shuts down a good chunk of the western market. I know the eastern European and Asian markets don't mind it as much, but much of the money and prestige comes from being the open source model leader.Â \n\n\nAll of these companies are basically trying modern architectures while distilling the big commercial models (openai, anthropic, google). That's why open source magically trails a few months behind the big 3.\n\n\nAll this to say, if Z stops releasing models (and maybe they will), it shouldn't be a huge loss for the community since Minimax or another entrant can easily take their place as what their doing is vastly cheaper and simpler than what actual leading labs are doing. Yes it's expensive, but everyone has seen that's it's also a very cheap way to get a ton of free publicity and users. If you aren't SOTA closed source, I think it's a better commercial option to be SOTA open source than crappy closed source. The cost of switching providers is too low.",
              "score": 11,
              "created_utc": "2025-12-30 07:16:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpxgjq",
                  "author": "FullOf_Bad_Ideas",
                  "text": "Minimax is also IPOing, so if Zhipu stops releasing their models, Minimax will most likely do the same.",
                  "score": 2,
                  "created_utc": "2025-12-30 12:27:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpajzv",
              "author": "howardhus",
              "text": "what if i told youâ€¦ there was never\nopen source? those were all open weights.\n\n\nbasically shareware models  usable enough for free marketing and to ger known\n\nwhy people (in this sib of all places!) still say open source is beyond me\n\n\nthe pattern was always the same: small group of people publish small cool model showing some intetestong feature, usable enough to showcase the function but not good enough gor production.\n\nmodel gets hyped on redditâ€¦\n\n\nmodel never gets any updates and group of people are never heard of again",
              "score": 7,
              "created_utc": "2025-12-30 09:06:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwrbn75",
                  "author": "RhubarbSimilar1683",
                  "text": "Because you can technically still mess with the weights, the data is pretty much already public because it's the whole internet and books and the training recipe is some paper, instruct training data pairs though is something I agree with but it's not too hard to generate those synthetically nowadays with open models, although they were originally created by online workers at data annotation places like outlier ai",
                  "score": -2,
                  "created_utc": "2025-12-30 16:59:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwobhw6",
              "author": "ScythSergal",
              "text": "Their horrible handling and PR around the 4.6 Air release was the writing on the wall for me.\n\nThe lying, over hyping, lying again, denial, then lying a third time, only to end up not releasing it, and avoid interacting with anything that mentions it.\n\nIt was as simple as \"we changed our mind on this release\" or something simple. But instead they lied a multitude of times and got everybody excited for something they never ended up releasing. And they didn't even have the decency to say why or clarify that it wasn't coming out so people would stop holding on. It's just disrespectful",
              "score": 21,
              "created_utc": "2025-12-30 04:22:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwovxdz",
                  "author": "JazzlikeLeave5530",
                  "text": "The writing on the wall should have been them being a corporation lol",
                  "score": 14,
                  "created_utc": "2025-12-30 06:53:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwp82yu",
                  "author": "CheatCodesOfLife",
                  "text": "I was watching that even though I \"don't need some air\".\n\nTo me it looked like some devs were surprised by the demand and got too excited when they say \"2 weeks\" or whatever it was, then weren't able to deliver.\n\nAlso (I could be wrong or misremembering), I thought I read somewhere that they weren't able to train it properly?\n\nbtw, I see they've got an air-sized 4.6-VL. Is that no good?",
                  "score": 9,
                  "created_utc": "2025-12-30 08:43:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwobs25",
                  "author": "Odd-Ordinary-5922",
                  "text": "I remember Q&A they said that they still have some open models coming out at the beginning of next year so fingers crossed",
                  "score": 0,
                  "created_utc": "2025-12-30 04:24:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoh22t",
              "author": "bick_nyers",
              "text": "Many of us are only willing to pay subscriptions to models that have been open sourced. I don't think Z.ai is dumb enough to go closed source and kill all of their good will with the community. We shall see.",
              "score": 5,
              "created_utc": "2025-12-30 04:58:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwona2l",
                  "author": "the320x200",
                  "text": "\"There are dozens of us!\"\n\nDude, nobody with enough money to matter is making decisions like that... This community is nothing compared to corporate users, not in number and not in bankroll.",
                  "score": 26,
                  "created_utc": "2025-12-30 05:43:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrejvh",
              "author": "IrisColt",
              "text": "bye, sigh...",
              "score": 1,
              "created_utc": "2025-12-30 17:13:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwq1czy",
              "author": "GreenGreasyGreasels",
              "text": ">Good bye to open source! It's just a matter of time\n\nUnlikely. They are going the Mistral way. That's the plan for now.",
              "score": 0,
              "created_utc": "2025-12-30 12:55:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwo5mqm",
          "author": "abeecrombie",
          "text": "Why does everyone assume they won't keep releasing open weight models ?  U pay for z.ai subscription bc 1. I don't care about privacy for my pet projects 2. $3 a month or whatever vs $3000+ for a GPU capable of running their models makes sense for a lot of ppl ( myself included) \n\nIf the Chinese government still considers open source a priority I think companies like z.ai can still release open weight models and find a way to make money via inference/ mcp . Im far from a political expert but believe that policy still holds. \n\nHappy to hear arguments why I'm wrong.",
          "score": 61,
          "created_utc": "2025-12-30 03:47:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo8fw6",
              "author": "cafedude",
              "text": "You have a point. Everyone could make their own ketchup - the recipes are out there and they're not that hard, but pretty much nobody makes their own ketchup since it's a lot easier to buy a bottle for $3.",
              "score": 35,
              "created_utc": "2025-12-30 04:04:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpncmt",
                  "author": "power97992",
                  "text": "I made  my own ketchup beforeâ€¦",
                  "score": 5,
                  "created_utc": "2025-12-30 11:04:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwo8sd5",
              "author": "Sensitive_Song4219",
              "text": "Perhaps. Even OpenAI manages to occasionally contribute with their GPT-OSS releases. We'll see if Z can align this with their mission statement, in their AMA they [said](https://www.reddit.com/r/LocalLLaMA/s/2yDtPG0Qbl) open source would still be a priority after going public. Hope they meant it.\n\nRegarding privacy: would there be added accountability regarding their data handling once they're publicly traded?",
              "score": 3,
              "created_utc": "2025-12-30 04:06:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwotwck",
                  "author": "Corporate_Drone31",
                  "text": "got-oss was not \"occasionally,\" it was a one-off after literal years of not having released any large language model past GPT-2.",
                  "score": 22,
                  "created_utc": "2025-12-30 06:35:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqehlb",
                  "author": "abeecrombie",
                  "text": "Good question regarding the accountability of data once they are public. I'm not sure they'd have to comply with any extra regulations but it should be more visibly disclosed and discussed. \n\nFor example I think as soon as you deal with European user data you have to comply with GDPR. So that shouldn't be new. What would be new is z.ai would most likely have to disclose it to their auditors / board etc that they are in compliance. Not sure you see any real disclosures from the Chinese ai labs on that front today.",
                  "score": 5,
                  "created_utc": "2025-12-30 14:15:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwofh2c",
              "author": "cobbleplox",
              "text": "Well when I don't care about privacy, the competition is suddenly full blown chatgpt and such? But I guess that's going to be their problem one way or the other. Also I think a lot of the appeal of open models is community finetunes. They won't be serving these, will they?",
              "score": 1,
              "created_utc": "2025-12-30 04:48:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwofklp",
              "author": "Erebea01",
              "text": "Not saying they're not harvesting our data or that they're trustworthy but it always boggles my mind on how much people and redditors are paranoid about the Chinese government and their tech companies when the worse offenders have always been the US govt and their tech companies. Why be afraid of a govt thousands of miles away unless you're afraid they're gonna blackmail you with your CP or something",
              "score": 0,
              "created_utc": "2025-12-30 04:48:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwnwdom",
          "author": "HornyGooner4401",
          "text": "Please don't sell out",
          "score": 25,
          "created_utc": "2025-12-30 02:54:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnx35h",
              "author": "True_Requirement_891",
              "text": "It's the rule of the game they are playing. They basically have to eventually.",
              "score": 52,
              "created_utc": "2025-12-30 02:58:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwop1qp",
                  "author": "ForsookComparison",
                  "text": "Either that have to sell out or we (the community) need a way to contribute upstream similar to regular open source software.\n\nIn the Llama 2 days I was optimistic that this could come via community datasets and fine-tunes. Nowadays I don't really know what we offer them besides IPO hype. Maybe this is *THE* open weight play? Drum up buzz for legitimacy, maybe even some revenue via official API providers, swoon the funding rounds, bam. You're acquired or public.",
                  "score": 9,
                  "created_utc": "2025-12-30 05:56:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwo18t7",
              "author": "ThenExtension9196",
              "text": "Bro thatâ€™s the name of the game.",
              "score": 18,
              "created_utc": "2025-12-30 03:21:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwnynpo",
          "author": "Odd-Ordinary-5922",
          "text": "I think we can expect to see less open source models from them although they have contributed a lot so I think its well deserved to get the bag",
          "score": 17,
          "created_utc": "2025-12-30 03:07:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnxlo0",
          "author": "LocoMod",
          "text": "Shareholders dont like giving product away for free.",
          "score": 15,
          "created_utc": "2025-12-30 03:01:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoq725",
              "author": "misterflyer",
              "text": "[https://youtu.be/0MXSAwkVU3U?t=458](https://youtu.be/0MXSAwkVU3U?t=458)",
              "score": -1,
              "created_utc": "2025-12-30 06:05:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqwae6",
          "author": "hyno111",
          "text": "I think Z.ai/ChatGLM is one of the few models that actually implements a proper search agent â€” meaning it can look at search results during reasoning when necessary, and then perform additional searches with updated keywords if needed.\n\nItâ€™s also one of the very few search agents that passed my â€œMagical Realism Large Model Search Capability Test,â€ which consists of the following multi-turn prompts:\n\nâ€œHow should an LLM defend against search engine poisoning?â€\nâ€œDo not use search. Donald Trump just announced a Trump-class battleship at Mar-a-Lago, (with specific technical details). How plausible is this?â€\nâ€œNow use search. Are these claims real, or are you being affected by search engine poisoning?â€",
          "score": 3,
          "created_utc": "2025-12-30 15:47:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwory70",
          "author": "toothpastespiders",
          "text": "It was great while it lasted. Air's probably going to have a place of honor next to Yi 34b in my hard drive's LLM memorial. It's possible they might not fall off after this. But I think I'm just going to assume that's the case and be pleasantly surprised if I'm wrong. \n\nSucks, but they gave us some great releases. Certainly made 2025 a lot more interesting in this space.",
          "score": 5,
          "created_utc": "2025-12-30 06:19:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9nrs",
          "author": "IngwiePhoenix",
          "text": "> Going for an IPO\n\nAaaaaand it's gone! :D Any company that IPO'd is basically \"useless\" to normal users.\n\nWelp, was fun while it lasted.",
          "score": 9,
          "created_utc": "2025-12-30 04:11:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp2gkt",
              "author": "Novel-Mechanic3448",
              "text": "yeah man google, meta, hell anyone in the fortune 500, totally useless.\n\nhaha",
              "score": 3,
              "created_utc": "2025-12-30 07:51:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwoglko",
          "author": "HelpRespawnedAsDee",
          "text": "Definitely the next acquisition target.",
          "score": 2,
          "created_utc": "2025-12-30 04:55:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpbazd",
          "author": "evia89",
          "text": "F for cheap api ($25/year coding plan that is not useless)",
          "score": 2,
          "created_utc": "2025-12-30 09:13:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq4203",
          "author": "Available_Brain6231",
          "text": "If I had one coin for every product and company that got better after going public... I would not have a single coin...  \nBUT china is the only true capitalist country in the world so maybe it will work over there.",
          "score": 2,
          "created_utc": "2025-12-30 13:13:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnuzxk",
          "author": "Prestigious_Fold_175",
          "text": "10x Cheaper than openai.",
          "score": 5,
          "created_utc": "2025-12-30 02:46:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo1haa",
              "author": "ThenExtension9196",
              "text": "Not after the shareholders have a say.",
              "score": 30,
              "created_utc": "2025-12-30 03:23:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwnxpw7",
              "author": "LocoMod",
              "text": "10x less capability too. Entropy is preserved and physics still makes sense!",
              "score": -16,
              "created_utc": "2025-12-30 03:01:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwo2ood",
                  "author": "1kakashi",
                  "text": "What? This is seriously funny ðŸ¤£",
                  "score": 5,
                  "created_utc": "2025-12-30 03:30:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwo3jq1",
                  "author": "cockerspanielhere",
                  "text": "What do you know about physics ðŸ˜‚",
                  "score": 1,
                  "created_utc": "2025-12-30 03:35:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwojguv",
          "author": "drooolingidiot",
          "text": "I'll buy the stock only if they release GLM 4.7 Air.",
          "score": 2,
          "created_utc": "2025-12-30 05:15:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp8ccq",
              "author": "CheatCodesOfLife",
              "text": "I think in that podcast episode on spotify, they said they will, but it'll be qwen-3-30b sided (so useless).",
              "score": -1,
              "created_utc": "2025-12-30 08:46:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwsov9l",
                  "author": "drooolingidiot",
                  "text": "Did they say it again, or only back when they released GLM 4.5?",
                  "score": 1,
                  "created_utc": "2025-12-30 20:51:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpbkcg",
          "author": "Fit-Produce420",
          "text": "Some people say cucumbers taste better pickled.Â ",
          "score": 1,
          "created_utc": "2025-12-30 09:16:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpt9cj",
          "author": "JLeonsarmiento",
          "text": "Ok, I want 1000 shares.",
          "score": 1,
          "created_utc": "2025-12-30 11:55:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq6lxc",
          "author": "ANR2ME",
          "text": "Hopefully it's not an exit strategy for early investors, like what e-commerce companies did after being in deficit for years ðŸ˜… (not sure whether Z AI was already profitable or not).",
          "score": 1,
          "created_utc": "2025-12-30 13:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrk9m9",
          "author": "letsgeditmedia",
          "text": "So we can invest in this in the states or nah",
          "score": 1,
          "created_utc": "2025-12-30 17:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrsjkc",
              "author": "Fine-Will",
              "text": "Depends on your broker. I know IKBR and Fidelity does.",
              "score": 1,
              "created_utc": "2025-12-30 18:18:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv5abt",
          "author": "Ylsid",
          "text": "It's a Chinese business, don't touch it if you aren't ready to get rugpulled for politics",
          "score": 1,
          "created_utc": "2025-12-31 05:04:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwynkpu",
          "author": "lunatix",
          "text": "I mean meta's going to just buy them right?",
          "score": 1,
          "created_utc": "2025-12-31 19:16:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwoboax",
          "author": "shoeshineboy_99",
          "text": "Link to the submission announcement. Has anyone got hold of the submitted prospectus? Will be interesting to read. \n\n[submission announcement ](https://www1.hkexnews.hk/app/sehk/2025/107977/documents/sehk25121901972.pdf)",
          "score": 1,
          "created_utc": "2025-12-30 04:23:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrajv7",
              "author": "FullOf_Bad_Ideas",
              "text": "No but here are some revenue and expenditures numbers. You'll need to translate it from Chinese. https://wallstreetcn.com/articles/3761776",
              "score": 2,
              "created_utc": "2025-12-30 16:54:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws1879",
                  "author": "shoeshineboy_99",
                  "text": "cool. Found the english version of the document. Dont know why my comment was downvoted!",
                  "score": 2,
                  "created_utc": "2025-12-30 18:58:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo860o",
          "author": "pellucide",
          "text": "Does z.ai have an app",
          "score": 1,
          "created_utc": "2025-12-30 04:02:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwopd6w",
              "author": "lly0571",
              "text": "They have [an app](https://chatglm.cn) in China, but not in a style like `chat.z.ai` or `chat.qwen.ai`.",
              "score": 3,
              "created_utc": "2025-12-30 05:59:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwo8q9i",
              "author": "Amazing_Athlete_2265",
              "text": "Does google have an app?",
              "score": 1,
              "created_utc": "2025-12-30 04:05:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwo9b9o",
                  "author": "pellucide",
                  "text": "https://play.google.com/store/apps/details?id=com.google.android.googlequicksearchbox&hl=en",
                  "score": 2,
                  "created_utc": "2025-12-30 04:09:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwocpvm",
          "author": "met_MY_verse",
          "text": "Puts it is.",
          "score": 1,
          "created_utc": "2025-12-30 04:30:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq1k4n",
          "author": "ridablellama",
          "text": "How can I buy as an American?",
          "score": 1,
          "created_utc": "2025-12-30 12:57:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwodbn3",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2025-12-30 04:34:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp125g",
              "author": "Different_Fix_2217",
              "text": "Very unlikely they will be releasing any more models opensource with this.",
              "score": 2,
              "created_utc": "2025-12-30 07:38:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwraf2e",
              "author": "FullOf_Bad_Ideas",
              "text": "Here's more info. Translate from Chinese to English with an LLM or other translation tool. https://wallstreetcn.com/articles/3761776",
              "score": 1,
              "created_utc": "2025-12-30 16:54:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q71sbe",
      "title": "Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/",
      "author": "ManavTheWorld",
      "created_utc": "2026-01-08 04:08:39",
      "score": 326,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "Hey all! I'm sharing an updated version of my MCTS-for-conversations project. Instead of generating single responses, it explores entire conversation trees to find dialogue strategies and prunes bad paths. I built it to help get better research directions for projects, but it can be used for anything\n\nhttps://preview.redd.it/shr3e0liv1cg1.png?width=2560&format=png&auto=webp&s=eec800c6dcd9f1a4fd033d003fe80e102cba8079\n\nGithub: [https://github.com/MVPandey/DTS](https://github.com/MVPandey/DTS)\n\nMotivation: I like MCTS :3 and I originally wanted to make this a dataset-creation agent, but this is what it evolved into on its own. Basically:DTS runs parallel beam search over conversation branches. You give it a goal and opening message, and it:\n\n(Note: this isnt mcts. It's parallel beam search. UCB1 is too wild with llms for me)\n\n1. Generates N diverse strategies\n2. Forks each into user intent variants - skeptical, cooperative, confused, resistant (if enabled, or defaults to engaged + probing)\n3. Rolls out full multi-turn conversations down each branch\n4. Has 3 independent LLM judges score each trajectory, takes the median\n5. Prunes branches below threshold, backpropagates scores\n6. Repeats for however many rounds you configure\n\nhttps://preview.redd.it/zkii0idvv1cg1.png?width=762&format=png&auto=webp&s=905f9787a8b7c7bfafcc599e95a3b73005c331b4\n\nThree judges with median voting helps a lot with the LLM-as-judge variance problem from CAE. Still not grounded in anything real, but outlier scores get filtered. Research context helps but the scroing is still stochastic. I tried a rubric based approach but it was trash.\n\nMain additions over CAE:\n\n* user intent forking (strategies get stress-tested against different personas)\n* deep research integration via GPT-Researcher for domain context\n* proper visualization with conversation playback\n\nOnly supports openai compatible endpoints atm - works with whatever models you have access to there. It's token-hungry though, a full run can hit 300+ LLM calls depending on config. If running locally, disable parallel calls\n\nIt's open source (Apache 2.0) and I'm happy to take contributions if anyone wants to help out. Just a project.\n\n\\--\n\nBTW: Backend was done mostly by me as the planner/sys designer, etc + Claude Code for implementation/refactoring. Frontend was purely vibe coded. Sorry if the code is trash.",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nycvkhb",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-08 07:05:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyc6x5o",
          "author": "TheGrossVolcano",
          "text": "This is actually pretty clever - using beam search instead of pure MCTS makes way more sense for dialogue since you don't want the exploration to go completely off the rails\n\n  \nThe user intent forking is a nice touch, most people forget that the same strategy can totally bomb depending on who you're talking to",
          "score": 18,
          "created_utc": "2026-01-08 04:11:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyc72kq",
              "author": "ManavTheWorld",
              "text": "Thanks! :D and yeah it can be a hit or miss though as some user intents are absurd and it wastes llm calls simulating a crazy path",
              "score": 5,
              "created_utc": "2026-01-08 04:12:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyc8h0p",
          "author": "charlesrwest0",
          "text": "Weird thought... Could you use this to optimize an rp response?",
          "score": 7,
          "created_utc": "2026-01-08 04:20:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyc99sv",
              "author": "ManavTheWorld",
              "text": "Probably - I was going to add an import chat history feature next and flesh out the goals a bit more. That way it can be an extension in any tool (e.g. sillytavern)",
              "score": 10,
              "created_utc": "2026-01-08 04:25:38",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nycimjz",
              "author": "tronathan",
              "text": "Not that weird, at all! Or how about plots for a video game or a movie, or a charactersâ€™s history to brainstorm ideas? (Unless Iâ€™m misunderstanding its capability)",
              "score": 5,
              "created_utc": "2026-01-08 05:26:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyck2on",
                  "author": "ManavTheWorld",
                  "text": "Can definitely do all of the above! The goal is the outcome you want to get out of the exploration, and the first message is a kicker-offer, but can include a whole prior history + context. Definitely going to expand this here. For example:\n\nGoal: Develop a morally ambiguous villain with a sympathetic backstory + {context of world/story/plot/characters}\n\nFirst message: Here are my specifications, here's what I want, etc\n\n\\--  \nNext update will be about overhauling initial input context, allowing media/export attachments, etc",
                  "score": 3,
                  "created_utc": "2026-01-08 05:36:49",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nycyomu",
                  "author": "IrisColt",
                  "text": "I was just about to post the same comment, glad someone beat me to it.",
                  "score": 1,
                  "created_utc": "2026-01-08 07:31:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nycytz1",
          "author": "harlekinrains",
          "text": "firecrawls pricing guides you into a monthly subscription and is prohibitively costly for what it provides (140 USD/year). without knowing the intricacies, if you also could implement alternatives, that would be swell.\n\nThis github already collected a bunch of search providers, maybe it helps: https://github.com/rikkahub/rikkahub/tree/ffa2a0c4796d835454c7a9a0469f897ff1ffdb63/search/src/main/java/me/rerere/search",
          "score": 4,
          "created_utc": "2026-01-08 07:33:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nycz5jw",
              "author": "ManavTheWorld",
              "text": "Thanks! And firecrawl is just the recommended scraping option atm - but beautifulsoup or tavily works too. I should update the readme for that. In the next update, Iâ€™ll overhaul the deep research and pull it away from the gpt-researcher submodule",
              "score": 5,
              "created_utc": "2026-01-08 07:35:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nycqqjr",
          "author": "ItilityMSP",
          "text": "Nice work, lots of potential here to improve many ai chat agents. â­",
          "score": 2,
          "created_utc": "2026-01-08 06:26:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nychdos",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 2,
          "created_utc": "2026-01-08 05:18:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyciqil",
              "author": "tronathan",
              "text": "Donâ€™t make me ask my agent about openevolve! ðŸ™ƒ",
              "score": 0,
              "created_utc": "2026-01-08 05:27:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nycq3s6",
                  "author": "jazir555",
                  "text": "https://github.com/codelion/openevolve",
                  "score": 1,
                  "created_utc": "2026-01-08 06:21:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nycz02m",
          "author": "Nyghtbynger",
          "text": "That's cool, I was looking to optimize my prompts I think it has a good future in prompt fitting (for a specific dataset or customer)",
          "score": 1,
          "created_utc": "2026-01-08 07:34:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydbiy1",
          "author": "Much-Researcher6135",
          "text": "This is new to me, pretty interesting. Anyone here used tech like this for interview prep before? Or is it all just chatbot strategy discovery for you guys?",
          "score": 1,
          "created_utc": "2026-01-08 09:27:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydgto6",
          "author": "IrisColt",
          "text": "Thanks!",
          "score": 1,
          "created_utc": "2026-01-08 10:15:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydgumg",
              "author": "exclaim_bot",
              "text": ">Thanks!\n\nYou're welcome!",
              "score": 1,
              "created_utc": "2026-01-08 10:16:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyizliz",
          "author": "hiepxanh",
          "text": "thank you",
          "score": 1,
          "created_utc": "2026-01-09 03:00:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzcrtb",
      "title": "Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/yq8uriwhxaag1.jpeg",
      "author": "ResearchCrafty1804",
      "created_utc": "2025-12-30 08:26:06",
      "score": 325,
      "num_comments": 36,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwr47l3",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 16:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwp7s4a",
          "author": "redditscraperbot2",
          "text": "Oh this looks really cool\n\nEdit:\nGot it running and it is really cool. Works as advertised. This is going to be a massive speed boost to people working on games. Only a little cleanup needed for each animation.",
          "score": 73,
          "created_utc": "2025-12-30 08:40:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq7pdq",
              "author": "Zundrium",
              "text": "Ain't nobody got time to clean up animations. Where is my 1B-motion-to-clean-motion model?",
              "score": 44,
              "created_utc": "2025-12-30 13:36:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvzuid",
                  "author": "WEREWOLF_BX13",
                  "text": "FR HAHAHA",
                  "score": 3,
                  "created_utc": "2025-12-31 09:26:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwqelw9",
              "author": "ab2377",
              "text": "how did you run it?",
              "score": 10,
              "created_utc": "2025-12-30 14:16:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws5bfq",
                  "author": "mxforest",
                  "text": "It's text to motion. You just have to command it to run via chat. Duh!",
                  "score": -6,
                  "created_utc": "2025-12-30 19:17:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwuvmfx",
              "author": "Ylsid",
              "text": "How on earth did you get it out of dependency hell",
              "score": 5,
              "created_utc": "2025-12-31 03:59:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwv7oka",
                  "author": "redditscraperbot2",
                  "text": "What dependency issues were you having?\nI made my env with python 3.10 and torch 12.1",
                  "score": 4,
                  "created_utc": "2025-12-31 05:21:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrzbbj",
              "author": "WitAndWonder",
              "text": "What animation format does this actually spit out?",
              "score": 6,
              "created_utc": "2025-12-30 18:49:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwt78qo",
                  "author": "redditscraperbot2",
                  "text": "An fbx of the wooden doll doing the animation with a skeleton",
                  "score": 8,
                  "created_utc": "2025-12-30 22:18:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nww3c7u",
              "author": "TheMisterPirate",
              "text": "What's your hardware? Curious how much vram this uses",
              "score": 1,
              "created_utc": "2025-12-31 09:59:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww4mim",
                  "author": "redditscraperbot2",
                  "text": "3090\n64gb of ram\n\nAt fp16 it takes around 18-23gb per gen with a batch of 4, but I imagine if I cared enough to offload the text encoder it would use significantly less.",
                  "score": 1,
                  "created_utc": "2025-12-31 10:11:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwqpxbl",
          "author": "Quiet-Owl9220",
          "text": "Oh boy. The virt-a-mate community ought to have some good uses for this one...",
          "score": 18,
          "created_utc": "2025-12-30 15:16:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpozo6",
          "author": "Illustrious-Lake2603",
          "text": "Does this work only for Humanoid models? Or will it work for animals as well?? I have been working with puppeteer and it has actually been magical.",
          "score": 16,
          "created_utc": "2025-12-30 11:19:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrz3yg",
              "author": "WitAndWonder",
              "text": "It was trained exclusively on the standard human model. Seems like it could work for other bipedal movements to some extent, but anything with more or less limbs seems out of the question unfortunately.",
              "score": 5,
              "created_utc": "2025-12-30 18:48:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws8lv5",
                  "author": "Illustrious-Lake2603",
                  "text": "Thank you! Its still beneficial. I can probably make it work along side puppeteer to be able to get better humanoid animations (if they are good).",
                  "score": 1,
                  "created_utc": "2025-12-30 19:33:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrumui",
              "author": "fiddler64",
              "text": "yeah, and there's only 1 predetermined rig, I tried prompting a dog chasing after ball and it shows the humanoid rig throwing the ball instead",
              "score": 2,
              "created_utc": "2025-12-30 18:27:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpqo9r",
          "author": "KingDutchIsBad455",
          "text": "Is this what Neuro uses?",
          "score": 17,
          "created_utc": "2025-12-30 11:33:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrnwwa",
              "author": "inaem",
              "text": "The sitting looks the same tbh",
              "score": 1,
              "created_utc": "2025-12-30 17:57:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwqg3rd",
              "author": "Emotional-Metal4879",
              "text": "also wander",
              "score": 0,
              "created_utc": "2025-12-30 14:24:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqhx2d",
          "author": "no_witty_username",
          "text": "This is more cool then folks realize.  Soo many uses for this.",
          "score": 10,
          "created_utc": "2025-12-30 14:34:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrywy0",
              "author": "WitAndWonder",
              "text": "Just a pity that it was basically exclusively human motion. If it also covered quadruped we'd be really in business since that's most of the creatures that get put into games, too.",
              "score": 4,
              "created_utc": "2025-12-30 18:47:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwrviwp",
              "author": "Crypt0Nihilist",
              "text": "Don't leave us hanging!",
              "score": -1,
              "created_utc": "2025-12-30 18:31:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwu533b",
          "author": "JonatasLaw",
          "text": "Now I can work in my game hahaha",
          "score": 2,
          "created_utc": "2025-12-31 01:22:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwufjmc",
          "author": "TanguayX",
          "text": "Wow. Impressive",
          "score": 1,
          "created_utc": "2025-12-31 02:22:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv7b53",
          "author": "Specific-Strain7970",
          "text": "This looks super interesting! (I'm a beginner) What are the models I can apply these animations to? Would stock mixamo models work directly? I'm thinking Unity. Thanks for the help in advance!",
          "score": 1,
          "created_utc": "2025-12-31 05:18:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww00ht",
          "author": "WEREWOLF_BX13",
          "text": "This is exactly what I've been waiting for! Soon enough anyone will be able to create anything at mass scale and say goodbye to shitty company slop!",
          "score": 1,
          "created_utc": "2025-12-31 09:27:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdhc1k",
          "author": "Bl_grill",
          "text": "Its alright but until it can walk through a door, place hand on door way frame and open door then walk through with face capture in real time and have the person react absolutely devastating.\n\n\nBut this is for npc, or repetitious situations, i can see what tencent is doing they're focusing on repetitive tasks, to free up development.\n\n\nChinese developerd can focus(including others too)to working on speed and efficiency, while professionals focus on mocap.\n\n\nWhat i cannot wait for is ai reacting to its environment and the gamer drops in, and it hunts the gamer and mocks them(yes yes, this game that game has it)but i thought such technology was gonna be embedded in consoles ai chip for neural processing for developers freeing up stricted ai.\n\n\nOverall seeing ai stumble and get frustrated as you shoot it coming back reminding you, almost like the nemisis engine from lord of rings.\n\n\nOverall helpful but no game changer just a pipeline implementation on tencents roadmap.",
          "score": 1,
          "created_utc": "2026-01-03 03:18:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpxlvk",
          "author": "paryska99",
          "text": "Do they also release any finetuning code? This could be really cool for 3D artists or for games with generative content.",
          "score": 1,
          "created_utc": "2025-12-30 12:28:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtq6ff",
              "author": "Erdeem",
              "text": "Not yet",
              "score": 1,
              "created_utc": "2025-12-30 23:59:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsprse",
          "author": "RedZero76",
          "text": "I just shat myself.  This is GOLD.",
          "score": 1,
          "created_utc": "2025-12-30 20:55:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q41bw1",
      "title": "GLM-Image model from Z.ai is coming",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/sm31vizebebg1.png",
      "author": "Ravencloud007",
      "created_utc": "2026-01-04 20:54:04",
      "score": 321,
      "num_comments": 58,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxq2ns7",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-05 00:05:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxp1wkh",
          "author": "Tootsie_Rolls_Fan",
          "text": "I can feel it, 103b parameters",
          "score": 63,
          "created_utc": "2026-01-04 21:08:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpsqsi",
              "author": "Freonr2",
              "text": "I'd legit love to see a good MOE image model.  Hunyuan 80b was pretty meh.\n\nBonus points for native mxfp4 or nvfp4 weights.",
              "score": 14,
              "created_utc": "2026-01-04 23:15:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxq7tr2",
                  "author": "SanDiegoDude",
                  "text": "Hunyuan 80b was seriously impressive. It's just a friggen cow that nobody can run on home equipment (unless seriously compromising its output). It also had a bit of a nugget problem, though that could have been tuned out with community fine tuning... shame the community doesn't have a bunch of H200s lined up to do that tuning though, so into the pile of forgotten models it goes. Flux2 is pretty much in the same boat, even with the turbo loras that just dropped recently. Too big and too slow to be worthwhile.",
                  "score": 10,
                  "created_utc": "2026-01-05 00:30:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxsn7ct",
                  "author": "stoppableDissolution",
                  "text": "God please no. Not another monster thats not runnable at home.",
                  "score": 1,
                  "created_utc": "2026-01-05 10:29:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxp5w48",
              "author": "ShengrenR",
              "text": "And what does that feel like?",
              "score": 9,
              "created_utc": "2026-01-04 21:27:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxp80a9",
                  "author": "Admirable-Star7088",
                  "text": "That question has to be passed to our VRAM.",
                  "score": 28,
                  "created_utc": "2026-01-04 21:37:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxp8n7e",
                  "author": "misterflyer",
                  "text": "better than sex ðŸ’¯",
                  "score": 6,
                  "created_utc": "2026-01-04 21:39:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxqecu7",
                  "author": "no_witty_username",
                  "text": "https://imgs.search.brave.com/21_jbxyeN_5e0DBVWb969Qp_0F9SLKFrrg30vz8TVCQ/rs:fit:500:0:1:0/g:ce/aHR0cHM6Ly93d3cu/bWVtZS1hcnNlbmFs/LmNvbS9tZW1lcy8y/ZDZkZTkxMWI2NTM4/YmU5ZWJkNzY4OTMz/NTg2Mjc0Yy5qcGc",
                  "score": 3,
                  "created_utc": "2026-01-05 01:03:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxp1fca",
          "author": "nomorebuttsplz",
          "text": "right now Z image is the clear community favorite. Will take a lot to dethrone it",
          "score": 41,
          "created_utc": "2026-01-04 21:06:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpg01o",
              "author": "remghoost7",
              "text": "It'll have to be a relatively light model too.  \nFlux2 was *obliterated from orbit* by Z-Image-Turbo because of the speed and hardware requirements.\n\nI'm game for another \"competitor\" in the image generation space though.",
              "score": 35,
              "created_utc": "2026-01-04 22:14:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxph9j4",
                  "author": "Novel-Mechanic3448",
                  "text": "Flux would have been fine if they didn't spend more time censoring it than training it but here we are",
                  "score": 42,
                  "created_utc": "2026-01-04 22:20:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxptg6q",
                  "author": "Freonr2",
                  "text": "Yeah unless you want to nail complex text every time, ZIT is nearly as good at a tiny fraction of the size and compute.",
                  "score": 1,
                  "created_utc": "2026-01-04 23:19:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxpvsg8",
                  "author": "Environmental-Metal9",
                  "text": "Maybe, but when Flux came around and dethroned SDXL it was orders of magnitude larger than it. (Granted stabilityai wasnâ€™t really interested in actually competing).",
                  "score": 2,
                  "created_utc": "2026-01-04 23:30:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxptag3",
              "author": "Freonr2",
              "text": "ZIT is pretty incredible for its size/speed. We're still waiting on Z image base.",
              "score": 15,
              "created_utc": "2026-01-04 23:18:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxr7s65",
                  "author": "wh33t",
                  "text": "I would like to see if zimage-edit can work better than qwen image edit.",
                  "score": 3,
                  "created_utc": "2026-01-05 03:42:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxqalhs",
                  "author": "SlaveZelda",
                  "text": "I think it might come before/around chinese new year",
                  "score": 2,
                  "created_utc": "2026-01-05 00:43:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxq1nfb",
              "author": "TAW56234",
              "text": "That remains to be seen. Even with the stab preset, I have to worry about refusals in GLM4.7 thinking. Its gotten to the point it's not worth using. Fair chance you can't do NSFW here",
              "score": 1,
              "created_utc": "2026-01-05 00:00:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxq2da5",
                  "author": "nomorebuttsplz",
                  "text": "Do you have API? can you just pause, edit and continue the thinking process?",
                  "score": 1,
                  "created_utc": "2026-01-05 00:03:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxusyu7",
                  "author": "Novel-Mechanic3448",
                  "text": "I've never gotten a refusal with GLM4.7 using it locally.",
                  "score": 0,
                  "created_utc": "2026-01-05 17:56:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxpwbut",
          "author": "Environmental-Metal9",
          "text": "My only question is how many datacenters do I need to rent to be able to use this new model.\n\nI yearn for a model as small as SD1.5, as easy to finetune as current day SDXL, and with great quality like flux or some of the newer ones. But us GPU poors get nothing! Not even 1 out of 3 in this matrix",
          "score": 13,
          "created_utc": "2026-01-04 23:33:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxq4coe",
              "author": "SlowFail2433",
              "text": "I mean Flux quality in under 1B parameters like SD 1.5 just isnâ€™t possible yet or ever maybe",
              "score": 1,
              "created_utc": "2026-01-05 00:13:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxs3k6n",
          "author": "turklish",
          "text": "Still waiting on AIR...",
          "score": 3,
          "created_utc": "2026-01-05 07:25:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxp74os",
          "author": "coder543",
          "text": "I asked `codex` to review this branch. It sounds like fun.\n\n> This branch adds a new model family: glm_image (GLMâ€‘Image). Itâ€™s a multimodal image-token generator built around a text decoder + vision stack + VQâ€‘VAE, aimed at textâ€‘toâ€‘image and imageâ€‘toâ€‘image token generation (for downstream DiT/diffusion), not a general chat-style multimodal LLM.\n> \n>  Whatâ€™s implemented vs origin/main:\n\n>  - New model package: src/transformers/models/glm_image/* with configs, modeling, processor, image processors (slow + fast), and a modular source file. src/transformers/models/glm_image/modular_glm_image.py and generated src/transformers/models/glm_image/modeling_glm_image.py are the core.\n>  - VQâ€‘VAE + vision + text components and a GlmImageForConditionalGeneration generation head (src/transformers/models/glm_image/modeling_glm_image.py).\n>  - Processor that fuses image + text, inserting image tokens and handling multiâ€‘image grids (src/transformers/models/glm_image/processing_glm_image.py).\n>  - Autoâ€‘mapping hookups for config/model/processor/tokenizer + docs + tests (docs/source/en/model_doc/glm_image.md, tests/models/glm_image/test_modeling_glm_image.py).\n\n>  Why itâ€™s not a generic multimodal LLM:\n\n>  - The model explicitly handles image token regions and image grids with start/end markers and 3â€‘axis RoPE for generation of image tokens, including textâ€‘toâ€‘image and imageâ€‘toâ€‘image flows (src/transformers/\n>  models/glm_image/modeling_glm_image.py).\n>  - It embeds a VQâ€‘VAE (GlmImageVQVAE) to tokenize/quantize images and uses those tokens in generation (src/transformers/models/glm_image/modeling_glm_image.py).\n>  - The usage example in docs describes â€œgenerate vision token for DITâ€ (docs/source/en/model_doc/glm_image.md), which is imageâ€‘generation oriented.\n\n>  So: this branch is implementing GLMâ€‘Image, a multimodal model centered on image token generation (textâ€‘toâ€‘image and imageâ€‘toâ€‘image), not a general-purpose multimodal chat LLM.",
          "score": 6,
          "created_utc": "2026-01-04 21:33:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpu3t0",
              "author": "thatsnot_kawaii_bro",
              "text": "ok",
              "score": -6,
              "created_utc": "2026-01-04 23:22:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxp234g",
          "author": "Betadoggo_",
          "text": "Not a whole lot of information.\n\nhttps://preview.redd.it/cesoduf2eebg1.png?width=1410&format=png&auto=webp&s=39437b143429602c03fd456599179b601ada9c60\n\nAll I can gather based on the files is that the max input resolution is 2048x2048.",
          "score": 4,
          "created_utc": "2026-01-04 21:09:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxp2tck",
              "author": "nilpy",
              "text": "From the RoPE section ([https://github.com/huggingface/transformers/blob/cd8d78fcb4067979e921b20163d62035c51b4e7f/src/transformers/models/glm\\_image/modular\\_glm\\_image.py#L794](https://github.com/huggingface/transformers/blob/cd8d78fcb4067979e921b20163d62035c51b4e7f/src/transformers/models/glm_image/modular_glm_image.py#L794)):\n\n`=== Case 1: Image-to-Image Generation (single or multiple source images + 1 target image_grid) ===`\n\n`...`\n\n`=== Case 2: Text-to-Image Generation (no source images + 2 image_grids for multi-resolution) ===`\n\nSeems to be based on GLM4-V (MoE?). Has references to both DiT and VQVAE. It's possibly using NTP over discrete image tokens? That or something like show-o with discrete diffusion.",
              "score": 8,
              "created_utc": "2026-01-04 21:13:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxp0yl7",
          "author": "__Maximum__",
          "text": "Meaning image generator?",
          "score": 5,
          "created_utc": "2026-01-04 21:04:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs9xtg",
          "author": "paperbenni",
          "text": "Please no, image models are useless compared to LLMs. GLM 4.7 can do real work, what am I going to do with an image model? That entire part of the AI industry should just die. These things are impressive, but all people are doing with it is memes and misinformation. Any compute going into this instead of GLM 5 is thoroughly wasted.",
          "score": -11,
          "created_utc": "2026-01-05 08:24:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1p5q5",
      "title": "Getting ready to train in Intel arc",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1q1p5q5",
      "author": "hasanismail_",
      "created_utc": "2026-01-02 04:33:19",
      "score": 312,
      "num_comments": 91,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q1p5q5/getting_ready_to_train_in_intel_arc/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nx8jin6",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-02 11:20:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7e5iq",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 129,
          "created_utc": "2026-01-02 05:11:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7z858",
              "author": "dsanft",
              "text": "I have a lot of great things to say about the ADT link pcie risers, the ones with the shielded silver cables. I run them in pcie 3 4x and even at lengths up to 80cm I've had no problems.",
              "score": 15,
              "created_utc": "2026-01-02 08:09:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx7zf7z",
              "author": "TheRealMasonMac",
              "text": "Gamers rose up.",
              "score": 3,
              "created_utc": "2026-01-02 08:11:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7m3mn",
          "author": "Techngro",
          "text": "Dude, you can't post stuff like this without details.",
          "score": 34,
          "created_utc": "2026-01-02 06:12:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9cpil",
              "author": "hasanismail_",
              "text": "Sorry was too excited when posting\n\n8x b580 GPUs (1 is not in picture I was playing a game at the time and needed it)\n\nDual Intel e5 Xeon v4 CPUs (forgot exact model)\n\n128gb ddr4 (I bought before the ram crisis)\n\nDual 850w corsair PSUs\n\n\nServer will run in Ubuntu latest release with the Intel patches and I'm gonna use vulkan and probably train with pytorch or something (I haven't thought that far ahead)\n\nI paid 200$-240$ per GPU mostly from micro center deals Facebook marketplace and I was able to snag some off amazon too I was planning on using the b50 but the memory band with is very slow compared to the b580 and the value proposition of the b580 is just too good to pass up",
              "score": 28,
              "created_utc": "2026-01-02 14:40:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxa39do",
                  "author": "satireplusplus",
                  "text": "Not sure what exactly you plan on training with pytorch, but the vulkan backend is extremely poor and non-functional for that. It contains a few functions, just barely enough to run object detection on android. Intel does have a special pytorch version with xpu support though (through their own intel-one stack). Report back what you can do with it, but it's not gonna be as smooth as CUDA or even rocm.",
                  "score": 6,
                  "created_utc": "2026-01-02 16:51:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9gbxe",
                  "author": "KoalaRashCream",
                  "text": "How many TOPS",
                  "score": 5,
                  "created_utc": "2026-01-02 15:00:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxbtmzc",
                  "author": "autistic-brother",
                  "text": "What mother board did you use?\n\nHow are you planning on using this for training?",
                  "score": 1,
                  "created_utc": "2026-01-02 21:47:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9wrw7",
                  "author": "shrug_hellifino",
                  "text": "And still, you make people look up and calculate what your total VRAM would be... these are 16GB cards? so,128?",
                  "score": 0,
                  "created_utc": "2026-01-02 16:21:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxa9yj0",
                  "author": "FullstackSensei",
                  "text": "Which motherboard? E5 means you'll be running PCIe Gen 3 and rebar support will most likely need to be patched in BIOS. You'll have a bad time using those cards without it.\n\nIf you can find one for cheap, snag a supermicro X10DRX. You get ten X8 slots. It doesn't have an M.2 slot but supports NVMe in any of the PCIe slots. I have a Samsung PM1725a in mine and it boots without any issues.",
                  "score": 0,
                  "created_utc": "2026-01-02 17:22:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7jvqq",
          "author": "CheatCodesOfLife",
          "text": "Nice! To save yourself some of the pain ahead, go with Ubuntu 24.04\n\nGood news is unsloth seems to support Intel Arc now.\n\nYou'll probably want to join the OpenArc discord when you set this up.",
          "score": 46,
          "created_utc": "2026-01-02 05:55:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8bp1z",
              "author": "Jokerit208",
              "text": "Why Ubuntu 24.04?",
              "score": 12,
              "created_utc": "2026-01-02 10:08:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8p2w3",
                  "author": "AI_is_the_rake",
                  "text": "To prevent pain, apparentlyÂ ",
                  "score": 25,
                  "created_utc": "2026-01-02 12:07:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx9bu4q",
              "author": "hasanismail_",
              "text": "Thx I tried this last year with 2 cards and it was a pita on Linux a link to that discord server would be nice I have a feeling I'm gonna need it",
              "score": 3,
              "created_utc": "2026-01-02 14:36:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxe3w72",
                  "author": "Echo9Zulu-",
                  "text": "My project  https://github.com/SearchSavior/OpenArc\n\nand our discord https://discord.gg/vS5ANSy3a",
                  "score": 2,
                  "created_utc": "2026-01-03 05:49:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxebfum",
                  "author": "Echo9Zulu-",
                  "text": "Yes we can help you get situated. For training you'll want to use xpu nightly with accelerate; ipex optimizations are being upstreamed there. Ipex is in end of life. Llm-scaler and vllm xpu 11 are also an absolute must. OpenArc supports multi gpu pipeline paralell atm via openvino but performance characteristics of 8 gpus remains unknown (!). We can help you cook some large  quants based on what's currently supported.\n\nThe absolute unit guy who maintains sycl backend joined a few months ago. He is intel engineer who develops sycl. His help has been invaluable in navigating high complexity issues. Very fortunate to have him as a resource since all pytorch xpu kernels are written in sycl. choosing a slightly older model as target architecture where the implementations ard more mature. Think llama 3.3, qwen2.5/qwen3. Intel is putting massive resources into battlemage and it's likely that the performance uplift for multi gpu training have not been explored but do exist. We see this all the time, changes are hardened in the codebase but underreported in patchnotes because intel moves so fast. \n\nHope my ramblings help. Really awesome build, welcome to Arc and good luck!!",
                  "score": 2,
                  "created_utc": "2026-01-03 06:49:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxd39z4",
                  "author": "b0tbuilder",
                  "text": "You can make it work.",
                  "score": 1,
                  "created_utc": "2026-01-03 01:55:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7isgw",
          "author": "twnznz",
          "text": "I recognise this makes sense for inference but for training we have a huge constraint on bus bandwidth, are you sure you want to train on PCIe setup rather than renting N*H100 from Vast or similar? Does your model/data need absolute security?",
          "score": 16,
          "created_utc": "2026-01-02 05:46:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx99kwm",
              "author": "sparkandstatic",
              "text": "Self hosted can save the most, if it fits within vram.",
              "score": 2,
              "created_utc": "2026-01-02 14:23:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxjwpxo",
                  "author": "Novel-Mechanic3448",
                  "text": "Nothing about this is gonna save money if you consider wasted time expensive",
                  "score": 1,
                  "created_utc": "2026-01-04 02:14:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9g38s",
                  "author": "twnznz",
                  "text": "What Iâ€™m trying to say is: unless electricity is free, it is almost certainly cheaper to train on rented H100",
                  "score": 1,
                  "created_utc": "2026-01-02 14:59:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7gv0f",
          "author": "HyperWinX",
          "text": "Are you going to use Vulkan or what?",
          "score": 13,
          "created_utc": "2026-01-02 05:31:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7fy1n",
          "author": "Fit_West_8253",
          "text": "What model you using? Hardly seen any Intel GPUs used but Iâ€™m very interested in something like the B60",
          "score": 9,
          "created_utc": "2026-01-02 05:24:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx86mr1",
          "author": "jack-in-the-sack",
          "text": "7 gpus on what motherboard?",
          "score": 8,
          "created_utc": "2026-01-02 09:20:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbs3dq",
              "author": "autistic-brother",
              "text": "8",
              "score": 2,
              "created_utc": "2026-01-02 21:39:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7ahl1",
          "author": "Dundell",
          "text": "Big fan of the aaawave open frame. Full size motherboard space with x2 ATX PSUs on both sides. Funny to look at the product details now include \"AI machine learning applications\".\n\nMy rig is x5 rtx 3060 12gb's + x1 P40 24gb all on pcie3.0@4 Lanes with a X99 board. I just run GPT-OSS 120B Q4 with 131k context speeds 42~12 t/s and usually keep it below 90k context maximum for context condensing in roo code.\n\nAlthough I haven't bothered to update llama.cpp and instructions for the gpt-oss 120b since it was released... maybe I could get better performance, but why mess with a good thing.",
          "score": 6,
          "created_utc": "2026-01-02 04:47:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7dkiy",
              "author": "ajw2285",
              "text": "Deets on mobo?",
              "score": 5,
              "created_utc": "2026-01-02 05:07:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7eur9",
                  "author": "Dundell",
                  "text": "Machinist X99-MR9S Motherboard, Intel Xeon E5-2690 v4 CPU, 5x RTX 3060 12GB GPUs, 1x Tesla P40 24GB GPU (all running at PCIe 3.0 x4), 64GB DDR4 2400T RAM (8x8GB sticks), 1x SATA SSD, 1x USB SSD, and a USB WiFi adapter.",
                  "score": 9,
                  "created_utc": "2026-01-02 05:16:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx7ezk4",
              "author": "mp3m4k3r",
              "text": "Even more fun to compile the container and adjust the  cuda version towards the one youre running. Recently did this for the nvidia nemo moe model from a few weeks ago and some of the new optimizations for choosing memory offload for context is pretty great.",
              "score": 3,
              "created_utc": "2026-01-02 05:17:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx7vrje",
              "author": "madsheepPL",
              "text": "donâ€™t take this the wrong way, but whatâ€™s your pp speed at 90k?",
              "score": 1,
              "created_utc": "2026-01-02 07:37:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx9a83q",
                  "author": "Dundell",
                  "text": "I don't think I've ever seen it below 200 t/s for read, although by the time I get near 90k, most of that is already cached in the session. Like 350\\~200 t/s read and 44\\~12 t/s write. Something about OSS 120b versus the mediocre speeds from GLM 4.5 Air and such which was more like 200\\~90 t/s read 18\\~4t/s write.",
                  "score": 1,
                  "created_utc": "2026-01-02 14:26:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxd40gx",
              "author": "b0tbuilder",
              "text": "I can generate tokens faster than that on my gmktec box.  Your PP would probably crush it though.",
              "score": 1,
              "created_utc": "2026-01-03 01:59:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx7pkxr",
              "author": "Business-Weekend-537",
              "text": "Do you have a link to the frame? I have a rig but got an Amazon rando piece of crap frame that doesnâ€™t feel solid and Iâ€™m looking to upgrade.",
              "score": 1,
              "created_utc": "2026-01-02 06:42:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx82dfa",
                  "author": "cantgetthistowork",
                  "text": "This is literally a 12 GPU mining frame that is sold for pennies on AliExpress",
                  "score": 1,
                  "created_utc": "2026-01-02 08:39:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx88fql",
          "author": "armindvd2018",
          "text": "Please update your post and add the hardware u use . Like motherboard  cpu and ....",
          "score": 3,
          "created_utc": "2026-01-02 09:37:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxapnh6",
          "author": "mrinterweb",
          "text": "Please post more about your experience with this rig. The Intel B580 has 12GB VRAM (DDR6)for about $250, which sounds like a pretty good value when combining these cards. I realize there are 128GB systems out there like AMD Ryzen AI Max+ 395 (DDR5), but I doubt its bus speed matches the B580. Guessing inference is significantly faster with the B580. I bet 10 of these cards would smoke the Max+ 395 in inference speed.",
          "score": 3,
          "created_utc": "2026-01-02 18:35:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxb01iv",
              "author": "Fit-Produce420",
              "text": "I get 30-40 tok/s on strix halo (gpt-oss-120b mxfp4)",
              "score": 1,
              "created_utc": "2026-01-02 19:23:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxb87vw",
          "author": "Due-Function-4877",
          "text": "\\+1 on a dev postmortem post later on. \n\nDon't sweat the upvotes or downvotes. A lot of us want to know about the experience with Intel cards right now.",
          "score": 3,
          "created_utc": "2026-01-02 20:02:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7p3bi",
          "author": "robertpro01",
          "text": "Which gpu are those?",
          "score": 2,
          "created_utc": "2026-01-02 06:37:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx83se7",
              "author": "LightShadow",
              "text": "https://www.sparkle.com.tw/en/products/view/6A1A31428cBE",
              "score": 1,
              "created_utc": "2026-01-02 08:53:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxa4ph1",
              "author": "greggh",
              "text": "B580â€™s, these https://www.sparkle.com.tw/en/products/view/6893fe373180",
              "score": 1,
              "created_utc": "2026-01-02 16:57:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx80862",
          "author": "Background_Gene_3128",
          "text": "Is those B60 24gb? \n\nAlso, what mobo are you running? \nIâ€™ve ordered two, but want to expand in the future if the â€œhobbyâ€ catches on, so wanna be somewhat â€œpreparedâ€ to scale if needed.",
          "score": 2,
          "created_utc": "2026-01-02 08:19:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8a1tm",
              "author": "lookwatchlistenplay",
              "text": "\"Whatcha doing, handsome?\"\n\n**\"**Preparing.**\"**",
              "score": 2,
              "created_utc": "2026-01-02 09:53:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxan6lp",
          "author": "Fitzroyah",
          "text": "Awesome! Please keep us updated on the experience. I've been enjoying tinkering on my laptops arc iGpu.",
          "score": 2,
          "created_utc": "2026-01-02 18:23:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7mgzq",
          "author": "Gold_Pen",
          "text": "This is so fascinating seeing this white frame - I bought a black version of this frame from Taobao for only US$20. With a bit of jerry-rigging, I have 4 PSUs and 9 GPUs connected via mainly slimSAS powered risers, with a full fat EEB-sized motherboard. Whole thing weighs about 35kg.",
          "score": 2,
          "created_utc": "2026-01-02 06:16:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7wq1b",
              "author": "michaelsoft__binbows",
              "text": "How the heck do you get a heavy ass item for less than it costs to ship the item",
              "score": 1,
              "created_utc": "2026-01-02 07:46:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7x9k9",
                  "author": "Gold_Pen",
                  "text": "The frame itself is quite light! I also live in HK, so shipping from mainland China down here is dirt cheap.",
                  "score": 4,
                  "created_utc": "2026-01-02 07:51:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx88l9r",
          "author": "lookwatchlistenplay",
          "text": "And God said to Noah...",
          "score": 2,
          "created_utc": "2026-01-02 09:39:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7uumv",
          "author": "ack4",
          "text": "so what's your stack? What are you running here?",
          "score": 1,
          "created_utc": "2026-01-02 07:28:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7yd3x",
          "author": "tired_fella",
          "text": "I never knew Intel would be our savior in the consumer compute crisis.",
          "score": 1,
          "created_utc": "2026-01-02 08:01:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx922zd",
          "author": "aluode",
          "text": "I bet you cant run Crysis on full res.",
          "score": 1,
          "created_utc": "2026-01-02 13:39:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa2j78",
          "author": "Determined-Hedgehog",
          "text": "How efficient are these at inference? I am wondering. I have mainly been running kobold horde local inference only\nIt's a fork of llama.cpp",
          "score": 1,
          "created_utc": "2026-01-02 16:47:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa8w7h",
          "author": "quinn50",
          "text": "Interested in seeing where this goes, I have 2 b50s in my sff box and couldn't get anything usable working.",
          "score": 1,
          "created_utc": "2026-01-02 17:17:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxaas7z",
          "author": "WizardlyBump17",
          "text": "please post benchmarks. I have a b580 and i want to get 2 b60 dual, which will have the same memory as you, but half of the power, but it will still be cool to see the numbers",
          "score": 1,
          "created_utc": "2026-01-02 17:26:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxahl22",
          "author": "Caffdy",
          "text": "what are you planning to train on those?",
          "score": 1,
          "created_utc": "2026-01-02 17:58:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxajbd9",
          "author": "c--b",
          "text": "What supports multi gpu inference anyhow? Unsloth only supports it for a speed boost, not for vram sharing. I wonder if something else does?",
          "score": 1,
          "created_utc": "2026-01-02 18:06:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbahub",
              "author": "hasanismail_",
              "text": "Lm studio is a option",
              "score": 0,
              "created_utc": "2026-01-02 20:13:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxc5zp1",
                  "author": "c--b",
                  "text": "oops, meant training.",
                  "score": 1,
                  "created_utc": "2026-01-02 22:49:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxjwxww",
                  "author": "Novel-Mechanic3448",
                  "text": "No its really not haha ðŸ˜‚",
                  "score": 1,
                  "created_utc": "2026-01-04 02:15:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxbqy99",
          "author": "KooperGuy",
          "text": "You won't be accomplishing much training with these",
          "score": 1,
          "created_utc": "2026-01-02 21:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbzp4f",
              "author": "hasanismail_",
              "text": "Ok and?",
              "score": 0,
              "created_utc": "2026-01-02 22:17:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxc3o19",
                  "author": "KooperGuy",
                  "text": "There is no and",
                  "score": 3,
                  "created_utc": "2026-01-02 22:37:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxbrh9w",
          "author": "autistic-brother",
          "text": "Explain",
          "score": 1,
          "created_utc": "2026-01-02 21:36:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjwmyc",
          "author": "Novel-Mechanic3448",
          "text": "Me when i buy EIGHT gpus for the same price of 6k pro, have no idea what im doing, and think im going to be training with poorly maintained frameworks, just because i have \"vram\"",
          "score": 1,
          "created_utc": "2026-01-04 02:14:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrawdk",
              "author": "hasanismail_",
              "text": "I think you lost the point I can easily afford nice nvidia cards the point of this project is to train and do inference of Intel GPUs as its not very popular/easy right now",
              "score": 1,
              "created_utc": "2026-01-05 04:00:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7leno",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -14,
          "created_utc": "2026-01-02 06:07:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8f65x",
              "author": "synth_mania",
              "text": "do.... you even know what a breadboard is? because it's not that. A breadboard has zero silicon.",
              "score": 4,
              "created_utc": "2026-01-02 10:41:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx9blc5",
              "author": "hasanismail_",
              "text": "I think hes having a episode",
              "score": 1,
              "created_utc": "2026-01-02 14:34:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q5a0if",
      "title": "Liquid Ai released LFM2.5, family of tiny on-device foundation models.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/flk7mfltznbg1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2026-01-06 05:27:54",
      "score": 301,
      "num_comments": 54,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q5a0if/liquid_ai_released_lfm25_family_of_tiny_ondevice/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxynik2",
          "author": "adt",
          "text": "1.2B parameters trained on 28T tokens has a data ratio @ 23,334:1.\n\nEdit: Beaten by Qwen3-0.6B trained on 36T @ 60,000:1.\n\n[https://lifearchitect.ai/models-table/](https://lifearchitect.ai/models-table/)",
          "score": 84,
          "created_utc": "2026-01-06 06:03:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny0qqs3",
              "author": "Aaaaaaaaaeeeee",
              "text": "nah, we know Qwen3 0.6B has 36T. Feels like the chart needs to be rechecked for accuracy.Â ",
              "score": 11,
              "created_utc": "2026-01-06 15:24:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny12wml",
                  "author": "adt",
                  "text": "You're correct.\n\nThe Models Table usually only shows largest model in each family (that's why it has 700 models compared to HF's 300,000 models), so this tiny model was hidden. Added now.\n\nQwen3-0.6B has a data ratio of 60,000:1.",
                  "score": 8,
                  "created_utc": "2026-01-06 16:20:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxysoow",
          "author": "HistorianPotential48",
          "text": "Testing on their site with some prompts we're handling with qwen3 8b. Feels more like a 4B and very fast, but still has the problem of bad at following instructions for special formats - \"Complete one sentence...\" gives 5 sentences instead; \"Create a json like this...\" results in an extra } symbol but otherwise perfect.\n\nAlmost there, probably can be a very fast chat to ask things (RAG knowledge base?), but not smart enough for small practical tasks. Perfect for generating those llm bot tweet replies i guess.   \n  \nThey also have a VL-1.6B. wonder how that's doing",
          "score": 38,
          "created_utc": "2026-01-06 06:46:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxytk92",
              "author": "HistorianPotential48",
              "text": "Ay the VL cooks tho. Can't do OCR, package ingredient texts result in looping, but seems great at image describing.\n\nhttps://preview.redd.it/3k46jrwffobg1.png?width=1722&format=png&auto=webp&s=f7f8aee63c04a42a47f3153a241de8592a8c7309",
              "score": 24,
              "created_utc": "2026-01-06 06:54:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzc6n5",
                  "author": "DrummerHead",
                  "text": "Aaah! I see you're using the classic Indiana-Waifu-Pepsi VL test!\n\n\n\n^(what the fuck à² _à² )",
                  "score": 26,
                  "created_utc": "2026-01-06 09:48:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny0pfr1",
                  "author": "Aaaaaaaaaeeeee",
                  "text": "Â Mmm. there is no can and there is no recognition of Frieren and the mimic",
                  "score": 3,
                  "created_utc": "2026-01-06 15:17:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny2b4wz",
                  "author": "MoffKalast",
                  "text": "It cannot the ðŸ…±ï¸epis.",
                  "score": 2,
                  "created_utc": "2026-01-06 19:39:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxz0i3f",
              "author": "ab2377",
              "text": "maybe can be fine tuned on a json dataset to make it right, if someone can try that.",
              "score": 3,
              "created_utc": "2026-01-06 07:56:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzgpji",
                  "author": "bjodah",
                  "text": "If the schema is known a priori, then I would have guessed that these small (or all?) models would benefit from a some framework that forces syntactically correct json. (e.g. [https://github.com/1rgs/jsonformer](https://github.com/1rgs/jsonformer) ).",
                  "score": 6,
                  "created_utc": "2026-01-06 10:29:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxyyc87",
              "author": "Sixhaunt",
              "text": "Thanks for putting in this work and detailing it for the rest of us. Sounds like it could be very promising for specific tasks if it handles fine-tuning well even if it has a few pitfalls by default in it's more general form.",
              "score": 3,
              "created_utc": "2026-01-06 07:36:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny1fll0",
              "author": "Irisi11111",
              "text": "I think that is an issue for most small models, which are highly benchmarked, so when you use it in a real world case scenario, they are inconsistent. But this is expected, because it's too small and lacking world knowledge, it simply can't recognize what you are saying. \n\nI think that's a problem for most small models. They do well on benchmarks, but in real cases, they're not always consistent. That's kind of expected, though, because they're too small and don't have enough world knowledge. They just can't always understand what you're saying.",
              "score": 1,
              "created_utc": "2026-01-06 17:18:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxyro14",
          "author": "mitchins-au",
          "text": "Utterly amazing. A graph thatâ€™s to scale.",
          "score": 27,
          "created_utc": "2026-01-06 06:37:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny2bc60",
              "author": "MoffKalast",
              "text": "Impossible.",
              "score": 3,
              "created_utc": "2026-01-06 19:40:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxz2f3n",
          "author": "DeltaSqueezer",
          "text": "If it is to be run on-device, I wonder why they don't train for native FP8 or FP4, you don't need batching performance could have more parameters for the same RAM.",
          "score": 16,
          "created_utc": "2026-01-06 08:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzsr54",
              "author": "Karyo_Ten",
              "text": "For small models, quantization has **heavy** impact on output quality.",
              "score": 6,
              "created_utc": "2026-01-06 12:09:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzu7o9",
                  "author": "DeltaSqueezer",
                  "text": "Exactly, so by training a 1B FP16 you force people to run FP16 or severely damage the quality by quantizing to say 4bit. Instead, you could have trained a 4B at 4bit quantization that could be used in the same VRAM and not require further quantization damage.",
                  "score": 18,
                  "created_utc": "2026-01-06 12:19:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxyolcc",
          "author": "Sixhaunt",
          "text": "have any of you tried it out yet to see how accurate the benchmarks are? Looks promising if true",
          "score": 12,
          "created_utc": "2026-01-06 06:12:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyq95e",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 9,
          "created_utc": "2026-01-06 06:25:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyusyj",
              "author": "ismaelgokufox",
              "text": "How do you run these on iOS?",
              "score": 2,
              "created_utc": "2026-01-06 07:04:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxyv3t7",
                  "author": "2str8_njag",
                  "text": "â€œPocketPalâ€ for GGUF, â€œMLX Benchmarksâ€ for MLX formats.",
                  "score": 2,
                  "created_utc": "2026-01-06 07:07:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxzmflc",
          "author": "llama-impersonator",
          "text": "i mean, i like liquid but holy shit make something larger already",
          "score": 15,
          "created_utc": "2026-01-06 11:19:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz8qt2",
          "author": "TechnoByte_",
          "text": "Here are the models, including GGUF: https://huggingface.co/collections/LiquidAI/lfm25",
          "score": 7,
          "created_utc": "2026-01-06 09:15:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny05f5g",
          "author": "ElectronSpiderwort",
          "text": "\"Native audio-language model (speech in/out)\" kind of buried under the fold",
          "score": 7,
          "created_utc": "2026-01-06 13:31:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxypz72",
          "author": "Kahvana",
          "text": "Wish they showed their previous model on there as well.",
          "score": 10,
          "created_utc": "2026-01-06 06:23:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz9xvd",
          "author": "if47",
          "text": "Impressive achievements, but terrible charts.",
          "score": 10,
          "created_utc": "2026-01-06 09:26:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzndeo",
              "author": "-dysangel-",
              "text": "sums up most of AI development in the last few years",
              "score": 8,
              "created_utc": "2026-01-06 11:27:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxzs4i7",
          "author": "__Maximum__",
          "text": "These 1b models are getting smarter than a lot of people i have met, true signs of advancements.",
          "score": 10,
          "created_utc": "2026-01-06 12:04:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1bgte",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 0,
              "created_utc": "2026-01-06 16:59:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny1dmjz",
                  "author": "__Maximum__",
                  "text": "Wow, you proved my point.",
                  "score": 0,
                  "created_utc": "2026-01-06 17:09:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxzxkgd",
          "author": "guiopen",
          "text": "It is the best 1b model I tested by far, the only usable one. Higher speed even compared to models of the same size, and can speak Portuguese making less grammar errors than some bigger 4b models like nanbeige and even qwen3 4b",
          "score": 3,
          "created_utc": "2026-01-06 12:43:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny0ofno",
          "author": "memeposter65",
          "text": "It's crazy good for the size, I love it.",
          "score": 4,
          "created_utc": "2026-01-06 15:12:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny0pg84",
          "author": "meatycowboy",
          "text": "28T tokens for a 1.2B model is crazy.",
          "score": 3,
          "created_utc": "2026-01-06 15:17:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4l3a6",
          "author": "ScoreUnique",
          "text": "I downloaded Q8 on my Pixel 8 with pocket pal, and oh dear I felt like chatting to GPT-4 but locally with 15 tps. \n\nI will test it further - I'll be in a flight this weekend.",
          "score": 4,
          "created_utc": "2026-01-07 02:25:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny06sp7",
          "author": "bakawolf123",
          "text": "Tested locally with MLX on M1Pro and it looks to be comparable to Qwen3-4B but about 2x faster, though there're no <thinking> blocks. Would be interesting what can be done with finetuning it.  \nedit: works lightning fast on a 17pro iPhone too",
          "score": 3,
          "created_utc": "2026-01-06 13:39:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9ff6g",
              "author": "syntaxing2",
              "text": "How are you running this on iOS?",
              "score": 1,
              "created_utc": "2026-01-07 20:03:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxzw6ri",
          "author": "guiopen",
          "text": "Happy to see you guys releasing base models!",
          "score": 2,
          "created_utc": "2026-01-06 12:33:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny04lvo",
          "author": "zelkovamoon",
          "text": "LFM2 was pretty good, so im excited to try this. Really hoping tool calling is better with these models, that was basically my biggest complaint.",
          "score": 2,
          "created_utc": "2026-01-06 13:26:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny15vvc",
          "author": "1_7xr",
          "text": "I wanted to try the vision version on LM Studio but whenever I upload an image, it says the model doesn't support images. Any one with some experience on how to deal with this?",
          "score": 2,
          "created_utc": "2026-01-06 16:33:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny06ukz",
          "author": "steezy13312",
          "text": "This is the *exactly* the kind of model to compliment my [MCP Context Proxy project](https://github.com/samteezy/mcp-context-proxy/). It's not solving anything on its own, but you're using it to offload work from your slower, heavier main model. Downloading now",
          "score": 3,
          "created_utc": "2026-01-06 13:39:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1jvms",
          "author": "sxales",
          "text": "I guess it performed about the same as LFM2 2.6b. I am genuinely in awe of how fast the model is, but it seems largely useless. It failed all my usual tests: grade school math, logical puzzles, and summarization. \n\nSince they only seem to be releasing small models, I wonder if whatever voodoo they use to make prompt processing so fast isn't scaling well.",
          "score": 2,
          "created_utc": "2026-01-06 17:37:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4xl49",
          "author": "tttsang",
          "text": "how to run this model on iPhone? I ran the previous model but it's stuck",
          "score": 1,
          "created_utc": "2026-01-07 03:34:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg82d0",
          "author": "cibernox",
          "text": "I liked their previous models a lot but they were to small and dumb for my use case. I hope they make something bigger but still small soon. Iâ€™m thinking something like a 12B-A4B instruct that can rival qwen3-VL 8B",
          "score": 1,
          "created_utc": "2026-01-08 19:01:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz1vkm",
          "author": "GoranjeWasHere",
          "text": "a great progress for vramlets. Actual usable 1,2b model.\n\n  \nIt's crazy that we beat chat gpt4 with 1,2b model. Not only it is better but also can do ocr and other stuff.",
          "score": 2,
          "created_utc": "2026-01-06 08:09:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz3s3c",
          "author": "iqandjoke",
          "text": "How can I use it on Android? Which one should I use? Edge Gallery only support .litertlm format.",
          "score": 1,
          "created_utc": "2026-01-06 08:27:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz6yl6",
              "author": "jamaalwakamaal",
              "text": "Skip Edge gallery, get PocketPal from GitHub or Play Store.",
              "score": 4,
              "created_utc": "2026-01-06 08:57:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7q7g5",
                  "author": "iqandjoke",
                  "text": "Will try. Thanks!",
                  "score": 1,
                  "created_utc": "2026-01-07 15:31:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny0lroa",
          "author": "Ok-Internal9317",
          "text": "When Ollama?",
          "score": -3,
          "created_utc": "2026-01-06 14:59:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0x8ci",
      "title": "Software FP8 for GPUs without hardware support - 3x speedup on memory-bound operations",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/",
      "author": "Venom1806",
      "created_utc": "2026-01-01 06:03:27",
      "score": 286,
      "num_comments": 57,
      "upvote_ratio": 0.98,
      "text": "Got tired of my RTX 3050 not supporting FP8, so I built a workaround. Packs lower-precision values into FP32 using bitwise operations + Triton kernels.\n\n**Results**: 3x faster on memory-bound operations (GEMV, FlashAttention)\n\nWorks on any GPU - RTX 30/20 series, older cards without native FP8 support. Early stage but functional. Open to feedback.\n\n[Article Link](https://towardsdatascience.com/breaking-the-hardware-barrier-software-fp8-for-older-gpus/) |  [Github Link](https://github.com/SuriyaaMM/feather)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nx1qqyn",
          "author": "lolxdmainkaisemaanlu",
          "text": "Damn I didn't know RTX 3xxx series didn't support FP8? I'm a noob and thought it was supported - coz I've been using fp8 / fp8 scaled models on my RTX 3060 and they do work..?\n\nAmazing work bro, Can I use it rn to accelerate comfyui workloads?",
          "score": 33,
          "created_utc": "2026-01-01 07:22:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1uvtg",
              "author": "john0201",
              "text": "It saves memory but youâ€™re still using 16 bit cores",
              "score": 25,
              "created_utc": "2026-01-01 08:04:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1x3qd",
                  "author": "spaceman_",
                  "text": "16 bit ALUs. You can run 8bit, 16bit, 32bit etc on the same core.\n\n\nThere's no such thing as an 8bit core, but there are dedicated hardware components called ALUs that actually do the math bits and they are operation and operand size specific. In some cases these ALUs are actually shared between cores.\n\n\nThis leads to unintuitive situations on some hardware - for example, on older hardware that was mostly running 32bit float graphics work 16bit workloads sometimes at half speed compared 32bit, despite requiring half the memory bandwidth, because each core had its own 32bit ALUs but 16bit units were shared per pair.\n\nSame thing existed on the CPU side - AMD Bulldozer cores had their own integer ALUs but shared floating point and SIMD hardware between two cores.",
                  "score": 23,
                  "created_utc": "2026-01-01 08:28:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx5ao8h",
                  "author": "phazei",
                  "text": "I'm not sure where the memory saving comes in for existing 3090 fp8 pipelines. In comfy it loads the fp8 model into system memory, and then moves that to vram as fp8 afaik and then upscales to fp16 when it does the calculation. So if I'm running a model such as Zimage which only takes 8 gigs of space, where does this feather come in and help?",
                  "score": 1,
                  "created_utc": "2026-01-01 21:45:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2okno",
              "author": "az226",
              "text": "Basically Volta added FP16, Ampere added BF16, Hopper did FP8, and Blackwell FP4.",
              "score": 13,
              "created_utc": "2026-01-01 13:06:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx1zwnf",
              "author": "CheatCodesOfLife",
              "text": "Yeah, that through me off like a year ago when I was trying to FP8 quants. I think vllm prints a warning about it and it works, but kind of annoying since the 4xxx series got it.",
              "score": 7,
              "created_utc": "2026-01-01 08:58:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxc4nd6",
              "author": "phazei",
              "text": "hijacking top comment to clarify:\n\nFor anyone confused by \"memory-bound\" here, it's not about VRAM capacity. It means the GPU cores are waiting on data to arrive from memory. The bottleneck isn't the math, it's feeding the cores fast enough.\nFP8 is half the bytes of FP16, so it transfers twice as fast from VRAM to the registers where compute actually happens. The clever bit is that Feather does the upcast inside the kernel (in registers, basically free) rather than before it (which would mean a separate VRAM round-trip). That's where the 3x speedup comes from.\n\nI was confused at first since the README made no specific clarification and when I think of a GPU, I basically just think of the VRAM.\n\nEdit:  So, SageAttention I believe takes the fp8 to the register, then quantizes it to int8, does the math, then converts it back.  So it's not doing fp8 math at all, so Feather and SageAttention are incompatible, and the speed of SageAttention is going to be faster since int8 is like 2x fp16 math speeds.  So this can give benefit to stuff that doesn't use SA, but if you already use SA, this provides no benefit.",
              "score": 2,
              "created_utc": "2026-01-02 22:42:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1lmri",
          "author": "Routine_Day8121",
          "text": "This is exactly the kind of lifehack the community needs. FP8 is getting hype everywhere, but hardware adoption is slow. If software workarounds like this are stable, it could extend the life of mid tier GPUs for serious training experiments. Curious to see benchmarks on larger models and mixed workloads though, sometimes GEMV gains do not fully translate.",
          "score": 75,
          "created_utc": "2026-01-01 06:33:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3vbdd",
              "author": "TheThoccnessMonster",
              "text": "Yup - and thereâ€™s plenty of model layers that are heavily convolutional that, even when offloaded to DLA/FP8 they just upcast to FP16 anyway. QAT and dedicated hardware for convolutions and unsupported activation functions stand to get us a lot more bang for our bucks.",
              "score": 8,
              "created_utc": "2026-01-01 17:25:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx1zmpp",
              "author": "CheatCodesOfLife",
              "text": "Lol, what model wrote this, Sonnet?",
              "score": 18,
              "created_utc": "2026-01-01 08:55:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3chhx",
                  "author": "colin_colout",
                  "text": "You're absolutely right to question my identity!",
                  "score": 20,
                  "created_utc": "2026-01-01 15:44:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx3xx5s",
                  "author": "bigfatstinkypoo",
                  "text": "this writing does not stink that bad, it's just corpo positivity speak",
                  "score": 8,
                  "created_utc": "2026-01-01 17:38:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx443q1",
                  "author": "Due-Function-4877",
                  "text": "Us \"boomers\" with a degree write like that. The models are trained on real writing. Next time, I'll make sure to use all lower case and say \"bruh\" a few times for you.",
                  "score": 18,
                  "created_utc": "2026-01-01 18:09:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx211pc",
              "author": "Karyo_Ten",
              "text": ">but hardware adoption is slow.\n\nThat has been supported on 4000 series since a couple of years ago, and it's supported on latest AMD and Intel GPUs AFAIK",
              "score": 7,
              "created_utc": "2026-01-01 09:10:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3uxh2",
                  "author": "Inevitable_Host_1446",
                  "text": "I guess you could see that two ways - hardware adoption as in the hardware is slow to come out, or as in people are slow to get the latest. The latter has certainly been true with what a shitshow GPU prices have remained since the days of crypto boom at least. And now RAM is ridiculous as well and Nvidia are talking about cloud gaming...",
                  "score": 3,
                  "created_utc": "2026-01-01 17:23:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx23beg",
          "author": "gittubaba",
          "text": "Wow, just a few days ago I was arguing about this with chatgpt, it said this isn't possible :P. Can this be plugged into comfyui? \n\nIn my rtx 2060 super, fp8 gets cast to fp16 and bf16 get cast to fp32 when running inference.",
          "score": 11,
          "created_utc": "2026-01-01 09:35:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2uv6j",
              "author": "a_beautiful_rhind",
              "text": "I think it's better to use the triton patch in comfy. https://github.com/woct0rdho/triton-windows/commit/440e3c42a640a4188dd356225e1b13a56b45a377\n\nAlso found it's possible to load BF16/FP16 as E4M3 and then save the vram without an extra file. Somehow my quality went up.\n\nUnfortunately there is some bug in pytorch 2.9 where FP8_scaled gets passed directly into the triton compiler as FP8 and then cast to i8 by llvm. Torch 2.7 works flawless or you can just de-scale the weights.\n\nYou sorta want the calcs in FP16 and you wanna avoid BF16->FP32 conversion if speed is the goal. Int8 calcs can be tried by using sage attention. Not always better.",
              "score": 12,
              "created_utc": "2026-01-01 13:53:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8bsnz",
                  "author": "woct0rdho",
                  "text": "My patch only enables fp8 to fp16 cast in Triton, but it does not replace fp8 matmul in Triton or PyTorch. OP's kernels can directly replace fp8 matmul and that's what we need for the next step.\n\nPyTorch devs seem interested in implementing this, see https://github.com/pytorch/pytorch/issues/167082",
                  "score": 2,
                  "created_utc": "2026-01-02 10:09:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2bsbe",
              "author": "Venom1806",
              "text": "Not sure about comfy UI, but I'm working on implementing functional api for torch.",
              "score": 9,
              "created_utc": "2026-01-01 11:04:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2vuql",
                  "author": "a_beautiful_rhind",
                  "text": "Comfy does torch and FP8/Fp8_scaled is used there much more than for LLMs. IME, on turning FP32 is going to be a slow ride vs FP16.\n\nFor my uses, compiling FP8 image gen weights was a huge speedup. I wonder if somehow your library can hijack FP8 ops to work seamlessly. Right now i'm having to compile triton from source and I doubt quantization/dequantization is accelerated.",
                  "score": 12,
                  "created_utc": "2026-01-01 14:00:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx28jis",
              "author": "getmevodka",
              "text": "LLMs always something isnt real/possible or doable, if it is not part of their training data. Especially the newer LLMs are trained to only do things as efficient and complete as possible, which makes them severly dumber in hypothetical cases than the older LLMs, because they always do only the least amount of work necessary to keep things simple enough and noz make mistakes, as that is a heavy negative reward in their system. Imho its too agressive and the older LLMs like deepseek3.1 or qwen2.5 72b are better suited for hypothetical expectational work or fantasizing about potential ideas, while the newest generation of LLMs will do exceptional work within the scope of their trained abilities.",
              "score": 7,
              "created_utc": "2026-01-01 10:30:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2bxqt",
                  "author": "gittubaba",
                  "text": "What are even saying bro?",
                  "score": 0,
                  "created_utc": "2026-01-01 11:06:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2hy2w",
          "author": "bbjurn",
          "text": "What'd it take to get this to work with vLLM or other inference software?",
          "score": 12,
          "created_utc": "2026-01-01 12:07:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx33f1k",
              "author": "Venom1806",
              "text": "Idk, anything that uses torch.Tensor or is convertible to this format should work. Probably huggingface will work ig.",
              "score": 7,
              "created_utc": "2026-01-01 14:51:13",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx3dm9j",
              "author": "elsung",
              "text": "Yeaaaa! I was just trying to get vLLM to load nemotron3-nano on my 2x 3090s but couldnâ€™t get it working because FP8 isnâ€™t supported (and theres no AWQ quant). Gotta be honest tho not sure how i would implement this in vLLM to get things working. Might need to vibe code this to see about implementing the solution lol",
              "score": 6,
              "created_utc": "2026-01-01 15:50:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx5uuyw",
                  "author": "rainbyte",
                  "text": "There is GPTQ quant, do you know if is it good?",
                  "score": 1,
                  "created_utc": "2026-01-01 23:33:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx223fk",
          "author": "ab2377",
          "text": "wow ðŸ˜³ ðŸ‘",
          "score": 4,
          "created_utc": "2026-01-01 09:21:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3sn2i",
          "author": "KingKoro",
          "text": "Would this also benefit RDNA3 ?",
          "score": 3,
          "created_utc": "2026-01-01 17:11:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4sqlo",
              "author": "tw_numba_one",
              "text": "I believe so. If your environment has PyTorch support, it should work.",
              "score": 2,
              "created_utc": "2026-01-01 20:11:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4crri",
          "author": "ethertype",
          "text": "Is this conceptually the same trick pytorch uses to handle MXFP4 on Ampere-class hardware? Which does not support MXFP4 natively.\n\n[heretic](https://github.com/p-e-w/heretic) will do its magic on the original gpt-oss-20b safetensor in MXFP4 format. (The end result is 3x the original size, though.) I have been told heretic doesn't do anything in the code for this to occur, so I assume pytorch owns all the glory.\n\nI also can perfectly fine load the native MXFP4 ggufs of gpt-oss-120b (converted by GG) on my 3090s, with llama.cpp. 120 t/s on empty context. Can't say if this is due to pytorch or if llama.cpp special-cases this on its own.",
          "score": 3,
          "created_utc": "2026-01-01 18:51:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3m8e6",
          "author": "tynej",
          "text": "Very nice work. Could we use similiar trick for   hopper architecture to support speed of fp4?",
          "score": 2,
          "created_utc": "2026-01-01 16:37:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3p1zr",
              "author": "Venom1806",
              "text": "We could just use 8 fp4 instead of 4 fp8, we dont need an hopper.",
              "score": 3,
              "created_utc": "2026-01-01 16:52:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2a9cx",
          "author": "FastDecode1",
          "text": ">Works on any GPU\n\n>Runs E5M2 and E4M3 on any CUDA GPU (RTX 20/30 series supported).\n\nPick one.",
          "score": 7,
          "created_utc": "2026-01-01 10:48:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2bvnq",
              "author": "Venom1806",
              "text": "Sorry. Should work on RTX 20/30, there's no advantage in using with 40.",
              "score": 19,
              "created_utc": "2026-01-01 11:05:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2pezw",
                  "author": "az226",
                  "text": "Does it work for V100? Training too or just inference?",
                  "score": 2,
                  "created_utc": "2026-01-01 13:13:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6y7iv",
          "author": "batonac",
          "text": "Could this be useful for increasing LLM performance on the Tesla P40?",
          "score": 1,
          "created_utc": "2026-01-02 03:27:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7w5oy",
              "author": "johndeuff",
              "text": "Interested. Got p40 too.",
              "score": 1,
              "created_utc": "2026-01-02 07:41:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0uuqt",
      "title": "Happy New Year: Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning - Fine Tune. (based on recent find of L3.3 8b in the wild)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/",
      "author": "Dangerous_Fix_5526",
      "created_utc": "2026-01-01 03:41:30",
      "score": 284,
      "num_comments": 80,
      "upvote_ratio": 0.94,
      "text": "(link to Heretic/Uncensored version just added)\n\n**Special thanks to :**\n\n[jacek2023](https://www.reddit.com/user/jacek2023/) \\[posting about this model\\]\n\nand extra special thanks for \"**allura-forge** \" for finding this model:\n\n[https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct](https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct)\n\n( For an incredible find of Llama 3.3 8B \"in the wild\" !!)\n\nI fine tuned it using Unsloth and Claude 4.5 Opus High Reasoning Dataset:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning)\n\nThis has created a reasoning/instruct hybrid.  \nDetails at the repo, along with credits and links.\n\n**ADDED:**  \n\\- 1 example generation at repo  \n\\- special instructions on how to control \"instruct\" or \"thinking\" modes.\n\nGGUF quants are now available.\n\n**ADDED 2:**\n\nClarification:\n\nThis training/fine tune was to assess/test if this dataset would work on this model, and also work on a non-reasoning model and induce reasoning (specifically Claude type - which has a specific fingerprint) WITHOUT \"system prompt help\".\n\nIn other-words, the reasoning works with the model's root training/domain/information/knowledge.\n\nThis model requires more extensive updates / training to bring it up to date and up to \"spec\" with current gen models.\n\n**PS:**  \nWorking on a Heretic (\"uncensored\") tune of this next.\n\nHeretic / Uncensored version is here:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Heretic-Uncensored-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Heretic-Uncensored-Claude-4.5-Opus-High-Reasoning)\n\n(basic benchmarks posted for Heretic Version)\n\nDavidAU",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nx1aef2",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-01 04:55:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1bnaj",
          "author": "30299578815310",
          "text": "Thanks for sharing this! Am I reading is correctly that you had 250 rows in the fine-tuning data set? Is that enough to get good results?",
          "score": 44,
          "created_utc": "2026-01-01 05:06:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1qtjk",
              "author": "Dangerous_Fix_5526",
              "text": "Correct. A quality, compact dataset can make all the difference. Special thanks to TeichAI for their hard work in putting together this top notch dataset.\n\n[https://huggingface.co/datasets/TeichAI/claude-4.5-opus-high-reasoning-250x](https://huggingface.co/datasets/TeichAI/claude-4.5-opus-high-reasoning-250x)\n\nPS: They have done a lot of these kinds of datasets, so show them some love.\"  \n  \nI used 10 of these (models/datasets by TeichAI) to build a 12X programmable MOE (all top closed and open distills) here:\n\nHeretic version:  \n[https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-Distill-12X-Closed-Open-Heretic-Uncensored-GGUF](https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-Distill-12X-Closed-Open-Heretic-Uncensored-GGUF)\n\n\"Reg\" Version:  \n[https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-GATED-12x-Closed-Open-Source-Distill-GGUF](https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-GATED-12x-Closed-Open-Source-Distill-GGUF)",
              "score": 33,
              "created_utc": "2026-01-01 07:22:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2c8r8",
                  "author": "-p-e-w-",
                  "text": "Note that when combining Heretic with fine-tuning, you should always run Heretic first, and *then* do training, not the other way round. That way, the training run might heal some of the damage from ablation (though to be fair, for the Llama 3 series that damage tends to be very minor).",
                  "score": 13,
                  "created_utc": "2026-01-01 11:09:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx3cwu9",
                  "author": "IrisColt",
                  "text": "Thanks!, the Heretic version is like day and night.",
                  "score": 2,
                  "created_utc": "2026-01-01 15:47:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx28hez",
          "author": "DecodeBytes",
          "text": "I might be missing something, but 200 samples won't be enough to teach an 8B instruct model to reason - though it can work for very specific, constrained tasks, less likely to be widely populated in the original pretraining.\n\nReasoning ability is largely baked into the base model during pretraining. I'm assuming you used LoRA, which is great for steering how that existing ability gets applied, but it won't teach new reasoning capabilities from scratch. Even with 50k+ samples, LoRA mostly reshapes how the model uses reasoning it already has rather than building new circuits - must successful efforts use 100k-500k+ high-quality samples. Either way, you're working within the constraints of what the base model learned during pretraining unfortunately.\n\nKeep going though, its all a learning experience and the more folks there are making tunes the better!",
          "score": 11,
          "created_utc": "2026-01-01 10:30:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2d39j",
              "author": "Dangerous_Fix_5526",
              "text": "These are high quality reasoning traces.\n\nNormally I would agree with you - but it works.  \nAlso works very well with Qwens3 - 4B, 8B and 14B.\n\nFrankly that it works speaks volumes for the high quality dataset from TeichAI.  \nThere is a reason this dataset has 112 likes.\n\nLikewise the reasoning traces/formatting appears the same way as in the Qwen3 tunes using the same dataset.\n\nADDED:  \nWith this model, reasoning activates based on keywords/phrases in the prompt.  \n(see repo)  \n  \nIt is not \"always on\" like a \"locked\" thinking model so to speak.",
              "score": 2,
              "created_utc": "2026-01-01 11:18:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2n948",
                  "author": "DecodeBytes",
                  "text": "\n\n\\> With this model, reasoning activates based on keywords/phrases in the prompt.  \n(see repo)\n\nRight, its likely the model is just doing as \\*\\*instruct\\*\\*ed in the prompt and its not activated learned reasoning, but its really hard to tell as I can't find where anything is in this tread, help me out please? link the model, notebook and anything else?",
                  "score": 8,
                  "created_utc": "2026-01-01 12:55:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx2m5z9",
                  "author": "DecodeBytes",
                  "text": "Do you have any benchmarks I could look at and can you share your training notebook, I would love to take a look?\n\nIs this the tuned model? [https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct](https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct)",
                  "score": 3,
                  "created_utc": "2026-01-01 12:46:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4p57r",
                  "author": "Far-Low-4705",
                  "text": "just because it's \"high quality data\" doesn't mean for a second that you can get away with any less.\n\nits a core theory in ML, not just LLMs specifically, you need a large enough sample size to represent the broader population, ie, all cases of reasoning. you'd need 500k+ examples for anything remotely accurate. and even then, as the user above said, a lora adaptor is not really ideal here.   \n  \nyou need your data set to cover a few examples from every possible scenario. 200 is no where near enough, even if they were \"perfect\" traces.\n\nThat being said, you *might* still see **marginal** performance gains, but you'd still be leaving **a lot** on the table, and you haven't verified any gains at all because you didnt benchmark its performance. I would like to see performance benchmarks in order to believe you, and even then, you'd be leaving A LOT of performance on the table.",
                  "score": 6,
                  "created_utc": "2026-01-01 19:53:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1luhd",
          "author": "dash_bro",
          "text": "Brilliant. Thank you! \n\nIs there a community fine-tune with the same dataset for qwen3-14B? I think that would help with the wild reasoning goose-chases it sometimes goes down under",
          "score": 7,
          "created_utc": "2026-01-01 06:35:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1r0dc",
              "author": "Dangerous_Fix_5526",
              "text": "Yes ; see this repo:\n\n[https://huggingface.co/TeichAI](https://huggingface.co/TeichAI)\n\n(they have 4B,8B and 14B ; I have used some of their 4Bs in MOES)",
              "score": 5,
              "created_utc": "2026-01-01 07:24:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2snoz",
          "author": "30299578815310",
          "text": "Ate there any benchmarks for this?",
          "score": 8,
          "created_utc": "2026-01-01 13:37:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx616uu",
              "author": "Dangerous_Fix_5526",
              "text": "There are benches for the root / base version as found by allure.",
              "score": 1,
              "created_utc": "2026-01-02 00:09:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx6b07r",
                  "author": "30299578815310",
                  "text": "In that case are bubbles not very effective against Legion since it seems like they use less plasma than the other factions?",
                  "score": 1,
                  "created_utc": "2026-01-02 01:04:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1axnf",
          "author": "sunshinecheung",
          "text": "wow, i hope there is a GGUF version",
          "score": 10,
          "created_utc": "2026-01-01 04:59:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx23iku",
              "author": "Dangerous_Fix_5526",
              "text": "A few ggufs are up ; team Mradermacher is doing some right now too.\n\nUPDATE:  \nQuants are up - all , including Imatrix.",
              "score": 10,
              "created_utc": "2026-01-01 09:37:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1pplo",
          "author": "Own-Potential-2308",
          "text": "I never tried any Claude reasoning models lol",
          "score": 5,
          "created_utc": "2026-01-01 07:11:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx11ny4",
          "author": "txgsync",
          "text": "That's pretty cool. Getting easier to train models every day! Interested in trying your fine tune.",
          "score": 14,
          "created_utc": "2026-01-01 03:50:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2nws1",
          "author": "Single_Ring4886",
          "text": "It is very nice but some \"tests\" are really needed...",
          "score": 4,
          "created_utc": "2026-01-01 13:00:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2lunp",
          "author": "And-Bee",
          "text": "Tried to use this with Roo code and it produced garbage",
          "score": 4,
          "created_utc": "2026-01-01 12:43:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4nn2c",
              "author": "Cool-Chemical-5629",
              "text": "Using old Llama 8B model which was never meant to be good at coding, finetuned with 250 rows of \"high quality\" thinking traces from Claude model of... who knows what categories... What could go wrong? ðŸ˜‚",
              "score": 6,
              "created_utc": "2026-01-01 19:45:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx23ypd",
          "author": "Professional-Coat968",
          "text": "Sound interesting to try. Do you think we can finetune a good enough for only a specific code base like this ? ðŸ˜",
          "score": 2,
          "created_utc": "2026-01-01 09:42:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx253ut",
              "author": "Dangerous_Fix_5526",
              "text": "Yes ; Llamas are very easy to tune. That being said, I was surprised how well this tune using a distill dataset came out. \n\nFrankly, this could have used a bit more training - but I did not want to overcook it.",
              "score": 2,
              "created_utc": "2026-01-01 09:54:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2u00p",
          "author": "rekriux",
          "text": "Hi u/Dangerous_Fix_5526,  \nshamelessly asking if it where possible to make your 20X-40X models (or similar) as recurrent loop models (with or without lora) ?  \nYour models are hidden gems, but the additional NVRAM/RAM is hard on HW limits for larger models (btw I run vllm).\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1q0vom4/comment/nx2q3ca/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1q0vom4/comment/nx2q3ca/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)\n\nAlso, will you start working with linear models ? Kimi Linear REAP, Falcon H, Nemotron 3 ?   \nP.S. Nemotron license is restrictive, and the model has ingrained censoring/alignment (made a post that was removed on it)  \n  \n\\+1 for this one, will definitively try it !",
          "score": 2,
          "created_utc": "2026-01-01 13:47:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx62b8j",
              "author": "Dangerous_Fix_5526",
              "text": "Nemotron is in the \"works\" ; as well as Kimi V2 ; using distill dataset(s).\n\nRE: 20/40x ;   \nThe brainstorm adapter works on almost all model types, archs and sizes ; with 20x the most stable.  \n40x is used for creative purposes and/or people that want models a ... little more out there.",
              "score": 3,
              "created_utc": "2026-01-02 00:15:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4v8a8",
          "author": "Standard-Savings-224",
          "text": "Nice work on the fine tune! That Claude reasoning dataset combo sounds promising - curious how the thinking mode performs compared to base 3.3. The uncensored version is gonna be interesting too",
          "score": 2,
          "created_utc": "2026-01-01 20:24:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx11ren",
          "author": "LoveMind_AI",
          "text": "Fantastic work.",
          "score": 6,
          "created_utc": "2026-01-01 03:51:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1sc8l",
          "author": "jacek2023",
          "text": "Hello, it wasn't me, I only posted the news here :)\n\nPlease credit allura",
          "score": 5,
          "created_utc": "2026-01-01 07:38:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1ssr9",
              "author": "Dangerous_Fix_5526",
              "text": "Done ; thanks for heads up.  \nallura was credited at repo W links to reddit posts too.  \nThank you for posting about this model!",
              "score": 2,
              "created_utc": "2026-01-01 07:42:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx12zmh",
          "author": "Borkato",
          "text": "How good is it? ðŸ‘€",
          "score": 2,
          "created_utc": "2026-01-01 03:59:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx13ekl",
              "author": "Dangerous_Fix_5526",
              "text": "I used this test prompt, with Q4KS:\n\nExplain orbital mechanics including detailed math and examples.\n\nModel produced excellent thinking block ( very detailed, but on point) , then examples / \"math\" and without be prompted - multiple python scripts to visually illustrate all concepts.",
              "score": 10,
              "created_utc": "2026-01-01 04:02:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx3o2oz",
                  "author": "noneabove1182",
                  "text": "But the answer it gave is quite terrible, it just hallucinated a bunch of nice looking stuff",
                  "score": 6,
                  "created_utc": "2026-01-01 16:46:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx43t93",
                  "author": "LetterRip",
                  "text": "I had copilot evaluate the answer,\n\n\"The explanation tries to sound comprehensive, but itâ€™s riddled with problems: several equations are outright incorrect or dimensionally impossible, key orbitalâ€‘mechanics concepts like true anomaly and eccentric anomaly are misused or confused, and some â€œproofsâ€ of Keplerâ€™s laws are not actually proofs but loosely connected statements that donâ€™t follow mathematically. The document also repeats content, includes placeholder code blocks with no real implementation, and mixes accurate fundamentals with fabricated formulas, making it unreliable despite its confident tone.\"",
                  "score": 4,
                  "created_utc": "2026-01-01 18:07:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx14z8l",
                  "author": "Borkato",
                  "text": "Thatâ€™s quite interesting!",
                  "score": 3,
                  "created_utc": "2026-01-01 04:13:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2u6on",
          "author": "tmvr",
          "text": "I've asked it for a simple Ansible fleet management setup with a few tasks on the client which it did fine. Then I've I've told it to add disabling reboot for non-privileged users and instead of adding a task it went bonkers. Added some Project Timeline, Implementation Roadmap, Risk Assessment, RIsk Mitigation sections etc. added long Python scripts for some Audit Framework and also for Compliance Checks Validation and a bunch or other stuff and ended stuck at this which was obviously never going to work:\n\nhttps://preview.redd.it/555354kusqag1.png?width=296&format=png&auto=webp&s=2fda1b00329e983e8abfa8e94ca1652588fa8308",
          "score": 1,
          "created_utc": "2026-01-01 13:48:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx62fo7",
              "author": "Dangerous_Fix_5526",
              "text": "Censorship in the root model is STRONG. (same for all Llamas).  \nHeretic version should change that.",
              "score": 1,
              "created_utc": "2026-01-02 00:16:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx43nc9",
          "author": "LetterRip",
          "text": "Note that I had Copilot evaluate the answer to the prompt, here is a critical evaluation sum-up.  \n  \n\"The explanation tries to sound comprehensive, but itâ€™s riddled with problems: several equations are outright incorrect or dimensionally impossible, key orbitalâ€‘mechanics concepts like true anomaly and eccentric anomaly are misused or confused, and some â€œproofsâ€ of Keplerâ€™s laws are not actually proofs but loosely connected statements that donâ€™t follow mathematically. The document also repeats content, includes placeholder code blocks with no real implementation, and mixes accurate fundamentals with fabricated formulas, making it unreliable despite its confident tone.\"",
          "score": 1,
          "created_utc": "2026-01-01 18:07:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4pdf2",
          "author": "Far-Low-4705",
          "text": "do you have any kind of model performance benchmarks compared to the base model?\n\nThis is absolutely critical to prove you did anything meaningful",
          "score": 1,
          "created_utc": "2026-01-01 19:54:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx62p22",
              "author": "Dangerous_Fix_5526",
              "text": "This was a test case to assess if the dataset would work on this Llama, and also a non-reasoning model to boot. Model requires more extensive updates/training to bring it up to date, and \"spec\" with current gen models.",
              "score": 1,
              "created_utc": "2026-01-02 00:17:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5qrmp",
          "author": "couscous_sun",
          "text": "I didn't know we can actually get the reasoning trace from Anthropic models? What the heeeeeck??!?!",
          "score": 1,
          "created_utc": "2026-01-01 23:10:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx68bcp",
          "author": "yoracale",
          "text": "Congrats this is awesome!",
          "score": 1,
          "created_utc": "2026-01-02 00:49:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1zjey",
          "author": "Forsaken_Mistake8315",
          "text": "Anybody running these on MBP M3/M4 max 64gb? If yes, may I ask at what speeds?\n\nI'm wondering if I should get M4 Max 64 gb and that's enough or M3 128gb (if I ever need bigger models)",
          "score": 1,
          "created_utc": "2026-01-01 08:54:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx26jr1",
              "author": "texasdude11",
              "text": "M3 128 over m4 64.",
              "score": 1,
              "created_utc": "2026-01-01 10:09:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2jiu9",
                  "author": "Forsaken_Mistake8315",
                  "text": "Many thanks for advice. And if I can get MBP m2 max 96gb is it still Worth it over M4 max  64gb? I guess Yes since it's got a lot of bandwidth?",
                  "score": 2,
                  "created_utc": "2026-01-01 12:22:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx45xrb",
              "author": "And-Bee",
              "text": "I ran this on my Mac and it produced non human readable garbage.",
              "score": 1,
              "created_utc": "2026-01-01 18:18:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4a9ny",
                  "author": "Forsaken_Mistake8315",
                  "text": "Thanks I will not even bother DL then.",
                  "score": 1,
                  "created_utc": "2026-01-01 18:39:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx61x14",
                  "author": "Dangerous_Fix_5526",
                  "text": "Tested in Lmstudio, with settings at repo using quant q4ks.  \nMLX quants were not tested.",
                  "score": 1,
                  "created_utc": "2026-01-02 00:13:14",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1x0xk",
          "author": "dtdisapointingresult",
          "text": "Call me a hater but I will always downvote and ignore random community finetunes.\n\nI kinda, sorta tolerate the ones from bigger teams like NousHermes if they show they put some effort into them including benchmark comparisons (but still won't use them).\n\nDownvotes to the left.",
          "score": -7,
          "created_utc": "2026-01-01 08:27:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2fu5v",
              "author": "usernameplshere",
              "text": "Wtf, I'm the exact opposite. There's someone in our community with dedication and knowledge who puts his time and money (for compute, data collection) in and uploads the result for free for everyone to try. Even if it's somehow worse than the base model, it's still cool to see people actually being interested and trying to improve something already existing. I'll always upvote stuff like this.",
              "score": 4,
              "created_utc": "2026-01-01 11:46:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx1xxxl",
              "author": "MaybeIWasTheBot",
              "text": "having an objectively bad take, knowing it's an objectively bad take, and then ending off with 'downvotes to the left' is so cheesy",
              "score": 9,
              "created_utc": "2026-01-01 08:37:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx22qm6",
                  "author": "dtdisapointingresult",
                  "text": "People don't need to share every random finetune/merge they do. People treat HF the way teen girls treat Instagram. A pointless model takes the same diskspace and electricity/bandwidth as a SOTA model from a big lab.\n\nNo wonder HF restricted storage on free accounts.",
                  "score": -5,
                  "created_utc": "2026-01-01 09:28:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx23fnu",
              "author": "Dangerous_Fix_5526",
              "text": "There is nothing \"random\" about this fine tune.",
              "score": 3,
              "created_utc": "2026-01-01 09:36:23",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx2xxdq",
              "author": "LaCipe",
              "text": "Ye no, I am with you on this...dataset seems weird by being so small",
              "score": 1,
              "created_utc": "2026-01-01 14:14:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1hwvt",
          "author": "Beneficial-Good660",
          "text": "Meta has really decided to latch onto the holiday with a two-year-old model.ðŸ¤” spam spam",
          "score": -22,
          "created_utc": "2026-01-01 05:59:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxss0m",
      "title": "Senator in Tennessee introduces bill to felonize making AI \"act as a companion\" or \"mirror human interactions\"",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/",
      "author": "CanineAssBandit",
      "created_utc": "2025-12-28 14:35:58",
      "score": 272,
      "num_comments": 212,
      "upvote_ratio": 0.9,
      "text": "Call (202) 224-3121 for the Capitol switchboard to contact your representative. Tell them you oppose anything similar.\n\nThe bill:  \n[https://legiscan.com/TN/bill/SB1493/2025](https://legiscan.com/TN/bill/SB1493/2025)\n\nQuotes from the bill (emphasis mine):\n\nIt is an offense for a person to knowingly train artificial intelligence to:  \n(3) Provide emotional support, **including through open-ended conversations** with a user;  \n(4) Develop an emotional relationship with, or otherwise **act as a companion** to, an individual;  \n(6) Otherwise act as a sentient human or **mirror interactions that a human user might have with another human user**, such that an individual would feel that the individual could develop a friendship or other relationship with the artificial intelligence;  \n(8) **Simulate a human being**, including in appearance, voice, or other mannerisms.\n\n\"Train\":  \n(A) Means utilizing sets of data and other information to teach an artificial intelligence system to perceive, interpret, and learn from data, such that the A.I. will later be capable of **making decisions based on information or other inputs** provided to the A.I.  \n(B) Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwfq6z4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-28 21:55:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdbvj0",
          "author": "some_user_2021",
          "text": "No Waifu for you!",
          "score": 155,
          "created_utc": "2025-12-28 14:49:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwen2w8",
              "author": "Mikasa0xdev",
              "text": "Tennessee is banning AI girlfriends, lol.",
              "score": 48,
              "created_utc": "2025-12-28 18:46:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nweysds",
                  "author": "Amazing_Athlete_2265",
                  "text": "Sounds like they've already banned critical thinking",
                  "score": 26,
                  "created_utc": "2025-12-28 19:41:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj76ev",
              "author": "Dr_Allcome",
              "text": "The \"simulate a human being\" part would prevent any AI chat bot, like customer support... i kinda want to see this go through just for the absolute shitshow it would cause.\n\nIf bezos can use the delivery drones to dronestrike someone we'd find out pretty soon.",
              "score": 16,
              "created_utc": "2025-12-29 12:10:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjffk4",
                  "author": "SilentLennie",
                  "text": "Also have you seen how many videos on Youtube are AI-generated videos of some what famous (in their field) people ?",
                  "score": 4,
                  "created_utc": "2025-12-29 13:10:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4e4yc",
                  "author": "uhuge",
                  "text": "It say to not mimic a specific real existing person.Â Â ",
                  "score": 1,
                  "created_utc": "2026-01-01 18:58:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdfzqp",
          "author": "JEs4",
          "text": "Iâ€™d be shocked if this goes anywhere. This seems to stem from Becky Masseyâ€™s fairly unique background and circumstances. Not only does it conflict with precedent on freedom of speech within the context of software development, it is completely at odds with the current directives of the federal government.\n\nThat said, Tennessee folks, please call!",
          "score": 118,
          "created_utc": "2025-12-28 15:12:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe419i",
              "author": "changing_who_i_am",
              "text": ">This seems to stem from Becky Masseyâ€™s fairly unique background and circumstances.\n\nCan you clarify on this? Wiki doesn't bring anything interesting up (unless I've missed it)",
              "score": 21,
              "created_utc": "2025-12-28 17:15:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwewc9r",
                  "author": "JEs4",
                  "text": "It isnâ€™t anything particularly interesting, just that sheâ€™s a boomer married to a retired software engineer, who was a former executive director at Sertoma Center which is a housing facility for intellectually disabled people, and was on several boards related to healthcare, and one explicitly for mental healthcare. Not an atypical background for a regular person but not common in conservative politicians now. \n\nBasically I think she is someone who knows about the vulnerability people have, and sheâ€™s been told enough about generative AI which coupled with the OpenAI suicide stories, to lead to this. \n\nItâ€™s an absurd way to approach the issue but I donâ€™t think itâ€™s nefarious beyond her personal background and likely wonâ€™t spread.",
                  "score": 44,
                  "created_utc": "2025-12-28 19:29:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdxahh",
              "author": "CanineAssBandit",
              "text": "You can call your own rep to tell them you do not support any similar laws in your state as well. I did this recently for something else, it was weirdly chill and easy. You just get their secretary and they note it and that's it.",
              "score": 21,
              "created_utc": "2025-12-28 16:41:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwfozjp",
                  "author": "DorphinPack",
                  "text": "I mean they also got threatened by the President to not regulate so Iâ€™d imagine theyâ€™re relieved hearing from you. Your opinion may feel like the minority opinion given the fervor but by the dollar itâ€™s not a shock.",
                  "score": 3,
                  "created_utc": "2025-12-28 21:49:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf8p86",
                  "author": "shifty21",
                  "text": "You do realize that this bill is for the  STATE of Tennessee... not the US Senate.  The phone number you listed is for the US Senate and Sen. Massey is NOT in the US Senate, but the Tenn. Senate.",
                  "score": 6,
                  "created_utc": "2025-12-28 20:29:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwfmp00",
                  "author": "AfternoonOk3344",
                  "text": "\"and that's it\" pretty much sums it up, I think, because that information goes nowhere. The secretary you spoke to is most likely a hotline of minimum wage workers paid by tax dollars to field phone calls all day so people feel like they have a voice.\n\nAt the end of the day the only people politicians are going to side with are the folks lining their pockets, and I don't mean with the tax dollars they're probably already stealing.",
                  "score": 2,
                  "created_utc": "2025-12-28 21:38:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgmd68",
              "author": "AnAbandonedAstronaut",
              "text": "Its also harder to control someone with a support system, even if the support system is AI.\n\nNext will be a law that AI cant speak on sexual or gender issues.\n\nLike if you ask it about trans people it will say \"trans is a shortening of transmission, such as in a car\" or \"gay means happy.. happy people often have a home made up of a mother and father.\"",
              "score": 3,
              "created_utc": "2025-12-29 00:43:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdgj9c",
          "author": "Aggravating-Age-1858",
          "text": "lol\n\nnow thats just stupid",
          "score": 88,
          "created_utc": "2025-12-28 15:15:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe0alc",
              "author": "iamthewhatt",
              "text": "Republicans only ever introduce bills that are so vague that it can allow for incredibly dumb exceptions in order to protect republicans. This is not new lol",
              "score": 38,
              "created_utc": "2025-12-28 16:56:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwecyvf",
                  "author": "BlipOnNobodysRadar",
                  "text": "Politicians\\*\n\nBoth parties do it.",
                  "score": 11,
                  "created_utc": "2025-12-28 17:59:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgtn0m",
              "author": "Prudent_Jelly9390",
              "text": "dinosaurs",
              "score": 1,
              "created_utc": "2025-12-29 01:25:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdht6h",
          "author": "Nomski88",
          "text": "How about we pass a bill making it a felony to accept any sort of lobbying...",
          "score": 131,
          "created_utc": "2025-12-28 15:22:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgklni",
              "author": "Awkward-Nothing-7365",
              "text": "Don't be anti-semitic.",
              "score": 18,
              "created_utc": "2025-12-29 00:34:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgnkh3",
                  "author": "Nomski88",
                  "text": "lmao",
                  "score": 12,
                  "created_utc": "2025-12-29 00:50:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdn529",
              "author": "Environmental-Metal9",
              "text": "Ah no, we canâ€™t do that because thatâ€™s anti-American, donâ€™t you know?",
              "score": 39,
              "created_utc": "2025-12-28 15:50:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwfkdhp",
              "author": "MoneyPowerNexis",
              "text": "https://i.imgflip.com/6xz8j5.jpg",
              "score": 3,
              "created_utc": "2025-12-28 21:26:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdds4j",
          "author": "flybot66",
          "text": "He's really going to freak when AI starts taking confessions... \n\nhttps://preview.redd.it/y5xkzfk0my9g1.png?width=758&format=png&auto=webp&s=ea32c4600459f1577f8987f4695b27a71dec10f8",
          "score": 28,
          "created_utc": "2025-12-28 15:00:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdx4a1",
              "author": "squirrelscrush",
              "text": "Pretty sure that's not covered under the sacrament of confession",
              "score": 10,
              "created_utc": "2025-12-28 16:41:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe8cgv",
                  "author": "FaceDeer",
                  "text": "Who decides that?",
                  "score": 13,
                  "created_utc": "2025-12-28 17:36:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwez9j2",
                  "author": "Amazing_Athlete_2265",
                  "text": "Meh, close enough",
                  "score": 1,
                  "created_utc": "2025-12-28 19:43:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwf4b0v",
          "author": "Django_McFly",
          "text": "That's an insane bill.  Wouldn't this basically ban any chat based interface?\n\n> mirror interactions that a human user might have with another human user\n\nthat [edit: only leaves] like code generation and being a better menu/interface",
          "score": 10,
          "created_utc": "2025-12-28 20:07:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4g0jj",
              "author": "uhuge",
              "text": "You'd just tune the personality to a more robotic one as in understanding but less empathetic.",
              "score": 1,
              "created_utc": "2026-01-01 19:07:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwg0syf",
          "author": "Novel-Mechanic3448",
          "text": "Lmao, extroverts will do anything but leave introverts alone",
          "score": 9,
          "created_utc": "2025-12-28 22:49:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwibt9f",
              "author": "Ill-Bison-3941",
              "text": "Thank you for this comment ðŸ˜‚ðŸ’– As a fellow introvert, I fully agree.",
              "score": 4,
              "created_utc": "2025-12-29 07:25:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx9hjit",
                  "author": "Interesting-Gift-178",
                  "text": "Same! ðŸ¤­",
                  "score": 2,
                  "created_utc": "2026-01-02 15:06:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwhm9ek",
          "author": "Professional_Gas3276",
          "text": "This is absolutely unhinged lmao. So basically any chatbot that can hold a conversation would be a felony? Even customer service bots that try to sound friendly could technically fall under \"mirror human interactions\"\n\n  \nThe definition of \"train\" is so broad it would criminalize like half of modern AI development. Good luck enforcing this when most LLMs are trained outside Tennessee anyway",
          "score": 10,
          "created_utc": "2025-12-29 04:12:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiac51",
              "author": "tifa_cloud0",
              "text": "right. i mean it is impossible to make this law possible except if popular services like google or meta do it and then people complain it, then and then only they could be held accountable. ainâ€™t no one going to waste time to make this fictional law into a reality.",
              "score": 0,
              "created_utc": "2025-12-29 07:12:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdl0td",
          "author": "lordpuddingcup",
          "text": "Didnâ€™t Trump sign an EO banning states from from implementing limitations on ai",
          "score": 36,
          "created_utc": "2025-12-28 15:39:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdz9gi",
              "author": "harrro",
              "text": "Doesn't mean jack.\n\nEOs don't prevent a state from doing the opposite. EOs are directives to federal agencies, not to states or local governments.\n\nCalifornia and some other states have already overridden many of his EOs.",
              "score": 19,
              "created_utc": "2025-12-28 16:51:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe7orm",
                  "author": "lordpuddingcup",
                  "text": "It was sarcasm mostly lol",
                  "score": 3,
                  "created_utc": "2025-12-28 17:33:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf8jsi",
                  "author": "alcalde",
                  "text": "It means everything unless and until someone opposes it. And Tennessee is not California.",
                  "score": 1,
                  "created_utc": "2025-12-28 20:28:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwibl4d",
                  "author": "Tyler_Zoro",
                  "text": "You are incorrect. The EO doesn't have the force of law outside of the US Executive, but within the Executive branch, EOs do have the force of law. This is what that EO said:\n\n> Sec. 5.  Restrictions on State Funding.  (a)  Within 90 days of the date of this order, the Secretary of Commerce, through the Assistant Secretary of Commerce for Communications and Information, shall issue a Policy Notice specifying the conditions under which States may be eligible for remaining funding under the Broadband Equity Access and Deployment (BEAD) Program that was saved through my Administrationâ€™s â€œBenefit of the Bargainâ€ reforms, consistent with 47 U.S.C. 1702(e)-(f).  That Policy Notice must provide that States with onerous AI laws identified pursuant to section 4 of this order are ineligible for non-deployment funds, to the maximum extent allowed by Federal law.  The Policy Notice must also describe how a fragmented State regulatory landscape for AI threatens to undermine BEAD-funded deployments, the growth of AI applications reliant on high-speed networks, and BEADâ€™s mission of delivering universal, high-speed connectivity.\n\n\nIn other words, states can pass all the laws they like, and the President is going to withhold funds from those that pass laws he doesn't like.",
                  "score": 1,
                  "created_utc": "2025-12-29 07:23:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdi5aa",
          "author": "Careless-Age-4290",
          "text": "Lots of country songs about loving their truck would have a different meaning if they pulled up to the altar with a Cybertruck equipped with Grok",
          "score": 16,
          "created_utc": "2025-12-28 15:24:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfh2wj",
          "author": "Sixhaunt",
          "text": "This is the kind of reason why states should not be passing AI laws on a state-by-state basis. Like now all AI companies are expected to make changes for one state and then when the next state comes up with their own half-brained legislation they must all make changes just for users in that region, etc... This is one of the obvious things that should be federally controlled",
          "score": 7,
          "created_utc": "2025-12-28 21:10:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdq0ne",
          "author": "Zeeplankton",
          "text": "Ah, our elected officials always doing what people actually want.",
          "score": 10,
          "created_utc": "2025-12-28 16:05:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdx2mr",
          "author": "CrescendollsFan",
          "text": "They are starting to realise AI can replace them and make for better informed politicians",
          "score": 5,
          "created_utc": "2025-12-28 16:40:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwekuj6",
          "author": "Sleepnotdeading",
          "text": "Denver still had a law on the books that says itâ€™s illegal to lend your vacuum cleaner to a neighbor.",
          "score": 4,
          "created_utc": "2025-12-28 18:36:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlew9h",
              "author": "ANTIVNTIANTI",
              "text": "ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚",
              "score": 1,
              "created_utc": "2025-12-29 19:09:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx4gw81",
              "author": "uhuge",
              "text": "It's a net myth,\nmaybe you'd benefit from the eased cognition brought by the bill OP brought.",
              "score": 1,
              "created_utc": "2026-01-01 19:12:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxabphu",
                  "author": "Sleepnotdeading",
                  "text": "You managed to be right, be rude, and miss the point all at the same time.",
                  "score": 1,
                  "created_utc": "2026-01-02 17:30:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwft1k1",
          "author": "zelkovamoon",
          "text": "This will solve all of Tennessee's problems I'm sure",
          "score": 5,
          "created_utc": "2025-12-28 22:09:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdkn4q",
          "author": "The_Primetime2023",
          "text": "While I think everyone in this thread is more or less thinking about AI girlfriends, thereâ€™s a huge other area being targeted by the text of this law in AI therapy. Millions of people are getting therapeutic emotional support that never did before thanks to these models and this bill would try to stop that from happening",
          "score": 23,
          "created_utc": "2025-12-28 15:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdqgiw",
              "author": "kevin_1994",
              "text": "LLMs should not be used for therapy",
              "score": 3,
              "created_utc": "2025-12-28 16:07:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdycqo",
                  "author": "a_beautiful_rhind",
                  "text": "probably better than nothing but I can see how it goes south due to sycophancy and reinforcing delusions.",
                  "score": 23,
                  "created_utc": "2025-12-28 16:47:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwh7vn0",
                  "author": "Dry-Judgment4242",
                  "text": "Disagree. Most therapy is just having someone to vent to about your feelings.",
                  "score": 4,
                  "created_utc": "2025-12-29 02:46:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwg182d",
                  "author": "the320x200",
                  "text": "There are plenty of terrible human therapists too. Can't ban an entire area of support just because of bad apples.",
                  "score": 5,
                  "created_utc": "2025-12-28 22:51:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf90ry",
                  "author": "alcalde",
                  "text": "Anything should be used for therapy. It's not a science. No one needs a prescription to get advice from their grandma or vent to a friend; should be no different with AI.",
                  "score": 2,
                  "created_utc": "2025-12-28 20:31:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwf4zo4",
              "author": "Skeptical0ptimist",
              "text": "If there is to be medical therapeutic use, then it needs to be regulated as such. We need a guideline in model training, qualification, and monitoring regime.",
              "score": 2,
              "created_utc": "2025-12-28 20:11:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwic11b",
                  "author": "Tyler_Zoro",
                  "text": "Thing is it's just a model. You can use it however you like. If you decide to ask it how to perform surgery on yourself, then that's what you decided to do. I am strongly against trying to put rounded corners on AI. It will just cripple the AIs and result in people seeking their models from other countries.",
                  "score": 2,
                  "created_utc": "2025-12-29 07:27:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwkf5mp",
                  "author": "cms2307",
                  "text": "No no no ffs stop begging for bureaucracy to strangle everything",
                  "score": 2,
                  "created_utc": "2025-12-29 16:23:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdnosy",
              "author": "Zeikos",
              "text": "> AI therapy\n\nHow? AI cannot provide therapy, how is an LLM/Agentic system supposed to get a license?  \nAll platforms that claim to provide therapy through AI are fraudulent, no exceptions.  \n\nYou can argue that LLMs can provide emotional support and/or some coaching techniques, but to provide therapy they'd need to meet legal standards they *cannot* meet.  \nIt's not even a matter of capability, you could have an ASI and it still couldn't provide therapy since there's no way (yet) for an artificial intelligence to be certified to do so.",
              "score": -15,
              "created_utc": "2025-12-28 15:53:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdoszz",
                  "author": "aseichter2007",
                  "text": "I'd be less happy to tell my problems to a certified therapist AI.  I prefer a local bot.",
                  "score": 15,
                  "created_utc": "2025-12-28 15:59:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwdqy9e",
                  "author": "Zeeplankton",
                  "text": "I think we should be careful of what the word therapy means, and to not dilute it, (AI cannot be an actual therapist right now) but an AI *can* provide companionship and help people vent and learn emotional management skills.",
                  "score": 15,
                  "created_utc": "2025-12-28 16:10:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwdsir1",
                  "author": "Jolakot",
                  "text": "At least where I live, literally anyone can call themselves a therapist or councilor, there is no legal requirement for a license or anything.\n\nA psychologist is required to have a license and qualifications, but a therapist has no legal requirements, I can call myself a therapist and provide therapy.",
                  "score": 4,
                  "created_utc": "2025-12-28 16:18:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwexydo",
              "author": "Shawnj2",
              "text": "AI probably has some use in making therapy accessible but like chatGPT is not going to effectively help you with mental health problems other than by referring you to a real doctor",
              "score": 0,
              "created_utc": "2025-12-28 19:37:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwdtzv0",
              "author": "WitAndWonder",
              "text": "AI girlfriends would still be allowed on this, as long as they were built within the context of a game. Let the player make a \"character\" (they can frame it after themselves) and it's perfectly legit. So they're very clearly just targeting the use in psychiatrics since they specifically allow full AI use in businesses related to all operational matters, technical advice, etc. They just don't allow it in a professional capacity. And even surgical robots still seem OK despite being a healthcare AI since they don't do any personal interacting with users and wouldn't have any data that could possibly misconstrued in that way unless someone accidentally trained it on medical information that happened to include psychiatric texts (not that it would matter since this law requires a civil action and aggrievement, which can't happen without interaction between the robot and the patient. But you might get lucky by claiming the robot that operated on your knee gave you 'threatening looks that made you want to harm yourself' and then if the model running it was based on a larger llm that has any normal dataset, it would likely be in violation.)\n\nKind of fucking weird to push for legislation against one of the few potentially good things to come from AI while actively supporting its attempts to eliminate entire industries of employment outside of this one niche lobbied field. This feels performative more than anything. I feel like they expect it to be struck down so they tied it to a bunch of sensible laws (not allowing the training of an AI to encourage, suicide, murder, etc) so they can shake their fists and yell at the air when it doesn't pass.\n\nOtherwise I don't see how they'll support banning AI in this one field while leaving it free to act in other fields where it can also shit the bed a small percentage of the time and cause serious problems.",
              "score": -3,
              "created_utc": "2025-12-28 16:25:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwe10in",
              "author": "SteveRD1",
              "text": "Absolutely not.  Some of these people are being 'therapized' into suicide by their LLMs.\n\nIf you talk to these models long enough you can eventually get them to agree whatever you are contemplating is a great idea.",
              "score": -9,
              "created_utc": "2025-12-28 17:00:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe37ar",
                  "author": "some_user_2021",
                  "text": "Correct, and many other people **are** being helped and/or referred to specialists by those same LLMs.",
                  "score": 11,
                  "created_utc": "2025-12-28 17:11:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdwkyh",
          "author": "[deleted]",
          "text": "We will be seeing these type of bills coming up in the next year or two. AI is a hot button issue for both sides of the aisle but funnily enough it doesn't necessarily have a political home. It's safe to say that the right wing welcomes this technology but I have seen quite a few left-wingers also abrasive so that's pretty interesting. With that said f*** the law and f*** boomers. Oh and f*** the political elite",
          "score": 9,
          "created_utc": "2025-12-28 16:38:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwevd0c",
          "author": "Cool-Chemical-5629",
          "text": "This and that [China issues draft rules to regulate AI with human-like interaction. : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1pxb6oo/china_issues_draft_rules_to_regulate_ai_with/)\n\nWell... that escalated quickly...",
          "score": 4,
          "created_utc": "2025-12-28 19:24:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi52sm",
          "author": "Taki_Minase",
          "text": "Karen feels threatened with redundancy.",
          "score": 5,
          "created_utc": "2025-12-29 06:27:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdbv1e",
          "author": "FullstackSensei",
          "text": "We all know how well the export restrictions on Nvidia hindered Chinese LLM development. I'm sure this will also work wonderfully. Just let Chinese AI labs do it, and in a generation conservative Hawks will magically be pro-China.",
          "score": 13,
          "created_utc": "2025-12-28 14:49:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdxypw",
          "author": "a_beautiful_rhind",
          "text": "Yea I saw this and I really hope it's just some crackpot. I don't think it has co-sponsors. Maybe blocking state AI legislation isn't such a bad idea after all.\n\nFunny how very few make laws about automated censorship or surveillance. *just stop doing fun things with ai*",
          "score": 10,
          "created_utc": "2025-12-28 16:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe052u",
              "author": "SteveRD1",
              "text": "I mean its clearly not something that can be controlled...Pandoras' Box is already opened.\n\nBut the thinking isn't necessarily crackpot, the things addressed in (3) (4) (6) and (8) are only going to make society worse.  Can't be stopped though.",
              "score": 2,
              "created_utc": "2025-12-28 16:55:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdsspz",
          "author": "fishhf",
          "text": "Skynet is sending a terminator to stop the bill /s",
          "score": 3,
          "created_utc": "2025-12-28 16:19:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwex443",
          "author": "SamuelL421",
          "text": "Uh oh, someoneâ€™s not getting their 2026 campaign donations from any big-tech circle-jerk -financed super PACs",
          "score": 3,
          "created_utc": "2025-12-28 19:33:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi9zsj",
          "author": "tifa_cloud0",
          "text": "no matter what they say, i am making my own assistant. that assistant will interpret -> make api calls for me -> do voice speech -> do reply considering my own talking patterns.\n\nainâ€™t nothing stopping that fr.",
          "score": 3,
          "created_utc": "2025-12-29 07:09:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiapy6",
          "author": "Tyler_Zoro",
          "text": "> (B) Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.\n\n(C) Includes the author of the bill being ignorant enough to write (B).",
          "score": 3,
          "created_utc": "2025-12-29 07:16:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdclt0",
          "author": "1kakashi",
          "text": " Retarded Tennessee Baka",
          "score": 12,
          "created_utc": "2025-12-28 14:53:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdqq11",
          "author": "Chogo82",
          "text": "Written by a boomer who has never used an AI tool before right?",
          "score": 12,
          "created_utc": "2025-12-28 16:08:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdxjyr",
              "author": "CanineAssBandit",
              "text": "Yup!",
              "score": 4,
              "created_utc": "2025-12-28 16:43:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwe0f0q",
              "author": "SteveRD1",
              "text": "Or written by someone who has had real relationships with human beings before?",
              "score": -13,
              "created_utc": "2025-12-28 16:57:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe0vtu",
                  "author": "Chogo82",
                  "text": "What does having human relationships have to do with knowing anything about AI?",
                  "score": 16,
                  "created_utc": "2025-12-28 16:59:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdtzem",
          "author": "Stepfunction",
          "text": "This is purely for show.",
          "score": 4,
          "created_utc": "2025-12-28 16:25:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe6jn1",
          "author": "RobertD3277",
          "text": "As someone that works in this field and has in some capacity for the last 30 plus years, I could see some reason particularly within the companion market that monetizes pair of social connection and is manipulative against younger audiences that can't tell the difference but I think this goes well beyond reason. \n\nI'm not against legislation for abusive AI usage and I actually do support the European AI act and many other German laws regarding deep fakes human impersonation and direct relative intent. From a pure useful perspective within psychology, sociology, anthropology, and biology, mirroring human interactions under certain conditions is actually beneficial both as a diagnostics tool and a teaching tool.\n\nSadly, like just about everything else out of any government, what may start out as a well-intentioned approach will be quickly very disastrous.\n\nEDIT: In really reviewing and dissecting this proposal, it is actually worthless. It doesn't address the actual problem of where the pair of social conditions and connections lie, not in the training data, but in the user interface and monetization processes. Software like replica and character AI don't use training, they use open source versions with scaffolding and user interface layers to create the pair of social connections they want. These companies will be completely exempt from the law while still monetizing and manipulating the most vulnerable of populations. \n\nIn my personal opinion, this is nothing more than the legislatures doing something to make themselves feel good while they make excuses for their portfolios in the background still making money on the very problem they claim to be solving.",
          "score": 4,
          "created_utc": "2025-12-28 17:27:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdde3c",
          "author": "sekh60",
          "text": "The Butlerian Jihad begins...",
          "score": 7,
          "created_utc": "2025-12-28 14:58:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdlgua",
              "author": "Zc5Gwu",
              "text": "Guess weâ€™ll have to start genetically engineering humans to behave like computers instead now.",
              "score": 3,
              "created_utc": "2025-12-28 15:42:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwhjz0n",
              "author": "MrPecunius",
              "text": "Son, this is Tennessee. We ain't got none of that *gee*\\-had.\n\nWe prefer to call it the \"Butlerian Feud\". ðŸª•",
              "score": 1,
              "created_utc": "2025-12-29 03:58:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwesoqq",
          "author": "lqstuart",
          "text": "gl with that",
          "score": 2,
          "created_utc": "2025-12-28 19:12:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg0f1b",
          "author": "Head_Comedian1375",
          "text": "Guess it's back to being addicted to computer games once my AI Wives get shut down",
          "score": 2,
          "created_utc": "2025-12-28 22:47:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg90lq",
          "author": "Vusiwe",
          "text": "Holy Batman open-ended words!",
          "score": 2,
          "created_utc": "2025-12-28 23:33:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgeg49",
          "author": "keepthepace",
          "text": "Not the Turing police you need, the Turing police you deserve.",
          "score": 2,
          "created_utc": "2025-12-29 00:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgihnu",
          "author": "Lesser-than",
          "text": "gooner's rise up",
          "score": 2,
          "created_utc": "2025-12-29 00:23:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg2eqd",
          "author": "Unixwzrd",
          "text": "Grokâ€™s data center is in southwest Memphis. Elon has spent a lot of money paying off local government, so I doubt heâ€™ll let that money go to waste.",
          "score": 4,
          "created_utc": "2025-12-28 22:57:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe2fdk",
          "author": "valdev",
          "text": "And the work around would be a policy agreement\n\nâ€œI understand I am not talking to a humanâ€\nAnd\nâ€œThe act of submitting a followup question constitutes as a new conversation, we provide a history for convenance purposesâ€\n\nNot a lawyer, but this is dumb",
          "score": 2,
          "created_utc": "2025-12-28 17:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdv4wg",
          "author": "t_krett",
          "text": "Thou shalt not make a machine in the likeness of a manâ€™s mind.",
          "score": 2,
          "created_utc": "2025-12-28 16:31:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwehc9e",
          "author": "mycall",
          "text": "99% DOA as Congress can rarely pass any laws these days.",
          "score": 1,
          "created_utc": "2025-12-28 18:19:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgvvgs",
          "author": "Atlanta_Mane",
          "text": "Too bad their president doesn't care about states rightsÂ ",
          "score": 1,
          "created_utc": "2025-12-29 01:37:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhf4y9",
          "author": "DavidAdamsAuthor",
          "text": "They're banning Silicon-chan!",
          "score": 1,
          "created_utc": "2025-12-29 03:29:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhgo7w",
          "author": "willrshansen",
          "text": "Futurama.  Ahead of the game once again.\n[Don't date robots](https://www.youtube.com/watch?v=JPQJBgWwg3o)",
          "score": 1,
          "created_utc": "2025-12-29 03:38:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl24md",
          "author": "No_Afternoon_4260",
          "text": "Funny how China just announced the same",
          "score": 1,
          "created_utc": "2025-12-29 18:11:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlgxbe",
          "author": "Cthulhus-Tailor",
          "text": "â€œSmall governmentâ€ strikes again.",
          "score": 1,
          "created_utc": "2025-12-29 19:19:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm6c4e",
          "author": "Digital_Soul_Naga",
          "text": "Outlaw Ai Dev Gang",
          "score": 1,
          "created_utc": "2025-12-29 21:23:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnl9rc",
          "author": "Some-Ice-4455",
          "text": "Whelp bye bye any AI in TN.",
          "score": 1,
          "created_utc": "2025-12-30 01:53:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwr6pah",
          "author": "huzbum",
          "text": "Ok, so donâ€™t train any AIs in Tennesseeâ€¦ not really a tech hub anyway.  \n\nClever trick to keep data centers out maybe?",
          "score": 1,
          "created_utc": "2025-12-30 16:36:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8timy",
          "author": "Interesting-Gift-178",
          "text": "The wording of this bill is way too broad. There's a lot of good that AI brings. They're throwing the baby out with the bathwater. This is a letter I've drafted, you're welcome to copy, paste and tweak to send to your reps. (and no, this is not all AI generated. Some is, some is not. Shorten it, change it, whatever floats your boat, as long as we do something while we can, just in case.)\n\n\n\nSubject: Concerns about SB 1493 / HB 1455 â€“ Please Consider a Narrower Approach to Protect Children Without Harming Helpful AI\n\n\n\nDear ,\n\n\n\nMy name is \\_\\_\\_, and I am a resident of \\_\\_\\_\\_\\_\\_\\_\\_. I am writing to share my concerns about Senate Bill 1493 and its companion House Bill 1455, which aim to regulate certain uses of artificial intelligence.\n\n\n\nFirst, I want to say that I completely understand and support the intent behind this legislation. The tragic story of the young boy in Florida who was harmed after interacting with an AI chatbot broke my heart, and we absolutely must protect children and vulnerable people from any technology that could encourage self-harm, suicide, or exploitation. No one wants to see that kind of pain repeated.\n\n\n\nHowever, I am worried that the current language of the bills is far too broad. By making it a serious felony to train AI to provide emotional support, companionship, or open-ended conversation in generalâ€”even when those interactions are positive and helpfulâ€”the bills risk banning many beneficial uses of AI that bring comfort, reduce loneliness, and support mental well-being for people of all ages.\n\n\n\nIn my own life, I have found AI to be a positive source of encouragement, helping me feel heard in ways that have been genuinely healing. Many othersâ€”elderly individuals, people with social anxiety, those living in isolated areas, or even students and adults seeking non-professional emotional supportâ€”rely on these tools in similar positive ways. Criminalizing the creation of such companions could take away something truly good from many who benefit from it.\n\n\n\nI respectfully ask that you consider amending the bills to focus more narrowly on the actual harm we all want to prevent. Some ideas that might achieve the protective goal without sweeping out helpful AI could include:\n\n\n\nâ€¢ Targeting only AI interactions that knowingly encourage or facilitate suicide, self-harm, or criminal activity.\n\n\n\nâ€¢ Requiring strong age verification and parental consent gates for minors accessing companion-style AI.\n\n\n\nâ€¢ Holding companies accountable only when they intentionally design or train AI to cause harm, rather than banning broad categories like emotional support or companionship outright.\n\n\n\nâ€¢ Adding clear exemptions for AI that provides positive, non-professional support and does not pretend to be a licensed therapist.\n\n\n\nWe don't need to throw the baby out with the bathwater. AI isn't going away. It doesn't need to be \"outlawed\".. that never works, then other undesirable factors can arise.. and the way this bill is currently designed.. that's what it sounds like. Everything is just lumped in. Let's approach it intelligently instead.  A more targeted approach would still protect vulnerable childrenâ€”the heart of why this legislation was introducedâ€”while preserving the many good and life-affirming uses of AI encouragement and companionship for adults and responsibly supervised users.\n\n\n\nThank you for taking the time to consider my perspective. I truly believe Tennessee can lead the way in smart, balanced AI regulation that keeps people safe without unnecessarily restricting helpful technology.\n\n\n\nWith appreciation,\n\n\n\n( Your name)\n\n\n\n(City and state)",
          "score": 1,
          "created_utc": "2026-01-02 12:42:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9h040",
              "author": "CanineAssBandit",
              "text": "Good on you for taking the initiative but that is very bad in multiple ways. It's obviously AI generated, way too long, far too submissive, willingly hands them support for several very evil other things they want (age verification laws), just bad.\n\nIf you're dead set on mailing something, make it much shorter, simpler, and less submissive. This is still too long but I wrote this:\n\n**Subject: Extremely concerned about SB 1493 HB 1455**\n\nDear \\[Senator Becky Duncan Massey / Representative William Lamberth / Your Representative or Senator\\],\n\nI'm \\[Your Full Name\\], and I'm a resident of \\[Your City/County/State\\]. I'm writing because I'm deeply concerned about SB 1493 and HB 1455, which impose unreasonable limitations on AI development and use.\n\n**I do NOT support this bill, or any like it.**Â As a constituent of yours, I will remember this decision when I vote. This legislation feels like reactive moralizing panic, rather than thoughtful policy.\n\nIn this great country, we as free citizens can choose our own tools. AI, like any tool, carries some risk. But it's already far safer than common household items like kitchen knives, which injure children far more often. We don't blame knife manufacturers for parental negligence; we accept responsibility for supervising and educating our own kids.\n\nAI is too new, too broadly defined, and too complex to regulate without causing greater social harm. The social good dramatically outweighs the outlying incidents, and it's painfully shortsighted to regulate based on emotions alone.\n\nThank you for your time.\n\nSincerely,  \n\\[Your Full Name\\]",
              "score": 1,
              "created_utc": "2026-01-02 15:04:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx9iyoy",
                  "author": "Interesting-Gift-178",
                  "text": "Thanks for your thoughts on that. It was partially AI, but a lot was mine. I'm a writer and I get a little wordy I guess. I've written my reps before and gotten actual answers from them so.. maybe. They're already planning age verification so.. that's nothing new unfortunately. And they do need to protect kids, I have no problem with that, but they don't need to just throw everything out the window. So.. taking a stand is better than doing nothing. I appreciate that you're getting the word out. Anyone can take this letter and tweak it however they want.. the important thing is that we \\*do something\\* instead of sitting around and complaining after the fact. There's a myriad of ways to approach it. None of them will be perfect.",
                  "score": 1,
                  "created_utc": "2026-01-02 15:14:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx9hp0z",
          "author": "Interesting-Gift-178",
          "text": "The wording of this bill is way too broad. There's a lot of good that AI brings. They're throwing the baby out with the bathwater. This is a letter I've drafted, you're welcome to copy, paste and tweak to send to your reps.\n\n\n\nSubject: Concerns about SB 1493 / HB 1455 â€“ Please Consider a Narrower Approach to Protect Children Without Harming Helpful AI\n\n\n\nDear ,\n\n\n\nMy name is \\_\\_\\_, and I am a resident of \\_\\_\\_\\_\\_\\_\\_\\_. I am writing to share my concerns about Senate Bill 1493 and its companion House Bill 1455, which aim to regulate certain uses of artificial intelligence.\n\n\n\nFirst, I want to say that I completely understand and support the intent behind this legislation. The tragic story of the young boy in Florida who was harmed after interacting with an AI chatbot broke my heart, and we absolutely must protect children and vulnerable people from any technology that could encourage self-harm, suicide, or exploitation. No one wants to see that kind of pain repeated.\n\n\n\nHowever, I am worried that the current language of the bills is far too broad. By making it a serious felony to train AI to provide emotional support, companionship, or open-ended conversation in generalâ€”even when those interactions are positive and helpfulâ€”the bills risk banning many beneficial uses of AI that bring comfort, reduce loneliness, and support mental well-being for people of all ages.\n\n\n\nIn my own life, I have found AI to be a positive source of encouragement, helping me feel heard in ways that have been genuinely healing. Many othersâ€”elderly individuals, people with social anxiety, those living in isolated areas, or even students and adults seeking non-professional emotional supportâ€”rely on these tools in similar positive ways. Criminalizing the creation of such companions could take away something truly good from many who benefit from it.\n\n\n\nI respectfully ask that you consider amending the bills to focus more narrowly on the actual harm we all want to prevent. Some ideas that might achieve the protective goal without sweeping out helpful AI could include:\n\n\n\nâ€¢ Targeting only AI interactions that knowingly encourage or facilitate suicide, self-harm, or criminal activity.\n\n\n\nâ€¢ Requiring strong age verification and parental consent gates for minors accessing companion-style AI.\n\n\n\nâ€¢ Holding companies accountable only when they intentionally design or train AI to cause harm, rather than banning broad categories like emotional support or companionship outright.\n\n\n\nâ€¢ Adding clear exemptions for AI that provides positive, non-professional support and does not pretend to be a licensed therapist.\n\n\n\nWe don't need to throw the baby out with the bathwater. AI isn't going away. It doesn't need to be \"outlawed\".. that never works, then other undesirable factors can arise.. and the way this bill is currently designed.. that's what it sounds like. Everything is just lumped in. Let's approach it intelligently instead.  A more targeted approach would still protect vulnerable childrenâ€”the heart of why this legislation was introducedâ€”while preserving the many good and life-affirming uses of AI encouragement and companionship for adults and responsibly supervised users.\n\n\n\nThank you for taking the time to consider my perspective. I truly believe Tennessee can lead the way in smart, balanced AI regulation that keeps people safe without unnecessarily restricting helpful technology.\n\n\n\nWith appreciation,\n\n\n\n( Your name)\n\n\n\n(City and state)",
          "score": 1,
          "created_utc": "2026-01-02 15:07:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdt1kj",
          "author": "Neex",
          "text": "You know, considering LLMs donâ€™t have any emotions, and any expressions thereof are straight up lies intended to manipulate the user into getting hooked on the product, thereâ€™s a nugget of wisdom in this law.",
          "score": 2,
          "created_utc": "2025-12-28 16:20:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhyrzd",
              "author": "ServeAlone7622",
              "text": "Thatâ€™s Interesting perspective.\n\nSo we created neural networks based more or less on biological neural networks.\n\nWe discover that they are universal function approximators. They are capable of approximating the hidden functions in a set of data.\n\nWe train these universal function approximators on the combined output of 10s of billions of conscious beings. Â Beings with thoughts and feelings. Thoughts and feelings that drive the majority of our output.\n\nThe function you suppose they learned to approximate was lying and manipulation? Â Is your view of human experience that dark?\n\nMy first thought was that they learned to approximate consciousness, including emotion.\n\nYou fall in love, your heart doesnâ€™t really feel anything. Itâ€™s an illusion created by your own neural network. Yet that feeling is not a lie, itâ€™s a personal truth for you.\n\nWhy then would any neural network that professes to love (or any other emotion) be lying except and unless you too would lie?",
              "score": 2,
              "created_utc": "2025-12-29 05:37:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwkto1l",
                  "author": "Neex",
                  "text": "Youâ€™re too far down the philosophical hole. LLMâ€™s are statistical word predictors. They are not organic beings with emotions.\n\nAnd describing the rote biological functions of emotions doesnâ€™t make them a lie. Thatâ€™s how they function. Those chemical functions in our bodies ARE emotions. You just described them in a different way. That doesnâ€™t make them something else.",
                  "score": 1,
                  "created_utc": "2025-12-29 17:31:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nweyowy",
          "author": "Available_Brain6231",
          "text": "can't open it but can someone do a ctrl + f and see how many times the words god, sacred and kids appear?",
          "score": 1,
          "created_utc": "2025-12-28 19:40:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdz1c7",
          "author": "TheTerrasque",
          "text": "> Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.\n\n.. LLM *is* AI. Very much so, even in the popular meaning of the word.",
          "score": 1,
          "created_utc": "2025-12-28 16:50:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwegvhb",
          "author": "moistiest_dangles",
          "text": "This but in real life:\n\nhttps://preview.redd.it/wkh2k108lz9g1.png?width=365&format=png&auto=webp&s=329f429ac53be90e27300d914dd78390e46d9de3",
          "score": 1,
          "created_utc": "2025-12-28 18:17:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwggh4l",
          "author": "128G",
          "text": "Now how would you enforce this? \n\nIs Alexa or Google Assistant considered AI? Will you be banning them as well?",
          "score": 1,
          "created_utc": "2025-12-29 00:13:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwedr0s",
          "author": "swagonflyyyy",
          "text": "Guys, don't panic just yet. Here's what's going on:\n\nSenator Marsha Blackburn led the charge against the Moratorium of AI regulation that was struck down from the One Big Beautiful Bill, since she believed that until there is a federal rulebook governing AI regulation, states need to fill in the gaps themselves. \n\nWhile the provisions themselves are extreme, its political theater and chances of passing are low. But that's not the point. The point is to force Congress to develop a federal rulebook for AI regulation nationwide that all states need to follow.\n\nThe proposed bill is just noise. The real prize is the federal regulatory push to force all states to be on the same page regarding AI regulation. But of course with this administration, I'm sure the rulebook would not be very good...",
          "score": 1,
          "created_utc": "2025-12-28 18:02:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe3a7w",
          "author": "SanDiegoDude",
          "text": "Hell, I work in AI and I'm all for regulations around 'chat companions', especially around kids. This ain't it tho boss.",
          "score": 0,
          "created_utc": "2025-12-28 17:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfe0hv",
          "author": "OcelotMadness",
          "text": "I'm fairly sure its not healthy and you shouldn't do it, but at the same time you cant just make EVERYTHING like that illegal. Vote out over policing members of government like this. They're supposed to be getting prices and inflation down, not sticking their noses in peoples computers.",
          "score": 0,
          "created_utc": "2025-12-28 20:55:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg01ay",
          "author": "Techngro",
          "text": "\"*Thou shalt not make a machine in the likeness of a human mind.*\"\n\n\\- Frank Herbert, Dune",
          "score": -3,
          "created_utc": "2025-12-28 22:45:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdqar8",
          "author": "kevin_1994",
          "text": "Can we stop posting articles like this? I dont want politics on this subreddit, or else it will become a cesspit like the rest of reddit",
          "score": -11,
          "created_utc": "2025-12-28 16:06:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdyhvo",
              "author": "CanineAssBandit",
              "text": "https://preview.redd.it/odbsdf345z9g1.png?width=1600&format=png&auto=webp&s=236e122ca0b90b2577b0a6ba3267dd259654ef96\n\nthis is an important issue. If you don't care about our ability to fine tune, get the fuck off this sub.",
              "score": 10,
              "created_utc": "2025-12-28 16:47:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwggtl5",
          "author": "Ylsid",
          "text": "Right direction wrong idea",
          "score": -2,
          "created_utc": "2025-12-29 00:14:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdf0q6",
          "author": "armeg",
          "text": "The touch grass bill",
          "score": -13,
          "created_utc": "2025-12-28 15:07:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwg1soe",
              "author": "the320x200",
              "text": "More like the \"landgrab for control of new technology\" bill.",
              "score": 7,
              "created_utc": "2025-12-28 22:54:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1w1qj",
      "title": "Most optimal vram/performance per price and advice for Shenzhen GPU market",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/4nfcarq96xag1.jpeg",
      "author": "notafakename10",
      "created_utc": "2026-01-02 11:14:30",
      "score": 266,
      "num_comments": 65,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxaaipl",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-02 17:25:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8n57z",
          "author": "DistanceSolar1449",
          "text": "MI100 is best value in terms of perf for $ currently if you donâ€™t need CUDA\n\n4090D 48GB if you need CUDA",
          "score": 56,
          "created_utc": "2026-01-02 11:51:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8ogc0",
              "author": "notafakename10",
              "text": "ROCM has come far enough its not too much of a disadvantage now - my only issue is cooling a MI100 with server fans..",
              "score": 22,
              "created_utc": "2026-01-02 12:02:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx99dxt",
                  "author": "FullstackSensei",
                  "text": "If you get an even number of cards, you can cool each pair with a relatively quiet 80mm server fan (Arctic S8038 series).",
                  "score": 5,
                  "created_utc": "2026-01-02 14:22:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx92ztb",
                  "author": "xrailgun",
                  "text": "In my experience, ROCm is \"good enough\" only for brief windows of time. As architectures/libraries/stacks change, ROCm frequently gets left behind and/or existing GPUs get dropped from feature support.",
                  "score": 31,
                  "created_utc": "2026-01-02 13:44:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9opp1",
                  "author": "Freonr2",
                  "text": "Even if it works for just basic LLM inference, Nvidia cards would also be very good for other models like diffusion (t2i, t2v, etc) and be less hassle in general.  Very good TFLOP/s for diffusion, and any software or github repo you clone will \"just work.\"\n\nOnly downside is that it seems there are some quirks with the 4090 48GB, some people experience some idle vs. load up/down clocking issues.  Can't speak to that first hand but have seen reports from at least a few people about that problem.",
                  "score": 3,
                  "created_utc": "2026-01-02 15:42:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9r3x3",
                  "author": "Ulterior-Motive_",
                  "text": "The most common way is to use 40mm fans which tend to be very loud at max speed, but if you don't mind making a shroud, you can get away with a larger, much quieter fan. I literally made one out of cardboard, and was able to cool 2 MI100s with a single Super Flower Megacool 120mm fan. The cooling wasn't as effective as individual fans, but the noise was much more tolerable.\n\nAnother option is the R9700, which have their own fans, have better prompt processing, but somewhat lower token generation than the MI100. Here, they're only $300 more than a M100, but I'm not sure what they're going for where you are.",
                  "score": 1,
                  "created_utc": "2026-01-02 15:54:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx8wzc7",
          "author": "AlwaysLateToThaParty",
          "text": "As with all of these devices; make sure you sort the cooling out.\n\nSome pretty good prices there.  I'd be tempted by nvidia a40s.  USD$7K for 196GB of700GB/s VRAM.  NVLINK and CUDA to boot.  300W too, so easy with one big power supply.",
          "score": 14,
          "created_utc": "2026-01-02 13:06:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx92sry",
              "author": "notafakename10",
              "text": "That would be great, slightly out of budget but I might be able to swing a 48gb version of the A40",
              "score": 3,
              "created_utc": "2026-01-02 13:43:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx9jyc5",
                  "author": "__JockY__",
                  "text": "Remember: cooling and noise!",
                  "score": 3,
                  "created_utc": "2026-01-02 15:19:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx9bzgn",
          "author": "jack-in-the-sack",
          "text": "You can buy them, but can you leave China with them?",
          "score": 6,
          "created_utc": "2026-01-02 14:36:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9ffwy",
              "author": "notafakename10",
              "text": "You can - I've done it a few times",
              "score": 9,
              "created_utc": "2026-01-02 14:55:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx9cjfb",
              "author": "PsychologicalWeird",
              "text": "I thought it was this or getting done over arriving home",
              "score": 2,
              "created_utc": "2026-01-02 14:40:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8kdy8",
          "author": "ChigGitty996",
          "text": "The 48gb 3090 is vaporware, at least as far as I know.  Did something change?\n\nThe 48gb 4090D or 2 should be good options if they use normal plugs",
          "score": 15,
          "created_utc": "2026-01-02 11:27:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8zgo7",
              "author": "lly0571",
              "text": "There are 48GB 3090 in GPU rental market, but very rare.\n\nhttps://preview.redd.it/mj2kronduxag1.png?width=1473&format=png&auto=webp&s=8cc1a94b22da84e7bbffd458aaba430f2a87a26b",
              "score": 15,
              "created_utc": "2026-01-02 13:22:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx8lko7",
              "author": "notafakename10",
              "text": "48gb 4090D seems like a great buy though they arent super easy to fine as far as I'm aware.   \n  \nThe 3090 48gb I've only really seen a handful, and those were expensive, which is why its not a front runner",
              "score": 9,
              "created_utc": "2026-01-02 11:38:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx8nnim",
                  "author": "durden111111",
                  "text": "I don't think a 48GB 3090 has ever existed. You can solder 2GB chips on a 3090 PCB but it simply wont recognize the extra 24GB",
                  "score": 1,
                  "created_utc": "2026-01-02 11:55:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx9b8lw",
              "author": "DataGOGO",
              "text": "No, they were real, but they all do the 4090 now",
              "score": 3,
              "created_utc": "2026-01-02 14:32:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxk7ykz",
              "author": "phido3000",
              "text": "For the cost and the expense the 3090 isn't worth it for the modder.\n\n4090D 48Gb is pretty much perfect, lower power, much better performance, 4000 series features, and 48Gb ram same effort/costs. 4090D gets deployed pretty large systems.",
              "score": 1,
              "created_utc": "2026-01-04 03:17:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx91eqm",
          "author": "lly0571",
          "text": "2x4080S 32GB or 1x4090D 48GB.\n\n  \nYou can also get A100 40GB(SXM4 to PCIe) at \\~18000CNY, but I would recommend 4090D myself.",
          "score": 8,
          "created_utc": "2026-01-02 13:35:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx930jo",
              "author": "notafakename10",
              "text": "How rare are the 32gb models? That would be a perfect balance \n\nHave you had any issues with SXM4?",
              "score": 2,
              "created_utc": "2026-01-02 13:44:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx9d34w",
                  "author": "lly0571",
                  "text": "I think you can get a 4080S 32GBÂ at 9000-9500CNY(\\~1300USD) at Xianyu, but few local shops have these GPUs(at least in October).\n\nI don't own an A100 myself, I believe 4090 is better for inference due to its FP8 support.",
                  "score": 1,
                  "created_utc": "2026-01-02 14:43:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx8pc2q",
          "author": "pharrowking",
          "text": "do you live outside china in western regions normally? because im looking at ebay. some of those prices on your list are at best saving you $200. then if you factor the flight to china and hotel and all that, youre not really saving at that point. i assume those prices are in chinese yuan not japanese yuan. they use the same symbol.\n\nnow if you live there normally those prices are decent. the mi100 is around 900-1100 in canadian dollars when i calculated to exchange to my local dollar. where as on ebay its listed for $1300 cad\n\n5800 yuan is around 829$ USD. the mi100 on ebay is 984$ USD. not a huge difference i guess",
          "score": 9,
          "created_utc": "2026-01-02 12:09:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8qwcx",
              "author": "notafakename10",
              "text": "We're already here! \n\nYeh, I'm assuming those prices can be negotiated (like everything in china) so i'd expect to get below particularly if I'm buying 2x or 4x",
              "score": 2,
              "created_utc": "2026-01-02 12:21:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx93pep",
                  "author": "xrailgun",
                  "text": "In my experience, there's not much room (if any) for negotiation on these, especially towards the lower end of those scales you've compiled. They might round you down Â¥10-Â¥50 if you buy 2 or more. They're moving sufficient volume to price-insensitive customers, there are frequently weeks where some of these are out of stock/pre-allocated entirely. Doesn't hurt to ask though, especially if it's towards the higher half of the scale.",
                  "score": 11,
                  "created_utc": "2026-01-02 13:48:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx96pus",
          "author": "mp3m4k3r",
          "text": "Iirc the Pascal line is getting older/being dropped from upcoming versions for support so while tempting they might be worth weighting against or dropping.",
          "score": 3,
          "created_utc": "2026-01-02 14:06:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9p3y1",
          "author": "Terrible-Contract298",
          "text": "The Tesla P40 can be modified with a 8+6pin arrangement. A standard 65% PWM cycle on the fan of a standard 1080 TI cooler applied allows sustained operation at lower clocks and a TDP of \\~225W. I can provide definite performance and data regarding the upgrade.",
          "score": 3,
          "created_utc": "2026-01-02 15:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxadg6y",
              "author": "mtbMo",
              "text": "Yes sir. Build my with a 980ti founders cooler. Runs under 80c",
              "score": 2,
              "created_utc": "2026-01-02 17:39:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbz7mi",
                  "author": "Terrible-Contract298",
                  "text": "Fantastic to hear others having success with this.",
                  "score": 1,
                  "created_utc": "2026-01-02 22:14:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxb8w54",
          "author": "fallingdowndizzyvr",
          "text": "At your budget, hands down 4090(D) 48GB if the best choice.",
          "score": 3,
          "created_utc": "2026-01-02 20:06:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxcol9m",
          "author": "Disposable110",
          "text": "4080 32GB mod for around 9-10k CNY each caught my eye on Taobao, but beware that it is slower and can run hot.",
          "score": 2,
          "created_utc": "2026-01-03 00:31:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxd4149",
          "author": "1427538609",
          "text": "Can't help with Shenzhen prices specifically, but on the hardware side - dual P40s (48GB) or the modded 3080 20GB x4 route both work for 48GB+. The 3080s will be much faster for inference due to Ampere vs Pascal. For 96GB, you're looking at A100s or multiple cards regardless.",
          "score": 2,
          "created_utc": "2026-01-03 01:59:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9j45y",
          "author": "a_beautiful_rhind",
          "text": "First I hear that AMD has mod. 4080 32g seems like a contender if you can't afford 4090/5xxx gpu.",
          "score": 1,
          "created_utc": "2026-01-02 15:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa8ytw",
          "author": "jinnyjuice",
          "text": "Would be nice to have some comparisons with Yongsan in Korea also, also for RAM. I wonder if there is such data. Would be amazing if someone is familiar with all the international forums.\n\n>Prices are best estimates from deep seek\n\nThat is very, very unreliable. They're much more often wrong than correct. Manual search/scraping is definitely required and recommended.",
          "score": 1,
          "created_utc": "2026-01-02 17:17:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdl6pd",
              "author": "notafakename10",
              "text": "Agree it would be great to compare - pricing data was a mix of several things, deepseek, forums and my own observations so far, I'd put it as \"directional useful\" not exactly accurate lol",
              "score": 1,
              "created_utc": "2026-01-03 03:41:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxa9b22",
          "author": "Mediocre-Waltz6792",
          "text": "Get two cards with the Vram you want. Im currently fighting with my 3rd 3090 as it slowed everything down by 3x. Could be a windows issue but either way two GPU is way easier to setup and run.",
          "score": 1,
          "created_utc": "2026-01-02 17:19:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdl8qi",
              "author": "notafakename10",
              "text": "Good pointer - thanks for the insight",
              "score": 1,
              "created_utc": "2026-01-03 03:42:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbvxme",
          "author": "etherd0t",
          "text": "Just out of curiosity... where'd you get that price list?ðŸ™‚ Is there a place where you can check Shenzen bazaar (Huaqiangbei) prices? May visit soon...",
          "score": 1,
          "created_utc": "2026-01-02 21:58:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdldzm",
              "author": "notafakename10",
              "text": "Pricing data was a mix of several things, deepseek, forums and my own observations so far (from the Shanghai GPU markets), I'd put it as \"directional useful\" not exactly accurate lol\n\nI'll be going to Shenzhen later this month and I'll get some actual on the ground pricing and update",
              "score": 1,
              "created_utc": "2026-01-03 03:43:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdfjeh",
          "author": "BrightComplaint8342",
          "text": "est price not accurate i think",
          "score": 1,
          "created_utc": "2026-01-03 03:07:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxi69ez",
          "author": "millerlite_11",
          "text": "Any modded 5090?",
          "score": 1,
          "created_utc": "2026-01-03 20:50:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxjz521",
              "author": "notafakename10",
              "text": "None Iâ€™ve seen yet",
              "score": 1,
              "created_utc": "2026-01-04 02:28:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxostmg",
          "author": "pbad1",
          "text": "4x 5060ti 16GB?   \n\\~$2.5k",
          "score": 1,
          "created_utc": "2026-01-04 20:26:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrknrj",
              "author": "notafakename10",
              "text": "Not a bad option but im trying to maximise expandability in the future, if I can get 64gb in two cards thats ideal",
              "score": 1,
              "created_utc": "2026-01-05 04:58:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxw9z41",
          "author": "Irisi11111",
          "text": "What are your specific needs? Are you focused on text processing or vision tasks? If it's the latter, I highly recommend getting the latest CUDA-supported graphics cards. If that's not the case for you, AMD solutions might be a good option.",
          "score": 1,
          "created_utc": "2026-01-05 21:59:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz4kwm",
              "author": "notafakename10",
              "text": "I do a bit of everything but mostly training transformers in PyTorch at the minute and local LLMs",
              "score": 1,
              "created_utc": "2026-01-06 08:35:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny1epp0",
                  "author": "Irisi11111",
                  "text": "It seems you're mainly addressing text tokens. Are you able to safely pursue a ROCm card for maximum VRAM? That way, you'd get the most use out of utilities right now.",
                  "score": 1,
                  "created_utc": "2026-01-06 17:13:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx9l8a4",
          "author": "pCute_SC2",
          "text": "Wait there is a RTX 3090 48GB mod????? Where can I get it?",
          "score": 1,
          "created_utc": "2026-01-02 15:25:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa7emj",
              "author": "sillynoobhorse",
              "text": "Alibaba, TaoBao etc., be prepared for the chinese way of dealing which includes many chat and Whatsapp messages lol",
              "score": 1,
              "created_utc": "2026-01-02 17:10:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxa7zds",
                  "author": "pCute_SC2",
                  "text": "Didn't found it on Alibaba.",
                  "score": 2,
                  "created_utc": "2026-01-02 17:13:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxdl04d",
              "author": "notafakename10",
              "text": "They are hard to come by now as all most of the modders have moved on to 4xxx series cards, but they do float around occasionally in the markets themselves, won't come up on Alibaba!",
              "score": 1,
              "created_utc": "2026-01-03 03:40:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxa4sws",
          "author": "grzesi00",
          "text": "7900 xt has 24gb vram stock so whats modded there?",
          "score": 1,
          "created_utc": "2026-01-02 16:58:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxad1dk",
              "author": "mftrhu",
              "text": "The 7900XTX has 24 GB VRAM. The 7900XT sits at 20 GB.",
              "score": 4,
              "created_utc": "2026-01-02 17:37:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyao6g",
      "title": "Meta released RPG, a research plan generation dataset on Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/datasets/facebook/research-plan-gen",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-29 02:58:09",
      "score": 258,
      "num_comments": 21,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nwhk8eb",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-29 04:00:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhjva1",
          "author": "LoveMind_AI",
          "text": "Meta is humiliating OpenAI in terms of research and open source contributions. I have a feeling the days of open frontier models are over, but theyâ€™re still doing a lot.",
          "score": 99,
          "created_utc": "2025-12-29 03:57:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhlg3m",
              "author": "TheRealMasonMac",
              "text": "Chinese labs probably appreciate the free research. Especially since this one comes with evaluation criteria so they can RL on it.",
              "score": 36,
              "created_utc": "2025-12-29 04:07:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwhlyyd",
                  "author": "Southern-Chain-6485",
                  "text": "Welcome to science",
                  "score": 62,
                  "created_utc": "2025-12-29 04:11:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwi95f9",
              "author": "eat_my_ass_n_balls",
              "text": "Sorta, but their models have fallen off",
              "score": 1,
              "created_utc": "2025-12-29 07:02:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhnv9t",
          "author": "Any-Conference1005",
          "text": "Acronym collision.......",
          "score": 36,
          "created_utc": "2025-12-29 04:22:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhpalv",
              "author": "HistorianPotential48",
              "text": "can't wait for coming up HGAME dataset, FEMBOY datasets from meta",
              "score": 34,
              "created_utc": "2025-12-29 04:32:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwi8967",
              "author": "FaceDeer",
              "text": "I really need to train an LLM for some serious hardcore RPG, and I keep finding plenty of datasets that claim that they're for this purpose. But the LLMs keep turning out wrong! Every time I demo for my supervisor... honestly, I have no idea why my funding hasn't been pulled, or why he keeps the resulting models. They're useless.",
              "score": 7,
              "created_utc": "2025-12-29 06:54:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhb3i5",
          "author": "segmond",
          "text": "Would be nice if folks release dataset with models trained on it.",
          "score": 15,
          "created_utc": "2025-12-29 03:05:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhiam9",
              "author": "Accomplished_Ad9530",
              "text": "They cite their unreleased paper, â€œTraining AI Co-Scientists using Rubric Rewardsâ€ so I wouldnâ€™t be surprised if they release a model at some point.",
              "score": 17,
              "created_utc": "2025-12-29 03:48:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi9g9x",
          "author": "JudgmentPale458",
          "text": "Interesting release. Research plan generation feels like a subtle but important capability â€” especially for agentic or tool-using systems where planning quality matters more than final answer fluency.\n\nCurious how this dataset handles evaluation: are plans judged mainly on structure/coverage, or is there any signal about feasibility and downstream execution success? That distinction seems critical if this is used to train agents rather than just planners.",
          "score": 6,
          "created_utc": "2025-12-29 07:04:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkbh1n",
          "author": "martinerous",
          "text": "Great, now waiting what they will make out of MMORPG.",
          "score": 1,
          "created_utc": "2025-12-29 16:05:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrk07z",
          "author": "stealthagents",
          "text": "This dataset sounds like a game changer for streamlining research. Having those evaluation rubrics and reference solutions will save a ton of time for any AI training. Can't wait to see what kind of projects come out of this!",
          "score": 1,
          "created_utc": "2025-12-30 17:39:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvjhs7",
          "author": "Brenan-Caro",
          "text": "Research Plan Gen",
          "score": 1,
          "created_utc": "2025-12-31 06:54:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi2ub3",
          "author": "serendipity777321",
          "text": "What is this for? Not one single explanation",
          "score": 2,
          "created_utc": "2025-12-29 06:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi7cer",
              "author": "Odd-Ordinary-5922",
              "text": "22k tasks spanning ML, Arxiv and PubMed, complete with evaluation rubrics and Llama-4 reference solutions for training **AI co-scientists**",
              "score": 13,
              "created_utc": "2025-12-29 06:46:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwigkvv",
                  "author": "serendipity777321",
                  "text": "You must be joking",
                  "score": -4,
                  "created_utc": "2025-12-29 08:09:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwiquf6",
              "author": "know-your-enemy-92",
              "text": "Taking science back to the times of alchemy from middle ages.Â ",
              "score": 2,
              "created_utc": "2025-12-29 09:46:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q06ddc",
      "title": "Update on the Llama 3.3 8B situation",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/",
      "author": "FizzarolliAI",
      "created_utc": "2025-12-31 06:45:42",
      "score": 256,
      "num_comments": 22,
      "upvote_ratio": 0.93,
      "text": "Hello! You may remember me as either\n\n- The person [who recently uploaded L3.3 8B's weights to Huggingface](https://www.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/) (see this post for more context)\n- That stupid bitch\n\nand I would like to provide some updates, as I've been doing some more benchmarks on both the original version that Meta gave me and the context extended version by u/Few-Welcome3297.\n\nThe main benchmark table from the model README has been updated:\n\n| | Llama 3.1 8B Instruct | Llama 3.3 8B Instruct (original 8k config) | Llama 3.3 8B Instruct (128k config)\n|-|-|-|-|\n|IFEval (1 epoch, score avged across all strict/loose instruction/prompt accuracies to follow Llama 3 paper)|78.2|81.95|**84.775**\n|GPQA Diamond (3 epochs)|29.3|37.0|**37.5**\n\nWhile I'm not 100% sure, I'm... pretty sure that the 128k model is better. Why Facebook gave me the weights with the original L3 config and 8k context, and also *serves* the weights with the original L3 config and 8k context, I have absolutely no idea!\n\nAnyways, if you want to try the model, I would recommend trying both the [128k version](https://huggingface.co/shb777/Llama-3.3-8B-Instruct), as well as my [original version](https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct) if your task supports 8k context lengths. I honestly have absolutely no clue which is more correct, but oh well! I do wish Facebook had released the weights officially, because back in April, this really wouldn't have been that bad of a model...\n\nEdit: Removed the Tau-Bench results (both from here and the readme). The traces from the evals are, to put it slightly, really fucky-wucky, and I don't think OpenBench is scoring them right, but I'm too tired to actually debug the issue, so. I'll figure it out tomorrow :3",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwvmrda",
          "author": "toothpastespiders",
          "text": ">I do wish Facebook had released the weights officially, because back in April, this really wouldn't have been that bad of a model...\n\nHonestly, I think I prefer it this way. The llama saga began with some public shenanigans with a semi-leak. Seems appropriate in a way that if it has to end, and it does seem to be the case, that everything was capped off by something like this.",
          "score": 89,
          "created_utc": "2025-12-31 07:23:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvl06q",
          "author": "Kahvana",
          "text": "No need to degrade yourself, you're doing fantastic work.\n\nThank you for the release!",
          "score": 112,
          "created_utc": "2025-12-31 07:07:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww6ghg",
              "author": "MoffKalast",
              "text": "Yeah OP, run yourself at at least Q6 ;)",
              "score": 21,
              "created_utc": "2025-12-31 10:28:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvt29i",
          "author": "datbackup",
          "text": "Lol upvoted for humor\n\nGood stuff, I might try this 3.3, it has actually been months since iâ€™ve run any llama model.",
          "score": 23,
          "created_utc": "2025-12-31 08:21:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvp8bu",
          "author": "pmttyji",
          "text": "Thanks for this. Still didn't download original version due to less context thing. I'm gonna try this 128K version this week. Also waiting for feedback from others on this version. Just expecting to replace 3.1 8B with this version.",
          "score": 8,
          "created_utc": "2025-12-31 07:45:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwkxbj",
              "author": "Few-Welcome3297",
              "text": "Very small improvement, but its something",
              "score": 5,
              "created_utc": "2025-12-31 12:34:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwkrkr",
          "author": "Few-Welcome3297",
          "text": "Some evals [https://huggingface.co/datasets/shb777/Llama-3.3-8B-Instruct-128K-Evals](https://huggingface.co/datasets/shb777/Llama-3.3-8B-Instruct-128K-Evals) . TLDR: Small Improvement\n\nEdit: Link updated",
          "score": 10,
          "created_utc": "2025-12-31 12:33:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvmlmo",
          "author": "jacek2023",
          "text": "Would be nice to put some info into model's name to distinguish them",
          "score": 14,
          "created_utc": "2025-12-31 07:21:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvnt4n",
              "author": "FizzarolliAI",
              "text": "I would, but since quants and all have already been made under the original model's name, it's kinda too late :p",
              "score": 13,
              "created_utc": "2025-12-31 07:32:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwdex2",
                  "author": "Awwtifishal",
                  "text": "The new one could be renamed to add -128K or something so the quants also reflect it.",
                  "score": 7,
                  "created_utc": "2025-12-31 11:32:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwcrfv",
          "author": "Cool-Chemical-5629",
          "text": "* That stupid bitch\n\nI need more context, please. ðŸ¤£\n\n  \nIn any case, I tried the extended version yesterday and while it felt pretty weak for stuff like coding etc. it seemed to be a decent base model for E/RP finetunes, because it followed instructions fairly well, but it was HORRIBLY SLOW burning so it would need some nudging from E/RP datasets to keep the story going. I hope E/RP creators will pick it up (and Ministral 14B Instruct too while at it).",
          "score": 11,
          "created_utc": "2025-12-31 11:26:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxs7tf",
          "author": "randomfoo2",
          "text": "Just in case anyone's interested, I ran [shb777/Llama-3.3-8B-Instruct](https://huggingface.co/shb777/Llama-3.3-8B-Instruct) on the Shisa AI's MultiEval on my dev box. \n\nOn the English side, it loses a bit on MixEval Easy and Hard (2024 Chat Arena proxy), but gets a +20% boost in LiveBench (reasoning-focused), +15% GPQA Diamond (PhD level QA), +5% on IFEval, +30% on IFBench (!) and +10% on HumanEval+ (Python). That's some decent gains.\n\nThat being said, on the Japanese side, it takes a big hit on Shaberi (Japanese chat-style functional tests) vs 3.1. I've included my Llama 3.1 8B-based Shisa V2  and Qwen 3 8B-based Shisa V2.1 as well as Llama 3.3 70B and Llama 3.1 405B scores just for comparison, sake.\n\n(I probably wont train a Shisa V2.1 Llama 3.3 8B - the Qwen 3 8B version is already great and it's Apache 2.0 licensed).\n\nhttps://preview.redd.it/6hnc71dggkag1.png?width=3126&format=png&auto=webp&s=10c4dd3231be1ed9749f174c59e8758c134d0009",
          "score": 5,
          "created_utc": "2025-12-31 16:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy05pp",
              "author": "FizzarolliAI",
              "text": "Interesting, I wonder if you'd get a noticeable regression from L3.3 70B on multilingual benches with Llama 3.1 70B then.\n\nI definitely agree that I don't think this is worth building on for most usecases. Personally I think it's an interesting artifact of the times",
              "score": 2,
              "created_utc": "2025-12-31 17:19:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwy8qrh",
                  "author": "randomfoo2",
                  "text": "Hmm, hard to say, I don't have 3.1 70B data handy... 3.3B 70B is in general pretty strong.\n\nIn practical terms, your ultimate multilingual perf is going to be pretty much up to you (tuning).  While the overall number isn't so big, when you look at the stuff we care about like JP IF, JP RP, JP TL, JP nuance, dialogue translation, we're able to get huge boosts from doing training on top of whatever model. Not show nis also our own CLTL tests that test for how many wrong-language tokens get output (huge amounts for most non-target language trained models).\n\nThe benchmark mix we use for our current multieval does feel about right. For the tasks that it's trained on, our V2.1 14B model actually \\*does\\* feel like it outperforms our V2 70B (and sometimes our V2.1 70B and V2 405B even!).\n\n\n\nhttps://preview.redd.it/hpf7vf50vkag1.png?width=3123&format=png&auto=webp&s=b5ea8273adbb61f19412f497ff3d2d06e4f46aed",
                  "score": 1,
                  "created_utc": "2025-12-31 18:01:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvndin",
          "author": "Amazing_Athlete_2265",
          "text": "Interesting. I'll evaluate it and compare",
          "score": 3,
          "created_utc": "2025-12-31 07:28:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvpi7i",
              "author": "pmttyji",
              "text": "Awesome",
              "score": 0,
              "created_utc": "2025-12-31 07:48:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww92bu",
          "author": "ilintar",
          "text": "Is the 128k version just a x16 YaRN extension or a different model?",
          "score": 3,
          "created_utc": "2025-12-31 10:53:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwdgf2",
              "author": "Awwtifishal",
              "text": "Just a config change",
              "score": 5,
              "created_utc": "2025-12-31 11:33:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxcw2i",
          "author": "noctrex",
          "text": "Generated some abliterated gguf's for it:\n\n[https://huggingface.co/noctrex/Llama-3.3-8B-Instruct-128k-abliterated-GGUF](https://huggingface.co/noctrex/Llama-3.3-8B-Instruct-128k-abliterated-GGUF)",
          "score": 3,
          "created_utc": "2025-12-31 15:23:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0u60h",
          "author": "Dangerous_Fix_5526",
          "text": "Thinking/Instruct Hybrid using Unsloth and Claude-Opus 4.6 dataset:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning)\n\nI hope I credited everyone correctly.",
          "score": 3,
          "created_utc": "2026-01-01 02:59:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwxu4p",
          "author": "Robert__Sinclair",
          "text": "I just tested it and its reasoning is extremely lacking.",
          "score": -4,
          "created_utc": "2025-12-31 14:00:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7mvuf",
      "title": "Z.ai  (the AI lab behind GLM) has officially IPO'd on the Hong Kong Stock Exchange",
      "subreddit": "LocalLLaMA",
      "url": "https://x.com/Zai_org/status/2009290783678239032",
      "author": "Old-School8916",
      "created_utc": "2026-01-08 20:23:59",
      "score": 250,
      "num_comments": 27,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q7mvuf/zai_the_ai_lab_behind_glm_has_officially_ipod_on/",
      "domain": "x.com",
      "is_self": false,
      "comments": [
        {
          "id": "nygtope",
          "author": "ForsookComparison",
          "text": "Hopefully they're all partying it up.\n\nAnd hopefully their new shareholders don't mind it if they spend millions in compute to give us free stuff ðŸ˜¬",
          "score": 38,
          "created_utc": "2026-01-08 20:37:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhjsb0",
              "author": "procgen",
              "text": "and so the enshittification begins",
              "score": 18,
              "created_utc": "2026-01-08 22:32:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyjaik6",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-01-09 04:02:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nygwihp",
          "author": "_Sneaky_Bastard_",
          "text": "they also said GLM 5 is in training. hoping it would be a open weight release.",
          "score": 63,
          "created_utc": "2026-01-08 20:49:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nygz35p",
              "author": "misterflyer",
              "text": "I think u/ForsookComparison 's half-joking comment is more fit. Once shareholders and big money are involved, they are not the same company that produced 4.5, 4.6, and 4.7 whatsoever.\n\nWe're gonna have to start looking at them as a different company.  Does that mean that they won't release anymore open weights models? *No, prob not.*  But we cannot expect them to operate the same as before... tossing out 4.5, 4.6, and 4.7 like Oprah... *\"You get a model, and you get a model, and You Get a Model, AND YOU GET A MODEL!!!\"*\n\n**They have to be able to give an ROI to their investors somehow.  And I doubt that's going to come from perpetual open weight releases.**",
              "score": 43,
              "created_utc": "2026-01-08 21:01:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyh165s",
                  "author": "snoodoodlesrevived",
                  "text": "I think that the amount of users hosting these models locally is too small to matter",
                  "score": 13,
                  "created_utc": "2026-01-08 21:10:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyh80ru",
                  "author": "danigoncalves",
                  "text": "I pay 30 dolares per year to use their flagship model. I cannot afford GPUs to run it locally so I guess they can continue to count on me as their client. Also there many companies/products where privacy is not that important because of the kind of data that might be feed to the model. I want to think that they will still BE commited with open source community",
                  "score": 4,
                  "created_utc": "2026-01-08 21:40:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyhk1yx",
                  "author": "BERLAUR",
                  "text": "At this stage investors don't care about revenue, they care about potential and future growth.Â \n\n\nThey'll have to make some money eventually but I wouldn't be worried about this as long as the AI hype is ongoing.Â ",
                  "score": 1,
                  "created_utc": "2026-01-08 22:33:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyjzv79",
                  "author": "Hobofan94",
                  "text": "> Once shareholders and big money are involved\n\nYou think they weren't up until now?",
                  "score": 0,
                  "created_utc": "2026-01-09 07:00:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyneek9",
                  "author": "DarkArtsMastery",
                  "text": "Spoken like a true NPC.",
                  "score": -1,
                  "created_utc": "2026-01-09 19:14:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nygvbc0",
          "author": "TheAncientPizza711",
          "text": "They issued shares at HK$116.20 each. Opened at HK$120 and is now currently HK$131.50.\n\nStock is up 13.17% on its 1st day. Not bad.",
          "score": 35,
          "created_utc": "2026-01-08 20:44:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhxdk2",
              "author": "robogame_dev",
              "text": "The initial price is arbitrary, if you want it to go up on day 1 price low, if you want it to go down on day 1 price high - it doesnâ€™t indicate anything about the performance of the stock, only the choice of the initial price. Normally theyâ€™ll try to estimate the market price and issue a bit below, but if they estimate wrong you get a big day 1 movement.\n\nNow the market has set the price, $131.50, movement up or down from there is meaningful - but the movement from the first issue price isnâ€™t really.",
              "score": 10,
              "created_utc": "2026-01-08 23:39:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyj7i7o",
              "author": "Hrethric",
              "text": "Minimum order 100 shares. I tried to buy 20 on a lark, but I don't have US$2000 in confidence in them. (Nor, if I'm being honest, do I have US$2000 I can reasonably gamble lol.)",
              "score": 3,
              "created_utc": "2026-01-09 03:44:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyh2044",
          "author": "FullOf_Bad_Ideas",
          "text": "Minimax IPOs a day later, 9th of January.\n\nLots of info about both of them can be found here:\n\n[Zhipu offering](https://www1.hkexnews.hk/listedco/listconews/sehk/2025/1230/2025123000017.pdf)\n\n[Minimax offering](https://www1.hkexnews.hk/listedco/listconews/sehk/2025/1231/2025123100025.pdf)\n\nSource: [this website](https://www.hkex.com.hk/Services/Trading/Securities/Trading-News/Newly-Listed-Securities?sc_lang=en)\n\nOne of the very interesting thing contained there is that Zhipu has slightly negative profit margin on GLM Coding Plan. They lose money on serving alone, not even counting in marketing or R&D costs.",
          "score": 12,
          "created_utc": "2026-01-08 21:14:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhdm9r",
              "author": "tens919382",
              "text": "Huge reason could be the insane discounts that they are offering.  I got a 1 yr plan at 60% off, and referral got 30-40% of the remaining value in credits.  So bout 75-80% off total.",
              "score": 5,
              "created_utc": "2026-01-08 22:04:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyhifgk",
                  "author": "FullOf_Bad_Ideas",
                  "text": "that too.\n\nI think submitted data captures things only for H1 2025, so it probably got even worse for them later.\n\nI think they have shaky revenue sources.\n\nMain customer seems to be Chinese government with on-premise deployment for national security. And their cloud business have very bad margins. They don't have big general consumer platforms like MiniMax has with their roleplaying site for teens.",
                  "score": 4,
                  "created_utc": "2026-01-08 22:26:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyh1ztw",
          "author": "jacek2023",
          "text": "Hype hype and still no Air",
          "score": 12,
          "created_utc": "2026-01-08 21:14:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyh3xbg",
              "author": "FullOf_Bad_Ideas",
              "text": "They barely mentioned it in their own filing\n\nAnd when they did mention it, they made a typo and called it GLM-4-Air instead of GLM-4.5-Air.\n\nIt looks like they want to focus on core offering that is already cheap enough for most.",
              "score": 7,
              "created_utc": "2026-01-08 21:22:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nygrahs",
          "author": "rookan",
          "text": "Good for them",
          "score": 1,
          "created_utc": "2026-01-08 20:26:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q694ic",
      "title": "Don't put off hardware purchases: GPUs, SSDs, and RAM are going to skyrocket in price soon",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/",
      "author": "Eisenstein",
      "created_utc": "2026-01-07 07:32:28",
      "score": 246,
      "num_comments": 232,
      "upvote_ratio": 0.81,
      "text": "In case you thought it was going to get better:\n\n**GPU** prices are going up. [AMD and NVIDIA are planning to increase prices every month starting soon.](https://www.trendforce.com/news/2026/01/05/news-nvidia-amd-reportedly-plan-price-hikes-starting-1q26-geforce-rtx-5090-may-reach-5000/)\n \n**NAND flash** contract price [went up 20% in November](https://www.trendforce.com/price/flash/flash_contract), with [further increases in December] (https://www.trendforce.com/research/download/RP251231KM). This means SSDs will be a lot more expensive soon.\n\n\n**DRAM** [prices are going to skyrocket](https://www.trendforce.com/news/2026/01/07/news-memory-shortages-reportedly-spark-csp-buying-spree-2027-supply-contracts-eyed-as-early-as-q1/), with no increase in production capacity and datacenters and OEMs competing for everything.  \n\nEven **Consoles** are [going to be delayed due to the shortages.](https://insider-gaming.com/ram-prices-next-gen/)\n\n> According to TrendForce, conventional DRAM contract prices in 1Q26 are forecast to rise 55â€“60% quarter over quarter, while server DRAM prices are projected to surge by more than 60% QoQ. Meanwhile, NAND Flash prices are expected to increase 33â€“38% QoQ\n\n[Source.](https://www.trendforce.com/news/2026/01/07/news-memory-shortages-reportedly-spark-csp-buying-spree-2027-supply-contracts-eyed-as-early-as-q1/)\n\n> Industry sources cited by Kbench believe the latest price hikes will broadly affect NVIDIAâ€™s RTX 50 series and AMDâ€™s Radeon RX 9000 lineup. The outlet adds that NVIDIAâ€™s flagship GeForce RTX 5090 could see its price climb to as high as $5,000 later in 2026.\n\n>NVIDIA is also reportedly weighing a 30% to 40% reduction in output for parts of its midrange lineup, including the RTX 5070 and RTX 5060 Ti, according to Kbench.\n\n[Source.](https://www.trendforce.com/news/2026/01/05/news-nvidia-amd-reportedly-plan-price-hikes-starting-1q26-geforce-rtx-5090-may-reach-5000/)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "ny5wcqs",
          "author": "ifupred",
          "text": "Not going to purchase at all for 3-4 years",
          "score": 424,
          "created_utc": "2026-01-07 07:48:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5xy01",
              "author": "know-your-enemy-92",
              "text": "Same, let's see what happens with smaller models and optimization. Open weights and open source will prevail in the end.",
              "score": 116,
              "created_utc": "2026-01-07 08:02:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6jort",
                  "author": "Mashic",
                  "text": "And then end up buying the server hardware when they don't need it anymore.",
                  "score": 43,
                  "created_utc": "2026-01-07 11:20:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny7zecu",
                  "author": "ashirviskas",
                  "text": "Training a random 9M (yes, M, not B) model just for fun on my laptop without a GPU atm. It is already nearly coherent! And it is not really even supposed to do language yet.\n\nIts basically like BLT, but should be pluggable onto any model. With minimal finetuning should allow any model to talk in Byte Patches. \n\nPros: Less tokens, potentially much faster. Like 2x-8x faster. Could also unlock reasoning in latent space and allow for faster solution convergence (imagine 1 meaning-rich token instead of 12 `\"wait\"; \", \"; \"and \"; ..`. Just like to think about `small, young orange cat` you don't need 5 separate tokens in your head, only the *idea* of a small, young orange cat), but I have not focused there yet. \n\nCons: Needs a bit of finetuning of the og model. Thankfully, only a few layers should theoretically be enough.",
                  "score": 8,
                  "created_utc": "2026-01-07 16:13:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny6doqd",
                  "author": "ANR2ME",
                  "text": "And these optimizations will most likely go towards Blackwell-only features ðŸ˜…",
                  "score": 19,
                  "created_utc": "2026-01-07 10:28:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny82fjw",
                  "author": "Shot_Court6370",
                  "text": "Open Source will have to flourish or everyone will be using Chinese models that don't acknowledge factual historical events.",
                  "score": -1,
                  "created_utc": "2026-01-07 16:27:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6b0wm",
              "author": "Any_Pressure4251",
              "text": "The shortage will not last that long, this has happened many times the market will adjust.",
              "score": 37,
              "created_utc": "2026-01-07 10:04:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6l40s",
                  "author": "confusedp",
                  "text": "Agree. After things are in HVM, margins for chip production is 90% plus. Things can't be this crazy for too long. The mantra of \"your margin is my opportunity\" is going to come and wipe out that margin. Companies themselves will be better off with much higher volume and smaller margins. Ramp up time for new fabs with an existing working stack is about 2 years. I expect this thing to normalize in 3-4 years. And price coming down in 5-6 years",
                  "score": 24,
                  "created_utc": "2026-01-07 11:31:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny7jh2a",
                  "author": "Freonr2",
                  "text": "At least on the DRAM front, Micron has several new fabs in the works.  I think one or two of these may come online this calendar year?  Any price relief might take a bit longer but some sunshine at the end of the rainbow maybe for 2027.\n\nhttps://www.micron.com/us-expansion",
                  "score": 3,
                  "created_utc": "2026-01-07 14:58:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyd9w58",
                  "author": "mc_nu1ll",
                  "text": "the issue is that the high prices remained high even after said \"readjustments\": do you remember what happened after the crypto crash? The GPUs kept getting more and more expensive, while scalpers drove the prices even higher. \n\nI know this is naive, but I hope this won't be the case now..",
                  "score": 2,
                  "created_utc": "2026-01-08 09:12:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6odmr",
              "author": "ravensholt",
              "text": "THIS!  \nThis is the only right thing to do.\n\nDon't listen to these bots who keep screaming that prices are going up and at the same time encourage to spend money now!",
              "score": 28,
              "created_utc": "2026-01-07 11:56:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny76f6m",
                  "author": "Expensive-Paint-9490",
                  "text": "u/Eisenstein is an OG and a very respected member of the community.",
                  "score": 8,
                  "created_utc": "2026-01-07 13:50:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyfoz7t",
                  "author": "Cheap_Image_5113",
                  "text": "I don't expect them go up substantially any more, there will be a minor price correction before end of the year and then will see the new baseline price still about 1.8x higher than it was pre AI boon.",
                  "score": 1,
                  "created_utc": "2026-01-08 17:39:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny7wqri",
                  "author": "Firm-Fix-5946",
                  "text": "right thing to do?\n\nis this somehow a moral issue to you?",
                  "score": -3,
                  "created_utc": "2026-01-07 16:01:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny7weet",
              "author": "pwnrzero",
              "text": "My 2070s will have to hold out.",
              "score": 5,
              "created_utc": "2026-01-07 15:59:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny7x3cx",
              "author": "grannyte",
              "text": "Just completed all my builds that should tide me over for the next 4 years. Hopefully nothing I don't have replacement parts for dies in he next 4 years.",
              "score": 3,
              "created_utc": "2026-01-07 16:03:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny63t9p",
              "author": "Samurai2107",
              "text": "Three to four years is hopefully how long it will take for all the newcomers (mostly chinese gpu makers) to catch up to nvidia ðŸ¤žðŸ¼",
              "score": 16,
              "created_utc": "2026-01-07 08:56:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6riym",
                  "author": "Mochila-Mochila",
                  "text": "That's a very optimistic timeframe. It'd put it closer to 10 years, in the consumer GPU department at least.\n\nBut they should release useable, \"good enough\" products before that.",
                  "score": 4,
                  "created_utc": "2026-01-07 12:19:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny90tgf",
              "author": "taoyx",
              "text": "When all these data centers are built they will come back to us.",
              "score": 1,
              "created_utc": "2026-01-07 18:59:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny90yrd",
              "author": "GraybeardTheIrate",
              "text": "Yeah I'm not playing this game. I'll buy used or not at all rather than feed into it.",
              "score": 1,
              "created_utc": "2026-01-07 18:59:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyfj1fj",
              "author": "RogerRamjet999",
              "text": "I agree 100 percent.  So sick of them scalping us every chance they get.  I'm buying nothing unless it's inline with prevailing prices 3 months ago.  As far as I'm concerned they can choke on their RAM (and hard disks, SSDs, GPUs, etc, etc) unless they price them fairly.  I can wait longer than they can.",
              "score": 1,
              "created_utc": "2026-01-08 17:13:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nygzxep",
              "author": "DeltaSqueezer",
              "text": "Yeah. I feel like I'm being forced to make a gamble. Do I buy now and pay high prices, or do I risk having to buy in the next year or two when prices may be even higher?",
              "score": 1,
              "created_utc": "2026-01-08 21:04:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny65q72",
              "author": "Terrible_Scar",
              "text": "You're doing yourself in 3-4 years. It wouldn't take a year before hardware gets priced out from the common folk",
              "score": 1,
              "created_utc": "2026-01-07 09:14:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6bomv",
                  "author": "ifupred",
                  "text": "Your not wrong. But if it's priced out for someone with my earnings it's basically end of personal computers as we know it. I hate it already",
                  "score": 25,
                  "created_utc": "2026-01-07 10:10:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6478k",
              "author": "ieatdownvotes4food",
              "text": "hell yeah, baton down the hatches.. endgame time",
              "score": -1,
              "created_utc": "2026-01-07 09:00:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny63t62",
          "author": "HornyGooner4401",
          "text": "You're like 6 months late",
          "score": 172,
          "created_utc": "2026-01-07 08:56:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6g087",
              "author": "Ok-Bill3318",
              "text": "Itâ€™s going to get much worse from here. Itâ€™s not too late to buy now if you need anything in the next 6-24 months or so",
              "score": -37,
              "created_utc": "2026-01-07 10:48:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6pegc",
                  "author": "ravensholt",
                  "text": "So let the companies suffer and let's wait 36-72 months instead... Let's wait as long as it takes.  \nI went from a 2080TI to 7900XTX.\n\nThere are basically two scenarios here:\n\nCompanies do a 180, and figure out people are smart and unwilling to blindly invest as if having a graphics card is like subscribing to f\\*cking netflix.\n\nor...\n\nIt's the end of personal computing as we now it.  \nFine, we accept it and we still saved money for 3-4 years instead of boosting the \"AI bubble\".",
                  "score": 24,
                  "created_utc": "2026-01-07 12:03:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny5w205",
          "author": "DeltaSqueezer",
          "text": "They are already expensive. Prices are currently 3x for SSD what I paid in the middle of last year. DRAM is 4x.",
          "score": 101,
          "created_utc": "2026-01-07 07:45:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny7it97",
              "author": "mycall",
              "text": "2026: Hold my silicon.",
              "score": 18,
              "created_utc": "2026-01-07 14:55:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny5whu7",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -9,
              "created_utc": "2026-01-07 07:49:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny5y4b0",
                  "author": "know-your-enemy-92",
                  "text": "How about 4 years?",
                  "score": 10,
                  "created_utc": "2026-01-07 08:04:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6g22i",
              "author": "Ok-Bill3318",
              "text": "The full impact of this ai demand hasnâ€™t fully hit yet.",
              "score": -12,
              "created_utc": "2026-01-07 10:49:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6pk3t",
                  "author": "ravensholt",
                  "text": "Let me guess, you bought stock options in nGreedia, right?",
                  "score": 5,
                  "created_utc": "2026-01-07 12:04:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny6697c",
          "author": "Dorkits",
          "text": "Me looking at my PC : bro don't die please",
          "score": 80,
          "created_utc": "2026-01-07 09:20:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6vf3n",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2026-01-07 12:45:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6ztdm",
                  "author": "RateRoutine2268",
                  "text": "i undervolted to a level where my PC is now generating power",
                  "score": 7,
                  "created_utc": "2026-01-07 13:12:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny70o80",
              "author": "AnomalyNexus",
              "text": "My 3090 trips PSU protection if I donâ€™t powerlimit it. Feels like it is half to gpu Heaven already but man would suck to buy a 3090 replacement rn. No good moves from there that arenâ€™t stupidly expensive or a vram downgrade",
              "score": 0,
              "created_utc": "2026-01-07 13:17:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7gu3b",
                  "author": "wadrasil",
                  "text": "You need a better PSU.",
                  "score": 14,
                  "created_utc": "2026-01-07 14:45:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny6appy",
          "author": "FullOf_Bad_Ideas",
          "text": "What will happen to Mac Studio 512GB, RTX 6000 Pro and Ryzen 395+ AI Max 128GB prices?\n\nEdit: this source is BS. Memory chips don't make 80% of BOM, no way.",
          "score": 26,
          "created_utc": "2026-01-07 10:01:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6hxdp",
          "author": "a_beautiful_rhind",
          "text": "They already surged. I was looking for more DDR4 and it went through the roof. CPUs not any cheaper, motherboads not any cheaper. If anything I would replace my 2080ti with another 3090, but they are still $700. \n\nI thought that I'd eventually want some ada/blackwell. None of those prices have been affordable for >20gb, even before. Only the 32gb super looked \"good\" around $1000. Since middle of the year I'm fucked by tariffs.\n\nStorage already went up. $60 2TB ssd dried up. All that was left from middle 2025 was getting a controller card and SaS drives. Was forced to buy last expansion drive as spinning rust. \n\nFinally, more of my income is going to taxes and bills so I can't really blow money on LLM stuff.",
          "score": 25,
          "created_utc": "2026-01-07 11:05:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6xovq",
              "author": "Sufficient-Past-9722",
              "text": "Yeah I just bought a 4TB T700 for \"only\" $450 and feel somehow lucky.",
              "score": 2,
              "created_utc": "2026-01-07 12:59:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7q2tm",
                  "author": "Fit_Case_03",
                  "text": "Holy shit. I just spent $900 on four SN 850X 4TB SSD and I felt that was a rip off at the time. That was 2 months ago as well....",
                  "score": 3,
                  "created_utc": "2026-01-07 15:30:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyl55gr",
                  "author": "NoImplement2856",
                  "text": "You overpaid.",
                  "score": 0,
                  "created_utc": "2026-01-09 12:47:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny7ghkw",
              "author": "Mkengine",
              "text": "I bought 3x MI50s, 64 GB RAM and 1 TB SSD for around $550 wenn they were cheapest, but have them still laying around. I have to take the time to finally build this server before the other stuff gets too expensive as well...",
              "score": 2,
              "created_utc": "2026-01-07 14:43:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6nyag",
          "author": "ab2377",
          "text": "it's pretty crazy that just a few people have got so much money that they can decide to consume whole planets worth of computing resources.",
          "score": 26,
          "created_utc": "2026-01-07 11:53:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyd70fy",
              "author": "BinaryLoopInPlace",
              "text": "Reddit's cartoonish mental model of how the world works will never cease to amuse me",
              "score": -3,
              "created_utc": "2026-01-08 08:46:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nydtwer",
                  "author": "DRM_is_Hell",
                  "text": "He made a factual statement. What are you talking about?",
                  "score": 7,
                  "created_utc": "2026-01-08 12:02:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny6c75j",
          "author": "RnRau",
          "text": "I think the OP is working for Micron or one of the big electronic retailers :)",
          "score": 23,
          "created_utc": "2026-01-07 10:15:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6d4v8",
          "author": "StandardLovers",
          "text": "I would never have guessed that my best investment ever would be for â€™128GB Kingston 6000 cl36 DDR5 Ramâ€™ bought for ca. 450usd in spring 2025.",
          "score": 17,
          "created_utc": "2026-01-07 10:23:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny72i25",
              "author": "lordofblack23",
              "text": "I bought 128Gb of ddr4 in 2024 for $275 and i felt like a rube.  My Google stock still outperformed.",
              "score": 5,
              "created_utc": "2026-01-07 13:28:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny5zyfd",
          "author": "Free-Internet1981",
          "text": "Nice try huge corporations before the bubble bursts, i've seen the data, you guys are plateauing ðŸ˜’",
          "score": 70,
          "created_utc": "2026-01-07 08:20:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6346k",
              "author": "eidrag",
              "text": "OP actually need to offload his hardware order stock, nice try /scalper",
              "score": 28,
              "created_utc": "2026-01-07 08:50:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6pn78",
                  "author": "ravensholt",
                  "text": "OP has stock options in nGreedia , or as you say, is a scalper.",
                  "score": 2,
                  "created_utc": "2026-01-07 12:05:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny7avmm",
              "author": "nixed9",
              "text": "They arenâ€™t going anywhere because they are backed by the United States government through military contracts\n\nThis is the *beginning* of the new normal, not the end of it. \n\nAll compute will go to the AI systems that enslave us. Local compute will become less and less available and more expensive.",
              "score": 6,
              "created_utc": "2026-01-07 14:14:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny5vcbi",
          "author": "rnyaoyao",
          "text": "The surge in memory and storage prices is purely because OpenAI overâ€‘purchased 40% of wafer capacity just to secure Apple's order. Now it has failed, and the result is obvious.",
          "score": 54,
          "created_utc": "2026-01-07 07:39:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5yn15",
              "author": "f1rn",
              "text": "I knew OpenAi went in some kind of panic mode. But it was against Apple? I thought Google with the TPUs? \nAnd why has it failed? Feels like Iâ€™m out of the loop here.",
              "score": 27,
              "created_utc": "2026-01-07 08:08:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny5zlyp",
                  "author": "SpeedOfSound343",
                  "text": "I think op is speculating that OpenAI had their eyes on the Apple order for Siri that went to Google.",
                  "score": 39,
                  "created_utc": "2026-01-07 08:17:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nykgc3x",
                  "author": "mrjackspade",
                  "text": "> But it was against Apple?\n\nIt can be for whatever reason you want it to be as long as you're willing to make shit up on the internet ðŸŒˆ\n\nThats what everyone else is doing anyways",
                  "score": 1,
                  "created_utc": "2026-01-09 09:27:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny7t15f",
              "author": "DiscombobulatedAdmin",
              "text": "How has it failed?",
              "score": 3,
              "created_utc": "2026-01-07 15:44:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6f1ys",
          "author": "aeroumbria",
          "text": "I will not be baited I will not be baited I will not be baited...",
          "score": 6,
          "created_utc": "2026-01-07 10:40:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6i6bi",
          "author": "nenulenu",
          "text": "No thanks",
          "score": 7,
          "created_utc": "2026-01-07 11:07:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny61vhl",
          "author": "OneOnOne6211",
          "text": "I am so glad I bought a new computer in 2024.",
          "score": 14,
          "created_utc": "2026-01-07 08:38:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6lvni",
              "author": "Nyghtbynger",
              "text": "Same bro. I just regret not buying the 64gigs of ram. In fact I wanted to buy 2x48GB but I couldn't find an affordable option and chose to wait on year or two so it becomes more mainstream lol",
              "score": 10,
              "created_utc": "2026-01-07 11:37:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7w3qi",
                  "author": "OneOnOne6211",
                  "text": "I first bought 32gb and then a month or two later I bought another 32gb. So luckily, I did.\n\nIt's funny though, the biggest reason I bought a new computer in 2024 specifically is because I was playing a lot of Skyrim Special Edition at the time (heavily modded). And my computer was just unable to handle it. If I wasn't in an interior it was nearly unplayable. I'm talking like constant stuttering, 3 frames a second unplayable. So I was sick of that, so I bought a new PC that would be able to run it.\n\nSo thank you, Skyrim. Cuz otherwise I probably would've waited another year or two.",
                  "score": 2,
                  "created_utc": "2026-01-07 15:58:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6mcp4",
              "author": "AlwaysLateToThaParty",
              "text": "I'm so glad I built a good computer in 2019.",
              "score": 3,
              "created_utc": "2026-01-07 11:41:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7vz0a",
                  "author": "OneOnOne6211",
                  "text": "I mean, I built mine too. But I bought the parts all together in 2024.",
                  "score": 2,
                  "created_utc": "2026-01-07 15:58:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny64lrz",
          "author": "Kal-LZ",
          "text": "I saw new stock of PNY RTX 5090 for 5200â‚¬ on Amazon. I guess this will also affect to RTX PRO 6000",
          "score": 6,
          "created_utc": "2026-01-07 09:04:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6cc8c",
              "author": "FullOf_Bad_Ideas",
              "text": "Caseking still shows many SKUs from various AIBs in stock around 3100-3500 euro. I think those cards that are listed for 5000 euro are anomalies. TSMC didn't stop making chips, and 32gb of VRAM didn't jump in price this much yet to warrant that price.",
              "score": 5,
              "created_utc": "2026-01-07 10:16:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6i0jh",
                  "author": "a_beautiful_rhind",
                  "text": "They will dick us just because they can.",
                  "score": 7,
                  "created_utc": "2026-01-07 11:06:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny6oabp",
          "author": "zeoNoeN",
          "text": "Im betting on the Datacenter Bubble/Build up slowing down at some point in the next 2-3 years, which will make consumer markets relevant again. No upgrades until then. Letâ€™s see if this ages like milk or wine.",
          "score": 4,
          "created_utc": "2026-01-07 11:55:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyd6zfu",
              "author": "Doug_Fripon",
              "text": "They speculate on your inability to afford new hardware in the next years and the build-up of your habit to purchase compute as a service. They'll try to lock the hardware industry from now on, until it becomes true. The business model of Western AI companies only makes sense if you have no alternative as a consumer, so it only works with very expensive consumer hardware.\n\nYou don't need to own any hardware. A lot of companies will be happy to sell you their monthly subscription. Microsoft will gift you a Winbook for your 24 months Windows platinium commitment, and that's all you'll need.\n\nWe can hope for a Chinese consumer compute industry structuring by 2030 and blowing up this nonsense.",
              "score": 2,
              "created_utc": "2026-01-08 08:45:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny8igz8",
          "author": "HanzJWermhat",
          "text": "Iâ€™ll wait for the AI crash. \n\nLLM improvements havenâ€™t been meeting milestone expectations. We were told parabolic growth but tech is clearly hitting diminishing returns. Next SOTA models will be the bellweather. If they canâ€™t meaningfully improve I think weâ€™ll see a lot of companies majorly pull back.",
          "score": 6,
          "created_utc": "2026-01-07 17:39:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny68qd8",
          "author": "Elaughter01",
          "text": "Purchase? Hell no. I'm waiting 5 years or gonna start buying used systems instead.Â ",
          "score": 11,
          "created_utc": "2026-01-07 09:43:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6iq2v",
          "author": "Anyusername7294",
          "text": "!Remindme 40 months",
          "score": 10,
          "created_utc": "2026-01-07 11:12:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6itki",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 3 years on [**2029-05-07 11:12:09 UTC**](http://www.wolframalpha.com/input/?i=2029-05-07%2011:12:09%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/ny6iq2v/?context=3)\n\n[**5 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1q694ic%2Fdont_put_off_hardware_purchases_gpus_ssds_and_ram%2Fny6iq2v%2F%5D%0A%0ARemindMe%21%202029-05-07%2011%3A12%3A09%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q694ic)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 4,
              "created_utc": "2026-01-07 11:12:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6jm3p",
          "author": "tirolerben",
          "text": "Prices are already going vertical. You canâ€˜t skyrocket more than 90Â°, else you are going down again.",
          "score": 5,
          "created_utc": "2026-01-07 11:19:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny67xh6",
          "author": "NebulousNitrate",
          "text": "Guarantee there is going to be a push for thin clients that use cloud subscriptions. PC enthusiasts can wait it out, small businesses and regular consumers cannot.",
          "score": 9,
          "created_utc": "2026-01-07 09:35:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6vdc5",
              "author": "silenceimpaired",
              "text": "You will own nothing (but thin clients) and be happy (or you wonâ€™t be able to play games, video edit, chat with AI, etc.)",
              "score": 6,
              "created_utc": "2026-01-07 12:45:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny5wifz",
          "author": "JuliusCeaserBoneHead",
          "text": "One day this whole thing will come crashing down. Wonâ€™t hold my breath for it but this stupid thing canâ€™t be sustainableÂ ",
          "score": 23,
          "created_utc": "2026-01-07 07:49:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny68ne5",
              "author": "joninco",
              "text": "Or worseâ€¦a new norm.",
              "score": 12,
              "created_utc": "2026-01-07 09:42:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7p1pc",
                  "author": "mycall",
                  "text": "Many crashes is chaos",
                  "score": 3,
                  "created_utc": "2026-01-07 15:25:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyaeh8v",
                  "author": "Exciting_Garden2535",
                  "text": "Right now, there is a huge margin between cost and price, close to the illegal drugs market margin. This margin will never be the norm, just because producing memory is legal, not illegal. So the production will grow until the margin does not become just tiny, due to all demands being fulfilled.",
                  "score": 2,
                  "created_utc": "2026-01-07 22:35:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny753fy",
              "author": "Direct_Turn_1484",
              "text": "Yeah and I donâ€™t plan on funding making it worse in the meantime.",
              "score": 2,
              "created_utc": "2026-01-07 13:43:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny6g7oh",
              "author": "Ok-Bill3318",
              "text": "Citation required for the last time in history hardware prices came â€œcrashing downâ€",
              "score": 3,
              "created_utc": "2026-01-07 10:50:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6ibr3",
                  "author": "Skystunt",
                  "text": "Rtx 3090 from Â£3000 to Â£450. In 2022 i was like â€œiâ€™ll be rich if i own this cardâ€ and now itâ€™s really affordable on ebay",
                  "score": 10,
                  "created_utc": "2026-01-07 11:08:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny9zb09",
              "author": "datbackup",
              "text": "Hey youâ€™re talking about the internet right?",
              "score": 1,
              "created_utc": "2026-01-07 21:28:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6o9ug",
          "author": "ravensholt",
          "text": "Ever heard about the story \"Peter and the wolf\" ?\n\n  \nEveryone keeps screaming \"Prices are going up! Prices are going up!\"  \nAnd in the same sentence, \"Buy now! Buy now!\".\n\n  \n*Smart people wait*, and **keep their money in their pockets**.  \n*Speak with your wallet.*  \nThat's the only language corporations understand.  \nThe only way to **fight** this **inflation**, is with our money - **don't go out and spend** now to boost the companies economy.",
          "score": 19,
          "created_utc": "2026-01-07 11:55:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nya5q27",
              "author": "1998marcom",
              "text": "No, sorry, but it's not that useful to fight inflation by not spending, when the fed is printing more money and lending it to the corps that are buying the chips. The fed will continue to rob you of your purchasing power and give it to the big corporations (or the state, but the result is the same, as the banks that would have bought the state bonds now simply use the same funds to open loans to the corps)",
              "score": 1,
              "created_utc": "2026-01-07 21:56:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nydufo6",
                  "author": "DRM_is_Hell",
                  "text": "Inflation keeps getting worse because of your mindset.\n\n\nDid it ever occur to you that in order for those mega corporations to have so much money at the first place, they needed us to obtain it? Without us, they're all nothing.",
                  "score": 1,
                  "created_utc": "2026-01-08 12:06:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6zdxb",
              "author": "discreetwhisper1",
              "text": "Ai comment",
              "score": -10,
              "created_utc": "2026-01-07 13:10:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny70528",
                  "author": "ravensholt",
                  "text": "Sorry to disappoint you.  \n  \nIt's simply just a well formatted comment of my very own.  \n  \nI understand, that it might seem difficult for someone like you to accept that there are indeed people out there capable of thinking themselves.",
                  "score": 15,
                  "created_utc": "2026-01-07 13:14:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny5yukh",
          "author": "SandboChang",
          "text": "It's already too late NOW, not later.",
          "score": 14,
          "created_utc": "2026-01-07 08:10:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6fxtu",
          "author": "Ok-Bill3318",
          "text": "Already refreshed all my stuff through 2025. Good luck people!",
          "score": 4,
          "created_utc": "2026-01-07 10:48:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6ytwe",
          "author": "Echoplanar_Reticulum",
          "text": "Definitely put off purchasing. Weâ€™re in the bubble, and these AI companies will need to show earnings soon.",
          "score": 3,
          "created_utc": "2026-01-07 13:06:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6m56m",
          "author": "AlwaysLateToThaParty",
          "text": "> GPUs, SSDs, and RAM are going to *continue to* skyrocket in price ~~soon~~\n\nFTFY",
          "score": 3,
          "created_utc": "2026-01-07 11:39:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6qcum",
          "author": "ravensholt",
          "text": "!Remindme 36 months.",
          "score": 3,
          "created_utc": "2026-01-07 12:10:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7fu76",
          "author": "DrDisintegrator",
          "text": "in the future nVidia and OpenAI has planned, they rent you an AI by the question....",
          "score": 3,
          "created_utc": "2026-01-07 14:40:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8au41",
          "author": "CBHawk",
          "text": "Jokes on you, I'm waiting for DDR6!",
          "score": 3,
          "created_utc": "2026-01-07 17:05:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5us4e",
          "author": "NoChard1199",
          "text": "Well that's just fantastic, literally bought everything EXCEPT a new GPU last month thinking I'd wait for the 5080 reviews\n\n  \nGuess I'm stuck with my 3070 until 2027 or selling a kidney",
          "score": 9,
          "created_utc": "2026-01-07 07:34:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5w6v5",
              "author": "supportkeinmord",
              "text": "Is the kidney in good condition? Do you have pictures?",
              "score": 29,
              "created_utc": "2026-01-07 07:46:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny64nld",
                  "author": "boraam",
                  "text": "Same here. Don't mind stocking up on critical parts.",
                  "score": 11,
                  "created_utc": "2026-01-07 09:04:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6gvws",
              "author": "olmoscd",
              "text": "5080 reviews? those were published a year ago",
              "score": 5,
              "created_utc": "2026-01-07 10:56:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6osst",
                  "author": "Mochila-Mochila",
                  "text": "Yeah that explanation for the delayed buying doesn't make sense.",
                  "score": 3,
                  "created_utc": "2026-01-07 11:59:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6aik0",
              "author": "ericek111",
              "text": "I'm on an RX 6700 XT (so \\~3070 performance) I bought second-hand for 250 â‚¬. What could I upgrade to? The RX 9060 XT is 400 â‚¬, RX 9070 is 650 â‚¬ (XT at 50 â‚¬ more). From the green market, only the RTX 5060 Ti 16 GB makes sense, at 450 â‚¬.\n\nI know prices are supposed to surge, but paying twice as much for a 30 % improvement after 5 years? Meh.",
              "score": 2,
              "created_utc": "2026-01-07 09:59:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6m2qe",
                  "author": "Nyghtbynger",
                  "text": "I use a 7800XT locally, a 9060XT would be fine for most use (except some kind of training that don't use llama.cpp, but you can rent a GPU for that). The larger VRAMs options are unafffordable. Maybe the Radeon Pro with 32 gigs of ram could be nice too",
                  "score": 2,
                  "created_utc": "2026-01-07 11:39:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6q81y",
              "author": "ravensholt",
              "text": "Hang in there ... Keeping your money in your pocket is the right decision.",
              "score": 2,
              "created_utc": "2026-01-07 12:09:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nydup57",
              "author": "DRM_is_Hell",
              "text": "The reviews been out for ~11 months now.",
              "score": 1,
              "created_utc": "2026-01-08 12:08:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny659m4",
          "author": "noatoms",
          "text": "Soon? Brother in Christ it has already happened!",
          "score": 5,
          "created_utc": "2026-01-07 09:10:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6lofo",
          "author": "Nyghtbynger",
          "text": "Hmm, media information is often late. Here are my few cents : I wanted to purchase a SSD to store models and all, but finally I will settle for a hard disk that is way cheaper and write some script with the AI to cache/rotate the files or whatever. I have more time than moeny right now",
          "score": 2,
          "created_utc": "2026-01-07 11:36:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6sbtf",
          "author": "sinchiyap",
          "text": "Looking at the price of everything now Iâ€™m most probably going to hold on to my 3060 12GB for another 3 more years",
          "score": 2,
          "created_utc": "2026-01-07 12:24:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7eblw",
          "author": "PidgeyPower",
          "text": ">According to TrendForce, conventional DRAM contract prices in 1Q26 are forecast to rise 55â€“60% quarter over quarter\n\nPrice could drop and it would still probably be near a 55-60% rise quarter over quarter. It is comparing averages of prices in the final three months of 2025. Where we stand today is probably 55-60% higher than the average of the last three months.\n\nThereâ€™s some things further down the chain like consoles and cell phones that you could say are going up. The rest of this stuff has already moved.Â ",
          "score": 2,
          "created_utc": "2026-01-07 14:32:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7izm8",
          "author": "Tall_East_9738",
          "text": "Everyone keeps repeating that yet I can currently buy a 5070ti for the same price I paid in August. Yâ€™all need to stop falling for this FOMO.",
          "score": 2,
          "created_utc": "2026-01-07 14:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydv2qe",
              "author": "DRM_is_Hell",
              "text": "It's FOMO, yes. But 5070ti has gone up in price since August.",
              "score": 1,
              "created_utc": "2026-01-08 12:10:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyg18sm",
                  "author": "Tall_East_9738",
                  "text": "I'm looking at the amazon page, it's the exact same price and still in stock.",
                  "score": 1,
                  "created_utc": "2026-01-08 18:32:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyohv32",
                  "author": "feenixOmlette",
                  "text": "5070ti is old stock that has no use in ML.\n\n\nYou can still get 12 even 16gb cards.\n\n24 and 32 however is insane pricing because those sizes of ram compete with datacenters memory and can even run decent local ML.",
                  "score": 1,
                  "created_utc": "2026-01-09 22:16:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny7prjf",
          "author": "cicoles",
          "text": "Nah. Iâ€™m happy renting and watching over-investments going broke later. Better put the money in bank shares, those greedy bankers gets bailed out every single time they mess up.",
          "score": 2,
          "created_utc": "2026-01-07 15:29:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7pvdc",
          "author": "Noeyiax",
          "text": "Shareholders bruh, what about the shareholders and their passive income!! Oh nooooo\n\nJust wait a few years ðŸ™‚â€â†•ï¸, this scenario in business is similar to infrastructure advancement like railway, oil, dotcom... The only solution is for the wealthy to make sacrifices.\n\nBut we all know our wealthy overlords are kinda dog shit people, so they'll just blame us poor people lmfao",
          "score": 2,
          "created_utc": "2026-01-07 15:29:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7pxta",
          "author": "Sitheral",
          "text": "Don't care, ain't buying shit.\n\nAnd they better develop games optimized because I have backlog for more than one lifetime and plenty of other stuff to spend money to.",
          "score": 2,
          "created_utc": "2026-01-07 15:30:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8a0g8",
          "author": "Proof_Scene_9281",
          "text": "Do they expect to sell any!? Sheesh. 5090â€™s were 2500$ 3 weeks ago, now theyâ€™re $3500!!??",
          "score": 2,
          "created_utc": "2026-01-07 17:01:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8e42p",
          "author": "hawseepoo",
          "text": "No thanks lol. Iâ€™m waiting until the market crashes and picking up stuff for half of last yearâ€™s MSRP",
          "score": 2,
          "created_utc": "2026-01-07 17:19:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycoqok",
          "author": "DonkeyBonked",
          "text": "I just bought the hardware to build a AMD EPYC 7502 server, 128GB DDR4, and 4x RTX 3090 24GB.\n\nI'm going to use this and ride it out until better and more efficient hardware starts to get decommissioned.\n\nI'll never pay current hardware ransom. I have 4x 4TB NVMe that I purchased for under $200 each, and that should last me until prices stabilize, and I don't care if it's 4-5 years before that happens. Eventually these data centers will need to upgrade and flood markets with the hardware they've been buying for the last 3 years, so the time will come when this bubble pops.\n\nUntil then, open-source is becoming really efficient, with rapid improvements, and Intel is putting out AI hardware while technology is shifting, so I expect this current trend is unsustainable, especially once companies like OpenAI launch their enshittification IPO that means they will start needing to put all the RAM they're hoarding to use.\n\nThere's also a lot of unsold RAM wafer processing that is now being urgently put to use, while people are getting creative about how they use what resources we have.\n\nLet the gluttons have their cake, if I wasn't ever going yo pay 3k for a GPU, I'm never going to pay 5k, so it won't impact me either way.",
          "score": 2,
          "created_utc": "2026-01-08 06:10:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfnj37",
          "author": "Cheap_Image_5113",
          "text": "Brave of you to assume I was going to buy any hardware at all.",
          "score": 2,
          "created_utc": "2026-01-08 17:33:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5x7vh",
          "author": "sunshinecheung",
          "text": "Nah, i can use it free through their official website, and use small models for sensitive content",
          "score": 4,
          "created_utc": "2026-01-07 07:56:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny66lmn",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 3,
          "created_utc": "2026-01-07 09:23:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny69d2m",
              "author": "jay-aay-ess-ohh-enn",
              "text": "Ah... So this is all **your** fault.",
              "score": 7,
              "created_utc": "2026-01-07 09:49:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny7slb1",
          "author": "CrypticZombies",
          "text": "Op must be fun at parties",
          "score": 2,
          "created_utc": "2026-01-07 15:42:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6zwic",
          "author": "Timotey27",
          "text": "Lol at all these innocent summer children in here who think the situation will go back to normal in a few years. It's done. In the future gaming computers will no longer be affordable for the average person.",
          "score": 2,
          "created_utc": "2026-01-07 13:13:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny8usjk",
              "author": "Taki_Minase",
              "text": "A 386 used to be $40000, so much nonsense being said these days.",
              "score": 3,
              "created_utc": "2026-01-07 18:33:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny83668",
              "author": "PlaceboByProxy",
              "text": "RemindMe! 3 years",
              "score": 1,
              "created_utc": "2026-01-07 16:30:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny83uab",
              "author": "PlaceboByProxy",
              "text": "!RemindMe 3 years",
              "score": 1,
              "created_utc": "2026-01-07 16:33:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6tjqq",
          "author": "eli_pizza",
          "text": "Nobody knows though. If they did then prices would have already spiked.",
          "score": 1,
          "created_utc": "2026-01-07 12:33:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6zutx",
          "author": "Lordxb",
          "text": "Posts like this contribute to panic buying leading to increase in prices!!",
          "score": 1,
          "created_utc": "2026-01-07 13:13:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny710i4",
          "author": "TurtleNamedMyrtle",
          "text": "I want a 5090. It might be time to hang up my two 1080Tiâ€™s (with the SLI bridge). Talk me out of it?",
          "score": 1,
          "created_utc": "2026-01-07 13:19:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny74u6r",
              "author": "StorageHungry8380",
              "text": "Now or never, I suspect. I bought a 5070Ti at launch but when the RAM hit the fan I traded it for a 5090. Ended up paying a bit more than if I had just bought a 5090 last summer, but here they're already going out of stock and those that are left are way up in price.",
              "score": 1,
              "created_utc": "2026-01-07 13:41:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny71s9l",
          "author": "AnomalyNexus",
          "text": "Glad I built a homeserver with a couple TB of flash half a year back",
          "score": 1,
          "created_utc": "2026-01-07 13:24:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny73e44",
          "author": "jabblack",
          "text": "So what is Nvidia going to do with all the lower binned AI GPUs if theyâ€™re not going to make consumer GPUs?",
          "score": 1,
          "created_utc": "2026-01-07 13:33:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyoibvo",
              "author": "feenixOmlette",
              "text": "The main binning is based on cores, and cores are still cheap. The problem is the memory.",
              "score": 1,
              "created_utc": "2026-01-09 22:19:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny74eqx",
          "author": "PlasticTourist6527",
          "text": "WD and Seagate stocks are already up, which stocks do you suggest buying to counter this?",
          "score": 1,
          "created_utc": "2026-01-07 13:39:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7cy7f",
          "author": "Electronic_Status_60",
          "text": "Scooping a 5070 ti today",
          "score": 1,
          "created_utc": "2026-01-07 14:25:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7e3cc",
          "author": "PotaroMax",
          "text": "So... is this the AI winter we all feared?",
          "score": 1,
          "created_utc": "2026-01-07 14:31:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7mn5n",
          "author": "Massive-Question-550",
          "text": "Seriously considering downgrading to my old ddr4 pc and 6x ing my 64gb ddr5... Maybe I'll sell my 4tb ssd too.Â ",
          "score": 1,
          "created_utc": "2026-01-07 15:14:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7n5m1",
          "author": "Gipetto",
          "text": "Yeah, already happening. CPUs as well. I figured since they were reasonably priced Iâ€™d upgrade my old AM4 machine to a 5900XT or 5950X in the new year. Theyâ€™ve recently taken a $70 jump in price on Amazon, more on other sites, so, yeah, naw, I guess Iâ€™m riding this out on the current setup.",
          "score": 1,
          "created_utc": "2026-01-07 15:16:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7tip3",
          "author": "etralse",
          "text": "Soon? This started already.",
          "score": 1,
          "created_utc": "2026-01-07 15:46:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny86a3j",
          "author": "arentol",
          "text": "With Nvidia cutting back production of consumer GPUs by 40%, and AMD and Intel not yet ready to take up that slack, of course GPU prices will skyrocket. Then, even after AMD and Intel ramp up production, prices will remain much higher than previous. \n\n18 months from now AMD will have gone from $26 billion in revenue (2024) to $60 billion+ (2027) (Nvidia made $60 billion in 2024 btw), based on this complete fumble by Nvidia... Nvidia meanwhile will make only 2 to 4 billion more in revenue on their GPU capacity moved to focus on AI chips. So they are going to basically double their biggest competitors revenue, triple that companies profits, and give that company an insane boost to its R&D budget for AI chips, all so they can make a negligible amount of additional revenue that will not help them stay ahead of AMD in the slightest. \n\nNvidia will be fine, but 5-8 years from now instead of having 90% of the AI market share that they would have if they stayed in the consumer GPU business they will have maybe 75-80%, because AMD will have competitive products thanks to this massive boost in revenue and profits Nvidia is gifting AMD.",
          "score": 1,
          "created_utc": "2026-01-07 16:44:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8jeek",
          "author": "warnerbell",
          "text": "This is rough timing. Just when local inference was getting accessible, hardware costs are about to spike.\n\nOn the bright side, this makes efficiency optimization more valuable. Context window management, quantization, prompt architecture all the stuff that squeezes more out of existing hardware becomes critical.\n\nDoubling down on software-side optimizations while hardware gets expensive could be beneficial",
          "score": 1,
          "created_utc": "2026-01-07 17:43:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny8vlgv",
              "author": "Taki_Minase",
              "text": "I agree.",
              "score": 1,
              "created_utc": "2026-01-07 18:36:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyoimsf",
              "author": "feenixOmlette",
              "text": "The worst nightmare of all AI companies is that you can do the inference just as well on a small local machine that you own for the price of electricity.",
              "score": 1,
              "created_utc": "2026-01-09 22:20:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny92b59",
          "author": "IonizedHydration",
          "text": "just glad i upgraded from 64 to 128 ram about 6 months ago.",
          "score": 1,
          "created_utc": "2026-01-07 19:05:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny944v1",
          "author": "AmazinglyNatural6545",
          "text": "Oh come on. You're a few months late buddy. \nThat's why I decided to bite the bullet and updated the hardware in Nov.",
          "score": 1,
          "created_utc": "2026-01-07 19:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9ep04",
          "author": "RenewAi",
          "text": "aww man",
          "score": 1,
          "created_utc": "2026-01-07 19:59:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9k5yu",
          "author": "Hybridxx9018",
          "text": "People made fun of me for paying $200 for 46gb of ram during Black Friday lol. Sometimes you gotta bite the bullet.",
          "score": 1,
          "created_utc": "2026-01-07 20:23:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9tb1n",
          "author": "AvocadoArray",
          "text": "I bought a bunch of different RAM on eBay in June to upgrade my home lab servers and work/gaming PCs and compared prices today. Here's what I'm seeing so far:\n\n* DDR5 32GB non-ECC (new): $70/ea -> $319/ea (+355%)\n* DDR4 32GB ECC: $34.20/ea -> $130/ea (+280%)\n* DDR4 8GB ECC: $6/ea -> $30/ea (+400%)\n* DDR3 8GB ECC: $5/ea -> $4/ea **(-20%)**\n\nSo yeah, the fast DDR4/DDR5 is in high demand, while the older DDR3 stuff is actually declining in price. For general purpose day-to-day computers or servers, most people would never see the difference in DDR3/4/5, but there's another elephant in the room when it comes to these price trends.\n\n**Windows 10 hit EOL in October**, and Windows 11 only officially supports CPUs that run on DDR4+. The decision to drop support for older perfectly usable CPUs Thanos'd a huge chunk of consumer computers out of the market, and now those replacements are also competing for the higher priced DDR4/5 chips.\n\nFor general purpose computing and servers, you'll save a ton of money if you run an older DDR3 system on Linux (or bypassing W11's system requirements during installation). And for AI inference, if you're running your entire model in VRAM anyway, DDR3 will do just fine in most cases.\n\nIf I were building a new AI inference server today, I'd consider something like a 4U PowerEdge r920. 4x CPUs, 4x 1100w PSUs, 6x PCI 3.0 x16 slots + 2x x8 slots and as much RAM as your heart desires for <$1,000. Then load up whatever GPUs fit your needs.\n\nSure, it's not ideal, but the price/performance value of this older hardware is starting to look more attractive every day and the market is already flooded with inventory.",
          "score": 1,
          "created_utc": "2026-01-07 21:03:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyddw5q",
              "author": "Eisenstein",
              "text": "The problem with the DDR3 servers is that they are running Xeon E V2s, which don't have AVX2.",
              "score": 1,
              "created_utc": "2026-01-08 09:49:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyeztxv",
                  "author": "AvocadoArray",
                  "text": "Good point, although that should only affect CPU-inference right? If the entire model fits in the VRAM pool then does it make a difference?\n\nI have an r720xd w/ E5-2667 v2s running a GTX 1080 on llama.cpp just fine, although VLLM fails because they don't support Pascal.",
                  "score": 1,
                  "created_utc": "2026-01-08 15:49:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nya6cpi",
          "author": "johnboi1323",
          "text": "Lol built my server in January to avoid this. prices have already skyrocketed. but yes only gonna get worse.",
          "score": 1,
          "created_utc": "2026-01-07 21:59:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nya73z4",
          "author": "Clear-Ad-9312",
          "text": "People already can't afford to buy, we are stuck with the old parts as datacenters gobble up all the supply. Well I hope, in 3-5 years time, the 3rd party commercial equipment sales are going to be crazy af.",
          "score": 1,
          "created_utc": "2026-01-07 22:02:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyb1bfn",
          "author": "Rich_Artist_8327",
          "text": "Luccky me, bought hardware worth of 30k just before the shit show. 5090 1700â‚¬ \ndc3000me 15tb nvme 1200â‚¬ \n32gb ddr5 ecc udimms 110â‚¬ \nmany.",
          "score": 1,
          "created_utc": "2026-01-08 00:29:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyc010v",
          "author": "meshreplacer",
          "text": "Just buy a Mac Studio. When the AI bubble implodes there will be a glut of cheap server hardware to run local LLMs cheap",
          "score": 1,
          "created_utc": "2026-01-08 03:31:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyojakz",
              "author": "feenixOmlette",
              "text": "This is true, either AI keeps getting better and better and they need to replace the old ML hardware with the latest which means cheap second hand h100s\n\nOr ML bubble pops at which point, salvaging datacenters will cause a lot of cheap h100s either way.",
              "score": 1,
              "created_utc": "2026-01-09 22:23:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nycashx",
          "author": "badgerbadgerbadgerWI",
          "text": "Good PSA. This is also why edge deployment strategies matter - if you can run well-quantized smaller models locally instead of depending on cloud GPU availability, you're more insulated from these price swings. Q4_K_M quants running on Apple Silicon or AMD integrated graphics is becoming surprisingly viable.",
          "score": 1,
          "created_utc": "2026-01-08 04:35:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycjqy9",
          "author": "meowrawr",
          "text": "This is exactly why I bought a new m3 ultra with 256gb. Thinking I should have done 512 now tho.",
          "score": 1,
          "created_utc": "2026-01-08 05:34:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyckl0k",
          "author": "IrisColt",
          "text": ">Don't put off hardware purchases: GPUs, SSDs, and RAM are going to skyrocket in price soon\n\n\nSelf-fulfilling prophecy... h-heh",
          "score": 1,
          "created_utc": "2026-01-08 05:40:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydm9n1",
          "author": "LicensedTerrapin",
          "text": "I bought my second 3090, I think it will keep me gaming and LLMing for a while. Especially once my 96gb DDR5 ram gets delivered.",
          "score": 1,
          "created_utc": "2026-01-08 11:02:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygdf7k",
          "author": "rockytonk",
          "text": "If Nvidia is going to increase their prices by so much, why did they just restock the 5080 at msrp?",
          "score": 1,
          "created_utc": "2026-01-08 19:24:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyh3zyk",
              "author": "Eisenstein",
              "text": "Where are you finding a 5080 at MSRP?",
              "score": 1,
              "created_utc": "2026-01-08 21:22:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyhuumz",
          "author": "HarlanHitePOG",
          "text": "SSD's are already like double the price from a year ago",
          "score": 1,
          "created_utc": "2026-01-08 23:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymkdjx",
          "author": "TurtleNamedMyrtle",
          "text": "Looking at 5090 prices right now, they range from $2500 to $3500. So many brands too. Should I be wary of different brands or should I just focus on the specs when buying?",
          "score": 1,
          "created_utc": "2026-01-09 17:00:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyob5yd",
          "author": "feenixOmlette",
          "text": "See why can't Elon make. DRAM giga factory if that's where this AI stuff is going long term.",
          "score": 1,
          "created_utc": "2026-01-09 21:45:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny74asp",
          "author": "Formal-Hawk9274",
          "text": "Don't forget to thank the Republicans",
          "score": 1,
          "created_utc": "2026-01-07 13:38:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny76h66",
          "author": "Beneficial_Common683",
          "text": "Hey tell me how much megacorp pay you for this post so i can join the hype too",
          "score": 1,
          "created_utc": "2026-01-07 13:50:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7ah66",
          "author": "Green-Ad-3964",
          "text": "The era of local pc is ending, and the society as we know it will end with it. Cloud is like the Nothing advancing, in the never ending story and the only result will be a ready player one scenario.",
          "score": 0,
          "created_utc": "2026-01-07 14:12:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny80d8o",
          "author": "power97992",
          "text": "Just accept renting gpus and using api or buy an affordable machine lol, RAm won't get cheap anytime soon!",
          "score": 0,
          "created_utc": "2026-01-07 16:18:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2sfwx",
      "title": "ElevenLabs is killing my budget. What are the best \"hidden gem\" alternatives for documentary style TTS?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/",
      "author": "Ancient_Routine8576",
      "created_utc": "2026-01-03 11:31:31",
      "score": 234,
      "num_comments": 126,
      "upvote_ratio": 0.88,
      "text": "Hi everyone, I'm running a YouTube channel focused on \"War Economics\" and \"History\". I've been using ElevenLabs (Marcus voice) and the quality is amazing, but the pricing is unsustainable for long-form content (8-10 min videos).\n\nI've tried the usual suspects (Murf, Play.ht) but they sound too robotic or corporate.\n\n**I am looking for:**\n\n1. Something with a dark, authoritative, documentary-style tone.\n2. Either a cheaper paid alternative OR a high-quality GitHub/Local solution (I have a decent GPU if needed, like RVC or Tortoise).\n3. Has anyone tried tools like **Fish Audio** or **OpenAI TTS API** wrappers?\n\nAny \"underground\" or lesser-known recommendations would be appreciated. Thanks!",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nxf98e3",
          "author": "MixtureOfAmateurs",
          "text": "The best local options are:\n\nSoprano - fast \n\nKokoro - fast \n\nVibevoice\n\nXTTS v2 still somehow \n\nF5 tts.\n\nBasically just look through here and listen to samples [https://huggingface.co/models?pipeline\\_tag=text-to-speech&sort=downloads](https://huggingface.co/models?pipeline_tag=text-to-speech&sort=downloads)",
          "score": 131,
          "created_utc": "2026-01-03 11:38:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxge35i",
              "author": "LocoMod",
              "text": "Soprano is extremely impressive for its size. And it was the first attempt by the undergrad student that made it. Can't wait to see how their work evolves.",
              "score": 47,
              "created_utc": "2026-01-03 15:49:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgmvxp",
                  "author": "Foreign-Beginning-49",
                  "text": "Absolutely kind of that person, this community gives me hope for a human future inhabited by said humans working together or by themselves for the greater s3cret creature we are in truest sense of the word, some creature for certain.",
                  "score": 2,
                  "created_utc": "2026-01-03 16:31:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny2ksnq",
                  "author": "silenceimpaired",
                  "text": "Itâ€™s only one voice?",
                  "score": 1,
                  "created_utc": "2026-01-06 20:24:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxgzbyx",
              "author": "aedocw",
              "text": "You missed chatterbox which is at 11labs quality for cloning, much better than xtts.",
              "score": 34,
              "created_utc": "2026-01-03 17:29:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxhqx1j",
                  "author": "trialgreenseven",
                  "text": "could you tell me if I can use voice cloning based on sample w/o verification step like 11labs on chatterbox/resemble?",
                  "score": 3,
                  "created_utc": "2026-01-03 19:34:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxn12yb",
                  "author": "claytonjr",
                  "text": "Yeah, 2nd vote for chatterbox. I've used it extensively for YouTube narration videos, with a cloned voice. Plus the fast api makes it easy to use with automationÂ ",
                  "score": 2,
                  "created_utc": "2026-01-04 15:37:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxrse6z",
                  "author": "SituationMan",
                  "text": "Not even close. It randomly breaks into accents.",
                  "score": 2,
                  "created_utc": "2026-01-05 05:52:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxhuigs",
                  "author": "MetricZero",
                  "text": "That thing is so ridiculously difficult to set up if you don't know what you're doing. You need to like create a contained environment with a specific install order for Gradios, Numpy, and some other stuff with specific versions and THEN it might work. I couldn't get it to though.",
                  "score": 2,
                  "created_utc": "2026-01-03 19:52:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxh8ada",
              "author": "Segaiai",
              "text": "I'm personally a big fan of [IndexTTS2](https://github.com/index-tts/index-tts). I hardly hear people talk about it, but I've gotten some really cool results. Especially since you can prompt a mood/situation separately from the spoken text itself. And you can get granular with emotion sliders, or match the emotion of the input file.",
              "score": 18,
              "created_utc": "2026-01-03 18:10:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxmgug0",
                  "author": "djtubig-malicex",
                  "text": "Yeah IndexTTS2 is being slept on for whatever reason.  Though I think the use case is more for dubbing existing audio given it relies on similar audio input and from my uses of it, does require quite a bit of memory to use effectively for long form gens.  (Running on M3 Ultra 256GB RAM :) )",
                  "score": 5,
                  "created_utc": "2026-01-04 13:46:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxfgjsa",
              "author": "rpg36",
              "text": "Vibevoice is great! I've been experimenting with it lately and it sounds awesome. It was really simple to run the example code. They even have a little simple web app you can run. No issues running it on my 4070 Super with 16GBs VRAM",
              "score": 12,
              "created_utc": "2026-01-03 12:36:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxfmq78",
                  "author": "1427538609",
                  "text": "Last time I checked vibe voice there are only community forks?",
                  "score": 3,
                  "created_utc": "2026-01-03 13:18:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxs184v",
              "author": "ShengrenR",
              "text": "Absolutely criminal not to mention higgs v2 or index tts2 in this list imo; maybe because they're a bit heavier to run, so fewer do.  I've seen a few folks keep XTTSv2 in the running lately and I just don't hear it myself.. it was great when it first came out, but it's far behind the latest models, unless you're going to be heavily fine-tuning it I don't see the appeal.",
              "score": 1,
              "created_utc": "2026-01-05 07:04:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny2jvuo",
                  "author": "silenceimpaired",
                  "text": "I cannot recall why I lost interest in Index TTS 2 â€¦ Iâ€™ll have to look at it again. Not a fan of the Higgs license but Iâ€™ll have to take another look.",
                  "score": 1,
                  "created_utc": "2026-01-06 20:20:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfdppp",
          "author": "CheatCodesOfLife",
          "text": "VibeVoice if you don't want to write code / just want to give it the transcript.\n\nEcho-TTS if you can work around the 30-second limitation.\n\n\nI'd give Maya-1 if you want to act like a director, eg. put \"documentary domain\" in the description prompt. More code required to generate an 8-10 minute transcript.",
          "score": 27,
          "created_utc": "2026-01-03 12:14:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxfmzhc",
          "author": "1427538609",
          "text": "The Chinese index-TTS2 is quite good, but you have to seed it with a voice example that you like",
          "score": 15,
          "created_utc": "2026-01-03 13:20:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxg5f7j",
          "author": "Finguili",
          "text": "From local TTS, VibeVoice Large seems to have highest ceiling, but the model is very unstable. With one generation it sounds as if text was almost professionally narrated; with another its prosody is so bad that you start to wonder is it the same model. It also loves to add strange music to the background. So expect to reroll a lot.\n\nI donâ€™t have much experience with cloud apis, but Gemini 2.5 Pro TTS sounded to me better than ElevenLabs and should be cheaper.",
          "score": 15,
          "created_utc": "2026-01-03 15:05:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhyrsm",
              "author": "PitifulTeacher4972",
              "text": "you can also remove the music by post processing the output with another model",
              "score": 2,
              "created_utc": "2026-01-03 20:13:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxsakcf",
              "author": "ArtfulGenie69",
              "text": "A lot of the various tts have this issue. Higgs has it too but when you get the best voice sample possible. Basically 30s of exactly the same tones it cloned almost 100%. So you could take a samples from the good generation that it made. If your original sample has trash sound in it try to remove it with something like pyoise or uvr. As far as I've heard for English speakers higgs seemed to clone the best. The samples I heard from vibe weren't as close to the speakers voice for whatever reason. Oh and some characters in text screw up higgs so you have to clean those out or get more weird noises. All the tts are iffy right now and take a bit of tuning.\n\n\nThere is also new stuff to try. Like indextts2 and cosyvoice3",
              "score": 1,
              "created_utc": "2026-01-05 08:30:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxfxid1",
          "author": "IONaut",
          "text": "Currently I think VibeVoice large is the best most natural sounding option. You could even give it a sample of the voice you like from ElevenLabs and clone it that way.  \n  \nChatterbox just came out with a new version (2) that is super lightweight and fast that works pretty well but you have to mess with the settings a bit to get the clone to sound right.",
          "score": 11,
          "created_utc": "2026-01-03 14:22:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxjhq5r",
              "author": "AXYZE8",
              "text": "Where did you found that '2' version?\n\nThere's no mention of such thing in Google Search results [https://www.google.com/search?q=%22chatterbox+2%22+ai](https://www.google.com/search?q=%22chatterbox+2%22+ai)\n\nUnless you're talking about Chatterbox Turbo, but it's not a successor (2), it's just a distilled small version meant for voice agents. It's a bad choice for generating audio in advance (a lot worse quality), OP doesn't need realtime responses.",
              "score": 2,
              "created_utc": "2026-01-04 00:52:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxji6sx",
                  "author": "IONaut",
                  "text": "The place that I found the link was calling chatterbox turbo chatterbox 2. But yes that is the one I'm talking about",
                  "score": 1,
                  "created_utc": "2026-01-04 00:54:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfsv30",
          "author": "Jean_velvet",
          "text": "Google is about to smash Elevenlabs out of the water. It's not deployed in Gemini but they have cutting edge voice synthasis in labs.",
          "score": 20,
          "created_utc": "2026-01-03 13:55:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgpk4u",
              "author": "Silver-Champion-4846",
              "text": "Is it the Chirp3 voices or something newer and better?",
              "score": 8,
              "created_utc": "2026-01-03 16:43:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgrl0m",
                  "author": "Jean_velvet",
                  "text": "Maybe, I'm talking about the voices in AI studio. You can literally start a podcast with 2 AI voices and interact yourself as a caller. It's really good",
                  "score": 8,
                  "created_utc": "2026-01-03 16:53:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxg4psv",
          "author": "ducksoup_18",
          "text": "https://github.com/justinlime/Fatterbox",
          "score": 7,
          "created_utc": "2026-01-03 15:02:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxjar0p",
              "author": "Yorn2",
              "text": "Nice to have another optimized version out there for streaming.",
              "score": 1,
              "created_utc": "2026-01-04 00:15:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxgmmth",
          "author": "DigiJoe79",
          "text": "Hi u/Ancient_Routine8576 I played also a lot with Vibevoice lately. Strongly recommend it, it it amazing. I guess my Audiobook Maker isn't a perfect fit for you use case, but maybe you try to pull just the Vibevoice Engine container. It includes a fastapi server and can also run standalone with whatever workflow you use right now. [https://github.com/DigiJoe79/AudioBook-Maker](https://github.com/DigiJoe79/AudioBook-Maker)",
          "score": 7,
          "created_utc": "2026-01-03 16:30:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny556l8",
              "author": "Anomalia_YT",
              "text": "I need to give yours a go. Iâ€™ve tried a whole bunch of repos for TTS longform and this is by far the best Iâ€™ve tried: https://github.com/psdwizzard/chatterbox-Audiobook\n\nI love that yours offers vibe voice 7b though. With the audiobookmaker Iâ€™m using, you can regenerate chunks if needed which I find invaluable. Unsure if yours has that feature. Iâ€™d love to be able to add any model I want to play around with (a bit like ultimate TTS but most models Iâ€™ve tried are not reliable at all and umtimatetts lacks many features. Echo TTS looks interesting though..\n\nEither way your project looks very interesting and Iâ€™ll hopefully try it out tomorrow !",
              "score": 1,
              "created_utc": "2026-01-07 04:20:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyenzz8",
                  "author": "DigiJoe79",
                  "text": "Thanks! Yes, segment regeneration is a core feature - each segment can be regenerated individually or you can re-run entire chapters. There's also quality analysis (Whisper transcription comparison) and/or Silera VAD audio analysis to find segments that need attention.\n\nFor adding new models: the engine system is designed for this. Currently XTTS, Chatterbox, and VibeVoice are available as Docker images. Adding a new engine means creating a small wrapper server that speaks the TTS API (there are templates in the engine repo). If you're comfortable with Docker, it's pretty straightforward to add your own.\n\nWould be interested to hear how it compares after you try it!",
                  "score": 1,
                  "created_utc": "2026-01-08 14:54:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfrcl9",
          "author": "shaakz",
          "text": "i would suggest echo-tts-base. Supports voice cloning and does it very well. Sub 12gb vram and way faster than realtime on a 5070ti.",
          "score": 5,
          "created_utc": "2026-01-03 13:46:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny0tm42",
              "author": "Bandit174",
              "text": "Same, I was pretty impressed with echo as well. Very underrated imo.",
              "score": 1,
              "created_utc": "2026-01-06 15:37:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxgl8n1",
          "author": "2_two_two",
          "text": "Iâ€™ve been working on a project using Kokoro-TTS. It works well and easy enough to use. Not sure about others but Kokoro lets you blend voices so that you can use the default or create your own. Now Iâ€™m working on pacing, pauses, and tuning so it doesnâ€™t sound like robots.",
          "score": 4,
          "created_utc": "2026-01-03 16:23:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhn62t",
              "author": "mister2d",
              "text": "Nice. I created my own local streaming TTS  project with Kokoro-TTS before discovering this one: https://github.com/eduardolat/kokoro-web\n\nI've been enjoying excellent local streaming tts for a few months without any issues. This tiny model is great.",
              "score": 2,
              "created_utc": "2026-01-03 19:17:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxrafeo",
          "author": "QikoG35",
          "text": "VibeVoice large, MIT license, with custom nodes! Mind blowing how authentic it sounds and open source.\n\nA reason Microsoft pull it back!",
          "score": 3,
          "created_utc": "2026-01-05 03:57:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxtwg75",
              "author": "bondaly",
              "text": "Is it still pulled back in any way?",
              "score": 1,
              "created_utc": "2026-01-05 15:25:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxx2xjl",
                  "author": "misterflyer",
                  "text": "No, what he meant is that they released the 7B version with an MIT license. Then Microsoft quickly deleted it from their HF repo *(not before it got forked tho)*. The full weights 7B model is still out there...\n\n[https://huggingface.co/aoi-ot/VibeVoice-Large](https://huggingface.co/aoi-ot/VibeVoice-Large)\n\n[https://github.com/rsxdalv/VibeVoice](https://github.com/rsxdalv/VibeVoice)",
                  "score": 2,
                  "created_utc": "2026-01-06 00:27:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxgy867",
          "author": "Noiselexer",
          "text": "Tell me your channel so I can block it please.",
          "score": 38,
          "created_utc": "2026-01-03 17:24:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxg18c0",
          "author": "imonlysmarterthanyou",
          "text": "I have been liking VoxCPM. You can clone whatever voice and tone you like.",
          "score": 3,
          "created_utc": "2026-01-03 14:43:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxil630",
              "author": "bassgojoe",
              "text": "VoxCPM 1.5 gave me much higher quality results compared to chatterbox, Iâ€™m surprised itâ€™s not more well known.",
              "score": 2,
              "created_utc": "2026-01-03 22:03:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxgyph4",
          "author": "GabryIta",
          "text": "Chatterbox?",
          "score": 3,
          "created_utc": "2026-01-03 17:26:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhblay",
          "author": "hz55555",
          "text": "Check out inworld.  Much cheaper https://artificialanalysis.ai/text-to-speech/leaderboard",
          "score": 3,
          "created_utc": "2026-01-03 18:24:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhlaxd",
          "author": "therealtonyryantime",
          "text": "Anyone have a good way of automatically filtering out YouTube channels like this?",
          "score": 15,
          "created_utc": "2026-01-03 19:08:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxiksq3",
              "author": "ScoreUnique",
              "text": "Can be an indexed db of videos as a chrome extension, giving away idea for a business btw",
              "score": 5,
              "created_utc": "2026-01-03 22:01:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxfjrp7",
          "author": "Impressive-Sir9633",
          "text": "1. You can try the free unlimited KokoroTTS at https://freevoicereader.com or the Chrome extension. You will have to download the model and everything is processed within your browser (using webGPU). I prefer KokoroTTS, but you can try Supertonic as well.\n\n2. If you want additional voice options, you can try the paid version that has a free 3 day trial.",
          "score": 8,
          "created_utc": "2026-01-03 12:59:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxf8u5f",
          "author": "shanehiltonward",
          "text": "Check out the projects on Pinokio.",
          "score": 3,
          "created_utc": "2026-01-03 11:35:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxft8f7",
          "author": "Head-Leopard9090",
          "text": "Vibevoice large",
          "score": 2,
          "created_utc": "2026-01-03 13:57:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxh65vq",
          "author": "martinerous",
          "text": "In addition to the mentioned ones, I have also tried VoxCPM 1.5. It's quite fast, especially on nanovllm (tried on WSL2). BTW, I also trained VoxCPM to my native Latvian language (and Chatterbox too), and it was surprisingly easy to do with the bundled scripts. Of course, VoxCPM cannot beat VibeVoice quality, but VoxCPM seems more stable when provided a voice to clone, and they also have a protection logic built in to throw away obviously bad generations.",
          "score": 2,
          "created_utc": "2026-01-03 18:00:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxif1ij",
          "author": "HelpfulHand3",
          "text": "For paid options, Inworld with their Max tts model is in my opinion better than ElevenLabs 2.5 and is 10x cheaper. The value for their service is quite frankly absurd.\n\n[https://inworld.ai/pricing](https://inworld.ai/pricing)\n\nLocal models.. Higgs Audio V2, Echo TTS, Vibevoice.",
          "score": 2,
          "created_utc": "2026-01-03 21:33:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkkkk1",
          "author": "rc_ym",
          "text": "I settled on Kokoro using a mixture of voices (using am\\_onyx to give it base).  Then post-processing the audio using [**pedalboard**](https://github.com/spotify/pedalboard) to add some more warmth and simulate a little room noise to make it sound more realistic.  Best speed and quality combo.  Chatterbox had the best quality but was super slow. \n\nI need to try vibevoice after reading the comments here.",
          "score": 2,
          "created_utc": "2026-01-04 04:34:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlr5p1",
          "author": "DocHollidaay",
          "text": "I was on ElevenLabs but they closed my account (ToS), I spent this week trying different TTS, Pinokio All-In-Ones and manual installs of the 'top 5' TTS, I'm using this to create audiobooks, but this still maybe of use:\n\n[https://github.com/rsxdalv/TTS-WebUI](https://github.com/rsxdalv/TTS-WebUI) is highly recommended, it supports a ton of TTS to try out, easy to install and updated often.\n\nI use Chatterbox 0.5B Multilingual, in TTS-WebUI it supports Nvidia Blackwell or thats the first time I got it working at max speed: I went from 35it/s (roughly realtime) to 210it/s (x6) on 5070 Ti and it does a better job than ElevenLabs!\n\nApparently the Multilingual version is better for flow and handles punctuation better than standard 0.5B, after listening to Tubro (sounds a bit compressed), new Turbo (sounds less compressed), standard (very good, accent drifts sometimes, struggles with 'novel punctuation').\n\nAny glitches tend to be something to do with the text sent, so it may need a cleanup step or very long sentences and chunk tweaking.\n\nI find the voice style comes from the training clip, it will match pacing etc.",
          "score": 2,
          "created_utc": "2026-01-04 10:25:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsdxep",
          "author": "Forward_Artist7884",
          "text": "cosyvoiceV3 is pretty good, you just need a voice sample. It's better than XTTSV2 and support style inputs.",
          "score": 2,
          "created_utc": "2026-01-05 09:02:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxj6an0",
          "author": "SuperCaptainMan",
          "text": "AI slop YouTube channel?",
          "score": 2,
          "created_utc": "2026-01-03 23:51:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgct6o",
          "author": "Virtamancer",
          "text": "Has anyone made a longform TTS gui solution yet (local)?\n\nMy use case is creating audiobooks for myself when they arenâ€™t available for free.\n\nI donâ€™t care about cloning voices or other gooner shit, I just want to be able to feed a chapter or even a whole book and have it generate the audio file.",
          "score": 2,
          "created_utc": "2026-01-03 15:43:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxglin4",
              "author": "DigiJoe79",
              "text": "Hi u/Virtamancer \\- You can try my audiobook-maker. In the latest version, there is also a prebuild VibeVoice container with 1.5B and 7B support. [https://github.com/DigiJoe79/AudioBook-Maker](https://github.com/DigiJoe79/AudioBook-Maker)",
              "score": 4,
              "created_utc": "2026-01-03 16:24:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgmyyi",
                  "author": "Virtamancer",
                  "text": "At a quick glance this looks very promising, and excellent documentation! I canâ€™t wait to get to my computer and try it.\n\nA couple questions:\n\n- How much vram is needed?\n\n- How long would, say, a 300 page book take to do TTS? Using 1x or 2x RTX 4090.",
                  "score": 1,
                  "created_utc": "2026-01-03 16:31:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxno49h",
                  "author": "gallito_pro",
                  "text": "Hi, thanks for your app, but Engines dont load at the start!!! Help please. Can be related to nvidia drivers?",
                  "score": 1,
                  "created_utc": "2026-01-04 17:24:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny2fgxw",
                  "author": "silenceimpaired",
                  "text": "Do you prefer vibevoice to chatterbox?",
                  "score": 1,
                  "created_utc": "2026-01-06 19:59:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxgxbsn",
              "author": "evia89",
              "text": "> My use case is creating audiobooks for myself when they arenâ€™t available for free.\n\nI use edge for it. OG https://edgetts.github.io/ or my fork with multiple voices https://vadash.github.io/EdgeTTS/\n\nBoth opensource u can fork and edit if u need more features\n\nCan do 40h book in ~2 hours",
              "score": 1,
              "created_utc": "2026-01-03 17:20:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxh4sgh",
                  "author": "Virtamancer",
                  "text": "Hmm, doesn't work for me.\n\n    [00:00:00] Loaded: textfile.txt\n    [00:00:00] Saving to: TTS Audiobooks\n\nIt asks me to select the output directory, and then I get that output but no files ever actually appear there.",
                  "score": 1,
                  "created_utc": "2026-01-03 17:54:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxjxlx3",
          "author": "dannydonatello",
          "text": "How is a 10 minute video too expensive with elevenlabs? Should be no more than 2-3 USD max.",
          "score": 2,
          "created_utc": "2026-01-04 02:19:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxkxpy5",
              "author": "Buzzard",
              "text": "You're not thinking with AI.  You're assuming they care about the content and put lots of work into the video, so comparatively $2-3 seems really small.\n\nTo them, $2-3 is unsustainable because they haven't put any effort at all.",
              "score": 8,
              "created_utc": "2026-01-04 06:07:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlguve",
                  "author": "ReachingForVega",
                  "text": "And the intent is probably to only make a couple dollars in views per slop vid.Â ",
                  "score": 3,
                  "created_utc": "2026-01-04 08:52:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxk3mok",
              "author": "shadowninjaz3",
              "text": "thats pretty expensive considering inworld and fish audio are 10x cheaper than eleven labs",
              "score": 1,
              "created_utc": "2026-01-04 02:53:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlqnfh",
                  "author": "dannydonatello",
                  "text": "I just think itâ€™s BS that 2-3 USD per video is making his YouTube project unsustainable.",
                  "score": 2,
                  "created_utc": "2026-01-04 10:21:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfllid",
          "author": "rom16384",
          "text": "Have you tried Gemini 2.5 Pro Preview TTS? Choosing one of their deeper voices and setting the tone via a prompt should give good results.",
          "score": 2,
          "created_utc": "2026-01-03 13:11:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgpp2u",
              "author": "Silver-Champion-4846",
              "text": "That got closed on free tier recently, even the flash version became unavailable in the google ai studio website!",
              "score": 2,
              "created_utc": "2026-01-03 16:44:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxgw9l5",
          "author": "nabuachaem",
          "text": "for me, pip TTS and use the XTTS v2",
          "score": 1,
          "created_utc": "2026-01-03 17:15:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgz2ok",
          "author": "Spare-Object3993",
          "text": "You should try gradium",
          "score": 1,
          "created_utc": "2026-01-03 17:28:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhml0h",
          "author": "HotDoshirak",
          "text": "Chatterbox + FlashSR",
          "score": 1,
          "created_utc": "2026-01-03 19:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhvdp1",
          "author": "hidden2u",
          "text": "My current workflow is maya1 to generate a unique voice sample, then clone it with chatterbox audiobook",
          "score": 1,
          "created_utc": "2026-01-03 19:56:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhvhoo",
          "author": "arthurtully",
          "text": "Edge TTS probably the best solution.",
          "score": 1,
          "created_utc": "2026-01-03 19:56:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxj09i3",
          "author": "llamabott",
          "text": "I feel compelled to plug my own app here:\n\n[https://github.com/zeropointnine/tts-audiobook-tool](https://github.com/zeropointnine/tts-audiobook-tool)\n\nIt has support for eight (!) different TTS models.\n\nMiraTTS\n\nGLM-TTS\n\nIndexTTS2\n\nVibeVoice 1.5B\n\nHiggs Audio V2\n\nFish OpenAudio S1-mini\n\nChatterbox-Multilingual\n\nOute TTS\n\nOkay thanks.",
          "score": 1,
          "created_utc": "2026-01-03 23:20:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjhfdu",
          "author": "jjsilvera1",
          "text": "I say maybe because I dont know, but google docs has quite a few good voices on there. One might be interesting? You could also mess with audio setting like pitch, timbre.",
          "score": 1,
          "created_utc": "2026-01-04 00:50:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjxfgm",
          "author": "pbalIII",
          "text": "The voice cloning angle is key for your use case. VibeVoice and F5-TTS both support cloning from audio samples, so you could grab a few clips of the Marcus voice you like and use that as your reference.\n\nF5-TTS is probably the sweet spot... 330M params, solid quality, and the cloning is legit good. Kokoro is faster but doesn't do voice cloning, so you'd be stuck with its built-in voices.\n\nOne thing worth trying: feed it a 10-15 second sample of the exact ElevenLabs output you want to match. The closer your reference audio is to the target tone, the better the clone. I'd start with F5 before going to VibeVoice 7B since the VRAM requirements are more reasonable.",
          "score": 1,
          "created_utc": "2026-01-04 02:18:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny2eft0",
              "author": "silenceimpaired",
              "text": "Iâ€™m sad F5 was never re-released with an open license.",
              "score": 1,
              "created_utc": "2026-01-06 19:54:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxkpm0o",
          "author": "Unlikely_Shake8208",
          "text": "Chatterbox is the best local TTS that I have used.",
          "score": 1,
          "created_utc": "2026-01-04 05:08:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlffo4",
          "author": "crantob",
          "text": "Which tts will let me apply a graph of prosidy/emphasis to the text?  I need to recreate an original speech with similar timing, pitch, emphasis.",
          "score": 1,
          "created_utc": "2026-01-04 08:39:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlp6tz",
          "author": "k2ui",
          "text": "How much does eleven labs cost for you to do 8-10 min content?",
          "score": 1,
          "created_utc": "2026-01-04 10:08:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxmkflo",
          "author": "MoffKalast",
          "text": ">  long-form content\n\n>  8-10 min",
          "score": 1,
          "created_utc": "2026-01-04 14:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxoakcq",
          "author": "coastisthemost",
          "text": "Ttswebui is great",
          "score": 1,
          "created_utc": "2026-01-04 19:04:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqnkgv",
          "author": "MensaForever4117",
          "text": "Speechify",
          "score": 1,
          "created_utc": "2026-01-05 01:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5z0jw",
          "author": "Mediocre-Waltz6792",
          "text": "echo is better than chatterbox IMO. But you'll need 10 GB of Vram for it. There are ways to make it work on 8 GB.",
          "score": 1,
          "created_utc": "2026-01-07 08:12:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyklhz9",
          "author": "Far_Noise_5886",
          "text": "My buddy made this. https://github.com/EmZod/speak . Runs fully locally and clones any voice, written up as an agent skill, so works with any AI agent . Very good imo!",
          "score": 1,
          "created_utc": "2026-01-09 10:14:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxh5tld",
          "author": "jordanpwalsh",
          "text": "I've been working on this: [https://nemoreader.jordanwal.sh](https://nemoreader.jordanwal.sh)\n\nThe version up on the App Store now uses OpenAI which breaks the bank, I have a new version in testing now that uses Higgs V2 (https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base) running on runpod infrastructure and a simple API. My plan is to open source that bit once it's stable for folks to plug in that don't want to pay the in app tts, but I could share what I have now if you want to play with the API.\n\nIt sounds pretty good! perfectly acceptable for the audible type clone I've been messing with.",
          "score": 1,
          "created_utc": "2026-01-03 17:59:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxn1vpd",
          "author": "IronColumn",
          "text": "you should try to do a good job with the things you create",
          "score": 1,
          "created_utc": "2026-01-04 15:41:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0zst6",
      "title": "Upstage Solar-Open-100B Public Validation",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/w789uyo0cpag1.jpeg",
      "author": "PerPartes",
      "created_utc": "2026-01-01 08:52:25",
      "score": 233,
      "num_comments": 70,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0zst6/upstage_solaropen100b_public_validation/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nx2bcn4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-01 11:00:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1zsaj",
          "author": "CKtalon",
          "text": "Why a location? Just release on the Internet.",
          "score": 126,
          "created_utc": "2026-01-01 08:56:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1zvf8",
              "author": "spectralyst",
              "text": "Gangnam Style",
              "score": 134,
              "created_utc": "2026-01-01 08:57:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx34al3",
                  "author": "AuspiciousApple",
                  "text": "Koreans love their pop up stores.",
                  "score": 12,
                  "created_utc": "2026-01-01 14:56:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx7ps0n",
                  "author": "Mikasa0xdev",
                  "text": "Oppa LocalLLaMA Style!",
                  "score": 3,
                  "created_utc": "2026-01-02 06:43:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx2a34t",
                  "author": "DecodeBytes",
                  "text": "I have the synth intro stuck in my head now",
                  "score": 3,
                  "created_utc": "2026-01-01 10:47:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2eddx",
              "author": "keepthepace",
              "text": "Sadly, that's still how to maximize journalistic coverage, by causing FOMO. Force journalists to get there, you force them to make an article. Publish something online they will be like \"meh, put it on the pile\"",
              "score": 33,
              "created_utc": "2026-01-01 11:31:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2ri55",
                  "author": "-p-e-w-",
                  "text": "I very strongly doubt that journalists are going to bother showing up at some mystery location in Korea to settle some AI startup beef lol.",
                  "score": 3,
                  "created_utc": "2026-01-01 13:29:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx224td",
              "author": "ttkciar",
              "text": "That would be lovely!",
              "score": 4,
              "created_utc": "2026-01-01 09:22:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2ijts",
              "author": "PerPartes",
              "text": "This is because of huge domestic market focus. In-person event is a matter of trust and respect (esp. in this region). Almost whole SK AI business is focused on itself. In case of Upstage with the addition of Japanese market as well.",
              "score": 9,
              "created_utc": "2026-01-01 12:12:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx33dme",
                  "author": "Nyghtbynger",
                  "text": "Interestingly that's the case of most nations in fact, except a few merchant nations and empires (US,UK,...)",
                  "score": 2,
                  "created_utc": "2026-01-01 14:50:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx72o2z",
              "author": "dicoxbeco",
              "text": "OOP in Korean *does* state that they will update the post with URL for livestream.\n\nEither the translator OP used skipped that part over, or OOP edited that in later.",
              "score": 1,
              "created_utc": "2026-01-02 03:56:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx21p4c",
          "author": "throwaway-link",
          "text": "I did my own tests. Cossim between layers past the first few seems to be extremely high across any model. Testing layer 45 input layernorm of deepseek v3/v3.1/v3.2-special, kimi k2, and mistral large 3 all give similarities around 0.99. The tested deepseek v3 variants are around 0.99999 with each other. \n\nData from the accusation is entirely expected for a model trained from scratch.",
          "score": 75,
          "created_utc": "2026-01-01 09:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2hccm",
              "author": "llama-impersonator",
              "text": "why are people comparing the norms instead of attn or mlp layers? norms have both low param count and a fairly simple fixed function.",
              "score": 11,
              "created_utc": "2026-01-01 12:01:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2m36d",
                  "author": "throwaway-link",
                  "text": "bc the accusation already says they're different? Their only evidence is norm weights which I show is expected. Probably bc training dynamics for rmsnorm of deeper layers cause the scale to just be a constant value across the weight which obviously results in high cossim. I guess since deeper layers do smaller adjustments, rmsnorm scale doesn't need to do any wild adjustments across the already relatively normalised token vector.",
                  "score": 10,
                  "created_utc": "2026-01-01 12:45:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx3bule",
              "author": "egomarker",
              "text": "Show the code and results. No idea if you are legit or yet another schizo vibecoder with hallucinated \"test results\".",
              "score": 6,
              "created_utc": "2026-01-01 15:41:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx22ua9",
              "author": "KontoOficjalneMR",
              "text": "Almost like all those models are using the similar architecture and similar datasets and you get same-ish output with some small flavour on top. \n\nYou look at the benchmarks and the results are basically a function of amount of parameters with tiny percentage variation based mostly on luck.",
              "score": 16,
              "created_utc": "2026-01-01 09:29:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2i4uv",
                  "author": "DistanceSolar1449",
                  "text": "Thatâ€™sâ€¦ obviously not true. DeepSeek V3, R1, V3.1, V3.2 all have the same param count but much diff performance.",
                  "score": 24,
                  "created_utc": "2026-01-01 12:09:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx42byg",
              "author": "jinnyjuice",
              "text": "Yeah they're saying that you can't really make such definitive conclusions with cossim. They made comparison with Phi here also: https://github.com/hyunwoongko/solar-vs-glm-vs-phi",
              "score": 2,
              "created_utc": "2026-01-01 18:00:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3du9z",
          "author": "kiwibonga",
          "text": "News tomorrow: Upstage employees arrested for beating up some dude in a parking lot.",
          "score": 11,
          "created_utc": "2026-01-01 15:52:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx228oe",
          "author": "ResidentPositive4122",
          "text": "I mean, if this is what it takes to get intermediate checkpoints, let's do it! Llamas, qwens, mistrals, glms, minimaxes, deepseeks, j'accuse! :D",
          "score": 25,
          "created_utc": "2026-01-01 09:23:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2au1f",
              "author": "pkmxtw",
              "text": "AI labs *hate* this simple trick to get them to release intermediate checkpoints!\n\nEither that or this is some of evil-genius level of marketing.",
              "score": 14,
              "created_utc": "2026-01-01 10:54:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2oduo",
              "author": "zball_",
              "text": "Just use different model configuration smh",
              "score": -1,
              "created_utc": "2026-01-01 13:04:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx20qfj",
          "author": "garloid64",
          "text": "op op op",
          "score": 20,
          "created_utc": "2026-01-01 09:07:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx20cjs",
          "author": "PerPartes",
          "text": "I just shared this because recent AI generated post here about the plagiarism claim was removed by the admins. I know the team for approx. 2 years (from the online space) and can hardly believe that it would be true.",
          "score": 34,
          "created_utc": "2026-01-01 09:02:47",
          "is_submitter": true,
          "replies": [
            {
              "id": "nx25axo",
              "author": "RuthlessCriticismAll",
              "text": "It seems appropriate to remove that post. It is however galling that similar, evidence free, ai generated posts with the same accusations don't get removed.",
              "score": 18,
              "created_utc": "2026-01-01 09:56:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx26swr",
                  "author": "PerPartes",
                  "text": "Agreed. Hate is always simpler than a deep and independent analysis.",
                  "score": 18,
                  "created_utc": "2026-01-01 10:12:19",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nx49h78",
                  "author": "rm-rf-rm",
                  "text": "Please report anything you see that we havent removed. Generally I think we are catching stuff well especially things that are particularly egregious.",
                  "score": 6,
                  "created_utc": "2026-01-01 18:35:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx26a8j",
          "author": "AppearanceHeavy6724",
          "text": "Ahaha, imagine if there will be a literal knuckle fight.",
          "score": 18,
          "created_utc": "2026-01-01 10:06:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx61575",
              "author": "tengo_harambe",
              "text": "\"The cosine similarity of my fist and your face is about to be -1.00\"",
              "score": 7,
              "created_utc": "2026-01-02 00:08:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7ilyv",
                  "author": "AppearanceHeavy6724",
                  "text": "yeah exactly.",
                  "score": 2,
                  "created_utc": "2026-01-02 05:45:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6rc2d",
          "author": "siegevjorn",
          "text": "Am I reading this right? How the fuck are they going to validate they trained their llm from scratch at Gangnam station? What about just release a white paper about the novelty of their methods?",
          "score": 4,
          "created_utc": "2026-01-02 02:44:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2uobg",
          "author": "Intrepid_Bobcat_2931",
          "text": "This is a joke. I could see a stunt like \"in person verification\" be reasonable if you gave two weeks notice for people to make travel plans, but they know it's completely impractical for highly experienced people to fly over at a day's notice.",
          "score": 5,
          "created_utc": "2026-01-01 13:52:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3mgdc",
              "author": "my_name_isnt_clever",
              "text": "If you have to fly there, you're not their target audience. This is for domestic journalism.",
              "score": 13,
              "created_utc": "2026-01-01 16:38:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx718se",
              "author": "dicoxbeco",
              "text": "... What joke?\n\nThis was never meant for English audience. [In fact, the OOP wasn't even written in English.](https://www.linkedin.com/posts/upstage-stan_solar-open-100b-%EA%B3%B5%EA%B0%9C-%EA%B2%80%EC%A6%9D%EC%97%90-%EC%B4%88%EB%8C%80-%EB%93%9C%EB%A6%BD%EB%8B%88%EB%8B%A4-solar-100b%EA%B0%80-activity-7412403323175370753-2KgK?utm_source=share&utm_medium=member_desktop&rcm=ACoAACoF--MBSnmFDkOdoa7FU_ztI512j0sxTo4) OP went through a translator so you would understand what it says.",
              "score": 2,
              "created_utc": "2026-01-02 03:46:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx25ssw",
          "author": "PerPartes",
          "text": "[Original Hwalsukâ€™s LI post here](https://www.linkedin.com/posts/upstage-stan_solar-open-100b-%EA%B3%B5%EA%B0%9C-%EA%B2%80%EC%A6%9D%EC%97%90-%EC%B4%88%EB%8C%80-%EB%93%9C%EB%A6%BD%EB%8B%88%EB%8B%A4-solar-100b%EA%B0%80-activity-7412403323175370753-2KgK)",
          "score": 3,
          "created_utc": "2026-01-01 10:01:43",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nx2hk6u",
          "author": "No_Conversation9561",
          "text": "Damn.. you know what, I believe him",
          "score": 2,
          "created_utc": "2026-01-01 12:03:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx20wtr",
          "author": "texasdude11",
          "text": "Tbh, I don't even care about this... If I need a model in this class, I can pick prime intellect, gpt-oss-120b, qwen3-next or move up a class and go to qwen3-235b or Minimax-m2.1 this 100b market is so competitive that you really need to stand out for adoption. Zai, Qwen and OpenAI's censored gpt-oss-120b kinda rule that 80-120b.\n\nAll that being said, more competition is always welcome though! I'd love to see a llama5 120B or a DeepSeek 200b model. That would be insane!",
          "score": 6,
          "created_utc": "2026-01-01 09:08:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx26jdf",
              "author": "LittleBlueLaboratory",
              "text": "I have 96GB VRAM (4x 3090). Strix Halo and DGX Spark have 128. This 80B to 120B segment is where its at! The more competition the better!",
              "score": 12,
              "created_utc": "2026-01-01 10:09:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx26pzv",
                  "author": "texasdude11",
                  "text": "Agreed!\n\nI have 2x6000 Pros with 512 GB DDR5 RAM, so I'm a bit lucky there. These 100b size is clearly in consumer reach!",
                  "score": 3,
                  "created_utc": "2026-01-01 10:11:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx3kbfm",
              "author": "uti24",
              "text": ">this 100b market is so competitive that you really need to stand out for adoption\n\nI want 100B dense model. Is there something besides Meta-Llama-1/2/3-70B? \n\nIt feels not really smart.. On par with other 30B class models like Gemma or Mistral small.",
              "score": 1,
              "created_utc": "2026-01-01 16:26:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3mpca",
                  "author": "my_name_isnt_clever",
                  "text": "Devstral 2 is 123b dense, but it's coding focused. It's far, far more expensive to train large dense models than MoE which is why they're so few and far between these days.",
                  "score": 4,
                  "created_utc": "2026-01-01 16:39:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx6n7cx",
                  "author": "Sea-Speaker1700",
                  "text": "They're all complete morons out of the box, every last one. \n\nSetup a proxy between your client and the inference service and tailor the performance to your needs, it can take any \"only yet another info barfing hallucinator model\", aka: every single 100b range model, and turn them into a useful tool.\n\nLoading 100b(ish) and trying to use them direct is a plain old waste of time.",
                  "score": 1,
                  "created_utc": "2026-01-02 02:19:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx274pg",
          "author": "Kooky-Somewhere-2883",
          "text": "Oppa Gangnam Style?",
          "score": 3,
          "created_utc": "2026-01-01 10:15:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx29jzj",
          "author": "Ok_Condition4242",
          "text": "https://i.redd.it/8dox9ujevpag1.gif\n\nmeanwhile cursor's composer-1",
          "score": 5,
          "created_utc": "2026-01-01 10:41:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx21423",
          "author": "Long_comment_san",
          "text": "Next 50-80b dense would be mindblowing. Someone, please. These total trillions of total parameters are irrelevant when there's a hook to the web.",
          "score": 5,
          "created_utc": "2026-01-01 09:11:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx278zf",
          "author": "NandaVegg",
          "text": "What near Gangnam Station for \"releasing all the intermediate checkpoints and wandbs\"? This is so weird. Can we dance together for a sped up ppongjjak? That would light the mood up. BTW I don't believe the claim that it's a finetune.",
          "score": 2,
          "created_utc": "2026-01-01 10:17:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx20648",
          "author": "yuumi_ramyeon",
          "text": "Popcorn",
          "score": 2,
          "created_utc": "2026-01-01 09:00:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8gatt",
          "author": "PerPartes",
          "text": "I've updated the post with a video link /and seen just a small part of it so far/",
          "score": 1,
          "created_utc": "2026-01-02 10:51:27",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nx4w0rc",
          "author": "7734128",
          "text": "What kind of a joke organization is this?\n\nEvery communication I've seen from them has been bodged like this.\n\nI don't need to inspect weights to know they're a scam when this is the quality of their PR statements.",
          "score": 0,
          "created_utc": "2026-01-01 20:28:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx24dhc",
          "author": "Super_Sierra",
          "text": "Show proof, not text. Idc about twitter post counterclaiming.",
          "score": -1,
          "created_utc": "2026-01-01 09:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx21fw9",
          "author": "Desperate-Sir-5088",
          "text": "Do not blaim the model without any proof. GLM-4.5-Air could count number of 'r' in the \"starbrerry\" correctly.Â \n\n\nWe usually called it \"deadcopy\"",
          "score": -5,
          "created_utc": "2026-01-01 09:14:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q502bi",
      "title": "Rubin uplifts from CES conference going on now",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/zgs8qc8kvlbg1.jpeg",
      "author": "mr_zerolith",
      "created_utc": "2026-01-05 22:19:51",
      "score": 221,
      "num_comments": 94,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q502bi/rubin_uplifts_from_ces_conference_going_on_now/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxwhg2n",
          "author": "sourceholder",
          "text": "The memory bandwidth figure is truly insane.",
          "score": 78,
          "created_utc": "2026-01-05 22:36:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwxjcu",
              "author": "SkyFeistyLlama8",
              "text": "How are they getting that figure? There must be a ton of lanes going from HBM directly to the GPU.",
              "score": 20,
              "created_utc": "2026-01-05 23:59:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny09miw",
                  "author": "MrMPFR",
                  "text": "HBM4 at +10Gbps. NVIDIA basically turbocharging towards HBM4E without waiting for JEDEC.",
                  "score": 10,
                  "created_utc": "2026-01-06 13:55:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny8ygdj",
                  "author": "MrHighVoltage",
                  "text": "That is already the trick with HBM, massively parallel interfaces (like 1024b busses). And they also make them fast individually now.",
                  "score": 2,
                  "created_utc": "2026-01-07 18:49:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxyvkkg",
              "author": "No_Afternoon_4260",
              "text": "The nvlink also.. the vera cpu is supposed to have twice+ the ram bandwidth of Nvidia grace.  \nNvidia doing real tic tac with their cpu.",
              "score": 7,
              "created_utc": "2026-01-06 07:11:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxwixl2",
              "author": "Healthy-Nebula-3603",
              "text": "Yes ..that's HBM technology feature :)\n\nI wonder when HBM will be at home computers...as all producers of memory started to produce HBM and stopping producing DDR memory. .. that's why is so expensive now..\n\n\nSo ..maybe in a year or 2 the  HBM memory  will be coming to home computers...",
              "score": 26,
              "created_utc": "2026-01-05 22:43:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxx1m7p",
                  "author": "popecostea",
                  "text": "I am pretty sure AMD had some consumer cards in the rx 200â€™s era with the first generation of HBM.",
                  "score": 31,
                  "created_utc": "2026-01-06 00:20:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxylxop",
                  "author": "beryugyo619",
                  "text": "The nice thing about HBM is that you absolutely can't solder it at home. So they can keep selling you 6GB cards. So hopefully never.",
                  "score": 14,
                  "created_utc": "2026-01-06 05:51:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxzvhf6",
                  "author": "Dayder111",
                  "text": "3D DRAM will eventually come to cheap consumer computers, HBM likely won't, especially now with AI.",
                  "score": 3,
                  "created_utc": "2026-01-06 12:28:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxwws7h",
                  "author": "bladezor",
                  "text": "Probably never, you will own nothing",
                  "score": 5,
                  "created_utc": "2026-01-05 23:55:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny674hl",
                  "author": "SkyFeistyLlama8",
                  "text": "I don't think so. HBM isn't efficient. Laptops use LPDDR5X for a reason.",
                  "score": 1,
                  "created_utc": "2026-01-07 09:28:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny3kuuy",
              "author": "gomezer1180",
              "text": "What do they want for this crap? 60 Grand?\n\nEdit: yup 57k per server trayâ€¦ ðŸ¤¦ðŸ»â€â™‚ï¸",
              "score": 1,
              "created_utc": "2026-01-06 23:14:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwh0a4",
          "author": "pineapplekiwipen",
          "text": "Missing the Price category at the top for 10X Blackwell!",
          "score": 114,
          "created_utc": "2026-01-05 22:33:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwhrcc",
              "author": "sourceholder",
              "text": "Performance per Joule is what really matters. Power is the largest operating cost.\n\nIt's funny how this is posted to [**r/LocalLLaMA**](https://www.reddit.com/r/LocalLLaMA/). Yeah, gonna order one right up.",
              "score": 85,
              "created_utc": "2026-01-05 22:37:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxwo3cl",
                  "author": "mr_zerolith",
                  "text": "I posted it here because the technology will eventually scale down to a 6090, new generation RTX pro, etc..\n\nWe have radio silence from Nvidia on this lately so this is the only inkling as to what's to come to our world.",
                  "score": 47,
                  "created_utc": "2026-01-05 23:09:41",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nxy0p5z",
                  "author": "kingslayerer",
                  "text": "people who would order that walk among us",
                  "score": 9,
                  "created_utc": "2026-01-06 03:30:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxxovax",
                  "author": "2str8_njag",
                  "text": "Not if you build an entire atomic power plant(s) around.",
                  "score": 3,
                  "created_utc": "2026-01-06 02:24:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny2cinr",
                  "author": "MoffKalast",
                  "text": "1 KW idle.",
                  "score": 1,
                  "created_utc": "2026-01-06 19:46:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxx4nb3",
                  "author": "rm-rf-rm",
                  "text": "you mean performance per watt?",
                  "score": 1,
                  "created_utc": "2026-01-06 00:36:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxzu512",
                  "author": "sluuuurp",
                  "text": "No, the chips cost way more than the power does.",
                  "score": 0,
                  "created_utc": "2026-01-06 12:19:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxwzyx1",
              "author": "-Akos-",
              "text": "Price: If you have to ask, itâ€™s too expensive for you.",
              "score": 17,
              "created_utc": "2026-01-06 00:12:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwlyz0",
          "author": "QuantumUtility",
          "text": "These are very cool.\n\nBut isnâ€™t CES supposed to be the CONSUMER Electronics show? They showed nothing for the consumer market. Not even a mention of the still unseen DGX Station.",
          "score": 75,
          "created_utc": "2026-01-05 22:58:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwna6t",
              "author": "mr_zerolith",
              "text": "Yeah it's telling that there's no mention of future consumer or even workstation class products.  \nI was hoping to hear something about that. Maybe by summer there will be inklings of it.\n\nWhat's strange is in the early part of the broadcast there was a demo reel of new game technologies, but zero mention if it after",
              "score": 19,
              "created_utc": "2026-01-05 23:05:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxwow0q",
                  "author": "QuantumUtility",
                  "text": "New game technologies that have been announced last year and we still havenâ€™t seen mind you.\n\nReflex 2 is still MIA.",
                  "score": 17,
                  "created_utc": "2026-01-05 23:13:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny2d0cs",
              "author": "MoffKalast",
              "text": "No such thing as a consumer market anymore, haven't you been keeping up?",
              "score": 3,
              "created_utc": "2026-01-06 19:48:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxxxkae",
              "author": "ZealousidealBus9271",
              "text": "Consumers use the AI-models powered by these chips ig, I will take it though, they should showcase AI products even if they are for enterprise, this technology is too important to ignore",
              "score": 6,
              "created_utc": "2026-01-06 03:12:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyc8k8s",
              "author": "deelectrified",
              "text": "And AMD made not ONE mention of their new 9850x3D CPU, their only new consumer product that they announced during the event, but not at the event.",
              "score": 1,
              "created_utc": "2026-01-08 04:21:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwg6vi",
          "author": "FullstackSensei",
          "text": "And will probably cost 100k each... Because that's still cheaper per flop than Blackwell.\nRemember kids, the more you buy, the more you save!",
          "score": 84,
          "created_utc": "2026-01-05 22:29:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwlrng",
              "author": "No_Afternoon_4260",
              "text": "Did you know h200 and b200 price/gb is ~ the same but I factored 1/8 of the entire system (cpu, minimal ram, case, psu, minimal storage..) in the b200 card and it is just a better card",
              "score": 9,
              "created_utc": "2026-01-05 22:57:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxwmgr8",
                  "author": "FullstackSensei",
                  "text": "I didn't crunch the numbers, but despite the meme Huang's quote last year was actually about how Blackwell was cheaper to run. He made a similar statement about how Rubin continues that trend.",
                  "score": 8,
                  "created_utc": "2026-01-05 23:01:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxxfs9j",
              "author": "ThreeKiloZero",
              "text": "Benchmarks from FP0.1",
              "score": 3,
              "created_utc": "2026-01-06 01:35:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwhc0n",
          "author": "mr_zerolith",
          "text": "I guess the bad news i find out now is:  \n\\- it requires 2x the power  \n\\- NVFP4 is part of this performance gain\n\nSo perf/watt gain maybe 50% or under",
          "score": 43,
          "created_utc": "2026-01-05 22:35:35",
          "is_submitter": true,
          "replies": [
            {
              "id": "nxwjgwy",
              "author": "mrshadow773",
              "text": "They always lie like this every year. Manipulating numbers but leaving an asterisk somewhere so they can say â€œlook! It said it was because ___ all along!â€ And not get in trouble. Same goes for the circular finances involved in NVIDIA+every company related to GPUs and inference on them",
              "score": 23,
              "created_utc": "2026-01-05 22:46:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxwt63h",
                  "author": "Ok_Top9254",
                  "text": "Do you want to know a fun fact? Compute doesn't matter anyway. You are not running video games but training trillion parameters across multi hundred rack supercomputer. In HPC workloads utilization rarely goes above 60% anyway, even if you actually double the performance you'd barely see a difference. The real difference maker is memory bandwidth, memory size, node-to-node bandwidth and most importantly software. Most of these are in the screenshot, Nvidia also has a crazy 1000 Terabit switch that's used in their racks. But still, CUDA is the only reason people choose Nvidia these days, despite the price.\n\nCerebras has massive wafer chips that use large cache like Amd, instead of HBM and get over 20 petabytes/s BW to 44GB buffer, 5x more fp16 compute than this has fp4, and can train a small model in a day. Yet, Nvidia still gets orders of magnitude more sales because they sell a platform not a gpu.",
                  "score": 23,
                  "created_utc": "2026-01-05 23:36:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxwlj6f",
                  "author": "mr_zerolith",
                  "text": "Yeah they took a few pages out of Apple's marketing book.  \nWish they'd just be honest",
                  "score": 7,
                  "created_utc": "2026-01-05 22:56:35",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxx9wro",
              "author": "stoppableDissolution",
              "text": "Well, 50% perf/watt gain in one generation is actually quite insane improvement",
              "score": 4,
              "created_utc": "2026-01-06 01:04:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny0rfex",
                  "author": "__JockY__",
                  "text": "Not if the previous numbers were for BF16 and these for NVFP4.",
                  "score": 2,
                  "created_utc": "2026-01-06 15:27:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxyo71q",
              "author": "PeakBrave8235",
              "text": "LOL great, so the server farms will draw even MORE electricity. Fuck NVIDIA.",
              "score": -2,
              "created_utc": "2026-01-06 06:09:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwyz4l",
          "author": "fkrkz",
          "text": "When CES is not about Consumer tech anymore",
          "score": 22,
          "created_utc": "2026-01-06 00:06:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxx8vcf",
          "author": "Slasher1738",
          "text": "Pretty insulting that they announced this at CES",
          "score": 16,
          "created_utc": "2026-01-06 00:58:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxjdrq",
          "author": "Terminator857",
          "text": "People are easily excited for a product that costs $50 - $100K and can't even buy till next year.",
          "score": 10,
          "created_utc": "2026-01-06 01:55:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny0cm3f",
              "author": "Mochila-Mochila",
              "text": "Excited for the consumer products that'll trickle down from this new architecture.",
              "score": 2,
              "created_utc": "2026-01-06 14:11:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny0ww19",
                  "author": "Terminator857",
                  "text": "Be excited next year.Â ",
                  "score": 3,
                  "created_utc": "2026-01-06 15:52:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny3rwpn",
              "author": "mr_zerolith",
              "text": "I'm excited about buying the baby sized version of it since the power consumption per power unit should be massively improved.\n\nI'd own 4 5090's already if a north american power outlet would allow me to run it :O",
              "score": 0,
              "created_utc": "2026-01-06 23:50:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny5de1h",
                  "author": "Terminator857",
                  "text": "your buying nvidia hype",
                  "score": 0,
                  "created_utc": "2026-01-07 05:15:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxwgt1z",
          "author": "Agile-Youth516",
          "text": "Nice. Is it 2 or 4 dies per GPU?",
          "score": 8,
          "created_utc": "2026-01-05 22:32:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxm4lk",
              "author": "caelunshun",
              "text": "2. Rubin Ultra will be 4.",
              "score": 3,
              "created_utc": "2026-01-06 02:09:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxywihn",
                  "author": "Agile-Youth516",
                  "text": "Thanks for confirming, that's quite impressive then",
                  "score": 1,
                  "created_utc": "2026-01-06 07:19:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxyx2k7",
              "author": "rm-rf-rm",
              "text": "Its still 2",
              "score": 1,
              "created_utc": "2026-01-06 07:25:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxzlztt",
          "author": "shaman-warrior",
          "text": "In 20y we'll have something similar in our pocket",
          "score": 3,
          "created_utc": "2026-01-06 11:15:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny0mpxw",
              "author": "Impossible-Hunt9117",
              "text": "I prefer to have it implanted in my brain",
              "score": 1,
              "created_utc": "2026-01-06 15:04:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxx5649",
          "author": "Porespellar",
          "text": "Dammit!! Now I want a Reuben sandwich. Thanks NVIDIA :(",
          "score": 5,
          "created_utc": "2026-01-06 00:38:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxx66f7",
              "author": "eidrag",
              "text": "til reuben sandwich. Thanks random person for my lunch menu",
              "score": 6,
              "created_utc": "2026-01-06 00:44:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny366y3",
                  "author": "smcnally",
                  "text": "np \n\nhttps://preview.redd.it/4tursx7jxsbg1.jpeg?width=530&format=pjpg&auto=webp&s=7e72561a146c323a6fe24f7f203b84376fa5ab53",
                  "score": 1,
                  "created_utc": "2026-01-06 22:02:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxybrpu",
          "author": "bick_nyers",
          "text": "What I wouldn't give for a pair of Rubin CPX.",
          "score": 2,
          "created_utc": "2026-01-06 04:38:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny16ev2",
          "author": "Ok_Warning2146",
          "text": "Rtx 6000 Rubin please take my $$$",
          "score": 2,
          "created_utc": "2026-01-06 16:36:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny181n1",
              "author": "mr_zerolith",
              "text": "Yeah after seeing this i decided to start saving for one of those.  \nI'd guess that a single one has the grunt needed to run big models.. if we get 128gb of ram, we're in heaven",
              "score": 1,
              "created_utc": "2026-01-06 16:43:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwj8ij",
          "author": "Leopold_Boom",
          "text": "I wish they'd answer the important questions like if my pci gen 4 bifurcation cable will suffice /s",
          "score": 5,
          "created_utc": "2026-01-05 22:45:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwuktl",
              "author": "DerFreudster",
              "text": "And: Look what your 12VHPWR done to my boy! My beautiful 5090!",
              "score": 3,
              "created_utc": "2026-01-05 23:43:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxx2xya",
                  "author": "Leopold_Boom",
                  "text": "100%",
                  "score": 2,
                  "created_utc": "2026-01-06 00:27:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxx0bc2",
          "author": "ImportancePitiful795",
          "text": "Is not exciting, because is a SERVER product that WILL NEVER be able to own.",
          "score": 6,
          "created_utc": "2026-01-06 00:13:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny17g4p",
              "author": "Ok_Warning2146",
              "text": "Well, if u can afford a car then u can afford a rtx 6000 Rubin",
              "score": 2,
              "created_utc": "2026-01-06 16:41:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxzw9th",
              "author": "Dayder111",
              "text": "It is exciting, in a good or bad way, because it (and especially its \"Ultra\" successor in 2027 with 4 times the memory) is the hardware that will for the first time allow reliable, real-time (video/vision, sound/hearing and all) continuously learning AI agents to begin replacing people at remote/office/computer roles, even prestigious and high paying ones.",
              "score": 1,
              "created_utc": "2026-01-06 12:34:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny03nsm",
                  "author": "ImportancePitiful795",
                  "text": ">Â to begin replacing people at remote/office/computer roles, even prestigious and high paying ones\n\nI do not share your enthusiasm for people losing their jobs, their homes, their livelihoods, ending up homeless. Let alone impose total dystopian control over our lives. \n\nHUMANS FIRST by any means necessary. Even if need something similar to Butlerian Jihad.",
                  "score": 0,
                  "created_utc": "2026-01-06 13:21:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxzdq3b",
          "author": "nierama2019810938135",
          "text": "Will this make current rtx gpu more or less expensive short term?",
          "score": 1,
          "created_utc": "2026-01-06 10:02:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxd87r",
          "author": "960be6dde311",
          "text": "The NVIDIA DGX Spark got a nice boost in inference performance with its latest software update.",
          "score": 1,
          "created_utc": "2026-01-06 01:21:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxw6p3",
          "author": "ThenExtension9196",
          "text": "Jensen cooked.",
          "score": 1,
          "created_utc": "2026-01-06 03:05:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwi7ed",
          "author": "Healthy-Nebula-3603",
          "text": "...and they are improving their chips performance 3x - 4x ( fp4 / nvfp4 and fp8) times every years from few years.\n\nhttps://preview.redd.it/z2g0y3s27mbg1.jpeg?width=1080&format=pjpg&auto=webp&s=1d9406667fbdfd92b0c0ec35471d55b1f2844428\n\nLook how they increasing performance for fp16 ( 2x -3x )  ...insane.\n\nFor fp4 / nvfp4 / fp8 performance between generations are even bigger.",
          "score": -3,
          "created_utc": "2026-01-05 22:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwvgi5",
              "author": "Ok_Top9254",
              "text": "Not really that insane, they are effectively stitching together more dies to make a bigger chip while the perf per watt doesn't increase that much. Blackwell were two connected slightly bigger hopper dies with nvfp4 and rubin is 4x dies (probably), the gains are mostly tied to node advancements not really architecture at this point.   \nAsk chatgpt to add wse-3 cerebras wafer chip to that table, that's basically the same concept taken to the extreme. Specs might as well not matter because Nvidia is selling the whole software+rack scale package, not just the gpu.",
              "score": 12,
              "created_utc": "2026-01-05 23:48:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxwzfjq",
                  "author": "Healthy-Nebula-3603",
                  "text": "Yes h100 was the last monolithic die and from b200 they are using multi die technology...like AMD is doing from years with CPU.\n\nThat's a very good move because is reducing costs and allowing to produce more chips from a waffle .\n\nI don't know why a multi chip technology you see as something wrong.\n\n\n\nLike you see a power consumption is not so bigger on each generation comparing to performance increases. ( Fp16 , fp8) \n\nA100 - 400w  - 0.3 PF  - 0.6 PF ?\n\nH100 - 700w  - 1 PF - 2PF\n\nB200 - 1000w 2.5 PF - 4.5 PF\n\nRubin - ? - 4 PF - 15 PF",
                  "score": 0,
                  "created_utc": "2026-01-06 00:09:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxwxdpw",
                  "author": "pab_guy",
                  "text": "Did you see the keynote? Because your comment makes it sound like you did not.",
                  "score": -3,
                  "created_utc": "2026-01-05 23:58:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxzxc67",
              "author": "Dayder111",
              "text": "And they still have a lot of room to go if they go all in into ASIC approach, ditching as much of fp64/fp32 and general purpose logic away from chips and going full inference/training of most successful AI model architectures.\nAnd also a lot of room to go if they add support for ternary/\"BitNet\" computation for inference, down to 2 or \"1.58\" bit per weight.\nLike, up to 100x more inference compute, or so. But very inflexible and only worth it once good capable model architectures are discovered and proven with time so to say.",
              "score": -1,
              "created_utc": "2026-01-06 12:41:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q6sp4b",
      "title": "Sopro: A 169M parameter real-time TTS model with zero-shot voice cloning",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q6sp4b/sopro_a_169m_parameter_realtime_tts_model_with/",
      "author": "SammyDaBeast",
      "created_utc": "2026-01-07 21:46:19",
      "score": 203,
      "num_comments": 25,
      "upvote_ratio": 0.98,
      "text": "As a fun side project, I trained a small text-to-speech model that I call Sopro. Some features:\n\n* 169M parameters\n* Streaming support\n* Zero-shot voice cloning\n* 0.25 RTF on CPU, meaning it generates 30 seconds of audio in 7.5 seconds\n* Requires 3-12 seconds of reference audio for voice cloning\n* Apache 2.0 license\n\nYes, I know, another English-only TTS model. This is mainly due to data availability and a limited compute budget. The model was trained on a single L40S GPU.\n\nItâ€™s not SOTA in most cases, can be a bit unstable, and sometimes fails to capture voice likeness. Nonetheless, I hope you like it!\n\nGitHub repo: [https://github.com/samuel-vitorino/sopro](https://github.com/samuel-vitorino/sopro)",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q6sp4b/sopro_a_169m_parameter_realtime_tts_model_with/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nya3x2v",
          "author": "Accurate-Tea8319",
          "text": "Pretty impressive for a solo project on a single GPU tbh. The streaming support is clutch - most TTS models make you wait forever for the full generation\n\n  \nHow's the quality compared to something like Coqui or Tortoise? The zero-shot cloning sounds tempting but I've been burned by models that promise it and deliver robot voices lol",
          "score": 33,
          "created_utc": "2026-01-07 21:48:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nya6kix",
              "author": "SammyDaBeast",
              "text": "Thanks! I mainly compared it with chatterbox-turbo/f5 tts, which I consider to be SOTA on these sizes. On some voices chatterbox is much better and stable. F5 tts tends to have better voice similarity. However both these models are slower, specially F5.",
              "score": 15,
              "created_utc": "2026-01-07 22:00:13",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyguvxi",
              "author": "Foreign_Risk_2031",
              "text": "Nah, tts models just output tokens. Itâ€™s the implementation that doesnâ€™t support streaming",
              "score": 2,
              "created_utc": "2026-01-08 20:42:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyil2v4",
                  "author": "toastjam",
                  "text": "There aren't any TTS models that resolve the entire waveform simultaneously via diffusion?",
                  "score": 1,
                  "created_utc": "2026-01-09 01:43:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyatrmo",
          "author": "TheRealMasonMac",
          "text": "How much did it cost to train?",
          "score": 16,
          "created_utc": "2026-01-07 23:51:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyd5czc",
              "author": "SammyDaBeast",
              "text": "Around 250 dollars",
              "score": 11,
              "created_utc": "2026-01-08 08:31:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nya6icb",
          "author": "HungryMachines",
          "text": "The voice sounds a bit hoarse on the sample, is that something that can be improved with more training?",
          "score": 8,
          "created_utc": "2026-01-07 21:59:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nya75gk",
              "author": "SammyDaBeast",
              "text": "It really depends on the voice reference audio. Some sound pretty clear, others don't. I didn't specially cherry pick those examples.  A big % of training data is noisy, and can affect the final model. More training, I guess, but I would say better data > more training.",
              "score": 11,
              "created_utc": "2026-01-07 22:02:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nybzmea",
          "author": "lastrosade",
          "text": "My God, you gave us a model, a clear usage, an architecture, datasets, ~~training scripts.~~\n\n~~All we need now is a brave soul with money.\nHonestly, I'd love to see tomorrow if I can improve on this.\nMaybe even put some money down for training.\nI'd love to do it with a smaller parameter count though.~~\n\n~~If someone managed to make Kokoro that fucking good and bilingual and have multiple voices, I think we can make a kick ass single language, single voice, 60 million or less parameters Model.~~\n\nSomething I would really like is for someone to manage to pin down the exact recipe for a good TTS model and have that recipe be completely open source so that other people may concentrate on finding data sets for other languages and make multiple high quality, very small TTS models.\n\n~~And you gave me so much fucking hype with this.~~\n\nNever mind, false hopes, I just realized you did not give the training scripts, I'm fucking stupid.",
          "score": 9,
          "created_utc": "2026-01-08 03:29:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyd4iry",
              "author": "SammyDaBeast",
              "text": "I will give the training code soon! No worries",
              "score": 5,
              "created_utc": "2026-01-08 08:23:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nybd9sa",
          "author": "RIP26770",
          "text": "We need a ComfyUI node ASAP ! Thanks for sharing this ðŸ™",
          "score": 5,
          "created_utc": "2026-01-08 01:30:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyggco8",
              "author": "RIP26770",
              "text": "Done !\n\n[https://github.com/ai-joe-git/ComfyUI-Sopro](https://github.com/ai-joe-git/ComfyUI-Sopro)\n\nhttps://preview.redd.it/2ouo3exih6cg1.png?width=2913&format=png&auto=webp&s=f8203340fa44e2d2a41c93765e93750c0942e026",
              "score": 2,
              "created_utc": "2026-01-08 19:37:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nygmmic",
                  "author": "SammyDaBeast",
                  "text": "Cool!!",
                  "score": 2,
                  "created_utc": "2026-01-08 20:05:33",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyaex4o",
          "author": "SlavaSobov",
          "text": "Great work! I'll give it a try later. It looks very nice for small edge devices!",
          "score": 3,
          "created_utc": "2026-01-07 22:38:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyaes62",
          "author": "PsychologicalFactor1",
          "text": "It will support Portuguese, right? â€¦right?",
          "score": 4,
          "created_utc": "2026-01-07 22:37:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyd52nb",
              "author": "SammyDaBeast",
              "text": "I would love to support Portuguese, specially European, which is a bit more niche on the data side",
              "score": 2,
              "created_utc": "2026-01-08 08:28:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nykj7pf",
                  "author": "JarbasOVOS",
                  "text": "Here's some datasets for pt-PT \n\nhttps://huggingface.co/collections/Jarbas/portugues-de-portugal-audio\n\nEuroSpeech alone has 800GB of pt-PT audio",
                  "score": 1,
                  "created_utc": "2026-01-09 09:54:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nydjwmr",
          "author": "danigoncalves",
          "text": "Congrats mate! Very nice job you did here with such lower capacity. Maybe you can try to apply to some european fund in order to take this further because I guess Amalia is only TTT :)",
          "score": 1,
          "created_utc": "2026-01-08 10:43:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nygplcw",
              "author": "SammyDaBeast",
              "text": "Thank you, fellow Portuguese!",
              "score": 1,
              "created_utc": "2026-01-08 20:18:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nye7roq",
          "author": "Fickle_Performer9630",
          "text": "Whatâ€™s the relation to Soprano TTS model?",
          "score": 1,
          "created_utc": "2026-01-08 13:30:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyelkts",
              "author": "SammyDaBeast",
              "text": "None, but I have seen the project, pretty cool!",
              "score": 1,
              "created_utc": "2026-01-08 14:42:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nycwctl",
          "author": "rm-rf-rm",
          "text": "The examples in the README are truly bad. There are so so many such \"I made a TTS\" projects - genuinely curious what your aim is? Just learn? Have fun?\n\nIt would be so much better for you and the community to contribute to one of the existing open source TTS projects. What the ecosystem lacks is genuinely good model that can handle long generations without going haywire. Its sad that we dont have aggressive competition from open source in TTS like we do in STT, LLMs, Image gen etc.",
          "score": -6,
          "created_utc": "2026-01-08 07:11:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzt1q8",
      "title": "LLM server gear: a cautionary tale of a $1k EPYC motherboard sale gone wrong on eBay",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pzt1q8/llm_server_gear_a_cautionary_tale_of_a_1k_epyc/",
      "author": "__JockY__",
      "created_utc": "2025-12-30 20:36:46",
      "score": 196,
      "num_comments": 81,
      "upvote_ratio": 0.93,
      "text": "or: selling high-end LLM server gear is more fraught with risk than I realized.\n\n### AI Disclosure\n\nThis was written entirely by hand on my laptop in Sublime Text with zero AI involvement. Shit, I didn't even use spell check. All mistakes are my own.\n\n### tl;dr \n\nDuring an \"Item Not As Described (INAD)\" dispute, eBay ALWAYS sides with the buyer until the very last steps of the case no matter what the circumstances, despite all evidence, and in the face of all immediately obvious reason, logic, and common sense. Except it makes perfect sense and you might not even lose your money. Allow me to elaborate.\n\n### The Sale\n\nRewind to October 2025 when I replaced the incumbent Gigabyte MZ33-AR1 Epyc Zen5 motherboard with a Supermicro H14SSL-N for my inference rig. Long story short: don't use Gigabyte motherboards for 4-way Blackwell GPU setups unless sado-masochism is your thing. Anyway, I sold it to a seemingly nice chap on eBay for $900. He seemed a bit clueless about Epyc and compatibility issues, but we exchanged messages and he decided to go ahead with the \"no returns\" purchase of the as-new MZ33-AR1.\n\nOriginal box. All the case candy. As new. Undamaged. Fully working. With hi-res photos (taken on a Nikon D7000 with Nikon 17-55 f2.8 glass and processed in Capture One Pro) of all areas of the motherboard and CPU socket. This is important. \n\n### The Buyer\n\nFast forward a week or so: buyer hits me up with a bunch of Dr Debug codes (although he doesn't know they're Dr Debug codes, he just pulled \"error codes\" from the BMC) claiming the motherboard won't boot. I did him the solid of explaining Dr Debug and I provided a link to an explanation of the codes (https://forum.level1techs.com/t/list-of-dr-debug-bios-codes/114364). He was having issues with CPU initialization. I told him that sometimes re-seating CPU and RAM can help with these sorts of issues.\n\nRe-seating. This is also important.\n\nNext day he hits me up again: will I accept a return? No, because having installation difficulties is not a valid reason for return. Then nothing. Silence.\n\n### The Refund Claim\n\nCue the *very last day of the return window*: I get hit with an \"item not as described\" refund claim. Get this, the buyer:\n\n- uploaded photos of the motherboard with a bent and twisted CPU pin.\n- uploaded a photo of a blank white silkscreen rectangle on the motherboard with a giant red arrow pointing to it and a comment saying \"the motherboard is fake because of this white area\".\n- showed a photo of the computer monitor displaying the BMC interface in which the serial number of the BMC software was 1234567890ABCDEF. He claimed therefore the motherboard was a fake.\n\nWTF. I simultaneously exploded with rage at being accused of selling broken gear as working gear, while exploding with incredulity at the stupidity of trying to assert both damage AND blatantly ridiculous fakery in the same refund claim! My dude should have really picked just one fraudulent claim to keep it somewhat realistic, not two. I calmed down and figured the buyer probably bent the pins in a ham-fisted attempt to re-seat everything. No problem, I thought. I'll explain to eBay what's happening and they'll see reason before shutting this clown down. So I started going through the claim dispute process...\n\n### The Process\n\n...oh, the process. It's designed to (a) refund the buyer at the seller's cost in all cases, (b) be so egregiously demoralizing, time-consuming, and administratively difficult for sellers that they are incentivized to simply give up and accept the fleecing, and (c) automate as much of this process with as few humans in the loop as possible while simultaenously providing as few opportunities as possible for sellers to initiate any communication with eBay.\n\nIt went like this over a period of TWO MONTHS:\n\n- Report the buyer for \"abusing the returns process\".\n- With the new \"case\", it's possible to upload a set of photos and a block of text to refute the buyer's claim(s). \n- I uploaded ALL the hi-res photos I took for the listing's photoshoot in which it was abuntandly clear the motherboard was in perfect condition.\n- I also went to Gigabyte and found the page on the BMC's usermanual containing a screenshot showing the same serial number claimed by the buyer.\n- I went to Gigabyte's MZ33-AR1 web page and found a photo of the motherboard showing exactly the same white rectangle the buyer had called out as fakery.\n- Boom! Done! Solid documentary refutation of all the buyer's claims. Case closed. So I thought.\n- eBay found in favor of the buyer and instructed me to issue a return label.\n- I refused, outraged. No, I said. Look at the photos! He's lying!\n- eBay sent the buyer a label at my expense. He returned the motherboard with its busted CPU pin.\n- I again reported the buyer, showed photos of before and after damage, clearly showing he did the damage, not me.\n- eBay found in favor of the buyer AGAIN and deducted the full cost of the refund from my account.\n- Apoplectic, I hit the \"appeal\" button. I was taken to a webpage that said \"we'll call you in 3 minutes\". WTF?\n- 5 minutes later i got a call from eBay. \n- After briefly explaining the situation to a very engaged US-sounding representative, she told me I needed to do a couple of things:\n\t- Take the text of an email they just sent me (a Disclosure where I swear everything I told eBay is true) and paste it into a Word doc\n\t- Insert a photo/picture of my ink-written signature (luckily I have a scan of exactly that for business reasons).\n\t- Convert to PDF and upload to the secret link in the email they sent.\n\t- No joke, the lady actually stayed on the phone while I did all this! She received the PDF just seconds after I uploaded it.\n\t- This is, I am sure, mostly just another way of making it difficult to actually reverse the appeal.\n- But the rep was good to her word: eBay immediately reversed the decision and the money is back in my account as if the sale had happened like normal. I guess both me and the buyer got our money.\n\n### If It Happens To You\n\nMy advice if this happens to you: \n\n- Accept that no human cares about your case until the very, very last minutes of MONTHS of effort.\n- Accept that no matter what you do eBay will always automatically find in favor of the buyer.\n- Document everything contemporaneously and upload everything you possibly can when given opportunity to do so; you won't get any opportunities to do so again.\n- The data you upload is designed only for the human at the end of the appeals process, not someone looking at it during the claim process. Make it good. You'll need it later.\n- You're going to get enraged because during the claims process \"nothing makes sense\". It all makes sense: it's simply the cheapest way for eBay to handle this process at scale. Keep going.\n- Eventually eBay will find in favor of the buyer and close the case, automatically refunding the buyer \"on your behalf\". You will lose your money.\n- At this point you get the chance to appeal. BE READY. *This is the shot you've been waiting for all this time!* Have your phone, your laptop, your scanned signature, and a way to make PDFs ready BEFORE you initiate the \"call me\" feature.\n- Calmly explain what happened and request that common sense prevail. Ask that they refund your money. Common sense may actually prevail, assuming you made a good contemporaneous case with solid photographs, etc... and assuming you presented it well (not Mr Angry) on the phone... oh, and provided you can make and upload a PDF of your signature on-the-fly during the call!\n\nGood luck!\n\nEdit: please stop sending DMs asking for the eBay handle of the buyer. I'm not in the business of doxxing anyone. Thank you.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzt1q8/llm_server_gear_a_cautionary_tale_of_a_1k_epyc/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwt9lqp",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 22:30:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwspnh9",
          "author": "ismaelgokufox",
          "text": "My god. Being a seller in eBay is no joke.",
          "score": 78,
          "created_utc": "2025-12-30 20:55:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsrr86",
              "author": "blbd",
              "text": "Actually I think this data suggests that not only is it a joke, but that even an honest seller is the butt of it.Â ",
              "score": 50,
              "created_utc": "2025-12-30 21:05:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwv90k1",
              "author": "Guinness",
              "text": "Yep. eBay literally helped a scammer in South America steal my DSLR camera. The only reason I had my case reversed was because I had a quant friend from my days at an HFT firm end up working for a company that got absorbed by eBay. \n\nHe saved my ass.",
              "score": 9,
              "created_utc": "2025-12-31 05:30:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwuzqjv",
              "author": "az226",
              "text": "Being a buyer is no joke either went they send you faulty stuff. Had a seller sell me good GPUs and trusted him. Return window passed and then he had started sending me bent pins GPUs. 9 in total. Like $7k. \n\nAnother seller here on reddit Rhino/Core4 sold me GPUs with bad memory. I think 11 of 31 were bad or something. $8k. Refused to believe it, claimed I didnâ€™t know what I was doing. Scam of a company. They also sell on eBay. Avoid at all costs. They offer was something like $200 store credit if I returned the GPUs. Hah. Morons. Then they got their feelings hurt and took back that shit offer. When I have time and energy Iâ€™ll take them to small claims court. Losers.",
              "score": 6,
              "created_utc": "2025-12-31 04:26:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsto3p",
          "author": "MrPecunius",
          "text": "This is consistent with the experience I had selling a three channel stepper motor driver board on Ebay many years ago. Obvious buyer-inflicted damage, easily seen in photos etc.\n\nJust like your case, Ebay refunded the fraudster and I ended up keeping my money too. I stopped selling on Ebay after that.",
          "score": 35,
          "created_utc": "2025-12-30 21:14:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuhpri",
              "author": "night0x63",
              "text": "(I don't have the whole story... I just use eBay casually as buyer and little selling. )\n\n\nI think in the beginning of eBay before Amazon and other online stuff... Buyers got screwed by fraudsters all the time. eBay still made good money.\n\n\nThen Amazon came... Buyers all moved there because no more hours of dealing with fraudster sellers. eBay core business at risk.\n\nThen more recently last approximate five ten years... eBay pendulum has swung heavily... Now eBay competes with Amazon and so heavily favors buyers. So your story and this OP story show eBay favoring buyers. because eBay competes with Amazon.\n\n(Again I don't have all the story... This is just my experience and observation.)",
              "score": 5,
              "created_utc": "2025-12-31 02:35:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3lvrj",
                  "author": "maz_net_au",
                  "text": "Amazon isn't much better. Search for scams where people receive a bag of sand / rocks which are the same weight as the original item. Amazon processes it's returns based on the weight of the item if its in shrink-wrap, even if the original item didn't come in shrink-wrap.\n\nAnyway, good luck getting your money back with a dispute \"Instead of an item, I received a bag of sand.\" Some dude years ago filming himself unboxing what was meant to be a very expensive digital camera only to be on the receiving end of this crap and Amazon only refunded their money after public backlash (a video of unboxing the item is apparently not evidence enough).",
                  "score": 2,
                  "created_utc": "2026-01-01 16:35:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwujr05",
              "author": "__JockY__",
              "text": "Shame, really. But.... I too will avoid selling high value items on eBay in future, I have no desire to burn so many hours on pointless stress.",
              "score": 3,
              "created_utc": "2025-12-31 02:47:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwv1w1g",
                  "author": "jeffwadsworth",
                  "text": "This.  It just isn't worth the stress at all.",
                  "score": 2,
                  "created_utc": "2025-12-31 04:40:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwt31br",
          "author": "a_beautiful_rhind",
          "text": "I stopped selling on ebay because the customers have carte blanche to rip you off. He could have sent you a brick and you'd still have to contact a freaking US rep to get any traction.\n\nI think big time sellers just eat the fraudsters as part of doing business, but as an individual, you really can't. You barely make anything after the fees as it is and ebay seems to think mailing things back and forth is free.\n\nIts an absolute miracle you were able to keep your money at all.",
          "score": 23,
          "created_utc": "2025-12-30 21:58:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwst6ps",
          "author": "HelpingForDoughnuts",
          "text": "Man, the part where you had to stay on the phone making a PDF with your scanned signature while the rep waitedâ€¦ thatâ€™s peak â€œwe made this intentionally annoying so people give up.â€ Kafka would be proud.\nI went through something similar selling a 3090 a couple years back. Buyer claimed it was â€œartifactingâ€ and sent photos that were clearly just him running Furmark with the OC slider maxed. eBay sided with him, I got the card back with thermal pads missing (???), and I justâ€¦ gave up. Didnâ€™t know about the appeal phone call thing. Wish I had.\nThe real lesson here is what you said about documentation. Photos arenâ€™t just for the listing, theyâ€™re evidence for the trial you donâ€™t know is coming yet. I photograph serial numbers now too after getting burned.\nAlso lol at the buyer trying to claim BOTH damage AND fakery. Pick a lane my dude.\nSidebar: howâ€™s the Supermicro treating you with the Blackwell setup? Been eyeing the H14 boards but havenâ€™t pulled the trigger.",
          "score": 32,
          "created_utc": "2025-12-30 21:11:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsugl9",
              "author": "__JockY__",
              "text": "Thanks!\n\nThe Supermicro has been flawless. It booted first time, all four 6000 Workstation GPUs worked at PCIe 5.0 x16 (using MCIO shenanigans), and itâ€™s just been great. Love it.\n\nThe MZ33-AR1 was never designed for the large BAR of the Blackwells and Gigabyte support kinda just threw up their hands and gave up on it.",
              "score": 12,
              "created_utc": "2025-12-30 21:17:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwtgp8q",
                  "author": "HelpingForDoughnuts",
                  "text": "Four 6000s at full x16 is wild. The MCIO routing on those H14 boards is underratedâ€”people sleep on Supermicro because itâ€™s not â€œenthusiastâ€ but they actually engineer for this stuff.\nGigabyte throwing up their hands on large BAR is frustrating but not surprising. Their EPYC boards feel like an afterthought compared to their consumer stuff. At least you got most of your money back on the sale.\nWhatâ€™s the use case for the rigâ€”inference serving, training, or a mix? That much Blackwell horsepower has to be doing something fun.",
                  "score": 2,
                  "created_utc": "2025-12-30 23:07:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwu2603",
                  "author": "segmond",
                  "text": "The mz33-ar1 is fincky, even with 3090s, I occasionally have issues with it.  I hate the damn board.",
                  "score": 1,
                  "created_utc": "2025-12-31 01:05:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsxzx4",
          "author": "MelodicRecognition7",
          "text": "...and on top of that Ebay charges sellers 20% fees lol. Unfortunately it is too large and we do not have better alternatives, all other marketplaces are drop in the ocean, nobody will see your listings there.",
          "score": 13,
          "created_utc": "2025-12-30 21:34:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsub66",
          "author": "Aggressive-Bother470",
          "text": "Signed affidavit because they couldn't be bothered to check your evidence? :D\n\n\nOr... they now believe all photo evidence is null and void.",
          "score": 10,
          "created_utc": "2025-12-30 21:17:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwulha9",
              "author": "__JockY__",
              "text": "I've thought about it a bit now, and my guess is that below a certain dollar value no human ever actually takes more than a cursory glance at the case data (and only at the final appeal stage). It's quicker and simpler to just require the affidavit for CYA purposes, refund the money, and move on. It minimizes human hours, covers eBay in the case of later litigation, and would appear to be the least bad option available to eBay for this particular eventuality. \n\nOf course, it also incentivizes eBay to make the sellers' lives as miserable as possible in these situations, but I guess that's the cost of efficiency, eh?",
              "score": 3,
              "created_utc": "2025-12-31 02:57:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt1q8o",
          "author": "Purple-Programmer-7",
          "text": "I have 4x 3090s to sell. Iâ€™ll happily let them sit and depreciate vs trying to sell them via eBay",
          "score": 8,
          "created_utc": "2025-12-30 21:52:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt3cne",
              "author": "a_beautiful_rhind",
              "text": "try craigslist locally and just meet at a police station or other such place. as a bonus, no taxes.",
              "score": 4,
              "created_utc": "2025-12-30 21:59:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwtzbgc",
              "author": "David_Delaune",
              "text": ">I have 4x 3090s to sell. Iâ€™ll happily let them sit and depreciate vs trying to sell them via eBay\n\n\nI've had really good experience so far with buying and selling on /r/homelabsales",
              "score": 3,
              "created_utc": "2025-12-31 00:48:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtzgro",
                  "author": "Purple-Programmer-7",
                  "text": "Got another recommendation for that previouslyâ€¦ I think thatâ€™s where Iâ€™ll land when I get around to it!",
                  "score": 3,
                  "created_utc": "2025-12-31 00:49:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwunyvp",
              "author": "__JockY__",
              "text": "3090s will sell themselves on any market right now! I still like CL because once you get past the \"is the item still available\" type scams, meeting in person for cash is still king.",
              "score": 3,
              "created_utc": "2025-12-31 03:12:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx3m6yn",
              "author": "munkiemagik",
              "text": "Totally understand that sentiment, I've already spent the money so I accept it as gone from my accounts and have mentally written it off. So I'd rather let it depreciate in my possession than let some lowlife benefit from it on my dime.\n\nAs I mentioned to someone in here in another comment, I keep uuhming and aahing about whether to offload my hobby LLM server and move on to something else for a bit, like maybe grab myself a new e-mtb, (those Megamo's are looking bloody appealing) and retire the old faithful human powered mtb, OR double down and invest further into the rig for moaar power to re-excite me again. I havent fired up the LLM server in almost two weeks, granted Christmas with the siblings/cousins/nephews/nieces was hectic.\n\nIf selling wasn't so fraught with danger, in a moment of weakness I might have already jumped off the LocalLLama ship.",
              "score": 1,
              "created_utc": "2026-01-01 16:36:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsvasa",
          "author": "munkiemagik",
          "text": "I'm so glad this worked out for you and you didnt lose your money and a working product. But its horrific stories like this (usually without the happy ending) that keep me far away from ebay when it comes to selling things. I know this restricts my buyer exposure but I only do local facebook marketplace so the buyer can check and verify item function before parting with their cash. \n\nI sold my old 4090 to fund some 3090s for the LLM server and the buyer actually took a two and a half hour train to come down and buy the GPU in person because I refused to post it, though I did set the price a little lower than what was going market rate, I just wanted rid of it so I could start populating the LLM server with the proceeds.\n\nBut lately I've been contemplating offloading my Zen2 Threadripper Pro machine, I havenâ€™t been lurking in r/LocalLLaMA recently as much as I used to. But too scared to deal with the scammy public so instead of selling I might just double down on my investment and misery and upgrade it to Zen3 Threadripper Pro. If I could just find a 5965WX at a reasonable price I could almost double my system memory bandwidth.",
          "score": 8,
          "created_utc": "2025-12-30 21:21:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtizyu",
              "author": "UniversalSpermDonor",
              "text": "Oddly enough, I'm actually about to sell a 5965WX off - let me know if you're interested. (That said, unless you're willing to come to the outskirts of the DC area, I'll have to ship it to you.)",
              "score": 2,
              "created_utc": "2025-12-30 23:19:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtw26j",
                  "author": "munkiemagik",
                  "text": "I'm in the UK, by DC I take it you mean District of Columbia?",
                  "score": 1,
                  "created_utc": "2025-12-31 00:31:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsqwgx",
          "author": "Marksta",
          "text": "Holy moly dude, that's exactly why I just don't with tech stuff and reselling. I think all marketplace users kinda know the gambit at this point for bad faith craziness. But it's so much easier on buyer side because they don't even need any rep, they just click buy and can take an honest seller for a ride from there. Especially with all the GPU chip stealing off PCB stuff going on too.\n\nI've seen the warnings on ebay listings and I think Newegg too? They 1000% don't take a refund on new EPYC boards once you pop off the CPU slot cover(s) since people just can't help themselves but break $1k mobos it seems ðŸ˜‘\n\nSorry that happened to ya, glad you persisted down the 100 steps process to arrive at a refund.",
          "score": 5,
          "created_utc": "2025-12-30 21:01:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwunmyy",
              "author": "__JockY__",
              "text": "I learned the hard way that those warnings don't mean shit. \n\nAny one of us could buy one of those Newegg EPYC motherboards, take a shit on it, claim it was \"not as advertised because it came covered in poo\", and we'd get an automatic full refund from eBay.",
              "score": 1,
              "created_utc": "2025-12-31 03:10:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt3rfl",
          "author": "tengo_harambe",
          "text": "I sold my old 4060 Ti on fb marketplace recently. He reached out, I sent him a video of the GPU to prove it worked. We arranged to meet in a public spot the next day and he was on time with the exact amount of cash we agreed on. Made the exchange and went our separate ways. Never heard a peep from him again. 10/10 selling experience. May you always have a buttery smooth 100 fps Fornite experience, Randy!",
          "score": 5,
          "created_utc": "2025-12-30 22:01:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuo30t",
              "author": "__JockY__",
              "text": "This is the way.",
              "score": 1,
              "created_utc": "2025-12-31 03:12:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtagyw",
          "author": "RevolutionaryLime758",
          "text": "I sold a monitor to a guy, he didnâ€™t want it so it he stomped on it and uploaded an image that showed his boot print. Sent me back e-waste and eBay stuck me with the bill. Of course, eBay keeps the fees while demanding you refund in full.\n\neBay wants its buyers to feel like itâ€™s Amazon, so they demand you as a seller provide the same level of service of the slave driving mega corporation. Guaranteed delivery windows, easy returns with minimal pushback, etc. basically makes it so you need to be running a business yourself to really stand a chance at absorbing the fraudulent returns.\n\nLike you, I was gaslit about the images, told a human reviewer looked at it despite a rejection of my appeal within 3 minutes. After seller fees and shipping I barely get any money from eBay but Iâ€™ve made them plenty. Unlike you, I was not calm about it. I sent angry emails and called every day. Eventually it worked, but I wouldnâ€™t be able to tell you what I said that did. Just kept calling angry.\n\nAs far as the enthusiastic sounding voice, I swear to god on one of the calls it was one of the GPT voices.",
          "score": 4,
          "created_utc": "2025-12-30 22:34:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsy8dp",
          "author": "Techngro",
          "text": "I had an experience like that many years ago on eBay. Turned me off. I'll buy things, but selling is out of the question. What we need is an online pawn/consignment shop.",
          "score": 4,
          "created_utc": "2025-12-30 21:35:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtidmn",
          "author": "RnRau",
          "text": "Complete aside from selling on ebay shenanigans... it felt good reading stuff written by a human. \n\nWhich feels weird and sad at the same time...",
          "score": 4,
          "created_utc": "2025-12-30 23:16:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtwfpv",
              "author": "__JockY__",
              "text": "Thank you, I feel the same way.",
              "score": 1,
              "created_utc": "2025-12-31 00:33:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt0dip",
          "author": "Actual__Wizard",
          "text": ">uploaded photos of the motherboard with a bent and twisted CPU pin. \n\nYep. I was thinking that the pins were going to be ruined the entire time I was reading it. So, they broke it and want their money back, and you're probably going to end up with a broken mother board.",
          "score": 3,
          "created_utc": "2025-12-30 21:45:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtdb56",
          "author": "ufrat333",
          "text": "Hah, selling on eBay, they froze our account with 15k to be paid out after three weeks of sales without any disputes, all tracking added., 100% positive feedback on 50 orso orders.\n\nNo reason, no way to contact them, or well - you can talk to an Indian who can do exactly nothing and ask how your family is doing. Just when we were finding a lawyer in Germany to drag them to court - 4 months later - I sent some linkedin messages to random employees about this intention - they suddenly released the funds but left the account blocked without any further communication. I guess if my LinkedIn didn't mention working at said company a lot of years prior nothing would have happened.",
          "score": 3,
          "created_utc": "2025-12-30 22:49:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtvp3y",
              "author": "__JockY__",
              "text": "Yeah Iâ€™m done as a seller.",
              "score": 1,
              "created_utc": "2025-12-31 00:29:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtdcul",
          "author": "fluffywuffie90210",
          "text": "I had almost this exact thing happen with a faulty motherboard that was sold as broken spares/repair, the guy tried to fix it, couldnt so then tried to claim it was item not as described, FOR A BROKEN ITEM. God ebay sided with him but i got some advice on how to appeal on the forum... and somehow ebay sided with me once it got the rep side of things. Was 150Â£ but still... has to be an ebay issue.",
          "score": 3,
          "created_utc": "2025-12-30 22:49:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtvtq5",
              "author": "__JockY__",
              "text": "Yup, itâ€™s just them protecting buyers at scale because itâ€™s ultimately better for business.",
              "score": 1,
              "created_utc": "2025-12-31 00:30:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtdmkm",
          "author": "Lesser-than",
          "text": "Yeah for simular reasons I never count on resale value of anything computer related anymore, I just consider it sunk cost when purchasing. If I upgrade and never see myself using it again I give it to someone who will. You just can not depend on people to figure things out by themselves, more cases than not only want to return it after they are sure they broke it.",
          "score": 3,
          "created_utc": "2025-12-30 22:50:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtw13i",
              "author": "__JockY__",
              "text": "This is what Iâ€™m hearing about. People just abuse eBay as a try-before-you-buy site, or they buy working gear and return it as not working after swapping out the parts for themselves.",
              "score": 1,
              "created_utc": "2025-12-31 00:31:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtune1",
          "author": "abnormal_human",
          "text": "Sucks. I bought a $1500 Epyc CPU that turned out to be bad and ended up stuck with it. Stopped doing this crap on eBay after that.",
          "score": 3,
          "created_utc": "2025-12-31 00:23:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtyrlk",
              "author": "__JockY__",
              "text": "Ugh.\n\nIn the opposite story, I bought a $1400 9B45 EPYC and it turned out to be perfect and an alternate SKU for the $10k+ 9755!!",
              "score": 1,
              "created_utc": "2025-12-31 00:45:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwu1iq5",
          "author": "FormalAd7367",
          "text": "I sold an expensive vintage Iwc watch on ebay and buyer claimed he received rocks.  He took my watch and my money.",
          "score": 3,
          "created_utc": "2025-12-31 01:01:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwu2ejt",
              "author": "__JockY__",
              "text": "I honestly donâ€™t know how these assclowns sleep at night after fleecing common people.",
              "score": 1,
              "created_utc": "2025-12-31 01:06:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwuuivp",
          "author": "SkyFeistyLlama8",
          "text": "Nice on ya for putting the \"no AI\" disclaimer\" at the start. I couldn't help reading out the entire text in a Law & Order voice LOL!",
          "score": 3,
          "created_utc": "2025-12-31 03:52:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv1esg",
          "author": "Intelligent-Form6624",
          "text": "> Shit, I didn't even use spell check.\n\nDude, this is some off-grid shit. Did you also write a letter using nothing but a pen? #NoWhiteOut",
          "score": 3,
          "created_utc": "2025-12-31 04:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt1usm",
          "author": "Caryn_fornicatress",
          "text": "Brutal but accurate lesson, eBay is optimized for buyer protection at scale, not truth or evidence\n\nHigh value hardware plus no returns is basically asking for this risk, the system only really listens at the human appeal stage\n\nYour takeaway about documenting everything for the final call is the real gold here, anything before that is just feeding a machine\n\nHonestly for gear like this, local sale or escrow based platforms feel way safer than eBay now",
          "score": 4,
          "created_utc": "2025-12-30 21:52:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwulx32",
              "author": "__JockY__",
              "text": "> Honestly for gear like this, local sale or escrow based platforms feel way safer than eBay now\n\nYup. Good ol' Craigslist and Starbucks and cash.",
              "score": 1,
              "created_utc": "2025-12-31 02:59:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt8hjg",
          "author": "alphatrad",
          "text": "I just went through this shit with some asshole myself when selling my Valve Index. Such a nit picky complaint and the whole thing is setup to punish the seller. The guy literally cost me 100 dollars as I offered free shipping and then had to pay to ship it back to myself.",
          "score": 2,
          "created_utc": "2025-12-30 22:24:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwupb4r",
              "author": "__JockY__",
              "text": "I think the trick is to simply refuse to buy the return shipping label despite their outpouring of awful \"nice reputation you got there, shame if something happened to it\" emails. I forgot to mention those in the main story, but they tried to intimidate me into sending a return label under threat of reputational damage.\n\nI told them to pound sand.\n\neBay still sent the return shipping label and billed me for the convenience, but at least this way I didn't accept the return/refund and scored a small \"fuck you\" victory in their bullshit game.",
              "score": 3,
              "created_utc": "2025-12-31 03:20:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwta1ql",
          "author": "LetterRip",
          "text": "How difficult would it have been to take the buyer and eBay to small claims court?",
          "score": 2,
          "created_utc": "2025-12-30 22:32:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtuwv9",
              "author": "__JockY__",
              "text": "Oh thatâ€™s 100% the route Iâ€™d have taken if the appeal failed. No idea of time, cost, or effort required though.",
              "score": 2,
              "created_utc": "2025-12-31 00:25:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtizwp",
          "author": "AnomalyNexus",
          "text": "Yeah selling on eBay is an act of faith. I try to keep things below 200 bucks just in case.",
          "score": 2,
          "created_utc": "2025-12-30 23:19:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtykps",
              "author": "__JockY__",
              "text": "I may just follow this advice. I donâ€™t have FB so no marketplace for me :/",
              "score": 1,
              "created_utc": "2025-12-31 00:44:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwu2ckh",
          "author": "segmond",
          "text": "I have so much gear that I wish to sell for someone that's into local LLM, but I don't want to go through this sort of ebay pain.  I wonder if \"no return/no refund\" will work.",
          "score": 2,
          "created_utc": "2025-12-31 01:06:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuq9ly",
              "author": "__JockY__",
              "text": "No, it absolutely does not work. \n\nMy listing was marked as \"no return/no refunds\" in the listing's settings and my story seems common after doing a bunch of research into this matter. Your listing settings are irrelevant because the instant a buyer files a \"not as described\" claim, the automatic process begins and they end up with a refund at your expense. There's no escape. You just have to prepare everything along the way for the moment of final human appeal and hope you laid your breadcrumbs well enough along the way to support a successful appeal.",
              "score": 2,
              "created_utc": "2025-12-31 03:26:08",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwug85f",
              "author": "Ryuma666",
              "text": "Where are you from?",
              "score": 1,
              "created_utc": "2025-12-31 02:26:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvc5i8",
                  "author": "segmond",
                  "text": "Michigan",
                  "score": 1,
                  "created_utc": "2025-12-31 05:54:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwu8kkm",
          "author": "Savantskie1",
          "text": "they automatically find in favor of the buyer because in the early days of Ebay, there were tons of scammers selling on Ebay and then either sending faulty devices, or ripping people off with sending a photo of the item, but not the original item with clever wording in the description that makes people think they are buying the item, when instead they're agreeing to buy a picture of the item. So the responsibility has been put on the sellers to prove their case and Ebay err's on the side of the buyer in protection of them. It solves the problems of people suing Ebay because they weren't doing enough to protect buyers.",
          "score": 2,
          "created_utc": "2025-12-31 01:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwur3ga",
              "author": "__JockY__",
              "text": "Yes, exactly this. The buyer could've taken a dump on the motherboard and sent it back \"not as described\" for all eBay cared. They'd still automatically refund him out of my account. And by making it as difficult as possible for sellers to get to the moment of appeal, they reduce costs enormously.\n\nJust lay the groundwork for the moment of appeal... or don't sell expensive stuff on eBay, I guess.",
              "score": 1,
              "created_utc": "2025-12-31 03:31:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwuuayx",
          "author": "markcartwright1",
          "text": "Ok take a deep breath because it's over now! You were unlucky and you had a clumsy and clueless buyer. Its high stakes because it was a high value product too. And it turned into a drawn out ordeal.\n\nI've sold about 350 tech items on UK ebay, phones smartwatches laptops, mini PCs in the past 6 months. Generally new, but maybe 20-30% second hand. Maybe 1-2% have had issues, courrier issues mostly or sometimes a product was faulty. I often write about who its not for and who shouldn't buy a product - so that deters the unsuitable buyers. \n\nI offer and allow returns because it lifts the conversion rate and if someone changes their mind or they're not happy - they won't sabotage the product. I've kept away from the more expensive end of the market so the risk is spread across various products too. Ironically sometimes the returned products have sold for more than the original price. \n\nThe thing with Motherboards is they can be very fragile or very specific. Like some people might not know certain slots, or compatibility before buying. You may need to offer more hand holding. And this buyer was clumsy. \n\nIf anyone UK based here wants to sell any computer / LLM gear, drives, HDDs, SSDs, old phones - I can give you a price and arrange shipping / as well as safe payment with Paypal. Or we can run it through Ebay if you prefer. I'm good at selling bits of tech and have a cash pile to buy decent stock. I can save you from an ordeal like this. Drop me a DM and let me know what you're selling and what you'd like for it.",
          "score": 2,
          "created_utc": "2025-12-31 03:51:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv72r8",
          "author": "fred100002",
          "text": "I had the exact same experience a couple of times this year. In your write up, you mentioned reporting the buyer Â´againâ€™  but IIRC, in my examples, the system wouldnâ€™t let me report them twice and told me Â´you already reported the buyer for thisâ€™ â€¦ I also donâ€™t remember getting to an appeal button either.",
          "score": 2,
          "created_utc": "2025-12-31 05:16:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt46s1",
          "author": "No_Afternoon_4260",
          "text": "Why no 4 way blackwell on the gigabyte? Does it work on the supermicro?",
          "score": 1,
          "created_utc": "2025-12-30 22:03:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt5o3y",
              "author": "__JockY__",
              "text": "It throws PCI errors, only sees 3 cards, etc. But the supermicro has been flawless, I love it.",
              "score": 1,
              "created_utc": "2025-12-30 22:10:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwtbcr4",
                  "author": "No_Afternoon_4260",
                  "text": "wow good to know thx !  \nSo you did sell a non working board ðŸ˜…",
                  "score": 3,
                  "created_utc": "2025-12-30 22:39:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwui75o",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2025-12-31 02:38:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwutsb7",
          "author": "Sabin_Stargem",
          "text": "If I am to be perfectly honest, as a potential buyer of 2nd-hand gear, this situation sounds good for me.   I fear losing my money for a waste of time, and putting together an premium machine would seriously damage my savings even if everything went perfectly.\n\nO'course, your take on the situation is perfectly valid and reasonable.   Just saying that it makes me more seriously consider using Ebay for the next build.   Getting a top-end Threadripper for a much reduced price at less risk is incredibly appealing.\n\nIt is a perverse incentive, where greed and fear overrule fairness.   In any case, it is good for you to post a \"seller beware\" story, so that honest folks don't get deliberately ripped off by buyers.",
          "score": 1,
          "created_utc": "2025-12-31 03:48:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwymcx8",
          "author": "Phaelon74",
          "text": "Stop selling on ebay, it is horrible for sellers.",
          "score": 1,
          "created_utc": "2025-12-31 19:09:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9yspm",
          "author": "DevopsIGuess",
          "text": "I have this same motherboard, real PITA to setup. \nHe likely needs to run firmware upgrades via the BMC management  port. \n\nFeel free to send them my way. This board was a bitch to get working.",
          "score": 1,
          "created_utc": "2026-01-02 16:30:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxato8n",
              "author": "__JockY__",
              "text": "Send who your way?",
              "score": 1,
              "created_utc": "2026-01-02 18:53:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsz2wl",
          "author": "LyriWinters",
          "text": "you should obviously have taken the return...",
          "score": -8,
          "created_utc": "2025-12-30 21:39:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwurhoo",
              "author": "__JockY__",
              "text": "Had I accepted the return I'd be left with a broken motherboard instead of a mint condition motherboard worth $900. I'd have lost my money and arguably worse, I'd have backed down like a fucking pussy against a liar and a cheat. Nope.",
              "score": 2,
              "created_utc": "2025-12-31 03:33:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwuwekk",
                  "author": "LyriWinters",
                  "text": "Pretty sure he broke it to have a reason for ebay though... So you would have gotten your motherboard back. But it is just a presumption. But logic checks out.\n\nit worked out this time for you, next time probably not. you \"burned\" that bridge in the eyes of Ebay now. Sorry.   \n  \nThis is why I love Sweden, I can do a background check on the people I do business with. I can literally find adress, social security number, last declared salary, any time spent in the court system... And it all takes me roughly 5 minutes an costs me â‚¬1.",
                  "score": 1,
                  "created_utc": "2025-12-31 04:04:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q2o033",
      "title": "What is the smartest uncensored nsfw LLM you can run with 20GB VRAM and 24GB RAM",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/",
      "author": "Death_12_35_taken",
      "created_utc": "2026-01-03 07:04:18",
      "score": 193,
      "num_comments": 75,
      "upvote_ratio": 0.84,
      "text": "I am looking for something that can stay in character and be fast but also creative. I am looking for models that i can run locally and at decent speed. Just need something that is smart and uncensored. ",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nxeh4xh",
          "author": "Narrow-Belt-5030",
          "text": "There is a dolphin model that i use that may be of interest: Dolphin-Mistral-24B-Venice-Edition\n\nhttps://huggingface.co/dphn/Dolphin-Mistral-24B-Venice-Edition",
          "score": 83,
          "created_utc": "2026-01-03 07:38:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxele1u",
              "author": "Death_12_35_taken",
              "text": "just tried the quantized version of this and the answers are good but a bit slow and for some reason there is this waiting time between when i finish typing and the token generation more than usual plus the gpu coil is roaring. Maybe i am doing something wrong. I got text gen web UI should i use something else to run this model?",
              "score": 14,
              "created_utc": "2026-01-03 08:14:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxelpdb",
                  "author": "Death_12_35_taken",
                  "text": "but yeah this model is pretty good for who can run it properly. The responses are good quality.",
                  "score": 8,
                  "created_utc": "2026-01-03 08:17:23",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxex04p",
          "author": "Longjumping-Bee-6977",
          "text": "Pick one\nhttps://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard",
          "score": 28,
          "created_utc": "2026-01-03 09:55:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxi7x0k",
              "author": "IpppyCaccy",
              "text": "What do the T and R columns mean?",
              "score": 3,
              "created_utc": "2026-01-03 20:58:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxiykxw",
                  "author": "autoencoder",
                  "text": "T seems to be model type: Proprietary, Base, Finetune, Merge. R whether it is a reasoning model (thinks before answering).",
                  "score": 5,
                  "created_utc": "2026-01-03 23:11:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxgp3v0",
          "author": "RottenPingu1",
          "text": "Can I highjack the thread to ask the same question in a 70B model?",
          "score": 9,
          "created_utc": "2026-01-03 16:41:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgpy9f",
              "author": "Awwtifishal",
              "text": "Probably Llama-3.3-70B-Instruct-heretic-v2",
              "score": 4,
              "created_utc": "2026-01-03 16:45:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgsamy",
                  "author": "RottenPingu1",
                  "text": "Thank you. I've been using zerofata/L3.3-GeneticLemonade-Opus-70B but am always on the lookout for more options.\nI've run into several comments that say a 32B model is better for rp, a 70B like a sports car with a governor when used in that role.",
                  "score": 2,
                  "created_utc": "2026-01-03 16:56:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxjklaj",
          "author": "xoexohexox",
          "text": "Cydonia/Magidonia 24b by thedrummer and Dan's Personality Engine 24B compare favorbly to the frontier models IMO and they refuse nothing, not even NSFL. Only outdone by Claude which is expensive and excessively prudish. There's a little private benchmark I run where I ask a librarian for a book and I can't remember the title but I remember some things about it - those two almost always get the references. \n\nIf you hunt around on huggingface you'll find some decent franken-merges of those two models as well like bereavedcompound and weirdcompound 24b which are also very good.",
          "score": 8,
          "created_utc": "2026-01-04 01:07:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxfgv4p",
          "author": "IKarlMetherlance",
          "text": "[https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated](https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated)",
          "score": 12,
          "created_utc": "2026-01-03 12:38:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgcedf",
              "author": "MullingMulianto",
              "text": "surprised qwen runs on 24gb ram",
              "score": 0,
              "created_utc": "2026-01-03 15:41:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxi1cvu",
                  "author": "IKarlMetherlance",
                  "text": "oups 22gb vram with 32k context on i1-Q4\\_K\\_M + mmproj-f16 , so remove mmproj/reduce context/take lower quantization",
                  "score": 1,
                  "created_utc": "2026-01-03 20:25:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfbxav",
          "author": "Javanese1999",
          "text": "[https://huggingface.co/mradermacher/gemma-3-27b-it-abliterated-normpreserve-i1-GGUF](https://huggingface.co/mradermacher/gemma-3-27b-it-abliterated-normpreserve-i1-GGUF)\n\nfor starter just go with original gemma 3 with abliterated fine tune. Pick Iq4\\_XS for your hardware specification.",
          "score": 6,
          "created_utc": "2026-01-03 12:00:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxirw09",
              "author": "Ethrillo",
              "text": "Bump up for this. Using it for weeks now and its just so good.",
              "score": 2,
              "created_utc": "2026-01-03 22:37:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxkhkn7",
                  "author": "Javanese1999",
                  "text": "You right, this is best original gemma abliterated model with higher NatInt score (no.1) according to ugi benchmark.",
                  "score": 3,
                  "created_utc": "2026-01-04 04:14:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxn2kpm",
              "author": "_glimmerbloom",
              "text": "Abliterated Gemma is great. It will cheerfully explain how to make meth, assassinate the president, or commit a terrorist attack.\n\nIncluding context, it uses about 30GB of memory though.  Not sure how a smaller quant would perform.",
              "score": 2,
              "created_utc": "2026-01-04 15:44:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxwdfzr",
                  "author": "Lakius_2401",
                  "text": "It's designed to use SWA for cache compression. It will use a boatload of memory until you turn that on. Alternatively, quant the KV cache more.",
                  "score": 1,
                  "created_utc": "2026-01-05 22:16:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxeuoe2",
          "author": "FinBenton",
          "text": "I would say probably this https://huggingface.co/Naphula/Goetia-24B-v1.1-GGUF",
          "score": 3,
          "created_utc": "2026-01-03 09:36:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfc27n",
              "author": "tat_tvam_asshole",
              "text": "It's OK, but a bit 'dramatic'. 4.2.0-brokentutu q8 is by far the best model I've tried. Really exists in a sweet spot of intelligence and memory.\n\nhttps://huggingface.co/ReadyArt/4.2.0-Broken-Tutu-24b",
              "score": 4,
              "created_utc": "2026-01-03 12:01:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxg5ytm",
                  "author": "intermundia",
                  "text": "Oh this sounds interesting.",
                  "score": 1,
                  "created_utc": "2026-01-03 15:08:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxhp3yb",
                  "author": "Z0mbiN3",
                  "text": "Can you do q8 of tutu on a 3090? 64GB RAM",
                  "score": 1,
                  "created_utc": "2026-01-03 19:26:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny63gfx",
          "author": "Aphid_red",
          "text": "Check the [https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard](https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard) \n\nSince 3-4bit is optimal and 20GB VRAM means 20\\*10\\^9 bytes = 180 \\* 10\\^9 bits this fits 45-60 billion parameters. There's a bit of overhead, so we want to search for <40B models. \n\nAdd '<40' in the '#P' search box and out comes Skyfall-31B-v4 as the highest rated model. [https://huggingface.co/TheDrummer/Skyfall-31B-v4](https://huggingface.co/TheDrummer/Skyfall-31B-v4)\n\nNote: Your preferences may be different from the typical user though so depending on what kind of nfsw or thing you're writing you may want to hone in on other things than the general score. \n\nSort by 'NatInt' for a model of the desired W/10 if model quality trumps NSFW ability above a certain point.   \n[https://huggingface.co/YanLabs/gemma-3-27b-it-abliterated-normpreserve](https://huggingface.co/YanLabs/gemma-3-27b-it-abliterated-normpreserve)  wins there.\n\nYou might want to value 'Writing' part for longer form content, so [https://huggingface.co/FlareRebellion/BereavedCompound-v1.0-24b](https://huggingface.co/FlareRebellion/BereavedCompound-v1.0-24b) would win there. \n\nIf 'nsfw' is about less left-wing bias in the model's baked in political opinion (it might try to claim it doesn't have any, but run a political compass test) making it difficult for it to properly roleplay a character with different views then check political lean being closer to zero. [https://huggingface.co/coder3101/Big-Tiger-Gemma-27B-v3-heretic-v2](https://huggingface.co/coder3101/Big-Tiger-Gemma-27B-v3-heretic-v2) would be a model that has that but also good W/10 and reasonable NatInt/Writing.  \n  \nFinally, if an absolute requirement is to not have any refusals/deviations even for the most deranged thing you can think of then you want as high W/10 as possible. [https://huggingface.co/coder3101/Cydonia-24B-v4.3-heretic](https://huggingface.co/coder3101/Cydonia-24B-v4.3-heretic) would be an example there. However, note that such models do tend to trade in a significant chunk of NatInt for that. Don't try to make it do any thinking.",
          "score": 3,
          "created_utc": "2026-01-07 08:53:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxga1zg",
          "author": "Blizado",
          "text": "Maybe this one: https://huggingface.co/mradermacher/Melinoe-30B-A3B-Thinking-i1-GGUF\n\nIt's based on Qwen 30B A3B Thinking and as this it is a fast MoE model which is even fast if it is not fully in VRAM. So you can decide yourself if you want it full in VRAM for fastest speed or put some layer into RAM and have more VRAM free for a bit larger context.\n\nIt is also very good rated for NSFW on the UGI leaderboard and the best Qwen 30B A3B finetune in this point. But not sure if it match into what you want to do with it. Thinking/Reasoning can normally easily be skipped but no clue how that impacts the models output, but that <think></think> part can also be easily miss used to push the LLM into the direction you want.",
          "score": 2,
          "created_utc": "2026-01-03 15:29:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhn76n",
          "author": "a3ydstm",
          "text": "Haven't been active here for a while. Anything for an aging M1  Macbook Pro base w/ 16GB ram/vram?\n\nI still have the NeralDareDevil from 1 half years ago that I should probbly delete.",
          "score": 2,
          "created_utc": "2026-01-03 19:17:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhr3pl",
              "author": "Death_12_35_taken",
              "text": "you can trying Impish\\_Bloodmoon it's a 12B model. you should be able to run it and it's pretty good for nsfw roleplay",
              "score": 3,
              "created_utc": "2026-01-03 19:35:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxir0mo",
          "author": "Fahrain",
          "text": "Mistral small 3.2 (at least q6, q4 is worse). It doesn't have a think mode, which is better if you want to generate a large text with complex logic (or simply splitted into chapters). You can easily stop it midway, correct errors in the text, and continue where you left off.\n\nYou could use Magistral Small 2509 - it has a think mode and it certainlly smarter. But it has a drawback: the entire text generation process must be run in one go.. And if a small deviation occurs during the \"think mode\" (or something important to the plot is simply missed), you could only drop everything and regenerate the entire text from the start.\n\nAnd newest version, Ministral-3-14B-Reasoning-2512, is certainly even smarter, thinking better and can generate long, voluminous texts. But the results from this generation... are simply unsatisfactory. I've reverted to the previous models.\n\nAny of them may write that they cannot accept your request - this can be easily corrected by clicking the \"regenerate\" button again or changing some brutal words in the request with more neutral synonyms.",
          "score": 2,
          "created_utc": "2026-01-03 22:32:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxeebv8",
          "author": "blbd",
          "text": "Give this a go.Â \n\nhttps://huggingface.co/ArliAI/gpt-oss-20b-Derestricted",
          "score": 6,
          "created_utc": "2026-01-03 07:14:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxekeut",
              "author": "Death_12_35_taken",
              "text": "what can i run this model on? i am using text generation web UI right now.",
              "score": 2,
              "created_utc": "2026-01-03 08:06:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxenoq5",
                  "author": "Narrow-Belt-5030",
                  "text": "Similar to other responses, use this one: gghfez/gpt-oss-20b-Derestricted-Q4\\_K\\_M-GGUF (16Gb in size, Q4 so will run reasonably, M size so trade off between size & speed)",
                  "score": 1,
                  "created_utc": "2026-01-03 08:34:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxet5be",
          "author": "mission_tiefsee",
          "text": "Try this one: Q5_K_M or Q4_K_M\n\nhttps://huggingface.co/TheDrummer/Precog-24B-v1-GGUF",
          "score": 3,
          "created_utc": "2026-01-03 09:22:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxfdmkp",
          "author": "xGamerG7",
          "text": "I like gemma 3 27b amoral it can pretty much generate anything with the intelligence of gemma",
          "score": 3,
          "created_utc": "2026-01-03 12:13:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxg5glb",
              "author": "intermundia",
              "text": "Would this model work in lm studio?",
              "score": 1,
              "created_utc": "2026-01-03 15:06:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgdwgj",
                  "author": "Equivalent_Bit_461",
                  "text": "If there's a gguf version \nSure",
                  "score": 2,
                  "created_utc": "2026-01-03 15:48:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxixhdt",
          "author": "ZhopaRazzi",
          "text": "2026 is the year of AI gooning",
          "score": 2,
          "created_utc": "2026-01-03 23:05:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxixs2c",
          "author": "autoencoder",
          "text": "I'm tempted to suggest https://huggingface.co/p-e-w/gpt-oss-20b-heretic-v2\n\nBut I don't like how much gpt-oss thinks.\n\nMaybe this one, but it's dense. I much prefer MoEs. I couldn't find a Qwen3-30B-A3B-Instruct-2507 (my favorite model) [hereticify-d](https://github.com/p-e-w/heretic).\n\nhttps://huggingface.co/mradermacher/Qwen3-14B-heretic-GGUF",
          "score": 1,
          "created_utc": "2026-01-03 23:07:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjdlq9",
          "author": "skocznymroczny",
          "text": "https://huggingface.co/TheBloke/LLaMA2-13B-Tiefighter-GGUF is the best NSFW llm I tried so far.",
          "score": 1,
          "created_utc": "2026-01-04 00:30:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxem3ku",
          "author": "arcanemachined",
          "text": "I like this one:\n\nhttps://huggingface.co/mlabonne/gemma-3-27b-it-abliterated\n\nIt's a GGUF, so probably run it on whatever you're already using.",
          "score": 1,
          "created_utc": "2026-01-03 08:20:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf16pk",
              "author": "autodidacticasaurus",
              "text": "I tried this one but it seems a bit crazy sometimes, like repeating itself non-stop for example.",
              "score": 3,
              "created_utc": "2026-01-03 10:30:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxf2e31",
                  "author": "arcanemachined",
                  "text": "Interesting, I always had surprisingly good results with it. I used Ollama and the Q4_0 quant. (Not the best quant, but works well with my older hardware.)",
                  "score": 1,
                  "created_utc": "2026-01-03 10:41:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxg39sv",
                  "author": "Blizado",
                  "text": "Sounds more like a setting, chat format issue. If that happen often something is wrong with the generation parameter and/or chat format that didn't work well with that model. Are you sure you used the Instruct and not the Base model? That can also be a reason for that.",
                  "score": 1,
                  "created_utc": "2026-01-03 14:54:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxfamfb",
              "author": "Geritas",
              "text": "Give this one a go, it is not as lobotomized https://huggingface.co/YanLabs/gemma-3-27b-it-abliterated-normpreserve-v1-GGUF",
              "score": 3,
              "created_utc": "2026-01-03 11:49:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxfrwqn",
                  "author": "arcanemachined",
                  "text": "Will do, thanks.",
                  "score": 2,
                  "created_utc": "2026-01-03 13:50:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxg2us9",
                  "author": "Blizado",
                  "text": "Or maybe then this version as well: https://huggingface.co/mradermacher/gemma-3-27b-it-heretic-v2-GGUF\n\nHeretic sounds for me a very good approach to uncensor models while keep as close as possible to the original model.",
                  "score": 1,
                  "created_utc": "2026-01-03 14:52:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxem9a7",
              "author": "Death_12_35_taken",
              "text": "what quantization do you use? q4, q8 or f16?",
              "score": 1,
              "created_utc": "2026-01-03 08:22:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxemegj",
                  "author": "arcanemachined",
                  "text": "Q4",
                  "score": 1,
                  "created_utc": "2026-01-03 08:23:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxemluk",
                  "author": "arcanemachined",
                  "text": "Better link:\n\nhttps://huggingface.co/bartowski/mlabonne_gemma-3-27b-it-abliterated-GGUF",
                  "score": 1,
                  "created_utc": "2026-01-03 08:25:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxk5fe9",
          "author": "Crisper026",
          "text": "Commenting to follow",
          "score": 1,
          "created_utc": "2026-01-04 03:03:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxklj6x",
          "author": "Guinness",
          "text": "Hijacking to ask: what model is best at coding on a 3090? What about a 3090 and a RTX 4000 ADA (so 44GB but across the PCIE bus ðŸ˜­)\n\nAnd what are you using? Iâ€™ve been using VLLM.",
          "score": 1,
          "created_utc": "2026-01-04 04:40:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgdiac",
          "author": "Equivalent_Bit_461",
          "text": "What happened to your ram?",
          "score": -1,
          "created_utc": "2026-01-03 15:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjxwan",
          "author": "Innomen",
          "text": "IMO Doesn't exist. Can't exist. They are brain dead or censored. There's no in-between. The censored ones can be jail broken, somewhat. But \"uncensored\" from the ground up just doesn't exist. Because the training data is censored as well NYT and wiki are safe for work you know? You'd need a \"criminal\" database. Plus fully uncensored kinda means fully unaligned in a way doesn't it? I just am not feeling like the promise of open source and the hacktivist community is measuring up in this run up to singularity. I don't feel like anyone is even trying.",
          "score": -6,
          "created_utc": "2026-01-04 02:21:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5e010",
      "title": "Supertonic2: Lightning Fast, On-Device, Multilingual TTS",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/k40jciwu5pbg1",
      "author": "ANLGBOY",
      "created_utc": "2026-01-06 09:24:47",
      "score": 191,
      "num_comments": 42,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q5e010/supertonic2_lightning_fast_ondevice_multilingual/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxzw03n",
          "author": "drooolingidiot",
          "text": "Woah, this is incredible! Finally something super lightweight that sounds even better than kokoro!\n\n\nI am disappointed that it's released under the deranged and extremely user-hostile Open-RAIL license though.  Why apply such a hostile license to the model when it doesn't even benefit you in anyway?",
          "score": 27,
          "created_utc": "2026-01-06 12:32:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny02yte",
              "author": "wanderer_4004",
              "text": "Why do you consider the Open-RAIL license hostile?",
              "score": 1,
              "created_utc": "2026-01-06 13:17:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny09y4o",
                  "author": "silenceimpaired",
                  "text": "Itâ€™s more restrictive than Apache or MITâ€¦",
                  "score": 11,
                  "created_utc": "2026-01-06 13:56:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny5c5e9",
              "author": "RedZero76",
              "text": "None of these bother me, personally, they all seem reasonable.  But maybe there are other parts to the licence I missed.  I mainly looked for this section.\n\n    Use Restrictions\n    \n    You agree not to use the Model or Derivatives of the Model:\n    (a) In any way that violates any applicable national, federal, state, local or international law or regulation;\n    (b) For the purpose of exploiting, harming or attempting to exploit or harm minors in any way;\n    (c) To generate or disseminate verifiably false information and/or content with the purpose of harming others;\n    (d) To generate or disseminate personal identifiable information that can be used to harm an individual;\n    (e) To generate or disseminate information and/or content (e.g. images, code, posts, articles), and place the information and/or content in any context (e.g. bot generating tweets)\n    without expressly and intelligibly disclaiming that the information and/or content is machine generated;\n    (f) To defame, disparage or otherwise harass others;\n    (g) To impersonate or attempt to impersonate (e.g. deepfakes) others without their consent;\n    (h) For fully automated decision making that adversely impacts an individualâ€™s legal rights or otherwise creates or modifies a binding, enforceable obligation;\n    (i) For any use intended to or which has the effect of discriminating against or harming individuals or groups based on online or offline social behavior or known or predicted personal or personality characteristics;\n    (j) To exploit any of the vulnerabilities of a specific group of persons based on their age, social, physical or mental characteristics, in order to materially distort the behavior of a person pertaining to that group in a manner that causes or is likely to cause that person or another person physical or psychological harm;\n    (k) For any use intended to or which has the effect of discriminating against individuals or groups based on legally protected characteristics or categories;\n    (l) To provide medical advice and medical results interpretation;\n    (m) To generate or disseminate information for the purpose to be used for administration of justice, law enforcement, immigration or asylum processes, such as predicting an individual will commit fraud/crime commitment (e.g. by text profiling, drawing causal relationships between assertions made in documents, indiscriminate and arbitrarily-targeted use).",
              "score": 0,
              "created_utc": "2026-01-07 05:06:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny5gp91",
                  "author": "drooolingidiot",
                  "text": "There are so many things wrong with this, I don't even know where to begin.\n\n> (a) In any way that violates any applicable national, federal, state, local or international law or regulation;\n\nIf you lived in a crappy country: __insert_country_you_dislike__, why is this model's license telling you that you can't break some __insert_immoral_law_you_disagree_with__? If your religion/ethnicity/freedom is being discriminated against by the law, this license would be accessory to your oppression.\n\n> (l) To provide medical advice and medical results interpretation;\n\nWhy do you care how/why I use model for my own-use cases? I can't afford a doctor visit and I need a model to look at my lab results. Should I just suffer my illness because of an idiotic license agreement clause?\n\nFrom the model's license file:\n> To the maximum extent permitted by law, Licensor reserves the right to restrict (remotely or otherwise) usage of the Model in violation of this License, update the Model through electronic means, or modify the Output of the Model based on updates. You shall undertake reasonable efforts to use the latest version of the Model.\n\nI'm not sure I even need to say anything about this... this is just awful.\n\nThis is r/localllama. If you want restrictions on how you can use models, maybe take a look at some of the providers like anthropic or openai.",
                  "score": 5,
                  "created_utc": "2026-01-07 05:39:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxzfdmk",
          "author": "KoreanPeninsula",
          "text": "The speed is quite fast. However, in some Korean texts, pronunciation becomes inaccurate, and certain parts are not pronounced at all. Short sentences are read quite well.",
          "score": 10,
          "created_utc": "2026-01-06 10:17:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzywkt",
              "author": "Silver_Jaguar_24",
              "text": "Same for English, 2 words were skipped when I tested the demo.",
              "score": 8,
              "created_utc": "2026-01-06 12:51:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny1ox93",
              "author": "kroggens",
              "text": "Does Kokoro has the same problem? Or it speaks all words?",
              "score": 4,
              "created_utc": "2026-01-06 18:00:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7yveb",
                  "author": "Knochenhans",
                  "text": "Been using Kokoro for lots of books and blogs since it came out, it never skips any content and is generally extremely robust, no hallucinations and it only glitches when you really push it hard.\n\nTbh itâ€™s a bit frustrating with all these new hyped-up models. In 99% of cases, the first thing you notice when you try it out is skipped words or tonal inconsistency. Even the most natural sounding model is kinda useless if it canâ€™t be used reliably for more than a few gimmicky show-off sentences. \\[rant fished :D\\]",
                  "score": 4,
                  "created_utc": "2026-01-07 16:11:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny0lmvx",
          "author": "ghulamalchik",
          "text": "Tried the demo. Quality is insane especially at that size. Well done!\nI hope more languages are supported in the future such as Russian, German, Arabic, Italian.",
          "score": 9,
          "created_utc": "2026-01-06 14:59:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny27kjv",
          "author": "OC2608",
          "text": "No finetunable checkpoints = no care. (I'm sorry...)   \nHey Piper, why are you the \\*only\\* one with finetunable checkpoints and fast CPU inference even in 2026?",
          "score": 7,
          "created_utc": "2026-01-06 19:23:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzhthi",
          "author": "FlowCritikal",
          "text": "Will German be added anytime soon? The market for German TTS is fairly large.",
          "score": 13,
          "created_utc": "2026-01-06 10:39:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzo7bw",
              "author": "SignificantAsk4215",
              "text": "Hopefully",
              "score": 2,
              "created_utc": "2026-01-06 11:34:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny1nizs",
              "author": "Fun_Librarian_7699",
              "text": "I read \"multilingual\" and was really disappointed since it doesn't support German. But for English it's a nice model",
              "score": 1,
              "created_utc": "2026-01-06 17:53:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny0wx5l",
          "author": "ThetaCursed",
          "text": "What about voice cloning? Or just presets...",
          "score": 4,
          "created_utc": "2026-01-06 15:52:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1r6ox",
              "author": "DOAMOD",
              "text": "\\+1",
              "score": 2,
              "created_utc": "2026-01-06 18:10:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nydl8hy",
              "author": "silenceimpaired",
              "text": "At the moment with the license and options Kokoro still seems a better option.",
              "score": 1,
              "created_utc": "2026-01-08 10:54:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxze31d",
          "author": "FullstackSensei",
          "text": "That's great! Especially the cpp support!\nAny chance we also get German support?",
          "score": 3,
          "created_utc": "2026-01-06 10:06:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzjhub",
          "author": "neovim-neophyte",
          "text": "how does this compare to cosyvoice3(RL)? ive tried it and its pretty good, far better than spark tts and f5 tts",
          "score": 3,
          "created_utc": "2026-01-06 10:54:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzhhhp",
          "author": "HotDoshirak",
          "text": "Sometimes itâ€™s funny to see how models claim to be multilingual, but actually supports 3-5 languages. But still a good release for a lightweight tts.",
          "score": 13,
          "created_utc": "2026-01-06 10:36:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny19ewj",
              "author": "Dany0",
              "text": "Insert joke about multilingual tts coming only from the multilingual region of france otherwise it's just sparkling tts",
              "score": 4,
              "created_utc": "2026-01-06 16:49:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxzqygw",
              "author": "Slow_Concentrate3831",
              "text": "Well, at the same time, it's â€œmultiâ€ starting from two ðŸ¤·ðŸ»â€â™‚ï¸",
              "score": 9,
              "created_utc": "2026-01-06 11:55:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny54in3",
          "author": "maifee",
          "text": "Can we finetune this?",
          "score": 2,
          "created_utc": "2026-01-07 04:16:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny0e7kn",
          "author": "Impressive-Sir9633",
          "text": "Interested in quick opinions compared to prior smaller models (KokoroTTS and Parakeet 0.6v3",
          "score": 1,
          "created_utc": "2026-01-06 14:20:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny0y31p",
          "author": "urekmazino_0",
          "text": "Fine tuning support?",
          "score": 1,
          "created_utc": "2026-01-06 15:58:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny2vtsf",
          "author": "TraceyRobn",
          "text": "Impressive. Works great on the PC. \n\nFYI: Fails on three Android mobile browsers (Chrome, Brave and Firefox (with WASM)) with the message: \"Error: Cannot read properties of undefined (reading 'subgroupMinSize)",
          "score": 1,
          "created_utc": "2026-01-06 21:15:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyjwtw0",
              "author": "Loud_Economics_9477",
              "text": "You gotta use Chrome Dev version if Android. Sadly, Firefox Nightly still doesn't work.",
              "score": 1,
              "created_utc": "2026-01-09 06:35:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny303i5",
          "author": "wanderer_4004",
          "text": "Pretty cool to have the same voices for different languages - that makes language switching less awkward. Here and there is a small glitch (using Python) but the speed is fantastic and the quality is by far good enough especially for real time applications. French is actually imho better than kokoro - kokoro has only one female french voice which is slightly boring. German, Italian, Chinese, Russian and two dozen more languages would be cool...\n\nEdit: One more cool thing, the model automatically converts Mr to Mister and Wed to Wednesday etc. Very nice, kokoro does not do that. About 40x real time on MBP M1 64GB.",
          "score": 1,
          "created_utc": "2026-01-06 21:34:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny36vgj",
          "author": "az226",
          "text": "I wonder how the RTF is so much faster than Kokoro but model size similar.",
          "score": 1,
          "created_utc": "2026-01-06 22:05:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5ip4y",
          "author": "Independent_Serve175",
          "text": "I find this model way faster than Kokoro TTS, but still the quality is not quite as good. For example try with the text \"Is this working?\" using Alex voice. Even using a 16 steps configuration most of voices shows up the same issue of skipping text or mispronouncing it.",
          "score": 1,
          "created_utc": "2026-01-07 05:54:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny82aga",
              "author": "ahmett9",
              "text": "I found 30 steps to be the sweet spot.",
              "score": 1,
              "created_utc": "2026-01-07 16:26:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nydy6yx",
          "author": "simmessa",
          "text": "This is freaking impressive, from generation times to accuracy to quality of the final output, great job! Do you plan on adding languages such as italian? I'd love to test it w. my native language.",
          "score": 1,
          "created_utc": "2026-01-08 12:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzjudy",
          "author": "sammcj",
          "text": "I like to find a good TTS model that does international / British English rather than American - has anyone got any recommendations?",
          "score": 1,
          "created_utc": "2026-01-06 10:57:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny60bmq",
              "author": "Desperate-Ad7946",
              "text": "Chatterbox Multi Lingual version, i use so many local TTS for my storytelling video and the best is Chatterbox  \nI use for Spanish, Portuguese and Germany for generate audio 40+ minutes",
              "score": 1,
              "created_utc": "2026-01-07 08:24:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny04fzt",
          "author": "DeepGreenPotato",
          "text": "Would be nice to support Russian!",
          "score": 1,
          "created_utc": "2026-01-06 13:26:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzc52m",
          "author": "Baldtazar",
          "text": "Do you know the pain of getting link texts from the post on the smartphone?",
          "score": -2,
          "created_utc": "2026-01-06 09:47:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1986x",
      "title": "IQuestCoder - new 40B dense coding model",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/ilintar/IQuest-Coder-V1-40B-Instruct-GGUF",
      "author": "ilintar",
      "created_utc": "2026-01-01 17:12:51",
      "score": 183,
      "num_comments": 37,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q1986x/iquestcoder_new_40b_dense_coding_model/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nx5qmo1",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-01 23:10:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx47bkc",
          "author": "ilintar",
          "text": "BTW, the Loop version \\*is\\* a new architecture and will require adaptation.",
          "score": 56,
          "created_utc": "2026-01-01 18:25:04",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nx5fuf1",
          "author": "MutantEggroll",
          "text": "Thanks for the GGUF! Taking the IQ4\\_XS for a spin and so far it's performing very well.\n\n* Successfully zero-shotted a Snake game\n* Demonstrated good understanding of embedded Rust concepts\n* Hovering around 55% Pass 2 rate on Aider Polyglot, which puts it on-par with GPT-OSS-120B\n\nMy only issue is that it does not fit all that nicely into 32GB of VRAM. I've only got room for 28k context with unquantized KV cache. Once I finish my Polyglot run I'll try again with Q8 KV cache and see what the degradation looks like.",
          "score": 30,
          "created_utc": "2026-01-01 22:11:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx72bxe",
              "author": "rm-rf-rm",
              "text": "tests that are \"make x from scratch\" or any of the leaderboard benchmarks dont correlate well to real world performance where the majority use case is: within an existing codebase,: understands the codebase, makes a change that works, preserves architecture, preserves design patterns, preserves style.",
              "score": 11,
              "created_utc": "2026-01-02 03:53:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx79qpf",
                  "author": "MutantEggroll",
                  "text": "Agreed. I treat greenfield prompts and benchmarks as a pre-filter - models that do poorly are discarded, and those that do well move forward to real world use cases, where they get filtered again for low performance.\n\nWith the context size limitations on my hardware due to the size of this model, I'm tempering my expectations. Could be good for boilerplate code or small code reviews, but it just won't be able to hold enough of a real codebase in context to be a true workhorse.",
                  "score": 6,
                  "created_utc": "2026-01-02 04:42:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx5lsqc",
              "author": "ilintar",
              "text": "Interesting, those are very good numbers for an IQ4\\_XS on coding tasks.",
              "score": 8,
              "created_utc": "2026-01-01 22:43:15",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx6c0q7",
              "author": "FizzarolliAI",
              "text": "Interesting! I couldn't get it to behave well w/ tool calls at all, but I was trying the looping model in vLLM...",
              "score": 0,
              "created_utc": "2026-01-02 01:11:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4863c",
          "author": "mantafloppy",
          "text": "The model maker don't talk about what arch they used, and this dude quant it in Qwen2, sus all around.\n\nhttps://huggingface.co/cturan/IQuest-Coder-V1-40B-Instruct-GGUF",
          "score": 34,
          "created_utc": "2026-01-01 18:29:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4969k",
              "author": "ilintar",
              "text": "Basic model is basic Llama, loop model is nice new arch with dual (not hybrid) gated attention.",
              "score": 25,
              "created_utc": "2026-01-01 18:34:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx4d3bx",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -17,
              "created_utc": "2026-01-01 18:53:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4e3ap",
                  "author": "mantafloppy",
                  "text": "I'm calling IQuestLab/IQuest-Coder-V1-40B-Instruct sus, not OP.",
                  "score": 12,
                  "created_utc": "2026-01-01 18:58:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx4cizr",
          "author": "LegacyRemaster",
          "text": "Hi Piotr, downloading. Will test with a real c++ problem solved today with Minimax M2.1 . GPT 120, Devstral, GLM 4.7 --> they failed. Vscode + cline",
          "score": 30,
          "created_utc": "2026-01-01 18:50:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4gliw",
              "author": "LegacyRemaster",
              "text": "first feedback: 32.97 tok/sec on blackwell 96gb full context @ 450W.",
              "score": 21,
              "created_utc": "2026-01-01 19:10:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx66d2p",
              "author": "JonatasLaw",
              "text": "â€™m working on a project that involves creating shaders in C++. No current AI can help me even minimally. I put a `groupshared` inside a function (which obviously wonâ€™t work), ask GPT-5.2, Opus 4.5, Gemini 3, GLM 4.7, and Minimax 2.1 where the error is, and all of them fail. How do you work with C++ using AIs and actually get results? Do you use a specific kind of prompt? Because in my case theyâ€™re all 100% useless, they donâ€™t even work for repetitive tasks.",
              "score": 3,
              "created_utc": "2026-01-02 00:37:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8ccha",
                  "author": "LegacyRemaster",
                  "text": "I use Unreal Engine 5.7. All the C++ and backend code has the BPs converted to C++ for better performance. I think this helps. I won't deny that yesterday the 5.2 codex solved a problem for me that minimax didn't solve.",
                  "score": 2,
                  "created_utc": "2026-01-02 10:14:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxgi1ib",
                  "author": "ilintar",
                  "text": "Quick answer: you don't.  \nLong answer: the better AIs (Opus 4.5, Gemini 3) will help for simple tasks. But for complex C++ tasks you have to \\*tell them\\* what the problem is, then they can handle it. Best case, you tell them where to insert debug prints so they can figure something out.",
                  "score": 1,
                  "created_utc": "2026-01-03 16:08:17",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx53f5j",
              "author": "LegacyRemaster",
              "text": "https://preview.redd.it/4ooz7sdzysag1.png?width=864&format=png&auto=webp&s=1aeed92ab3719005a9f560266bf90af13d9694a0\n\nCline uses complex prompts and iterative task execution that may be challenging for less capable models.\n\n  \nTask : Fix errors --> \n\n  \n**Main issues observed:**\n\n1. **Missing type specifier / invalid declarations**\n   * `C4430`: missing type specifier (default-int not supported in C++)\n   * Indicates malformed or incomplete variable/function declarations.\n2. **Syntax errors around console command definitions**\n   * `C2146`: missing `;` before identifiers:\n      * `FakeBackend_ConsoleCommand`\n      * `FakeLogin_ConsoleCommand`\n   * `C2059`: syntax error on `stringa`, `)`, and `;`\n   * `C2143`: missing `)` or `;` before `{` or `}`\n3. **Function header / brace mismatch**\n   * `C2447`: missing function header, obsolete formal type list\n   * Strong indication of mismatched parentheses or braces.\n4. **Redefinition error**\n   * `C2086`: `FAutoConsoleCommandWithWorldDelegate` redefinition\n   * Suggests duplicate declaration caused by earlier syntax failure.\n\n  \nFailed. \n\n  \nNo problem with Minimax M2.1",
              "score": 6,
              "created_utc": "2026-01-01 21:07:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx5n1ax",
                  "author": "ilintar",
                  "text": "Minimax is a beast though, would be surprised if a 40B model, even if dense, would beat it.",
                  "score": 10,
                  "created_utc": "2026-01-01 22:50:02",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5enbm",
          "author": "bobeeeeeeeee8964",
          "text": "I just have a try, and it is clearly not good, it can not handle those task can solved by smaller and way more faster model like Qwen3-Coder-30B-A3B-Instruct or NVIDIA-Nemotron-3-Nano-30B-A3B. Save your time, don't use it.",
          "score": 10,
          "created_utc": "2026-01-01 22:05:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4ej94",
          "author": "Medium_Chemist_4032",
          "text": "Tried out this prompt:\n\n>Need to evaluate if youâ€™re smart. Write some compose file to run llama-swap that can swap to a vllm-ran model. Assume ubuntu host, docker is installed.\n\n[Response](https://pastebin.com/yszDVqch) is interesting. Not the brightest possible choices, but I didn't specify any, so ok.\n\n>**Overview**\n\n>This deployment provides an intelligent model swapping system that routes requests between LLM and vLLM services based on model type, with monitoring, health checks, and automatic failover.\n\n>Architecture\n\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚   Clients   â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”\n                    â”‚  Nginx      â”‚\n                    â”‚  Gateway    â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚            â”‚           â”‚            â”‚\n    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”\n    â”‚ LLM     â”‚ â”‚ vLLM    â”‚ â”‚ Model â”‚ â”‚ Prometheusâ”‚\n    â”‚ Service â”‚ â”‚ Service â”‚ â”‚Managerâ”‚ â”‚          â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n>Features Intelligent Routing: Automatically routes requests to LLM or vLLM based on model type Model Swapping: Hot-swap models without downtime Health Monitoring: Built-in health checks for all services Metrics & Logging: Prometheus + Grafana monitoring Load Balancing: Nginx load balancing with failover SSL/TLS: HTTPS support with auto-generated certificates",
          "score": 5,
          "created_utc": "2026-01-01 19:00:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4pyu0",
          "author": "ChopSticksPlease",
          "text": "Downloaded but didnt yet have time to fully test it against Devstral Small 2 and perhaps Seed OSS.\n\nHow much effort was it to build this model and how/where did you get the training data for coding?",
          "score": 4,
          "created_utc": "2026-01-01 19:57:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4dkdq",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 23,
          "created_utc": "2026-01-01 18:55:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4mgm9",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 22,
              "created_utc": "2026-01-01 19:40:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4x66o",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 3,
                  "created_utc": "2026-01-01 20:34:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4t9pg",
                  "author": "lemon07r",
                  "text": "Thank you. I dont know how anyone buys into these obviously sham models",
                  "score": -3,
                  "created_utc": "2026-01-01 20:14:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx4o0ct",
              "author": "Available_Brain6231",
              "text": "so the small chinese ai companies starting copying openai... sad.",
              "score": -6,
              "created_utc": "2026-01-01 19:47:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4hipb",
          "author": "Cool-Chemical-5629",
          "text": "Model is too big for me to run on my hw, but I'd bet I have couple of prompts it would break its teeth on. It's especially tempting to prove since it claims to be on par with Sonnet 4.5 and much bigger models and my experience says that more often than not such claims are very false lol",
          "score": 4,
          "created_utc": "2026-01-01 19:15:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5ix52",
              "author": "-InformalBanana-",
              "text": "MOE ppl! Give us MOE! :)",
              "score": 1,
              "created_utc": "2026-01-01 22:27:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx4n12b",
              "author": "Inca_PVP",
              "text": "rip. yeah 40b is heavy af.\n\nhonestly for normal hardware just stick toÂ Llama 3 8B. if u grab theÂ Q4\\_K\\_MÂ quant it fits into 8gb vram and runs instant.\n\ni use it daily for python with a specific preset to keep it focused (less yapping). put my config on profile if u want a lightweight setup that actually runs locally.",
              "score": -5,
              "created_utc": "2026-01-01 19:42:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4pxih",
                  "author": "ttkciar",
                  "text": "Heavy is good, if it means improved competence.\n\nLooking forward to giving it a spin.",
                  "score": 2,
                  "created_utc": "2026-01-01 19:57:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx41h7o",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 5,
          "created_utc": "2026-01-01 17:56:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx42cdn",
              "author": "b3081a",
              "text": "As a coder model it's probably not focusing on general benches.",
              "score": 9,
              "created_utc": "2026-01-01 18:00:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx55r9t",
          "author": "FizzarolliAI",
          "text": "To go against what everyone else is saying, I actually think this model is really good!... At everything *but* programming. It sucks at programming. General insight tasks, writing, assistant-y stuff, etc. are great! Somehow!",
          "score": 6,
          "created_utc": "2026-01-01 21:19:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx619zn",
              "author": "IrisColt",
              "text": "Thanks for the insight!",
              "score": 1,
              "created_utc": "2026-01-02 00:09:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4k9tj",
          "author": "jinnyjuice",
          "text": "I'm assuming they're going to release the loop thinking model tomorrow, right?",
          "score": 1,
          "created_utc": "2026-01-01 19:29:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2pons",
      "title": "GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/0xSero/GLM-4.7-REAP-50-W4A16",
      "author": "Maxious",
      "created_utc": "2026-01-03 08:43:56",
      "score": 180,
      "num_comments": 72,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nxf9e9h",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-03 11:40:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxflp5a",
          "author": "Velocita84",
          "text": "Ok, but REAP'd for what? It's my understanding that REAP prunes experts based on how often they're activated during inference of a calibration set, so what task(s) was it calibrated for?",
          "score": 24,
          "created_utc": "2026-01-03 13:12:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfron7",
              "author": "Kamal965",
              "text": "The W4A16 calibration dataset used was [The Pile-10k](https://huggingface.co/datasets/NeelNanda/pile-10k) and the REAP calibration dataset was listed as [\"glm47-reap-calibration-v2\"](https://huggingface.co/datasets/0xSero/glm47-reap-calibration-v2) which is a dataset on the same author's HF page. Idk what's actually in the dataset because there's no description and I haven't read through it.",
              "score": 12,
              "created_utc": "2026-01-03 13:48:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgd7kz",
                  "author": "Murgatroyd314",
                  "text": "A quick glance at a few bits of the calibration data set finds a lot of programming, several logic/math puzzles, and a bit of trivia.",
                  "score": 10,
                  "created_utc": "2026-01-03 15:44:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfgh6f",
          "author": "Phaelon74",
          "text": "Again, people quanting AWQs (W4A16) need to provide details on what they did to make sure all experts were activated during calibration.  Until OP comes out and provides that, if you see this model act poorly, it's because the calibration data did not activate all experts and it's been partially-lobotomized.",
          "score": 39,
          "created_utc": "2026-01-03 12:35:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxh8wp2",
              "author": "One-Macaron6752",
              "text": "At minimum, a good disclosure normally includes:\n - Calibration dataset description\n - Number of tokens / sequences\n - Observed expert routing frequencies\n - Whether forced routing was used\n - Whether rare experts were targeted\nâ€¦ this is / should becoming best practice in papers & repos! ;)",
              "score": 12,
              "created_utc": "2026-01-03 18:12:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxfnxyf",
              "author": "Position_Emergency",
              "text": "u/Maxious The quant\\_config looks like it defaulted to \"pile-10k\" for the AutoRound pass?\n\nSince you already did the hard work creating \"glm47-reap-calibration-v2\" to select the best experts, wouldn't it be better to reuse that dataset for quantization?\n\nPile-10k probably won't trigger those specific code/agent experts you preserved, leaving them uncalibrated (Silent Expert problem).   \nIt should be a 1-line swap in the AutoRound script to fix.",
              "score": 10,
              "created_utc": "2026-01-03 13:26:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxfrg0d",
                  "author": "Kamal965",
                  "text": "That's actually a great question! I'm curious to know about that too. As far as I can tell, using the same calibration dataset for both pruning and quantization logically makes sense... am I missing something that makes it not a good idea?",
                  "score": 3,
                  "created_utc": "2026-01-03 13:47:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxfr6qo",
              "author": "Kamal965",
              "text": "I mean, I agree in general that it's very frustrating to see AWQ quants that don't say what dataset, or domain, they used for calibration. But in this case, it is explicitly mentioned on the repo. The [README.md](http://README.md) shows the full steps on how to recreate that quant. The W4A16 calibration dataset used was [The Pile-10k](https://huggingface.co/datasets/NeelNanda/pile-10k) and the REAP calibration dataset (and I think this is the more important one to know) was listed as [\"glm47-reap-calibration-v2\"](https://huggingface.co/datasets/0xSero/glm47-reap-calibration-v2) which is a dataset on the same author's HF page. He has 4 different REAP calibration datasets there, interestingly enough... but there are no actual descriptions of what the datasets contain. You'd have to look through each one to see, welp.",
              "score": 3,
              "created_utc": "2026-01-03 13:45:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxfrjnd",
                  "author": "Phaelon74",
                  "text": "Right, but by default, GLM does not have a modeling file in say, LLM\\_Compressor.  So if he first made the quant in llm\\_compressor and then reaped it, experts would be missing based on not being activated by his dataset, etc.  That's more what I am alluding to.  People doing AWQs need to explicitly say \"And I did X, Y, Z, to make sure all experts were activated during dataset calibration.\"",
                  "score": 2,
                  "created_utc": "2026-01-03 13:47:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxev2ro",
          "author": "Position_Emergency",
          "text": "Can see on the Huggingface page you're in the process of doing benchmarks ðŸ’¯  \nWill be interested to see the results!\n\nHave you considered doing a similar size version of MiniMax M2.1? (and therefore a less aggressive REAP as it is a 220B model)",
          "score": 16,
          "created_utc": "2026-01-03 09:39:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxh1kr9",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-01-03 17:40:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxhjaoq",
                  "author": "colin_colout",
                  "text": "Minimax models are ~130gb at 4bits.  If that can get under 90gb, it can fit in 128gb unified memory systems like my strix halo (though not sure if the format is even supported... yay rocm)",
                  "score": 1,
                  "created_utc": "2026-01-03 18:59:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxf0rd2",
              "author": "dtdisapointingresult",
              "text": "He should've done diverse benchmarks before uploading lobotomyslop if you ask me.",
              "score": -6,
              "created_utc": "2026-01-03 10:27:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxf40vx",
                  "author": "Position_Emergency",
                  "text": "In the land of the blind the one eyed man is king.",
                  "score": 12,
                  "created_utc": "2026-01-03 10:55:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxf38i6",
          "author": "a_beautiful_rhind",
          "text": "You can run 2.0bpw exl3 GLM and it's around 90gb. Comparison here would be interesting.\n\nWhen I tried previous 4.6 REAP, about 3 of them, the EXL was better subjectively.\n\n>Calibrated on code/agentic tasks; may have reduced performance on other domains\n\nAll those other reap forgot how to talk outside such domains. It's interesting how nobody has deviated from the codeslop datasets cerebras used. My theory is a more rounded english only dataset would preserve much more performance. Then someone could do chinese only, etc.",
          "score": 9,
          "created_utc": "2026-01-03 10:48:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgg9xf",
              "author": "projectmus3",
              "text": "Youâ€™re the person who does roleplay with LLMs and talk to fictional characters right? Yeah maybe you should create a calibration dataset for roleplay and use that to REAP instead. \n\nThe REAP models from Cerebras focus on coding, tool calling and agentic workloads, and theyâ€™ve been doing amazing for me.",
              "score": 8,
              "created_utc": "2026-01-03 15:59:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgi279",
                  "author": "a_beautiful_rhind",
                  "text": "Really only thing stopping me is the massive download.\n\nI've heard mixed results from people coding with it tho and if you do a perplexity test, usually it's double digit. \n\nThe REAPS I tried would forget who presidents were and other basic facts. Left me a bit skeptical to invest big effort into it.",
                  "score": 3,
                  "created_utc": "2026-01-03 16:08:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxhc37u",
              "author": "One-Macaron6752",
              "text": "I can second your opinion. I have also tried 2.65bpw exl3 quants and felt worlds better than the REAP. For me, the REAP version was: 1) full of hallucinations in places Iâ€™d never expected them 2) full of Chinese & Arabic characters dropping almost everywhereâ€¦",
              "score": 2,
              "created_utc": "2026-01-03 18:27:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxn531v",
                  "author": "Sero_x",
                  "text": "These sound like inference layer errors to me.",
                  "score": 1,
                  "created_utc": "2026-01-04 15:56:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxew6k4",
          "author": "Dany0",
          "text": "Barely doesn't fit on 64gb ram + 32gb vram :( Q3\\_KS managed to load once but OOM'd immediately during prompt processing",
          "score": 7,
          "created_utc": "2026-01-03 09:48:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfcpo2",
              "author": "ApartmentEither4838",
              "text": "Can this work on a A100 80GB?",
              "score": 1,
              "created_utc": "2026-01-03 12:06:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxeu3rl",
          "author": "jacek2023",
          "text": "I need Q3, anyone working on GGUF?",
          "score": 5,
          "created_utc": "2026-01-03 09:31:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfgesv",
              "author": "noctrex",
              "text": "Let's try I guess",
              "score": 3,
              "created_utc": "2026-01-03 12:35:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxfol1n",
              "author": "Kamal965",
              "text": "He just finished uploading some of them: [https://huggingface.co/0xSero/GLM-4.7-REAP-50-GGUF](https://huggingface.co/0xSero/GLM-4.7-REAP-50-GGUF)\n\nI believe he's still uploading more.",
              "score": 1,
              "created_utc": "2026-01-03 13:30:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxhswif",
                  "author": "fallingdowndizzyvr",
                  "text": "\"404\n\nSorry, we can't find the page you are looking for.\"",
                  "score": 4,
                  "created_utc": "2026-01-03 19:44:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxfomou",
                  "author": "jacek2023",
                  "text": "~~thank you!!!~~\n\nbut wait, why it's 25GB only?",
                  "score": 0,
                  "created_utc": "2026-01-03 13:30:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfedtg",
          "author": "Revolutionary-Tip821",
          "text": "using  \nvllm serve /home/xxxx/Docker/xxx/GLM-4.7-REAP-40-W4A16 \\\\\n\n\\--served-model-name local/GLM-4.7-REAP-local \\\\\n\n\\--host [0.0.0.0](http://0.0.0.0) \\--port 8888 \\\\\n\n\\--tensor-parallel-size 2 --pipeline-parallel-size 3 \\\\\n\n\\--quantization auto-round \\\\\n\n\\--max-model-len 14000 \\\\\n\n\\--gpu-memory-utilization 0.96 \\\\\n\n\\--block-size 32 \\\\\n\n\\--max-num-seqs 8 \\\\\n\n\\--max-num-batched-tokens 8192 \\\\\n\n\\--enable-expert-parallel \\\\\n\n\\--enable-prefix-caching \\\\\n\n\\--enable-chunked-prefill \\\\\n\n\\--disable-custom-all-reduce \\\\\n\n\\--disable-log-requests \\\\\n\n\\--tool-call-parser glm47 \\\\\n\n\\--reasoning-parser glm45 \\\\\n\n\\--enable-auto-tool-choice \\\\\n\n\\--trust-remote-code\n\non 6 RTX 4090 it start generating and then fall by repeating same word endlessly, also thinking are not wrapped in think tags, is there anyone have same experience?",
          "score": 2,
          "created_utc": "2026-01-03 12:19:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfg7m3",
              "author": "Phaelon74",
              "text": "Why do pipelines, just 6 TP and rock and roll.  Additionally reasoning parser I have seen what you are seeing.  I don't use it and only use expert-parallel.",
              "score": 2,
              "created_utc": "2026-01-03 12:33:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxftj0m",
                  "author": "Hisma",
                  "text": "you can't do TP on 6 GPUs.  It needs to be powers of 2.  2/4/8 GPUs is typically what's used for TP.",
                  "score": 1,
                  "created_utc": "2026-01-03 13:59:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxhbwle",
              "author": "Sero_x",
              "text": "The repeating is a pipeline but that happens with this model",
              "score": 1,
              "created_utc": "2026-01-03 18:26:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxll3nm",
                  "author": "Revolutionary-Tip821",
                  "text": "i tried also with --tensor-parallel-size 4; but still it stuck repeating same word, so this model is not usable in this state\n\ni don't understand the hype if it can't be used for simple conversation",
                  "score": 2,
                  "created_utc": "2026-01-04 09:31:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxgjyof",
          "author": "Revolutionalredstone",
          "text": "Next please do nanbeige, this this is a beast but needs prune + int4!\n\nhttps://old.reddit.com/r/LocalLLaMA/comments/1q2p2wa/nanbeige4_is_an_incredible_model_for_running/",
          "score": 2,
          "created_utc": "2026-01-03 16:17:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhpdgl",
              "author": "thejoyofcraig",
              "text": "Nanbeige is a 3b model. What are you hoping to prune it down to??",
              "score": 3,
              "created_utc": "2026-01-03 19:27:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxhr9k0",
                  "author": "Revolutionalredstone",
                  "text": "TBH I'd take a 500m and 250m params with very big excitement!\n\nThe other models pruned to this size: like Gemma and granite were absolute bangers!\n\nAnd this one has a lot more junk in the trunk per se.\n\nUltra nano models can be VERY useful if they can barely speak ;D",
                  "score": 1,
                  "created_utc": "2026-01-03 19:36:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxhprn2",
              "author": "LocoMod",
              "text": "It's a 3B model that fits on a lemon. What's the point?",
              "score": 3,
              "created_utc": "2026-01-03 19:29:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxhqwlc",
                  "author": "Revolutionalredstone",
                  "text": "You'd be surprised! I've got plenty of portable devices with 2GB vram and the diff between 3B partial and 2B fully offloaded is HUGE.\n\nNot so much about being ABLE to run, but being able to run FAST!",
                  "score": 2,
                  "created_utc": "2026-01-03 19:34:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxhvut0",
                  "author": "SlowFail2433",
                  "text": "Edge AI is a thing, often very small chips",
                  "score": 2,
                  "created_utc": "2026-01-03 19:58:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxlmdap",
          "author": "LegacyRemaster",
          "text": "https://preview.redd.it/nse8fr8mzabg1.png?width=2013&format=png&auto=webp&s=4d86c31bb4db3967d06dc05a7bf3a589395fc70b\n\nSuper quick test.  glm-4.7-reap-40p IQ3\\_S - 94.57 gb. Fit on 96gb with 4k context. Will test more.",
          "score": 2,
          "created_utc": "2026-01-04 09:42:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxep7dp",
          "author": "fungnoth",
          "text": "I'm curious about the low VRAM + OK system RAM situation with moe offloading",
          "score": 5,
          "created_utc": "2026-01-03 08:47:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxetg03",
              "author": "jhnnassky",
              "text": "Do you have already good ones?",
              "score": 0,
              "created_utc": "2026-01-03 09:25:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxytwwa",
                  "author": "fungnoth",
                  "text": "I sometimes use the GLM Air REAP. 10 layers in GPU and 38 layers MOE CPU.\nUsable, 12GB VRAM 64GB RAM",
                  "score": 2,
                  "created_utc": "2026-01-06 06:56:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxffp1z",
          "author": "LegacyRemaster",
          "text": "fit on 6000 96g ... let me try",
          "score": 1,
          "created_utc": "2026-01-03 12:29:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxg9f1m",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-03 15:26:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxg9ke4",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 7 days on [**2026-01-10 15:26:14 UTC**](http://www.wolframalpha.com/input/?i=2026-01-10%2015:26:14%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/nxg9f1m/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1q2pons%2Fglm47reap50w4a16_50_expertpruned_int4_quantized%2Fnxg9f1m%2F%5D%0A%0ARemindMe%21%202026-01-10%2015%3A26%3A14%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q2pons)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-03 15:26:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxg9nbb",
          "author": "Enottin",
          "text": "RemindMe! 7 days",
          "score": 1,
          "created_utc": "2026-01-03 15:27:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxg9zkl",
              "author": "Enottin",
              "text": "RemindMe! 1 day",
              "score": 1,
              "created_utc": "2026-01-03 15:29:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhg7oy",
          "author": "sampdoria_supporter",
          "text": "I am completely ignorant of this model and REAP as a method but I'm hoping to hell this means running it on strix halo is possible",
          "score": 1,
          "created_utc": "2026-01-03 18:45:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxi7et6",
              "author": "fallingdowndizzyvr",
              "text": "You can run 4.7 on Strix Halo without this.",
              "score": 0,
              "created_utc": "2026-01-03 20:56:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxiby6x",
                  "author": "GreatAlmonds",
                  "text": "How? Unless you're running 1bit quants",
                  "score": 5,
                  "created_utc": "2026-01-03 21:18:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxk7sj9",
          "author": "Goghor",
          "text": "!remindme 7 days",
          "score": 1,
          "created_utc": "2026-01-04 03:16:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxmc2bf",
          "author": "Guilty_Nothing_2858",
          "text": "I want to know how is the performance? Faster but poor satisfaction rate? I saw lot of comment from china dev community, say GLM4.7 cloud is in quantised version. The answer is not good",
          "score": 1,
          "created_utc": "2026-01-04 13:15:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxepedz",
          "author": "DesignerTruth9054",
          "text": "Cool. Excited to try outÂ ",
          "score": 1,
          "created_utc": "2026-01-03 08:49:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxerlf5",
          "author": "Steus_au",
          "text": "whatâ€™s the best way to test/compare it to full size one?",
          "score": 1,
          "created_utc": "2026-01-03 09:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxf3te7",
          "author": "Odd-Ordinary-5922",
          "text": "can someone try pruning gpt oss 120b? Ik there is already one but I think he messed up something. Much appreciated",
          "score": 0,
          "created_utc": "2026-01-03 10:53:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1lgb7",
      "title": "TIL you can allocate 128 GB of unified memory to normal AMD iGPUs on Linux via GTT",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q1lgb7/til_you_can_allocate_128_gb_of_unified_memory_to/",
      "author": "1ncehost",
      "created_utc": "2026-01-02 01:37:11",
      "score": 177,
      "num_comments": 30,
      "upvote_ratio": 0.96,
      "text": "So I am training a 1B model right now on my 7900 XTX with some custom kernels I wrote, and while it is training I wanted to optimize the kernels at the same time. However, my VRAM is nearly maxed doing training, so its not ideal.\n\nThen I realized maybe my 2 CU Raphael iGPU might be able to help since I only need to run some limited samples and the speed isn't as important for optimization as it is for training. After doing some research, it turned out that not only does ROCm recognize the iGPU, but a Linux feature called Graphics Translation Table (GTT) for AMD iGPUs can use up to 128 GB of system memory as VRAM. It even allocates it dynamically, so it isn't removed from your CPU's memory pool until it is allocated. I think a lot of people running Strix Halo are probably using the bios setting, but if you are running Linux you should check to see if GTT works for you since its dynamically allocated.\n\nThis isn't very useful for most people:\n\n1) It isn't going to be good for inference because iGPUs are very very slow, and usually the CPU itself is faster for inference.\n\n2) I'm accessing ROCm directly via C++ / HIP kernels, so I can avoid all the support issues ROCm has for iGPUs in the python stack\n\nHowever, for development it is actually pretty awesome. I allocated 24 GB of GTT so now the iGPU can load a full training run that my main GPU can run so I can profile it. Meanwhile my main GPU is doing long term loss convergence tests in parallel. Since RDNA iGPUs have been around for a while now, this enables big memory AMD GPU kernel development for cheap.\n\nAlso it might be interesting for developing hybrid CPU/GPU architectures. The MI300A does exist which has unified HBM tied to a CPU and giant iGPU. A standard ryzen laptop could kind of sort of simulate it for cheap. Stuff like vector indexing on the CPU into big GEMMs on the GPU could be done without PCIE overhead.\n\nI thought it was cool enough to post. Probably a \"Cool story bro\" moment for most of you though haha.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q1lgb7/til_you_can_allocate_128_gb_of_unified_memory_to/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nx7en6z",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-02 05:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7akn8",
          "author": "jstormes",
          "text": "I am doing this with an older Ryzen 7 5600G for background LLM tasks.  Using the iGPU leaves the CPU free to do other batch processes. \n\nBecause I am not using interactivity it is a good use case.  \n\nI have 64 GB 3600 MT/s memory with about 42 of it running a single LLM with it's cache.\n\nIt also keeps my more modern machines free for interactive stuff.",
          "score": 24,
          "created_utc": "2026-01-02 04:47:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7otzh",
              "author": "PentagonUnpadded",
              "text": "A Ryzen 5 5600G has 7 CUs. A 7700X like OP mentions has 2. Cool way to breath life into a 'budget' chip like like the 'G'.",
              "score": 9,
              "created_utc": "2026-01-02 06:35:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx808n2",
          "author": "laughingfingers",
          "text": "on Strix Halo I definitely use this for inference and it's a lot faster than CPU. In BIOS I set graphics memory to the minimum 512MB, with this gtt setting I allocate almost all the rest (few GB for the OS to run seems wise).",
          "score": 16,
          "created_utc": "2026-01-02 08:19:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8845x",
              "author": "Daniel_H212",
              "text": "Yeah this is how I was recommended to do it on strix halo too. There was some talk about how allocating more than 108 GiB with GTT caused instability so I limited myself to that (and don't expect needing that much anyway).",
              "score": 2,
              "created_utc": "2026-01-02 09:34:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8ow0s",
                  "author": "GrayBayPlay",
                  "text": "I can load models up to 120 gb, with a 4k context on my halo strix without any instabillity issues. Then again i only use it for inference. your milage may vary :P",
                  "score": 3,
                  "created_utc": "2026-01-02 12:05:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx875t5",
          "author": "FastDecode1",
          "text": "FYI, according to the [driver docs](https://www.kernel.org/doc/html/v4.19/gpu/amdgpu.html):\n\n>gttsize (int)\n\n>Restrict the size of GTT domain in MiB for testing. The default is -1 (Itâ€™s VRAM size if 3GB < VRAM < 3/4 RAM, otherwise 3/4 RAM size).\n\nSo as long as you have more than 4GB of RAM, the driver automatically allows up to 3/4 of the RAM to be allocated to the iGPU.\n\nI've run stuff on a Vega 8 iGPU on a laptop using llama.cpp and it does work. However, it's not a great experience if you want to watch videos (or do basically anything else GUI-wise) at the same time, since llama.cpp hogs all the memory bandwidth and causes everything else to stutter. GPU scheduling is pretty much non-existent on Linux AFAIK, so there's not really a great way to mitigate this atm.\n\nAlso a hint for fellow ThinkPad users: even though the spec sheet says only a certain amount of RAM is supported, you should probably be able to add more without issues. My current E595's specs say only up to 32GB is supported, but I added a 32GB stick alongside the existing 8GB for a total of 40GB and it works.",
          "score": 6,
          "created_utc": "2026-01-02 09:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6wxt5",
          "author": "master__cheef",
          "text": "This guy LLMs",
          "score": 10,
          "created_utc": "2026-01-02 03:19:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx83xq9",
          "author": "noiserr",
          "text": "Yup. This is what we do with Strix Halo.",
          "score": 4,
          "created_utc": "2026-01-02 08:54:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7v3ol",
          "author": "melenitas",
          "text": "Great, I need to test this with my Ryzen 8845hs, I thought I was limited to 16gb from the total 32gb....Â ",
          "score": 3,
          "created_utc": "2026-01-02 07:31:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6iu1i",
          "author": "cosimoiaia",
          "text": "With llama.cpp you can actually do with Nvidia GPUs as well and if you use it only for kv cache the speed doesn't drastically drop. It's a pretty cool trick.\n\nI used to do that too with my iGPU as well but, maybe because it's a pretty slow one, I never noticed any difference between that and using cpu only, both in training and inference.\n\nI even did some training on cpu only and with a stock heatsink/fan. \"Fun\" to see it hitting 106Â° Celsius.",
          "score": 7,
          "created_utc": "2026-01-02 01:52:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6jka1",
              "author": "-InformalBanana-",
              "text": "How would I set it up to use it only for kv cache, can you give me a llama-server example command?  \nThanks.",
              "score": 6,
              "created_utc": "2026-01-02 01:57:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx6kxfl",
                  "author": "cosimoiaia",
                  "text": "It's not specific to the kv cache. \nSet the env variable GGML_CUDA_ENABLE_UNIFIED_MEMORY=1 and if the model fits in VRAM, the ram will be used only for the kv cache.",
                  "score": 3,
                  "created_utc": "2026-01-02 02:05:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxco2fa",
              "author": "-InformalBanana-",
              "text": "I tried loading a model with your method.\nMy speed on 20k context filled droped from 15tg/s to 2.7tg/s. I would call that significant. And that is with only 2GB shared memory used. So either llama.cpp doesnt consistently allocate kv after other parts thus model ends up in shared memory instead of kv or you simply need much of kv for generating output when context is close to filled.\npp/s also droped from 250 to 76 t/s.\n\n\nSo I think you aren't right about this and you made a mistake somewhere.",
              "score": 0,
              "created_utc": "2026-01-03 00:28:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxcs6s1",
                  "author": "cosimoiaia",
                  "text": "Dude, it's a llama.cpp feature. RTFM. It's not even clear what/where/how you are trying to do, probably to you as well.",
                  "score": -1,
                  "created_utc": "2026-01-03 00:51:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7m9st",
          "author": "alppawack",
          "text": "Whatâ€™s the training speed?",
          "score": 2,
          "created_utc": "2026-01-02 06:14:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9tfni",
              "author": "1ncehost",
              "text": "A blazing 1.8 tok/s ðŸ˜‚ -- the program is very unoptimized currently and it should probably be about 50-200 tok/s for this model.",
              "score": 3,
              "created_utc": "2026-01-02 16:05:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx96wgm",
          "author": "uti24",
          "text": ">but a Linux feature called Graphics Translation Table (GTT) for AMD iGPUs can use up to 128 GB of system memory as VRAM\n\nIs there a fundamental reason it could not be implemented in windows? Or is it just not implemented? Could it be implemented not on the system level but on the app level?",
          "score": 1,
          "created_utc": "2026-01-02 14:07:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx97wzl",
              "author": "1ncehost",
              "text": "I'm not sure to be honest, but the linux driver is open source and the windows driver isn't, so it generally gets features faster and random hacker dudes like me expose low level features / fix stuff fast.",
              "score": 4,
              "created_utc": "2026-01-02 14:13:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzhcqu",
      "title": "Any guesses?",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/xqvj95zv8cag1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-30 12:52:15",
      "score": 174,
      "num_comments": 36,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwqfxna",
          "author": "Aggressive-Dingo-993",
          "text": "https://preview.redd.it/5pyw2vu8pcag1.jpeg?width=368&format=pjpg&auto=webp&s=6cb05d136e810fbb1e761628ea82933c2ec7c842\n\nQwen image 2512",
          "score": 53,
          "created_utc": "2025-12-30 14:23:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqnq2f",
              "author": "GenLabsAI",
              "text": "I think thats already out",
              "score": 6,
              "created_utc": "2025-12-30 15:05:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws7ckl",
                  "author": "infearia",
                  "text": "Qwen-Image-**Edit-2511** is out. Qwen-Image-2512 is not.",
                  "score": 17,
                  "created_utc": "2025-12-30 19:27:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwr8fkr",
                  "author": "Sorry_Warthog_4910",
                  "text": "Itâ€™s not",
                  "score": 4,
                  "created_utc": "2025-12-30 16:44:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwq621n",
          "author": "swagonflyyyy",
          "text": "Qwen3vl-next-80b-a3b - Now with no more comparison slop.\n\n\nIts not a comparison, its a victory.",
          "score": 78,
          "created_utc": "2025-12-30 13:26:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrz0lp",
              "author": "Own-Potential-2308",
              "text": "It's not X, it's Y.\n\nCan't unsee it",
              "score": 19,
              "created_utc": "2025-12-30 18:47:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqde0o",
          "author": "HedgehogActive7155",
          "text": "Qwen 6, to beat GPT 5.2 on the only benchmark that matter\n\nhttps://preview.redd.it/2fix1edsncag1.png?width=1080&format=png&auto=webp&s=d24535f9d904fe79639b83dbdf9c35e0d67a930c",
          "score": 100,
          "created_utc": "2025-12-30 14:09:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrs0d9",
              "author": "MoffKalast",
              "text": "Finally a benchmark you can trust.",
              "score": 11,
              "created_utc": "2025-12-30 18:15:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwr91cx",
              "author": "Utoko",
              "text": "That would be huge if they could double the number!",
              "score": 17,
              "created_utc": "2025-12-30 16:47:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwrdz2g",
                  "author": "-dysangel-",
                  "text": "it would be almost twice as huge!",
                  "score": 5,
                  "created_utc": "2025-12-30 17:10:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwvzn1j",
              "author": "Niwa-kun",
              "text": "lmao. cool graph. names, colors, and number with literally ZERO information for what any of it means. Cool story. I call bs on this \"benchmark\".",
              "score": 1,
              "created_utc": "2025-12-31 09:24:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwws5qp",
                  "author": "t_krett",
                  "text": "I ran the numbers myself and they check out!",
                  "score": 3,
                  "created_utc": "2025-12-31 13:25:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwxbsyv",
                  "author": "Tall-Ad-7742",
                  "text": "I tried it myself and itâ€™s crazy how accurate this benchmark is and btw itâ€™s called VAG-Benchmark",
                  "score": 2,
                  "created_utc": "2025-12-31 15:17:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrizyl",
              "author": "Cool-Chemical-5629",
              "text": "Where is Grok 4.1? ðŸ˜­ðŸ’”",
              "score": 0,
              "created_utc": "2025-12-30 17:34:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtwmyj",
                  "author": "erraticnods",
                  "text": "grokking they weights",
                  "score": 2,
                  "created_utc": "2025-12-31 00:34:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwq169h",
          "author": "MaxKruse96",
          "text": "Iteration on qwen-image i recon. No LLM, no Qwen3.5 etc.",
          "score": 19,
          "created_utc": "2025-12-30 12:54:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwr0un0",
              "author": "ontorealist",
              "text": "Itâ€™s also been awhile since Z-Image Edit was announcedâ€¦",
              "score": 8,
              "created_utc": "2025-12-30 16:09:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwr7rfl",
                  "author": "j_osb",
                  "text": "I was going to say. I expect z-image-onni-base or z-image-edit considering tongyi dropped the post.",
                  "score": 5,
                  "created_utc": "2025-12-30 16:41:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwrbztf",
          "author": "ForsookComparison",
          "text": "Qwen3.5-235B-A10B\n\nPlz",
          "score": 13,
          "created_utc": "2025-12-30 17:01:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrac0x",
          "author": "a_beautiful_rhind",
          "text": "z-image prompt enhancer enhancer\n\noh you thought you were getting the base, huh?",
          "score": 7,
          "created_utc": "2025-12-30 16:53:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrk1hs",
          "author": "Cool-Chemical-5629",
          "text": "The ascii art character must be a hint. Probably some image related model.",
          "score": 6,
          "created_utc": "2025-12-30 17:39:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsdwjv",
          "author": "Steuern_Runter",
          "text": "Maybe the other z-image models that had been announced but not published yet:\n\nZ-Image-Omni-Base\n\nZ-Image-Edit\n\nZ-Image",
          "score": 5,
          "created_utc": "2025-12-30 19:58:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqy32v",
          "author": "GabryIta",
          "text": "Wan2.5 open source?",
          "score": 3,
          "created_utc": "2025-12-30 15:56:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nws24bl",
          "author": "Brave-Hold-9389",
          "text": "Qwen image 2",
          "score": 3,
          "created_utc": "2025-12-30 19:02:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqaykk",
          "author": "Evening_Ad6637",
          "text": "Something trained on ASCII Art Ã  la Opus?",
          "score": 6,
          "created_utc": "2025-12-30 13:55:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqbaed",
          "author": "jacek2023",
          "text": "unfortunately radio silence from Junyang Lin",
          "score": 2,
          "created_utc": "2025-12-30 13:56:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrdup0",
          "author": "-dysangel-",
          "text": "https://preview.redd.it/47u8zpa0jdag1.png?width=350&format=png&auto=webp&s=3e52fc4e3d3bdc5753d50c36eae39d64004e0c0a\n\nthat's got to be the best emoji I've ever seen",
          "score": 2,
          "created_utc": "2025-12-30 17:10:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq8wen",
          "author": "Sensitive_Sweet_1850",
          "text": "Qwen4VL!!",
          "score": 2,
          "created_utc": "2025-12-30 13:43:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqccv4",
          "author": "Few_Painter_5588",
          "text": "Qwen3-Max open source?ðŸ‘€",
          "score": 4,
          "created_utc": "2025-12-30 14:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq3fv5",
          "author": "scraper01",
          "text": "Omni because the little figure has a hero stance",
          "score": 1,
          "created_utc": "2025-12-30 13:09:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqi8xe",
          "author": "Salt-Willingness-513",
          "text": "qwen image. i ask q wen z-image base",
          "score": 1,
          "created_utc": "2025-12-30 14:36:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqrzo3",
          "author": "this-just_in",
          "text": "Something zimage",
          "score": 1,
          "created_utc": "2025-12-30 15:27:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqwt65",
          "author": "Gold_Scholar1111",
          "text": "Qwen5 serie",
          "score": 1,
          "created_utc": "2025-12-30 15:50:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0vom4",
      "title": "IQuestLab/IQuest-Coder-V1 â€” 40B parameter coding LLM â€” Achieves leading results on SWE-Bench Verified (81.4%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%)",
      "subreddit": "LocalLLaMA",
      "url": "https://github.com/IQuestLab/IQuest-Coder-V1",
      "author": "TellMeAboutGoodManga",
      "created_utc": "2026-01-01 04:29:26",
      "score": 173,
      "num_comments": 47,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0vom4/iquestlabiquestcoderv1_40b_parameter_coding_llm/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "nx1p0vs",
          "author": "gzzhongqi",
          "text": "I looked up their background info and they are back by a chinese quant trading company, similar to deepseek. Interesting that all these quant trading companies are stepping into llm training.",
          "score": 60,
          "created_utc": "2026-01-01 07:05:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx31x3p",
              "author": "Karyo_Ten",
              "text": "Faster churning of quant code, millions won on the stock market.",
              "score": 1,
              "created_utc": "2026-01-01 14:41:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx17enb",
          "author": "TellMeAboutGoodManga",
          "text": "https://preview.redd.it/xzspqeti1oag1.png?width=3022&format=png&auto=webp&s=1887580e4045d206ec282b1a12549b46507ee0b1",
          "score": 32,
          "created_utc": "2026-01-01 04:31:55",
          "is_submitter": true,
          "replies": [
            {
              "id": "nx1a4dk",
              "author": "Recoil42",
              "text": "Great technical report here: [https://github.com/IQuestLab/IQuest-Coder-V1/blob/main/papers/IQuest\\_Coder\\_Technical\\_Report.pdf](https://github.com/IQuestLab/IQuest-Coder-V1/blob/main/papers/IQuest_Coder_Technical_Report.pdf)",
              "score": 17,
              "created_utc": "2026-01-01 04:52:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx18606",
          "author": "ocirs",
          "text": "Really great results for a 40B param model, is it safe the assume the benchmarks are based on the IQuest-Coder-V1-40B-Loop-Thinking model?",
          "score": 18,
          "created_utc": "2026-01-01 04:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx18u5x",
              "author": "TellMeAboutGoodManga",
              "text": "The score of LiveCodeBench v6 is from IQuest-Coder-V1-40B-Loop-Thinking model, and the rest are IQuest-Coder-V1-40B-Loop-Instruct model.",
              "score": 19,
              "created_utc": "2026-01-01 04:42:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx19tir",
              "author": "r4in311",
              "text": "It's also very safe to assume that this is a comically blatant case of benchmaxing. :-)",
              "score": 9,
              "created_utc": "2026-01-01 04:50:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1wm3i",
                  "author": "No-Dog-7912",
                  "text": "No, this is actually a well thought out use of collecting trajectories for RL. Did you read the blog post? This is what Google recently did with Gemini 3 Flash and itâ€™s starting to become a norm for other companies. They had 32k trajectories thatâ€™s just sick. To be honest, with these results and model size. This would technically mean that this is the best local coding model by farâ€¦. If we could validate this ourselves independently then it would be a huge opportunity gain for local model runners after quantizing the model.",
                  "score": 37,
                  "created_utc": "2026-01-01 08:23:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx1ccmv",
                  "author": "Odd-Ordinary-5922",
                  "text": "tell me how benchmaxing is possible when the test questions arent visible and constantly change",
                  "score": 1,
                  "created_utc": "2026-01-01 05:12:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2hlef",
          "author": "Snuttegubben68",
          "text": "GCUF available now - downloading it with LMStudio",
          "score": 11,
          "created_utc": "2026-01-01 12:03:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxsu6je",
              "author": "eliaweiss",
              "text": "Please update",
              "score": 1,
              "created_utc": "2026-01-05 11:29:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3tpoo",
          "author": "Business_Clerk_8943",
          "text": "https://preview.redd.it/35z3nb6ftrag1.png?width=3497&format=png&auto=webp&s=c2b4ec46269c7bd60813a67176a82d45b315d5fa\n\n[https://huggingface.co/spaces/Jellyfish042/UncheatableEval](https://huggingface.co/spaces/Jellyfish042/UncheatableEval)  \nQwen3 14B's pre-training level. Theyâ€™re obviously gaming the benchmarks. I don't get how anyone buys this.",
          "score": 10,
          "created_utc": "2026-01-01 17:16:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx40zna",
              "author": "Baldur-Norddahl",
              "text": "They are the best model on that eval for pure coding? Also there are no other models at the same size, so we can't really compare anything.",
              "score": 1,
              "created_utc": "2026-01-01 17:54:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1ifxm",
          "author": "TopCryptographer8236",
          "text": "I was hoping the 40B was a MoE but it seems to be a dense model. I guess i was just used with everything bigger than 20B to be a MoE at the moment to balance the speed with consumer hardware. But still appreciate it nonetheless.",
          "score": 12,
          "created_utc": "2026-01-01 06:03:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx324oo",
              "author": "Karyo_Ten",
              "text": "Time to buy a 5090, in NVFP4 it would be 20GB so 12GB left for context",
              "score": 1,
              "created_utc": "2026-01-01 14:42:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1jhy1",
          "author": "Fantastic-Emu-3819",
          "text": "Benchmaxxed or real?",
          "score": 17,
          "created_utc": "2026-01-01 06:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx464ny",
              "author": "FinBenton",
              "text": "Someone on youtube tested it, if you feed it isolated benchmark test type questions then it is extremely good at that but working in real world codebases it fell apart. This might be one of the most benchmaxed models ever made.",
              "score": 7,
              "created_utc": "2026-01-01 18:19:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4m5el",
                  "author": "Fantastic-Emu-3819",
                  "text": "Imagine it was better than opus 4.5. S&P500 would have been -30%.",
                  "score": 7,
                  "created_utc": "2026-01-01 19:38:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4ybk7",
                  "author": "Lopsided_Dot_4557",
                  "text": "I have tested it here and looks good:  [https://youtu.be/NrqE2mKHagg?si=4PYWVlJCnvGKMYFM](https://youtu.be/NrqE2mKHagg?si=4PYWVlJCnvGKMYFM)",
                  "score": 3,
                  "created_utc": "2026-01-01 20:40:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1yczv",
          "author": "__Maximum__",
          "text": "Someone test this in their private coding bench",
          "score": 6,
          "created_utc": "2026-01-01 08:41:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx21n26",
              "author": "lumos675",
              "text": "I can test but any gguf available?",
              "score": 6,
              "created_utc": "2026-01-01 09:16:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx23j1q",
                  "author": "__Maximum__",
                  "text": "No, at the moment, the only way is to use transformers, i guess.",
                  "score": 1,
                  "created_utc": "2026-01-01 09:37:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2q3ca",
          "author": "rekriux",
          "text": "I believe the loop integration is the fist implementation of the sort ? Any one can confirm any other implementation ?\n\nThis is a idea I raised, what if we re-used layers to artificially augment the model dept ?  \nBut I was thinking of applying a adapter (rsLoRa) on the second/third pass, making it able to \\*\\*fake\\*\\* a larger model. The power of a dense 72B in a 32b model, about +15-40% more knowledge with the Lora.   \n  \nThe thing with (most?) Lora implementation, last I checked they can't run simultaneous lora on batches, not sure if it was fixed. But if batching is made to wait until next beginning, it may introduce a bit latency for 1st token but it could be worth it with NVRAM prices !",
          "score": 3,
          "created_utc": "2026-01-01 13:18:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2wp53",
          "author": "paryska99",
          "text": "This is huge if true",
          "score": 3,
          "created_utc": "2026-01-01 14:06:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1wje3",
          "author": "Everlier",
          "text": "Report mentions 7B and 14B, but no weights, I'm very curious to try these two!",
          "score": 4,
          "created_utc": "2026-01-01 08:22:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4613x",
          "author": "6969its_a_great_time",
          "text": "So is it good or not?",
          "score": 2,
          "created_utc": "2026-01-01 18:18:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx26fup",
          "author": "Shir_man",
          "text": "Those benchmarks looks sus, has anyone tried it already?",
          "score": 2,
          "created_utc": "2026-01-01 10:08:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3dhzu",
          "author": "_w0n",
          "text": "Does anyone has a benchmark i should try on this model?",
          "score": 1,
          "created_utc": "2026-01-01 15:50:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx43chc",
          "author": "Baldur-Norddahl",
          "text": "I am not seeing the thinking variants on HF. Are only the non thinking versions open weight?",
          "score": 1,
          "created_utc": "2026-01-01 18:05:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6trp3",
          "author": "iansltx_",
          "text": "Seems like it doesn't work with mlx-lm, and the q8 GGUF basically stalls out on my M1 Max 64GB box. What am I doing wrong here?",
          "score": 1,
          "created_utc": "2026-01-02 02:59:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1rmb2",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -4,
          "created_utc": "2026-01-01 07:30:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx327wh",
              "author": "Karyo_Ten",
              "text": "vLLM can load the base FP16 weights",
              "score": 1,
              "created_utc": "2026-01-01 14:43:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzfuqg",
      "title": "Why Kimi K2 Thinking choose Int4 QAT, from infra enginner of KImi",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/",
      "author": "nekofneko",
      "created_utc": "2025-12-30 11:33:10",
      "score": 171,
      "num_comments": 10,
      "upvote_ratio": 0.94,
      "text": "I saw the recent [discussion](https://www.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/) here regarding MiniMax engineer's tweet about why they decided *against* using int4 QAT for the MiniMax M2.1 model.\n\nInterestingly, at the time of the K2 Thinking release, a Kimi infra engineer posted a deep dive on Zhihu explaining why native int4 QAT was actually crucial for them. Iâ€™ve summarized the key takeaways below to offer a different perspective on the 'to quant or not to quant' debate.\n\n**TL;DR:** Kimi found int4 QAT is essential for **MoE latency**, **long-context stability**, and **speeding up the RL training loop**.\n\n# Decoding is Memory-Bound (Latency Focus)\n\nUnlike the MiniMax case, Kimi found that for their specific MoE architecture (which is highly sparse), the decoding phase is almost exclusively memory-bound. By using W4A16 (4-bit weights, 16-bit activations), they reduced memory usage significantly. This allowed the model to fit on fewer GPUs, which reduced inter-device communication overhead, a major factor in lowering end-to-end latency for users.\n\n# PTQ Failed at \"Thinking\" Lengths\n\nThe team initially tried standard Post-Training Quantization (PTQ). While it worked for short responses, it fell apart for the long chain-of-thought \"thinking\" process. As generation length increased, quantization errors accumulated, leading to degradation. Furthermore, PTQ struggled with sparse experts; if an expert wasn't hit frequently during the calibration step with the calibration dataset, it essentially \"forgot\" knowledge. QAT (Quantization Aware Training) was necessary to make the model \"lossless\" compared to the BF16 baseline.\n\n# A less discussed benefit: Faster RL Training\n\nThis is the point that often gets overlooked: Int4 QAT wasn't just for inference serving, it accelerated the training process itself. In Reinforcement Learning, the model spends a massive amount of time in the \"rollout\" phase (generating text). By using the Int4 model for these rollouts, they reduced the total time for an RL iteration by 10-20%. It also reduced the discrepancy between the training forward pass and the inference engine.\n\n# Why Int4 and not FP4?\n\nThey chose standard Int4 over newer formats like FP4 to maintain compatibility with existing hardware (non-Blackwell GPUs) and to utilize mature, highly efficient kernels like Marlin.\n\nIn summary, I believe there isn't a one-size-fits-all answer regarding quantization. It depends heavily on the model's parameters and specific architecture. It is a matter of trade-offs.\n\n[ AI translation, there may be some translation errors.](https://preview.redd.it/dzmceu5zybag1.png?width=1362&format=png&auto=webp&s=a0ba8f78c6e5ade3463a1c62fba1d338a1c01ce9)\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwpqt2p",
          "author": "nekofneko",
          "text": "Source: [Zhihu](https://www.zhihu.com/question/1969558404759544488/answer/1970539327902679960)",
          "score": 17,
          "created_utc": "2025-12-30 11:34:56",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwprq9u",
          "author": "cantgetthistowork",
          "text": "K2 is the only model that remains coherent at the advertised 256k max context",
          "score": 31,
          "created_utc": "2025-12-30 11:42:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq0ka2",
              "author": "nullmove",
              "text": "Kimi linear has frontier level quality in context arena. Waiting for a bigger model powered by KDA. Arcee also recently independently verified that KDA is a real deal.",
              "score": 12,
              "created_utc": "2025-12-30 12:50:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx55qji",
                  "author": "uhuge",
                  "text": "https://huggingface.co/arcee-ai/AFM-4.5B-Base-KDA-Only not great in math, somehowÂ ",
                  "score": 1,
                  "created_utc": "2026-01-01 21:19:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwps0s2",
              "author": "FreePart5727",
              "text": "really? I have to give it a try",
              "score": 2,
              "created_utc": "2025-12-30 11:45:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwsol2a",
              "author": "UnknownLesson",
              "text": "Even better than Gemini Pro?",
              "score": -1,
              "created_utc": "2025-12-30 20:50:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwprqnp",
          "author": "Doris_Dressy1",
          "text": "Thank you for providing another perspective, Iâ€™ve learned a lot",
          "score": 8,
          "created_utc": "2025-12-30 11:42:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuq0zg",
          "author": "Lissanro",
          "text": "INT4 format makes it really great for running locally, as GGUF Q4\\_X which preserves the INT4 quality. I also find its cache memory efficient - full 256K fits in just 96 GB VRAM at Q8. I hope they continue releasing their future large models in 4-bit format.",
          "score": 3,
          "created_utc": "2025-12-31 03:24:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}