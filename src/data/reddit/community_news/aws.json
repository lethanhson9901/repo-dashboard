{
  "metadata": {
    "last_updated": "2026-02-21 08:57:51",
    "time_filter": "week",
    "subreddit": "aws",
    "total_items": 20,
    "total_comments": 164,
    "file_size_bytes": 199687
  },
  "items": [
    {
      "id": "1r4yqqp",
      "title": "Small PSA regarding ECR and Docker CLI for pushing images",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r4yqqp/small_psa_regarding_ecr_and_docker_cli_for/",
      "author": "magnetik79",
      "created_utc": "2026-02-14 23:05:45",
      "score": 140,
      "num_comments": 16,
      "upvote_ratio": 0.99,
      "text": "Hey all.\n\n  Quick post of something I noticed over the weekend which might trip up someone else.\n\n\nWas pushing a Docker image into ECR using a GitHub Actions deployment workflow, a workflow that's been same-same for a good six months and suddenly two days prior was failing with the following error:\n\n```\nunknown: unexpected status from HEAD request to https://XXXXX.dkr.ecr.ap-southeast-2.amazonaws.com/v2/XXXX/XXXX/manifests/sha256:XXXX: 403 Forbidden\nmake: *** [Makefile:68: burp] Error 1\nError: Process completed with exit code 2.\n```\n\nAfter a little head scratching, I pulled out a few community threads via Google - all from 1 - 2 years ago, but suspiciously had some very recent comments (two days prior) on them with similar issues:\n\n- https://repost.aws/questions/QUYf5U-mW3SqaYKFEvbr9fzw/suddenly-getting-403-on-pushing-my-containers-to-ecs\n- https://stackoverflow.com/questions/79137398/gitlab-cicd-issue-403-forbidden-while-pushing-docker-image-to-aws-ecr\n\nThe IAM role used in my GitHub workflow was (as it should be) fairly restrictive - with the following IAM actions only:\n\n```\necr:BatchCheckLayerAvailability\necr:CompleteLayerUpload\necr:InitiateLayerUpload\necr:PutImage\necr:UploadLayerPart\n```\n\nThese are all honed against a [specific ECR repository ARN](https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonelasticcontainerregistry.html#amazonelasticcontainerregistry-repository).\n\nTurns out, adding `ecr:BatchGetImage` was the fix - this provides the ability for querying image digests from within ECR, which is exactly where the HTTP HEAD error lies.\n\nSo, it seems a recent release of Docker CLI has changed the behavior of `docker push` to now query image digests during an image push and I can only assume this version recently landed on GitHub managed workflow runners.\n\nAnyway... hopefully this helps someone else out of a bind!\n",
      "is_original_content": false,
      "link_flair_text": "technical resource",
      "permalink": "https://reddit.com/r/aws/comments/1r4yqqp/small_psa_regarding_ecr_and_docker_cli_for/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5ffwnp",
          "author": "l0g0ut",
          "text": "Thank you for the in depth investigation report. These kind of post and people like you really made Reddit a treasure",
          "score": 26,
          "created_utc": "2026-02-15 00:20:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fji5o",
              "author": "magnetik79",
              "text": "thanks for the kind words.",
              "score": 5,
              "created_utc": "2026-02-15 00:41:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6gwm4m",
                  "author": "Useful-Process9033",
                  "text": "Posts like this are worth more than half the AWS docs out there. Silent breaking changes in CI/CD pipelines are the worst kind of incident because everything looks green until it isn't. We've been cataloging these kinds of \"nothing changed but everything broke\" scenarios and they're way more common than people think.",
                  "score": 1,
                  "created_utc": "2026-02-20 18:43:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5f5bs0",
          "author": "phaubertin",
          "text": "This is really good to know, thanks for posting.",
          "score": 16,
          "created_utc": "2026-02-14 23:14:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5f9egh",
              "author": "magnetik79",
              "text": "cheers.",
              "score": 5,
              "created_utc": "2026-02-14 23:39:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5fsc73",
          "author": "MonkeyArmpit",
          "text": "I wasn‚Äôt aware of docker cli change. I always just set it up as the official documentation suggested \n\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/image-push-iam.html",
          "score": 5,
          "created_utc": "2026-02-15 01:39:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lu3kg",
              "author": "magnetik79",
              "text": "Good callout.\n\nSmall issue there, `ecr:GetAuthorizationToken` doesn't have an ARN association, so technically that second policy example may not actually work.",
              "score": 1,
              "created_utc": "2026-02-16 01:07:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ltpri",
          "author": "puttak",
          "text": "You are saved my life.",
          "score": 2,
          "created_utc": "2026-02-16 01:05:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rileh",
          "author": "FlowPuzzleheaded4995",
          "text": "Thanks for timely post we were having our CI/CD pipelines failing today this helped alot resolving quickly",
          "score": 2,
          "created_utc": "2026-02-16 22:22:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5riqtg",
              "author": "magnetik79",
              "text": "Not a problem - wasn't sure if my post was gonna just be Internet points farming, but seems I've helped a few people out :)",
              "score": 2,
              "created_utc": "2026-02-16 22:23:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5riv1v",
                  "author": "FlowPuzzleheaded4995",
                  "text": "absolutely!",
                  "score": 1,
                  "created_utc": "2026-02-16 22:24:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5hs9uk",
          "author": "thebru",
          "text": "Hit something similar. \n\nWe ended up disabling `provenance` in the build. It was pushing an image index along with the two images we built through. \n\nLambda didn't like this.",
          "score": 1,
          "created_utc": "2026-02-15 11:47:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ffyqc",
          "author": "burnbern",
          "text": "Had the same issue and Opus saved me‚Ä¶lol",
          "score": 1,
          "created_utc": "2026-02-15 00:20:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8ehng",
      "title": "Everyone says \"tag your resources\" for cost control. Nobody explains how to actually do it well.",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r8ehng/everyone_says_tag_your_resources_for_cost_control/",
      "author": "alex_aws_solutions",
      "created_utc": "2026-02-18 20:35:12",
      "score": 99,
      "num_comments": 58,
      "upvote_ratio": 0.88,
      "text": "Every AWS cost optimization post says the same thing: \"tag your resources, use Cost Allocation Tags.\" Great advice, very helpful, thanks.\n\nBut after 18 months of cleaning up a pretty messy AWS setup I realized that having tags is not the hard part. The hard part is having the right tags in a structure that actually tells you something useful. We went from \"yeah we tag stuff\" to genuinely understanding our spend down to the feature level, and the difference is night and day.\n\nHere's what worked for us.\n\n**Three mandatory tags, everything else optional**\n\nWe use exactly three required tags on every resource:\n\n* **Environment**: prod, staging, dev and sandbox. Obvious but you'd be surprised how many things don't have this.\n* **Service**: this is YOUR service, not the AWS service. So not \"RDS\" but \"payment-processor\" or \"user-api\" or \"data-pipeline\". This is the one that matters most.\n* **Team**: who owns this when it breaks at 2am. Also who gets asked when the cost spikes.\n\nThe key insight for us was Service. We used to tag by AWS product type which told us basically nothing we didn't allready know from Cost Explorer. Once we started tagging by our own service names, everything changed. A single Service:payment-processor tag now spans the ALB, the ECS tasks, the RDS cluster, the SQS queues. I can see what it actually costs to run payments across all infrastructure, not just what individual resources cost in isolation.\n\n**Why only three**\n\nWe started with 12 required tags. Compliance was maybe 40% at best. People just didn't bother or tagged inconsistently. Dropped to 3 mandatory + 5 optional and we're at around 95% now. Turns out people will actually do it if you keep it simple.\n\n**Enforce tagging at creation, not with angry Slack messages**\n\nThis was probably our biggest lesson. We handle this on two levels now:\n\n1. We use OPA policies with Terraform now (see picture). If a resource doesn't have the three mandatory tags, the apply just fails. No exceptions, no \"I'll add it later\". Retroactive tagging is a nightmare and honestly a waste of everyones time.\n2. At the AWS Organization level with SCPs, they block the creation of resources that don‚Äôt include those tags. This covers cases where someone spins up resources manually in the console, through the CLI or SDK, outside of terraform.\n\nWe spent almost two weeks tagging old resources manually before we accepted it would have been cheaper to just let them expire and recreate them properly. If you're early enough, enforce from day one. If you're late, don't try to fix everything, just enforce going forward and let the old stuff cycle out.\n\n**The report that actually gets read**\n\nWe have a simple monthly report that flags any service where cost went up more than 30% month over month. The catch is this only works if tagging is consistent, which is why enforcement matters so much.\n\nWhen payment-processor jumps from $800 to $2,400, thats a conversation worth having. And it‚Äôs a very different conversation than \"our EC2 bill went up\". Finance doesn't care about EC2 vs Lambda. They want to know what business capability costs what and whether the increase makes sense. \"The recommendation engine doubled because we shipped a new model\" is an answer people can actually work with.\n\n**The unsolved problem: shared infrastructure**\n\nThe one thing we still don't have a clean answer for is shared resources. Databases that serve multiple services, shared Redis clusters, that kind of thing. Right now we tag those with the primary consumer and accept it‚Äôs not perfectly accurate. Looked into split cost allocation tags but honestly it felt like over-engineering for our size.\n\nCurious how others handle this. Anyone have a tagging strategy that actually survived contact with reality? Especially for shared infrastructure.",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1r8ehng/everyone_says_tag_your_resources_for_cost_control/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o64h47r",
          "author": "ryancoplen",
          "text": "IMHO, while tags can be helpful its not the end all be all, specifically because of the issues you called out at the end: shared \"stuff\".\n\nInstead of trying to use tags and assigning ownership that way, I massively prefer to separate everything by account.\n\nYou bring up Environment, Service and Team as ways that you want to enforce control. This is backwards from my preference.\n\nTeams are the top level of ownership and accountability. AWS accounts should be owned by a single team. What happens in those accounts is the responsibility of the owning team.\n\nServices should be owned by a single team, and they should run in accounts owned by those teams.\n\nStages (Dev/Test/Prod) are how Services are deployed. Each stage of a service should have its own account. This makes sure that someone whacking an IAM role in your devo environment doesn't somehow impact your Prod envionment which is (stupidly) running  in that same account.\n\nI'd add that regional distribution is another thing that should also be happening in their own accounts. So your NA prod environment for Service A is in a different account than the EU prod environment for ServiceA.\n\nBreak your infra up by accounts and make one team the operational and financial owner of each account. Then when you (the FinOps person) get a billing alert for an account (or the security team gets an alert for someting), you know exactly who to reach out to and everyone should already understand and agree with how responsibility is distributed.\n\nTeams are free to use tags in their own accounts however they like (or not use them), it doesn't matter to the bigger organization or company.",
          "score": 44,
          "created_utc": "2026-02-18 20:59:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o655aau",
              "author": "rariety",
              "text": "Agree with this, with the caveat that you need to be careful about going too granular and creating too many accounts. Yes it's great for blast radius, but there's a base cost for a new account typically, and if everything needs to be privately networked together, it can get incredibly expensive if your accounts start numbering in the hundreds",
              "score": 6,
              "created_utc": "2026-02-18 22:52:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o657gmy",
                  "author": "ryancoplen",
                  "text": "Yeah, in general I would try to avoid any shared networking requirements. Instead I prefer to use things like ALB, APIGW, SNS, SQS, etc to manage communication across accounts. These constructs make your relationships and dependencies between accounts much more clear.\n\nBut yes, if its impossible to avoid having something like a shared VPC or (heaven forbid) something like Transit Gateway, then yes, there is a cost for having too many accounts.\n\nMy advice would be, **don't do that**! But sometimes you are just saddled with legacy designs and systems and have no choice. At least you can stop making it worse and extending the poor design, but that takes buy-in from leadership.",
                  "score": 4,
                  "created_utc": "2026-02-18 23:03:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o689ws6",
              "author": "camelConsulting",
              "text": "This is the way! I do cloud strategy & lots of FinOps and have come to the same conclusion.\n\nYou need separate accounts anyway for different environments, as you say, and mostly you can group accounts by specific application/service. Accounts attach to dedicated network accounts with shared VPCs.\n\nThen @OP to answer your question, there will be accounts with shared resources. Whether those are things like foundations / network / observability or large shared databases like BQ.\n\nThe way I approach those is to sort those accounts descending by $ spend and work with the platform teams involved to come up with an allocation methodology. You might ‚Äúpeanut butter‚Äù spread core foundational infrastructure like VPCs or even observability, basically assign a % of the cost to applications based on their % of total directly attributable spend and therefore a ‚Äúprobably accurate‚Äù but imprecise mechanism.\n\nBut I‚Äôve also done ‚Äòshared platforms‚Äô like container platforms or shared large databases where you need to ensure there‚Äôs a proper intake associating container apps or DB tables with cost centers or applications. You can then pull operational data like # of x-sized container hours per month as a % of total x-sized container hours to build allocation ratios.\n\nIt gets pretty granular, which is why it is critical to:\n\n1. Start with the most expensive shared accounts and continue descending; and\n2. Make the account owner (I.e. shred platform team) accountable for the spend until that account‚Äôs spend can be allocated to ‚Äúcustomers‚Äù effectively.\n\nYou‚Äôll never hit every single account, but if you do this, the 80/20 rule will help you attribute / allocate the majority of spend with a reasonable output of effort.",
              "score": 2,
              "created_utc": "2026-02-19 12:17:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o68blf5",
              "author": "alex_aws_solutions",
              "text": "I agree with that. That would be the more appropiate choice for a bigger infra. But still need to use the right naming convention for this as well to work, which is almost like tagging.",
              "score": 1,
              "created_utc": "2026-02-19 12:29:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o69br03",
                  "author": "ryancoplen",
                  "text": "The key is that any \"thing\" in AWS is going to be tied to an account. Its usually right there in the ARN. So you only need to maintain a single relationship between account numbers and the \"owning\" team in order to deal with essentially anything.\n\nAnd teams that ignore a naming convention or forget tags or whatever won't impact the ability to link them to the resources that they own.",
                  "score": 2,
                  "created_utc": "2026-02-19 15:51:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o64jldo",
          "author": "SpecialistMode3131",
          "text": "You defined your tag ontology.  Good stuff.  Realize that pretty much every business has a different one, and that's why no one gives you the one-size-fits-all answer since there isn't one.\n\nFor the shared resources, my first instinct is to add \\*all\\* the consumers as tags, and then when costs spike, you know who all's involved.  Then insist on per-service/team metrics (if necessary, custom cloudwatch, but log aggregating works probably pretty well) with a naming scheme that lets you tie them together with the tags, to provide more granular insight.  Really though, this is why backend isn't going to be a solved problem any time soon.  Bespoke solutions for bespoke business.",
          "score": 9,
          "created_utc": "2026-02-18 21:10:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65mkap",
          "author": "katatondzsentri",
          "text": "My advice: never assume that your teams and service ownership will not change. Because it will. Sooner than you expect.\n\nI'd rather go with environment and service tags, and a separate service catalog that (among many others) contains ownership information.",
          "score": 5,
          "created_utc": "2026-02-19 00:25:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69692m",
          "author": "KayeYess",
          "text": "We use two \"golden\" tags .. one for access and the other for cost. These tags (both name and value) are enforced at provisioning time. This is setup when a tenant is onboarded to an AWS account through a highly controlled workflow (many of our accounts have multiple tenants).\n\nA combination of SCP, RCP, ABAC/RBAC policies, CFN Custom Registry, Service Catalog and TF Sentinel are used for enforcement and vertical segmentation.¬†Compliance is also tracked through Governance and FinOPs tools.\n\nThere is no silver bullet solution. AWS provides the tools. Customer has to implement based on their situation, architecture and requirements.",
          "score": 4,
          "created_utc": "2026-02-19 15:24:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65f79d",
          "author": "FransUrbo",
          "text": "Tags for cost control and monitoring is .. dumb!\n\nNo matter how much time you spend on it, you'll never get enough..\n\nBest, imo, is different accounts for everything. With a bit of thougt on cross-account roles, you don't even need SSO and AD etc.",
          "score": 3,
          "created_utc": "2026-02-18 23:45:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64kg0b",
          "author": "Inner_Butterfly1991",
          "text": "None of this seems super bad, but why are you posting AI slop? It's so obvious and so painful to read.",
          "score": 13,
          "created_utc": "2026-02-18 21:14:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64kt2s",
              "author": "Maleficent-Story-861",
              "text": "Accusing everything of AI slop is becoming its own version of slop.",
              "score": -3,
              "created_utc": "2026-02-18 21:16:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o64rthn",
                  "author": "Suspicious-Tough-390",
                  "text": "Yeah but this is",
                  "score": 11,
                  "created_utc": "2026-02-18 21:48:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o64szqp",
                  "author": "nemec",
                  "text": "The post is obviously completely written with LLMs. If there's no way to distinguish your output from a completely fabricated LLM role play... is it worth sharing?",
                  "score": 8,
                  "created_utc": "2026-02-18 21:53:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o65ns0x",
                  "author": "Inner_Butterfly1991",
                  "text": "I could have just said it's horribly written and unnecessarily flowery and doesn't make its point in a concise manner even to the level that if it was a 9th grade essay it would fail. But instead I pointed out what we all know, it came out of an llm, was low effort, and is just written insanely terribly.",
                  "score": 1,
                  "created_utc": "2026-02-19 00:32:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o64fpc6",
          "author": "Mchlpl",
          "text": "Environment: sure\n\nService: yes - as long as these are consistent with how services are actually named in documentation. Think how to automate it.\n\nTeam/owner: meh... this should be in service catalog which you can reference by service name. Makes switching ownership so much easier.\n\n  \nShared resources? Extract into a separate system and tag independently.",
          "score": 3,
          "created_utc": "2026-02-18 20:52:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64kp9q",
              "author": "methods2121",
              "text": "LOL, Owner is the most valuable tag, IMHO, out of these (unless your in accounting) and it's not a person but a distro list or similar , because this is exactly what the team needs when there's a sev1 and the odds that the CMDB is correct or even mapped to a cloud resource is relatively slim.\n\nWe also include the APP\\_ID, (ELUSID in CMDB) because this maps back to the CMDB and during the ATO phase this is checked and validated before deployment is approved.  This then should have all the additional meta data you may need regarding the service/app.",
              "score": 6,
              "created_utc": "2026-02-18 21:15:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o655a2b",
                  "author": "shiftedcloud",
                  "text": "I generally prefer tags to be for things that are unlikely to change because otherwise you have the overhead of changing tags everywhere when those values change.\n\n\nWhen a service's team ownership changes, I don't want to have to modify the IaC for the database.¬†\n\n\nIt's easy enough to have a spreadsheet or table or service catalog that contains the service to team mapping. And it's much easier to modify that mapping when services move to different teams.\n\n\nTo Mchlpl's point, I've had to deal with too many stale tags because no one got around to updating the ownership to want to do that again.",
                  "score": 2,
                  "created_utc": "2026-02-18 22:52:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o64xh7x",
                  "author": "Mchlpl",
                  "text": "Yes, having the way to find owner ASAP is crucial. My experience is that having this information in resource tags leads to incorrect information though, unless you can enforce engineering action (terraform apply) on organisational one (transferring ownership). A service catalog which is the source of truth on who owns what works a lot better in my experience. Just make sure one of your tags is an unambiguous reference to the entry in catalog.",
                  "score": 1,
                  "created_utc": "2026-02-18 22:13:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o64gyul",
              "author": "extreme4all",
              "text": "please elaborate  \n\\> Shared resources? Extract into a separate system and tag independently.",
              "score": 3,
              "created_utc": "2026-02-18 20:58:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o64wr1z",
                  "author": "Mchlpl",
                  "text": "From OP  \n\\> ¬†Databases that serve multiple services, shared Redis clusters, that kind of thing. Right now we tag those with the primary consumer and accept it‚Äôs not perfectly accurate.\n\nTrack it separately and either accept that you can't assign the costs to individual consumers, or work on how to measure actual usage.",
                  "score": 2,
                  "created_utc": "2026-02-18 22:10:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o65pjx3",
                  "author": "NotTooDeep",
                  "text": "Databases are almost always a shared resource. They handle transactions from applications. They provide the data source for reporting databases. The can hold valuable auditing information for cybercrime forensics. They are the target for API calls. \n\nIt makes no sense to try to conform a database to a tag system, with the exception of gross costs. This is actually useful. Making the costs visible can force really interesting questions to be asked, especially about the instance type that's hosting the database.",
                  "score": 1,
                  "created_utc": "2026-02-19 00:42:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o660zwh",
          "author": "ppafford",
          "text": "we use, but I'm looking at other comments to see what I could add/remove/change\n\n```\nproject=\"acme-api\"\ncost_center=\"acme-code\"\nowner_name=\"acme-team\"\nowner_email=\"team@acme.com\"\norganization=\"acme\"\ndepartment=\"billing\"\n```",
          "score": 2,
          "created_utc": "2026-02-19 01:48:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6778mn",
          "author": "T0X1C0P",
          "text": "Thanks for sharing this, I got to learn a few things from this.",
          "score": 2,
          "created_utc": "2026-02-19 06:28:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jxzmv",
          "author": "AlphaToBe",
          "text": "Bro the ONE thing that will silently destroy your tagging strategy and NOBODY warns you about: tag keys are case-sensitive in AWS.\n\n\n`Service` and `service` and `SERVICE` are THREE different tags. They all show up separately in Cost Explorer, they all count against your 500 active cost allocation tag limit, and your 4 teams will absolutely pick different casings because why wouldn't they.\n\n\nI spent a whole afternoon wondering why my cost breakdown numbers didn't add up. Turns out I had `Team`, `team`, and one genius who used `TEAM`. Three versions of the same data, none of them complete. Beautiful.\n\n\nAnd here is the other thing nobody tells you. Cost Allocation Tags have to be MANUALLY activated in the Billing Console before they show up in Cost Explorer. And they are NOT retroactive. So you tag everything today, forget to activate? Last month's data is gone. Forever. Just gone.\n\n\nQuick sanity check, run this and see what tags you actually have activated:\n\n\n```\naws ce list-cost-allocation-tags --status active\n```\n\n\nAnd this one to find resources with missing tags before you waste time in Tag Editor:\n\n\n```\naws resourcegroupstaggingapi get-resources \\\n  --tag-filters Key=Service,Values= \\\n  --query 'ResourceTagMappingList[].ResourceARN'\n```\n\n\nEnforce lowercase-only from day one. Activate your cost allocation tags BEFORE you need the data. Trust me on this one.",
          "score": 2,
          "created_utc": "2026-02-21 05:02:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65um08",
          "author": "edthesmokebeard",
          "text": "AI-format slop.",
          "score": 4,
          "created_utc": "2026-02-19 01:11:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6abbt1",
              "author": "n00lp00dle",
              "text": "reddit has turned into tiktok for medium articles. bullshit listicles that tickle your balls and say what a good engineer you are for having the same opion as chatgpt are completely inescapable.",
              "score": 1,
              "created_utc": "2026-02-19 18:40:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o64ynlv",
          "author": "the_coffee_maker",
          "text": "Where picture?",
          "score": 1,
          "created_utc": "2026-02-18 22:19:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o689pih",
              "author": "alex_aws_solutions",
              "text": "[https://imgur.com/ASxWmvE](https://imgur.com/ASxWmvE)",
              "score": 1,
              "created_utc": "2026-02-19 12:16:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o68foik",
          "author": "ThyDarkey",
          "text": "We do a lot of shared infrastructure as we provision resources for the companies inside our group.\n\nWe just wack on a Company = ALL, there then is some % the finance team then split that recharge back down to those companies. This is all shown in our recharging dashboard in quicksight.",
          "score": 1,
          "created_utc": "2026-02-19 12:56:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64nrpz",
          "author": "JPJackPott",
          "text": "I have a very similar structure to what you describe and it works well. We are single tenanted so service describes the tenant for us. \n\nEach AWS account is a dimension for us too, which describes a logic grouping of customers. \n\nI do two billing breakdowns: one that distributes all ‚Äòunallocated‚Äô costs to the grouping proportionally. Then a second breakdown which breaks each grouping down to tenant level, again folding untagged costs into each tenant proportionally. \n\nNot at the point of making tags mandatory in SCP yet but all deployments are automated so getting the tags in the templates wasn‚Äôt too difficult.",
          "score": 1,
          "created_utc": "2026-02-18 21:29:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64r3cy",
          "author": "blocked_user_name",
          "text": "Thanks this could be helpful.",
          "score": 1,
          "created_utc": "2026-02-18 21:44:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64rnym",
          "author": "religionisanger",
          "text": "We deploy everything in terraform so have git repositories for our resources; it makes it easier to track things like when there‚Äôs a set of load balancers.",
          "score": 1,
          "created_utc": "2026-02-18 21:47:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o650qpv",
          "author": "GoofAckYoorsElf",
          "text": "Add another tag that says where the resource is deployed from if you have no mono repo. Add others that state when the resource has been deployed, from what branch (if on DEV and deployed from an FB), what commit SHA, pipeline ID, who triggered the pipeline... This is all valuable information in the case of a failure.",
          "score": 1,
          "created_utc": "2026-02-18 22:29:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68ycg9",
          "author": "YetMoreSpaceDust",
          "text": "> Nobody explains how to actually do it well.\n\nBut if you want to know how to do it poorly, I can definitely help with that!",
          "score": 0,
          "created_utc": "2026-02-19 14:43:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64jnxr",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -2,
          "created_utc": "2026-02-18 21:10:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6snkt",
      "title": "DynamoDB single-table pattern: SaaS Multi-Tenant with 10 access patterns, 1 GSI (full breakdown)",
      "subreddit": "aws",
      "url": "https://singletable.dev/blog/pattern-saas-multi-tenant",
      "author": "tejovanthn",
      "created_utc": "2026-02-17 01:38:12",
      "score": 65,
      "num_comments": 42,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1r6snkt/dynamodb_singletable_pattern_saas_multitenant/",
      "domain": "singletable.dev",
      "is_self": false,
      "comments": [
        {
          "id": "o5spqqi",
          "author": "cachemonet0x0cf6619",
          "text": "me likes. i don‚Äôt run single table for multi tenant but  if i ever do this will be the reference. \n\ni really liked the site too. only suggestion is that the tables on mobile are tough to read but not a big deal i look forward to more patterns",
          "score": 4,
          "created_utc": "2026-02-17 02:29:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t3xfa",
              "author": "tejovanthn",
              "text": "Thank you :) \n\nWhat patterns would you like to see sooner? üòÅ",
              "score": 1,
              "created_utc": "2026-02-17 03:58:16",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5t400s",
              "author": "tejovanthn",
              "text": "Also, how do you handle multitenant? Separate tables per tenant?",
              "score": 1,
              "created_utc": "2026-02-17 03:58:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5t5j35",
          "author": "finitepie",
          "text": "If i understand you correctly, you are worried, that using an GSI to create a tenant index, could cause a hot partition? But how often do you actually need to make that request? I would just create a {pk TENANT#<tenant-id>, sk:METADATA} for each tenant, that has a property TYPE=TENANT and use a GSI to query for the type, to get a full tenant list or something similiar. But would be more worried, that your general pk/sk design leads to hot partitioning, since your pk is always the tenant id, and the pk determines the partition. What I do is, to break it down into subcategories. like {pk: TENANT#<tenant-id>#USER, sk: <user-id>} or {pk: TENANT#<tenant-id>#PROJECT, sk: <project-id>}. That would be already two distinct paritition, instead of a single one by just using the pattern {pk: TENANT#<tenant-id>, sk: PROJECT#<project-id>}, where¬†all items for that tenant compete for the same¬†1,000 WCU / 3,000 RCU per-partition throughput limit. That works also well for me, since i usually have dedicated api routes for subcategories. What is your security model to enforce tenant isolation?",
          "score": 3,
          "created_utc": "2026-02-17 04:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t8673",
              "author": "tejovanthn",
              "text": "Great points ‚Äî here's my thinking:\n\nTenant index isn't really a worry because it's an admin operation with low volume. I've handled this with the TENANT\\_LIST GSI, but your TYPE=TENANT approach seems functionally similar.\n\nSplitting PKs into subcategories is an interesting approach - you spread writes across multiple partitions with the tradeoff that you lose the ability to read across entity types in a single operation. I think this really depends on the access patterns. For the multi-tenant case, being able to query \\`TENANT#<id>\\` and get metadata + subscription + users in one call is something I reach for a lot.  \nFor most SaaS apps, from what I understand, the hot partition concern is overblown - 1,000 WCU per partition is a lot, and since 2018 DynamoDB's adaptive capacity redistributes throughput to handle hot partitions without you needing to intervene. It won't proactively split them, but it handles the imbalance. If you're at a scale where a single tenant is consistently pushing past that, you probably have bigger architectural decisions to make anyway.\n\nThe tenant isolation point is legit and something I should address in the article. In my production apps I handle this at the application layer (tRPC + OpenAuth ‚Äî every query is scoped to the authenticated tenant). I'm aware of IAM fine-grained access control with \\`dynamodb:LeadingKeys\\` condition keys as the DB-level option, but haven't needed it yet. Have you had success with that approach in practice, or is there something else you'd recommend?",
              "score": 2,
              "created_utc": "2026-02-17 04:27:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5tb242",
                  "author": "finitepie",
                  "text": "I didn't actually have tRPC on the radar. Looks very interesting. Have to learn more about it. So basically, I make sure that the relevant data (tenant id, role, etc) is part of the signed access token, like you do. I have predefined IAM roles with role tags, using the LeadingKeys pattern, and at the API level, the actual dynamodb requests are being done while assuming those roles. This will enforce tenant isolation. But I also have more RBAC/ABAC style of permissions enforced at the middleware level. For all that I build an universal authentication and authorisation system I deploy once (or as often as I want to get more isolation for other reasons)  and can reuse for any other app. It's just plug and play at this point. But was a lot of work to get there. But at the moment I'm still doing REST APIs. Which works nicely, because I'm running Hono with OpenAPI extension, where I only have to define a zod schema as single source of truth, and can easily generate the correctly typed client code via the OpenAPI specs, it automatically generates from that. But the system would work with GraphQL too. ",
                  "score": 1,
                  "created_utc": "2026-02-17 04:47:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5u8q35",
          "author": "SikhGamer",
          "text": "This is a god awful idea if you have a complicated setup. If your setup is flat and easy to understand _forever_ then _maybe_ this would be a good idea.\n\nOtherwise use an RDBMS _please_.",
          "score": 11,
          "created_utc": "2026-02-17 09:42:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uftvd",
              "author": "tejovanthn",
              "text": "True. DynamoDB isn't the right choice for every workload, and forcing single-table design onto a domain with unpredictable or constantly evolving access patterns is going to hurt. No argument there.\n\nBut when the access patterns are well-understood upfront - which they are for a lot of SaaS CRUD apps - the operational simplicity and scaling characteristics of DynamoDB are hard to beat. The goal of this pattern library is to make the \"well-understood\" part easier to get to. :)",
              "score": 5,
              "created_utc": "2026-02-17 10:47:50",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o63y5qg",
              "author": "AntDracula",
              "text": "> use an RDBMS please.\n\nI still don't know why this isn't the *default* thought process, versus being a last ditch effort.",
              "score": 1,
              "created_utc": "2026-02-18 19:30:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6gx1ks",
              "author": "Useful-Process9033",
              "text": "\"Just use an RDBMS\" is fine advice until you need single digit millisecond reads at scale with zero operational overhead. DynamoDB single table design is a tradeoff, not a mistake. The key is knowing your access patterns upfront, which most SaaS CRUD apps absolutely do.",
              "score": 1,
              "created_utc": "2026-02-20 18:45:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tou1y",
          "author": "mamaBiskothu",
          "text": "Why not have separate tables for each tenant?",
          "score": 3,
          "created_utc": "2026-02-17 06:36:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vwxjn",
              "author": "pablo__c",
              "text": "I'd second this. Have multiple tables, one for each tenant, but then do a single table approach for the rest of the stuff. Btw, ignore the \"just use a relational db\" comments. It's ok to try new/different things, and DynamoDB is great for new simple projects with its scale to zero pay as you go model.",
              "score": 3,
              "created_utc": "2026-02-17 16:09:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6du5n5",
                  "author": "tejovanthn",
                  "text": "Agreed - and the table-per-tenant hybrid approach you mentioned is underrated. Works well when you need hard isolation per tenant but want single-table ergonomics within each one.",
                  "score": 1,
                  "created_utc": "2026-02-20 07:22:10",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5tspj0",
              "author": "tejovanthn",
              "text": "It's a valid approach and some teams do this ‚Äî especially when you need hard isolation for compliance (HIPAA, SOC2) or you want to offer dedicated-tenancy as a premium tier.\n\nThe tradeoffs though:\n\n\\- Operational overhead scales linearly - Every new tenant means a new table, new GSIs, new CloudWatch alarms, new backup configs. At 100 tenants that's manageable. At 10,000 it's a nightmare.  \n\\- Cross-tenant queries become expensive - \"List all tenants\" or \"aggregate usage across tenants\" requires scanning every table.  \n\\- AWS account limits - There's a default limit of 2,500 tables per account per region. You can request increases, but it's a signal you're fighting the grain.  \n\\- Cost - Each table with on-demand pricing has its own minimum throughput allocation. One shared table is cheaper than N separate ones.\n\nThe single-table approach gives you logical isolation (tenant-scoped partition keys) with the operational simplicity of one table. If you need stronger isolation, the IAM LeadingKeys approach another commenter mentioned gives you DB-level enforcement without separate tables.\n\nThat said, table-per-tenant is the right call for some use cases ‚Äî particularly when tenants have wildly different scale or strict data residency requirements. It's not wrong, just different tradeoffs.",
              "score": 1,
              "created_utc": "2026-02-17 07:10:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5tuox5",
                  "author": "mamaBiskothu",
                  "text": "Fair points. I get it. This was a fascinating post, thanks a million.",
                  "score": 2,
                  "created_utc": "2026-02-17 07:28:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5t356u",
          "author": "MmmmmmJava",
          "text": "Edit: you‚Äôve fixed it!\n\n~~Your article‚Äôs phrasing seems to indicate you have 3 GSIs, vs 3 access patterns in 1 GSI.~~",
          "score": 2,
          "created_utc": "2026-02-17 03:53:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t4w4w",
              "author": "tejovanthn",
              "text": "Thanks for the feedback :) could you clarify where I can word it better - the blog article, or the post here?",
              "score": 1,
              "created_utc": "2026-02-17 04:04:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5t657t",
                  "author": "MmmmmmJava",
                  "text": "No problem. I think your post could be rephrased to clarify that you walk through two variations. first multiple GSIs and then an overloaded one.\n\nYour article may also benefit from having an index (no pun intended) at the beginning to show the sections and what‚Äôs coming later in the article.",
                  "score": 1,
                  "created_utc": "2026-02-17 04:13:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5uj1om",
          "author": "teo-tsirpanis",
          "text": "That was the best single-table explainer I've ever seen. üëèüèª üëèüèª",
          "score": 2,
          "created_utc": "2026-02-17 11:15:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5um2mk",
              "author": "tejovanthn",
              "text": "Thanks, appreciate it! üòÑ",
              "score": 1,
              "created_utc": "2026-02-17 11:41:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5td0od",
          "author": "gottcha-",
          "text": "How do you handle schema changes?",
          "score": 1,
          "created_utc": "2026-02-17 05:01:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tfy7a",
              "author": "tejovanthn",
              "text": "Good question - this is one of the genuine pain points with DynamoDB, and one that kept me sticking to rdbms for a very long time. \n\nFor attribute-level changes (adding a new field, changing a default), it's straightforward - DynamoDB is schemaless per item, so new items get the new attribute and old items don't. I handle backfills lazily at read time or with a one-off migration script depending on whether the field is required.\n\nFor key structure changes (modifying a PK/SK pattern or GSI), it's more involved. You can't alter keys on existing items - you have to write new items with the new key pattern and clean up the old ones. ElectroDB's versioning helps here: you define a new entity version and can read both old and new formats during the transition.\n\nFor GSI changes, adding a new GSI is non-disruptive (DynamoDB backfills it from the existing table). Changing or removing one requires a migration plan.\n\nHonestly, this is probably the strongest argument against overly complex single-table designs - the more entities and overloaded indexes you have, the harder migrations get. It's why I'd rather start with clean, well-separated key prefixes and only overload GSIs when the access patterns are stable.\n\nSchema migration tooling is a big gap in the DynamoDB ecosystem right now. It's something I'm thinking about for singletable.dev down the road.",
              "score": -1,
              "created_utc": "2026-02-17 05:23:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ti5x8",
          "author": "kingslayerer",
          "text": "If you are on rust maybe you will find my lib useful \n\nhttps://github.com/Salman-Sali/dynorow",
          "score": 1,
          "created_utc": "2026-02-17 05:41:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67lb5r",
              "author": "tejovanthn",
              "text": "Thank you! I'll check it out! I'm not on rust though :)",
              "score": 1,
              "created_utc": "2026-02-19 08:36:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tr86z",
          "author": "Patient-Swordfish906",
          "text": "Good write up, been using single table design in production apps for a few years now.\n\nMy only nitpick with your article is that access pattern #5 is not really covered by your design. You claim you can get a project by ID by using get item on the full SK, but you have the date as a prefix, so you can‚Äôt query only by project ID.",
          "score": 1,
          "created_utc": "2026-02-17 06:57:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tse2s",
          "author": "Soccham",
          "text": "People at my work have been doing this and it‚Äôs a fucking disaster. \n\nJust use relational databases.",
          "score": 1,
          "created_utc": "2026-02-17 07:07:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dttb5",
              "author": "tejovanthn",
              "text": "Sorry to hear that - genuinely. The \"disaster\" pattern I keep seeing isn't single-table being wrong, it's teams designing keys before listing access patterns. You end up with a table that can't serve the queries the product actually needs.\n\nWrote the counterpoint post based partly on this thread: https://singletable.dev/blog/when-not-to-use-single-table-design - curious if any of the failure modes there match what you've seen at work.",
              "score": 2,
              "created_utc": "2026-02-20 07:18:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vyl5y",
          "author": "TechDebtSommelier",
          "text": "Scan is a \"we'll fix it later\" that becomes a 3am incident. Write-shard the GSI key (TENANT#<0-N>), scatter-gather on read, done. Yes it's annoying. No there's no cleaner way. Welcome to DynamoDB.",
          "score": 1,
          "created_utc": "2026-02-17 16:17:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dtww6",
              "author": "tejovanthn",
              "text": "This is the right answer and I should have included it in the post. The scatter-gather overhead is real but it's a one-time design decision vs. the ongoing pain of a Scan at scale. Adding a note on write-sharding to the article. :) thank you!",
              "score": 1,
              "created_utc": "2026-02-20 07:19:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yke8x",
          "author": "ShakataGaNai",
          "text": "What is this, a [schema diagram for ants](https://imgur.com/a/gPKV00U)? But seriously, I can't read it and can't make it any larger.",
          "score": 1,
          "created_utc": "2026-02-17 23:54:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67l225",
              "author": "tejovanthn",
              "text": "Thanks for the feedback üòÖ  \nFixed it - made a lightbox so you can zoom into the image too! :) Let me know if this is better  \n[https://singletable.dev/blog/pattern-saas-multi-tenant](https://singletable.dev/blog/pattern-saas-multi-tenant)",
              "score": 2,
              "created_utc": "2026-02-19 08:34:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6duac1",
          "author": "tejovanthn",
          "text": "A few people in this thread described single-table as a disaster at their companies. They're not wrong in those contexts. Wrote the honest counterpoint based on the discussion here: https://singletable.dev/blog/when-not-to-use-single-table-design - covers the specific cases where you shouldn't reach for it, and what the actual failure pattern usually is.\nAny other use cases I should include in the list?",
          "score": 1,
          "created_utc": "2026-02-20 07:23:23",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5t8lgm",
          "author": "the_corporate_slave",
          "text": "Single table pattern is over complicated trash. Unless you are doing an app rewrite where you know exactly what schema you need, this is a mistake",
          "score": -3,
          "created_utc": "2026-02-17 04:30:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tb0tu",
              "author": "tejovanthn",
              "text": "There's a real point here - single-table design has a steep learning curve and if you get your access patterns wrong upfront, refactoring is painful.\n\nBut \"you need to know exactly what schema you need\" is true of DynamoDB in general, not just single-table. Multi-table DynamoDB still requires you to define access patterns before you design. It's not a relational database where you can normalize first and figure out queries later.\n\nWhere I'd push back: single-table isn't all-or-nothing. The modern approach is pragmatic - group entities that are queried together, use a few clean GSIs, don't overload everything into one index just because you can. That's what this pattern does.\n\nThat said, if your app's access patterns are genuinely unknown and/or evolving fast, DynamoDB itself might not be the right choice - and that's a totally valid position.",
              "score": 1,
              "created_utc": "2026-02-17 04:47:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5tc35e",
                  "author": "finitepie",
                  "text": "I actually prefer the single table design. Everything boils down to how your data model is defined. But as you said, the problem is often not the single table design, but that you might need access patterns, that you are not aware of, yet. But a profound schema migration is always a pain in the a\\*. :D",
                  "score": 4,
                  "created_utc": "2026-02-17 04:55:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r6fzf7",
      "title": "Amazon EC2 supports nested virtualization on virtual Amazon EC2 instances",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r6fzf7/amazon_ec2_supports_nested_virtualization_on/",
      "author": "KayeYess",
      "created_utc": "2026-02-16 17:30:58",
      "score": 39,
      "num_comments": 15,
      "upvote_ratio": 0.98,
      "text": "[https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-ec2-nested-virtualization-on-virtual/](https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-ec2-nested-virtualization-on-virtual/)\n\n\"Posted on:¬†Feb 16, 2026: Starting today, customers can create nested environments within virtualized Amazon EC2 instances. Previously, customers could only create and manage virtual machines inside bare metal EC2 instances. With this launch, customers can create nested virtual machines by running KVM or Hyper-V on virtual EC2 instances. Customers can leverage this capability for use cases such as running emulators for mobile applications, simulating in-vehicle hardware for automobiles, and running Windows Subsystem for Linux on Windows workstations.\n\nThis capability is available in all commercial regions on C8i, M8i, and R8i instances. To learn more about enabling hardware virtualization extensions in your environment, see the Amazon EC2 nested virtualization documentation.\"\n\nLink to documentation: [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/amazon-ec2-nested-virtualization.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/amazon-ec2-nested-virtualization.html)",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1r6fzf7/amazon_ec2_supports_nested_virtualization_on/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5q15jf",
          "author": "im-a-smith",
          "text": "I can then run Docker inside and have images running inside VMs inside VMs\n\nSweet.¬†",
          "score": 14,
          "created_utc": "2026-02-16 18:05:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qg9wt",
              "author": "visicalc_is_best",
              "text": "This is a great idea, particularly because RAM is so cheap right now",
              "score": 10,
              "created_utc": "2026-02-16 19:14:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5rx567",
                  "author": "phaubertin",
                  "text": "It is when it's not your RAM. üòÄ",
                  "score": 3,
                  "created_utc": "2026-02-16 23:41:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5q1fao",
              "author": "samrwalker",
              "text": "Inception",
              "score": 2,
              "created_utc": "2026-02-16 18:06:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5yq7pr",
              "author": "dudeman209",
              "text": "This was Alan Turing‚Äôs vision!",
              "score": 1,
              "created_utc": "2026-02-18 00:26:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q42ia",
          "author": "mezbot",
          "text": "Lol, the comments are exactly what I wanted to say‚Ä¶ needs more layers!",
          "score": 5,
          "created_utc": "2026-02-16 18:18:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5q4xli",
              "author": "pixeladdie",
              "text": "Yo dawg! I heard you like abstractions‚Ä¶.",
              "score": 5,
              "created_utc": "2026-02-16 18:22:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5qcubx",
                  "author": "HiCookieJack",
                  "text": "So we've put podman in your docker, so you can download layers to download layers of layers¬†",
                  "score": 1,
                  "created_utc": "2026-02-16 18:58:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5q5dsq",
          "author": "UnluckyTiger5675",
          "text": "I hope this soon gets propagated to AWS workspaces, so my work mandated Windows 11 workspace can run WSL two instead of just WSL one",
          "score": 9,
          "created_utc": "2026-02-16 18:24:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qax17",
              "author": "bobkiwi",
              "text": "I am in the same boat- WSL2 support would help immensely to avoid the \"but Windows 365 VDIs can do it!\"\n\nIf it's in the m8 series, I wouldn't be surprised if it comes to WorkSpaces by Q3... but which year!",
              "score": 1,
              "created_utc": "2026-02-16 18:49:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ylu6j",
              "author": "SammichAffectionate",
              "text": "We are thinking of migrating away from Workspaces but it keeps getting pushed off. Nested virtualization is just one of the reasons.",
              "score": 1,
              "created_utc": "2026-02-18 00:01:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q1i7c",
          "author": "Alternative-Expert-7",
          "text": "Lets go deeper.",
          "score": 4,
          "created_utc": "2026-02-16 18:07:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qs9j2",
          "author": "MassPatriot",
          "text": "VMception",
          "score": 3,
          "created_utc": "2026-02-16 20:13:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yy2mt",
          "author": "VIDGuide",
          "text": "Yo dawg.. we heard you liked vms, so we put vms in your vms, so you can vm while you vm!",
          "score": 1,
          "created_utc": "2026-02-18 01:09:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60nbpe",
          "author": "ComplianceAuditor",
          "text": "This fucks.",
          "score": 1,
          "created_utc": "2026-02-18 07:59:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r76u7k",
      "title": "How to build a distributed queue in a single JSON file on object storage (S3)",
      "subreddit": "aws",
      "url": "https://turbopuffer.com/blog/object-storage-queue",
      "author": "itty-bitty-birdy-tb",
      "created_utc": "2026-02-17 14:03:33",
      "score": 29,
      "num_comments": 9,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1r76u7k/how_to_build_a_distributed_queue_in_a_single_json/",
      "domain": "turbopuffer.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5vx5ax",
          "author": "the8bit",
          "text": "Starting this 'seems like this won't really scale'\n\nEnding it 'ah you are basically reimplementing Kafka. Bold choice '",
          "score": 17,
          "created_utc": "2026-02-17 16:10:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yk71d",
              "author": "itty-bitty-birdy-tb",
              "text": "Ha. Well I guess we'll¬†take¬†\"Kafka but¬†it's one¬†JSON¬†file\" as¬†a¬†compliment",
              "score": 4,
              "created_utc": "2026-02-17 23:52:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6gwxiy",
              "author": "Useful-Process9033",
              "text": "\"Kafka but it's one JSON file\" is honestly a great pitch. CAS on S3 is surprisingly powerful now that conditional writes exist. The real question is what happens when that JSON file gets large enough that read-modify-write cycles start contending, but for indexer scheduling the write volume is probably fine.",
              "score": 1,
              "created_utc": "2026-02-20 18:44:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yxfp8",
          "author": "ruibranco",
          "text": "The Iceberg comparison is actually really apt ‚Äî both are fundamentally betting that object storage conditional writes are reliable enough to build coordination primitives on top of. The fact that you can get away with a single JSON file instead of a proper consensus protocol says a lot about how far S3's consistency model has come since they went strongly consistent in 2020.",
          "score": 6,
          "created_utc": "2026-02-18 01:06:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vt3gm",
          "author": "AdCharacter3666",
          "text": "This is kinda like Iceberg but for queues.",
          "score": 4,
          "created_utc": "2026-02-17 15:50:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yk7cz",
              "author": "itty-bitty-birdy-tb",
              "text": "yeah decent comparison actually, same idea of using object storage as the source of truth with atomic metadata updates. Iceberg uses manifest files, we use CAS. the nice thing about both patterns is that object storage handles durability and availability so the compute layer can stay stateless.",
              "score": 2,
              "created_utc": "2026-02-17 23:53:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yaex3",
          "author": "Hackinet",
          "text": "Wait, why use a single file? Why not just do a group commit to a new file for a worker to pick up? \n\nIt still doesn't solve the issue with two workers picking up duplicate work in your article but I feel it would simplify the architecture and the write race conditions that you might run into.",
          "score": 2,
          "created_utc": "2026-02-17 22:59:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yjidl",
              "author": "itty-bitty-birdy-tb",
              "text": "the single file with CAS is what gives us strong consistency for free. the full queue state is always in one place, and CAS guarantees that any mutation¬†(push, claim, heartbeat) is¬†atomic with¬†respect to the current¬†state. if¬†you write new¬†files instead, you need¬†a¬†separate¬†coordination¬†mechanism to establish¬†ordering, track¬†which¬†files have been consumed, and¬†garbage¬†collect old¬†ones. you're¬†basically rebuilding the consistency¬†guarantees that¬†CAS on¬†a single file gives¬†you out¬†of¬†the box.",
              "score": 2,
              "created_utc": "2026-02-17 23:49:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r8s1yy",
      "title": "how bad is it to launch without a proper cloud architecture plan?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r8s1yy/how_bad_is_it_to_launch_without_a_proper_cloud/",
      "author": "These_Run_7070",
      "created_utc": "2026-02-19 06:40:35",
      "score": 23,
      "num_comments": 71,
      "upvote_ratio": 0.71,
      "text": "We are 8 months from launch and honestly we have just been spinning up services as we need them. no real architecture doc, just \"lets use this because it works.\" our AWS bill went from 2k to 8k in 3 months and we're not even at scale yet though.   \nmy co founder keeps saying we'll \"fix it after launch\" but I'm getting nervous. what if we hit product market fit and the whole thing falls apart because we built on sand?\n\nIs it crazy to pause feature dev for a month to actually design this properly? or do most startups just figure it out as they grow?\n\n",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1r8s1yy/how_bad_is_it_to_launch_without_a_proper_cloud/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o679a5q",
          "author": "sobeitharry",
          "text": "We are battling mistakes from 10 years ago.  A penny saved is a penny earned.",
          "score": 86,
          "created_utc": "2026-02-19 06:46:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67kc6k",
              "author": "tumes",
              "text": "The number of conference talks I have been to in the last year or two that are just people confidently post mortem-ing, in public no less, the same fucking mistakes every startup makes and has made for the last 15 years in the same order is just‚Ä¶ too magnificent for words. I‚Äôm begging you, we are gross and old but hire one reliable greybeard who has seen some shit, they will save you one funding round worth of flailing and churn by shooting down your galaxy brain bullshit that relies on one non-load balanced server for ingress or a terminally unindexed database column or whatever.",
              "score": 44,
              "created_utc": "2026-02-19 08:26:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o69l4i8",
                  "author": "sobeitharry",
                  "text": "Yet we refuse to bring in outside expertise even when our in house experts recommend.  Just get it done,  yesterday.",
                  "score": 6,
                  "created_utc": "2026-02-19 16:36:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o68s2i1",
              "author": "SpecialistMode3131",
              "text": "You're still alive from 10 years ago.  People seriously underrate survival.  You \\*get\\* to fix the mistakes now because you're still in business.",
              "score": 15,
              "created_utc": "2026-02-19 14:09:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6gwu0b",
              "author": "Useful-Process9033",
              "text": "10 years of compounding tech debt is no joke. The teams I've seen recover from this always start with observability first, you can't fix what you can't see. At minimum get cost alerts and incident tracking in place before launch so you know where the fires are.",
              "score": 1,
              "created_utc": "2026-02-20 18:44:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gypb2",
                  "author": "sobeitharry",
                  "text": "Our company decided to just start building new products and is trying to phase out the old ones.  Of course the people building the new stuff are making the same mistakes,  but i just work here.  We like to repeat our mistakes.",
                  "score": 2,
                  "created_utc": "2026-02-20 18:52:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o679ntm",
          "author": "suddenly_kitties",
          "text": "Push your AWS account team to access whatever startup/Activate credits they are willing to throw at you, and ask for a meeting with a Solutions Architect. You might want to consider getting a contractor/consultant/partner involved at a point, technical debt and your bill will keep growing.",
          "score": 38,
          "created_utc": "2026-02-19 06:49:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i3jhz",
              "author": "Donnelding0",
              "text": "Ditto, there is money you may be leaving on the table that you are entitled to.",
              "score": 1,
              "created_utc": "2026-02-20 22:12:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o679dve",
          "author": "seany1212",
          "text": "I‚Äôd love to know what product you were working on that would allow for not being ready for release in 8 months. Also, how are you spending 8k with no throughput? Just what services at what sizing are you actually running?",
          "score": 42,
          "created_utc": "2026-02-19 06:47:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68h8py",
              "author": "ImNewHere05",
              "text": "Yeah, these numbers are wild",
              "score": 15,
              "created_utc": "2026-02-19 13:06:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6a5hy9",
              "author": "nemec",
              "text": "I hate these AI posts because I can't even trust that the numbers they're giving weren't just made up by an LLM.",
              "score": 2,
              "created_utc": "2026-02-19 18:13:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6boy0q",
                  "author": "Wrectal",
                  "text": "What's the giveaway to you that you believe this is an AI post?",
                  "score": 2,
                  "created_utc": "2026-02-19 22:44:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67ebao",
          "author": "spicypixel",
          "text": "Never seen a company successfully defer good decisions and reclaim that lost ground later.\n\nSome things just fuck you forever.",
          "score": 8,
          "created_utc": "2026-02-19 07:30:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gipxy",
              "author": "mezbot",
              "text": "I have, but it results in at least 10x as much work to retrofit, and it‚Äôs chaos to manage until they finally bite the bullet.  It ends up costing a small fortune.",
              "score": 2,
              "created_utc": "2026-02-20 17:40:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o69j8j6",
          "author": "Ok_Study3236",
          "text": "you're at $96k annual without a product yet? Lol, yes, you need to fix that now. That's a people problem not a technical one",
          "score": 7,
          "created_utc": "2026-02-19 16:27:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67bmz5",
          "author": "behusbwj",
          "text": "There are production services that cost a fraction of that at scale‚Ä¶ what did you do?\n\nCloud architecture ‚Äúplans‚Äù are a scam. Do design reviews with cost analyses. Use services properly with reasonable configurations for your needs. I have no idea what you could be doing with zero traffic that costs thousands already. Did you just spin up a bunch of random ec2 instances and call it a day?",
          "score": 15,
          "created_utc": "2026-02-19 07:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67szg9",
          "author": "mdivan",
          "text": "8k pre launch is huge, you are absolutely correct to be worried how that would scale.",
          "score": 11,
          "created_utc": "2026-02-19 09:52:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67w52e",
              "author": "mamaBiskothu",
              "text": "Also 8 MONTHS from launch it seems. Like what are you doing training the next GPT?",
              "score": 6,
              "created_utc": "2026-02-19 10:22:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67lx04",
          "author": "Sudoplays",
          "text": "Can you give some more insight into what services you‚Äôre using, why and the cost of each one?\n\n‚ÄúFix it after launch‚Äù - this is highly unlikely to happen as new features are needed, bugs get squashed and the focus shifts away from architecture.\n\nPlanning your cloud infrastructure is important, and should be done before deploying any resources. Infrastructure as code isn‚Äôt required, but it‚Äôll make everything easier down the road when you need to have separate dev/rc/prod environments.\n\n8k is a huge bill for something that isn‚Äôt even launched yet, even 3k is quite high.",
          "score": 4,
          "created_utc": "2026-02-19 08:42:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67cvli",
          "author": "watergoesdownhill",
          "text": "This whole thing reeks. Anything that takes more than six months typically doesn‚Äôt ship at all. \n\nI have a feeling you guys are battling customer requirements. While also doing some sort of grand infrastructure.",
          "score": 9,
          "created_utc": "2026-02-19 07:17:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67aohd",
          "author": "notospez",
          "text": "At this stage it depends on your runway. Do you have or expect to have millions available for growth? Then 8k/month in infra is peanuts and you can hire a bunch of people to reengineer next year once you know which product features resonate with customers and have actual usage data.",
          "score": 3,
          "created_utc": "2026-02-19 06:58:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67fiz6",
          "author": "dariusbiggs",
          "text": "Without infrastructure as code from the start you are shooting yourself in the foot. you need to understand what you are using and why you are using it. Then you can see what your minimum spec is and how you can scale it.\n\nYour description is a clear example of not understanding things clearly (your product, cloud infra, and your customer needs) . This is indicated by your costs and are very likely caused by premature over engineering.\n\nCut it down, simplify, and only build scaling when you have metrics demonstrating the need (or the upcoming need due to observable trends).\n\nWe over engineered from the start due to misunderstood requirements and cut down those costs to the point where our entire prod, staging, and dev environments combined cost about what your costs are. Those resources are all running at the minimum specifications for the various components but provide capacity for another 2000% user increase. (Constraints are caused by memory minimums and db connection numbers, whilst the compute is only running at 1-2% load and growth of users has minimal effects on memory usage and the number of DB connections, it's all compute and network IO based).\n\nAnd to cut down costs, the various non-prod environments are turned off when not needed for extended periods of time (like the entire month of the Christmas break period). That is achievable with proper use of IaC, turn it off. You don't need dev or test environments afterhours or on weekends, so kill them if you can.",
          "score": 2,
          "created_utc": "2026-02-19 07:41:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o679whl",
          "author": "Ready-Trick-8228",
          "text": "honestly we threw infros in just to see what was actually eating credits. kind of low effort, low stress way to keep an eye on things without overthinking it.",
          "score": 1,
          "created_utc": "2026-02-19 06:51:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67d50f",
          "author": "mitch3x3",
          "text": "Feel free to dm me. Happy to help if it‚Äôs in my wheelhouse",
          "score": 1,
          "created_utc": "2026-02-19 07:19:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67d8qr",
          "author": "Alsmack",
          "text": "Wall of text incoming; tl;dr: balance your efforts with the realities of time and money. Don't invest too early in things that don't help you prove product market fit. Do invest in things that let you iterate quickly without digging a deep tech debt hole, as being able to deliver changes reliably and quickly lets you get to that product market fit faster.  \n  \nThere is a balance to be had. As a startup there's a few things that really matter. Acquiring/validating product market fit, and the length of your runway (cash on hand / cost per month to run the business = runway in months) are the two that come to mind with this question.\n\nIt's very easy to do things well enough. It's also very easy to make a completely unmaintainable and overpriced mess. What you don't want to do is spend a month making things better than well enough only to launch a product that has no market or can't find it's feet in the market, even if one exists. This shortens your effective runway to solve those problems.\n\nPlatform/tech/infra/arch serve the operations and maintainability of a software product and business. It means nothing if you don't have a product. If what you are doing today is \\*honestly\\* getting in the way of delivering a product (reducing your runway to have a successful (growing) product) or costing too much money (again, increasing burn rate and reducing runway, or too much time due to inconsistency/bad dev flow/whatever) so that you won't hit your deadlines with adequate reserves, then it sounds like there is a business decision to made to solve those pain points.\n\nInvestment in platform is a business decision. You are limited by time and money. Any of those you spend on platform are not being spent on product. Yes, there are tradeoffs - if you don't spend enough time/money on solid foundations, you'll probably have a less reliable product. There's not a magic bullet here, you must evaluate your honest business needs and make a decision based on the facts at hand.\n\nIf you hit product market fit, you have a good problem on your hands. In theory, this means revenue that lowers your burn rate increasing your runway, or more opportunity for investment for more capital which increases your runway, both of which give you more options to get resourcing on stability/scalability/efficiency that you caused by not prematurely spending time on whatever that bottleneck is.\n\nAs an platform decision maker myself, 90% of what I do doesn't matter pre product launch. Make sure we're using IAC that's fast to iterate on, make sure we're documenting decisions, make sure we have reasonable deployment pipelines that are consistent, boring, and reliable, and have any choice for observability and alerting. That's the most I would care about pre launch. Post launch once running a live service, that's a whole different ball game. But, if I've got those 4 things in place we have consistency in provisioning, in deployment, we aren't blind, and we have documented business context. That's a solid foundation for working through operational challenges and pushing for reliability. And you don't need all of them right away. o11y can get expensive fast, for example.\n\nLastly, check with your AWS account rep about AWS startup credits or other programs you can get into. Your infra costs sound way too high for a pre-launch early stage startup, they often give credits for adopting new services and things like that. Depending on your level of support, you may be able to get some help from an aws solutions architect or similar to help manage those costs, though that typically comes with the super pricey stuff.",
          "score": 1,
          "created_utc": "2026-02-19 07:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67f19q",
          "author": "metaphorm",
          "text": "if you can't keep the scaling of operational expenditure below the scaling of revenue growth, the economics of the project are broken. infrastructure cost is a major component of operational expenditure. get this under control now.\n\nyou might reasonably decide to burn extra cash when the scaling cost problems aren't yet critical. you can do this for a short time, but if the curve isn't bent in the right direction, the more you scale the worse it gets, and you'll burn the whole runway and have to raise more money way too soon, probably under poor terms. don't let this one get away from you.",
          "score": 1,
          "created_utc": "2026-02-19 07:36:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g8orw",
              "author": "MaxMcregor",
              "text": "This is such an underrated point if the unit economics do not work, growth just accelerates losses instead of value. Scaling only helps when each new customer actually improves the business, not strains.",
              "score": 1,
              "created_utc": "2026-02-20 16:53:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67ifs9",
          "author": "sunny6333",
          "text": "this is very normal for start up culture because many VCs dont give a shit about how much money u burn, only about revenue growth\n\nyes this will almost guarantee bite u in the ass in the future, but with how frequently roadmaps and decisions change in a start up it's very difficult to nail it immediately",
          "score": 1,
          "created_utc": "2026-02-19 08:08:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67jg8m",
          "author": "Wide_Commission_1595",
          "text": "Honestly if you can afford it, don't worry too much.  Everything can be fixed later.\n\nThat said, a little work to set up a good org structure and a few guardrails isn't too hard.  It causes some initial pain because devs have to follow a plan, but it doesn't need to be horrible",
          "score": 1,
          "created_utc": "2026-02-19 08:18:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67wedo",
          "author": "pint",
          "text": "there are two types of startups i see. one is the garage type, short on resources, fueled by enthusiasm. the other is the investment burning type, which somehow lays a hand on huge sums of angel money, and a new round when the previous runs out.\n\nif you are the second type, why would you care?\n\nif you are the first type, you really need to consider how will you realize enough revenue to cover the 10k, 20k, whatever expense just on infra, and to recover the sunk cost too.\n\nin my experience, the fix it later phase never comes. you always have new things to do, bugs to fix, features to add. nothing lasts longer than a temporary solution.",
          "score": 1,
          "created_utc": "2026-02-19 10:25:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67xtv7",
          "author": "SikhGamer",
          "text": "> my co founder keeps saying we'll \"fix it after launch\"\n\nThis approach is fine, if you _actually_ do it. Otherwise trust your gut.",
          "score": 1,
          "created_utc": "2026-02-19 10:38:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o680pda",
          "author": "256BitChris",
          "text": "Your biggest risk is not getting marketing fit, not whether you'll be able to scale or not.\n\nScaling is a solved problem with well known paths forward - building a successful product isn't.    \n  \nFix it after launch, if it starts to break.  Losing a month to polish tech debt, before reaching PMF could easily kill your startup, especially in today's hyper fast world of AI.",
          "score": 1,
          "created_utc": "2026-02-19 11:03:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68vaz9",
              "author": "AndyWhiteman",
              "text": "Launching without a solid setup can definitely be risky, but a lot of founder get away with it at first and then run into bigger issues later. Having a clear plan for how things will scale from the start can save a lot of headaches down the line.",
              "score": 0,
              "created_utc": "2026-02-19 14:27:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6g9aky",
                  "author": "MaxMcregor",
                  "text": "Totally agree shipping fast is fine, but you need a path to harden things before real scale hits. Early shortcuts are survivable, scaling on top of them without fixing the foundation is where things usually break.",
                  "score": 1,
                  "created_utc": "2026-02-20 16:56:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6839r0",
          "author": "bot403",
          "text": "I was at a B2B business that had 8 figures of top line revenue with 8k/mo spend...\n\n\nWhy are you spending so much with no customers? Scale it down the scale it back up when you know what the bottlenecks are.",
          "score": 1,
          "created_utc": "2026-02-19 11:25:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o688ulo",
          "author": "requiem33",
          "text": "Way to familiar... trying to win the race to the bottom I see.  I've been around the block a few times (grey beard) and the classic mistake of \"it's only temporary\" and \"we'll fix it later\" are the two phrases that have killed more startups than those few that make it to buy out. ",
          "score": 1,
          "created_utc": "2026-02-19 12:09:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68glwb",
          "author": "never-starting-over",
          "text": "I don't know what your product is but I built a video marketplace like Netflix + Youtube with Kubernetes and analytics for stakeholders and it cost 500/month with two copies of the environment running (prod, staging)\n\n3k sounds like a lot. Jumping from 3k to 8k is insane. Spending this much with zero revenue (pre launch) sounds like suicide\n\nEver consider getting someone to review whats going on there?",
          "score": 1,
          "created_utc": "2026-02-19 13:02:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68klza",
          "author": "japanthrowaway",
          "text": "What services are you using that led to such a dramatic increase?",
          "score": 1,
          "created_utc": "2026-02-19 13:27:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68qw4t",
          "author": "rwodave",
          "text": "Technical debt is no joke.",
          "score": 1,
          "created_utc": "2026-02-19 14:02:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68sbsy",
          "author": "SpecialistMode3131",
          "text": "Figure out what your burn rate will be under different scenarios and make a call from that. If scaling up will bankrupt you, then yes.  If scaling up will just mean you have ongoing problems, then probably no - because you'll actually be alive to try and solve them.  Scaling up means you have market fit, and you can move on to series A.\n\nYou eat an elephant one bite at a time.",
          "score": 1,
          "created_utc": "2026-02-19 14:10:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68yir5",
          "author": "TheGutterBall",
          "text": "Based on the description, it almost sounds like you don‚Äôt have the skill set in house to fix it. If you had someone with experience or architectural knowledge, they should have raised the alarm (loudly) by now with these numbers. An architectural doc isn‚Äôt going to solve anything; find a professional, hire them, come up with a plan to get this fully managed, execute on it",
          "score": 1,
          "created_utc": "2026-02-19 14:44:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6965q7",
          "author": "CyclonusRIP",
          "text": "It's more about understanding your constraints.  If you guys have a ton of funding and $8K per month isn't a big deal then maybe you don't need to worry about it.  If you guys are bootstrapping or don't have a ton of runway then you should probably try to minimize hosting cost.  Creating a lot of services can be expensive from a hosting perspective and time consuming to manage that architecture for a small team.  Early on you probably want to minimize the number of services and technologies you use as much as possible.",
          "score": 1,
          "created_utc": "2026-02-19 15:23:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ala8f",
          "author": "ManBearHybrid",
          "text": "There's nothing wrong with a little technical debt... as long as you're realistic about paying it back, and you know both sides of the trade-off. Just blindly deploying stuff without a plan is, frankly, insane. This is before all the possible other concerns - security, reliability, durability, disaster recovery, etc. \n\nAlso, you're probably duplicating a lot of work - doing, undoing and then re-doing a lot of stuff because you're stumbling around in the dark. Taking a breath to put together a plan will give you a proper north star to steer towards. You'll be able to put together a proper road map to get there, and you'll know if you're falling behind. ",
          "score": 1,
          "created_utc": "2026-02-19 19:28:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b4ucr",
          "author": "Own-Manufacturer-640",
          "text": "As an AWS consultant i can assume the same mistakes made by every startup.\n\nDev uat qa resources running 24/7, \nOver provisioned resources because devs have the access to create resource, \nMight be default vpc and public ips, \nZombie resources, \nSnapshot sprawl, \nMulti region resources, \nOr too much NAT Gw usage, \nOver provisioned EBS, \nUsing services that are not useful for your startup,  \nZero ownership of resources etc etc etc. \n\nIf you guys are not using Sagemaker then i think 8k pre launch is too much.",
          "score": 1,
          "created_utc": "2026-02-19 21:03:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c7129",
          "author": "raptorraptor",
          "text": "I'd start looking for another job. Unless you're the most senior engineer, then I'd look for another line of work.",
          "score": 1,
          "created_utc": "2026-02-20 00:28:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ctsbl",
          "author": "DrollAntic",
          "text": "Foundations matter. If you don't have one, you'll struggle to scale.",
          "score": 1,
          "created_utc": "2026-02-20 02:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dy2w8",
          "author": "curiouscrustacean",
          "text": "Sounds like you guys need an experienced DevOps to unfuck the fucked things and do things as you go.\n\nI'm currently doing this for a country level application.",
          "score": 1,
          "created_utc": "2026-02-20 07:58:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ey3gh",
          "author": "RhoOfFeh",
          "text": "Fixing it after launch means never fixing it until a massive re-engineering effort which will be too expensive and too late.",
          "score": 1,
          "created_utc": "2026-02-20 13:00:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fjhk3",
          "author": "grsftw",
          "text": "Yes, do design. No, don't go too deep yet. If you are pre-revenue then your #1 goal is reaching revenue. This to me is a very bad idea: \"Is it crazy to pause feature dev for a month to actually design this properly?\" \n\nSide note, are you saying you are up to $8k/mo for AWS? Unless you are well-funded, yes, that is a bad idea. I'd do some lite design and also a monthly audit and decomm anything not essential.",
          "score": 1,
          "created_utc": "2026-02-20 14:56:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gdcuk",
          "author": "liverdust429",
          "text": "Don't pause for a month but don't wait until after launch either. Spend a week: tag everything, lock down IAM, turn on CloudTrail, set a billing alarm. That alone keeps 8k from silently becoming 20k and prevents the \"oh shit\" moment when your first enterprise customer asks for a security review.",
          "score": 1,
          "created_utc": "2026-02-20 17:15:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hkltb",
          "author": "AftyOfTheUK",
          "text": "The best scenario is the one in which you planned ahead, architected well, kept costs down, got to market with a good product fit, and then succeeded\n\nThe next best scenario is one in which you have to spend quite a lot of money to re-architect at some point your journey, but still achieve the above. DM if you like, that's what I do, and I will be looking for a new role in the summer.\n\nThe worst scenario is one in which you slow down to optimize, miss your window (or spend too much on architecture instead of demonstrable features), and fail completely.\n\nThere's almost nothing you can't architect your way out of, it's just a matter of cost and time. It may be better to allocate five guys to it in a year after a big funding round, than allocate one guy to it today, who would otherwise have been building critical features",
          "score": 1,
          "created_utc": "2026-02-20 20:37:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hzjxu",
          "author": "Neves_Space_Corps",
          "text": "Incremental phased builds in test, with CDK deploy / destroy cycles. Intelligently scaling infrastructure based on test findings. These will help. \n\nWithout a user load, there is no reason to keep infrastructure running all the time.\n\nIaC is your friend here.\n\n$8k/month pre launch sounds like something is hugely amiss.",
          "score": 1,
          "created_utc": "2026-02-20 21:51:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ie6dl",
          "author": "revanthmatha",
          "text": "don‚Äôt listen to any of the guys here unless they have practical experience. the solution is actually really simple. \n\ngo into cursor or ide of choice and literally give it a service account with full system admin api key. have it start going through all the resources you have with your code base. \n\nhave it write an as is document of everything. then prompt it to optimize. read the plan it comes up with and execute. \n\nthis is like a 1-2 days task depending on how many resources you have to consolidate. \n\nfor example if you have many different databases, why not just 1 db with many tables etc.",
          "score": 1,
          "created_utc": "2026-02-20 23:09:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6iz7wd",
          "author": "linux_n00by",
          "text": "OP you have at least a VPC configured?",
          "score": 1,
          "created_utc": "2026-02-21 01:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jzi46",
          "author": "AlphaToBe",
          "text": "Just want to add the practical side because I have been in your exact position and it helped me to just SEE the numbers first before making any big decisions.\n\n\nRun this and it will show you your top 5 money burners right now:\n\n\n```\naws ce get-cost-and-usage \\\n  --time-period Start=2026-02-01,End=2026-02-21 \\\n  --granularity DAILY \\\n  --metrics UnblendedCost \\\n  --group-by Type=SERVICE \\\n  --query 'ResultsByTime[-1].Groups | sort_by(@, &Metrics.UnblendedCost.Amount) | [-5:]'\n```\n\n\nOnce you see that list, I bet at least one of these will jump out:\n\n\n**NAT Gateway** , $0.045 per GB processed. Sounds like nothing but a chatty microservice pushing logs externally can burn $2k/mo through a single NAT. I have seen this catch people off guard more than once.\n\n\n**RDS** , running a db.r5.large 24/7 in dev? Thats ~$260/mo sitting idle. Three environments? $800/mo on databases nobody is querying. Easy to miss.\n\n\n**Public IPv4 addresses** , since Feb 2024 AWS charges $3.65/mo PER public IP. Got 20 EIPs sitting around from \"I was just testing something\"? Thats $73/mo for literally nothing.\n\n\nThese two commands helped me find the orphans in my own account:\n\n\n```\naws ec2 describe-addresses \\\n  --query 'Addresses[?AssociationId==`null`].[PublicIp,AllocationId]' \\\n  --output table\n```\n\n\n```\naws ec2 describe-volumes \\\n  --filters Name=status,Values=available \\\n  --query 'Volumes[].[VolumeId,Size,CreateTime]' \\\n  --output table\n```\n\n\nAnd honestly the one thing I wish I had done from day one is just set a budget alert. Takes 30 seconds and it would have saved me a lot of surprises:\n\n\n```\naws budgets create-budget --account-id YOUR_ACCOUNT \\\n  --budget '{\"BudgetName\":\"dont-surprise-me\",\"BudgetLimit\":{\"Amount\":\"5000\",\"Unit\":\"USD\"},\"TimeUnit\":\"MONTHLY\",\"BudgetType\":\"COST\"}'\n```\n\n\nYou dont need a perfect architecture to start. Just spend one afternoon with those commands and you will probably find $2-3k in quick wins. That buys you time to plan properly without bleeding money.",
          "score": 1,
          "created_utc": "2026-02-21 05:14:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k09p6",
          "author": "Jin-Bru",
          "text": "It all depends on your appetite for risk.\n\nYou could just spin up services and hack them together.  Your maintenance cost go up, your security is non existent, your feature deployment success rate goes down.  Constantly chasing bugs and metrics and looking for a reason why.\n\nOr do it your way, with architecture.  Design it first then build it.  Or design it while you build it.  But it ahould have an architect design something for you.\n\nAt least use what you have to get some IaC mappings and use an IaC (Terraform, CF, Ansible) to build for you.   In this way it will always be documented.   The other way is fraught with sleepless nights.\n\n\nIts a very bad idea.  The day you go public facing is the day you become a target.  It's not just the good guys who will visit your sites.... the bad guys are always looking for a new soft target.",
          "score": 1,
          "created_utc": "2026-02-21 05:20:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67onqb",
          "author": "snorberhuis",
          "text": "Most startups figure it out as they grow, but they don't spend 8k every month. They stay below the $ 1000-a-month threshold. \n\nSpending already 8k a month is ridiculously high. I would expect that this is not necessary. You can have some very bad decisions built into your AWS Architecture that lock in your base spend. The migration costs keep being too low versus the other business opportunities, but they keep eating away at your runway/profit every month. In the end, you can still end up with running out of money.\n\nLast week, I ran a quick scan of the company's architecture, and they were spending $400k a year on AWS because of these bad choices. They could reduce their bill by $100k a year if they had built it properly from the start. The expectation was that, with the forecasted exponential growth of their company, this would grow at the same rate.\n\nIt is not crazy if you are already at 8K a month. Dm me if you want me to take a quick look.\n\n  \n",
          "score": 0,
          "created_utc": "2026-02-19 09:09:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67kkiw",
          "author": "EconomistAnxious5913",
          "text": "8 months? launch quicker in today's times - thats the first thing, launch beta's 1,2,3 but generally launch sooner trather than later.\n\naws bill 8 k isn't a worry.\n\nsand castles crashing isn't a worry. \n\nyes, you can fix later, if you don't have time now. it doesnt matter. it may result in un-ideal spikes and failures, but your target is to get customers, especially on a new product launch, that is your first priority.\n\ncustomers ayenge to tehy will be happy, to sab easily fix and handle ho jayega. yes firgure it out as they grow.\n\n",
          "score": -1,
          "created_utc": "2026-02-19 08:29:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ar3iw",
              "author": "Garetht",
              "text": "I hope you are able to recover from your stroke quickly and easily.",
              "score": 3,
              "created_utc": "2026-02-19 19:55:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6c1np8",
                  "author": "beluga-fart2",
                  "text": "It‚Äôs not a stroke, it‚Äôs Hindi my bro.  He said everything gets figured out eventually , which is generally true.  Even after you go out of business :)",
                  "score": 1,
                  "created_utc": "2026-02-19 23:57:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r9nmsm",
      "title": "When DynamoDB single-table design is the wrong choice (and what to use instead)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r9nmsm/when_dynamodb_singletable_design_is_the_wrong/",
      "author": "tejovanthn",
      "created_utc": "2026-02-20 06:11:35",
      "score": 23,
      "num_comments": 15,
      "upvote_ratio": 0.77,
      "text": "Last week's multi-tenant post hit #1 here. The comments were more useful than the upvotes - three engineers described it as a disaster at their companies.\n\nThey're not wrong. I use single-table design in production and I'm building a tool around it, but there are real situations where it's the wrong call.\n\nThe cases where you shouldn't reach for it:\n\n* Access patterns are still changing (pre-PMF products)\n* No DynamoDB champion on the team - works great until that person leaves\n* Significant reporting or analytics requirements\n* Fewer than 6 access patterns (just use multi-table, it's fine)\n* Multiple teams or bounded contexts sharing the same deployment\n* Per-entity DynamoDB Streams processing (you get one stream per table, not per entity type)\n\nThe \"disaster\" pattern I keep seeing isn't single-table being wrong - it's teams starting with key design before listing access patterns. You design the table to serve access patterns, not the other way around.\n\nFull post covers: the decision framework table, the microservices/team ownership case (probably the most underrated reason to avoid it), and what I actually use for [rasika.life](http://rasika.life) vs what I'd use for a prototype.\n\n‚Üí [https://singletable.dev/blog/when-not-to-use-single-table-design](https://singletable.dev/blog/when-not-to-use-single-table-design)\n\nCurious what situations have pushed your teams away from it - or toward it despite the complexity.",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1r9nmsm/when_dynamodb_singletable_design_is_the_wrong/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6gbip1",
          "author": "menge101",
          "text": "Several of these \"single table design is the wrong choice\" are more \"DynamoDB is the wrong choice\" imo.\n\nParticularly: \"You have serious analytical or reporting requirements\"\n\nIt's an OLTP datastore, not an OLAP, it doesn't matter how many tables you use, you can't do arbitrary queries on the data.\n\nThis one: \"Fewer than 6 access patterns (just use multi-table, it's fine)\", i don't agree with.  It's not about how many access patterns, its about access patterns that capture relationships in the data.\n\nYou can have only one access pattern, but if the table uses the partition key and sort key to create a relationship between records, and then you pull all records for the partition key and have a full representation of the data relationship, you have used single table design correctly with a single access pattern.  (albeit, it would seem unlikely to only have that one access pattern form this multi-faceted related data)",
          "score": 7,
          "created_utc": "2026-02-20 17:06:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gei9r",
              "author": "tejovanthn",
              "text": "Both of these are fair pushback.\n\nOn the analytics point - you're right, and I should have been clearer. That item belongs in a \"DynamoDB is the wrong choice\" list, not a \"single-table is the wrong choice\" list. The underlying problem is using DynamoDB for OLAP workloads at all. The number of tables doesn't change that. I'll fix the framing.\n\nOn the access pattern count - also a fair correction. The \"6 access patterns\" heuristic is too blunt. What I was trying to capture is the case where your data relationships are simple enough that the cognitive overhead of single-table design isn't justified - but you're right that the real signal isn't the count, it's whether your access patterns require traversing relationships within a partition. One access pattern that pulls a full aggregate by partition key is a completely legitimate use of single-table design. I'll reframe that item around relationship complexity rather than access pattern count.",
              "score": 2,
              "created_utc": "2026-02-20 17:20:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6gl5cl",
                  "author": "menge101",
                  "text": "I'm not really pushing back, just discussing the ideas, I think your write up is fine for people who don't really know single table design and/or DynamoDB.\n\nLike you can't be all-use-case encompassingly correct on these things when you are talking in sort of abstract generalizations.",
                  "score": 2,
                  "created_utc": "2026-02-20 17:51:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6dssuw",
          "author": "mehneni",
          "text": "It was a disaster for us as well. Causing 6 digit monthly costs at some point. I guess a problem is always: You have to know what you are doing.\n\nReducing the number of tables compared to a relational design makes sense. But I'd almost limit the single table to something like one aggregate in DDD at most: [https://martinfowler.com/bliki/DDD\\_Aggregate.html](https://martinfowler.com/bliki/DDD_Aggregate.html)\n\nJust putting unrelated data in a complex data model with very different quantities in a single table is a disaster. In our case it was an event store, a production database and a (complex) configuration store. Mixing all of this is an absolutely horrible idea, because the different areas have vastly different requirements and change rates.\n\nScanning the table becomes impossible. This might not matter for day-to-day operations, but if every migration takes a week and is hugely expensive that is a problem and you become very inflexible.\n\nAnd always put an expiry on the data. Because deleting data by scanning the table is just so expensive that is makes more sense to create a new table and drop the old one (which is a stressful thing to do if your backup will take ages to reconstruct the table ;).\n\nThe risk in having more tables is also smaller. In that case you can only mess up one area of the data. So only do single table if you know exactly what you are doing. Merging multiple tables into one is easier than the other way around.",
          "score": 14,
          "created_utc": "2026-02-20 07:09:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ejxn5",
              "author": "tybit",
              "text": "Great point on the DDD aggregate. This is roughly how I‚Äôve viewed it for a while but not had the right term to describe it.\n\nI‚Äôve generally thought of it as only use single table design if you have a well bounded micro-service.",
              "score": 3,
              "created_utc": "2026-02-20 11:19:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6k5d92",
                  "author": "tejovanthn",
                  "text": "\"Well-bounded microservice\" is actually a cleaner way to say it for most developers than \"DDD aggregate\" - same idea, more accessible. The service boundary and the table boundary should roughly align.",
                  "score": 1,
                  "created_utc": "2026-02-21 06:02:26",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6dvg3e",
              "author": "tejovanthn",
              "text": "Six-digit monthly costs is the nightmare scenario - and the root cause you're describing is exactly what I called out in the post: mixing data with vastly different access patterns, quantities, and change rates into one table. An event store and a production database have almost nothing in common operationally. That's not a single-table design problem, that's a data modeling problem that single-table made worse.\n\nThe DDD aggregate framing is actually the clearest heuristic I've heard for where the boundary should be. One aggregate (or tightly related aggregates that are always queried together) per table makes the access patterns predictable and keeps migrations scoped. The moment you're mixing things that evolve independently - your config store vs. your event store - you've lost the main benefit and kept all the costs.\n\nThe TTL point is underrated. Designing for data expiry from day one changes how you think about the whole model. \"How does this data leave the table?\" should be in the access patterns list from the start.\n\nGoing to add the aggregate boundary heuristic to the post - it's a more actionable framing than what I had.",
              "score": 1,
              "created_utc": "2026-02-20 07:33:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gvyvw",
          "author": "AftyOfTheUK",
          "text": ">Last week's multi-tenant post hit #1 here\n\nLink?",
          "score": 1,
          "created_utc": "2026-02-20 18:40:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6junn3",
              "author": "tejovanthn",
              "text": "Post - https://www.reddit.com/r/aws/s/q8V8p4A9ab\nBlog -https://singletable.dev/blog/pattern-saas-multi-tenant\n\nI would love your thoughts too :)",
              "score": 1,
              "created_utc": "2026-02-21 04:37:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ju084",
          "author": "finitepie",
          "text": "You can not talk about DynamoDB vs relational DB without talking about scaling. Scalability is the whole point behind DynamoDB. Everything else is a trade-off to make it scalable. I like DDB. I like it a lot. It's not that one super dooper database to rule them all. It serves a well defined purpose. And sometimes it's just not the right tool for the job. If you understand that in advance, understand how to make the tool work for you and not against you, then DDB is your friend. And it might be 'schema-less', but I want to see you change relational schemas back and forth and not complain about the consequences :D. There is always a schema, a logic, a model, just that it is not explicitly defined and enforced at the DB level like it would be with a relational db. And schema definition for a relational db or data in general can be a lot of work.  It requires a lot of discipline to work it out and to keep it consistent in the long run. Having a good model/schema of your data, is to understand the data. And once you get there, you can also understand the access patterns better. So for me, modelling the data is always the very first step. Every mistake you make in that process will always result in plenty of pain and work. For DDB even more so. ",
          "score": 1,
          "created_utc": "2026-02-21 04:33:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k588j",
              "author": "tejovanthn",
              "text": "Hard agree on the sequencing - model the data first, derive access patterns from that, then design the table. The \"schema-less\" label does a lot of damage because it implies you can skip that step. You can't. The schema just lives in your application code instead of the database, which means when it breaks, it breaks silently.\n\nThe scalability point is the crux of it. People reach for DynamoDB because of the scaling promise, then design it like a relational database and get the worst of both worlds - none of the query flexibility, none of the operational simplicity.",
              "score": 1,
              "created_utc": "2026-02-21 06:01:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dnode",
          "author": "galnar",
          "text": "Thanks for sharing, this is really thought provoking. Would you mind sharing your job title? I‚Äôm wondering what role gets this deep in the weeds. Do you have the same level of experience with RDS and/or DocumentDB?",
          "score": -1,
          "created_utc": "2026-02-20 06:23:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6drp4t",
              "author": "tejovanthn",
              "text": "Thank you :) I used to lead engineering teams building on AWS for the better half of the last decade. Decided to step back and focus on some passion projects. :)   \n  \nCurrently I'm an indie developer. Day job is building [rasika.life](https://rasika.life) (a Carnatic music platform) where DynamoDB is the primary datastore ‚Äî so this isn't theoretical for me, it's the system I'm maintaining.\n\nOn RDS: yes, reasonably deep. I've built on Postgres for years and it's my default for anything with complex reporting, evolving schemas, or strong relational requirements ‚Äî which is basically why it made the \"when to avoid single-table\" list. DocumentDB I've touched but wouldn't claim expertise.\n\nThe honest answer is that I get deep in DynamoDB specifically because I made the choice to build on it and had to live with the consequences. Nothing sharpens your opinion on a tool like owning it in production.",
              "score": 5,
              "created_utc": "2026-02-20 06:59:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4k17f",
      "title": "How are you managing Bedrock?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r4k17f/how_are_you_managing_bedrock/",
      "author": "jmreicha",
      "created_utc": "2026-02-14 12:59:48",
      "score": 19,
      "num_comments": 36,
      "upvote_ratio": 0.91,
      "text": "Looking for perspective on how teams are managing their Bedrock architectures and trying to get a handle on some things. Some questions I have:\n\n\\- How are you managing cost and cost attribution?\n\n\\- Are teams centralizing Bedrock infrastructure and model management? Or deploying models in each account?\n\n\\- How are folks managing security? What kinds of governance and guardrails are being put in place?\n\n\\- What about AgentCore? How is that being managed?\n\n\\- What is everyone using to manage changes? Terraform? Something else? Terraform support seems to be lacking.",
      "is_original_content": false,
      "link_flair_text": "architecture",
      "permalink": "https://reddit.com/r/aws/comments/1r4k17f/how_are_you_managing_bedrock/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5c7m6h",
          "author": "2BucChuck",
          "text": "Built an API on the front of it in ECS and Lambda  to limit each user based on tokens which can be increased as needed.   In that an Admin can manage users and bots leveraging bedrock and while at it just made it an AWS MCP.  didn‚Äôt want to give bots any direct access to AWS IAM roles so tokens and JWT gateway seemed better to tamp down runaway usage since its early days until we could see how much costs and usage were coming from different places and users and tools",
          "score": 26,
          "created_utc": "2026-02-14 13:49:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dmhnw",
              "author": "aboothe726",
              "text": "I really like that approach. I've done something similar to give internal users access to vendor APIs while controlling for and tracking usage and without having to share the actual credentials for the vendor APIs. Worked really well.",
              "score": 4,
              "created_utc": "2026-02-14 18:18:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5cqkr2",
              "author": "2BucChuck",
              "text": "DM me I can share the setup we have but would call is an Alpha release",
              "score": 2,
              "created_utc": "2026-02-14 15:37:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5eb4oy",
              "author": "weirdbrags",
              "text": "did you look at litellm?",
              "score": 2,
              "created_utc": "2026-02-14 20:25:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ec3yu",
                  "author": "2BucChuck",
                  "text": "Interesting but no , we have to maintain lots of PII and SOC2 and this whole AI area has too many moving parts for us at the moment.  It‚Äôs the same idea though I see",
                  "score": 3,
                  "created_utc": "2026-02-14 20:31:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5dwsy7",
          "author": "CoopertheFluffy",
          "text": "Everyone always asks \"how are you managing bedrock?\" but nobody ever asks \"how are you managing, bedrock?\"",
          "score": 9,
          "created_utc": "2026-02-14 19:09:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c1h2w",
          "author": "FarkCookies",
          "text": "I could not figure out how to do cost attribution except for having acc per team/env. ",
          "score": 8,
          "created_utc": "2026-02-14 13:09:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cre3l",
              "author": "pixeladdie",
              "text": "I haven‚Äôt had to test this yet but does [application inference profiles](https://aws.amazon.com/blogs/machine-learning/manage-multi-tenant-amazon-bedrock-costs-using-application-inference-profiles/) do what you need?",
              "score": 7,
              "created_utc": "2026-02-14 15:41:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5czrfp",
                  "author": "iwearhaines",
                  "text": "This is what we use. SCP to deny any model invocation that didn't go through an AIP, with each AIP tagged to the owning team",
                  "score": 4,
                  "created_utc": "2026-02-14 16:24:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5enple",
                  "author": "FarkCookies",
                  "text": "Wow thanks had no idea",
                  "score": 3,
                  "created_utc": "2026-02-14 21:34:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5cd9ky",
          "author": "weirdbrags",
          "text": "question of the year.",
          "score": 6,
          "created_utc": "2026-02-14 14:23:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cos8z",
          "author": "Lba5s",
          "text": "you don‚Äôt",
          "score": 6,
          "created_utc": "2026-02-14 15:28:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ep3da",
          "author": "jojolejobar",
          "text": "We use litellm \nEach user has a key with a budget\nWorks with Claude, open code, openwebui..",
          "score": 5,
          "created_utc": "2026-02-14 21:41:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5c1oqv",
          "author": "AWSSupport",
          "text": "Hi there. I've forwarded your feedback to our Bedrock team for further review.\n\n\\- Roman Z.",
          "score": 5,
          "created_utc": "2026-02-14 13:10:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5crb89",
              "author": "2BucChuck",
              "text": "Ha you might regret posting here but also please fix the pricing on OpenSearch - it‚Äôs outrageous.  We setup one knowledge agent and bill went through the roof.  It‚Äôs not even clear how to undo it since it gets assigned in background.  That said appreciate how fast this was scaled up and the model ecosystem !",
              "score": 5,
              "created_utc": "2026-02-14 15:41:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ficlk",
                  "author": "weirdbrags",
                  "text": "s3 vectors?",
                  "score": 1,
                  "created_utc": "2026-02-15 00:34:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5cegjm",
              "author": "japanthrowaway",
              "text": "Hey while you're at it can you tell the team to maintain their bedrock access gateway a bit better? Bedrock doesn't have a native openai api endpoint so we have to use BAG which doesn't even support all the native models on bedrock itself. Insane how AWS preaches being AI forward but they ignore this piece of critical infra.",
              "score": 2,
              "created_utc": "2026-02-14 14:30:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5cqpon",
                  "author": "Maxious",
                  "text": "https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-mantle.html\n\n\n¬†is this not an openai API endpoint¬†",
                  "score": 3,
                  "created_utc": "2026-02-14 15:38:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5cy8le",
          "author": "Nearby-Tomato9925",
          "text": "Individual Inference profiles with tags and then those tags enabled for AWS Budgets. Is it amazing? No. But at least it is something.",
          "score": 1,
          "created_utc": "2026-02-14 16:16:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5faimo",
              "author": "jmreicha",
              "text": "How many profiles are you managing? I can get behind doing that part with Terraform if it doesn't become a huge number.",
              "score": 1,
              "created_utc": "2026-02-14 23:46:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5d7hs1",
          "author": "VladyPoopin",
          "text": "Application inference profiles for cost attribution to a specific pipeline. Getting more granular can be problematic, but it works.",
          "score": 1,
          "created_utc": "2026-02-14 17:02:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d927u",
          "author": "egoslicer",
          "text": "We use okta, so I built a cost attribution tool by login and token usage. From there, created a leaderboard so we can track usage.",
          "score": 1,
          "created_utc": "2026-02-14 17:10:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5el0dp",
          "author": "ShakataGaNai",
          "text": "Currently experimenting with [LiteLLM Proxy.](https://docs.litellm.ai/docs/simple_proxy) Conceptually it's perfect for the use case. As it has separate auth, admin API's, cost attribution, etc. However, it doesn't work for things like Claude Code (TBD on OpenCode, haven't tried it yet). \n\nBut, I'd really love something first party from Amazon. Having the ability to track token usage per API key or IAM role would be vastly superior than proxying every request with another tool.",
          "score": 1,
          "created_utc": "2026-02-14 21:19:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eqiym",
              "author": "Nick4753",
              "text": "LiteLLM is called out specifically in Claude Code's documentation https://code.claude.com/docs/en/llm-gateway#litellm-configuration\n\nLiteLLM is really underselling itself. You can use it as a gateway for just about any purpose, beyond just engineer access for coding.",
              "score": 2,
              "created_utc": "2026-02-14 21:49:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fzusw",
                  "author": "ShakataGaNai",
                  "text": "It is called out and thats why I tried it, but that doesn't mean it works well. Anthropic is rolling out features that break when running against things like LiteLLM. So it might work for a while, then might break at random. I was having issues with a new beta flag, which you can \"turn off\" in CC....except [it doesn't actually obey the setting](https://github.com/anthropics/claude-code/issues/21676).\n\nAnd the passthrough endpoints on LiteLLM are... meh. There are a bunch of limitations like they don't have good of an idea of the tokens used. They also can't stop usage for a specific key that's over allocation. Also it was, for me, logging hundreds of lines of ... errors(?)... when claude code was working against LiteLLM (and it was in fact working through LiteLLM to Bedrock).\n\nSo yes, it's possible. But if I were someone wanting to use something to control usage for Claude Code for an entire company... I wouldn't rely on that setup. Which is a shame because thats exactly who I am and what I wanted to do.\n\nIf someone has Bedrock/LiteLLM/ClaudeCode setup and working entirely correctly, without kneecapping yourself and losing out on major features, please tell me your config.",
                  "score": 3,
                  "created_utc": "2026-02-15 02:30:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5fdm0r",
              "author": "jmreicha",
              "text": "How does that approach work with inference profiles? Have you found much of the AgentCore tools to have similar functionality to litellm?",
              "score": 1,
              "created_utc": "2026-02-15 00:05:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5f3cm1",
          "author": "donkanator",
          "text": "AWS best practices say to segregate workloads into their own accounts. From there, you don't have to worry about teams stepping on each other's toes if they maintain separate applications. If they are fine being under the same account then whoever fits the bill should be fine to pay for them all.\n\nAt the end of the day, any AI application or system is going to have a normal system architecture first and then some API calls. Chances are, containers or storage or support engineers are going to be much more expensive than a few AI calls. \n\nWe use scp and guardrails to ensure that people use only the models we are comfortable with and invokemodel permissions contain a guardrail condition. \n\nAgentcore is still in the pipeline but I'm struggling with the concept of customers being able to call public cloud apis directly (with a role or IDP token). Normally we expect to have some kind of ingress like application load balancer or cloudfront, but agent core pretty much welcomes anyone to call your API which can be a problem with legal and trade. (Honestly, why do we have to go through this all over again)",
          "score": 1,
          "created_utc": "2026-02-14 23:02:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6kyza",
      "title": "Nested virtualization now available on EC2 instances",
      "subreddit": "aws",
      "url": "https://github.com/aws/aws-sdk-go-v2/commit/3dca5e45d5ad05460b93410087833cbaa624754e",
      "author": "ckilborn",
      "created_utc": "2026-02-16 20:29:52",
      "score": 16,
      "num_comments": 1,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "compute",
      "permalink": "https://reddit.com/r/aws/comments/1r6kyza/nested_virtualization_now_available_on_ec2/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5qvolp",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'compute'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+compute).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-16 20:29:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6fnfx",
      "title": "m8azn single-thread performance tops EC2 benchmarks",
      "subreddit": "aws",
      "url": "https://go.runs-on.com/instances/ec2/m8azn",
      "author": "crohr",
      "created_utc": "2026-02-16 17:19:08",
      "score": 15,
      "num_comments": 2,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "ci/cd",
      "permalink": "https://reddit.com/r/aws/comments/1r6fnfx/m8azn_singlethread_performance_tops_ec2_benchmarks/",
      "domain": "go.runs-on.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5quolc",
          "author": "SkywardSyntax",
          "text": "Finally something that can run my homelab's nginx-proxy-manager docker container and tailscale exit node at the same time",
          "score": 5,
          "created_utc": "2026-02-16 20:24:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5spe0z",
          "author": "Big-Razzmatazz-2899",
          "text": "‚ÄúIt‚Äôs the A Z N, better recognize!‚Äù",
          "score": 1,
          "created_utc": "2026-02-17 02:27:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8cd6p",
      "title": "Is Elastic Beanstalk down?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r8cd6p/is_elastic_beanstalk_down/",
      "author": "vvvwwwwvvwwwvwvvwvvw",
      "created_utc": "2026-02-18 19:16:15",
      "score": 14,
      "num_comments": 3,
      "upvote_ratio": 0.8,
      "text": "All of a sudden our Elastic Beanstalk environments started failing. We can no longer deploy or even change configurations. It seems like AWS made changes to Elastic Beanstalk, CloudFormation, and RDS, and they are no longer communicating correctly.\n\n\n\nWe have **12 environments affected, including production.**\nus-east1\n\nWhen checking AWS Repost, I see others reporting errors that started suddenly:  \n[https://repost.aws/questions/QUeJkV4KL9TNKzZeY0Z\\_0B0A/uploading-new-version-to-elastic-beanstalk-gives-vpc-migration-error](https://repost.aws/questions/QUeJkV4KL9TNKzZeY0Z_0B0A/uploading-new-version-to-elastic-beanstalk-gives-vpc-migration-error)\n\n\n\nSo far I have not seen any official AWS notice about this.\n",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1r8cd6p/is_elastic_beanstalk_down/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o64cng4",
          "author": "pixlgeek",
          "text": "Nothing on our end.",
          "score": 3,
          "created_utc": "2026-02-18 20:38:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64keo8",
          "author": "ruibranco",
          "text": "classic aws move - make breaking changes to the EB/CloudFormation integration and the health dashboard stays green the entire time. check if the repost thread mentions which region, we had similar cfn drift issues in eu-west-1 last month after a silent platform update",
          "score": 2,
          "created_utc": "2026-02-18 21:14:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r621ua",
      "title": "When can we get certification vouchers?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r621ua/when_can_we_get_certification_vouchers/",
      "author": "HistoricalTear9785",
      "created_utc": "2026-02-16 06:21:51",
      "score": 11,
      "num_comments": 5,
      "upvote_ratio": 0.87,
      "text": "I wanted to check if anyone knows about any upcoming AWS events where free certification vouchers might be offered. Last year, they provided 50% discount vouchers are there any similar opportunities coming up this year?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1r621ua/when_can_we_get_certification_vouchers/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5nejtq",
          "author": "MavZA",
          "text": "AWS provides them sporadically really. Sometimes they have certification drives, but it‚Äôs up to their internal team planning etc. we simply don‚Äôt have a view on that.",
          "score": 6,
          "created_utc": "2026-02-16 08:08:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o1cy4",
          "author": "alex_aws_solutions",
          "text": "When you take an exam they will provide you with an 50% discount for the next one. ",
          "score": 3,
          "created_utc": "2026-02-16 11:39:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qrxsu",
          "author": "Thinguist",
          "text": "Just get a job that pays for them. I‚Äôve done 7 now for free.",
          "score": 1,
          "created_utc": "2026-02-16 20:11:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5splkf",
          "author": "socaltrey",
          "text": "Our account manager always seems to be offering them.  I've never found anyone on the team that wants to take AWS on the offer of free certification.",
          "score": 1,
          "created_utc": "2026-02-17 02:28:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c8iba",
          "author": "Diablo-x-",
          "text": "They offered 50% discounts last year in May, just be careful of their payment gateway, i got charged twice.",
          "score": 1,
          "created_utc": "2026-02-20 00:37:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r723uk",
      "title": "How do you build intuition for AWS architecture trade-offs",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r723uk/how_do_you_build_intuition_for_aws_architecture/",
      "author": "Zephpyr",
      "created_utc": "2026-02-17 10:04:21",
      "score": 9,
      "num_comments": 15,
      "upvote_ratio": 0.8,
      "text": "I have been working with AWS for about two years now, mostly ECS deployments and some Lambda functions. My current company uses AWS but most of my work is maintaining what someone else built. I understand how the services work individually but I struggle when asked to design something from scratch.\n\nI have been trying to improve. I go through AWS documentation, watch re:Invent videos, use A Cloud Guru for structured learning and work through small projects to practice IaC code. I use Claude and beyz coding assistant when I am writing Terraform or CDK to make sure my logic makes sense and I am not missing obvious mistakes. I have also started reading through the AWS Well-Architected Framework to understand how AWS recommends thinking about these decisions.\n\nMy problem is I can follow a tutorial but I cannot make architecture trade-offs on my own. When I try to apply it to a real scenario I get stuck. When someone asks why I chose a specific service over another or how I would balance cost versus performance versus operational complexity I do not have a good answer beyond what I read in a blog post. I know the tools exist but I do not know when to pick one over the other.\n\nFor those who went from working with AWS to actually designing AWS solutions, how did you build that intuition for trade-offs? Did you just keep doing practice designs until it clicked or is there a better way to learn the reasoning behind architecture decisions?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1r723uk/how_do_you_build_intuition_for_aws_architecture/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5uceos",
          "author": "respectful_stimulus",
          "text": "Your north star should simply be solving the problem in the simplest way possible. And in the most cost effective way possible. Do this and you‚Äôre already at an advantage over the majority who implement unnecessarily expensive and complex architectures.",
          "score": 23,
          "created_utc": "2026-02-17 10:16:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wa5nc",
              "author": "justin-8",
              "text": "This plus as a thought exercise when picking various pieces of the architecture: what are the limits. E.g. you pick lambda, the 15m max runtime is an obvious limit, is that important for this workload? Or you're using EC2 instances and an auto scaling group - scaling up can take several minutes, do you need to support bursty workloads or are they predictable? Can you just schedule over scaling during peak hours instead? Or do the busiest apis need some other mitigation (caching, migration elsewhere, etc).\n\nBut picking the simplest way should always be first.",
              "score": 3,
              "created_utc": "2026-02-17 17:15:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6gxect",
              "author": "Useful-Process9033",
              "text": "This is the best advice in the thread. The number of over-engineered architectures I've seen that could have been a single ECS service behind an ALB is staggering. Simplicity is a feature, and the teams that ship fastest are usually the ones with the most boring infrastructure.",
              "score": 2,
              "created_utc": "2026-02-20 18:46:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5udo2l",
          "author": "Bub697",
          "text": "For me it‚Äôs all experience and solving the problems yourself.  Build a solution, get it working and then ask, ‚Äúwhat if I needed to handle 10x the load?‚Äù  ‚ÄúWhat happens if an AZ fails?  Let‚Äôs try it!‚Äù  ‚ÄúWhat does it actually cost to run this?  What did I estimate it would cost?  Why was I so far off?‚Äù  \n\nI‚Äôd also add the leaning on an LLM for all these answers will not help you improve in the long term.  Read the docs, try things out, break it, fix it.",
          "score": 7,
          "created_utc": "2026-02-17 10:28:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uq8vt",
          "author": "weirdbrags",
          "text": "it‚Äôs 100% experience. the best way to learn how (and why) to choose/do the right thing is to choose/do the wrong thing, and to then do it over. \n\ni know that‚Äôs easier said than done though. the challenge is finding yourself in a role / environment where you have that kind of an opportunity.",
          "score": 7,
          "created_utc": "2026-02-17 12:13:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uwrlj",
          "author": "Sirwired",
          "text": "Look for \"architecture patterns\"; most problems you encounter have been solved by others, who write a blog post or whatever about it.\n\nBut in the end it comes down to hard-won experience.  I was in IT for about seven years or so before I did any architecture, and fifteen years before \"architect\" was part of my job title.  Not saying you can't do it faster, just that you shouldn't be disappointed if you can't go from \"wow, cloud is neat!\" to \"cloud architect\" in a couple years.",
          "score": 3,
          "created_utc": "2026-02-17 12:57:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vac0p",
              "author": "Own-Manufacturer-640",
              "text": "Exactly this. If i have to make decisions as a junior i just follow the architecture pattern and then research on it",
              "score": 1,
              "created_utc": "2026-02-17 14:14:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5v27e9",
          "author": "mrbiggbrain",
          "text": "I have tried to wrap up a very complex topic into words that might make some sense but big concepts tend to not map so cleanly to a Reddit comment. \n\nAs others have said experience is a key factor. But I always like to say experience is not measured in years it is measured in moments. Good decisions, bad decisions, how you handled an outage, how you rebuilt after a disaster. You'll gain way more experience trying and failing then always staying safe. \n\nBut that does not mean you should not learn from others. There are lots of good books / articles / blogs / postmortems out there on systems design written from both the software engineers and infrastructure engineers point of view. \n\n* Understanding CAP theorem and what Consistency, Availability, and Partitioning are. \n* Understanding how large applications fail, when you should let them fail, and how to build in ways to fail gracefully. \n* Understanding how physics affects infrastructure design. \n* Understanding how various patterns work such as the transactional outbox or idempotent writes in log based messaging queues. \n\nLearn, apply, experience, repeat. I tend to read these books and mark off the things I can use now in one color and the things I think are useful but I don't have a use for in another. Then I come back later and see if I have a use for them later. ",
          "score": 2,
          "created_utc": "2026-02-17 13:29:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ufjrb",
          "author": "swiebertjee",
          "text": "Experience, but it's also logical and can be studied. Especially from other people's code.\n\nYou say ECS and Lambda. Have you set up projects with these tools? Do you have a preference? When would you use one vs another?\n\nFor me, I like Lambda for event driven architecture where you want replayability using (dead letter) queues. For API's serving users, I prefer traditional containers as they can keep multiple endpoints warm. Sure you can do that with Lambda too if you configuring API gateway to point multiple API endpoints to a single provisioned \"Lambdaton\". But at that point, wouldn't you like a service that is easier to develop and test locally? Etc etc.\n\nAnd for pricing, you can use the AWS cost calculator and test some scenarios. How does ECS on Fargate compare to ECS on EC2? You'll find out EC2 is cheaper, but does that mean it's better? How does the operational overhead cost compare? And is the (potential) cost saved worth the operational risk?\n\nIt all depends on context.",
          "score": 1,
          "created_utc": "2026-02-17 10:45:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ujh8o",
          "author": "SonOfSofaman",
          "text": "You mentioned using some tools to \"make sure ... I am not missing obvious mistakes\".\n\nI think you're doing yourself a disservice. Try to not rely on those tools. Don't get me wrong, I'm not an AI-hater. The problem isn't the tools. My point is you learn more from making mistakes than you do by avoiding them.\n\nIf you are baking a cake and you follow a recipe, the result is likely going to be a success. But what have you learned other than how to follow a recipe? If you start with no recipe, only a vague knowledge of which ingredients you need, you'll probably get it wrong at first. But then you'll adjust and iterate and learn from the experience.\n\nEmbrace mistakes. Don't avoid them.\n\nCaveat: with any cloud provider, mistakes can be costly! Always, always, always understand how much a service is going to cost before you use it, then clean up anything you aren't using.",
          "score": 1,
          "created_utc": "2026-02-17 11:19:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v9xtd",
          "author": "SpecialistMode3131",
          "text": "You may have to go work at a startup or similar environment so you're involved in making the tradeoffs.",
          "score": 1,
          "created_utc": "2026-02-17 14:12:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yt3xx",
          "author": "donkanator",
          "text": "First unzoom and then refine. Identify pattern first and general purpose and then follow the crumb trail of requirements. Somebody needs something crazy built? Welp, it looks like a three-tier web app to begin with. That's probably 90% of everything out there. Ingress, compute storage. Input > process > output. \n\nUse thought frameworks: Synchronous versus asynchronous (event driven).  Five architectural pillars. Data temperature. \n\nBDAT from TOGAF is a very good thought process. First you lay out business, application, data flows, and then technical architecture snaps in.",
          "score": 1,
          "created_utc": "2026-02-18 00:42:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o606uzj",
          "author": "philwills",
          "text": "Solve the problem without aws first. Figure out the data structures and interfaces needed. Then, find appropriate services for the different pieces of the solution.",
          "score": 1,
          "created_utc": "2026-02-18 05:37:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r53lyo",
      "title": "Any Advice on Billing?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r53lyo/any_advice_on_billing/",
      "author": "GenderSuperior",
      "created_utc": "2026-02-15 02:57:44",
      "score": 9,
      "num_comments": 13,
      "upvote_ratio": 0.84,
      "text": "I know this story is going to sound crazy, and I've been using AWS for years, and have never seen anything like this.\n\nSo, first -\nAWS billed me for resources I tried deleting from CLI. It was hundreds of dollars in resource usage. I found the resources still active when I looked at my account the following month seeing nearly $600 in billing on my account. I disputed the charges, but recognized that it would probably go nowhere.\n\nThen, the next month - I get billed AGAIN for the same resources I had actually deleted - and I look again, and they're still active.. so I'm scratching my head at this point. I also didnt have $1200 in my account so it's now delinquent. Then, the third month comes in, and another $600 bill, and I had actually just reached out to them about trying to get on a payment plan - yet, no response still. I tried setting up billing notifications to tell me when my resource limits hit $85 and $100 .. and I deleted everything but a micro instance.\n\nThen, it gets crazier. I put money in my bank account, and they immediately took $600 out of my account, and 2 days later I get an email stating that my account was suspended.\n\nSo, I'm like \"whatever\" at this point. I honestly am over it after getting screwed out of my account. Mind you, I've had an account with them for years, and managed many client accounts through AWS for years. I've never had any problems, and always had great customer support.\n\nSo.. I start getting emails stating that my account has gone over it's resource limits - and has hit an excess of $100 for the month. I'm panicking thinking WTF. I try to log in .. but my accounts suspended.. so, I reach out to AWS and they tell me - they won't discuss my account details unless I log in to my account. . Which is suspended?!\n\nSo, how is it that they can be continuing to bill me for resources I can't access. My endpoints are offline, so they can't be billing me for resources that aren't running? How can they be charging me for resources I'm not using, on an account I don't have access to?\n\nHow can I get them to resolve this, or to at least stop billing me monthly for resources I don't have access to?\n\nI'm sure I'm not without fault here, but am I crazy? This seems like absolutely insane business practices if they treat people like this regularly?",
      "is_original_content": false,
      "link_flair_text": "billing",
      "permalink": "https://reddit.com/r/aws/comments/1r53lyo/any_advice_on_billing/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5g3vd6",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'billing'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+billing).\n\nLooking for more information regarding billing, securing your account or anything related? [Check it out here!](https://www.reddit.com/r/aws/comments/vn4ebe/check_it_first_operating_within_amazon_web/)\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-15 02:57:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5g724d",
          "author": "nope_nope_nope_yep_",
          "text": "You obviously have not cleaned up everything or they won‚Äôt be charging you. If you don‚Äôt know what you‚Äôre doing the cloud can be an expensive lesson.",
          "score": 14,
          "created_utc": "2026-02-15 03:19:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5itjm8",
              "author": "GenderSuperior",
              "text": "Man I've been working on devops for years and never had this problem with aws with any clients, nor my own account.",
              "score": 0,
              "created_utc": "2026-02-15 15:42:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5gtpe1",
          "author": "Drumedor",
          "text": "It would be useful if you wrote what resource types you have deleted and are billed for. If it for example is EC2 instances and you have an ASG setup, then it would spin up more servers to replace the ones you deleted.",
          "score": 3,
          "created_utc": "2026-02-15 06:17:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5itfnv",
              "author": "GenderSuperior",
              "text": "Yeah it was ec2 but I didn't have asg set up",
              "score": 1,
              "created_utc": "2026-02-15 15:42:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5jfhnv",
                  "author": "Drumedor",
                  "text": "In that case you did something wrong when terminating the instances, or had instances in other regions than the one you were looking at.",
                  "score": 1,
                  "created_utc": "2026-02-15 17:29:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5g83qx",
          "author": "AWSSupport",
          "text": "I'm sorry to hear about the frustration this has caused. Even with a suspended account, you can sign in to create a support case for this issue here: https://go.aws/4tBuBFZ. Our Support team will be able to help with stopping the resources and associated charges. \\- Holly G.",
          "score": 4,
          "created_utc": "2026-02-15 03:27:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5its67",
              "author": "GenderSuperior",
              "text": "But I can't sign in. The resources were stopped and I'm still being charged, even after my account was suspended, and then closed.\n\nThis is mad work.",
              "score": 2,
              "created_utc": "2026-02-15 15:44:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ix1sz",
                  "author": "AWSSupport",
                  "text": "Is your account suspended or closed? If your account is closed, see this article on how to reopen your closed account: https://go.aws/4qBXYVW. You should have a case ID available if you received a response from Support. Share it with us via private chat, and we can look into this for you.\n\n\\- Marc O.",
                  "score": 1,
                  "created_utc": "2026-02-15 16:00:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5m9ypf",
              "author": "SilentPugz",
              "text": "Hi Holly , just want to say thank you for all the lessons you give on skillbuilder.",
              "score": 1,
              "created_utc": "2026-02-16 02:49:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5meom8",
          "author": "IridescentKoala",
          "text": "$100 AWS support is going to reply saying they already emailed you how to fix this. \n\nYou can log in with a suspended account. And what does \"tried to delete resources\" even mean here? You either did or didn't. And then you just waited a month without ever checking?",
          "score": 2,
          "created_utc": "2026-02-16 03:20:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r791ot",
      "title": "How to automate aws savings plans without manual quarterly analysis?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r791ot/how_to_automate_aws_savings_plans_without_manual/",
      "author": "My_Rhythm875",
      "created_utc": "2026-02-17 15:30:12",
      "score": 9,
      "num_comments": 18,
      "upvote_ratio": 0.91,
      "text": "Every quarter there's this ritual where you analyze usage patterns, try to predict future compute needs, calculate optimal savings plan coverage, submit recommendations to leadership, get approval, then finally buy commitments. By the time the whole process finishes usage has already changed and the analysis is outdated.\n\nCommitment recommendations in cost explorer are okay as a starting point but they don't account for upcoming projects, seasonal traffic patterns or planned architecture changes. They just look at historical usage and say \"buy this much\" which is often wrong.\n\nUnder committing means leaving savings on the table, over-committing means paying for capacity you don't use and the optimal middle ground requires constant adjustment. Three year commitments save more but lock you in longer which is risky for startups where everything changes constantly.\n\nCoverage percentage drops randomly when workloads shift and you need to evaluate whether to buy more which savings plan type makes sense (compute vs ec2) and what term length is appropriate. Feels like this should be automated somehow but I haven't found anything that actually works reliably\n\nIs there a good workflow for this or is manual quarterly analysis just the reality?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1r791ot/how_to_automate_aws_savings_plans_without_manual/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5wcfwj",
          "author": "SpecialistMode3131",
          "text": "If you pay for it, you can get another human being to do this for you (We do it).  But at the end of the day, unless someone who really cares is looking hard at your spend versus your business objectives, you cannot know if that money is being wisely spent.\n\nSo the only real decision is whether you're doing that in-house, paying someone for it out of house, or just choosing not to do it at all.  People do all three things, with varying degrees of competence, with the variance in results you'd expect.",
          "score": 4,
          "created_utc": "2026-02-17 17:27:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5whnw7",
          "author": "CharacterHand511",
          "text": "Three year commitments are scary for startups yeah, one-year terms are safer because who knows what infrastructure will look like in 2027",
          "score": 2,
          "created_utc": "2026-02-17 17:51:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wklc1",
          "author": "galiyonkegalib",
          "text": "cost explorer recommendations are too simplistic, they don't understand context like \"migrating to lambda next quarter\" or \"traffic doubles in q4 every year\"",
          "score": 2,
          "created_utc": "2026-02-17 18:05:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zk284",
          "author": "MateusKingston",
          "text": "If by the time you're buying the commitment the analysis is outdated either you're taking way too long (3+ months) or you're using SP the wrong way.\n\nIt's a commitment for 1/3 years, it can't get outdated in weeks, otherwise you would have made a wrongful commitment in the first place.\n\nOverall this isn't supposed to be an automated process as there is far too many external variables that no system will take into account and is a task that shouldn't realistically take too much time and isn't very frequent.\n\nYou can have tooling to assist you in that but I don't know of any (besides the built in aws console ones)",
          "score": 2,
          "created_utc": "2026-02-18 03:06:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w2np5",
          "author": "pausethelogic",
          "text": "There are some companies that will handle this for you. In general though, this is a manual process unless you want to automate it yourself. As you‚Äôve mentioned, there is a lot of real risk associated with the different options available",
          "score": 2,
          "created_utc": "2026-02-17 16:38:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w4qpr",
          "author": "BloodAndTsundere",
          "text": "As far as overcommitting goes, I think there is a secondary market for reserved instances which can mitigate some of that risk.",
          "score": 1,
          "created_utc": "2026-02-17 16:48:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5whzbt",
          "author": "Sea-Car8041",
          "text": "coverage dropping is annoying bc u have to investigate why, is it a good change (moved to cheaper instances) or bad change (accidentally lost coverage)... requires manual analysis either way",
          "score": 1,
          "created_utc": "2026-02-17 17:53:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wmmjr",
          "author": "lostsomewhere--",
          "text": "Automation would be nice but trusting a tool to automatically buy commitments feels risky without human review first like what if the algorithm makes a mistake and commits you to $50k of unnecessary capacity. Some tools like prosperops try to do this or there's vantage autopilot feature that handles it but I'd want to really understand the logic before letting it run unsupervised",
          "score": 1,
          "created_utc": "2026-02-17 18:14:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60zqpb",
              "author": "TeekhiSamosaa",
              "text": "yeah auto-buying without approval workflow is terrifying, needs at least a human-in-the-loop to review recommendations before executing",
              "score": 1,
              "created_utc": "2026-02-18 09:55:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o619ivt",
                  "author": "Vodka-_-Vodka",
                  "text": "Agreed, automation should help with analysis and maybe executing small incremental adjustments, but big commitment decisions need oversight",
                  "score": 1,
                  "created_utc": "2026-02-18 11:20:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o61j470",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-02-18 12:30:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wo86r",
          "author": "Connect_Street_867",
          "text": "Compute savings plans vs ec2 specific is another decision point, compute is more flexible but ec2 specific saves more... depends on whether you value flexibility or max savings",
          "score": 1,
          "created_utc": "2026-02-17 18:21:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6153xf",
              "author": "Unlucky_Abroad7440",
              "text": "compute makes sense when instance types change frequently, ec2 specific would lock you into old instance families",
              "score": 1,
              "created_utc": "2026-02-18 10:43:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o62asv8",
                  "author": "lunahanae",
                  "text": "Honestly this whole thing feels like something cloud providers should handle better natively like they have all the usage data why not just offer a \"smart commitment\" option that adjusts automatically",
                  "score": 1,
                  "created_utc": "2026-02-18 15:02:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5xhihu",
          "author": "TooMuchTaurine",
          "text": "Why quarterly, we just do it once a year, try to maintain about 95% coverage at peak to give us some buffer.",
          "score": 1,
          "created_utc": "2026-02-17 20:39:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r57wqh",
      "title": "any quick method or automation is available to delete iam roles that are unused ?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r57wqh/any_quick_method_or_automation_is_available_to/",
      "author": "Any_Animator4546",
      "created_utc": "2026-02-15 06:52:27",
      "score": 8,
      "num_comments": 18,
      "upvote_ratio": 0.79,
      "text": "For my better understanding I create a new IAM role every time I create a new service in AWS. I am still learning these access control permissions. I want to know if there is a quick automatic way in which I can delete the IAM roles that are no longer been used ?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1r57wqh/any_quick_method_or_automation_is_available_to/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5gzo9v",
          "author": "the_programmr",
          "text": "IaC is your friend here. If you use something with CDK/TF, you can delete all resources and associated IAM roles when you delete a stack.  \n\nIf the IAM roles you‚Äôre referring to here were created manually, would have to create a script using the AWS SDK to loop through roles and delete based on last usage time.",
          "score": 16,
          "created_utc": "2026-02-15 07:13:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5h0rbr",
              "author": "Any_Animator4546",
              "text": "thanks",
              "score": 2,
              "created_utc": "2026-02-15 07:23:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5jjfhf",
                  "author": "SpecialistMode3131",
                  "text": "Don't forget you can use IAC Generator to create the cloudformation stack, \\*then\\* blow away what you want gone.",
                  "score": 1,
                  "created_utc": "2026-02-15 17:48:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5hwjgm",
          "author": "weirdbrags",
          "text": "cloud custodian can help \n\nhttps://cloudcustodian.io/docs/aws/resources/iam.html",
          "score": 3,
          "created_utc": "2026-02-15 12:23:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ig0w0",
              "author": "pazarr",
              "text": "This is my preferred way too.",
              "score": 1,
              "created_utc": "2026-02-15 14:31:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hog6t",
          "author": "mrlikrsh",
          "text": "There is a last activity on the iam console for the role, not sure if you can get this programmatically",
          "score": 2,
          "created_utc": "2026-02-15 11:12:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hcp2k",
          "author": "pazarr",
          "text": "You can set up a cloud custodian. I quite like the tool.",
          "score": 1,
          "created_utc": "2026-02-15 09:19:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5he01n",
              "author": "Any_Animator4546",
              "text": "hi i am using a boto3 python script",
              "score": -2,
              "created_utc": "2026-02-15 09:32:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ii409",
                  "author": "pazarr",
                  "text": "If you don't want to use anything but boto, you can get last accessed information and than take action. \nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_last-accessed-view-data.html",
                  "score": 1,
                  "created_utc": "2026-02-15 14:43:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ndux5",
          "author": "ilyas-inthe-cloud",
          "text": "Check the \"Last activity\" column in the IAM console, it shows when each role was last used. For the ones showing 30+ days of inactivity you can safely nuke them. But honestly if you're learning, start using CloudFormation or CDK now. When you delete a stack it cleans up all the roles it created. Saves you from this exact problem going forward.",
          "score": 1,
          "created_utc": "2026-02-16 08:01:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hdfft",
          "author": "pint",
          "text": "no, because there is no such thing as \"used\" role. nobody knows if you have a script somewhere that uses that role. including you yourself, because what if you reused one of these auto-generated roles somewhere, and the forgot?\n\nroles have last access time. also, roles have a trust policy, which tells you where the role is allowed to be used. if only lambda is allowed for example, you know it is not used anywhere else.\n\nif you are not a programmer, you might get an ai chatbot to develop a script for you to make a list with these fields for review. trusting an ai to develop the deletion script is a little more fishy.\n\nif you are some of a programmer, or willing to take on the task, you can use any sdk (e.g. boto3) to do this programmatically.\n\na middle ground is ai developed listing script, followed by manual review, followed by an excel-generated list of aws cli commands in a cmd file (assuming windows).",
          "score": -1,
          "created_utc": "2026-02-15 09:26:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ihrbm",
              "author": "pazarr",
              "text": "In fact you can check the last time an IAM role has been accessed. https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_last-accessed-view-data.html\n\nThis can be easily accessible via aws api or cli.",
              "score": 2,
              "created_utc": "2026-02-15 14:41:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5je2ub",
                  "author": "pint",
                  "text": "that's what i said",
                  "score": 0,
                  "created_utc": "2026-02-15 17:22:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r593tt",
      "title": "How are the Nova 2 models for text processing",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r593tt/how_are_the_nova_2_models_for_text_processing/",
      "author": "2B-Pencil",
      "created_utc": "2026-02-15 08:05:50",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.9,
      "text": "I currently have a text processing workload that is using Gemini 3 Flash: summarization, keyword extraction, etc. Nothing fancy. But I do have to process the occasional 500k token document which is why I like Gemini. It does fairly well even with really big text  \n\n\n\nBut all my infra is on AWS and I have Activate credits for my project so I was strongly considering switching to Nova 2 models for cost savings. \n\n  \nwhat‚Äôs everyone‚Äòs experience with Nova 2 model family?",
      "is_original_content": false,
      "link_flair_text": "ai/ml",
      "permalink": "https://reddit.com/r/aws/comments/1r593tt/how_are_the_nova_2_models_for_text_processing/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5hdz7k",
          "author": "Sirwired",
          "text": "Every use case is different; while you could do something fancy with a whole system to score responses for comparison ( [https://aws.amazon.com/bedrock/evaluations/](https://aws.amazon.com/bedrock/evaluations/) ), you can just start by taking a couple of your trickier inputs and eyeballing the result.\n\nCertainly Nova 2 is pretty value-priced.",
          "score": 2,
          "created_utc": "2026-02-15 09:32:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r840au",
      "title": "Next AWS Associate after SAA? (Have Azure certs)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r840au/next_aws_associate_after_saa_have_azure_certs/",
      "author": "luffy6700",
      "created_utc": "2026-02-18 14:10:47",
      "score": 7,
      "num_comments": 12,
      "upvote_ratio": 0.89,
      "text": "Hey everyone,\nI have 9 months internship experience as a Multicloud Administrator and hold:\nMicrosoft Certified: Azure Administrator Associate\nMicrosoft Certified: Azure Security Engineer Associate\nMicrosoft Certified: Azure Solutions Architect Expert\nAWS Certified Solutions Architect ‚Äì Associate\nI have a 100% AWS voucher.\nWhich Associate cert should I take next to boost job opportunities",
      "is_original_content": false,
      "link_flair_text": "training/certification",
      "permalink": "https://reddit.com/r/aws/comments/1r840au/next_aws_associate_after_saa_have_azure_certs/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o638rhj",
          "author": "benpakal",
          "text": "CloudOps. This is 95% same as SAA + some multi org stuff",
          "score": 2,
          "created_utc": "2026-02-18 17:38:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o658bug",
              "author": "BloodAndTsundere",
              "text": "It‚Äôs that much an overlap? I thought Developer was the biggest overlap. I‚Äôm studying for SAA and Developer simultaneously right now but I‚Äôd consider swapping one out for or just adding CloudOps",
              "score": 1,
              "created_utc": "2026-02-18 23:07:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67rfuc",
                  "author": "benpakal",
                  "text": "I havent done Developer, but I have done SAA SAP DevOps and CloudOps. CloudOps was very similar to SAA imho",
                  "score": 2,
                  "created_utc": "2026-02-19 09:37:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o62lrmu",
          "author": "Ninjaivxx",
          "text": "I think you should do Developer",
          "score": 1,
          "created_utc": "2026-02-18 15:53:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62s79d",
          "author": "Willkuer__",
          "text": "I did Developer (which is basically the same as SAA) and DataEngineer, which is okish.\n\nI think the AI certs are also relevant for the current market and likely easy.\n\nI heard Devops is fun since it also has a partical component.\n\nMy next cert will hopefully be SA Professional.\n\nSo I would go for (in that order)\n1. AI stuff\n2. Data Engineer\n3. SA Professional\n4. Developer (just too much overlap with SAA)\n5. Devops",
          "score": 1,
          "created_utc": "2026-02-18 16:23:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64b2l0",
          "author": "alex_aws_solutions",
          "text": "If you're handling a Multiaccounts Environment or Organizations, I would to the CloudOps.",
          "score": 1,
          "created_utc": "2026-02-18 20:30:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66i2cw",
          "author": "Ok_Difficulty978",
          "text": "Nice lineup of certs already that‚Äôs solid.\n\nSince you already have SAA + strong Azure background, I‚Äôd probably go for AWS SysOps Admin Associate next. It fits well with multicloud/admin work and looks good for ops/cloud roles. Dev Associate is good too if you‚Äôre more into coding side.\n\nAlso depends what jobs you‚Äôre aiming for tbh. I usually check a few practice questions first before locking in (used sites like vmexam before, helped me see where I stand).\n\n[https://awscertexam.wordpress.com/2024/09/12/aws-sysops-administrator-jobs-pass-the-soa-c02-exam/](https://awscertexam.wordpress.com/2024/09/12/aws-sysops-administrator-jobs-pass-the-soa-c02-exam/)",
          "score": 1,
          "created_utc": "2026-02-19 03:29:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6jbie",
      "title": "How should i calculate IOPS for Aurora in AWS pricing calculator?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r6jbie/how_should_i_calculate_iops_for_aurora_in_aws/",
      "author": "xodmorfic",
      "created_utc": "2026-02-16 19:29:28",
      "score": 6,
      "num_comments": 15,
      "upvote_ratio": 0.81,
      "text": "I've spent over a week on this and I'm still unsure.\n\n  \nMy team wants to migrate from RDS MySQL to use Aurora (standard) for our database. I've tried to use the AWS pricing calculator to estimate the cost of the new DB, but i think i don't have thescsc right understanding of calculating the storage price for Aurora, and the estimations look way overpriced than expected.\n\n  \nI am replicating our current RDS MySQL setup with 800GB. Pricing calculator asks for \"Baseline I/O rate\" and \"Peak I/O rate\" for estimating the price of Aurora storage, but i am not sure of how to calculate those rates.\n\nThis is an example Total IOPS for test DB, from the metrics, for the last 1 day and a span period of 1 minute:\n\nhttps://preview.redd.it/e89erkt8rwjg1.png?width=567&format=png&auto=webp&s=3a2ede87a7535376607b848267ee9cc1cd04c981\n\n  \nIf i put those values of about 3.7k in the \"Baseline I/O rate\", i end up having a storage cost of about $2k which is a lot. \n\nOur current RDS MySQL database costs about $180 including storage (general purpose gp3). So i know that my input in those I/O fields in the AWS calculator might be wrong, but i don't know how then should i be calculating those values.\n\nhttps://preview.redd.it/mn0dz2yarwjg1.png?width=1700&format=png&auto=webp&s=0cd2675100aec5652be16f22bee1b99ce76b0c5e\n\n  \nHELP!",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1r6jbie/how_should_i_calculate_iops_for_aurora_in_aws/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5ru9yc",
          "author": "Psych76",
          "text": "Assume it will be one million billion and pay up haha this is one reason we had to back out of Aurora (serverless) - the io costs were impossible to predict and our dev instances were racking up fees so fast.",
          "score": 1,
          "created_utc": "2026-02-16 23:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ruyai",
              "author": "xodmorfic",
              "text": "I mean, our current RDS for testing only costs about $180/month and using aws calculator, for Aurora it jumps up to over $2k. That is ridiculous, but that is why i think my calculations for IOPS are wrong",
              "score": 2,
              "created_utc": "2026-02-16 23:28:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5sqqos",
                  "author": "Psych76",
                  "text": "The iops unknowns of Aurora were such an unknown for us, with zero temp files ever suddenly we‚Äôd need hundreds of spend daily just on iops but other days $10, it was too all over the map and unpredictable and we lacked a way to see what our ‚Äúfuture iops actuals‚Äù would be.",
                  "score": 1,
                  "created_utc": "2026-02-17 02:35:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5qkehs",
          "author": "Alternative-Theme885",
          "text": "To calculate IOPS for Aurora in the AWS pricing calculator, you need to understand that the baseline I/O rate is the average IOPS your database will use, and the peak I/O rate is the maximum IOPS your database will use. The fix is to monitor your current RDS MySQL instance using CloudWatch metrics, specifically the \"VolumeReadOps\" and \"VolumeWriteOps\" metrics, to get an accurate estimate of your IOPS usage. You can then use these metrics to estimate your baseline and peak I/O rates in the AWS pricing calculator, which should give you a more accurate cost estimate for your Aurora database.",
          "score": -1,
          "created_utc": "2026-02-16 19:34:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qoqag",
              "author": "TimGustafson",
              "text": "This is incorrect.  Aurora IOPS have nothing to do with EBS IOPs.  They're different things.  You can really only take an educated guess at what Aurora IOPs will look like for a given workload.  That's why AWS has IO Optimized: because this is so hard to figure out and regulate.\n\nThe only way to get accurate Aurora IOPs numbers is to measure them.\n\nSource: I was at AWS for 7+ years, working the last 4 as a Principal Database SA focusing on Aurora.",
              "score": 14,
              "created_utc": "2026-02-16 19:55:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5qr53v",
                  "author": "SpecialistMode3131",
                  "text": "Hence the usual \"take RDS and add 20%\" advice.",
                  "score": 7,
                  "created_utc": "2026-02-16 20:07:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5qwe4y",
                  "author": "Alternative-Theme885",
                  "text": "Good point, you're right that Aurora IOPS are separate from EBS. I oversimplified. For Aurora specifically, monitoring CloudWatch metrics like VolumeReadIOPs and VolumeWriteIOPs from an existing workload is probably the best way to estimate.",
                  "score": 2,
                  "created_utc": "2026-02-16 20:33:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5rem5n",
                  "author": "xodmorfic",
                  "text": "So, how can I really measure/estimate those \"Baseline I/O rate\" and \"Peak I/O rate\" for the calculator, if I don't have an Aurora DB yet but only our current RDS MySQL?\n\nI can rely on my current cloudwatch metrics, but if I use the values as i stated on the post/screenshot, the estimated cost just goes to the moon",
                  "score": 1,
                  "created_utc": "2026-02-16 22:02:36",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r60pah",
      "title": "Does the bulk api in OpenSearch Serverless has a limit ?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r60pah/does_the_bulk_api_in_opensearch_serverless_has_a/",
      "author": "Any_Animator4546",
      "created_utc": "2026-02-16 05:09:01",
      "score": 5,
      "num_comments": 12,
      "upvote_ratio": 0.78,
      "text": "My file has like 3000 documents, but only 700 gets synced and then it shows timeout",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1r60pah/does_the_bulk_api_in_opensearch_serverless_has_a/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5n3csa",
          "author": "luckVise",
          "text": "Yes.\n\nFrom my experience, OpenSearch, serverless or not it doesn't matter, bulk or single api calls doesn't matter, ingest documents without using an ingestion queue. \n\nWhen you send too much documents at once, it simply refuses to ingest them. The bulk api seems to don't return errors, but the single api does.\n\nYou must build your own ingestion queue and retry on errors.\n\nFor me this is non sense. A database (or search engine as someone clarified) of any kind should provide a way to insert data reliably.",
          "score": 2,
          "created_utc": "2026-02-16 06:26:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pk9uy",
              "author": "daredevil82",
              "text": "it is a mistake to think of search engines as a database",
              "score": 2,
              "created_utc": "2026-02-16 16:47:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5pp7kf",
                  "author": "luckVise",
                  "text": "Sorry, I labeled Opensearch as a database, it is a search engine. My bad.\n\nAnyway, for a search engine that is capable to search and aggregate through millions of documents, I expect at least a module or something already integrated in the cluster of nodes, to perform the insertion of many records, not to do it myself. \nBecause that is the first thing you need when you start working seriously with OpenSearch.",
                  "score": 1,
                  "created_utc": "2026-02-16 17:09:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5q7072",
              "author": "ItsHoney",
              "text": "Just today, I was trying to index documents from Snowflake to opensearch. I moved parquet files from Snowflake to S3 and buffered the messages into SQS. \n\nOut of 170k docs, only 130k got indexed. However, I didn't see any error messages pop up in the live tail logs. \n\nIs this one of those silent failure cases? Some of the single parquet files can have upto 100k records",
              "score": 1,
              "created_utc": "2026-02-16 18:32:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qgzeo",
          "author": "chrishrb",
          "text": "Opensearch serverless is pretty much useless. Expensive but doesn‚Äôt scale good. In the company I used to work before we tried to use it for ocpp logs but it was just shit. Dumped it for the non serverless version.",
          "score": 1,
          "created_utc": "2026-02-16 19:18:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qhaky",
              "author": "Any_Animator4546",
              "text": "thanks , I am just learning for an interview, I am mostly using serverless as it is easier to set up ",
              "score": 1,
              "created_utc": "2026-02-16 19:19:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vyz6e",
          "author": "TechDebtSommelier",
          "text": "Yes, 20MB per bulk request is the hard limit for OpenSearch Serverless, and it will just quietly stop there which is extremely fun to debug. Chunk your docs into batches of \\~500 and retry with backoff on 429s. The timeout you're seeing is almost definitely the OCU scaling to handle the spike, not the request itself.",
          "score": 1,
          "created_utc": "2026-02-17 16:19:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gx6ds",
              "author": "Useful-Process9033",
              "text": "Silent failures on bulk ingest are absolutely criminal API design. An indexing service that just quietly drops your documents with no error is worse than throwing a 500. At minimum they should return a partial success response with the count of what actually made it in.",
              "score": 1,
              "created_utc": "2026-02-20 18:45:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}