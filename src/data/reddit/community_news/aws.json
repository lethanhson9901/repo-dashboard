{
  "metadata": {
    "last_updated": "2026-01-23 08:49:57",
    "time_filter": "week",
    "subreddit": "aws",
    "total_items": 20,
    "total_comments": 111,
    "file_size_bytes": 152818
  },
  "items": [
    {
      "id": "1qjr0s7",
      "title": "ECR finally supports layer sharing",
      "subreddit": "aws",
      "url": "https://aws.amazon.com/about-aws/whats-new/2026/01/amazon-ecr-cross-repository-layer-sharing/",
      "author": "waitingforcracks",
      "created_utc": "2026-01-22 10:24:53",
      "score": 62,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "containers",
      "permalink": "https://reddit.com/r/aws/comments/1qjr0s7/ecr_finally_supports_layer_sharing/",
      "domain": "aws.amazon.com",
      "is_self": false,
      "comments": [
        {
          "id": "o13jwjl",
          "author": "TechDebtSommelier",
          "text": "TLDR: ECR can now reuse identical image layers across different repos, so pushes are faster and you stop paying to store the same base image over and over. Turn it on once at the registry level and it just works, which is especially nice if you have lots of microservices built on the same images.",
          "score": 20,
          "created_utc": "2026-01-22 19:05:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11g115",
          "author": "aviboy2006",
          "text": "Wow this great addition. Love it.",
          "score": 6,
          "created_utc": "2026-01-22 13:05:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16odyq",
          "author": "wwsean08",
          "text": "As a note this only works for your repositories that are using the same KMS key, if they are using different KMS keys, then the layers won't be shared, which should be expected but at least wasn't initially called out when i read about it earlier this week and asked my TAM to confirm.",
          "score": 4,
          "created_utc": "2026-01-23 04:59:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13l0os",
          "author": "waitingforcracks",
          "text": "Now just waiting for terraform to support the setting so we can enable it.",
          "score": 6,
          "created_utc": "2026-01-22 19:10:26",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhutpm",
      "title": "If a person spends a billion dollars and buys all the compute on EC2 for today, what happens to the rest of the people requesting it?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qhutpm/if_a_person_spends_a_billion_dollars_and_buys_all/",
      "author": "PrestigiousZombie531",
      "created_utc": "2026-01-20 07:40:30",
      "score": 38,
      "num_comments": 70,
      "upvote_ratio": 0.76,
      "text": "- Just an honest question / showerthought, whatever you want to call it",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1qhutpm/if_a_person_spends_a_billion_dollars_and_buys_all/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0mpzwk",
          "author": "Murky-Sector",
          "text": "Amazon would still limit your capacity. They would looove to setup all kinds of dedicated capacity just for you though, which you could use with wild abandon. For a price of course.",
          "score": 110,
          "created_utc": "2026-01-20 07:46:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mscs4",
              "author": "katatondzsentri",
              "text": "This. We actually use \"aws assisted capacity reservations\" which basically means they deployed a few racks with servers in the AZ we needed it.",
              "score": 31,
              "created_utc": "2026-01-20 08:07:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0rgst7",
                  "author": "x86brandon",
                  "text": "Those aren't even deploying racks, that's just earmarking things in the scheduler.   Their large customers have earmarked fleet allocations and don't run into the same capacity problems as most.  And when you get big enough, you stop fussing with reservations, etc and just get a fixed EDP discount across all services and you generally will never see an unavailable error.  From experience anyways.",
                  "score": 12,
                  "created_utc": "2026-01-20 23:49:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0rgcuz",
              "author": "x86brandon",
              "text": "You don't need dedicated capacity.   Amazon simply limits spend in incremental amounts as a credit risk management.  Brand new customer isn't going to spend $10 million in their first month without a conversation with finance, conversion to terms, etc.",
              "score": 6,
              "created_utc": "2026-01-20 23:47:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0rlypo",
                  "author": "Murky-Sector",
                  "text": ">Brand new customer isn't going to spend $10 million in their first month without a conversation with finance, conversion to terms, etc.\n\nI think its hysterical youre taking such great pains to analyze a hypothetical question about spending $1,000,000,000 on EC2. But ok.",
                  "score": -8,
                  "created_utc": "2026-01-21 00:17:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mpmrf",
          "author": "multidollar",
          "text": "Insufficient Capacity, please try your request again using another availability zone or try your request again shortly.",
          "score": 86,
          "created_utc": "2026-01-20 07:42:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0oikm4",
              "author": "yarenSC",
              "text": "This would only be if somehow the account had gotten infinite quotas approved, which wouldn't be the case",
              "score": 11,
              "created_utc": "2026-01-20 15:28:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0rmn5e",
                  "author": "multidollar",
                  "text": "Oh, but it often is :)",
                  "score": 1,
                  "created_utc": "2026-01-21 00:21:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0rftzz",
                  "author": "x86brandon",
                  "text": "Depends on the relationship, I have no limits on my accounts, but that's a 10 digit account spend.",
                  "score": -1,
                  "created_utc": "2026-01-20 23:44:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0sjhbi",
              "author": "mr_jim_lahey",
              "text": "Kind of moot given account limits/quotas, and that the question is more generally about \"all the compute\", but I think it's possible AWS could provision a billion dollars' worth of on-demand instances in a single (major) region, assuming they were somewhat spread across instance types. Certainly they could accommodate it easily if spread across multiple regions. They have an unfathomable amount of capacity. $1B is like 1% of their ARR.",
              "score": 1,
              "created_utc": "2026-01-21 03:28:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mqkhb",
          "author": "casce",
          "text": "It happens occasionally that AWS temporarily runs out of specific instance type in specific regions.\n\nWhat happens is you can't deploy new ones in that case but your running stuff is fine. Just don't stop and start it or you might be unable to start it again.\n\nThe same would happen if someone literally requested all of AWS' instances without AWS stopping them. AWS would most certainly stop them though (wouldn't be worth it to anger ALL other customers).",
          "score": 14,
          "created_utc": "2026-01-20 07:51:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0w28xt",
              "author": "look_of_centipede",
              "text": "If you need to stop/start it, a brief capacity reservation can prevent someone from sniping it out from under you.",
              "score": 1,
              "created_utc": "2026-01-21 17:31:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mq6ph",
          "author": "No_Pomegranate7508",
          "text": "Reminds me of another similar question: \"Would you still love me if I were a worm?\"",
          "score": 26,
          "created_utc": "2026-01-20 07:47:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mpp3u",
          "author": "soundman32",
          "text": "The people who really need have paid for reserved instances for years ahead.",
          "score": 9,
          "created_utc": "2026-01-20 07:43:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mq8ke",
              "author": "Tall-Reporter7627",
              "text": "Like....if they bought a Dell server and stuffed it in their own rack, but at twice the cost?",
              "score": 5,
              "created_utc": "2026-01-20 07:48:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0mqxsu",
                  "author": "Loko8765",
                  "text": "No. AWS allows you to reserve instances in advance. You actually pay a lower price, but of course you commit to the duration. If you know you will use it, it is win-win.",
                  "score": 8,
                  "created_utc": "2026-01-20 07:54:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0u47g2",
                  "author": "SlinkyAvenger",
                  "text": "More like two ISPs, two gateways, four switches, four firewalls, eight servers, two power connections, and two generators",
                  "score": 2,
                  "created_utc": "2026-01-21 11:14:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0msv40",
                  "author": "mba_pmt_throwaway",
                  "text": "2x the cost of a server isn’t bad, actually. Just the ops overhead maintaining the servers + DC costs could cost more than the raw server costs. Ofc at a certain scale the math tips back in favor of self managing, but for most customers 2x would be great value.",
                  "score": 3,
                  "created_utc": "2026-01-20 08:12:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0nc08x",
              "author": "pint",
              "text": "yes, because, you know, there are no use cases where you need ephemeral instances. none.",
              "score": 1,
              "created_utc": "2026-01-20 11:09:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0rh9ji",
              "author": "x86brandon",
              "text": "However, their billion dollar customers just get a flat discount and don't deal with this stuff at least.  EDP ends up being a revenue commitment and a flat discount off list without dealing with the micro managing of instance types, etc.",
              "score": 1,
              "created_utc": "2026-01-20 23:52:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0w31oe",
              "author": "look_of_centipede",
              "text": "Capacity reservations generally don't tie to long term cost, cost reservations generally don't reserve capacity.  There are edge cases but they're rare.",
              "score": 1,
              "created_utc": "2026-01-21 17:34:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mqm02",
          "author": "vacri",
          "text": "System isn't set up for putting on that much compute at once - you'll run into Limits, and you're not going to be able to set them particularly high without satisfying AWS\n\nAssuming that it was all prepped beforehand and they could buy up the compute power and pay double the going rate to make it worth AWS's time, AWS still wouldn't do it because the damage to the brand by taking everyone's servers off them for 1 day would have many substantial customers fleeing to other vendors.",
          "score": 9,
          "created_utc": "2026-01-20 07:51:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0rh2g1",
              "author": "x86brandon",
              "text": "System is set up for a lot of load.   Credit limits are not.  :)\n\nI have launched groups of 5,000 p4.24xd's at a time.",
              "score": 2,
              "created_utc": "2026-01-20 23:51:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0sqi2q",
                  "author": "Alborak2",
                  "text": "Ive probably seen the downsteam effects of that launch lol. ML workloads move sooooo much data around.\n\nAnd yeah your other comments about limits are right. If you have the keys to the castle we'll pretty much let you run until the physical capacity is gone. Not many of those around. Our internal test accounts are unlimited and will launch thousands of instances with 20+ EBS volumes on each.",
                  "score": 1,
                  "created_utc": "2026-01-21 04:12:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mt5c1",
          "author": "FlyingFalafelMonster",
          "text": "Insufficient capacity. Happens already for GPU instances. Unless you buy capacity reservations, then even if you do not use instance it still is reserved for you. ",
          "score": 7,
          "created_utc": "2026-01-20 08:14:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0rizzh",
              "author": "x86brandon",
              "text": "For what it's worth, east-1 is the only place I see GPU issues...  the other regions I haven't had problems with since 2021.  west-2 I have very high launch rate success.",
              "score": 1,
              "created_utc": "2026-01-21 00:01:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0vd1go",
                  "author": "nagyz_",
                  "text": "I haven't been able to reserve the p6 gb200. Does it show as reservable for you as in you get an actual time slot with a price?",
                  "score": 1,
                  "created_utc": "2026-01-21 15:38:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o10gn1t",
                  "author": "CategoryRepulsive699",
                  "text": "Try Australia",
                  "score": 1,
                  "created_utc": "2026-01-22 08:14:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0qlnfz",
          "author": "ComplianceAuditor",
          "text": "You can't do that even with a billion dollars, because it would cause a lot more than a billion dollars in \"damage\" to their other customers if they suddenly have no capacity.\n\nIt's also not possible. There is no scenario ever, where AWS takes away an on demand instance from a customer just because another customer wants to use it.\n\nIdk why this is being downvoted though. Guess being curious is off limits for the internet.",
          "score": 5,
          "created_utc": "2026-01-20 21:13:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n1m87",
          "author": "TekintetesUr",
          "text": "Unfortunately nothing, doesn't matter how hard we try. In-use on-demand capacity will not be taken away from current users. Existing reservations won't be cancelled by a higher bid. Etc.",
          "score": 5,
          "created_utc": "2026-01-20 09:34:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rg41y",
          "author": "x86brandon",
          "text": "AWS has an internal allocation mapping.  A single customer and account can't hit the entire fleet.  And also, a billion dollars at retail prices would not actually consume the entire fleet.   Their fleet is probably much larger than you might imagine.",
          "score": 3,
          "created_utc": "2026-01-20 23:46:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0p4pfm",
          "author": "Quinnypig",
          "text": "The more interesting version of this question revolves around spending that billion dollars in S3 storage. There are no known quotas around that.",
          "score": 5,
          "created_utc": "2026-01-20 17:11:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0muov0",
          "author": "danizumi",
          "text": "In the AWS management console, look at the Service Quotas page, each service has hard and soft limits for each service. You should definitely be looking at these limits before putting a service into a production environment to ensure you don’t hit a limit, or at least be aware of the limits.",
          "score": 2,
          "created_utc": "2026-01-20 08:29:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nkrre",
          "author": "KayeYess",
          "text": "Quotas and other limitations will kick in long before a single customer can purchase and use even a small fraction of all the available resources.",
          "score": 2,
          "created_utc": "2026-01-20 12:18:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0s2own",
          "author": "Sowhataboutthisthing",
          "text": "Nice try, Elon.",
          "score": 2,
          "created_utc": "2026-01-21 01:51:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0s4cjx",
          "author": "Human-Job2104",
          "text": "Service Quotas will stop them.",
          "score": 2,
          "created_utc": "2026-01-21 02:01:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0vsr0s",
          "author": "FinOps_4ever",
          "text": "All of the people who purchased ODCR's to properly cover their EC2 needs will all get substantial bonuses.\n\nBut the proper answer is account limits and usage quotas.",
          "score": 2,
          "created_utc": "2026-01-21 16:48:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0thzu2",
          "author": "oscarolim",
          "text": "Billion dollars? That’s like 5 instances for a week.",
          "score": 2,
          "created_utc": "2026-01-21 07:48:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mts6b",
          "author": "Complex86",
          "text": "This is impossible. Each family is not equal and it would be honestly really difficult for 1 person to consume 100% of instances across all zones in all regions",
          "score": 1,
          "created_utc": "2026-01-20 08:20:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0rhg2n",
              "author": "x86brandon",
              "text": "Not to mention, a billion dollars probably wouldn't get you there.   :)",
              "score": 4,
              "created_utc": "2026-01-20 23:53:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0mwzk3",
          "author": "Soccer_Vader",
          "text": "If you are consuming all of the instances that aws has available at any given point, and not reserved for any internal/external use case, you are spending some developing countries gdp in a day. They would love that tho.",
          "score": 1,
          "created_utc": "2026-01-20 08:50:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0oinsm",
          "author": "plaaam",
          "text": "What type of question is that :skull:",
          "score": 1,
          "created_utc": "2026-01-20 15:28:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0p0n8q",
          "author": "SpecialistMode3131",
          "text": "You can't buy it all precisely because that would be bad for Amazon's overall business, so they have limits across all services.",
          "score": 1,
          "created_utc": "2026-01-20 16:52:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ro677",
          "author": "Top-Shopping410",
          "text": "Keep getting out of capacity I guess. I had this issue when I worked for another cloud provider and we just waited for the hardware installed",
          "score": 1,
          "created_utc": "2026-01-21 00:29:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rpzjg",
          "author": "Storage-Proper",
          "text": "They would continue to sell it",
          "score": 1,
          "created_utc": "2026-01-21 00:39:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0t3yg1",
          "author": "Burekitas",
          "text": "He won't be able to do that, he will need so much accounts and quota increase requests, it will take 20 years to get to that position. \n\n  \nBut if he did, people asking for EC2 will encounter issues, but in a couple of weeks AWS will fill the gap.",
          "score": 1,
          "created_utc": "2026-01-21 05:47:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tg5t0",
          "author": "suur-siil",
          "text": "They have the official quotas on your account, and all kinds of other hidden limits too.\n\nI used to have a quota of 20k across various instance-types in a fairly small region and ended up having interesting chats with AWS engineers after trying to spin up large jobs.\n\nThere's also reserved capacity for those who think ahead.",
          "score": 1,
          "created_utc": "2026-01-21 07:31:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tme95",
          "author": "alapha23",
          "text": "This happens all the time for c8g in where binance is running. And gpus as well.",
          "score": 1,
          "created_utc": "2026-01-21 08:29:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0yd3yn",
          "author": "Expensive-Virus3594",
          "text": "Rest of the people will get insufficient capacity errors. This will trigger a severity 2 incident of not a sev 1. \n\nIn reality you can’t make a huge allocation due to account level caps etc.  \n\nThis issue happens in AWS time to time  when something internally breaks.",
          "score": 1,
          "created_utc": "2026-01-21 23:57:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0znyge",
          "author": "plinkoplonka",
          "text": "Nothing. It's limited by account quota.",
          "score": 1,
          "created_utc": "2026-01-22 04:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10h936",
          "author": "CategoryRepulsive699",
          "text": "There are various limits including how much you can procure per second. But I do remember moving sliders in the internal UI that caused truckloads of equipment delivered into the datacenters...",
          "score": 1,
          "created_utc": "2026-01-22 08:19:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12h6t3",
          "author": "PeteTinNY",
          "text": "People get ICE’d all the time.   Insufficient Capacity Error.   \n\nI helped a major broadcast network that wanted wanted to move their TV content from onprem to the cloud and we needed gpu instances.   Unfortunately there wasn’t enough capacity and we had to work with the EC2 team for months to get things sorted.",
          "score": 1,
          "created_utc": "2026-01-22 16:12:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mqct9",
          "author": "lasthunter657",
          "text": "You cant buy all of them AWS have limit on how many EC2 you can have at the same time",
          "score": 1,
          "created_utc": "2026-01-20 07:49:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qiehod",
      "title": "How are you segregating AWS IAM Identity Center (SSO) permission sets at scale?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qiehod/how_are_you_segregating_aws_iam_identity_center/",
      "author": "sajed8950",
      "created_utc": "2026-01-20 21:41:12",
      "score": 18,
      "num_comments": 12,
      "upvote_ratio": 0.96,
      "text": "Hello everyone,\n\nI am looking for guidance on how organizations design and manage AWS IAM Identity Center (SSO) permission sets at scale.\n\n**Context**  \nOur AWS permission sets are mapped to AD/Okta groups. Some groups are team-based and have access to multiple AWS accounts. Team membership changes frequently, and we also have users who work across multiple teams.\n\nBecause access is granted at the group level, we often run into situations where access requested for one individual results in broader access for others in the same group who didn’t need or ask for it.\n\nWe also receive a high volume of access change requests. While we try to enforce least privilege, we’re struggling to balance that with operational overhead and permission set sprawl.\n\n**Discussion points**\n\n* How do you structure permission sets and groups to scale without constant rework?\n* Do you use team-based, job-based, or hybrid permission sets?\n* Do you create separate groups per account + team + job role, or use a different model?\n* Do you provide birthright access for engineers? If so:\n   * What does that access look like?\n   * Is it different in sandbox vs non-prod vs prod?\n* How do you determine what access a team actually needs, especially when users don’t know what permissions they require?\n* How do you manage temporary access to a permission set? Do you use cyberark sca?\n* Who approves access to permission set groups (manager, app owner, platform, security, etc.)?\n\nAny real-world patterns, lessons learned, or “what not to do” stories would be appreciated.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1qiehod/how_are_you_segregating_aws_iam_identity_center/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0quz2h",
          "author": "tlf01111",
          "text": "I do Identity Center at very large scale (15,000 team members,  5000+ groups, 400 aws accounts, etc.)\n\nUsing IdC in conjunction with standard fare static RBAC IAM permissions & group assignments will result in permission set sprawl at scale.   It just is what it is.     \n  \nWe use set of enterprise-wide console roles that are used for most daily access needs, but also provide account-specific permission sets which engineering teams can use to fine-grain their access if it is needed over above the standard roles.\n\nWe had to build quite a bit of custom code to automate a lot of this (based on CloudTrail events coming in from identity store), but a few years in it's working pretty well.\n\nWe also use customer managed policy references heavily to build more granularity:  With some planning you can grant differing permissions using the same permission set, depending on the target (useful for environment tiers in our case).  \n\nBut it could be better.   The next big milestone is to slowly move to an ABAC model where feasible, which should (in theory) help cut down the number of permission sets needed. \n\nAnyway if you have any specific questions feel free to ask.  I'll try to answer best I can.",
          "score": 8,
          "created_utc": "2026-01-20 21:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ub54i",
              "author": "Lazy-Bicycle-8504",
              "text": " When you say \"we\" do you talk about a whole team of x people just doing this as their daily work? Given your scale this sound like at least 1-2 seniors/architects and 3-5 juniors, is this correct?",
              "score": 2,
              "created_utc": "2026-01-21 12:08:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0y13ak",
                  "author": "tlf01111",
                  "text": "Team of three architect-level, we me doing most of the lifting, and my peer assisting.\n\n\nI have to stress we have extensive automation which handles nearly 100% of the day to day.   That automation was a six month build for this team.",
                  "score": 2,
                  "created_utc": "2026-01-21 22:53:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0v7ccb",
                  "author": "AWS_Chaos",
                  "text": "This is what I came to ask. And notice it took the team \"a few years\". Meaning at scale, this isn't a one man job. the move to ABAC must be daunting.",
                  "score": 1,
                  "created_utc": "2026-01-21 15:11:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0qvhqr",
          "author": "bailantilles",
          "text": "We tend to use permission sets that are mapped to job role per account. Each job role and account is mapped to an AD group. Each AWS account has an owner, backup owner, and support group. It’s up to one of those constituencies to tell us what permissions each job role requires and who does each role. When the question arises of “this person needs different access than others in their job role” we as them the question: Do we need to create a new job role for this person or do all the others in that group get the additional permission. In the end, it’s up to them to keep track of all of that as they need to do user access audits quarterly for their application.",
          "score": 4,
          "created_utc": "2026-01-20 21:58:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vuyca",
              "author": "sajed8950",
              "text": "How many groups and accounts do you currently manage?",
              "score": 1,
              "created_utc": "2026-01-21 16:58:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qxw34",
          "author": "Healthy_Gap_5986",
          "text": "It's all about defining an RBAC structure and having the discipline to stick to it. Define roles, determine what they need to do in what environments, create PermissionsSets for them then rolling them out. The roles should be enforcing your operating model. e.g.\n\nExamples.\n\nDeveloperSet\n\n* gets almost FullAccess in Dev environments only.\n* SCP whitelist only grants everyone access to specific services.\n* PermissionsBoundary prevents them from self escalating.\n\nTesterSet\n\n* Can read logs, maybe set some data sources.\n* Applies to Dev and UAT.\n\nDevOpsSet\n\n* Access to Prod. Can read logs, manage Support tickets etc.\n* No write access.\n\nAdminSet\n\n* AD group is empty and only used through an audting temporary elevation process.\n* * Prod.\n\nPlatformAdmins\n\n* Gods. Like domain admins. \n* This is you, and you don't get involved in operations. :)\n\nThe same User can be in one or more of the above roles. e.g. Developers and Testers because they are performing those roles. If someone asks for a permission that's not included in the roles above then they are often asking to circumvent the operating model, either because the model is deficient, in which case it needs fixed, or they are just cowboys. (e.g. I want to get into UAT to clickops something because our testing isn't complete and product owner is all over me).\n\nOn defining privileges. YOu tailor it to allow them to get \"quality\" work done quickly. So developers can do a lot in the dev environment, but after that we want the model encourage the maturity of the CICD. So no clickops after Dev environment. It should all be driven by CICD with testing etc all automated.",
          "score": 3,
          "created_utc": "2026-01-20 22:10:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vuzgo",
              "author": "sajed8950",
              "text": "How many groups and accounts do you currently manage?",
              "score": 1,
              "created_utc": "2026-01-21 16:58:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0zv5ky",
                  "author": "Healthy_Gap_5986",
                  "text": "Tiny atm. 10 workload accounts + LZA platform accounts. Last place had 100+ accounts, pretty much the same model.",
                  "score": 1,
                  "created_utc": "2026-01-22 05:15:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qhotxo",
      "title": "Infrastructure as Software: Beyond Infrastructure as Code",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qhotxo/infrastructure_as_software_beyond_infrastructure/",
      "author": "whudduptho",
      "created_utc": "2026-01-20 02:36:43",
      "score": 17,
      "num_comments": 16,
      "upvote_ratio": 0.63,
      "text": "I've been working on a topic over the last 4 years: building out infrastructure using AWS CDK through an SRE lens.\n\nBeing in the DevOps, SRE, and Platform Engineering domains, I kept asking myself why aren't all the key NFRs built into the constructs we use as golden paths? Focused on reliability and developer experience, I put together a construct library where services have cost-savings, reliability, security, and scalability baked in from the start.\n\nThis is where I want to introduce a phrase I'm calling Infrastructure as Software. The idea is that these constructs, with minimal input, can be stitched together to build fault-tolerant systems. I built this site as a forcing function to showcase what I've been working on, but more importantly it's how an SRE approaches building self-healing infrastructure.\n\nThere's still more to this project, but for now I want to introduce the philosophy of Infrastructure as Software as I continue to illustrate how these constructs work together to build autonomous systems.\n\nWould love to get the community’s input. \n\n[https://github.com/crmagz/cdk-constructs-library](https://github.com/crmagz/cdk-constructs-library)\n\n[https://thepractitioner.cloud/blog/infrastructure-as-software](https://thepractitioner.cloud/blog/infrastructure-as-software)\n\n[https://thepractitioner.cloud/guides/infrastructure-as-software/introduction](https://thepractitioner.cloud/guides/infrastructure-as-software/introduction)\n\n",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1qhotxo/infrastructure_as_software_beyond_infrastructure/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0lppjt",
          "author": "lost12487",
          "text": "This looks like a vibe-coded [SST ](https://sst.dev/docs/examples)with your opinion of the \"golden path\" baked in. It sounds like you generally have good ideas about the topic, but there's just no way I'm letting anything with AI-generated everything anywhere near my critical infrastructure.",
          "score": 38,
          "created_utc": "2026-01-20 03:26:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0lv0i0",
              "author": "whudduptho",
              "text": "I believe you are referring to the site [https://thepractitioner.cloud](https://thepractitioner.cloud/blog/infrastructure-as-software). It uses [https://vite.dev/](https://vite.dev/). And no not vibe coded. Some of us actually know how to write software and use AI as the tool it is.\n\nThe project I'm discussing is a culmination of years of multi-region active-active systems building. If you know infra and can read code then taking a look at the constructs [https://github.com/crmagz/cdk-constructs-library/tree/main/packages](https://github.com/crmagz/cdk-constructs-library/tree/main/packages) shouldn't be an issue for you, but I don't believe you have.\n\nWhat this project offers is the same as Construct Hub but centralized, opinionated from an SRE/PE focus, and with minimal inputs needed.",
              "score": -16,
              "created_utc": "2026-01-20 03:56:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0lxtyl",
                  "author": "lost12487",
                  "text": "I'm not referring to your blog site, I'm referring to your obviously AI-generated source code.",
                  "score": 24,
                  "created_utc": "2026-01-20 04:13:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0pft5h",
              "author": "o5mfiHTNsH748KVq",
              "text": "> but there's just no way I'm letting anything with AI-generated everything anywhere near my critical infrastructure.\n\nwhy? you could always just read the code and make sure it does what you think it does. seems like unnecessary effort.",
              "score": -3,
              "created_utc": "2026-01-20 18:02:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0pkdi1",
                  "author": "lost12487",
                  "text": "Because if you don't even put in the effort to remove the completely superfluous comments that the agent adds to functions then I'm not going to put in the effort to find out where else you were lazy.",
                  "score": 8,
                  "created_utc": "2026-01-20 18:22:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0mdcaj",
          "author": "vincentdesmet",
          "text": "this topic would do much better on https://cdk.dev community channels \n\nSpecifically collaboration with OpenConstructs foundation may be interesting for you\n\nI’m still stuck enabling TF teams to adopt L2, moving to L3 afterwards (my project is terraconstructs.dev and I am one of core maintainers for http://cdktn.io - the CDKTF fork)",
          "score": 8,
          "created_utc": "2026-01-20 05:59:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0lsrns",
          "author": "behusbwj",
          "text": "Haven’t looked at the code, but the concept is solid and this is how the big players use CDK internally. The reason you don’t see libraries often is because the observability tends to be not worth abstracting when the whole company does it one or two ways.",
          "score": 3,
          "created_utc": "2026-01-20 03:43:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0m4ifz",
              "author": "whudduptho",
              "text": "Thanks for the feedback. I have a few nice abstractions on the roadmap that really capture the IaS philosophy of building self-healing multi-region infra. Feel free to leave any additional feedback if you get a chance to read/use the constructs. ",
              "score": 1,
              "created_utc": "2026-01-20 04:55:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0lm2bc",
          "author": "o5mfiHTNsH748KVq",
          "text": "I like that you included skills for the repo!",
          "score": -2,
          "created_utc": "2026-01-20 03:06:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0m4ujp",
              "author": "whudduptho",
              "text": "Yes, force multiplier for sure. I’ll likely create a repo for some of these soon across TS/Go/Python and GitOps tooling. ",
              "score": -6,
              "created_utc": "2026-01-20 04:57:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qgshy7",
      "title": "What is the value proposition of AWS MCP server?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qgshy7/what_is_the_value_proposition_of_aws_mcp_server/",
      "author": "Dry_Raspberry4514",
      "created_utc": "2026-01-19 03:00:43",
      "score": 12,
      "num_comments": 25,
      "upvote_ratio": 0.78,
      "text": "One of the tools (aws\\_\\_\\_call\\_aws) in AWS MCP server (confusing name, should have been called AWS Core MCP Server) simply takes same input as aws cli. Most the people using aws will have cli installed already and so if an MCP client has the cli command matching a prompt then it can simply invoke cli to get the job done. What is the advantage of using this tool over cli?\n\nMatching a prompt to corresponding cli command or input for aws query APIs is the main (and toughest) problem and most LLMs stuggle with it because their training data is old and web search tools used by these LLMs are not that effective.\n\nIdeally this tool should have accepted the prompt as input, use documentation search tool internally to find matching command and then return the result after executing the command.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1qgshy7/what_is_the_value_proposition_of_aws_mcp_server/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0f2ghv",
          "author": "[deleted]",
          "text": "Reason: Hype\n\nBoard: \"Large companies are providing access via MCP, we need to do the same.\"",
          "score": 18,
          "created_utc": "2026-01-19 04:00:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0he0fs",
              "author": "Xerxero",
              "text": "What does it add? \nI can feed the AWS documentation to Claude and let it run api calls.",
              "score": 1,
              "created_utc": "2026-01-19 14:42:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0g4fre",
          "author": "Sirwired",
          "text": "There are more uses for MCP servers than AI Coding tools; there are a ton of AI agents that are not going to have bash access.  (And the users might not have AWS permissions at all; you can assume a role with your agent and set permissions that way.)",
          "score": 8,
          "created_utc": "2026-01-19 09:04:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0gan5t",
              "author": "HiCookieJack",
              "text": "except you give them a bash MCP :D",
              "score": 3,
              "created_utc": "2026-01-19 10:03:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0gtq6i",
                  "author": "Sirwired",
                  "text": "I know you are kidding, but for those not getting the joke: AI agents that run on the web don't necessarily run in anything that has a shell prompt on it.",
                  "score": 2,
                  "created_utc": "2026-01-19 12:44:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0g4e9y",
          "author": "runitzerotimes",
          "text": "“CLI can handle everything, why do we need API?”\n\n“REST API can handle everything, why do we need SDK?”\n\n“Scripts can handle everything, why do we need IAC?”\n\nEvery single technology is just a different version of what came before it with QOL changes. Don’t be the wanker who stands there complaining like a dinosaur about new tech “when the old way works fine”.",
          "score": 14,
          "created_utc": "2026-01-19 09:03:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0fhsjf",
          "author": "Zenin",
          "text": "One possible reason I could think of is it seems difficult for some systems (such as Q / kiro) to safely trust specific CLI commands.  Meaning if I trust \"aws\" I'm also trusting \"rm\" because the actual tool is \"bash\".  So even if I have my aws profiles configured for safety, I still can't safely trust my agent to use it because AWS isn't actually the tool it's using, bash is.\n\nClaude Code IIRC is better about its tools having more nuance here so you can trust \"curl\" without trusting all possible CLI commands.\n\nIn these environments it may be easier to control trust with an MCP server than can be done with CLI commands.",
          "score": 3,
          "created_utc": "2026-01-19 05:47:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0h63ql",
              "author": "ObjectiveAide9552",
              "text": "Redditors: llm suck, you can’t trust them to do anything right. Also Redditors: why don’t you just give llm’s full access to your cli?",
              "score": 4,
              "created_utc": "2026-01-19 14:00:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0fpvgy",
          "author": "VIDGuide",
          "text": "Could it be to do with permissions? The CLi needs a IAM key assigned one way or another, effectively, how is authentication to the MCP handled? I haven’t looked into it, it’s probably similar or IAM based anyway, but curious if it could abstract away from the IAM key completely perhaps?",
          "score": 1,
          "created_utc": "2026-01-19 06:52:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0gvzha",
              "author": "Sirwired",
              "text": "Bingo.  Assume a role with your agent service, and you don't need to hand out IAM permissions to your users at all.",
              "score": 1,
              "created_utc": "2026-01-19 12:59:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0lhjv8",
          "author": "jed_l",
          "text": "It’s a checkbox tbh. This drives usage, but once the hype train slows down, you will end up with well designed MCP servers that are not just mimics of the API. 95% of the APIs AWS offers are not directly called, a quarter are paginated, and the rest have little documentation on usage. \n\nWell designed MCP servers are not parities of the API.",
          "score": 1,
          "created_utc": "2026-01-20 02:41:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ewrey",
          "author": "Spaceman_Zed",
          "text": "I was curious as well, although I use the MCP daily, and know it works better, I couldn't quite put words to it. So please accept this through AI answer.\n\n1. Reliability & Hallucination Prevention\nLLM + CLI (The Risk): When an LLM generates a CLI command, it is guessing the syntax based on its training data. If a flag has changed, or if the model \"hallucinates\" a parameter (e.g., inventing a --force-delete flag that doesn't exist), the command fails. You then have to paste the error back to the LLM to debug.  \nAWS MCP (The Solution): The MCP server exposes defined tools to the LLM. The LLM doesn't guess the command; it selects a tool from a list of valid options provided by the server. The MCP server then constructs the correct API call or CLI command under the hood, ensuring syntax accuracy.  \n2. Context Window Efficiency\nLLM + CLI: To get an LLM to understand your infrastructure via CLI, you often have to run aws ec2 describe-instances, copy the massive JSON output, and paste it into the chat. This eats up your context window rapidly with irrelevant noise.\nAWS MCP: MCP servers are \"context-aware.\" They can fetch only the relevant resources (resources) or summarize data before sending it to the LLM. This keeps the conversation focused and prevents the model from \"forgetting\" earlier instructions due to context overflow.  \n3. Security & Guardrails\nLLM + CLI: If you give an LLM access to a terminal (e.g., via a \"bash\" tool), it effectively has the permissions of your local user. It could accidentally delete resources or upload credentials if you aren't watching every character it types.\nAWS MCP:\nLeast Privilege: You can run the MCP server with a specific, restricted AWS profile or role, independent of your main local credentials.  \nSandboxing: MCP servers can verify the \"intent\" of a command before executing it.\nRead-Only Modes: Many MCP implementations allow you to set the server to \"read-only,\" meaning the LLM can look at your S3 buckets but physically cannot execute a delete or put command, regardless of what the prompt says.\n4. Structured Data vs. Text Parsing\nLLM + CLI: CLI output is text. The LLM has to parse whitespace, tables, or raw JSON text. Complex outputs (like CloudWatch logs or deeply nested JSON) are difficult for an LLM to read reliably without formatting errors.\nAWS MCP: The protocol allows the server to pass structured objects directly to the LLM. It acts like an API integration, meaning the LLM receives clean data structures (lists, dictionaries) rather than a wall of text it has to OCR/parse.\n5. Discovery & Up-to-Date Knowledge\nLLM + CLI: The LLM's knowledge of AWS CLI commands is cut off at its training date. It won't know about a new AWS service released last month.\nAWS MCP: The MCP server is a piece of software you update. If AWS releases a new feature and you update your MCP server, the LLM immediately has access to that \"tool\" and its documentation via the protocol, even if the model itself hasn't been retrained.",
          "score": -5,
          "created_utc": "2026-01-19 03:25:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ez4c3",
              "author": "Buttleston",
              "text": "We're so cooled",
              "score": 9,
              "created_utc": "2026-01-19 03:39:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0h5llh",
              "author": "ObjectiveAide9552",
              "text": "This is absolutely correct. Reddit just has a luddite hate boner for anything generated by llm.",
              "score": 1,
              "created_utc": "2026-01-19 13:57:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0et4oy",
          "author": "rlt0w",
          "text": "This? https://awslabs.github.io/mcp/#available-aws-mcp-servers",
          "score": 0,
          "created_utc": "2026-01-19 03:05:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0eufyf",
              "author": "Dry_Raspberry4514",
              "text": "This one - [https://docs.aws.amazon.com/aws-mcp/latest/userguide/what-is-mcp-server.html](https://docs.aws.amazon.com/aws-mcp/latest/userguide/what-is-mcp-server.html)",
              "score": 1,
              "created_utc": "2026-01-19 03:12:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0euqwd",
                  "author": "rlt0w",
                  "text": "I know what MCP is, I guess I have no idea what you're asking. I use them daily and find great utility in them.",
                  "score": 2,
                  "created_utc": "2026-01-19 03:13:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qj132z",
      "title": "I made DynamoLens: FOSS desktop companion for DynamoDB",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qj132z/i_made_dynamolens_foss_desktop_companion_for/",
      "author": "rasjonell",
      "created_utc": "2026-01-21 15:35:32",
      "score": 10,
      "num_comments": 3,
      "upvote_ratio": 0.91,
      "text": "I’ve been building DynamoLens, a free and open-source desktop app for Amazon DynamoDB. It’s a non-Electron (Wails) desktop client that makes it easy to explore tables, inspect/mutate items, and juggle multiple environments without living in the console or CLI.\n\nHighlights:\n\n\\- Visual workflows to compose repeatable item/table operations—save, share, and replay without redoing manual steps\n\n\\- Dynamo-first explorer: list tables, view schema details, scan/query, and create/update/delete items and tables\n\n\\- Multiple auth modes: AWS profiles, static creds, or custom endpoints (DynamoDB Local works great)\n\n\\- Modern UI with command palette, pinning, and theming\n\nIf you want to try it: [https://dynamolens.com/](https://dynamolens.com/)\n\nRepo: [https://github.com/rasjonell/dynamo-lens](https://github.com/rasjonell/dynamo-lens) (free & open source)\n\nWould love feedback from folks who live in DynamoDB day to day, what’s missing or rough?",
      "is_original_content": false,
      "link_flair_text": "database",
      "permalink": "https://reddit.com/r/aws/comments/1qj132z/i_made_dynamolens_foss_desktop_companion_for/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0vcgsw",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'database'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+database).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-01-21 15:35:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16fx3m",
          "author": "OkSadMathematician",
          "text": "nice work. wails is solid choice over electron bloat. add batch operations and you got a winner",
          "score": 2,
          "created_utc": "2026-01-23 04:04:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0vcgrk",
          "author": "AutoModerator",
          "text": "Here are a few handy links you can try:\n\n- https://aws.amazon.com/products/databases/\n- https://aws.amazon.com/rds/\n- https://aws.amazon.com/dynamodb/\n- https://aws.amazon.com/aurora/\n- https://aws.amazon.com/redshift/\n- https://aws.amazon.com/documentdb/\n- https://aws.amazon.com/neptune/\n\nTry [this search](https://www.reddit.com/r/aws/search?q=flair%3A'database'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+database).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": -2,
          "created_utc": "2026-01-21 15:35:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qg8ebs",
      "title": "Principals, tags, SCPs, and ABAC",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qg8ebs/principals_tags_scps_and_abac/",
      "author": "bobaduk",
      "created_utc": "2026-01-18 13:20:11",
      "score": 9,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hello friends.\n\nI have a reasonably complex AWS account structure with a bunch of workloads and sandboxes in an AWS Organization. I'm thinking about applying ABAC to simplify IAM setup in certain cases. For example, imagine that we have an account sandbox-bobaduk, where I have broad access for playing around. We also have an account secret-data where we store some dataset in an S3 bucket.\n\nWe use Google Workspace as our IDP, and I can apply tags to my role session based on attributes. For example, I authenticate as arn:aws:sts::$sandbox-bobaduk:assumed-role/AWSReservedSSO_MyRole_08759cec7ee3fdc9/bobaduk@org.org. Because I used sso to authenticate, I have the tag `team=data-guy` on my role session.\n\nI can write a resource policy for my s3 bucket that allows GetObject if the OrgId=myorg, and the team tag has the value \"data-guy\".\n\nSo far so good.\n\nMy question, which I'm struggling a little to answer is \"can I trust the provenance of that tag?\".\n\nMy thinking is that I can use an SCP that denies tagging a session with the \"team\" tag, unless the user is adopting a role matching \"AWSReservedSSO_*\". \n\nI should also have an SCP that prevents a user from creating a new role or user with that tag.\n\nthe AWSReservedSSO_* roles can only be created by identity centre, and the trust policy restricts their use to identity centre, so with those SCPs in place, am I missing anything? \n\nI don't need transitive tagging for role chaining, because these tags are _only_ used for this kind of cross-account access based on a resource policy. if I assume another role, I should only have the permissions granted explicitly to that role.",
      "is_original_content": false,
      "link_flair_text": "security",
      "permalink": "https://reddit.com/r/aws/comments/1qg8ebs/principals_tags_scps_and_abac/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0aklds",
          "author": "Iliketrucks2",
          "text": "Had a long chat about just this the other day, same set of problems.  We are moving trust from fairly easy to control IAM policy to tag management.  And we want our CD system and users to set both our required tags, but any tags they want, so we need to find a way to namespace tags so we can control how those tags get modified which looks doable but a lot of work.  \n\nAWS does not make this easy, as always.",
          "score": 3,
          "created_utc": "2026-01-18 14:18:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0amoy5",
              "author": "bobaduk",
              "text": "Namespacing I have *down*, I have two sets of tags, `myorg:sso:attrib` that gets used for humans, and `myorg:cap:attrib` that gets applied to roles used in automation etc.\n\nsoo attributes are used to say \"this person is in this team, this department whatever, so they can do these things\", capability tags are used to say \"this role can do this particular set of operations\". Given the namespacing, it's easy to say things like  \n\n* \"you may never tag a session as myorg:cap:...\" \n* or \"you may never tag a role myorg:sso:...\"  \n* and \"you may only tag a session as myorg:sso:... if you're assuming an SSO role\",\n\nbut I feel ... unsafe :D",
              "score": 1,
              "created_utc": "2026-01-18 14:29:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0b0vhq",
                  "author": "Iliketrucks2",
                  "text": "Clever. We were going to do similar with corp::{secuirty|cost|access|governance|misc}::{tags} and limit with scps which role(s) can manage which tags, but we still have the omnipotent deployer role that we want to protect against while allowing to create the tags we need in each namespace.  Our concern is around a malicious user rather than an ignorant one - someone who wanted to get access to data could plan a deploy to change tags on things and circumvent our controls.  We have a human review on PR but are concerned about asking devops teams to know enough about tagging to enforce policy - we want technical controls\n\nI think for our most sensitive data we may end up using tagging but adding auditing or a hard scp with specific resources and tag values so only our security team can set/change those tags, then loosen the control and guardrails as data becomes less sensitive so we minimize impact to developers",
                  "score": 1,
                  "created_utc": "2026-01-18 15:42:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qis9yz",
      "title": "Service recommendation",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qis9yz/service_recommendation/",
      "author": "Artistic-Analyst-567",
      "created_utc": "2026-01-21 08:15:57",
      "score": 8,
      "num_comments": 11,
      "upvote_ratio": 0.89,
      "text": "Hello folks,\n\nLooking for recommendations for storing and searching across a large volume of data\n\nWe basically have a flattened table structure that holds around 300 million records, probably close to 50 columns\n\nWe need to provide fuzzy text search on some fields, expecting fairly high queries per second volume, and latency has to be on par with synchronous api style (200ms up to 1s)\n\nWe were initially thinking about loading the data into our RDS Aurora (MySQL, r6g.xlarge) but i never dealt with that kind of data volume and i imagine the indexes will be massive and maintenance will be painful\n\nThen i thought about Dynamodb but the fuzzy search requirement ruled that option out\n\nNow thinking OpenSearch serverless might be a good candidate\n\nAnyone worked on a similar scenario? we don't expect that table to get much updates, maybe once a month at most",
      "is_original_content": false,
      "link_flair_text": "database",
      "permalink": "https://reddit.com/r/aws/comments/1qis9yz/service_recommendation/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0tkzyh",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'database'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+database).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-01-21 08:15:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tqhx8",
          "author": "proxwell",
          "text": "This is really squarely in OpenSearch's wheelhouse.\n\nYou have a high read/query to write ratio, high QPS, infrequent updates, and need to support fuzzy text search.\n\nWhile you could do this in Aurora, I think your experience would be very unpleasant relative to OpenSearch.  With Aurora, you'd have massive fulltext indexes, slow/unpredictable index rebuilds, and performance is likely to bog down quickly under concurrent fuzzy searches.  Also, the scaling story is pretty painful, as you’ll hit IOPS, buffer pool, or CPU ceilings faster than expected.\n\nI think OpenSearch serverless is the way to go in your scenario.  You'll have a little less low-level control for a couple niche tuning settings, and you'll want to do some proactive cost estimation to make sure you know what you're on the hook for, but I still think serverless is the clear winner here.",
          "score": 11,
          "created_utc": "2026-01-21 09:08:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ujx36",
          "author": "dataflow_mapper",
          "text": "your instinct is pretty solid. RDS will work on paper but at that scale the index bloat and tuning pain usually outweigh the benefits, especially for fuzzy search. DynamoDB is basically out once you need flexible text matching. OpenSearch is the tool most teams land on for this pattern, especially with mostly read traffic and infrequent bulk updates. The key is modeling the index carefully and being very intentional about analyzers so you do not blow up query latency. If the data really is mostly append or monthly refresh, reindexing is manageable. I have seen similar setups hit your latency targets as long as the queries stay scoped and you resist turning every field into a fuzzy search field.",
          "score": 3,
          "created_utc": "2026-01-21 13:07:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0umk1i",
              "author": "Artistic-Analyst-567",
              "text": "Very insightful, will keep those recommendations in mind\nI just saw that there is a dedicated ingestion service for OS serverless (OSIS), this will come in handy since the data will probably reside in S3",
              "score": 1,
              "created_utc": "2026-01-21 13:22:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0uk25w",
          "author": "solo964",
          "text": "You're storing 300m records and need to support sustained high query volumes with predictable performance, so I would say that regular managed OpenSearch could be a better option. Run the numbers but it could be more cost-effective as well as improve query latencies. Downside is the higher operational burden.",
          "score": 3,
          "created_utc": "2026-01-21 13:08:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0umrhk",
              "author": "Artistic-Analyst-567",
              "text": "Makes sense, it's probably worth running a couple of POCs and see what the trade-offs and cost implications are",
              "score": 1,
              "created_utc": "2026-01-21 13:24:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0tqeks",
          "author": "Horciodedayo",
          "text": "RemindMe! -1 day",
          "score": 1,
          "created_utc": "2026-01-21 09:07:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tqh1c",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 day on [**2026-01-22 09:07:43 UTC**](http://www.wolframalpha.com/input/?i=2026-01-22%2009:07:43%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/aws/comments/1qis9yz/service_recommendation/o0tqeks/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Faws%2Fcomments%2F1qis9yz%2Fservice_recommendation%2Fo0tqeks%2F%5D%0A%0ARemindMe%21%202026-01-22%2009%3A07%3A43%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qis9yz)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-21 09:08:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ve5ce",
          "author": "TechDebtSommelier",
          "text": "300 million rows plus fuzzy text search is basically OpenSearch’s whole personality, so your instinct there is right. Aurora will technically work but you’ll hate your life maintaining giant text indexes and still miss your latency targets once QPS ramps up. DynamoDB is a non starter for fuzzy search unless you bolt something else on.\n\nOpenSearch Serverless fits well here since your data is mostly read heavy and rarely updated, but do budget time for index tuning and shard sizing because it is not magic. If you want boring and predictable performance at that scale, search engine plus source of truth in S3 or RDS is the usual pattern, not trying to force a relational database to cosplay as a search engine.",
          "score": 1,
          "created_utc": "2026-01-21 15:43:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wnhal",
          "author": "gwinerreniwg",
          "text": "What about S3 Tables and like Athena on top or maybe OpenSearch?",
          "score": 1,
          "created_utc": "2026-01-21 19:04:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tkzxv",
          "author": "AutoModerator",
          "text": "Here are a few handy links you can try:\n\n- https://aws.amazon.com/products/databases/\n- https://aws.amazon.com/rds/\n- https://aws.amazon.com/dynamodb/\n- https://aws.amazon.com/aurora/\n- https://aws.amazon.com/redshift/\n- https://aws.amazon.com/documentdb/\n- https://aws.amazon.com/neptune/\n\nTry [this search](https://www.reddit.com/r/aws/search?q=flair%3A'database'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+database).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": -1,
          "created_utc": "2026-01-21 08:15:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qh10yo",
      "title": "AWS S3 Batch Replication (operation: replicate). Both buckets are versioned. What happens on object key collision?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qh10yo/aws_s3_batch_replication_operation_replicate_both/",
      "author": "IceAdministrative711",
      "created_utc": "2026-01-19 10:48:50",
      "score": 7,
      "num_comments": 2,
      "upvote_ratio": 0.89,
      "text": "**Context**  \nI configure [S3 Batch Operation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html) (to replicate existing objects). Manifest is generated automatically and includes all objects. Both buckets are versioned. Batch Job is configured based on existing (Live) replication configuration.\n\n**Question**  \nI know that both buckets have one object with the same key but different versions. Which version will become current? Is there any documentation on that matter?\n\n\\---\n\n**PS**  \nI observed 2 behaviours:\n\n1. source object's version becomes current version in the Destination Bucket\n2. The Destination object version remains current while source object version is added to the non-current versions in the Destination Bucket\n\nI can only assume that it depends on \\`last modified\\` date and the newest version (be it source or destination) wins",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1qh10yo/aws_s3_batch_replication_operation_replicate_both/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0hiji7",
          "author": "SpecialistMode3131",
          "text": "I think you're right about what will happen.\n\nMore importantly, you should change your system so this is never even an issue.  Just get rid of this problem.  There is no way this kind of complexity is benefiting you, and you will be able to eliminate it with some thought.",
          "score": 3,
          "created_utc": "2026-01-19 15:05:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0huxs1",
          "author": "menge101",
          "text": "Completely agree with /u/specialistmode3131 , this seems like an [XY problem](https://xyproblem.info/) as well.  \n\nWhat is it you are trying to do?",
          "score": 2,
          "created_utc": "2026-01-19 16:03:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qefo3w",
      "title": "Account suspended during active DDoS billing review — seeking guidance on escalation paths",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qefo3w/account_suspended_during_active_ddos_billing/",
      "author": "Plane-Management-176",
      "created_utc": "2026-01-16 13:16:31",
      "score": 7,
      "num_comments": 8,
      "upvote_ratio": 0.89,
      "text": "Looking for guidance from others who have dealt with AWS account suspensions during active billing or security reviews.\n\nOur production workload was hit by a large DDoS attack, which caused a sudden spike in AWS WAF, CloudFront, and CloudWatch usage and a very large, unexpected bill. We opened support cases immediately, shared ARNs, detailed timelines, WAF analytics, request counts in the millions per day, and attacker IP samples. AWS acknowledged the issue and escalated it for service-team review and possible billing adjustment.\n\nWhile this review was still ongoing, and despite requesting temporary billing hold during the investigation, the account was suspended for non-payment. We’re now unable to log in to the console, which has taken production applications offline and blocked access to CloudWatch and infrastructure management.\n\nAt this point, we’re trying to understand the correct escalation path. For those who’ve experienced something similar:  \nIs there a recommended way to get an account reinstated while a billing dispute is under review?  \nAre there escalation channels beyond the standard account support form once console access is blocked?\n\nAppreciate any guidance or experiences from the community.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1qefo3w/account_suspended_during_active_ddos_billing/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nzz8crh",
          "author": "AWSSupport",
          "text": "Hello,\n\nSorry to hear about the issue with your account. Feel free to share your case ID with us via chat and I can take a look.\n\n\\- Doug S.",
          "score": 3,
          "created_utc": "2026-01-16 19:37:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzby1d",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-01-16 19:54:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzzh9vr",
                  "author": "AWSSupport",
                  "text": "Hi there,\n\nThank you for providing your case ID. I've shared your concerns internally with our Support team on your behalf.\n\nI'd encourage you to continue to communicate via email with our Support team as they're equipped with the tools to best assist.\n\n\\- Doug S.",
                  "score": 1,
                  "created_utc": "2026-01-16 20:19:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o02fehd",
          "author": "Burekitas",
          "text": "Do you have a Shield Advanced subscription? Or do you work with AWS Partner? \n\n  \nIf not, it might be a major cash flow issue, but the invoice must be paid until this issue is resolved.\n\nI've dealt with DDoS cases in the past and 15K customer received a $413K and it took 4 months to resolve it. Not an easy situation but invoices must be paid :/ \n\nGood luck getting your account back.",
          "score": 3,
          "created_utc": "2026-01-17 07:02:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzz5tq1",
          "author": "The_Tree_Branch",
          "text": "Do you have an AWS account manager/account team? I'd be leaning on them heavily if so.",
          "score": 5,
          "created_utc": "2026-01-16 19:26:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzc8hz",
              "author": "Plane-Management-176",
              "text": "No i dont have any AWS account manager",
              "score": 2,
              "created_utc": "2026-01-16 19:55:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o09ihai",
          "author": "dataflow_mapper",
          "text": "That sounds brutal, and sadly I have seen variations of this before. Once an account is fully suspended, normal support cases tend to stall because billing enforcement is automated and not tightly coupled to ongoing reviews. What helped in one case was getting the account manager or TAM involved, even if they were only loosely attached before. If you do not have one, pushing hard through the billing escalation form and explicitly stating production impact sometimes triggers a different internal queue. It is also worth asking for temporary read-only console access so you can at least validate state and exports. The big lesson I took from this is that billing disputes and security incidents often move on totally different tracks internally, and they do not always talk to each other fast enough.",
          "score": 2,
          "created_utc": "2026-01-18 09:17:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qimtwo",
      "title": "The architecture behind my sub-500ms Llama 3.2 on Lambda benchmark (it's mostly about vCPUs)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qimtwo/the_architecture_behind_my_sub500ms_llama_32_on/",
      "author": "NTCTech",
      "created_utc": "2026-01-21 03:26:42",
      "score": 7,
      "num_comments": 6,
      "upvote_ratio": 0.77,
      "text": "A few days ago I posted a benchmark here showing Llama 3.2 (3B, Int4) running on Lambda with sub-500ms cold starts. The reaction was skeptical, with many folks sharing their own 10s+ spin-up times for similar workloads.\n\nI wanted to share the specific architecture and configuration that made that benchmark possible. It wasn't a private feature; it was about exploiting how Lambda allocates resources.\n\nHere is the TL;DR of the setup:\n\n**1. The 10GB Memory \"Hack\" is for vCPUs, not RAM.** This is the most critical part. A 3GB model doesn't need 10GB of RAM, but in Lambda, you can't get CPU without memory. At 1,769 MB, you only get 1 vCPU.\n\n* To get the **6 vCPUs** needed to saturate thread pools for parallel model deserialization (e.g., with PyTorch/ONNX Runtime), you need to provision **\\~10GB of memory**.\n* The higher memory also comes with more memory bandwidth, which helps immensely.\n* **Counter-intuitively, this can be cheaper.** The function runs so much faster that the total cost per invocation is often lower than a 4GB function that runs for 5x longer.\n\n**2. Defeating the \"Import Tax\" with Container Streaming.** Standard Python imports like `import torch` are slow. I used Lambda's **container image streaming**. By structuring the Dockerfile so the model weights are in the lower layers, Lambda starts streaming the data *before* the runtime fully initializes, effectively paralleling the two biggest bottlenecks.\n\n**The Results (from my lab):**\n\n* **Vanilla Python (S3 pull):** \\~8s cold start. Unusable.\n* **Optimized Python (10GB + Streaming):** \\~480ms cold start. This was the Reddit post.\n* **Rust + ONNX Runtime:** \\~380ms cold start. The fastest, but highest engineering effort.\n\nI wrote up a full deep dive with the Terraform code, a more detailed benchmark breakdown, and a decision matrix on when *not* to use this approach (e.g., high, steady QPS).\n\n[**https://www.rack2cloud.com/lambda-cold-start-optimization-llama-3-2-benchmark/**](https://www.rack2cloud.com/lambda-cold-start-optimization-llama-3-2-benchmark/)\n\nI'm curious if others have played with high-memory Lambdas specifically for the CPU benefits on CPU-bound init tasks. Is the trade-off worth it for your use cases?",
      "is_original_content": false,
      "link_flair_text": "architecture",
      "permalink": "https://reddit.com/r/aws/comments/1qimtwo/the_architecture_behind_my_sub500ms_llama_32_on/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0uj87e",
          "author": "Nater5000",
          "text": ">I'm curious if others have played with high-memory Lambdas specifically for the CPU benefits on CPU-bound init tasks.\n\n\nWe ended up doing this for some image processing that was part of a REST API. Since that much memory/vCPU was overkill for the rest of the app, we ended up having to have two Lambdas with different memory configs that effectively ran the same code, with the smaller REST API Lambda calling the bigger image processing Lambda as needed. It generally worked, but was more of a headache than one would think at first glance.\n\n\nStill, interesting you managed to make this work in Lambda so effectively. I've played around with running small LLMs in Lambda with some success, so adding some of the details you mentioned might make a big difference.",
          "score": 2,
          "created_utc": "2026-01-21 13:02:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0unqzg",
              "author": "NTCTech",
              "text": "That is a perfect real-world example of this dynamic in action. It’s validating to hear you ran into the same CPU-bound constraints with image processing.\n\nRegarding the \"headache\" of splitting into two Lambdas (small router vs. big processor): I feel your pain on the operational overhead, but architecturally, **you absolutely made the right call.**\n\nThis is the classic serverless trade-off: operational complexity vs. execution efficiency. If you ran your lightweight REST API handler on that 10GB instance, you’d be burning significant budget on idle vCPUs just waiting for network I/O. By decoupling them, you aligned the resource profiles to the actual work being done. It hurts to manage, but it's the correct design pattern for cost and performance.\n\nDefinitely give the container streaming setup a shot for your LLM experiments. The high vCPU count *really* shines when it can parallelize the layer download and the model deserialization simultaneously. Please let us know if you see similar gains.",
              "score": -1,
              "created_utc": "2026-01-21 13:29:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0uvv03",
                  "author": "Nater5000",
                  "text": "lol please, I really don't need the overly affirmative LLM-speak on reddit too",
                  "score": 4,
                  "created_utc": "2026-01-21 14:13:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0wo75z",
          "author": "OkSadMathematician",
          "text": "the vcpu angle is underrated. most people think of lambda memory as ram when its really a proxy for compute allocation. seen this same pattern work for build processes and data transforms where you overprovision memory just to get more cpu and end up paying less because runtime drops by 5x\n\ncontainer streaming is clever but i wonder about cache hit rates in production. if youre getting consistent traffic the warm pool keeps things fast anyway but for true sporadic workloads this makes sense. curious what your p99 looks like over a week vs just the cold start number",
          "score": 1,
          "created_utc": "2026-01-21 19:07:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0x0167",
              "author": "NTCTech",
              "text": "Yeah, the memory CPU realization is the unlock. I use that same over-provisioning trick for batch jobs now; it feels wrong to throw 10GB at a 500MB process, but the runtime speedup usually makes the pricing math work out in your favor.\n\nOn the weekly P99 question that's the real reality check.\n\nIn my testing over a week with \"choppy\" traffic (enough gaps to trigger frequent cold starts, but some sustained bursts), my weekly P99 hovered around 550ms–600ms.\n\nIt’s actually slightly *higher* than the pure cold start benchmark (480ms) because real-world noise drags it up network jitters, DynamoDB latency on the lookups, etc.\n\nIf your traffic is totally sporadic (like one request every 2 hours), your P99 is basically just going to be that cold start number every time. But in a mixed workload, the warm starts (sub-100ms) drag the average down, while the cold starts define the tail.",
              "score": 1,
              "created_utc": "2026-01-21 20:01:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qj44kp",
      "title": "Does AWS close accounts for lack of use?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qj44kp/does_aws_close_accounts_for_lack_of_use/",
      "author": "mntgoat",
      "created_utc": "2026-01-21 17:24:33",
      "score": 7,
      "num_comments": 14,
      "upvote_ratio": 0.67,
      "text": "I got an email this morning saying my account is closed. This is a personal account that I don't use. I think I created it years ago. I do use my business account but that is a different account. The last email prior to this from AWS was 2022. Could it have been closed because of lack of use? \n\n\n\n>This e-mail confirms that the Amazon Web Services account associated with account ID XXXX is permanently closed and cannot be reopened. Any content remaining in this account is inaccessible and will be erased.\n\n>  \n",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1qj44kp/does_aws_close_accounts_for_lack_of_use/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0w7s4i",
          "author": "xxwetdogxx",
          "text": "Most likely you had something running, and at some point you got a new credit card- after failing to bill the old card AWS shut down the account for non-payment. AWS doesn't shut down accounts simply due to lack of use.",
          "score": 21,
          "created_utc": "2026-01-21 17:55:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0wart0",
              "author": "mntgoat",
              "text": "It could be but I don't have any emails from them.\n\nI do remember I used have a charge of cents per month a while ago but I think I shut that down.",
              "score": 1,
              "created_utc": "2026-01-21 18:08:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0w1jz2",
          "author": "DarthKey",
          "text": "No.",
          "score": 7,
          "created_utc": "2026-01-21 17:28:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0w2ipq",
          "author": "seanv507",
          "text": "I'm pretty sure the same thing happened to me.",
          "score": 1,
          "created_utc": "2026-01-21 17:32:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1054g1",
          "author": "Every-Barracuda-320",
          "text": "Billing billing... I have an AWS account I haven't used for 10 years and it's still active. They don't close accounts like that unless you don't pay. It could be just a few cents but if not paid, it can cause them to close the account but they sent many warnings before they do it.",
          "score": 1,
          "created_utc": "2026-01-22 06:33:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15ku8i",
              "author": "gadgetvirtuoso",
              "text": "I use SES personally and my bill is often just $0.01 many months and they never actually charge me. I don’t know what the actual threshold is but I’d guess more than $1 otherwise they lose money on the CC processing fees.",
              "score": 1,
              "created_utc": "2026-01-23 01:08:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o161w03",
                  "author": "Every-Barracuda-320",
                  "text": "Same here. I had many of these $0.01 monthly bills but they were never charged. \n\nSometimes people miss services in other region. My previous company was getting high bills but they couldn't see many services to justify it. I went on exploring. They had a bunch of EC2 running in Singapore region. They were started months ago and forgot about them as they don't usually use that region./",
                  "score": 1,
                  "created_utc": "2026-01-23 02:43:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o121pb8",
          "author": "wesleyaplix",
          "text": "AWS generally doesn’t close accounts *just* for inactivity. The fact that the email says “permanently closed and cannot be reopened” makes it sound more like an internal compliance or billing trigger than simple lack of use.",
          "score": 1,
          "created_utc": "2026-01-22 15:00:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0w5iml",
          "author": "AWSSupport",
          "text": "Hi there, \n\nI'm sorry to hear about your AWS account being closed. You can fill out the following form to contact our Support team for assistance with this: go.aws/account-support. \n\n\\- Gee J.",
          "score": 1,
          "created_utc": "2026-01-21 17:45:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0wahwm",
              "author": "mntgoat",
              "text": "I did but they told me they couldn't help me without logging in but when I log in I just get message that it is closed and when I click on support it asks me to log in again.",
              "score": 4,
              "created_utc": "2026-01-21 18:07:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0wjr8q",
          "author": "AWSSupport",
          "text": "Hi there, \n\nThanks for the quick follow up. Could you chat message us your case ID? We can provide your feedback to our Support team, and let them know about your case. \n\n\\- Gee J.",
          "score": -4,
          "created_utc": "2026-01-21 18:48:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0wrfn7",
              "author": "mntgoat",
              "text": "Sure thing. Thanks.",
              "score": 2,
              "created_utc": "2026-01-21 19:22:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0x1yh5",
                  "author": "AWSSupport",
                  "text": "Hi Carlos, \n\nThanks for sending us your case ID. We're unable to discuss account details over social media for security purposes. I recommend working with our Support team on your support case for additional assistance. \n\n\\- Gee J.",
                  "score": -9,
                  "created_utc": "2026-01-21 20:10:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0w9lux",
          "author": "SoggyGrayDuck",
          "text": "They shut off access to ec2, you just have to change the root password and put a ticket in.\n\nEdit: I didn't read the whole comment, you must have waited longer than I did.",
          "score": -11,
          "created_utc": "2026-01-21 18:03:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qi650u",
      "title": "How do you keep system context from rotting over time?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qi650u/how_do_you_keep_system_context_from_rotting_over/",
      "author": "kennetheops",
      "created_utc": "2026-01-20 16:43:17",
      "score": 6,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "Former SRE here, looking for advice.\n\nI know there are a lot of tools focused on root cause analysis after things break. Cool, but that’s not what’s wearing me down. What actually hurts is the constant context switching while trying to understand how a system fits together, what depends on what, and what changed recently.\n\nAs systems grow, this feels like it gets exponentially harder. Add logs and now you’ve created a million new events to dig through.. Add another database and suddenly you’re dealing with subnet constraints or a DB choice that’s expensive as hell, and no one noticed until later. Everyone knows their slice, but the full picture lives nowhere, so bit rot just keeps creeping in.\n\nThis feels even worse now that AI agents are pushing a ton of slop ..i mean code and config changes quickly. Things are moving at lightspeed, I cant be the only one feeling like my understanding is falling behind daily.\n\nI’m honestly stuck on how people handle this well in practice. For folks dealing with real production systems, what’s actually helped? Diagrams, docs, tribal knowledge, tooling, something else? ",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1qi650u/how_do_you_keep_system_context_from_rotting_over/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0p00z3",
          "author": "SpecialistMode3131",
          "text": "You need to write real documents (preferably outside the codebase, as in wiki type environments) that pull together the business reasons for the system to exist, alongside the high level code decisions that were made.\n\nThere's a lot of different schools of thought - for example, my claim that putting docs outside the codebase is good will be disputed by some - but end of the day you cannot use a tool to skip taking the time to thoroughly describe your intent in laying out the systems as they exist now.\n\nDocs rot, too, so a dedicated hunk of time every week, month, etc to sweep your core foundational documents to ensure they're up to date is critical to keeping important stuff well understood and running.\n\nOnly you can decide if you have the will to do so - if it's important enough.  Just remember you reap as you sow.\n\nWhen I deliver for clients, I always leave behind a thorough high level documentation base for future maintainers, and when I am given existing legacy systems to deal with, building synthesis is the first order of business.",
          "score": 3,
          "created_utc": "2026-01-20 16:49:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0p0yso",
              "author": "kennetheops",
              "text": "I like the idea of having the docs outside of the code base. \n\nWhat are you doing to capture info said in chat threads? Or do you assume this as a just a losing battle?",
              "score": 1,
              "created_utc": "2026-01-20 16:53:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0p1onv",
                  "author": "SpecialistMode3131",
                  "text": "Human beings are present in chat threads. Hold them accountable to putting the important content they learn down in the docs in a good way.  Make keeping docs in good shape part of their evaluation criteria at review time.\n\nThere is a tendency in tech to try and make everything a tool. When work requires judgment, as in documentation, that's a big mistake and it leads to a completely predictable decaying useless mess. Just refuse to make that mistake, and require human beings to own the documentation fully.  And keep the documentation high level so it doesn't become a burden.",
                  "score": 1,
                  "created_utc": "2026-01-20 16:57:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0p58ws",
          "author": "oneplane",
          "text": "What's helped is having responsibilities tied together, i.e. a change in some IaC is done for a reason, and depending on the size and complexity that reason (or intent) needs to be in the code, in the docs along side the code, in the global system docs or in business docs, or a mix of all of them.\n\nIn theory with static systems you'd be tracing from a business need to a functional need to a technical need to a requirement to a design to an implementation. In reality that doesn't really work out very often, but what you can do is apply the same rules as you'd do in namespaces/modules/packages/boundaries, things that only matter very close to the technical 'thing' and don't spill over into other areas (outside of its own boundary) would be in your commit message, code comments, or repository docs for example. You'd have to have rules and processes in place to not merge code that doesn't have an intent described and attached to it.",
          "score": 1,
          "created_utc": "2026-01-20 17:13:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0pezrs",
              "author": "kennetheops",
              "text": "Is this a process or do you use a tool for this?",
              "score": 1,
              "created_utc": "2026-01-20 17:58:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0pzasq",
                  "author": "oneplane",
                  "text": "We use Atlantis, OPA and a custom coverage tool to only allow a Terraform apply if coverage on the PR is over 90%, we're experimenting with doing more in a workflow pipeline but it's mostly an optimisation rather than a change in features.\n\nSimilar results can be achieved with pre-commit.",
                  "score": 1,
                  "created_utc": "2026-01-20 19:30:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0panru",
          "author": "dr_barnowl",
          "text": "Descriptive code.\n\nThe slopcode is a problem for this approach.\n\nWrite abstractions that help you comprehend things. That's basically what all code is once you stop writing raw machine code as byte values into memory.\n\nOnce I get a project started, I don't write a VPC by writing all the little ins and outs. I have a module. I say \"this is VPC #23, it's for this\". The code works out the CIDR blocks from the VPC number, and the module has a standard structured output that application modules expect to see, that describes the available subnets, etc. I just pass this output to the application module which is written to use it.\n\nLook at the top and you can see a VPC, an application, a link of the VPC to the transit gateway. Dig down and you can see the detail, which you make as consistent as possible so it's understandable.\n\n(NB CloudFormation sucks at this, Terraform is much better).",
          "score": 1,
          "created_utc": "2026-01-20 17:38:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0peupd",
              "author": "kennetheops",
              "text": "we are doing this for infra, but how are you tracking code dependencies  to infra resources? For example say we have 2 dbs but 1 db is dev and the other is prod, and 10 vms. Obviously the prod vms have a higher risk for changes than the dev vms.",
              "score": 1,
              "created_utc": "2026-01-20 17:58:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0qf9db",
                  "author": "dr_barnowl",
                  "text": "Prod vs dev my choice would always be account separation ; setting up cross-account deployment involves some extra work, but in an ideal world, no dev has access to production resources. The clearest way to ensure this is to ensure they have no access to entire accounts.",
                  "score": 1,
                  "created_utc": "2026-01-20 20:44:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qh1n3l",
      "title": "SESv2 migration",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qh1n3l/sesv2_migration/",
      "author": "sloveubagukaraliui",
      "created_utc": "2026-01-19 11:23:20",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 0.88,
      "text": "Hi, I use terraform to manage aws deployments. \n\nSes is deployed using v1 api and now I want to migrate to v2. \n\nWhat are the steps? \n\nDo I destroy v1 resources first and deploy v2? \n\nwhat happens with dkim dns set up, would I need to configure new entries? \n\nI cant have any downtime, emails are a super critical part of our business. Switching to some other domain is not suitable due to need for warmup that can take up to 2 months. ",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1qh1n3l/sesv2_migration/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0gmwiv",
          "author": "CSYVR",
          "text": "They are the same resources, just with different APIs to configure them.\n\nYou can add the v2 resources and run imports gradually, there's not even a huge problem managing the two resources at the same time, as long as you're not making any changes.\n\nAlternative is just deleting the v1s from the state (terraform state rm <resourceid>) and importing the new ones (e.g.  terraform import aws\\_sesv2\\_email\\_identity.example [example.com](http://example.com) )",
          "score": 5,
          "created_utc": "2026-01-19 11:52:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jvjrr",
              "author": "sloveubagukaraliui",
              "text": "wait what\n\nare you saying that I should be able to create a v2 resource using the same domain alongside to an existing v1 domain identity?",
              "score": 1,
              "created_utc": "2026-01-19 21:34:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0jxdt7",
                  "author": "CSYVR",
                  "text": "Yes, just add and import it:\n\n    #v1 resource\n    resource \"aws_ses_domain_identity\" \"example\" {\n      domain = \"example.com\"\n    }\n    \n    #v2 resource\n    resource \"aws_sesv2_email_identity\" \"example\" {   \n      email_identity = \"example.com\" \n    } \n    \n    #Import statement for v2 resource so no new identity is created\n    import {\n      to = aws_sesv2_email_identity.example\n      id = \"example.com\"\n    }\n\nPretty simple once you get the hang of it :)",
                  "score": 2,
                  "created_utc": "2026-01-19 21:44:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0grhy6",
          "author": "shisologic",
          "text": "You can test the migration from ses v1 to ses v2.\n\nIf you can't create v1 using terraform, you can deploy it via AWS CLI ses (not sesv2).",
          "score": 1,
          "created_utc": "2026-01-19 12:28:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qg1udo",
      "title": "Centralized CI/CD security scanning for 30+ repos. Best practices?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qg1udo/centralized_cicd_security_scanning_for_30_repos/",
      "author": "_1noob_",
      "created_utc": "2026-01-18 07:06:54",
      "score": 5,
      "num_comments": 5,
      "upvote_ratio": 0.78,
      "text": "Hi everyone,\n\nWe are currently working on integrating CI/CD security tools across our platform and wanted to sanity-check our approach with the community.\n\nWe have 30+ repositories in bitbucket and are using AWS for CI/CD. \n\nWhat we are trying to achieve:\n\n* A centralized or shared pipeline for security scanning (SAST, SCA, Container Scanning, DAST).\n* Reuse the same scanning logic for all the repos \n* Keep pipelines scalable and maintainable as the number of repos grows.\n\nThe main challenge we are facing:\n\n* Each repository has different variables for SAST (eg sonarqube) \n\nQuestions:\n\n* Is it a good practice to have one shared security pipeline/template used by all repos for scanning?\n* How do teams typically manage repo-specific variables and Sonar tokens when using shared pipelines?\n* Any real-world patterns or pitfalls to watch out for at this scale (30+ pipelines)?\n\n\n\nAgain, goal is to keep security enforcement consistent without over-coupling pipelines as possible. \n\nWould really appreciate hearing how others have solved this in production.\n\nThanks in advance.",
      "is_original_content": false,
      "link_flair_text": "ci/cd",
      "permalink": "https://reddit.com/r/aws/comments/1qg1udo/centralized_cicd_security_scanning_for_30_repos/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o09995s",
          "author": "no1bullshitguy",
          "text": "My first question would be: Why do you want it separate? Things like SAST / DAST should ideally be part of the individual application pipeline. Then only you can fail the pipeline when the code does not hit a particular baseline you set, let it be Code Quality (Sonarqube), SAST / DAST etc.\n\nNow, having said that,  I have implemented the other way also, like a Central Pipeline. The way I did is, all the fields are parametrized with fields for example Application ID for that particular application in our scan tool, Packaged Artifact URL for doing Opensource Library Scan, GIT Repo URL for downloading source for SAST ,  Branch to name a few (it has been 5 years so I dont remember most)\n\nThen the application build pipeline will trigger the scanning Job by calling REST API of CI/CD tool with above parameters filled in. Things like Artifact URL, GIT Repo URL etc would be already available as environment variables. But application ID for Scanning tool, i had to set it manually as a parameter for each pipeline (Devs will fill it, and I just had to give the template)\n\nAPI key for your Scanning tool would be most probably global key. This key can be stored as a variable in the central pipeline itself\n\nI have scaled both the above models for 1000+ pipelines. Works well, but I strongly suggest you to keep these scans and Quality gates as part of the actual build pipeline itself. It can go in parallel with rest of the stages not affecting deployment times. Because at some point, you would want to break the pipelines when code quality goes south.",
          "score": 1,
          "created_utc": "2026-01-18 07:51:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0a7ylt",
          "author": "XohleT",
          "text": "I am working on the same problem but for github. In our company we have 2000+ repositories and a lot of variation in pipelines due to not standardising from the start. \n\nThis makes it hard to enforce a single pipeline for everyone. If we do create one it is up to us to make sure it works for everyone which is a burden we rather not take on. \n\nSo we decided to decouple enforcement from scanning. In github we can create rulesets that require certain scanning tools to have checked the repository before a PR can be merged. We use this for enforcement while providing pipeline templates and private github actions to help implementation of scanning tools. \n\nThis makes it easier to start enforcement while not being a burden because teams can do their own implementation if ours don’t work. \n\nFor scanning tools that dont integrate with github rulesets checks we have created our own tool to check if the scanning is sufficient.",
          "score": 1,
          "created_utc": "2026-01-18 12:58:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0bfrty",
          "author": "jefoso",
          "text": "I don't know if it's an approach that I'd follow. IMHO it's too late to fail.\n\nMost of these security/quality checks should happen at the left(the beginning) of the development so the earlier it fails the faster and cheaper it is to fix.\n\nI believe that: \n- developers shoulduse linters, pre-commit hooks, things that are cheaper to run and get possible issues locally \n- feature branches should also do some part of the job and execute more complex tools/scans\n \nCentralized tools should be part of the process, the company would have a release process where everyone agrees that if these integration tests or security scans fails, that feature would be removed from the release or the entire release would be blocked.\nI think this is not just about tools and how to implement them, but also how the company and teams works.",
          "score": 1,
          "created_utc": "2026-01-18 16:53:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0fbjet",
              "author": "_1noob_",
              "text": "we are also using pre-commit hooks with a decent configuration.",
              "score": 1,
              "created_utc": "2026-01-19 05:01:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0i396l",
          "author": "CodacyKPC",
          "text": "Hello, I'm Kendrick, VP of Technology at Codacy. I would say: use Codacy!\n\nWe connect to your Bitbucket directly and use webhooks to listen for changes and then scan the diff when you submit a PR. We do the scanning on our side in our cloud engine so there's no CI/CD configuration for you to have to handle. You can create multiple overlapping \"coding standards\" that can apply to whichever repositories you want, so can create e.g. a \"baseline security\" standard and a \"javascript standard\" and a \"frontend team standard\". \n\nThen you can gate merging of code into your main branch based on whether the Codacy checks passed in the PR.\n\nExtra plus: we have an IDE extension that will run the same checks locally so that by the time your devs get to the PR they should have already resolved all of the issues.\n\nExtra extra plus: the IDE extension \\_forces\\_ AI coding agents to resolve issues in their workflow, before they hand back control to the dev, so issues can get fixed without developers even knowing about them.\n\nYes, this was an advert. It still seemed relevant. Do DM me and we'll set you up with a free trial.",
          "score": 1,
          "created_utc": "2026-01-19 16:40:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qg7a1g",
      "title": "Moving to CloudFormation with Terraform/Terragrunt background, having difficulties",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qg7a1g/moving_to_cloudformation_with_terraformterragrunt/",
      "author": "hardvochtig",
      "created_utc": "2026-01-18 12:24:11",
      "score": 5,
      "num_comments": 26,
      "upvote_ratio": 0.65,
      "text": "Hi all, I'm used to Terraform/Terragrunt when setting up infra and got used to its DRY principles and all. However my new company requires me to use CloudFormation for setting up a whole infra from scratch due to audit/compliance reasons. Any tips? Because upon research it seems like everybody hates it and no one actually uses it in this great year of 2026. I've encountered it before, but that's when I was playing around AWS, not production.\n\nI've heard of CDK, might lean into this compared to SAM.\n\n[](https://www.reddit.com/submit/?source_id=t3_1qg79f4)",
      "is_original_content": false,
      "link_flair_text": "CloudFormation/CDK/IaC",
      "permalink": "https://reddit.com/r/aws/comments/1qg7a1g/moving_to_cloudformation_with_terraformterragrunt/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0a40s3",
          "author": "Sirwired",
          "text": "CDK generates CFn underneath, but it's still very different from TF. (CDK is a way to use Python, Java, TypeScript, JavaScript, C# and Go to generate CFn.)\n\nI have to wonder what audit reasons require them to use CFn directly.",
          "score": 22,
          "created_utc": "2026-01-18 12:29:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a4r2p",
              "author": "kei_ichi",
              "text": "All I can think is “no external tools other than AWS native tools” bull sh*t",
              "score": 8,
              "created_utc": "2026-01-18 12:34:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0a71m7",
                  "author": "hardvochtig",
                  "text": "Well, yes!",
                  "score": 4,
                  "created_utc": "2026-01-18 12:52:18",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0a70cm",
              "author": "hardvochtig",
              "text": "For easy drift detection and everything is managed by AWS including the state.",
              "score": 4,
              "created_utc": "2026-01-18 12:52:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0cyi21",
                  "author": "dr_barnowl",
                  "text": "The fact that you're forced to use the AWS backend of CloudFormation is likely the reasoning for this ; but there's no real reason why you can't set up a Terraform based IaC backend with the same constraints.\n\nIf you want full auditablility, the solution is the same ; log all the cloudtrail traffic and use that as the basis for any audit. The result is the same - the console, CF, and terraform, all use the same APIs.\n\nIf you want control, you have to do just as much work to do to prevent console meddling.\n\nDrift detection? TF can do that. State? You can put the state in an S3 bucket with strong controls on it.\n\nAll the attitudes preferring CloudFormation to Terraform I've seen ... seem to be rooted in dislike for the idea of programming. The only advantages CF has over TF in my book are that you have a prebuilt IaC setup, and lots of code samples. The rest ... CF is hard to grow, hard to refactor, and a PITA to modularize. And slow, because dependency graphs in CF treat stacks as an entire atomic unit, you can't update anything with an input that depends on an output until the whole stack is done.",
                  "score": 0,
                  "created_utc": "2026-01-18 21:17:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0coxcb",
              "author": "Davidhessler",
              "text": "Possibly CloudFormation Hooks / Guard. There are a lot of teams implementing controls these days using those techniques.",
              "score": 3,
              "created_utc": "2026-01-18 20:27:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0a43ra",
          "author": "mrsmiley32",
          "text": "I'd go CDK and then synth the cloudformation from that for the audit purposes. CDK is much nicer to work with than cloudformation.",
          "score": 14,
          "created_utc": "2026-01-18 12:29:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a73zn",
              "author": "hardvochtig",
              "text": "So I’ve heard! Definitely the top option, aside from asking them if CF is really necessary",
              "score": 3,
              "created_utc": "2026-01-18 12:52:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0aogh8",
                  "author": "AntDracula",
                  "text": "CDK generates CF",
                  "score": 3,
                  "created_utc": "2026-01-18 14:39:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0akqjl",
          "author": "TheCamerlengo",
          "text": "I used cloud formation and code pipeline for years. I became rather proficient at it. Took a while. It’s awkward and difficult to debug when things go wrong. It taught me a lot about how AWS IAC provisioning really works. \n\nNew job uses terraform and after 1 day I never looked back. Terraform is light years easier to use but it is an abstraction. Understanding how cloud formation works helped me fill some gaps when using terraform. \n\nBasically you are going to hate cloud formation coming from terraform, but may learn something about how AWS and terraform work under the hood.",
          "score": 2,
          "created_utc": "2026-01-18 14:18:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ba0dr",
          "author": "ycarel",
          "text": "What are you struggling with?\nSince I don’t know what your pain points are it is hard to give helpful tips.\nThe only high level thing I have for you is to find the tools provided by your IDE for cloud formation. It will provide auto complete, etc. \nUse cfn-nag to help catch errors. \nThink of cloud formation as a low level language where you have to be super detailed.\nWhen deploying use change sets to help you know exactly what is changing.\nAs many have recommended consider CDK as it is a higher level construct that compiles into CFN.",
          "score": 2,
          "created_utc": "2026-01-18 16:26:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0e9wwg",
          "author": "dafim",
          "text": "I'm going to get asked to leave over this opinion but here goes.\n\nTf sucks. Every new company or even department you go to has wildly different ways to do tf. You can \"learn\" tf, then move to another company and have no idea what TF is going on. There are more escape hatches in tf that it can be baffling how it even got a name for itself as iac. There is some real shit out there. It's like the little tikes learning kit for iac. In some ways it's the new perl vs python argument of \"there's more than one way to do it\" in perl vs \"there's only one way to do it\" in Python. Which I guess means in some ways it's a derivation of the (big|little) endiness argument.\n\nThe nicer thing about cf is that it's a bit more predictable (outside of using custom resources, etc) and as long as you're not naming things you can pick it up and deploy it in a similar account for dev or stage, etc. it's concept of \"state\" is the not defined by some file you need to keep track of, it's state is it and it's resources existence. It's also not as clever or fearureful as tf, which when you manage huge amounts of resources in huge amounts of stacks across huge amounts of accounts across modest amounts of departments can be a very very nice thing because you can still fit the concept in your head. And by cf I mean cdk and cf.",
          "score": 2,
          "created_utc": "2026-01-19 01:20:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jukjr",
              "author": "DaWizz_NL",
              "text": "I agree with the gist of it. I do think certain specific service implementations suck a bit with CFN, as every service team is responsible for this themselves. The ChangeSets are also lacking and Drift detection is an afterthought, while on TF these are first class citizens.\n\nOther than that, TF can indeed become really messy. It's also much harder to govern from an enterprise perspective. Multi-account is also very painful when you are talking about scalable platforms with ephemeral accounts and stuff, but I think they just solved the multi-region clunkiness at least.",
              "score": 1,
              "created_utc": "2026-01-19 21:29:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0a7o8t",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-18 12:56:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cnrff",
              "author": "TheP1000",
              "text": "Cdktf is dead last I saw.",
              "score": 1,
              "created_utc": "2026-01-18 20:21:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0aat83",
          "author": "pipesed",
          "text": "As others have pointed out, cdk is the programmatic way to compose cloud infrastructure. It does synth cfn templates, and it's code so it is as controllable and auditable as any other code. \n\nTerraform is still the most popular one we see, followed by cdk typescript.",
          "score": 1,
          "created_utc": "2026-01-18 13:18:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0cj098",
          "author": "oneplane",
          "text": "\\> due to audit/compliance reasons\n\nI highly doubt that is really going to 'require' CloudFormation.\n\n\\> Any tips?\n\nSadly, no. While they are both IaC tools, they are rather different in how they work and view implementation choices. Besides informing about the tech stack next time you interview at a company, there isn't much you can do as even  TFCDK (TF-to-Cfn) is dead at this point. If you are a software engineer, you can use the Cfn CDK, but unless it's some sort of useless checkbox compliance (well, most are) that might not fly.",
          "score": 1,
          "created_utc": "2026-01-18 19:58:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0f99bs",
          "author": "Dry_Raspberry4514",
          "text": "I don't understand the hate for CF. It helped us to solve the biggest problem in DevOps space -Stateless IaC.\n\nTerraform has two providers for AWS - aws and awscc. awscc uses cloud control api under the hood which in turn leverages most the stuff from CF excluding stack.\n\nIf you want support for new or updated aws resource types on day 1, you will need awscc which has dependency on CF indirectly as explained above. There have been cases in the past (and it will continue in future as well) where new aws features (e.g. regional NAT gateways) were added to aws provider after weeks when it was available in awscc provider on day 1 through AWS CC API. Unlike terraform, we use only CC API and have not seen any issue with it so far.",
          "score": 1,
          "created_utc": "2026-01-19 04:45:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jt505",
          "author": "DaWizz_NL",
          "text": "For platforms, I always use CFN. It's fine for Lambdas, S3, SNS, SQS, networking components, etc.. It sucks when you have to deploy a shitload of application components and ECS deployments often are painful with CFN if you don't know what you're doing.",
          "score": 1,
          "created_utc": "2026-01-19 21:21:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0a7wdv",
          "author": "Kitchen-Location-373",
          "text": "tbh terragrunt is way way way less DRY for me than sceptre for cloudformation. I get it's a bigger community but terragrunt always turns into spaghetti code meanwhile for cloudformation I usually have like a three line yaml config file per environment",
          "score": 1,
          "created_utc": "2026-01-18 12:58:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a86vc",
              "author": "hardvochtig",
              "text": "For some reason this is the first time I’ve heard of Sceptre despite researching for the past 3 hours. All I keep seeing is CDK. I’ll read more on this, thanks!",
              "score": 1,
              "created_utc": "2026-01-18 13:00:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0ajgiq",
                  "author": "wunderspud7575",
                  "text": "Sceptre is a poor man's Stacker. Stacker really should have been more successful. Sceptre is junk.",
                  "score": 1,
                  "created_utc": "2026-01-18 14:11:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0dnpfx",
              "author": "SnoopJohn",
              "text": "If you do terragrunt well it can end up very clean and make it really simple to ensure all environments are the same as the use the same module with just differences in the env vars",
              "score": 1,
              "created_utc": "2026-01-18 23:22:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qiycaa",
      "title": "I Created One Site to Check Any AWS Lambda Event Payload",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qiycaa/i_created_one_site_to_check_any_aws_lambda_event/",
      "author": "[deleted]",
      "created_utc": "2026-01-21 13:48:19",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 0.73,
      "text": "One Ring to rule them all\"\n\nI built a very simple and straightforward website to look up the payloads for each service that AWS Lambda can receive (through the event variable).\n\nIt is a simple piece of information, but the fact that we have to navigate through AWS documentation to find each payload, and that this information is not available on a single page, is quite frustrating for anyone who frequently builds Lambda functions.\n\nNot all services are covered yet, but I plan to complete them by the end of the month.\n\nNext week, I will also make the project open source.\n\nCompletely free :)\n\nI don't know if something similar is already in use by the community, but there you go:\n\nhttps://lambda.clis.codes/\n\nI miss websites that are simple and minimalist, that only display information and perform one action, but that actually help the professional: like gitignore.io\n\nI'm trying to create a opensource platform that has these \"minimalist mini-tools\": CLIs & Codes.\n\nBut that's a conversation for another time :)",
      "is_original_content": false,
      "link_flair_text": "serverless",
      "permalink": "https://reddit.com/r/aws/comments/1qiycaa/i_created_one_site_to_check_any_aws_lambda_event/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0ur2t4",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'serverless'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+serverless).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-01-21 13:48:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11ymgf",
          "author": "workmakesmegrumpy",
          "text": "The go doc for lambda events package lay it all out but really what’s a pain in the ass is that I can’t generate sample events easily",
          "score": 2,
          "created_utc": "2026-01-22 14:45:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16fzjh",
              "author": "OkSadMathematician",
              "text": "yeah sample event generator would be clutch. aws sam cli has some but limited",
              "score": 2,
              "created_utc": "2026-01-23 04:05:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qhx8d0",
      "title": "Looking for feedback for my CDK approach",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qhx8d0/looking_for_feedback_for_my_cdk_approach/",
      "author": "thexavikon",
      "created_utc": "2026-01-20 10:07:03",
      "score": 5,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "I usually work on small projects that share the same AWS stack (dynamodb, lambda, cognito, sqs, s3).\n\nI made a starter template for myself to standardize that.\n\nLooking for feedback if this is a good approach, or if there are better way to do this.   \nI have read people criticizing CodePipeline. Should I move to Github actions instead for the CI/CD pipeline?\n\nHere's the repo: [https://github.com/rohankshah/cdk-starter-template](https://github.com/rohankshah/cdk-starter-template)",
      "is_original_content": false,
      "link_flair_text": "technical resource",
      "permalink": "https://reddit.com/r/aws/comments/1qhx8d0/looking_for_feedback_for_my_cdk_approach/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0n6nms",
          "author": "TurboPigCartRacer",
          "text": "if you plan on sharing it as a starter template i would focus on the structure and configuration of the tools that make a starter worth using instead of supplying it with random constructs (maybe one construct to show as example would be fine). codepipeline for ci/cd is really niche, if you plan on hosting the repo in github it might make more sense to utilize what github has to offer with actions.",
          "score": 1,
          "created_utc": "2026-01-20 10:22:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0naddl",
              "author": "thexavikon",
              "text": "Thank you for your feedback! I guess it's time to move away from code pipeline. I have noticed more people are using Github Actions.",
              "score": 1,
              "created_utc": "2026-01-20 10:55:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0nosm1",
                  "author": "cachemonet0x0cf6619",
                  "text": "this would be my preference. if you need to run your pipeline in your account you can use codebuild as a github actions runner.",
                  "score": 1,
                  "created_utc": "2026-01-20 12:46:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0npz1x",
          "author": "cachemonet0x0cf6619",
          "text": "This is a good start. my only two points of feedback is that you might want to consider separating stacks by volatility. think about separating into stateful and stateless. stateful is things like your db table and your queue. stateless would be your lambdas. \n\nyou have two choices with that. nested stacks or using string parameter names and import existing resources. \n\ntwo, your constructs aren’t composable at all. your queue construct doesn’t allow me to configure it. allow a user to pass some things into the construct like the visibility timeout or something. \n\noh, and finally don’t name anything. you force a name into your queue. names are for pets. use tags for that stuff and let cdk create dynamic names for you. this makes replacement easier in the future.",
          "score": 1,
          "created_utc": "2026-01-20 12:53:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nzuuv",
              "author": "thexavikon",
              "text": "Thank you so much! This is very useful for me. \n\n>you have two choices with that. nested stacks or using string parameter names and import existing resources.\n\nI'm not really sure how to go about this.\n\nBut point 2 and 3 are something I will definitely implement.",
              "score": 1,
              "created_utc": "2026-01-20 13:51:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0odd17",
                  "author": "cachemonet0x0cf6619",
                  "text": "look it up. what’s the tradeoffs of using cdk nested stacks and using string parameters to share resources. you’ll want an educated answer to this question",
                  "score": 1,
                  "created_utc": "2026-01-20 15:02:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qgoay2",
      "title": "AWS EKS via terraform - cni plugin not initialized",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qgoay2/aws_eks_via_terraform_cni_plugin_not_initialized/",
      "author": "Meganig",
      "created_utc": "2026-01-18 23:50:30",
      "score": 5,
      "num_comments": 15,
      "upvote_ratio": 0.86,
      "text": "Ok, I am about to rip my hair out over this...I have been trying to create this eks cluster for a while and I have been stuck on this.  TF node group takes 30+ minutes than fails.  I go into the console and the nodes are showing errors.  I use k9s to connect to the cluster, there are no pods created.  The node description shows this:\n\n\\`\\`\\`\n\n│ Ready False Sun, 18 Jan 2026 18:10:45 -0500 Sun, 18 Jan 2026 18:10:33 -0500 KubeletNotReady │ │ container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin │ │ returns error: cni plugin not initialized\n\n\\`\\`\\`\n\nHere is my latest TF:\n\n[https://github.com/sPrime28/eks-test](https://github.com/sPrime28/eks-test)\n\nWhat could I be missing?\n\n  \nedit:\n\nno addons showing in the cluster:\n\naws eks list-addons --cluster-name <cluster-name> --region us-east-1\n\n{\n\n\"addons\": \\[\\]\n\n}",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1qgoay2/aws_eks_via_terraform_cni_plugin_not_initialized/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0dwpny",
          "author": "Freedomsaver",
          "text": "You are not specifying any EKS-managed addons to install in the input variables of the EKS module, thus none are installed.\n\nNo CNI addon = no networking = none of the nodes become healthy and the node group creation times out.\n\nSee the documentation, examples and input variables of the module for more information: https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest#eks-managed-node-group\n\nPS: In the past, the vpc-cni addon was installed on EKS clusters by default, but this has changed a long time ago ago and all addons need to be explicitly configured.",
          "score": 5,
          "created_utc": "2026-01-19 00:09:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0dx5x5",
              "author": "Meganig",
              "text": "I have them specified in addons: [https://github.com/sPrime28/eks-test/blob/main/addons.tf](https://github.com/sPrime28/eks-test/blob/main/addons.tf)\n\n  \nam i doing this wrong?",
              "score": 1,
              "created_utc": "2026-01-19 00:12:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0dyaun",
                  "author": "Freedomsaver",
                  "text": "Specify at least the vpc-cni addons in the module, not separately. Look at the examples from the module documentation.\n\nAs mentioned above, the node groups can't be successfully created without a cni addon. Since your separate aws_eks_addon resources depend_on the EKS module, Terraform will never provision them.\n\nJust read the Terraform output and you will see what resource has actually been created and where it is hanging/waiting until a timeout/failure.",
                  "score": 4,
                  "created_utc": "2026-01-19 00:18:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0dur7b",
          "author": "jaybrown0",
          "text": "In the console, in EKS, take a look at the add-ons and see if they show any errors or logs that are helpful.",
          "score": 1,
          "created_utc": "2026-01-18 23:59:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0dv4qn",
              "author": "Meganig",
              "text": "it shows no addons when I run:\n\naws eks list-addons --cluster-name <cluster-name> --region us-east-1\n\n{\n\n\"addons\": \\[\\]\n\n}",
              "score": 1,
              "created_utc": "2026-01-19 00:01:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0dwumq",
          "author": "jaybrown0",
          "text": "If you apply your TF again, does that install the add-ons?\n\nAlso, take a look at pod identity in place of IRSA",
          "score": 1,
          "created_utc": "2026-01-19 00:10:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0dxbq0",
              "author": "Meganig",
              "text": "I just destroyed and doing it again...it takes like 30 min lol.  Will report back, but I have tried it a few times and the addons never show.",
              "score": 1,
              "created_utc": "2026-01-19 00:12:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0dzj7g",
          "author": "bryantbiggs",
          "text": "When you look at our documentation, is there something not clear? What is the motivation for defining addons outside the module (there are drawbacks for addons like VPC CNI and EKS Pod Identity)",
          "score": 1,
          "created_utc": "2026-01-19 00:24:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0e6igg",
              "author": "Meganig",
              "text": "I initially had it in the eks module.  I moved it out because copilot recommended it :)  Let me try it again.",
              "score": 1,
              "created_utc": "2026-01-19 01:01:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0hcgn7",
                  "author": "spicypixel",
                  "text": "This is going to be the most amusing answer in the thread.   \nSo why would you do it? Why did you take the advice from something with no skin in the game - you're the one who's on the hook to get it done.",
                  "score": 1,
                  "created_utc": "2026-01-19 14:34:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0ea88q",
              "author": "Meganig",
              "text": "Updated the TF but still have the same issue.",
              "score": 1,
              "created_utc": "2026-01-19 01:22:28",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0ez6xe",
              "author": "Meganig",
              "text": "I have also removed the node group for now.  I have narrowed it down to the addons not being installed via TF.  I can go into the aws console and add them manually.  IDK why it doesnt work with TF yet.",
              "score": 1,
              "created_utc": "2026-01-19 03:39:49",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0f230u",
              "author": "Meganig",
              "text": "update: When I remove the node group it works.  However coredns and aws-ebs-csi are degraded.  So I think I will try to remove those for now and have the node group depend on the cluster, then install those two separately after the node group.",
              "score": 1,
              "created_utc": "2026-01-19 03:57:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0gqk9i",
                  "author": "iamtheconundrum",
                  "text": "They are degraded because there are no nodes to launch the necessary pods on.",
                  "score": 1,
                  "created_utc": "2026-01-19 12:21:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qgkswa",
      "title": "Migrating scheduled jobs to ECS",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qgkswa/migrating_scheduled_jobs_to_ecs/",
      "author": "Character_Status8351",
      "created_utc": "2026-01-18 21:28:24",
      "score": 5,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "Background: Moving about 8 C# apps from Windows Task Scheduler to AWS\n\nMost of these apps fetch data from the same db(sql server), preform some business logic and update data.\n\nSome questions I have:\n\n1. Should each scheduled task handle everything start to finish, or do people break it up? Like having one ECS task fetch work items and queue them, then separate tasks to actually process them?\n2. One repo per job or throw them all in a monorepo?\n3. Does everyone just use CloudWatch and the ECS console to manage jobs or a third party tool(preferably open source)? \n4. What's the standard approach for retries? CloudWatch alarms + SNS?",
      "is_original_content": false,
      "link_flair_text": "general aws",
      "permalink": "https://reddit.com/r/aws/comments/1qgkswa/migrating_scheduled_jobs_to_ecs/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o0d2sk6",
          "author": "NoMoreVillains",
          "text": "Is ECS a hard requirement? Because lambdas would likely be better suited and likely much cheaper",
          "score": 6,
          "created_utc": "2026-01-18 21:41:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0dag0j",
              "author": "Character_Status8351",
              "text": "shoot forgot to mention this. Some jobs can definitely be lambda others will pass that 15 min threshold. But lets say half can be converted to lambdas.\n\nWhat then? One repo per job or mono? My job doesn't really have a set standard yet .. that's what I am trying to do.",
              "score": 1,
              "created_utc": "2026-01-18 22:17:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0ex5x9",
                  "author": "NoMoreVillains",
                  "text": "We use a monorepo where each task is its own separate folder. We use GitHub Actions to handle deployment, configuring it to only run deployments if any changes occurred within that folder. It just makes handling massive projects easier is you don't have to pull down tons of different repos.\n\nPlus we use docker for the local infra, and a monorepo makes it easy because you can just have the docker compose file in the parent folder",
                  "score": 2,
                  "created_utc": "2026-01-19 03:27:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0d2pd6",
          "author": "burlyginger",
          "text": "    1. Should each scheduled task handle everything start to finish, or do people break it up? Like having one ECS task fetch work items and queue them, then separate tasks to actually process them?\n\nI wouldn't arbitrarily break up tasks. ECS tasks can run as long as you like. No need to add complexity.  \n\n\n    2. One repo per job or throw them all in a monorepo?\n\nThis is really up to you and how your org works. Monorepos are against my religion, but do what your org already is doing. \n\n\n    3. Does everyone just use CloudWatch and the ECS console to manage jobs or a third party tool(preferably open source)? \n\nNo, console is never a good option. We use Terraform. \n\n    4. What's the standard approach for retries? CloudWatch alarms + SNS?\n\nEvent -> SNS -> SQS (with a dead letter queue)\n\nYou can write alarms on dlq messages.",
          "score": 1,
          "created_utc": "2026-01-18 21:41:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0d50wm",
          "author": "Severe_Marketing651",
          "text": "How long are these jobs? If < 15 minutes lambda + event bridge might be the better tool for the job",
          "score": 1,
          "created_utc": "2026-01-18 21:52:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0da2mp",
          "author": "Scared_Mortgage_176",
          "text": "I did this exact same migration a few months ago. I just use the scheduled job in ECS and run them on Fargate. Reason we don’t use lambda is it has a 15 min timeout. \n\nRegarding the code structure, we have a monorepo, share all the db logic,",
          "score": 1,
          "created_utc": "2026-01-18 22:16:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hrtg6",
          "author": "SpecialistMode3131",
          "text": "1. Make a step function for anything complex, otherwise yes just one thing per job. default one job per job. The real criterion IMO is \"does this fail enough to even consider complexity? If not, shove it in a can and get on with your life.\"\n2. Monorepo.  This is so little code it's goofy to overthink it.\n3. eventbridge scheduler.  Cron for the cloud.\n4. See 3. has retry built in.  Yes, alarm if your retries aren't working out.\n\nTBH if some of them are unsuitable for Lambda, I wouldn't even bother with Lambda at all.  Why have a complex stack when you can use one tool?  Just use ECS and fargate and be done with it.\n\n  \nConsider app2container for this.",
          "score": 1,
          "created_utc": "2026-01-19 15:49:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}