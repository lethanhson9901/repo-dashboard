{
  "metadata": {
    "last_updated": "2026-02-22 08:50:00",
    "time_filter": "week",
    "subreddit": "aws",
    "total_items": 20,
    "total_comments": 161,
    "file_size_bytes": 200637
  },
  "items": [
    {
      "id": "1r8ehng",
      "title": "Everyone says \"tag your resources\" for cost control. Nobody explains how to actually do it well.",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r8ehng/everyone_says_tag_your_resources_for_cost_control/",
      "author": "alex_aws_solutions",
      "created_utc": "2026-02-18 20:35:12",
      "score": 101,
      "num_comments": 59,
      "upvote_ratio": 0.88,
      "text": "Every AWS cost optimization post says the same thing: \"tag your resources, use Cost Allocation Tags.\" Great advice, very helpful, thanks.\n\nBut after 18 months of cleaning up a pretty messy AWS setup I realized that having tags is not the hard part. The hard part is having the right tags in a structure that actually tells you something useful. We went from \"yeah we tag stuff\" to genuinely understanding our spend down to the feature level, and the difference is night and day.\n\nHere's what worked for us.\n\n**Three mandatory tags, everything else optional**\n\nWe use exactly three required tags on every resource:\n\n* **Environment**: prod, staging, dev and sandbox. Obvious but you'd be surprised how many things don't have this.\n* **Service**: this is YOUR service, not the AWS service. So not \"RDS\" but \"payment-processor\" or \"user-api\" or \"data-pipeline\". This is the one that matters most.\n* **Team**: who owns this when it breaks at 2am. Also who gets asked when the cost spikes.\n\nThe key insight for us was Service. We used to tag by AWS product type which told us basically nothing we didn't allready know from Cost Explorer. Once we started tagging by our own service names, everything changed. A single Service:payment-processor tag now spans the ALB, the ECS tasks, the RDS cluster, the SQS queues. I can see what it actually costs to run payments across all infrastructure, not just what individual resources cost in isolation.\n\n**Why only three**\n\nWe started with 12 required tags. Compliance was maybe 40% at best. People just didn't bother or tagged inconsistently. Dropped to 3 mandatory + 5 optional and we're at around 95% now. Turns out people will actually do it if you keep it simple.\n\n**Enforce tagging at creation, not with angry Slack messages**\n\nThis was probably our biggest lesson. We handle this on two levels now:\n\n1. We use OPA policies with Terraform now (see picture). If a resource doesn't have the three mandatory tags, the apply just fails. No exceptions, no \"I'll add it later\". Retroactive tagging is a nightmare and honestly a waste of everyones time.\n2. At the AWS Organization level with SCPs, they block the creation of resources that don‚Äôt include those tags. This covers cases where someone spins up resources manually in the console, through the CLI or SDK, outside of terraform.\n\nWe spent almost two weeks tagging old resources manually before we accepted it would have been cheaper to just let them expire and recreate them properly. If you're early enough, enforce from day one. If you're late, don't try to fix everything, just enforce going forward and let the old stuff cycle out.\n\n**The report that actually gets read**\n\nWe have a simple monthly report that flags any service where cost went up more than 30% month over month. The catch is this only works if tagging is consistent, which is why enforcement matters so much.\n\nWhen payment-processor jumps from $800 to $2,400, thats a conversation worth having. And it‚Äôs a very different conversation than \"our EC2 bill went up\". Finance doesn't care about EC2 vs Lambda. They want to know what business capability costs what and whether the increase makes sense. \"The recommendation engine doubled because we shipped a new model\" is an answer people can actually work with.\n\n**The unsolved problem: shared infrastructure**\n\nThe one thing we still don't have a clean answer for is shared resources. Databases that serve multiple services, shared Redis clusters, that kind of thing. Right now we tag those with the primary consumer and accept it‚Äôs not perfectly accurate. Looked into split cost allocation tags but honestly it felt like over-engineering for our size.\n\nCurious how others handle this. Anyone have a tagging strategy that actually survived contact with reality? Especially for shared infrastructure.",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1r8ehng/everyone_says_tag_your_resources_for_cost_control/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o64h47r",
          "author": "ryancoplen",
          "text": "IMHO, while tags can be helpful its not the end all be all, specifically because of the issues you called out at the end: shared \"stuff\".\n\nInstead of trying to use tags and assigning ownership that way, I massively prefer to separate everything by account.\n\nYou bring up Environment, Service and Team as ways that you want to enforce control. This is backwards from my preference.\n\nTeams are the top level of ownership and accountability. AWS accounts should be owned by a single team. What happens in those accounts is the responsibility of the owning team.\n\nServices should be owned by a single team, and they should run in accounts owned by those teams.\n\nStages (Dev/Test/Prod) are how Services are deployed. Each stage of a service should have its own account. This makes sure that someone whacking an IAM role in your devo environment doesn't somehow impact your Prod envionment which is (stupidly) running  in that same account.\n\nI'd add that regional distribution is another thing that should also be happening in their own accounts. So your NA prod environment for Service A is in a different account than the EU prod environment for ServiceA.\n\nBreak your infra up by accounts and make one team the operational and financial owner of each account. Then when you (the FinOps person) get a billing alert for an account (or the security team gets an alert for someting), you know exactly who to reach out to and everyone should already understand and agree with how responsibility is distributed.\n\nTeams are free to use tags in their own accounts however they like (or not use them), it doesn't matter to the bigger organization or company.",
          "score": 44,
          "created_utc": "2026-02-18 20:59:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o655aau",
              "author": "rariety",
              "text": "Agree with this, with the caveat that you need to be careful about going too granular and creating too many accounts. Yes it's great for blast radius, but there's a base cost for a new account typically, and if everything needs to be privately networked together, it can get incredibly expensive if your accounts start numbering in the hundreds",
              "score": 5,
              "created_utc": "2026-02-18 22:52:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o657gmy",
                  "author": "ryancoplen",
                  "text": "Yeah, in general I would try to avoid any shared networking requirements. Instead I prefer to use things like ALB, APIGW, SNS, SQS, etc to manage communication across accounts. These constructs make your relationships and dependencies between accounts much more clear.\n\nBut yes, if its impossible to avoid having something like a shared VPC or (heaven forbid) something like Transit Gateway, then yes, there is a cost for having too many accounts.\n\nMy advice would be, **don't do that**! But sometimes you are just saddled with legacy designs and systems and have no choice. At least you can stop making it worse and extending the poor design, but that takes buy-in from leadership.",
                  "score": 4,
                  "created_utc": "2026-02-18 23:03:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o689ws6",
              "author": "camelConsulting",
              "text": "This is the way! I do cloud strategy & lots of FinOps and have come to the same conclusion.\n\nYou need separate accounts anyway for different environments, as you say, and mostly you can group accounts by specific application/service. Accounts attach to dedicated network accounts with shared VPCs.\n\nThen @OP to answer your question, there will be accounts with shared resources. Whether those are things like foundations / network / observability or large shared databases like BQ.\n\nThe way I approach those is to sort those accounts descending by $ spend and work with the platform teams involved to come up with an allocation methodology. You might ‚Äúpeanut butter‚Äù spread core foundational infrastructure like VPCs or even observability, basically assign a % of the cost to applications based on their % of total directly attributable spend and therefore a ‚Äúprobably accurate‚Äù but imprecise mechanism.\n\nBut I‚Äôve also done ‚Äòshared platforms‚Äô like container platforms or shared large databases where you need to ensure there‚Äôs a proper intake associating container apps or DB tables with cost centers or applications. You can then pull operational data like # of x-sized container hours per month as a % of total x-sized container hours to build allocation ratios.\n\nIt gets pretty granular, which is why it is critical to:\n\n1. Start with the most expensive shared accounts and continue descending; and\n2. Make the account owner (I.e. shred platform team) accountable for the spend until that account‚Äôs spend can be allocated to ‚Äúcustomers‚Äù effectively.\n\nYou‚Äôll never hit every single account, but if you do this, the 80/20 rule will help you attribute / allocate the majority of spend with a reasonable output of effort.",
              "score": 2,
              "created_utc": "2026-02-19 12:17:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o68blf5",
              "author": "alex_aws_solutions",
              "text": "I agree with that. That would be the more appropiate choice for a bigger infra. But still need to use the right naming convention for this as well to work, which is almost like tagging.",
              "score": 1,
              "created_utc": "2026-02-19 12:29:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o69br03",
                  "author": "ryancoplen",
                  "text": "The key is that any \"thing\" in AWS is going to be tied to an account. Its usually right there in the ARN. So you only need to maintain a single relationship between account numbers and the \"owning\" team in order to deal with essentially anything.\n\nAnd teams that ignore a naming convention or forget tags or whatever won't impact the ability to link them to the resources that they own.",
                  "score": 2,
                  "created_utc": "2026-02-19 15:51:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o64jldo",
          "author": "SpecialistMode3131",
          "text": "You defined your tag ontology.  Good stuff.  Realize that pretty much every business has a different one, and that's why no one gives you the one-size-fits-all answer since there isn't one.\n\nFor the shared resources, my first instinct is to add \\*all\\* the consumers as tags, and then when costs spike, you know who all's involved.  Then insist on per-service/team metrics (if necessary, custom cloudwatch, but log aggregating works probably pretty well) with a naming scheme that lets you tie them together with the tags, to provide more granular insight.  Really though, this is why backend isn't going to be a solved problem any time soon.  Bespoke solutions for bespoke business.",
          "score": 10,
          "created_utc": "2026-02-18 21:10:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65mkap",
          "author": "katatondzsentri",
          "text": "My advice: never assume that your teams and service ownership will not change. Because it will. Sooner than you expect.\n\nI'd rather go with environment and service tags, and a separate service catalog that (among many others) contains ownership information.",
          "score": 4,
          "created_utc": "2026-02-19 00:25:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69692m",
          "author": "KayeYess",
          "text": "We use two \"golden\" tags .. one for access and the other for cost. These tags (both name and value) are enforced at provisioning time. This is setup when a tenant is onboarded to an AWS account through a highly controlled workflow (many of our accounts have multiple tenants).\n\nA combination of SCP, RCP, ABAC/RBAC policies, CFN Custom Registry, Service Catalog and TF Sentinel are used for enforcement and vertical segmentation.¬†Compliance is also tracked through Governance and FinOPs tools.\n\nThere is no silver bullet solution. AWS provides the tools. Customer has to implement based on their situation, architecture and requirements.",
          "score": 4,
          "created_utc": "2026-02-19 15:24:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65f79d",
          "author": "FransUrbo",
          "text": "Tags for cost control and monitoring is .. dumb!\n\nNo matter how much time you spend on it, you'll never get enough..\n\nBest, imo, is different accounts for everything. With a bit of thougt on cross-account roles, you don't even need SSO and AD etc.",
          "score": 3,
          "created_utc": "2026-02-18 23:45:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64kg0b",
          "author": "Inner_Butterfly1991",
          "text": "None of this seems super bad, but why are you posting AI slop? It's so obvious and so painful to read.",
          "score": 12,
          "created_utc": "2026-02-18 21:14:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64kt2s",
              "author": "Maleficent-Story-861",
              "text": "Accusing everything of AI slop is becoming its own version of slop.",
              "score": -2,
              "created_utc": "2026-02-18 21:16:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o64rthn",
                  "author": "Suspicious-Tough-390",
                  "text": "Yeah but this is",
                  "score": 11,
                  "created_utc": "2026-02-18 21:48:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o64szqp",
                  "author": "nemec",
                  "text": "The post is obviously completely written with LLMs. If there's no way to distinguish your output from a completely fabricated LLM role play... is it worth sharing?",
                  "score": 8,
                  "created_utc": "2026-02-18 21:53:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o65ns0x",
                  "author": "Inner_Butterfly1991",
                  "text": "I could have just said it's horribly written and unnecessarily flowery and doesn't make its point in a concise manner even to the level that if it was a 9th grade essay it would fail. But instead I pointed out what we all know, it came out of an llm, was low effort, and is just written insanely terribly.",
                  "score": 1,
                  "created_utc": "2026-02-19 00:32:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o64fpc6",
          "author": "Mchlpl",
          "text": "Environment: sure\n\nService: yes - as long as these are consistent with how services are actually named in documentation. Think how to automate it.\n\nTeam/owner: meh... this should be in service catalog which you can reference by service name. Makes switching ownership so much easier.\n\n  \nShared resources? Extract into a separate system and tag independently.",
          "score": 3,
          "created_utc": "2026-02-18 20:52:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64kp9q",
              "author": "methods2121",
              "text": "LOL, Owner is the most valuable tag, IMHO, out of these (unless your in accounting) and it's not a person but a distro list or similar , because this is exactly what the team needs when there's a sev1 and the odds that the CMDB is correct or even mapped to a cloud resource is relatively slim.\n\nWe also include the APP\\_ID, (ELUSID in CMDB) because this maps back to the CMDB and during the ATO phase this is checked and validated before deployment is approved.  This then should have all the additional meta data you may need regarding the service/app.",
              "score": 5,
              "created_utc": "2026-02-18 21:15:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o655a2b",
                  "author": "shiftedcloud",
                  "text": "I generally prefer tags to be for things that are unlikely to change because otherwise you have the overhead of changing tags everywhere when those values change.\n\n\nWhen a service's team ownership changes, I don't want to have to modify the IaC for the database.¬†\n\n\nIt's easy enough to have a spreadsheet or table or service catalog that contains the service to team mapping. And it's much easier to modify that mapping when services move to different teams.\n\n\nTo Mchlpl's point, I've had to deal with too many stale tags because no one got around to updating the ownership to want to do that again.",
                  "score": 2,
                  "created_utc": "2026-02-18 22:52:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o64xh7x",
                  "author": "Mchlpl",
                  "text": "Yes, having the way to find owner ASAP is crucial. My experience is that having this information in resource tags leads to incorrect information though, unless you can enforce engineering action (terraform apply) on organisational one (transferring ownership). A service catalog which is the source of truth on who owns what works a lot better in my experience. Just make sure one of your tags is an unambiguous reference to the entry in catalog.",
                  "score": 1,
                  "created_utc": "2026-02-18 22:13:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o64gyul",
              "author": "extreme4all",
              "text": "please elaborate  \n\\> Shared resources? Extract into a separate system and tag independently.",
              "score": 3,
              "created_utc": "2026-02-18 20:58:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o64wr1z",
                  "author": "Mchlpl",
                  "text": "From OP  \n\\> ¬†Databases that serve multiple services, shared Redis clusters, that kind of thing. Right now we tag those with the primary consumer and accept it‚Äôs not perfectly accurate.\n\nTrack it separately and either accept that you can't assign the costs to individual consumers, or work on how to measure actual usage.",
                  "score": 2,
                  "created_utc": "2026-02-18 22:10:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o65pjx3",
                  "author": "NotTooDeep",
                  "text": "Databases are almost always a shared resource. They handle transactions from applications. They provide the data source for reporting databases. The can hold valuable auditing information for cybercrime forensics. They are the target for API calls. \n\nIt makes no sense to try to conform a database to a tag system, with the exception of gross costs. This is actually useful. Making the costs visible can force really interesting questions to be asked, especially about the instance type that's hosting the database.",
                  "score": 1,
                  "created_utc": "2026-02-19 00:42:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o660zwh",
          "author": "ppafford",
          "text": "we use, but I'm looking at other comments to see what I could add/remove/change\n\n```\nproject=\"acme-api\"\ncost_center=\"acme-code\"\nowner_name=\"acme-team\"\nowner_email=\"team@acme.com\"\norganization=\"acme\"\ndepartment=\"billing\"\n```",
          "score": 2,
          "created_utc": "2026-02-19 01:48:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6778mn",
          "author": "T0X1C0P",
          "text": "Thanks for sharing this, I got to learn a few things from this.",
          "score": 2,
          "created_utc": "2026-02-19 06:28:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jxzmv",
          "author": "AlphaToBe",
          "text": "Bro the ONE thing that will silently destroy your tagging strategy and NOBODY warns you about: tag keys are case-sensitive in AWS.\n\n\n`Service` and `service` and `SERVICE` are THREE different tags. They all show up separately in Cost Explorer, they all count against your 500 active cost allocation tag limit, and your 4 teams will absolutely pick different casings because why wouldn't they.\n\n\nI spent a whole afternoon wondering why my cost breakdown numbers didn't add up. Turns out I had `Team`, `team`, and one genius who used `TEAM`. Three versions of the same data, none of them complete. Beautiful.\n\n\nAnd here is the other thing nobody tells you. Cost Allocation Tags have to be MANUALLY activated in the Billing Console before they show up in Cost Explorer. And they are NOT retroactive. So you tag everything today, forget to activate? Last month's data is gone. Forever. Just gone.\n\n\nQuick sanity check, run this and see what tags you actually have activated:\n\n\n```\naws ce list-cost-allocation-tags --status active\n```\n\n\nAnd this one to find resources with missing tags before you waste time in Tag Editor:\n\n\n```\naws resourcegroupstaggingapi get-resources \\\n  --tag-filters Key=Service,Values= \\\n  --query 'ResourceTagMappingList[].ResourceARN'\n```\n\n\nEnforce lowercase-only from day one. Activate your cost allocation tags BEFORE you need the data. Trust me on this one.",
          "score": 2,
          "created_utc": "2026-02-21 05:02:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lzcz6",
              "author": "alex_aws_solutions",
              "text": "That's is absolutely true and an important point if you want to highlight specific tags in your standard reports (CE or CUR). However, if you're creating reports manually, which is often the case when consulting or cleaning up other people's mess, you can still filter by tags if they were set.\n\nThanks for the good contribution!\n\n(Also: it's case-sensitive -> use --status \\[Active, Inactive\\]) ;-)",
              "score": 1,
              "created_utc": "2026-02-21 15:08:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o65um08",
          "author": "edthesmokebeard",
          "text": "AI-format slop.",
          "score": 4,
          "created_utc": "2026-02-19 01:11:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6abbt1",
              "author": "n00lp00dle",
              "text": "reddit has turned into tiktok for medium articles. bullshit listicles that tickle your balls and say what a good engineer you are for having the same opion as chatgpt are completely inescapable.",
              "score": 1,
              "created_utc": "2026-02-19 18:40:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o64ynlv",
          "author": "the_coffee_maker",
          "text": "Where picture?",
          "score": 1,
          "created_utc": "2026-02-18 22:19:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o689pih",
              "author": "alex_aws_solutions",
              "text": "[https://imgur.com/ASxWmvE](https://imgur.com/ASxWmvE)",
              "score": 1,
              "created_utc": "2026-02-19 12:16:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o68foik",
          "author": "ThyDarkey",
          "text": "We do a lot of shared infrastructure as we provision resources for the companies inside our group.\n\nWe just wack on a Company = ALL, there then is some % the finance team then split that recharge back down to those companies. This is all shown in our recharging dashboard in quicksight.",
          "score": 1,
          "created_utc": "2026-02-19 12:56:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64nrpz",
          "author": "JPJackPott",
          "text": "I have a very similar structure to what you describe and it works well. We are single tenanted so service describes the tenant for us. \n\nEach AWS account is a dimension for us too, which describes a logic grouping of customers. \n\nI do two billing breakdowns: one that distributes all ‚Äòunallocated‚Äô costs to the grouping proportionally. Then a second breakdown which breaks each grouping down to tenant level, again folding untagged costs into each tenant proportionally. \n\nNot at the point of making tags mandatory in SCP yet but all deployments are automated so getting the tags in the templates wasn‚Äôt too difficult.",
          "score": 1,
          "created_utc": "2026-02-18 21:29:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64r3cy",
          "author": "blocked_user_name",
          "text": "Thanks this could be helpful.",
          "score": 1,
          "created_utc": "2026-02-18 21:44:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64rnym",
          "author": "religionisanger",
          "text": "We deploy everything in terraform so have git repositories for our resources; it makes it easier to track things like when there‚Äôs a set of load balancers.",
          "score": 1,
          "created_utc": "2026-02-18 21:47:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o650qpv",
          "author": "GoofAckYoorsElf",
          "text": "Add another tag that says where the resource is deployed from if you have no mono repo. Add others that state when the resource has been deployed, from what branch (if on DEV and deployed from an FB), what commit SHA, pipeline ID, who triggered the pipeline... This is all valuable information in the case of a failure.",
          "score": 1,
          "created_utc": "2026-02-18 22:29:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68ycg9",
          "author": "YetMoreSpaceDust",
          "text": "> Nobody explains how to actually do it well.\n\nBut if you want to know how to do it poorly, I can definitely help with that!",
          "score": 0,
          "created_utc": "2026-02-19 14:43:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64jnxr",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -4,
          "created_utc": "2026-02-18 21:10:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6snkt",
      "title": "DynamoDB single-table pattern: SaaS Multi-Tenant with 10 access patterns, 1 GSI (full breakdown)",
      "subreddit": "aws",
      "url": "https://singletable.dev/blog/pattern-saas-multi-tenant",
      "author": "tejovanthn",
      "created_utc": "2026-02-17 01:38:12",
      "score": 64,
      "num_comments": 42,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1r6snkt/dynamodb_singletable_pattern_saas_multitenant/",
      "domain": "singletable.dev",
      "is_self": false,
      "comments": [
        {
          "id": "o5spqqi",
          "author": "cachemonet0x0cf6619",
          "text": "me likes. i don‚Äôt run single table for multi tenant but  if i ever do this will be the reference. \n\ni really liked the site too. only suggestion is that the tables on mobile are tough to read but not a big deal i look forward to more patterns",
          "score": 5,
          "created_utc": "2026-02-17 02:29:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t3xfa",
              "author": "tejovanthn",
              "text": "Thank you :) \n\nWhat patterns would you like to see sooner? üòÅ",
              "score": 1,
              "created_utc": "2026-02-17 03:58:16",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5t400s",
              "author": "tejovanthn",
              "text": "Also, how do you handle multitenant? Separate tables per tenant?",
              "score": 1,
              "created_utc": "2026-02-17 03:58:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5t5j35",
          "author": "finitepie",
          "text": "If i understand you correctly, you are worried, that using an GSI to create a tenant index, could cause a hot partition? But how often do you actually need to make that request? I would just create a {pk TENANT#<tenant-id>, sk:METADATA} for each tenant, that has a property TYPE=TENANT and use a GSI to query for the type, to get a full tenant list or something similiar. But would be more worried, that your general pk/sk design leads to hot partitioning, since your pk is always the tenant id, and the pk determines the partition. What I do is, to break it down into subcategories. like {pk: TENANT#<tenant-id>#USER, sk: <user-id>} or {pk: TENANT#<tenant-id>#PROJECT, sk: <project-id>}. That would be already two distinct paritition, instead of a single one by just using the pattern {pk: TENANT#<tenant-id>, sk: PROJECT#<project-id>}, where¬†all items for that tenant compete for the same¬†1,000 WCU / 3,000 RCU per-partition throughput limit. That works also well for me, since i usually have dedicated api routes for subcategories. What is your security model to enforce tenant isolation?",
          "score": 4,
          "created_utc": "2026-02-17 04:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t8673",
              "author": "tejovanthn",
              "text": "Great points ‚Äî here's my thinking:\n\nTenant index isn't really a worry because it's an admin operation with low volume. I've handled this with the TENANT\\_LIST GSI, but your TYPE=TENANT approach seems functionally similar.\n\nSplitting PKs into subcategories is an interesting approach - you spread writes across multiple partitions with the tradeoff that you lose the ability to read across entity types in a single operation. I think this really depends on the access patterns. For the multi-tenant case, being able to query \\`TENANT#<id>\\` and get metadata + subscription + users in one call is something I reach for a lot.  \nFor most SaaS apps, from what I understand, the hot partition concern is overblown - 1,000 WCU per partition is a lot, and since 2018 DynamoDB's adaptive capacity redistributes throughput to handle hot partitions without you needing to intervene. It won't proactively split them, but it handles the imbalance. If you're at a scale where a single tenant is consistently pushing past that, you probably have bigger architectural decisions to make anyway.\n\nThe tenant isolation point is legit and something I should address in the article. In my production apps I handle this at the application layer (tRPC + OpenAuth ‚Äî every query is scoped to the authenticated tenant). I'm aware of IAM fine-grained access control with \\`dynamodb:LeadingKeys\\` condition keys as the DB-level option, but haven't needed it yet. Have you had success with that approach in practice, or is there something else you'd recommend?",
              "score": 2,
              "created_utc": "2026-02-17 04:27:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5tb242",
                  "author": "finitepie",
                  "text": "I didn't actually have tRPC on the radar. Looks very interesting. Have to learn more about it. So basically, I make sure that the relevant data (tenant id, role, etc) is part of the signed access token, like you do. I have predefined IAM roles with role tags, using the LeadingKeys pattern, and at the API level, the actual dynamodb requests are being done while assuming those roles. This will enforce tenant isolation. But I also have more RBAC/ABAC style of permissions enforced at the middleware level. For all that I build an universal authentication and authorisation system I deploy once (or as often as I want to get more isolation for other reasons)  and can reuse for any other app. It's just plug and play at this point. But was a lot of work to get there. But at the moment I'm still doing REST APIs. Which works nicely, because I'm running Hono with OpenAPI extension, where I only have to define a zod schema as single source of truth, and can easily generate the correctly typed client code via the OpenAPI specs, it automatically generates from that. But the system would work with GraphQL too. ",
                  "score": 1,
                  "created_utc": "2026-02-17 04:47:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5u8q35",
          "author": "SikhGamer",
          "text": "This is a god awful idea if you have a complicated setup. If your setup is flat and easy to understand _forever_ then _maybe_ this would be a good idea.\n\nOtherwise use an RDBMS _please_.",
          "score": 11,
          "created_utc": "2026-02-17 09:42:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uftvd",
              "author": "tejovanthn",
              "text": "True. DynamoDB isn't the right choice for every workload, and forcing single-table design onto a domain with unpredictable or constantly evolving access patterns is going to hurt. No argument there.\n\nBut when the access patterns are well-understood upfront - which they are for a lot of SaaS CRUD apps - the operational simplicity and scaling characteristics of DynamoDB are hard to beat. The goal of this pattern library is to make the \"well-understood\" part easier to get to. :)",
              "score": 4,
              "created_utc": "2026-02-17 10:47:50",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o63y5qg",
              "author": "AntDracula",
              "text": "> use an RDBMS please.\n\nI still don't know why this isn't the *default* thought process, versus being a last ditch effort.",
              "score": 1,
              "created_utc": "2026-02-18 19:30:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6gx1ks",
              "author": "Useful-Process9033",
              "text": "\"Just use an RDBMS\" is fine advice until you need single digit millisecond reads at scale with zero operational overhead. DynamoDB single table design is a tradeoff, not a mistake. The key is knowing your access patterns upfront, which most SaaS CRUD apps absolutely do.",
              "score": 1,
              "created_utc": "2026-02-20 18:45:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tou1y",
          "author": "mamaBiskothu",
          "text": "Why not have separate tables for each tenant?",
          "score": 3,
          "created_utc": "2026-02-17 06:36:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vwxjn",
              "author": "pablo__c",
              "text": "I'd second this. Have multiple tables, one for each tenant, but then do a single table approach for the rest of the stuff. Btw, ignore the \"just use a relational db\" comments. It's ok to try new/different things, and DynamoDB is great for new simple projects with its scale to zero pay as you go model.",
              "score": 3,
              "created_utc": "2026-02-17 16:09:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6du5n5",
                  "author": "tejovanthn",
                  "text": "Agreed - and the table-per-tenant hybrid approach you mentioned is underrated. Works well when you need hard isolation per tenant but want single-table ergonomics within each one.",
                  "score": 1,
                  "created_utc": "2026-02-20 07:22:10",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5tspj0",
              "author": "tejovanthn",
              "text": "It's a valid approach and some teams do this ‚Äî especially when you need hard isolation for compliance (HIPAA, SOC2) or you want to offer dedicated-tenancy as a premium tier.\n\nThe tradeoffs though:\n\n\\- Operational overhead scales linearly - Every new tenant means a new table, new GSIs, new CloudWatch alarms, new backup configs. At 100 tenants that's manageable. At 10,000 it's a nightmare.  \n\\- Cross-tenant queries become expensive - \"List all tenants\" or \"aggregate usage across tenants\" requires scanning every table.  \n\\- AWS account limits - There's a default limit of 2,500 tables per account per region. You can request increases, but it's a signal you're fighting the grain.  \n\\- Cost - Each table with on-demand pricing has its own minimum throughput allocation. One shared table is cheaper than N separate ones.\n\nThe single-table approach gives you logical isolation (tenant-scoped partition keys) with the operational simplicity of one table. If you need stronger isolation, the IAM LeadingKeys approach another commenter mentioned gives you DB-level enforcement without separate tables.\n\nThat said, table-per-tenant is the right call for some use cases ‚Äî particularly when tenants have wildly different scale or strict data residency requirements. It's not wrong, just different tradeoffs.",
              "score": 1,
              "created_utc": "2026-02-17 07:10:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5tuox5",
                  "author": "mamaBiskothu",
                  "text": "Fair points. I get it. This was a fascinating post, thanks a million.",
                  "score": 2,
                  "created_utc": "2026-02-17 07:28:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5t356u",
          "author": "MmmmmmJava",
          "text": "Edit: you‚Äôve fixed it!\n\n~~Your article‚Äôs phrasing seems to indicate you have 3 GSIs, vs 3 access patterns in 1 GSI.~~",
          "score": 2,
          "created_utc": "2026-02-17 03:53:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t4w4w",
              "author": "tejovanthn",
              "text": "Thanks for the feedback :) could you clarify where I can word it better - the blog article, or the post here?",
              "score": 1,
              "created_utc": "2026-02-17 04:04:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5t657t",
                  "author": "MmmmmmJava",
                  "text": "No problem. I think your post could be rephrased to clarify that you walk through two variations. first multiple GSIs and then an overloaded one.\n\nYour article may also benefit from having an index (no pun intended) at the beginning to show the sections and what‚Äôs coming later in the article.",
                  "score": 1,
                  "created_utc": "2026-02-17 04:13:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5uj1om",
          "author": "teo-tsirpanis",
          "text": "That was the best single-table explainer I've ever seen. üëèüèª üëèüèª",
          "score": 2,
          "created_utc": "2026-02-17 11:15:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5um2mk",
              "author": "tejovanthn",
              "text": "Thanks, appreciate it! üòÑ",
              "score": 1,
              "created_utc": "2026-02-17 11:41:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5td0od",
          "author": "gottcha-",
          "text": "How do you handle schema changes?",
          "score": 1,
          "created_utc": "2026-02-17 05:01:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tfy7a",
              "author": "tejovanthn",
              "text": "Good question - this is one of the genuine pain points with DynamoDB, and one that kept me sticking to rdbms for a very long time. \n\nFor attribute-level changes (adding a new field, changing a default), it's straightforward - DynamoDB is schemaless per item, so new items get the new attribute and old items don't. I handle backfills lazily at read time or with a one-off migration script depending on whether the field is required.\n\nFor key structure changes (modifying a PK/SK pattern or GSI), it's more involved. You can't alter keys on existing items - you have to write new items with the new key pattern and clean up the old ones. ElectroDB's versioning helps here: you define a new entity version and can read both old and new formats during the transition.\n\nFor GSI changes, adding a new GSI is non-disruptive (DynamoDB backfills it from the existing table). Changing or removing one requires a migration plan.\n\nHonestly, this is probably the strongest argument against overly complex single-table designs - the more entities and overloaded indexes you have, the harder migrations get. It's why I'd rather start with clean, well-separated key prefixes and only overload GSIs when the access patterns are stable.\n\nSchema migration tooling is a big gap in the DynamoDB ecosystem right now. It's something I'm thinking about for singletable.dev down the road.",
              "score": -1,
              "created_utc": "2026-02-17 05:23:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ti5x8",
          "author": "kingslayerer",
          "text": "If you are on rust maybe you will find my lib useful \n\nhttps://github.com/Salman-Sali/dynorow",
          "score": 1,
          "created_utc": "2026-02-17 05:41:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67lb5r",
              "author": "tejovanthn",
              "text": "Thank you! I'll check it out! I'm not on rust though :)",
              "score": 1,
              "created_utc": "2026-02-19 08:36:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tr86z",
          "author": "Patient-Swordfish906",
          "text": "Good write up, been using single table design in production apps for a few years now.\n\nMy only nitpick with your article is that access pattern #5 is not really covered by your design. You claim you can get a project by ID by using get item on the full SK, but you have the date as a prefix, so you can‚Äôt query only by project ID.",
          "score": 1,
          "created_utc": "2026-02-17 06:57:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tse2s",
          "author": "Soccham",
          "text": "People at my work have been doing this and it‚Äôs a fucking disaster. \n\nJust use relational databases.",
          "score": 1,
          "created_utc": "2026-02-17 07:07:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dttb5",
              "author": "tejovanthn",
              "text": "Sorry to hear that - genuinely. The \"disaster\" pattern I keep seeing isn't single-table being wrong, it's teams designing keys before listing access patterns. You end up with a table that can't serve the queries the product actually needs.\n\nWrote the counterpoint post based partly on this thread: https://singletable.dev/blog/when-not-to-use-single-table-design - curious if any of the failure modes there match what you've seen at work.",
              "score": 2,
              "created_utc": "2026-02-20 07:18:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vyl5y",
          "author": "TechDebtSommelier",
          "text": "Scan is a \"we'll fix it later\" that becomes a 3am incident. Write-shard the GSI key (TENANT#<0-N>), scatter-gather on read, done. Yes it's annoying. No there's no cleaner way. Welcome to DynamoDB.",
          "score": 1,
          "created_utc": "2026-02-17 16:17:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dtww6",
              "author": "tejovanthn",
              "text": "This is the right answer and I should have included it in the post. The scatter-gather overhead is real but it's a one-time design decision vs. the ongoing pain of a Scan at scale. Adding a note on write-sharding to the article. :) thank you!",
              "score": 1,
              "created_utc": "2026-02-20 07:19:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yke8x",
          "author": "ShakataGaNai",
          "text": "What is this, a [schema diagram for ants](https://imgur.com/a/gPKV00U)? But seriously, I can't read it and can't make it any larger.",
          "score": 1,
          "created_utc": "2026-02-17 23:54:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67l225",
              "author": "tejovanthn",
              "text": "Thanks for the feedback üòÖ  \nFixed it - made a lightbox so you can zoom into the image too! :) Let me know if this is better  \n[https://singletable.dev/blog/pattern-saas-multi-tenant](https://singletable.dev/blog/pattern-saas-multi-tenant)",
              "score": 2,
              "created_utc": "2026-02-19 08:34:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6duac1",
          "author": "tejovanthn",
          "text": "A few people in this thread described single-table as a disaster at their companies. They're not wrong in those contexts. Wrote the honest counterpoint based on the discussion here: https://singletable.dev/blog/when-not-to-use-single-table-design - covers the specific cases where you shouldn't reach for it, and what the actual failure pattern usually is.\nAny other use cases I should include in the list?",
          "score": 1,
          "created_utc": "2026-02-20 07:23:23",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5t8lgm",
          "author": "the_corporate_slave",
          "text": "Single table pattern is over complicated trash. Unless you are doing an app rewrite where you know exactly what schema you need, this is a mistake",
          "score": -1,
          "created_utc": "2026-02-17 04:30:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tb0tu",
              "author": "tejovanthn",
              "text": "There's a real point here - single-table design has a steep learning curve and if you get your access patterns wrong upfront, refactoring is painful.\n\nBut \"you need to know exactly what schema you need\" is true of DynamoDB in general, not just single-table. Multi-table DynamoDB still requires you to define access patterns before you design. It's not a relational database where you can normalize first and figure out queries later.\n\nWhere I'd push back: single-table isn't all-or-nothing. The modern approach is pragmatic - group entities that are queried together, use a few clean GSIs, don't overload everything into one index just because you can. That's what this pattern does.\n\nThat said, if your app's access patterns are genuinely unknown and/or evolving fast, DynamoDB itself might not be the right choice - and that's a totally valid position.",
              "score": 1,
              "created_utc": "2026-02-17 04:47:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5tc35e",
                  "author": "finitepie",
                  "text": "I actually prefer the single table design. Everything boils down to how your data model is defined. But as you said, the problem is often not the single table design, but that you might need access patterns, that you are not aware of, yet. But a profound schema migration is always a pain in the a\\*. :D",
                  "score": 5,
                  "created_utc": "2026-02-17 04:55:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r6fzf7",
      "title": "Amazon EC2 supports nested virtualization on virtual Amazon EC2 instances",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r6fzf7/amazon_ec2_supports_nested_virtualization_on/",
      "author": "KayeYess",
      "created_utc": "2026-02-16 17:30:58",
      "score": 41,
      "num_comments": 15,
      "upvote_ratio": 0.96,
      "text": "[https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-ec2-nested-virtualization-on-virtual/](https://aws.amazon.com/about-aws/whats-new/2026/02/amazon-ec2-nested-virtualization-on-virtual/)\n\n\"Posted on:¬†Feb 16, 2026: Starting today, customers can create nested environments within virtualized Amazon EC2 instances. Previously, customers could only create and manage virtual machines inside bare metal EC2 instances. With this launch, customers can create nested virtual machines by running KVM or Hyper-V on virtual EC2 instances. Customers can leverage this capability for use cases such as running emulators for mobile applications, simulating in-vehicle hardware for automobiles, and running Windows Subsystem for Linux on Windows workstations.\n\nThis capability is available in all commercial regions on C8i, M8i, and R8i instances. To learn more about enabling hardware virtualization extensions in your environment, see the Amazon EC2 nested virtualization documentation.\"\n\nLink to documentation: [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/amazon-ec2-nested-virtualization.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/amazon-ec2-nested-virtualization.html)",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1r6fzf7/amazon_ec2_supports_nested_virtualization_on/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5q15jf",
          "author": "im-a-smith",
          "text": "I can then run Docker inside and have images running inside VMs inside VMs\n\nSweet.¬†",
          "score": 15,
          "created_utc": "2026-02-16 18:05:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qg9wt",
              "author": "visicalc_is_best",
              "text": "This is a great idea, particularly because RAM is so cheap right now",
              "score": 10,
              "created_utc": "2026-02-16 19:14:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5rx567",
                  "author": "phaubertin",
                  "text": "It is when it's not your RAM. üòÄ",
                  "score": 5,
                  "created_utc": "2026-02-16 23:41:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5q1fao",
              "author": "samrwalker",
              "text": "Inception",
              "score": 2,
              "created_utc": "2026-02-16 18:06:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5yq7pr",
              "author": "dudeman209",
              "text": "This was Alan Turing‚Äôs vision!",
              "score": 1,
              "created_utc": "2026-02-18 00:26:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q42ia",
          "author": "mezbot",
          "text": "Lol, the comments are exactly what I wanted to say‚Ä¶ needs more layers!",
          "score": 5,
          "created_utc": "2026-02-16 18:18:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5q4xli",
              "author": "pixeladdie",
              "text": "Yo dawg! I heard you like abstractions‚Ä¶.",
              "score": 5,
              "created_utc": "2026-02-16 18:22:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5qcubx",
                  "author": "HiCookieJack",
                  "text": "So we've put podman in your docker, so you can download layers to download layers of layers¬†",
                  "score": 1,
                  "created_utc": "2026-02-16 18:58:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5q5dsq",
          "author": "UnluckyTiger5675",
          "text": "I hope this soon gets propagated to AWS workspaces, so my work mandated Windows 11 workspace can run WSL two instead of just WSL one",
          "score": 9,
          "created_utc": "2026-02-16 18:24:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qax17",
              "author": "bobkiwi",
              "text": "I am in the same boat- WSL2 support would help immensely to avoid the \"but Windows 365 VDIs can do it!\"\n\nIf it's in the m8 series, I wouldn't be surprised if it comes to WorkSpaces by Q3... but which year!",
              "score": 1,
              "created_utc": "2026-02-16 18:49:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ylu6j",
              "author": "SammichAffectionate",
              "text": "We are thinking of migrating away from Workspaces but it keeps getting pushed off. Nested virtualization is just one of the reasons.",
              "score": 1,
              "created_utc": "2026-02-18 00:01:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q1i7c",
          "author": "Alternative-Expert-7",
          "text": "Lets go deeper.",
          "score": 4,
          "created_utc": "2026-02-16 18:07:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qs9j2",
          "author": "MassPatriot",
          "text": "VMception",
          "score": 3,
          "created_utc": "2026-02-16 20:13:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yy2mt",
          "author": "VIDGuide",
          "text": "Yo dawg.. we heard you liked vms, so we put vms in your vms, so you can vm while you vm!",
          "score": 1,
          "created_utc": "2026-02-18 01:09:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60nbpe",
          "author": "ComplianceAuditor",
          "text": "This fucks.",
          "score": 1,
          "created_utc": "2026-02-18 07:59:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1raski8",
      "title": "AWS Certificate Manager updates default certificate validity to comply with new guidelines",
      "subreddit": "aws",
      "url": "https://aws.amazon.com/about-aws/whats-new/2026/02/aws-certificate-manager-updates-default/",
      "author": "SudoAlex",
      "created_utc": "2026-02-21 14:32:40",
      "score": 39,
      "num_comments": 5,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "security",
      "permalink": "https://reddit.com/r/aws/comments/1raski8/aws_certificate_manager_updates_default/",
      "domain": "aws.amazon.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6lu1ai",
          "author": "SudoAlex",
          "text": "Posted this mostly due to the future uncertainty from their announcement back in June 2025, with exportable public certificates: https://www.reddit.com/r/aws/comments/1ldpw1d/aws_certificate_manager_introduces_public/ - as it was well known that certificate lifetimes would be limited in the long run.\n\nThey'll be reducing the duration, along with the price:\n\n> We have reduced the pricing for ACM‚Äôs exportable public certificates in line with the shorter validity period. 198-day exportable public certificate will now cost $7/Fully Qualified domain name (down from $15) and $79/ wildcard name (down from $149). Please refer to ACM‚Äôs pricing page for more details. For more information about ACM, visit the [ACM documentation](https://docs.aws.amazon.com/acm/latest/userguide/acm-certificate-characteristics.html).\n\n~~Although the wildcard pricing feels like a slightly sneaky $4.50 increase (half of $149 is $74.50)~~.\n\nUpdated numbers below.",
          "score": 5,
          "created_utc": "2026-02-21 14:38:53",
          "is_submitter": true,
          "replies": [
            {
              "id": "o6m80fe",
              "author": "Mutjny",
              "text": ">Although the wildcard pricing feels like a slightly sneaky $4.50 increase (half of $149 is $74.50).\n\n198 days is not half of 365.  The new price is exactly equivalent to the old price with the reduced number of days.  Its actually like $1.50 less.",
              "score": 6,
              "created_utc": "2026-02-21 15:53:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6mq2vo",
                  "author": "SudoAlex",
                  "text": "Certificates weren't valid for 365 days - it was 395 days. Got the calculations slightly off, but it's probably more of a price bump:\n\nBefore: 395 day certificate, renews 60 days before expiry, $149 per 335 days  \nAfter: 198 day certificate, renews 45 days before expiry, $79 per 153 days\n\nShould be closer to $68 for it to be the same price.",
                  "score": 2,
                  "created_utc": "2026-02-21 17:23:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m02lx",
          "author": "wlonkly",
          "text": "Ah thanks for this, I was wondering what their plan was (but not enough to ask our TAM I guess, heh).",
          "score": 1,
          "created_utc": "2026-02-21 15:12:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9nmsm",
      "title": "When DynamoDB single-table design is the wrong choice (and what to use instead)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r9nmsm/when_dynamodb_singletable_design_is_the_wrong/",
      "author": "tejovanthn",
      "created_utc": "2026-02-20 06:11:35",
      "score": 33,
      "num_comments": 16,
      "upvote_ratio": 0.83,
      "text": "Last week's multi-tenant post hit #1 here. The comments were more useful than the upvotes - three engineers described it as a disaster at their companies.\n\nThey're not wrong. I use single-table design in production and I'm building a tool around it, but there are real situations where it's the wrong call.\n\nThe cases where you shouldn't reach for it:\n\n* Access patterns are still changing (pre-PMF products)\n* No DynamoDB champion on the team - works great until that person leaves\n* Significant reporting or analytics requirements\n* Fewer than 6 access patterns (just use multi-table, it's fine)\n* Multiple teams or bounded contexts sharing the same deployment\n* Per-entity DynamoDB Streams processing (you get one stream per table, not per entity type)\n\nThe \"disaster\" pattern I keep seeing isn't single-table being wrong - it's teams starting with key design before listing access patterns. You design the table to serve access patterns, not the other way around.\n\nFull post covers: the decision framework table, the microservices/team ownership case (probably the most underrated reason to avoid it), and what I actually use for [rasika.life](http://rasika.life) vs what I'd use for a prototype.\n\n‚Üí [https://singletable.dev/blog/when-not-to-use-single-table-design](https://singletable.dev/blog/when-not-to-use-single-table-design)\n\nCurious what situations have pushed your teams away from it - or toward it despite the complexity.",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1r9nmsm/when_dynamodb_singletable_design_is_the_wrong/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6gbip1",
          "author": "menge101",
          "text": "Several of these \"single table design is the wrong choice\" are more \"DynamoDB is the wrong choice\" imo.\n\nParticularly: \"You have serious analytical or reporting requirements\"\n\nIt's an OLTP datastore, not an OLAP, it doesn't matter how many tables you use, you can't do arbitrary queries on the data.\n\nThis one: \"Fewer than 6 access patterns (just use multi-table, it's fine)\", i don't agree with.  It's not about how many access patterns, its about access patterns that capture relationships in the data.\n\nYou can have only one access pattern, but if the table uses the partition key and sort key to create a relationship between records, and then you pull all records for the partition key and have a full representation of the data relationship, you have used single table design correctly with a single access pattern.  (albeit, it would seem unlikely to only have that one access pattern form this multi-faceted related data)",
          "score": 10,
          "created_utc": "2026-02-20 17:06:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gei9r",
              "author": "tejovanthn",
              "text": "Both of these are fair pushback.\n\nOn the analytics point - you're right, and I should have been clearer. That item belongs in a \"DynamoDB is the wrong choice\" list, not a \"single-table is the wrong choice\" list. The underlying problem is using DynamoDB for OLAP workloads at all. The number of tables doesn't change that. I'll fix the framing.\n\nOn the access pattern count - also a fair correction. The \"6 access patterns\" heuristic is too blunt. What I was trying to capture is the case where your data relationships are simple enough that the cognitive overhead of single-table design isn't justified - but you're right that the real signal isn't the count, it's whether your access patterns require traversing relationships within a partition. One access pattern that pulls a full aggregate by partition key is a completely legitimate use of single-table design. I'll reframe that item around relationship complexity rather than access pattern count.",
              "score": 2,
              "created_utc": "2026-02-20 17:20:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6gl5cl",
                  "author": "menge101",
                  "text": "I'm not really pushing back, just discussing the ideas, I think your write up is fine for people who don't really know single table design and/or DynamoDB.\n\nLike you can't be all-use-case encompassingly correct on these things when you are talking in sort of abstract generalizations.",
                  "score": 2,
                  "created_utc": "2026-02-20 17:51:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6dssuw",
          "author": "mehneni",
          "text": "It was a disaster for us as well. Causing 6 digit monthly costs at some point. I guess a problem is always: You have to know what you are doing.\n\nReducing the number of tables compared to a relational design makes sense. But I'd almost limit the single table to something like one aggregate in DDD at most: [https://martinfowler.com/bliki/DDD\\_Aggregate.html](https://martinfowler.com/bliki/DDD_Aggregate.html)\n\nJust putting unrelated data in a complex data model with very different quantities in a single table is a disaster. In our case it was an event store, a production database and a (complex) configuration store. Mixing all of this is an absolutely horrible idea, because the different areas have vastly different requirements and change rates.\n\nScanning the table becomes impossible. This might not matter for day-to-day operations, but if every migration takes a week and is hugely expensive that is a problem and you become very inflexible.\n\nAnd always put an expiry on the data. Because deleting data by scanning the table is just so expensive that is makes more sense to create a new table and drop the old one (which is a stressful thing to do if your backup will take ages to reconstruct the table ;).\n\nThe risk in having more tables is also smaller. In that case you can only mess up one area of the data. So only do single table if you know exactly what you are doing. Merging multiple tables into one is easier than the other way around.",
          "score": 14,
          "created_utc": "2026-02-20 07:09:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ejxn5",
              "author": "tybit",
              "text": "Great point on the DDD aggregate. This is roughly how I‚Äôve viewed it for a while but not had the right term to describe it.\n\nI‚Äôve generally thought of it as only use single table design if you have a well bounded micro-service.",
              "score": 3,
              "created_utc": "2026-02-20 11:19:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6k5d92",
                  "author": "tejovanthn",
                  "text": "\"Well-bounded microservice\" is actually a cleaner way to say it for most developers than \"DDD aggregate\" - same idea, more accessible. The service boundary and the table boundary should roughly align.",
                  "score": 1,
                  "created_utc": "2026-02-21 06:02:26",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6dvg3e",
              "author": "tejovanthn",
              "text": "Six-digit monthly costs is the nightmare scenario - and the root cause you're describing is exactly what I called out in the post: mixing data with vastly different access patterns, quantities, and change rates into one table. An event store and a production database have almost nothing in common operationally. That's not a single-table design problem, that's a data modeling problem that single-table made worse.\n\nThe DDD aggregate framing is actually the clearest heuristic I've heard for where the boundary should be. One aggregate (or tightly related aggregates that are always queried together) per table makes the access patterns predictable and keeps migrations scoped. The moment you're mixing things that evolve independently - your config store vs. your event store - you've lost the main benefit and kept all the costs.\n\nThe TTL point is underrated. Designing for data expiry from day one changes how you think about the whole model. \"How does this data leave the table?\" should be in the access patterns list from the start.\n\nGoing to add the aggregate boundary heuristic to the post - it's a more actionable framing than what I had.",
              "score": 2,
              "created_utc": "2026-02-20 07:33:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ju084",
          "author": "finitepie",
          "text": "You can not talk about DynamoDB vs relational DB without talking about scaling. Scalability is the whole point behind DynamoDB. Everything else is a trade-off to make it scalable. I like DDB. I like it a lot. It's not that one super dooper database to rule them all. It serves a well defined purpose. And sometimes it's just not the right tool for the job. If you understand that in advance, understand how to make the tool work for you and not against you, then DDB is your friend. And it might be 'schema-less', but I want to see you change relational schemas back and forth and not complain about the consequences :D. There is always a schema, a logic, a model, just that it is not explicitly defined and enforced at the DB level like it would be with a relational db. And schema definition for a relational db or data in general can be a lot of work.  It requires a lot of discipline to work it out and to keep it consistent in the long run. Having a good model/schema of your data, is to understand the data. And once you get there, you can also understand the access patterns better. So for me, modelling the data is always the very first step. Every mistake you make in that process will always result in plenty of pain and work. For DDB even more so. ",
          "score": 2,
          "created_utc": "2026-02-21 04:33:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k588j",
              "author": "tejovanthn",
              "text": "Hard agree on the sequencing - model the data first, derive access patterns from that, then design the table. The \"schema-less\" label does a lot of damage because it implies you can skip that step. You can't. The schema just lives in your application code instead of the database, which means when it breaks, it breaks silently.\n\nThe scalability point is the crux of it. People reach for DynamoDB because of the scaling promise, then design it like a relational database and get the worst of both worlds - none of the query flexibility, none of the operational simplicity.",
              "score": 1,
              "created_utc": "2026-02-21 06:01:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gvyvw",
          "author": "AftyOfTheUK",
          "text": ">Last week's multi-tenant post hit #1 here\n\nLink?",
          "score": 1,
          "created_utc": "2026-02-20 18:40:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6junn3",
              "author": "tejovanthn",
              "text": "Post - https://www.reddit.com/r/aws/s/q8V8p4A9ab\nBlog -https://singletable.dev/blog/pattern-saas-multi-tenant\n\nI would love your thoughts too :)",
              "score": 2,
              "created_utc": "2026-02-21 04:37:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6m265x",
          "author": "AttentionIsAllINeed",
          "text": "introduction of Multi Attribute GSIs made single table design even worse.¬†",
          "score": 1,
          "created_utc": "2026-02-21 15:23:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dnode",
          "author": "galnar",
          "text": "Thanks for sharing, this is really thought provoking. Would you mind sharing your job title? I‚Äôm wondering what role gets this deep in the weeds. Do you have the same level of experience with RDS and/or DocumentDB?",
          "score": -1,
          "created_utc": "2026-02-20 06:23:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6drp4t",
              "author": "tejovanthn",
              "text": "Thank you :) I used to lead engineering teams building on AWS for the better half of the last decade. Decided to step back and focus on some passion projects. :)   \n  \nCurrently I'm an indie developer. Day job is building [rasika.life](https://rasika.life) (a Carnatic music platform) where DynamoDB is the primary datastore ‚Äî so this isn't theoretical for me, it's the system I'm maintaining.\n\nOn RDS: yes, reasonably deep. I've built on Postgres for years and it's my default for anything with complex reporting, evolving schemas, or strong relational requirements ‚Äî which is basically why it made the \"when to avoid single-table\" list. DocumentDB I've touched but wouldn't claim expertise.\n\nThe honest answer is that I get deep in DynamoDB specifically because I made the choice to build on it and had to live with the consequences. Nothing sharpens your opinion on a tool like owning it in production.",
              "score": 4,
              "created_utc": "2026-02-20 06:59:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r76u7k",
      "title": "How to build a distributed queue in a single JSON file on object storage (S3)",
      "subreddit": "aws",
      "url": "https://turbopuffer.com/blog/object-storage-queue",
      "author": "itty-bitty-birdy-tb",
      "created_utc": "2026-02-17 14:03:33",
      "score": 29,
      "num_comments": 9,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1r76u7k/how_to_build_a_distributed_queue_in_a_single_json/",
      "domain": "turbopuffer.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5vx5ax",
          "author": "the8bit",
          "text": "Starting this 'seems like this won't really scale'\n\nEnding it 'ah you are basically reimplementing Kafka. Bold choice '",
          "score": 17,
          "created_utc": "2026-02-17 16:10:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yk71d",
              "author": "itty-bitty-birdy-tb",
              "text": "Ha. Well I guess we'll¬†take¬†\"Kafka but¬†it's one¬†JSON¬†file\" as¬†a¬†compliment",
              "score": 4,
              "created_utc": "2026-02-17 23:52:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6gwxiy",
              "author": "Useful-Process9033",
              "text": "\"Kafka but it's one JSON file\" is honestly a great pitch. CAS on S3 is surprisingly powerful now that conditional writes exist. The real question is what happens when that JSON file gets large enough that read-modify-write cycles start contending, but for indexer scheduling the write volume is probably fine.",
              "score": 1,
              "created_utc": "2026-02-20 18:44:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yxfp8",
          "author": "ruibranco",
          "text": "The Iceberg comparison is actually really apt ‚Äî both are fundamentally betting that object storage conditional writes are reliable enough to build coordination primitives on top of. The fact that you can get away with a single JSON file instead of a proper consensus protocol says a lot about how far S3's consistency model has come since they went strongly consistent in 2020.",
          "score": 6,
          "created_utc": "2026-02-18 01:06:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vt3gm",
          "author": "AdCharacter3666",
          "text": "This is kinda like Iceberg but for queues.",
          "score": 4,
          "created_utc": "2026-02-17 15:50:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yk7cz",
              "author": "itty-bitty-birdy-tb",
              "text": "yeah decent comparison actually, same idea of using object storage as the source of truth with atomic metadata updates. Iceberg uses manifest files, we use CAS. the nice thing about both patterns is that object storage handles durability and availability so the compute layer can stay stateless.",
              "score": 2,
              "created_utc": "2026-02-17 23:53:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yaex3",
          "author": "Hackinet",
          "text": "Wait, why use a single file? Why not just do a group commit to a new file for a worker to pick up? \n\nIt still doesn't solve the issue with two workers picking up duplicate work in your article but I feel it would simplify the architecture and the write race conditions that you might run into.",
          "score": 2,
          "created_utc": "2026-02-17 22:59:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yjidl",
              "author": "itty-bitty-birdy-tb",
              "text": "the single file with CAS is what gives us strong consistency for free. the full queue state is always in one place, and CAS guarantees that any mutation¬†(push, claim, heartbeat) is¬†atomic with¬†respect to the current¬†state. if¬†you write new¬†files instead, you need¬†a¬†separate¬†coordination¬†mechanism to establish¬†ordering, track¬†which¬†files have been consumed, and¬†garbage¬†collect old¬†ones. you're¬†basically rebuilding the consistency¬†guarantees that¬†CAS on¬†a single file gives¬†you out¬†of¬†the box.",
              "score": 2,
              "created_utc": "2026-02-17 23:49:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r8s1yy",
      "title": "how bad is it to launch without a proper cloud architecture plan?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r8s1yy/how_bad_is_it_to_launch_without_a_proper_cloud/",
      "author": "These_Run_7070",
      "created_utc": "2026-02-19 06:40:35",
      "score": 23,
      "num_comments": 74,
      "upvote_ratio": 0.71,
      "text": "We are 8 months from launch and honestly we have just been spinning up services as we need them. no real architecture doc, just \"lets use this because it works.\" our AWS bill went from 2k to 8k in 3 months and we're not even at scale yet though.   \nmy co founder keeps saying we'll \"fix it after launch\" but I'm getting nervous. what if we hit product market fit and the whole thing falls apart because we built on sand?\n\nIs it crazy to pause feature dev for a month to actually design this properly? or do most startups just figure it out as they grow?\n\n",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1r8s1yy/how_bad_is_it_to_launch_without_a_proper_cloud/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o679a5q",
          "author": "sobeitharry",
          "text": "We are battling mistakes from 10 years ago.  A penny saved is a penny earned.",
          "score": 90,
          "created_utc": "2026-02-19 06:46:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67kc6k",
              "author": "tumes",
              "text": "The number of conference talks I have been to in the last year or two that are just people confidently post mortem-ing, in public no less, the same fucking mistakes every startup makes and has made for the last 15 years in the same order is just‚Ä¶ too magnificent for words. I‚Äôm begging you, we are gross and old but hire one reliable greybeard who has seen some shit, they will save you one funding round worth of flailing and churn by shooting down your galaxy brain bullshit that relies on one non-load balanced server for ingress or a terminally unindexed database column or whatever.",
              "score": 46,
              "created_utc": "2026-02-19 08:26:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o69l4i8",
                  "author": "sobeitharry",
                  "text": "Yet we refuse to bring in outside expertise even when our in house experts recommend.  Just get it done,  yesterday.",
                  "score": 6,
                  "created_utc": "2026-02-19 16:36:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o68s2i1",
              "author": "SpecialistMode3131",
              "text": "You're still alive from 10 years ago.  People seriously underrate survival.  You \\*get\\* to fix the mistakes now because you're still in business.",
              "score": 14,
              "created_utc": "2026-02-19 14:09:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6lq6ho",
              "author": "Dabnician",
              "text": "I was asked to \"just move it to ecs\", not knowing shit about docker, kubernetes, fargate, ecs or any of that shit.\n\nI took 2 months to launch my first task in ecs cause i was worried about doing it right, or at least in a way where im not spending $300+/month for 1 containers before usage came into play.\n\nI avoided so many \"oopsies\" because I refused to rush.",
              "score": 2,
              "created_utc": "2026-02-21 14:16:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6gwu0b",
              "author": "Useful-Process9033",
              "text": "10 years of compounding tech debt is no joke. The teams I've seen recover from this always start with observability first, you can't fix what you can't see. At minimum get cost alerts and incident tracking in place before launch so you know where the fires are.",
              "score": 1,
              "created_utc": "2026-02-20 18:44:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gypb2",
                  "author": "sobeitharry",
                  "text": "Our company decided to just start building new products and is trying to phase out the old ones.  Of course the people building the new stuff are making the same mistakes,  but i just work here.  We like to repeat our mistakes.",
                  "score": 2,
                  "created_utc": "2026-02-20 18:52:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o679ntm",
          "author": "suddenly_kitties",
          "text": "Push your AWS account team to access whatever startup/Activate credits they are willing to throw at you, and ask for a meeting with a Solutions Architect. You might want to consider getting a contractor/consultant/partner involved at a point, technical debt and your bill will keep growing.",
          "score": 40,
          "created_utc": "2026-02-19 06:49:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i3jhz",
              "author": "Donnelding0",
              "text": "Ditto, there is money you may be leaving on the table that you are entitled to.",
              "score": 1,
              "created_utc": "2026-02-20 22:12:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o679dve",
          "author": "seany1212",
          "text": "I‚Äôd love to know what product you were working on that would allow for not being ready for release in 8 months. Also, how are you spending 8k with no throughput? Just what services at what sizing are you actually running?",
          "score": 41,
          "created_utc": "2026-02-19 06:47:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68h8py",
              "author": "ImNewHere05",
              "text": "Yeah, these numbers are wild",
              "score": 15,
              "created_utc": "2026-02-19 13:06:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6a5hy9",
              "author": "nemec",
              "text": "I hate these AI posts because I can't even trust that the numbers they're giving weren't just made up by an LLM.",
              "score": 2,
              "created_utc": "2026-02-19 18:13:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6boy0q",
                  "author": "Wrectal",
                  "text": "What's the giveaway to you that you believe this is an AI post?",
                  "score": 2,
                  "created_utc": "2026-02-19 22:44:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67ebao",
          "author": "spicypixel",
          "text": "Never seen a company successfully defer good decisions and reclaim that lost ground later.\n\nSome things just fuck you forever.",
          "score": 8,
          "created_utc": "2026-02-19 07:30:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gipxy",
              "author": "mezbot",
              "text": "I have, but it results in at least 10x as much work to retrofit, and it‚Äôs chaos to manage until they finally bite the bullet.  It ends up costing a small fortune.",
              "score": 2,
              "created_utc": "2026-02-20 17:40:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o69j8j6",
          "author": "Ok_Study3236",
          "text": "you're at $96k annual without a product yet? Lol, yes, you need to fix that now. That's a people problem not a technical one",
          "score": 7,
          "created_utc": "2026-02-19 16:27:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67bmz5",
          "author": "behusbwj",
          "text": "There are production services that cost a fraction of that at scale‚Ä¶ what did you do?\n\nCloud architecture ‚Äúplans‚Äù are a scam. Do design reviews with cost analyses. Use services properly with reasonable configurations for your needs. I have no idea what you could be doing with zero traffic that costs thousands already. Did you just spin up a bunch of random ec2 instances and call it a day?",
          "score": 14,
          "created_utc": "2026-02-19 07:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67szg9",
          "author": "mdivan",
          "text": "8k pre launch is huge, you are absolutely correct to be worried how that would scale.",
          "score": 11,
          "created_utc": "2026-02-19 09:52:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67w52e",
              "author": "mamaBiskothu",
              "text": "Also 8 MONTHS from launch it seems. Like what are you doing training the next GPT?",
              "score": 6,
              "created_utc": "2026-02-19 10:22:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67lx04",
          "author": "Sudoplays",
          "text": "Can you give some more insight into what services you‚Äôre using, why and the cost of each one?\n\n‚ÄúFix it after launch‚Äù - this is highly unlikely to happen as new features are needed, bugs get squashed and the focus shifts away from architecture.\n\nPlanning your cloud infrastructure is important, and should be done before deploying any resources. Infrastructure as code isn‚Äôt required, but it‚Äôll make everything easier down the road when you need to have separate dev/rc/prod environments.\n\n8k is a huge bill for something that isn‚Äôt even launched yet, even 3k is quite high.",
          "score": 4,
          "created_utc": "2026-02-19 08:42:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67cvli",
          "author": "watergoesdownhill",
          "text": "This whole thing reeks. Anything that takes more than six months typically doesn‚Äôt ship at all. \n\nI have a feeling you guys are battling customer requirements. While also doing some sort of grand infrastructure.",
          "score": 10,
          "created_utc": "2026-02-19 07:17:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68glwb",
          "author": "never-starting-over",
          "text": "I don't know what your product is but I built a video marketplace like Netflix + Youtube with Kubernetes and analytics for stakeholders and it cost 500/month with two copies of the environment running (prod, staging)\n\n3k sounds like a lot. Jumping from 3k to 8k is insane. Spending this much with zero revenue (pre launch) sounds like suicide\n\nEver consider getting someone to review whats going on there?",
          "score": 2,
          "created_utc": "2026-02-19 13:02:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67aohd",
          "author": "notospez",
          "text": "At this stage it depends on your runway. Do you have or expect to have millions available for growth? Then 8k/month in infra is peanuts and you can hire a bunch of people to reengineer next year once you know which product features resonate with customers and have actual usage data.",
          "score": 2,
          "created_utc": "2026-02-19 06:58:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67fiz6",
          "author": "dariusbiggs",
          "text": "Without infrastructure as code from the start you are shooting yourself in the foot. you need to understand what you are using and why you are using it. Then you can see what your minimum spec is and how you can scale it.\n\nYour description is a clear example of not understanding things clearly (your product, cloud infra, and your customer needs) . This is indicated by your costs and are very likely caused by premature over engineering.\n\nCut it down, simplify, and only build scaling when you have metrics demonstrating the need (or the upcoming need due to observable trends).\n\nWe over engineered from the start due to misunderstood requirements and cut down those costs to the point where our entire prod, staging, and dev environments combined cost about what your costs are. Those resources are all running at the minimum specifications for the various components but provide capacity for another 2000% user increase. (Constraints are caused by memory minimums and db connection numbers, whilst the compute is only running at 1-2% load and growth of users has minimal effects on memory usage and the number of DB connections, it's all compute and network IO based).\n\nAnd to cut down costs, the various non-prod environments are turned off when not needed for extended periods of time (like the entire month of the Christmas break period). That is achievable with proper use of IaC, turn it off. You don't need dev or test environments afterhours or on weekends, so kill them if you can.",
          "score": 2,
          "created_utc": "2026-02-19 07:41:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o679whl",
          "author": "Ready-Trick-8228",
          "text": "honestly we threw infros in just to see what was actually eating credits. kind of low effort, low stress way to keep an eye on things without overthinking it.",
          "score": 1,
          "created_utc": "2026-02-19 06:51:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67d50f",
          "author": "mitch3x3",
          "text": "Feel free to dm me. Happy to help if it‚Äôs in my wheelhouse",
          "score": 1,
          "created_utc": "2026-02-19 07:19:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67d8qr",
          "author": "Alsmack",
          "text": "Wall of text incoming; tl;dr: balance your efforts with the realities of time and money. Don't invest too early in things that don't help you prove product market fit. Do invest in things that let you iterate quickly without digging a deep tech debt hole, as being able to deliver changes reliably and quickly lets you get to that product market fit faster.  \n  \nThere is a balance to be had. As a startup there's a few things that really matter. Acquiring/validating product market fit, and the length of your runway (cash on hand / cost per month to run the business = runway in months) are the two that come to mind with this question.\n\nIt's very easy to do things well enough. It's also very easy to make a completely unmaintainable and overpriced mess. What you don't want to do is spend a month making things better than well enough only to launch a product that has no market or can't find it's feet in the market, even if one exists. This shortens your effective runway to solve those problems.\n\nPlatform/tech/infra/arch serve the operations and maintainability of a software product and business. It means nothing if you don't have a product. If what you are doing today is \\*honestly\\* getting in the way of delivering a product (reducing your runway to have a successful (growing) product) or costing too much money (again, increasing burn rate and reducing runway, or too much time due to inconsistency/bad dev flow/whatever) so that you won't hit your deadlines with adequate reserves, then it sounds like there is a business decision to made to solve those pain points.\n\nInvestment in platform is a business decision. You are limited by time and money. Any of those you spend on platform are not being spent on product. Yes, there are tradeoffs - if you don't spend enough time/money on solid foundations, you'll probably have a less reliable product. There's not a magic bullet here, you must evaluate your honest business needs and make a decision based on the facts at hand.\n\nIf you hit product market fit, you have a good problem on your hands. In theory, this means revenue that lowers your burn rate increasing your runway, or more opportunity for investment for more capital which increases your runway, both of which give you more options to get resourcing on stability/scalability/efficiency that you caused by not prematurely spending time on whatever that bottleneck is.\n\nAs an platform decision maker myself, 90% of what I do doesn't matter pre product launch. Make sure we're using IAC that's fast to iterate on, make sure we're documenting decisions, make sure we have reasonable deployment pipelines that are consistent, boring, and reliable, and have any choice for observability and alerting. That's the most I would care about pre launch. Post launch once running a live service, that's a whole different ball game. But, if I've got those 4 things in place we have consistency in provisioning, in deployment, we aren't blind, and we have documented business context. That's a solid foundation for working through operational challenges and pushing for reliability. And you don't need all of them right away. o11y can get expensive fast, for example.\n\nLastly, check with your AWS account rep about AWS startup credits or other programs you can get into. Your infra costs sound way too high for a pre-launch early stage startup, they often give credits for adopting new services and things like that. Depending on your level of support, you may be able to get some help from an aws solutions architect or similar to help manage those costs, though that typically comes with the super pricey stuff.",
          "score": 1,
          "created_utc": "2026-02-19 07:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67f19q",
          "author": "metaphorm",
          "text": "if you can't keep the scaling of operational expenditure below the scaling of revenue growth, the economics of the project are broken. infrastructure cost is a major component of operational expenditure. get this under control now.\n\nyou might reasonably decide to burn extra cash when the scaling cost problems aren't yet critical. you can do this for a short time, but if the curve isn't bent in the right direction, the more you scale the worse it gets, and you'll burn the whole runway and have to raise more money way too soon, probably under poor terms. don't let this one get away from you.",
          "score": 1,
          "created_utc": "2026-02-19 07:36:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g8orw",
              "author": "MaxMcregor",
              "text": "This is such an underrated point if the unit economics do not work, growth just accelerates losses instead of value. Scaling only helps when each new customer actually improves the business, not strains.",
              "score": 1,
              "created_utc": "2026-02-20 16:53:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67ifs9",
          "author": "sunny6333",
          "text": "this is very normal for start up culture because many VCs dont give a shit about how much money u burn, only about revenue growth\n\nyes this will almost guarantee bite u in the ass in the future, but with how frequently roadmaps and decisions change in a start up it's very difficult to nail it immediately",
          "score": 1,
          "created_utc": "2026-02-19 08:08:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67jg8m",
          "author": "Wide_Commission_1595",
          "text": "Honestly if you can afford it, don't worry too much.  Everything can be fixed later.\n\nThat said, a little work to set up a good org structure and a few guardrails isn't too hard.  It causes some initial pain because devs have to follow a plan, but it doesn't need to be horrible",
          "score": 1,
          "created_utc": "2026-02-19 08:18:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67wedo",
          "author": "pint",
          "text": "there are two types of startups i see. one is the garage type, short on resources, fueled by enthusiasm. the other is the investment burning type, which somehow lays a hand on huge sums of angel money, and a new round when the previous runs out.\n\nif you are the second type, why would you care?\n\nif you are the first type, you really need to consider how will you realize enough revenue to cover the 10k, 20k, whatever expense just on infra, and to recover the sunk cost too.\n\nin my experience, the fix it later phase never comes. you always have new things to do, bugs to fix, features to add. nothing lasts longer than a temporary solution.",
          "score": 1,
          "created_utc": "2026-02-19 10:25:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67xtv7",
          "author": "SikhGamer",
          "text": "> my co founder keeps saying we'll \"fix it after launch\"\n\nThis approach is fine, if you _actually_ do it. Otherwise trust your gut.",
          "score": 1,
          "created_utc": "2026-02-19 10:38:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o680pda",
          "author": "256BitChris",
          "text": "Your biggest risk is not getting marketing fit, not whether you'll be able to scale or not.\n\nScaling is a solved problem with well known paths forward - building a successful product isn't.    \n  \nFix it after launch, if it starts to break.  Losing a month to polish tech debt, before reaching PMF could easily kill your startup, especially in today's hyper fast world of AI.",
          "score": 1,
          "created_utc": "2026-02-19 11:03:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68vaz9",
              "author": "AndyWhiteman",
              "text": "Launching without a solid setup can definitely be risky, but a lot of founder get away with it at first and then run into bigger issues later. Having a clear plan for how things will scale from the start can save a lot of headaches down the line.",
              "score": 0,
              "created_utc": "2026-02-19 14:27:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6g9aky",
                  "author": "MaxMcregor",
                  "text": "Totally agree shipping fast is fine, but you need a path to harden things before real scale hits. Early shortcuts are survivable, scaling on top of them without fixing the foundation is where things usually break.",
                  "score": 1,
                  "created_utc": "2026-02-20 16:56:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6839r0",
          "author": "bot403",
          "text": "I was at a B2B business that had 8 figures of top line revenue with 8k/mo spend...\n\n\nWhy are you spending so much with no customers? Scale it down the scale it back up when you know what the bottlenecks are.",
          "score": 1,
          "created_utc": "2026-02-19 11:25:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o688ulo",
          "author": "requiem33",
          "text": "Way to familiar... trying to win the race to the bottom I see.  I've been around the block a few times (grey beard) and the classic mistake of \"it's only temporary\" and \"we'll fix it later\" are the two phrases that have killed more startups than those few that make it to buy out. ",
          "score": 1,
          "created_utc": "2026-02-19 12:09:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68klza",
          "author": "japanthrowaway",
          "text": "What services are you using that led to such a dramatic increase?",
          "score": 1,
          "created_utc": "2026-02-19 13:27:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68qw4t",
          "author": "rwodave",
          "text": "Technical debt is no joke.",
          "score": 1,
          "created_utc": "2026-02-19 14:02:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68sbsy",
          "author": "SpecialistMode3131",
          "text": "Figure out what your burn rate will be under different scenarios and make a call from that. If scaling up will bankrupt you, then yes.  If scaling up will just mean you have ongoing problems, then probably no - because you'll actually be alive to try and solve them.  Scaling up means you have market fit, and you can move on to series A.\n\nYou eat an elephant one bite at a time.",
          "score": 1,
          "created_utc": "2026-02-19 14:10:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68yir5",
          "author": "TheGutterBall",
          "text": "Based on the description, it almost sounds like you don‚Äôt have the skill set in house to fix it. If you had someone with experience or architectural knowledge, they should have raised the alarm (loudly) by now with these numbers. An architectural doc isn‚Äôt going to solve anything; find a professional, hire them, come up with a plan to get this fully managed, execute on it",
          "score": 1,
          "created_utc": "2026-02-19 14:44:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6965q7",
          "author": "CyclonusRIP",
          "text": "It's more about understanding your constraints.  If you guys have a ton of funding and $8K per month isn't a big deal then maybe you don't need to worry about it.  If you guys are bootstrapping or don't have a ton of runway then you should probably try to minimize hosting cost.  Creating a lot of services can be expensive from a hosting perspective and time consuming to manage that architecture for a small team.  Early on you probably want to minimize the number of services and technologies you use as much as possible.",
          "score": 1,
          "created_utc": "2026-02-19 15:23:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ala8f",
          "author": "ManBearHybrid",
          "text": "There's nothing wrong with a little technical debt... as long as you're realistic about paying it back, and you know both sides of the trade-off. Just blindly deploying stuff without a plan is, frankly, insane. This is before all the possible other concerns - security, reliability, durability, disaster recovery, etc. \n\nAlso, you're probably duplicating a lot of work - doing, undoing and then re-doing a lot of stuff because you're stumbling around in the dark. Taking a breath to put together a plan will give you a proper north star to steer towards. You'll be able to put together a proper road map to get there, and you'll know if you're falling behind. ",
          "score": 1,
          "created_utc": "2026-02-19 19:28:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b4ucr",
          "author": "Own-Manufacturer-640",
          "text": "As an AWS consultant i can assume the same mistakes made by every startup.\n\nDev uat qa resources running 24/7, \nOver provisioned resources because devs have the access to create resource, \nMight be default vpc and public ips, \nZombie resources, \nSnapshot sprawl, \nMulti region resources, \nOr too much NAT Gw usage, \nOver provisioned EBS, \nUsing services that are not useful for your startup,  \nZero ownership of resources etc etc etc. \n\nIf you guys are not using Sagemaker then i think 8k pre launch is too much.",
          "score": 1,
          "created_utc": "2026-02-19 21:03:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c7129",
          "author": "raptorraptor",
          "text": "I'd start looking for another job. Unless you're the most senior engineer, then I'd look for another line of work.",
          "score": 1,
          "created_utc": "2026-02-20 00:28:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ctsbl",
          "author": "DrollAntic",
          "text": "Foundations matter. If you don't have one, you'll struggle to scale.",
          "score": 1,
          "created_utc": "2026-02-20 02:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dy2w8",
          "author": "curiouscrustacean",
          "text": "Sounds like you guys need an experienced DevOps to unfuck the fucked things and do things as you go.\n\nI'm currently doing this for a country level application.",
          "score": 1,
          "created_utc": "2026-02-20 07:58:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ey3gh",
          "author": "RhoOfFeh",
          "text": "Fixing it after launch means never fixing it until a massive re-engineering effort which will be too expensive and too late.",
          "score": 1,
          "created_utc": "2026-02-20 13:00:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fjhk3",
          "author": "grsftw",
          "text": "Yes, do design. No, don't go too deep yet. If you are pre-revenue then your #1 goal is reaching revenue. This to me is a very bad idea: \"Is it crazy to pause feature dev for a month to actually design this properly?\" \n\nSide note, are you saying you are up to $8k/mo for AWS? Unless you are well-funded, yes, that is a bad idea. I'd do some lite design and also a monthly audit and decomm anything not essential.",
          "score": 1,
          "created_utc": "2026-02-20 14:56:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gdcuk",
          "author": "liverdust429",
          "text": "Don't pause for a month but don't wait until after launch either. Spend a week: tag everything, lock down IAM, turn on CloudTrail, set a billing alarm. That alone keeps 8k from silently becoming 20k and prevents the \"oh shit\" moment when your first enterprise customer asks for a security review.",
          "score": 1,
          "created_utc": "2026-02-20 17:15:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hkltb",
          "author": "AftyOfTheUK",
          "text": "The best scenario is the one in which you planned ahead, architected well, kept costs down, got to market with a good product fit, and then succeeded\n\nThe next best scenario is one in which you have to spend quite a lot of money to re-architect at some point your journey, but still achieve the above. DM if you like, that's what I do, and I will be looking for a new role in the summer.\n\nThe worst scenario is one in which you slow down to optimize, miss your window (or spend too much on architecture instead of demonstrable features), and fail completely.\n\nThere's almost nothing you can't architect your way out of, it's just a matter of cost and time. It may be better to allocate five guys to it in a year after a big funding round, than allocate one guy to it today, who would otherwise have been building critical features",
          "score": 1,
          "created_utc": "2026-02-20 20:37:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hzjxu",
          "author": "Neves_Space_Corps",
          "text": "Incremental phased builds in test, with CDK deploy / destroy cycles. Intelligently scaling infrastructure based on test findings. These will help. \n\nWithout a user load, there is no reason to keep infrastructure running all the time.\n\nIaC is your friend here.\n\n$8k/month pre launch sounds like something is hugely amiss.",
          "score": 1,
          "created_utc": "2026-02-20 21:51:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ie6dl",
          "author": "revanthmatha",
          "text": "don‚Äôt listen to any of the guys here unless they have practical experience. the solution is actually really simple. \n\ngo into cursor or ide of choice and literally give it a service account with full system admin api key. have it start going through all the resources you have with your code base. \n\nhave it write an as is document of everything. then prompt it to optimize. read the plan it comes up with and execute. \n\nthis is like a 1-2 days task depending on how many resources you have to consolidate. \n\nfor example if you have many different databases, why not just 1 db with many tables etc.",
          "score": 1,
          "created_utc": "2026-02-20 23:09:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6iz7wd",
          "author": "linux_n00by",
          "text": "OP you have at least a VPC configured?",
          "score": 1,
          "created_utc": "2026-02-21 01:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jzi46",
          "author": "AlphaToBe",
          "text": "Just want to add the practical side because I have been in your exact position and it helped me to just SEE the numbers first before making any big decisions.\n\n\nRun this and it will show you your top 5 money burners right now:\n\n\n```\naws ce get-cost-and-usage \\\n  --time-period Start=2026-02-01,End=2026-02-21 \\\n  --granularity DAILY \\\n  --metrics UnblendedCost \\\n  --group-by Type=SERVICE \\\n  --query 'ResultsByTime[-1].Groups | sort_by(@, &Metrics.UnblendedCost.Amount) | [-5:]'\n```\n\n\nOnce you see that list, I bet at least one of these will jump out:\n\n\n**NAT Gateway** , $0.045 per GB processed. Sounds like nothing but a chatty microservice pushing logs externally can burn $2k/mo through a single NAT. I have seen this catch people off guard more than once.\n\n\n**RDS** , running a db.r5.large 24/7 in dev? Thats ~$260/mo sitting idle. Three environments? $800/mo on databases nobody is querying. Easy to miss.\n\n\n**Public IPv4 addresses** , since Feb 2024 AWS charges $3.65/mo PER public IP. Got 20 EIPs sitting around from \"I was just testing something\"? Thats $73/mo for literally nothing.\n\n\nThese two commands helped me find the orphans in my own account:\n\n\n```\naws ec2 describe-addresses \\\n  --query 'Addresses[?AssociationId==`null`].[PublicIp,AllocationId]' \\\n  --output table\n```\n\n\n```\naws ec2 describe-volumes \\\n  --filters Name=status,Values=available \\\n  --query 'Volumes[].[VolumeId,Size,CreateTime]' \\\n  --output table\n```\n\n\nAnd honestly the one thing I wish I had done from day one is just set a budget alert. Takes 30 seconds and it would have saved me a lot of surprises:\n\n\n```\naws budgets create-budget --account-id YOUR_ACCOUNT \\\n  --budget '{\"BudgetName\":\"dont-surprise-me\",\"BudgetLimit\":{\"Amount\":\"5000\",\"Unit\":\"USD\"},\"TimeUnit\":\"MONTHLY\",\"BudgetType\":\"COST\"}'\n```\n\n\nYou dont need a perfect architecture to start. Just spend one afternoon with those commands and you will probably find $2-3k in quick wins. That buys you time to plan properly without bleeding money.",
          "score": 1,
          "created_utc": "2026-02-21 05:14:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k09p6",
          "author": "Jin-Bru",
          "text": "It all depends on your appetite for risk.\n\nYou could just spin up services and hack them together.  Your maintenance cost go up, your security is non existent, your feature deployment success rate goes down.  Constantly chasing bugs and metrics and looking for a reason why.\n\nOr do it your way, with architecture.  Design it first then build it.  Or design it while you build it.  But it ahould have an architect design something for you.\n\nAt least use what you have to get some IaC mappings and use an IaC (Terraform, CF, Ansible) to build for you.   In this way it will always be documented.   The other way is fraught with sleepless nights.\n\n\nIts a very bad idea.  The day you go public facing is the day you become a target.  It's not just the good guys who will visit your sites.... the bad guys are always looking for a new soft target.",
          "score": 1,
          "created_utc": "2026-02-21 05:20:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kwv4w",
          "author": "PokeRestock",
          "text": "I rushed an Elastic Cache adoption for better user experience and estimated the cost incorrectly. The cost was 1.5x more due to other necessary infra and I was already losing money with the move.\n\nIf I spent a day or two planning I could have had the same solution with S3, SQS, and Lambda at fraction of cost but I rushed for UX. Then I rushed to redesign and redefine when costs were too much. \n\nId say take the time to measure twice as bad design decisions take twice as much time and sometimes money to fix",
          "score": 1,
          "created_utc": "2026-02-21 10:25:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kx2jd",
          "author": "PokeRestock",
          "text": "Ill also add that sometimes infra optimization saves time later for amendments and recreation. Ideally you should have your entire stack defined in CDK and have a scaled down version for test, stage, and then production. Doing everything in production is a mistake that will cost you when live.",
          "score": 1,
          "created_utc": "2026-02-21 10:28:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67onqb",
          "author": "snorberhuis",
          "text": "Most startups figure it out as they grow, but they don't spend 8k every month. They stay below the $ 1000-a-month threshold. \n\nSpending already 8k a month is ridiculously high. I would expect that this is not necessary. You can have some very bad decisions built into your AWS Architecture that lock in your base spend. The migration costs keep being too low versus the other business opportunities, but they keep eating away at your runway/profit every month. In the end, you can still end up with running out of money.\n\nLast week, I ran a quick scan of the company's architecture, and they were spending $400k a year on AWS because of these bad choices. They could reduce their bill by $100k a year if they had built it properly from the start. The expectation was that, with the forecasted exponential growth of their company, this would grow at the same rate.\n\nIt is not crazy if you are already at 8K a month. Dm me if you want me to take a quick look.\n\n  \n",
          "score": 0,
          "created_utc": "2026-02-19 09:09:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67kkiw",
          "author": "EconomistAnxious5913",
          "text": "8 months? launch quicker in today's times - thats the first thing, launch beta's 1,2,3 but generally launch sooner trather than later.\n\naws bill 8 k isn't a worry.\n\nsand castles crashing isn't a worry. \n\nyes, you can fix later, if you don't have time now. it doesnt matter. it may result in un-ideal spikes and failures, but your target is to get customers, especially on a new product launch, that is your first priority.\n\ncustomers ayenge to tehy will be happy, to sab easily fix and handle ho jayega. yes firgure it out as they grow.\n\n",
          "score": -1,
          "created_utc": "2026-02-19 08:29:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ar3iw",
              "author": "Garetht",
              "text": "I hope you are able to recover from your stroke quickly and easily.",
              "score": 3,
              "created_utc": "2026-02-19 19:55:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6c1np8",
                  "author": "beluga-fart2",
                  "text": "It‚Äôs not a stroke, it‚Äôs Hindi my bro.  He said everything gets figured out eventually , which is generally true.  Even after you go out of business :)",
                  "score": 1,
                  "created_utc": "2026-02-19 23:57:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rata1g",
      "title": "If S3 vectors offer sub second latency, why does AWS say it's designed for infrequent access?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rata1g/if_s3_vectors_offer_sub_second_latency_why_does/",
      "author": "nucleustt",
      "created_utc": "2026-02-21 15:03:03",
      "score": 20,
      "num_comments": 22,
      "upvote_ratio": 0.8,
      "text": "I'm building a customer service agent and need a vector DB for RAG.\n\nNaturally, I gravitated toward S3 vectors because the 90% cost reduction was super attractive.\n\nI'm wondering if I'm making the right choice (even though I see RAG as a use case).\n\nBasically, the chatbot has to answer questions via WhatsApp.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rata1g/if_s3_vectors_offer_sub_second_latency_why_does/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6lylv2",
          "author": "Sirwired",
          "text": "Because \"sub-second\" is super slow vs. the alternatives.",
          "score": 62,
          "created_utc": "2026-02-21 15:04:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6m0aee",
              "author": "nucleustt",
              "text": "wow. Thanks. I can only imagine the throughput of an in-memory Redis vector DB then. \"Near instant\"",
              "score": -12,
              "created_utc": "2026-02-21 15:13:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6m2aog",
                  "author": "CoastRedwood",
                  "text": "How big is the data you‚Äôre storing? Bigger the dataset the higher the latency and greater the cpu usage.",
                  "score": 5,
                  "created_utc": "2026-02-21 15:24:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m2h5q",
          "author": "barnaclebill22",
          "text": "That's sub- second for each retrieval. You then need to submit the returned text as context to your LLM, so any question response might end up taking a few seconds. \nIt's easy to switch to something like Opensearch (or use it as a fast cache with S3 vectors), so might be worth trying. \nChatting with a human agent on WhatsApp typically takes several minutes per conversational turn (since they're all handling dozens of conversations).",
          "score": 16,
          "created_utc": "2026-02-21 15:25:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6meti1",
              "author": "HiCookieJack",
              "text": "Or PG Vector with Postgres Serverless ",
              "score": 10,
              "created_utc": "2026-02-21 16:27:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6n0ogv",
                  "author": "hergabr",
                  "text": "PgVector on Postgres works surprisingly well for a chatbot",
                  "score": 1,
                  "created_utc": "2026-02-21 18:16:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6mhr80",
              "author": "nucleustt",
              "text": "Thanks, these are all solutions I would test and compare",
              "score": 1,
              "created_utc": "2026-02-21 16:41:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6mfh90",
          "author": "jdanton14",
          "text": "If I had a database that has 900ms of latency, I'd fire my DBA and my storage vendor.",
          "score": 12,
          "created_utc": "2026-02-21 16:30:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6p44mo",
              "author": "coinclink",
              "text": "If you're working at scale though, vector algorithms are extremely taxing on a db engine, especially on huge datasets. 900ms consistently across a huge vector store is a good tradeoff vs spending thousands of dollars scaling out a bunch of read replicas that might still get deadlocked when a flood of requests come in.",
              "score": 3,
              "created_utc": "2026-02-22 01:11:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6mhxcr",
              "author": "nucleustt",
              "text": "lol. I wonder if the whatsapp api supports the \"is typing\" indicator.",
              "score": 1,
              "created_utc": "2026-02-21 16:42:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nshiy",
          "author": "Due-Horse-5446",
          "text": "500ms is sub second too..",
          "score": 4,
          "created_utc": "2026-02-21 20:38:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ol5r5",
              "author": "nucleustt",
              "text": "true. But even a second isn't bad. Hopefully someone on chat can wait \\~5 sec for a response... hopefully",
              "score": 1,
              "created_utc": "2026-02-21 23:14:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6oabfi",
          "author": "tank_of_happiness",
          "text": "I‚Äôm using it for your same use case. The LLM‚Äôs thinking takes way longer. I‚Äôm happy with the results I‚Äôm getting and the cost is almost nothing at my volume. Previously I was using open search and paying a couple hundred a month.",
          "score": 3,
          "created_utc": "2026-02-21 22:13:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6olb1w",
              "author": "nucleustt",
              "text": "wtf, couple hundred a month vs almost nothing!? That's crazy cost reduction!",
              "score": 1,
              "created_utc": "2026-02-21 23:15:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6qlamu",
              "author": "nucleustt",
              "text": "Also, it's reassuring that someone else is using it for my use case.",
              "score": 1,
              "created_utc": "2026-02-22 07:50:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6n2cnh",
          "author": "yarenSC",
          "text": "In S3 terms, infrequent access is a billing term more than anything else",
          "score": 4,
          "created_utc": "2026-02-21 18:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6okz0x",
              "author": "nucleustt",
              "text": "ha. gotcha",
              "score": 1,
              "created_utc": "2026-02-21 23:13:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6mz94n",
          "author": "rexspook",
          "text": "Sub second is infrequent in computing terms",
          "score": -2,
          "created_utc": "2026-02-21 18:09:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6kyza",
      "title": "Nested virtualization now available on EC2 instances",
      "subreddit": "aws",
      "url": "https://github.com/aws/aws-sdk-go-v2/commit/3dca5e45d5ad05460b93410087833cbaa624754e",
      "author": "ckilborn",
      "created_utc": "2026-02-16 20:29:52",
      "score": 17,
      "num_comments": 1,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "compute",
      "permalink": "https://reddit.com/r/aws/comments/1r6kyza/nested_virtualization_now_available_on_ec2/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5qvolp",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'compute'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+compute).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-16 20:29:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6fnfx",
      "title": "m8azn single-thread performance tops EC2 benchmarks",
      "subreddit": "aws",
      "url": "https://go.runs-on.com/instances/ec2/m8azn",
      "author": "crohr",
      "created_utc": "2026-02-16 17:19:08",
      "score": 17,
      "num_comments": 2,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "ci/cd",
      "permalink": "https://reddit.com/r/aws/comments/1r6fnfx/m8azn_singlethread_performance_tops_ec2_benchmarks/",
      "domain": "go.runs-on.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5quolc",
          "author": "SkywardSyntax",
          "text": "Finally something that can run my homelab's nginx-proxy-manager docker container and tailscale exit node at the same time",
          "score": 7,
          "created_utc": "2026-02-16 20:24:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5spe0z",
          "author": "Big-Razzmatazz-2899",
          "text": "‚ÄúIt‚Äôs the A Z N, better recognize!‚Äù",
          "score": 1,
          "created_utc": "2026-02-17 02:27:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8cd6p",
      "title": "Is Elastic Beanstalk down?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r8cd6p/is_elastic_beanstalk_down/",
      "author": "vvvwwwwvvwwwvwvvwvvw",
      "created_utc": "2026-02-18 19:16:15",
      "score": 14,
      "num_comments": 3,
      "upvote_ratio": 0.82,
      "text": "All of a sudden our Elastic Beanstalk environments started failing. We can no longer deploy or even change configurations. It seems like AWS made changes to Elastic Beanstalk, CloudFormation, and RDS, and they are no longer communicating correctly.\n\n\n\nWe have **12 environments affected, including production.**\nus-east1\n\nWhen checking AWS Repost, I see others reporting errors that started suddenly:  \n[https://repost.aws/questions/QUeJkV4KL9TNKzZeY0Z\\_0B0A/uploading-new-version-to-elastic-beanstalk-gives-vpc-migration-error](https://repost.aws/questions/QUeJkV4KL9TNKzZeY0Z_0B0A/uploading-new-version-to-elastic-beanstalk-gives-vpc-migration-error)\n\n\n\nSo far I have not seen any official AWS notice about this.\n",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1r8cd6p/is_elastic_beanstalk_down/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o64cng4",
          "author": "pixlgeek",
          "text": "Nothing on our end.",
          "score": 3,
          "created_utc": "2026-02-18 20:38:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64keo8",
          "author": "ruibranco",
          "text": "classic aws move - make breaking changes to the EB/CloudFormation integration and the health dashboard stays green the entire time. check if the repost thread mentions which region, we had similar cfn drift issues in eu-west-1 last month after a silent platform update",
          "score": 2,
          "created_utc": "2026-02-18 21:14:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r621ua",
      "title": "When can we get certification vouchers?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r621ua/when_can_we_get_certification_vouchers/",
      "author": "HistoricalTear9785",
      "created_utc": "2026-02-16 06:21:51",
      "score": 11,
      "num_comments": 6,
      "upvote_ratio": 0.83,
      "text": "I wanted to check if anyone knows about any upcoming AWS events where free certification vouchers might be offered. Last year, they provided 50% discount vouchers are there any similar opportunities coming up this year?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1r621ua/when_can_we_get_certification_vouchers/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5nejtq",
          "author": "MavZA",
          "text": "AWS provides them sporadically really. Sometimes they have certification drives, but it‚Äôs up to their internal team planning etc. we simply don‚Äôt have a view on that.",
          "score": 5,
          "created_utc": "2026-02-16 08:08:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o1cy4",
          "author": "alex_aws_solutions",
          "text": "When you take an exam they will provide you with an 50% discount for the next one. ",
          "score": 3,
          "created_utc": "2026-02-16 11:39:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qrxsu",
          "author": "Thinguist",
          "text": "Just get a job that pays for them. I‚Äôve done 7 now for free.",
          "score": 1,
          "created_utc": "2026-02-16 20:11:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5splkf",
          "author": "socaltrey",
          "text": "Our account manager always seems to be offering them.  I've never found anyone on the team that wants to take AWS on the offer of free certification.",
          "score": 1,
          "created_utc": "2026-02-17 02:28:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c8iba",
          "author": "Diablo-x-",
          "text": "They offered 50% discounts last year in May, just be careful of their payment gateway, i got charged twice.",
          "score": 1,
          "created_utc": "2026-02-20 00:37:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r723uk",
      "title": "How do you build intuition for AWS architecture trade-offs",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r723uk/how_do_you_build_intuition_for_aws_architecture/",
      "author": "Zephpyr",
      "created_utc": "2026-02-17 10:04:21",
      "score": 8,
      "num_comments": 15,
      "upvote_ratio": 0.79,
      "text": "I have been working with AWS for about two years now, mostly ECS deployments and some Lambda functions. My current company uses AWS but most of my work is maintaining what someone else built. I understand how the services work individually but I struggle when asked to design something from scratch.\n\nI have been trying to improve. I go through AWS documentation, watch re:Invent videos, use A Cloud Guru for structured learning and work through small projects to practice IaC code. I use Claude and beyz coding assistant when I am writing Terraform or CDK to make sure my logic makes sense and I am not missing obvious mistakes. I have also started reading through the AWS Well-Architected Framework to understand how AWS recommends thinking about these decisions.\n\nMy problem is I can follow a tutorial but I cannot make architecture trade-offs on my own. When I try to apply it to a real scenario I get stuck. When someone asks why I chose a specific service over another or how I would balance cost versus performance versus operational complexity I do not have a good answer beyond what I read in a blog post. I know the tools exist but I do not know when to pick one over the other.\n\nFor those who went from working with AWS to actually designing AWS solutions, how did you build that intuition for trade-offs? Did you just keep doing practice designs until it clicked or is there a better way to learn the reasoning behind architecture decisions?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1r723uk/how_do_you_build_intuition_for_aws_architecture/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5uceos",
          "author": "respectful_stimulus",
          "text": "Your north star should simply be solving the problem in the simplest way possible. And in the most cost effective way possible. Do this and you‚Äôre already at an advantage over the majority who implement unnecessarily expensive and complex architectures.",
          "score": 23,
          "created_utc": "2026-02-17 10:16:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wa5nc",
              "author": "justin-8",
              "text": "This plus as a thought exercise when picking various pieces of the architecture: what are the limits. E.g. you pick lambda, the 15m max runtime is an obvious limit, is that important for this workload? Or you're using EC2 instances and an auto scaling group - scaling up can take several minutes, do you need to support bursty workloads or are they predictable? Can you just schedule over scaling during peak hours instead? Or do the busiest apis need some other mitigation (caching, migration elsewhere, etc).\n\nBut picking the simplest way should always be first.",
              "score": 3,
              "created_utc": "2026-02-17 17:15:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6gxect",
              "author": "Useful-Process9033",
              "text": "This is the best advice in the thread. The number of over-engineered architectures I've seen that could have been a single ECS service behind an ALB is staggering. Simplicity is a feature, and the teams that ship fastest are usually the ones with the most boring infrastructure.",
              "score": 2,
              "created_utc": "2026-02-20 18:46:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5udo2l",
          "author": "Bub697",
          "text": "For me it‚Äôs all experience and solving the problems yourself.  Build a solution, get it working and then ask, ‚Äúwhat if I needed to handle 10x the load?‚Äù  ‚ÄúWhat happens if an AZ fails?  Let‚Äôs try it!‚Äù  ‚ÄúWhat does it actually cost to run this?  What did I estimate it would cost?  Why was I so far off?‚Äù  \n\nI‚Äôd also add the leaning on an LLM for all these answers will not help you improve in the long term.  Read the docs, try things out, break it, fix it.",
          "score": 7,
          "created_utc": "2026-02-17 10:28:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uq8vt",
          "author": "weirdbrags",
          "text": "it‚Äôs 100% experience. the best way to learn how (and why) to choose/do the right thing is to choose/do the wrong thing, and to then do it over. \n\ni know that‚Äôs easier said than done though. the challenge is finding yourself in a role / environment where you have that kind of an opportunity.",
          "score": 6,
          "created_utc": "2026-02-17 12:13:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uwrlj",
          "author": "Sirwired",
          "text": "Look for \"architecture patterns\"; most problems you encounter have been solved by others, who write a blog post or whatever about it.\n\nBut in the end it comes down to hard-won experience.  I was in IT for about seven years or so before I did any architecture, and fifteen years before \"architect\" was part of my job title.  Not saying you can't do it faster, just that you shouldn't be disappointed if you can't go from \"wow, cloud is neat!\" to \"cloud architect\" in a couple years.",
          "score": 3,
          "created_utc": "2026-02-17 12:57:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vac0p",
              "author": "Own-Manufacturer-640",
              "text": "Exactly this. If i have to make decisions as a junior i just follow the architecture pattern and then research on it",
              "score": 1,
              "created_utc": "2026-02-17 14:14:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5v27e9",
          "author": "mrbiggbrain",
          "text": "I have tried to wrap up a very complex topic into words that might make some sense but big concepts tend to not map so cleanly to a Reddit comment. \n\nAs others have said experience is a key factor. But I always like to say experience is not measured in years it is measured in moments. Good decisions, bad decisions, how you handled an outage, how you rebuilt after a disaster. You'll gain way more experience trying and failing then always staying safe. \n\nBut that does not mean you should not learn from others. There are lots of good books / articles / blogs / postmortems out there on systems design written from both the software engineers and infrastructure engineers point of view. \n\n* Understanding CAP theorem and what Consistency, Availability, and Partitioning are. \n* Understanding how large applications fail, when you should let them fail, and how to build in ways to fail gracefully. \n* Understanding how physics affects infrastructure design. \n* Understanding how various patterns work such as the transactional outbox or idempotent writes in log based messaging queues. \n\nLearn, apply, experience, repeat. I tend to read these books and mark off the things I can use now in one color and the things I think are useful but I don't have a use for in another. Then I come back later and see if I have a use for them later. ",
          "score": 2,
          "created_utc": "2026-02-17 13:29:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ufjrb",
          "author": "swiebertjee",
          "text": "Experience, but it's also logical and can be studied. Especially from other people's code.\n\nYou say ECS and Lambda. Have you set up projects with these tools? Do you have a preference? When would you use one vs another?\n\nFor me, I like Lambda for event driven architecture where you want replayability using (dead letter) queues. For API's serving users, I prefer traditional containers as they can keep multiple endpoints warm. Sure you can do that with Lambda too if you configuring API gateway to point multiple API endpoints to a single provisioned \"Lambdaton\". But at that point, wouldn't you like a service that is easier to develop and test locally? Etc etc.\n\nAnd for pricing, you can use the AWS cost calculator and test some scenarios. How does ECS on Fargate compare to ECS on EC2? You'll find out EC2 is cheaper, but does that mean it's better? How does the operational overhead cost compare? And is the (potential) cost saved worth the operational risk?\n\nIt all depends on context.",
          "score": 1,
          "created_utc": "2026-02-17 10:45:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ujh8o",
          "author": "SonOfSofaman",
          "text": "You mentioned using some tools to \"make sure ... I am not missing obvious mistakes\".\n\nI think you're doing yourself a disservice. Try to not rely on those tools. Don't get me wrong, I'm not an AI-hater. The problem isn't the tools. My point is you learn more from making mistakes than you do by avoiding them.\n\nIf you are baking a cake and you follow a recipe, the result is likely going to be a success. But what have you learned other than how to follow a recipe? If you start with no recipe, only a vague knowledge of which ingredients you need, you'll probably get it wrong at first. But then you'll adjust and iterate and learn from the experience.\n\nEmbrace mistakes. Don't avoid them.\n\nCaveat: with any cloud provider, mistakes can be costly! Always, always, always understand how much a service is going to cost before you use it, then clean up anything you aren't using.",
          "score": 1,
          "created_utc": "2026-02-17 11:19:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v9xtd",
          "author": "SpecialistMode3131",
          "text": "You may have to go work at a startup or similar environment so you're involved in making the tradeoffs.",
          "score": 1,
          "created_utc": "2026-02-17 14:12:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yt3xx",
          "author": "donkanator",
          "text": "First unzoom and then refine. Identify pattern first and general purpose and then follow the crumb trail of requirements. Somebody needs something crazy built? Welp, it looks like a three-tier web app to begin with. That's probably 90% of everything out there. Ingress, compute storage. Input > process > output. \n\nUse thought frameworks: Synchronous versus asynchronous (event driven).  Five architectural pillars. Data temperature. \n\nBDAT from TOGAF is a very good thought process. First you lay out business, application, data flows, and then technical architecture snaps in.",
          "score": 1,
          "created_utc": "2026-02-18 00:42:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o606uzj",
          "author": "philwills",
          "text": "Solve the problem without aws first. Figure out the data structures and interfaces needed. Then, find appropriate services for the different pieces of the solution.",
          "score": 1,
          "created_utc": "2026-02-18 05:37:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r791ot",
      "title": "How to automate aws savings plans without manual quarterly analysis?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r791ot/how_to_automate_aws_savings_plans_without_manual/",
      "author": "My_Rhythm875",
      "created_utc": "2026-02-17 15:30:12",
      "score": 8,
      "num_comments": 19,
      "upvote_ratio": 0.9,
      "text": "Every quarter there's this ritual where you analyze usage patterns, try to predict future compute needs, calculate optimal savings plan coverage, submit recommendations to leadership, get approval, then finally buy commitments. By the time the whole process finishes usage has already changed and the analysis is outdated.\n\nCommitment recommendations in cost explorer are okay as a starting point but they don't account for upcoming projects, seasonal traffic patterns or planned architecture changes. They just look at historical usage and say \"buy this much\" which is often wrong.\n\nUnder committing means leaving savings on the table, over-committing means paying for capacity you don't use and the optimal middle ground requires constant adjustment. Three year commitments save more but lock you in longer which is risky for startups where everything changes constantly.\n\nCoverage percentage drops randomly when workloads shift and you need to evaluate whether to buy more which savings plan type makes sense (compute vs ec2) and what term length is appropriate. Feels like this should be automated somehow but I haven't found anything that actually works reliably\n\nIs there a good workflow for this or is manual quarterly analysis just the reality?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1r791ot/how_to_automate_aws_savings_plans_without_manual/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5wcfwj",
          "author": "SpecialistMode3131",
          "text": "If you pay for it, you can get another human being to do this for you (We do it).  But at the end of the day, unless someone who really cares is looking hard at your spend versus your business objectives, you cannot know if that money is being wisely spent.\n\nSo the only real decision is whether you're doing that in-house, paying someone for it out of house, or just choosing not to do it at all.  People do all three things, with varying degrees of competence, with the variance in results you'd expect.",
          "score": 5,
          "created_utc": "2026-02-17 17:27:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5whnw7",
          "author": "CharacterHand511",
          "text": "Three year commitments are scary for startups yeah, one-year terms are safer because who knows what infrastructure will look like in 2027",
          "score": 2,
          "created_utc": "2026-02-17 17:51:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wklc1",
          "author": "galiyonkegalib",
          "text": "cost explorer recommendations are too simplistic, they don't understand context like \"migrating to lambda next quarter\" or \"traffic doubles in q4 every year\"",
          "score": 2,
          "created_utc": "2026-02-17 18:05:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zk284",
          "author": "MateusKingston",
          "text": "If by the time you're buying the commitment the analysis is outdated either you're taking way too long (3+ months) or you're using SP the wrong way.\n\nIt's a commitment for 1/3 years, it can't get outdated in weeks, otherwise you would have made a wrongful commitment in the first place.\n\nOverall this isn't supposed to be an automated process as there is far too many external variables that no system will take into account and is a task that shouldn't realistically take too much time and isn't very frequent.\n\nYou can have tooling to assist you in that but I don't know of any (besides the built in aws console ones)",
          "score": 2,
          "created_utc": "2026-02-18 03:06:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w2np5",
          "author": "pausethelogic",
          "text": "There are some companies that will handle this for you. In general though, this is a manual process unless you want to automate it yourself. As you‚Äôve mentioned, there is a lot of real risk associated with the different options available",
          "score": 2,
          "created_utc": "2026-02-17 16:38:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w4qpr",
          "author": "BloodAndTsundere",
          "text": "As far as overcommitting goes, I think there is a secondary market for reserved instances which can mitigate some of that risk.",
          "score": 1,
          "created_utc": "2026-02-17 16:48:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5whzbt",
          "author": "Sea-Car8041",
          "text": "coverage dropping is annoying bc u have to investigate why, is it a good change (moved to cheaper instances) or bad change (accidentally lost coverage)... requires manual analysis either way",
          "score": 1,
          "created_utc": "2026-02-17 17:53:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wmmjr",
          "author": "lostsomewhere--",
          "text": "Automation would be nice but trusting a tool to automatically buy commitments feels risky without human review first like what if the algorithm makes a mistake and commits you to $50k of unnecessary capacity. Some tools like prosperops try to do this or there's vantage autopilot feature that handles it but I'd want to really understand the logic before letting it run unsupervised",
          "score": 1,
          "created_utc": "2026-02-17 18:14:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60zqpb",
              "author": "TeekhiSamosaa",
              "text": "yeah auto-buying without approval workflow is terrifying, needs at least a human-in-the-loop to review recommendations before executing",
              "score": 1,
              "created_utc": "2026-02-18 09:55:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o619ivt",
                  "author": "Vodka-_-Vodka",
                  "text": "Agreed, automation should help with analysis and maybe executing small incremental adjustments, but big commitment decisions need oversight",
                  "score": 1,
                  "created_utc": "2026-02-18 11:20:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o61j470",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-02-18 12:30:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wo86r",
          "author": "Connect_Street_867",
          "text": "Compute savings plans vs ec2 specific is another decision point, compute is more flexible but ec2 specific saves more... depends on whether you value flexibility or max savings",
          "score": 1,
          "created_utc": "2026-02-17 18:21:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6153xf",
              "author": "Unlucky_Abroad7440",
              "text": "compute makes sense when instance types change frequently, ec2 specific would lock you into old instance families",
              "score": 1,
              "created_utc": "2026-02-18 10:43:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o62asv8",
                  "author": "lunahanae",
                  "text": "Honestly this whole thing feels like something cloud providers should handle better natively like they have all the usage data why not just offer a \"smart commitment\" option that adjusts automatically",
                  "score": 1,
                  "created_utc": "2026-02-18 15:02:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5xhihu",
          "author": "TooMuchTaurine",
          "text": "Why quarterly, we just do it once a year, try to maintain about 95% coverage at peak to give us some buffer.",
          "score": 1,
          "created_utc": "2026-02-17 20:39:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6notk1",
          "author": "AlphaToBe",
          "text": "The quarterly ritual you described is painfully familiar. What helped me simplify it:\n\n**Stop chasing perfect coverage.** The goal isnt 100%. Its covering your stable floor and leaving the variable portion on-demand. Trying to optimize the last 5-10% of coverage is where most of the analysis time goes and the savings are marginal.\n\nPractical workflow that replaced quarterly analysis for me:\n\n1. Pull your last 90 days of on-demand spend from Cost Explorer. Look at the MINIMUM daily spend, not the average. Thats your floor.\n\n```\naws ce get-cost-and-usage \\\n  --time-period Start=2025-11-21,End=2026-02-21 \\\n  --granularity DAILY \\\n  --metrics UnblendedCost \\\n  --filter '{\"Dimensions\":{\"Key\":\"PURCHASE_TYPE\",\"Values\":[\"\"]}}' \\\n  --query 'ResultsByTime[].{Date:TimePeriod.Start,Cost:Total.UnblendedCost.Amount}'\n```\n\n2. Buy 1-year no-upfront Compute SPs to cover ~70-80% of that floor. Not 95%. Compute SPs because they flex across instance families, regions, and even Lambda/Fargate. If you migrate from m5 to m7g next quarter it still applies.\n\n3. Revisit every 6 months, not quarterly. If your floor shifted significantly, top up. If not, dont touch it.\n\nThe reason quarterly feels like a treadmill is because youre re-analyzing the variable portion that changes constantly. Ignore it. Let it stay on-demand. The floor is what matters and it barely moves for most workloads.\n\nOn the 3-year commitment question. I wouldnt do it at a startup. The extra discount over 1-year is maybe 10-15% more savings but you lose all flexibility. One architecture change and youre stuck paying for capacity you dont use. 1-year no-upfront gives you 30-40% discount with an exit every 12 months.",
          "score": 1,
          "created_utc": "2026-02-21 20:18:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9r4il",
      "title": "How to guarantee consistency when deleting items from dynamodb?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r9r4il/how_to_guarantee_consistency_when_deleting_items/",
      "author": "Select_Extenson",
      "created_utc": "2026-02-20 09:45:29",
      "score": 7,
      "num_comments": 26,
      "upvote_ratio": 1.0,
      "text": "Let's say I want to delete 100000 items from dynamodb, what is the best approach to delete \"all-or-nothing\", TransactWriteItems only support 100 items, so I don't want to cause inconsistency in my data if for some reason the delete function fails alongs the way.\n\nAnd in my case, I simply couldn't find a solution to implement it with GSI, so the only solution for me is to delete them manually.",
      "is_original_content": false,
      "link_flair_text": "database",
      "permalink": "https://reddit.com/r/aws/comments/1r9r4il/how_to_guarantee_consistency_when_deleting_items/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6e9dq3",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'database'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+database).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-20 09:45:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ee2vq",
          "author": "pint",
          "text": "this is a problem you should not have. the requirement itself screams badly that something is really wrong there, and you should seriously reconsider.\n\nwithout knowing more about the problem, here is a theoretical solution.\n\n1. add a new data field e.g. \"obsolete\" to the records, optionally add a ttl too\n1. modify the software to obey that field\n1. deploy the software, which means at an instant all the records are now \"gone\"\n1. let the ttl delete the records, or delete them manually at your convenience\n\nstep 2 is the most problematic, because the \"software\" might be a dozen different systems, and they might rely heavily on the assumption that queries will return rows in a timely manner, which is now not guaranteed.\n\nsuch operations have to be considered *in advance* with dynamodb.",
          "score": 10,
          "created_utc": "2026-02-20 10:28:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6eeyt9",
              "author": "Select_Extenson",
              "text": "I think my mistake is I shouldn't use dynamodb and use relational databases instead, the project I'm working on contains a lot of related data and I need to gunaratne consistency across them.\n\nIt was my first time using it, can you please tell me your opinion on this? is it actually a bad choice to use dynamodb when you have a project with a lot of related that or is it just me that I didn't design my database properly? but I don't think I did design it poorly, I tried my best to design in the most optimal way but it misses flexibility when it comes to querying and manipulating related data.",
              "score": 1,
              "created_utc": "2026-02-20 10:36:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6eg08o",
                  "author": "xtraman122",
                  "text": "There‚Äôs a way to do just about any pattern with Dynamo, but it often requires lots of careful planning around indexes and keys. What‚Äôs really rough with Dynamo is having  changes to the access patterns and relationships down the road. \n\nLots of people end up in the same situation as you, choosing a NoSQL DB because they think it‚Äôs cool, solves all their problems, or just heard about it too much in blog posts and conferences (Like your CTO probably did‚Ä¶). Unless you have very high throughout in both reads and writes that a traditional relational DB can‚Äôt handle, it‚Äôs likely not actually necessary for your use case.",
                  "score": 5,
                  "created_utc": "2026-02-20 10:45:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6eglzf",
                  "author": "RecordingForward2690",
                  "text": "Without knowing the details, but just going on what you say here, I agree that your problem screams \"Relational Database\" to me. DynamoDB is simply not designed or intended for your problem.\n\nFor all practical purposes, if you use DynamoDB for something like you describe, you will be writing a custom layer that tries to give you Relational Database functionality (multi-table queries, ACID compliance across multi-table operations and such) on top of DynamoDB. That has already been done, and it's called a Relational Database.\n\nDynamoDB shines when you just have a handful of tables, need extremely high performance at any scale, and are able to live with the loss of ACID compliance - or are willing to write additional code to get some of that ACID compliance back.",
                  "score": 5,
                  "created_utc": "2026-02-20 10:51:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6egq5d",
                  "author": "pint",
                  "text": "to be honest, relying rdbms referential integrity for such huge operations is also not recommended. it is bad design there too, even if at least possible.\n\ni advocate for separation of data. in the old days, we just dumped everything in \"the database\", because where else data would go, right? so different types of data ended up there, configuration, users and privileges, transactions, logs, web sessions, temporary data. all these data types have very different usage patterns, and probably shouldn't be in the same database.\n\none nice pattern is to keep operational data in dynamodb, and use dynamodb streams to deliver historic data to s3 or a rdbms for statistical analysis. meanwhile, keep configuration in ssm, user data maybe in whatever authentication tool you are using, logs in cloudwatch.\n\nrely more on program logic when aggregating data from different sources (as opposed to sql).\n\nwhen it comes to referential integrity, ask yourself the question: can we somehow get away without it? can be employ a little bit of cleverness or extra code to not have to deal with it?",
                  "score": 4,
                  "created_utc": "2026-02-20 10:52:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6eioux",
                  "author": "SonOfSofaman",
                  "text": "I think almost everyone goes through what you're going through. It's sort of a rite of passage with DynamoDB.\n\nDynamoDB is, as you know, very different from relational databases. With DynamoDB it is imperative that you fully understand all of your access patterns ahead of time, then model the table accordingly. If your access patterns change, then you may need to remodel your table.\n\nWith relational databases, you don't need to fully understand the access patterns ahead of time. It helps to know your access patterns ahead of time, but relational databases are flexible and adaptable.\n\nYou can do what you want with DynamoDB, but the table in its current form wasn't designed to support this \"bulk delete\" access pattern.\n\nI think you're at a point where you need to do a bit of redesign. Some of the other comments have practical solutions that may be helpful.",
                  "score": 1,
                  "created_utc": "2026-02-20 11:09:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6eg7sl",
          "author": "RecordingForward2690",
          "text": "I had a somewhat similar problem, where DynamoDB would collect millions of transactions, and at some point in time all transactions (within a Partition Key set) older than a particular timestamp needed to be deleted/invalidated/ignored as a whole. That timestamp was not known in advance - it depended on a user action - so I could not use the TTL mechanism. Like you noticed, there is no way to do that directly in a consistent manner.\n\nSome of the other proposals, where you update each item with a deleted=true or other attribute, suffer from the same problem: You need to either update or delete a large number of items in bulk, and the bulk operations in the DynamoDB are simply too limited in the number of items they can handle in one go, and in an atomic way.\n\nIn my case, my transactions fortunately were timestamped, and the transactions that needed to be deleted from the table were all done before a particular timestamp - the time of that user action. So I added an additional table \"DeleteMarkers\". As soon as the user event happened, I entered the timestamp of the Delete event into the DeleteMarkers, with the same partition key as the transaction table. Now, instead of doing one query to the TransactionsTable, I had to do two queries:\n\n1. Query to the DeleteMarkers table to get the up-to-date DeleteMarker\n2. Query to the TransactionTable to get the transactions, with the limitation that the results returned should be newer than the DeleteMarker.\n\nDynamoDB is quick enough that the additional query time did not impact latency.\n\nAfter this, I could delete the old transactions from the TransactionTable in the background. I used the TTL mechanism for that, but you can also do a query or even a scan if you want to. This is not time-critical or transaction-critical anymore.\n\nFrom a design point of view, DeleteMarkers has the SessionID as my Partition Key, no Sort Key (so when queried it returns one item only), and a field \"Timestamp\". TransactionTable has the SessionID as my Partition Key, and the Timestamp as the Sort Key.\n\nIn my application I did not need to do any other queries so I did not need any GSIs.\n\nFor your application, if you are able to formulate a SQL-query-like-thing that would be able to delete the \\~1M items, then you can also put the variables of that query in a similar DeleteMarkers table, and use those fields in your query to your TransactionTable. It's a bit more complex than the Timestamps I used, but not impossible.",
          "score": 2,
          "created_utc": "2026-02-20 10:47:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ebxoi",
          "author": "safeinitdotcom",
          "text": "DynamoDB just doesn't support this natively at that scale. You can either:\n\n\\- add a`deleted=true` attribute or smth like that, filter it in queries and clean up later.\n\n\\- BatchWriteItems but log which batches completed somewhere, on failure you resume instead of starting over.\n\nIf you need true all-or-nothing for 100k records, that's a relational DB problem. DynamoDB isn't designed for it.",
          "score": 3,
          "created_utc": "2026-02-20 10:08:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6echoq",
              "author": "Select_Extenson",
              "text": ">If you need true all-or-nothing for 100k records, that's a relational DB problem. DynamoDB isn't designed for it.\n\nYeah, that's the a mistake, I got told to use Dynamodb by our CTO, I had no experience with it so I didn't know the props and downs for it, after months of struggling trying to build our project that is mostly contains a lot of related data in dynamodb, it became really pain in the ass to maintain it",
              "score": 3,
              "created_utc": "2026-02-20 10:14:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6eykkr",
                  "author": "KainMassadin",
                  "text": "> I got told to use Dynamodb by our CTO\n\nBeen there, sucks.",
                  "score": 5,
                  "created_utc": "2026-02-20 13:03:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mqa1u",
                  "author": "csharpwarrior",
                  "text": "A lot of people just hear how cool DynamoDB is and they don‚Äôt spend enough time understanding the use cases it solves. And more importantly understanding the use cases it is bad at.",
                  "score": 1,
                  "created_utc": "2026-02-21 17:24:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6eb1vf",
          "author": "ebykka",
          "text": "This is one of the reasons why, after six years of using DynamoDB, we decided to give it up and migrate to RDS Aurora.\n\nWhile it was great for prototyping, maintenance, usage and consistency slowly became increasingly problematic.",
          "score": 1,
          "created_utc": "2026-02-20 10:00:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6elidc",
          "author": "SonOfSofaman",
          "text": "Are you able to ignore the unwanted items at read-time instead of deleting them? If those unwanted items have some common value upon which you can filter when you perform a read, to your consumer the items will be as good as deleted.\n\nThen you can casually delete the items in the background in batches or one by one at your leisure.",
          "score": 1,
          "created_utc": "2026-02-20 11:32:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f3con",
          "author": "garrettj100",
          "text": "You don‚Äôt want carpet, you want an area rug. ¬†And when I say ‚Äúcarpet‚Äù and ‚Äúarea rug‚Äù I mean ‚ÄúDynamoDB‚Äù and ‚ÄúRDS‚Äù.\n\nYou‚Äôre asking how to do a transaction and that means a relational database. ¬†That‚Äôs why they exist, because 40 years ago banks needed transactions.\n",
          "score": 1,
          "created_utc": "2026-02-20 13:31:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ft8jd",
              "author": "SpecialistMode3131",
              "text": "Maybe.  If you have 100 use cases for a table that are ideal for nosql (different schemas etc etc), and one use case requiring a transaction, do you instantly reject nosql?  Or do you hack around that one case?  Opinions and outcomes vary.\n\nOP didn't provide nearly enough business context to seriously decide one way or another.  \"The CTO told me to do it this way\" can be taken either way.",
              "score": 1,
              "created_utc": "2026-02-20 15:43:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fbk2e",
          "author": "solo964",
          "text": "How do you currently identify the items to be deleted? For example, are they collections of items where each item in a given collection has a common primary key? If you can identify them simply e.g. all items with a PK in the set { customer#12, customer#479, customer#90210 } then you could soft delete them by writing these PKs to control records e.g. an item with (PK = \"customer#12\", SK = \"control\", status = \"deleted\") and then modify the consumers of the table to first check if the given PK/control item was present with status=\"deleted\". Independently, have an async process that slowly deletes the actual items in batches, eventually deleting the control item.",
          "score": 1,
          "created_utc": "2026-02-20 14:15:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6l7rwi",
          "author": "GeorgeMaheiress",
          "text": "If the delete fails, retry until it succeeds. This doesn't have to be a problem.",
          "score": 1,
          "created_utc": "2026-02-21 12:07:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6oewe0",
          "author": "rexspook",
          "text": "I‚Äôm so curious about the use case where 100k records need to be deleted in an all or nothing approach. I think you‚Äôve already got a lot of good answers here. My first thought is soft delete for that portion and then real deletes in some batched cleanup job",
          "score": 1,
          "created_utc": "2026-02-21 22:38:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fseud",
          "author": "SpecialistMode3131",
          "text": "Leaving aside the debate over RDBMS, one option:\n\n1. mark the rows in the main table (A) with a deleted\\_id value (a new attribute, added to all elements to be deleted)\n2. When you're 100% sure you have them all marked, you have achieved transactional consistency the Stone Age way.  Now have a small additional table B you atomically add a row to, saying \"rows in A with this deleted\\_id are to be treated as deleted\".\n3. Modify your code to filter those out/never use them as valid rows (check B and if a row has that attribute, filter it).  (do this step before adding the row to B, if you want a true transactional experience).\n4. Delete the rows from A at leisure.  Then remove the row from B.\n\nNot pretty, but it will easily achieve what you want, and you can keep it around as a management mechanism.",
          "score": 0,
          "created_utc": "2026-02-20 15:39:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e9dpe",
          "author": "AutoModerator",
          "text": "Here are a few handy links you can try:\n\n- https://aws.amazon.com/products/databases/\n- https://aws.amazon.com/rds/\n- https://aws.amazon.com/dynamodb/\n- https://aws.amazon.com/aurora/\n- https://aws.amazon.com/redshift/\n- https://aws.amazon.com/documentdb/\n- https://aws.amazon.com/neptune/\n\nTry [this search](https://www.reddit.com/r/aws/search?q=flair%3A'database'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+database).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": -1,
          "created_utc": "2026-02-20 09:45:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r840au",
      "title": "Next AWS Associate after SAA? (Have Azure certs)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r840au/next_aws_associate_after_saa_have_azure_certs/",
      "author": "luffy6700",
      "created_utc": "2026-02-18 14:10:47",
      "score": 6,
      "num_comments": 12,
      "upvote_ratio": 0.81,
      "text": "Hey everyone,\nI have 9 months internship experience as a Multicloud Administrator and hold:\nMicrosoft Certified: Azure Administrator Associate\nMicrosoft Certified: Azure Security Engineer Associate\nMicrosoft Certified: Azure Solutions Architect Expert\nAWS Certified Solutions Architect ‚Äì Associate\nI have a 100% AWS voucher.\nWhich Associate cert should I take next to boost job opportunities",
      "is_original_content": false,
      "link_flair_text": "training/certification",
      "permalink": "https://reddit.com/r/aws/comments/1r840au/next_aws_associate_after_saa_have_azure_certs/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o638rhj",
          "author": "benpakal",
          "text": "CloudOps. This is 95% same as SAA + some multi org stuff",
          "score": 2,
          "created_utc": "2026-02-18 17:38:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o658bug",
              "author": "BloodAndTsundere",
              "text": "It‚Äôs that much an overlap? I thought Developer was the biggest overlap. I‚Äôm studying for SAA and Developer simultaneously right now but I‚Äôd consider swapping one out for or just adding CloudOps",
              "score": 1,
              "created_utc": "2026-02-18 23:07:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67rfuc",
                  "author": "benpakal",
                  "text": "I havent done Developer, but I have done SAA SAP DevOps and CloudOps. CloudOps was very similar to SAA imho",
                  "score": 2,
                  "created_utc": "2026-02-19 09:37:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o62lrmu",
          "author": "Ninjaivxx",
          "text": "I think you should do Developer",
          "score": 1,
          "created_utc": "2026-02-18 15:53:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62s79d",
          "author": "Willkuer__",
          "text": "I did Developer (which is basically the same as SAA) and DataEngineer, which is okish.\n\nI think the AI certs are also relevant for the current market and likely easy.\n\nI heard Devops is fun since it also has a partical component.\n\nMy next cert will hopefully be SA Professional.\n\nSo I would go for (in that order)\n1. AI stuff\n2. Data Engineer\n3. SA Professional\n4. Developer (just too much overlap with SAA)\n5. Devops",
          "score": 1,
          "created_utc": "2026-02-18 16:23:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64b2l0",
          "author": "alex_aws_solutions",
          "text": "If you're handling a Multiaccounts Environment or Organizations, I would to the CloudOps.",
          "score": 1,
          "created_utc": "2026-02-18 20:30:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66i2cw",
          "author": "Ok_Difficulty978",
          "text": "Nice lineup of certs already that‚Äôs solid.\n\nSince you already have SAA + strong Azure background, I‚Äôd probably go for AWS SysOps Admin Associate next. It fits well with multicloud/admin work and looks good for ops/cloud roles. Dev Associate is good too if you‚Äôre more into coding side.\n\nAlso depends what jobs you‚Äôre aiming for tbh. I usually check a few practice questions first before locking in (used sites like vmexam before, helped me see where I stand).\n\n[https://awscertexam.wordpress.com/2024/09/12/aws-sysops-administrator-jobs-pass-the-soa-c02-exam/](https://awscertexam.wordpress.com/2024/09/12/aws-sysops-administrator-jobs-pass-the-soa-c02-exam/)",
          "score": 1,
          "created_utc": "2026-02-19 03:29:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6jbie",
      "title": "How should i calculate IOPS for Aurora in AWS pricing calculator?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r6jbie/how_should_i_calculate_iops_for_aurora_in_aws/",
      "author": "xodmorfic",
      "created_utc": "2026-02-16 19:29:28",
      "score": 6,
      "num_comments": 15,
      "upvote_ratio": 0.81,
      "text": "I've spent over a week on this and I'm still unsure.\n\n  \nMy team wants to migrate from RDS MySQL to use Aurora (standard) for our database. I've tried to use the AWS pricing calculator to estimate the cost of the new DB, but i think i don't have thescsc right understanding of calculating the storage price for Aurora, and the estimations look way overpriced than expected.\n\n  \nI am replicating our current RDS MySQL setup with 800GB. Pricing calculator asks for \"Baseline I/O rate\" and \"Peak I/O rate\" for estimating the price of Aurora storage, but i am not sure of how to calculate those rates.\n\nThis is an example Total IOPS for test DB, from the metrics, for the last 1 day and a span period of 1 minute:\n\nhttps://preview.redd.it/e89erkt8rwjg1.png?width=567&format=png&auto=webp&s=3a2ede87a7535376607b848267ee9cc1cd04c981\n\n  \nIf i put those values of about 3.7k in the \"Baseline I/O rate\", i end up having a storage cost of about $2k which is a lot. \n\nOur current RDS MySQL database costs about $180 including storage (general purpose gp3). So i know that my input in those I/O fields in the AWS calculator might be wrong, but i don't know how then should i be calculating those values.\n\nhttps://preview.redd.it/mn0dz2yarwjg1.png?width=1700&format=png&auto=webp&s=0cd2675100aec5652be16f22bee1b99ce76b0c5e\n\n  \nHELP!",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1r6jbie/how_should_i_calculate_iops_for_aurora_in_aws/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5ru9yc",
          "author": "Psych76",
          "text": "Assume it will be one million billion and pay up haha this is one reason we had to back out of Aurora (serverless) - the io costs were impossible to predict and our dev instances were racking up fees so fast.",
          "score": 1,
          "created_utc": "2026-02-16 23:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ruyai",
              "author": "xodmorfic",
              "text": "I mean, our current RDS for testing only costs about $180/month and using aws calculator, for Aurora it jumps up to over $2k. That is ridiculous, but that is why i think my calculations for IOPS are wrong",
              "score": 2,
              "created_utc": "2026-02-16 23:28:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5sqqos",
                  "author": "Psych76",
                  "text": "The iops unknowns of Aurora were such an unknown for us, with zero temp files ever suddenly we‚Äôd need hundreds of spend daily just on iops but other days $10, it was too all over the map and unpredictable and we lacked a way to see what our ‚Äúfuture iops actuals‚Äù would be.",
                  "score": 1,
                  "created_utc": "2026-02-17 02:35:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5qkehs",
          "author": "Alternative-Theme885",
          "text": "To calculate IOPS for Aurora in the AWS pricing calculator, you need to understand that the baseline I/O rate is the average IOPS your database will use, and the peak I/O rate is the maximum IOPS your database will use. The fix is to monitor your current RDS MySQL instance using CloudWatch metrics, specifically the \"VolumeReadOps\" and \"VolumeWriteOps\" metrics, to get an accurate estimate of your IOPS usage. You can then use these metrics to estimate your baseline and peak I/O rates in the AWS pricing calculator, which should give you a more accurate cost estimate for your Aurora database.",
          "score": -1,
          "created_utc": "2026-02-16 19:34:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qoqag",
              "author": "TimGustafson",
              "text": "This is incorrect.  Aurora IOPS have nothing to do with EBS IOPs.  They're different things.  You can really only take an educated guess at what Aurora IOPs will look like for a given workload.  That's why AWS has IO Optimized: because this is so hard to figure out and regulate.\n\nThe only way to get accurate Aurora IOPs numbers is to measure them.\n\nSource: I was at AWS for 7+ years, working the last 4 as a Principal Database SA focusing on Aurora.",
              "score": 14,
              "created_utc": "2026-02-16 19:55:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5qr53v",
                  "author": "SpecialistMode3131",
                  "text": "Hence the usual \"take RDS and add 20%\" advice.",
                  "score": 5,
                  "created_utc": "2026-02-16 20:07:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5qwe4y",
                  "author": "Alternative-Theme885",
                  "text": "Good point, you're right that Aurora IOPS are separate from EBS. I oversimplified. For Aurora specifically, monitoring CloudWatch metrics like VolumeReadIOPs and VolumeWriteIOPs from an existing workload is probably the best way to estimate.",
                  "score": 2,
                  "created_utc": "2026-02-16 20:33:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5rem5n",
                  "author": "xodmorfic",
                  "text": "So, how can I really measure/estimate those \"Baseline I/O rate\" and \"Peak I/O rate\" for the calculator, if I don't have an Aurora DB yet but only our current RDS MySQL?\n\nI can rely on my current cloudwatch metrics, but if I use the values as i stated on the post/screenshot, the estimated cost just goes to the moon",
                  "score": 1,
                  "created_utc": "2026-02-16 22:02:36",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1ralvuk",
      "title": "36hr+ Downtime - Response Required: Your Account is on Hold - Need Help",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1ralvuk/36hr_downtime_response_required_your_account_is/",
      "author": "Realistic-Lab6157",
      "created_utc": "2026-02-21 08:25:28",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 0.67,
      "text": "I received this 2 days ago, and I have seen other posts too about it.  \nThe pain is that I have all the right documents to get this verification done but the communication process is so slow and confusing which keeps delaying this and my services are still down.\n\nI have already created support tickets and cases but there has been no response for the last 12 hours. I am stuck here and need urgent help. u/AWSSupport\n\nI am not even sure if the verification team works on the weekends which might add 2 more days of downtime.\n\nDoes anyone have any idea on how to get this escalated or prioritized?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1ralvuk/36hr_downtime_response_required_your_account_is/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6kqbq3",
          "author": "Sirwired",
          "text": "Which support plan do you have?",
          "score": 4,
          "created_utc": "2026-02-21 09:21:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6l304i",
              "author": "Realistic-Lab6157",
              "text": "It‚Äôs basic. I was trying to upgrade it but it‚Äôs not allowing me.",
              "score": 2,
              "created_utc": "2026-02-21 11:24:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r60pah",
      "title": "Does the bulk api in OpenSearch Serverless has a limit ?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r60pah/does_the_bulk_api_in_opensearch_serverless_has_a/",
      "author": "Any_Animator4546",
      "created_utc": "2026-02-16 05:09:01",
      "score": 5,
      "num_comments": 12,
      "upvote_ratio": 0.78,
      "text": "My file has like 3000 documents, but only 700 gets synced and then it shows timeout",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1r60pah/does_the_bulk_api_in_opensearch_serverless_has_a/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o5n3csa",
          "author": "luckVise",
          "text": "Yes.\n\nFrom my experience, OpenSearch, serverless or not it doesn't matter, bulk or single api calls doesn't matter, ingest documents without using an ingestion queue. \n\nWhen you send too much documents at once, it simply refuses to ingest them. The bulk api seems to don't return errors, but the single api does.\n\nYou must build your own ingestion queue and retry on errors.\n\nFor me this is non sense. A database (or search engine as someone clarified) of any kind should provide a way to insert data reliably.",
          "score": 2,
          "created_utc": "2026-02-16 06:26:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pk9uy",
              "author": "daredevil82",
              "text": "it is a mistake to think of search engines as a database",
              "score": 2,
              "created_utc": "2026-02-16 16:47:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5pp7kf",
                  "author": "luckVise",
                  "text": "Sorry, I labeled Opensearch as a database, it is a search engine. My bad.\n\nAnyway, for a search engine that is capable to search and aggregate through millions of documents, I expect at least a module or something already integrated in the cluster of nodes, to perform the insertion of many records, not to do it myself. \nBecause that is the first thing you need when you start working seriously with OpenSearch.",
                  "score": 1,
                  "created_utc": "2026-02-16 17:09:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5q7072",
              "author": "ItsHoney",
              "text": "Just today, I was trying to index documents from Snowflake to opensearch. I moved parquet files from Snowflake to S3 and buffered the messages into SQS. \n\nOut of 170k docs, only 130k got indexed. However, I didn't see any error messages pop up in the live tail logs. \n\nIs this one of those silent failure cases? Some of the single parquet files can have upto 100k records",
              "score": 1,
              "created_utc": "2026-02-16 18:32:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qgzeo",
          "author": "chrishrb",
          "text": "Opensearch serverless is pretty much useless. Expensive but doesn‚Äôt scale good. In the company I used to work before we tried to use it for ocpp logs but it was just shit. Dumped it for the non serverless version.",
          "score": 1,
          "created_utc": "2026-02-16 19:18:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qhaky",
              "author": "Any_Animator4546",
              "text": "thanks , I am just learning for an interview, I am mostly using serverless as it is easier to set up ",
              "score": 1,
              "created_utc": "2026-02-16 19:19:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vyz6e",
          "author": "TechDebtSommelier",
          "text": "Yes, 20MB per bulk request is the hard limit for OpenSearch Serverless, and it will just quietly stop there which is extremely fun to debug. Chunk your docs into batches of \\~500 and retry with backoff on 429s. The timeout you're seeing is almost definitely the OCU scaling to handle the spike, not the request itself.",
          "score": 1,
          "created_utc": "2026-02-17 16:19:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gx6ds",
              "author": "Useful-Process9033",
              "text": "Silent failures on bulk ingest are absolutely criminal API design. An indexing service that just quietly drops your documents with no error is worse than throwing a 500. At minimum they should return a partial success response with the count of what actually made it in.",
              "score": 1,
              "created_utc": "2026-02-20 18:45:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1raayhv",
      "title": "Help with cognito: Code security resource quotas not enforced?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1raayhv/help_with_cognito_code_security_resource_quotas/",
      "author": "TutorNeat2724",
      "created_utc": "2026-02-20 23:20:47",
      "score": 4,
      "num_comments": 3,
      "upvote_ratio": 0.84,
      "text": "Hi everyone,\nI‚Äôve noticed what seems to be unexpected behavior regarding Cognito User Pools code security resource quotas.\nAccording to the [documented limits](https://docs.aws.amazon.com/cognito/latest/developerguide/quotas.html#resource-quotas), certain operations (e.g. GetUserAttributeVerificationCode) should be rate-limited (for example, max 5 consecutive requests). However, in my tests, I‚Äôm able to call GetUserAttributeVerificationCode more than 5 times in a row without receiving any throttling error or limit exception.\nHas anyone experienced the same behavior?\nIs there any additional configuration required to enforce these quotas, or are they applied under specific conditions only?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1raayhv/help_with_cognito_code_security_resource_quotas/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6ipsm5",
          "author": "Skytram_",
          "text": "How long are you calling that API above the max TPS for?",
          "score": 1,
          "created_utc": "2026-02-21 00:16:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k1edw",
          "author": "baever",
          "text": "`GetUserAttributeVerificationCode` has a quota of 5 requests/user/hour. The only thing I can think of is that the bucket is based on hour of day. So if you start at 1:59 and make 4 requests and then at 2:01 you make 4 requests (for a total of 8) it won't block you because those span 2 hour buckets. However, if you make 6 requests in the same hour (say between 2:01 and 2:05) it should block you because it's in 1 hour bucket. Are you never seeing it enforced or are you seeing it intermittently enforced?",
          "score": 1,
          "created_utc": "2026-02-21 05:29:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}