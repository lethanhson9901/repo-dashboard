{
  "metadata": {
    "last_updated": "2026-02-27 16:59:57",
    "time_filter": "week",
    "subreddit": "aws",
    "total_items": 20,
    "total_comments": 76,
    "file_size_bytes": 103195
  },
  "items": [
    {
      "id": "1raski8",
      "title": "AWS Certificate Manager updates default certificate validity to comply with new guidelines",
      "subreddit": "aws",
      "url": "https://aws.amazon.com/about-aws/whats-new/2026/02/aws-certificate-manager-updates-default/",
      "author": "SudoAlex",
      "created_utc": "2026-02-21 14:32:40",
      "score": 48,
      "num_comments": 8,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "security",
      "permalink": "https://reddit.com/r/aws/comments/1raski8/aws_certificate_manager_updates_default/",
      "domain": "aws.amazon.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6lu1ai",
          "author": "SudoAlex",
          "text": "Posted this mostly due to the future uncertainty from their announcement back in June 2025, with exportable public certificates: https://www.reddit.com/r/aws/comments/1ldpw1d/aws_certificate_manager_introduces_public/ - as it was well known that certificate lifetimes would be limited in the long run.\n\nThey'll be reducing the duration, along with the price:\n\n> We have reduced the pricing for ACM’s exportable public certificates in line with the shorter validity period. 198-day exportable public certificate will now cost $7/Fully Qualified domain name (down from $15) and $79/ wildcard name (down from $149). Please refer to ACM’s pricing page for more details. For more information about ACM, visit the [ACM documentation](https://docs.aws.amazon.com/acm/latest/userguide/acm-certificate-characteristics.html).\n\n~~Although the wildcard pricing feels like a slightly sneaky $4.50 increase (half of $149 is $74.50)~~.\n\nUpdated numbers below.",
          "score": 10,
          "created_utc": "2026-02-21 14:38:53",
          "is_submitter": true,
          "replies": [
            {
              "id": "o6m80fe",
              "author": "Mutjny",
              "text": ">Although the wildcard pricing feels like a slightly sneaky $4.50 increase (half of $149 is $74.50).\n\n198 days is not half of 365.  The new price is exactly equivalent to the old price with the reduced number of days.  Its actually like $1.50 less.",
              "score": 6,
              "created_utc": "2026-02-21 15:53:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6mq2vo",
                  "author": "SudoAlex",
                  "text": "Certificates weren't valid for 365 days - it was 395 days. Got the calculations slightly off, but it's probably more of a price bump:\n\nBefore: 395 day certificate, renews 60 days before expiry, $149 per 335 days  \nAfter: 198 day certificate, renews 45 days before expiry, $79 per 153 days\n\nShould be closer to $68 for it to be the same price.",
                  "score": 2,
                  "created_utc": "2026-02-21 17:23:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m02lx",
          "author": "wlonkly",
          "text": "Ah thanks for this, I was wondering what their plan was (but not enough to ask our TAM I guess, heh).",
          "score": 2,
          "created_utc": "2026-02-21 15:12:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74hhmh",
          "author": "KayeYess",
          "text": "CA Browser forum voted to reduce public CA signed certificate validity to 200 days on March 26, 2026. It will reduce to 100 days in March, 2027 and 47 days in March 2029. Time to start preparing and automate. We automated certificate issuance and rotation over a decade ago (especially for external certs imported into ACM). Also, start adopting quantum safe ciphers. And mask unnecessary NPI/PII in all communications.",
          "score": 1,
          "created_utc": "2026-02-24 12:22:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rata1g",
      "title": "If S3 vectors offer sub second latency, why does AWS say it's designed for infrequent access?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rata1g/if_s3_vectors_offer_sub_second_latency_why_does/",
      "author": "nucleustt",
      "created_utc": "2026-02-21 15:03:03",
      "score": 36,
      "num_comments": 36,
      "upvote_ratio": 0.85,
      "text": "I'm building a customer service agent and need a vector DB for RAG.\n\nNaturally, I gravitated toward S3 vectors because the 90% cost reduction was super attractive.\n\nI'm wondering if I'm making the right choice (even though I see RAG as a use case).\n\nBasically, the chatbot has to answer questions via WhatsApp.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rata1g/if_s3_vectors_offer_sub_second_latency_why_does/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6lylv2",
          "author": "Sirwired",
          "text": "Because \"sub-second\" is super slow vs. the alternatives.",
          "score": 80,
          "created_utc": "2026-02-21 15:04:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6m0aee",
              "author": "nucleustt",
              "text": "wow. Thanks. I can only imagine the throughput of an in-memory Redis vector DB then. \"Near instant\"",
              "score": -13,
              "created_utc": "2026-02-21 15:13:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6m2aog",
                  "author": "CoastRedwood",
                  "text": "How big is the data you’re storing? Bigger the dataset the higher the latency and greater the cpu usage.",
                  "score": 6,
                  "created_utc": "2026-02-21 15:24:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6mfh90",
          "author": "jdanton14",
          "text": "If I had a database that has 900ms of latency, I'd fire my DBA and my storage vendor.",
          "score": 21,
          "created_utc": "2026-02-21 16:30:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6p44mo",
              "author": "coinclink",
              "text": "If you're working at scale though, vector algorithms are extremely taxing on a db engine, especially on huge datasets. 900ms consistently across a huge vector store is a good tradeoff vs spending thousands of dollars scaling out a bunch of read replicas that might still get deadlocked when a flood of requests come in.",
              "score": 4,
              "created_utc": "2026-02-22 01:11:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6mhxcr",
              "author": "nucleustt",
              "text": "lol. I wonder if the whatsapp api supports the \"is typing\" indicator.",
              "score": 1,
              "created_utc": "2026-02-21 16:42:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6m2h5q",
          "author": "barnaclebill22",
          "text": "That's sub- second for each retrieval. You then need to submit the returned text as context to your LLM, so any question response might end up taking a few seconds. \nIt's easy to switch to something like Opensearch (or use it as a fast cache with S3 vectors), so might be worth trying. \nChatting with a human agent on WhatsApp typically takes several minutes per conversational turn (since they're all handling dozens of conversations).",
          "score": 18,
          "created_utc": "2026-02-21 15:25:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6meti1",
              "author": "HiCookieJack",
              "text": "Or PG Vector with Postgres Serverless ",
              "score": 11,
              "created_utc": "2026-02-21 16:27:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6n0ogv",
                  "author": "hergabr",
                  "text": "PgVector on Postgres works surprisingly well for a chatbot",
                  "score": 3,
                  "created_utc": "2026-02-21 18:16:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6mhr80",
              "author": "nucleustt",
              "text": "Thanks, these are all solutions I would test and compare",
              "score": 1,
              "created_utc": "2026-02-21 16:41:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nshiy",
          "author": "Due-Horse-5446",
          "text": "500ms is sub second too..",
          "score": 6,
          "created_utc": "2026-02-21 20:38:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ol5r5",
              "author": "nucleustt",
              "text": "true. But even a second isn't bad. Hopefully someone on chat can wait \\~5 sec for a response... hopefully",
              "score": -1,
              "created_utc": "2026-02-21 23:14:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6qw3no",
                  "author": "Due-Horse-5446",
                  "text": "thats a pretty extreme amount, espetfpr chatbots you should be shaving miliseconds where you can..",
                  "score": 1,
                  "created_utc": "2026-02-22 09:33:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6oabfi",
          "author": "tank_of_happiness",
          "text": "I’m using it for your same use case. The LLM’s thinking takes way longer. I’m happy with the results I’m getting and the cost is almost nothing at my volume. Previously I was using open search and paying a couple hundred a month.",
          "score": 6,
          "created_utc": "2026-02-21 22:13:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6olb1w",
              "author": "nucleustt",
              "text": "wtf, couple hundred a month vs almost nothing!? That's crazy cost reduction!",
              "score": 1,
              "created_utc": "2026-02-21 23:15:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6qlamu",
              "author": "nucleustt",
              "text": "Also, it's reassuring that someone else is using it for my use case.",
              "score": 1,
              "created_utc": "2026-02-22 07:50:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6n2cnh",
          "author": "yarenSC",
          "text": "In S3 terms, infrequent access is a billing term more than anything else",
          "score": 3,
          "created_utc": "2026-02-21 18:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6okz0x",
              "author": "nucleustt",
              "text": "ha. gotcha",
              "score": 1,
              "created_utc": "2026-02-21 23:13:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6suj0a",
          "author": "CrazedBotanist",
          "text": "We are using S3 vectors for a couple projects. Overall, they have worked great for us and are cheap. However, the two things to look out for is the lack of native support for hybrid search (semantic and keyword search) and the latency. Latency has only become any issue for us in Agentic RAG solutions where the agents tend to search multiple times based on search results and the initial question.",
          "score": 1,
          "created_utc": "2026-02-22 17:02:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6uu3b5",
              "author": "nucleustt",
              "text": "Super useful comment. Especially the \"lack of hybrid (semantic and keyword) search\" part that I didn't even consider. That's a huge disadvantage.\n\nThank you",
              "score": 1,
              "created_utc": "2026-02-22 22:53:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6vog30",
                  "author": "CrazedBotanist",
                  "text": "No problem! If you need hybrid search and don’t mind the overhead of using managed open search clusters you can use s3 vectors as the storage engine for some cost savings on storage.",
                  "score": 1,
                  "created_utc": "2026-02-23 01:49:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6trq8l",
          "author": "netik23",
          "text": "You can’t tell me you have so much data you can’t store it on a SSD instance.",
          "score": 1,
          "created_utc": "2026-02-22 19:37:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6uubn3",
              "author": "nucleustt",
              "text": "You're right. I can't. What do you suggest?",
              "score": 1,
              "created_utc": "2026-02-22 22:54:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6mz94n",
          "author": "rexspook",
          "text": "Sub second is infrequent in computing terms",
          "score": -3,
          "created_utc": "2026-02-21 18:09:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdff6p",
      "title": "Price increase at AWS?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rdff6p/price_increase_at_aws/",
      "author": "servermeta_net",
      "created_utc": "2026-02-24 12:52:15",
      "score": 24,
      "num_comments": 19,
      "upvote_ratio": 0.78,
      "text": "Recently many non hyperscaler providers I use (Hetzner, OVH) increased their prices due to the supply issues we all know. Do you think AWS and other hyperscalers will follow through, or will they shield their customers from the hardware market fluctuations? ",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rdff6p/price_increase_at_aws/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o74p0sd",
          "author": "PaintDrinkingPete",
          "text": "AWS likely has an advantage of already having a massive infrastructure and having priority with manufacturers...and already charging more for their services than many of the smaller budget VPS and cloud computing services providers operating on tighter margins, so they don't have to be as reactionary to the market in cases like this. \n\neventually though, if things keep trending as they are... yes, prices will have to go up.",
          "score": 47,
          "created_utc": "2026-02-24 13:11:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jui2i",
              "author": "cranberrie_sauce",
              "text": "even with increased prices - comparable hetzner/ovh compute is 60-70% cheaper than aws.  \n\naws increased prices long in advance of anticipated shortages.",
              "score": 2,
              "created_utc": "2026-02-26 17:42:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74uxv9",
          "author": "criminalsunrise",
          "text": "AWS tend not to do price rises - I believe it's one of their core principles. What they will do is have new service levels (instance types) that are more expensive and slowly sunset the old ones. We may see that accelerated a bit with the supply crunch.",
          "score": 40,
          "created_utc": "2026-02-24 13:44:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o750bw8",
              "author": "RickySpanishLives",
              "text": "Highly likely. The systems already in the field are just being amortized out. No real point increasing their price because they are already there. New instance types will incur an increased cost since they will cost more and have a BOM with more expensive parts.",
              "score": 13,
              "created_utc": "2026-02-24 14:13:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74mnrx",
          "author": "Old_Cry1308",
          "text": "aws will probably absorb some of it short-term. long-term, they'll pass it on. they’re not running a charity.",
          "score": 39,
          "created_utc": "2026-02-24 12:56:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74nwec",
              "author": "moduspol",
              "text": "They might have bigger longer term deals on hardware like this and haven’t had to eat any higher costs yet.",
              "score": 14,
              "created_utc": "2026-02-24 13:04:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7712h9",
          "author": "FlatCondition6222",
          "text": "Less spot capacity, for example, is also a \"way\" to raise prices.",
          "score": 7,
          "created_utc": "2026-02-24 19:48:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7q5huq",
              "author": "classicrock40",
              "text": "only if you don't believe spot=excess capacity. ",
              "score": 1,
              "created_utc": "2026-02-27 16:37:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74pe0u",
          "author": "Negative-Cook-5958",
          "text": "There is already about a 5% increase with each EC2 generation change (r6i => r7i => r8i for example), they won't increase pricing for existing SKUs, just make the newer ones expensive when they are available.",
          "score": 9,
          "created_utc": "2026-02-24 13:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74vbkc",
              "author": "davewritescode",
              "text": "The thing is you should be using less compute as new generations usually perform better",
              "score": 9,
              "created_utc": "2026-02-24 13:46:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o750u03",
                  "author": "RickySpanishLives",
                  "text": "It's really the instance shape that determines if that is possible. Sometimes the compute is indeed a better performer but the instance shape gives you more than you need at an increased price requiring you to binpack your computer to cover it. Less a concern if you breathe EKS/ECS - but an issue if you use raw EC2.",
                  "score": 3,
                  "created_utc": "2026-02-24 14:16:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74ps5y",
          "author": "JonnyBravoII",
          "text": "Yes. Many of their prices have been the same for years (EBS, lambda, data transfer) and are obviously cash cows. On the EC2 front, prices started going up after the 6 series.  The t series, while wildly popular, are clearly being sunset as the t4g is 6 years.old at this point. I would expect storage costs will be the first to go up.",
          "score": 4,
          "created_utc": "2026-02-24 13:15:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74u5uo",
          "author": "d70",
          "text": "Who is keeping track? https://aws.amazon.com/blogs/aws/category/price-reduction/\nThat said, I agree that some new instance types are slightly more expensive than older ones but they are different products, no?",
          "score": 4,
          "created_utc": "2026-02-24 13:40:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78ssws",
          "author": "Complex86",
          "text": "I think AWS will pass this on in future generations of instance family or other services they may provide down the line.",
          "score": 2,
          "created_utc": "2026-02-25 01:05:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ousiy",
          "author": "nucleustt",
          "text": "Meanwhile, a jackass on my feed keeps posting AI slop images of him riding capybaras.\n\nI'm clenching my fists, saying, this is why RAM prices are increasing.",
          "score": 1,
          "created_utc": "2026-02-27 12:28:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74yw4o",
          "author": "Shington501",
          "text": "Everything is going up",
          "score": 1,
          "created_utc": "2026-02-24 14:06:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfbpnj",
      "title": "Bypassing SCP Enforcement with Long-Lived API Keys in Bedrock",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rfbpnj/bypassing_scp_enforcement_with_longlived_api_keys/",
      "author": "SonraiSecurity",
      "created_utc": "2026-02-26 14:33:21",
      "score": 22,
      "num_comments": 1,
      "upvote_ratio": 0.89,
      "text": "I wanted to share a finding regarding an SCP (Service Control Policy) bypass I discovered in Amazon Bedrock. For those of us using SCPs as the sort of final guardrail in a multi-account setup, this was a surprising edge case where a specific type of credential completely ignored SCP \"Deny\" statements.\n\nMost of us interact with Bedrock via standard IAM users/roles. However, Bedrock also supports Short-Term and Long-Term API Keys. Long-term Bedrock API keys are actually backed by Service Specific Credentials - an ad-hoc authentication mechanism also used in AWS CodeCommit and Amazon Keyspaces.\n\n**The Vulnerability: SCP Bypass**\n\nWhen using permissions in the bedrock IAM namespace, SCPs are properly enforced no matter the authentication mechanism. When testing permissions in the bedrock-mantle namespace however, I found a discrepancy in how Bedrock evaluated these three credential types against Organization-level policies:\n\n1. **SigV4 (IAM Authentication) & Short-term keys:** Behave as expected. If an SCP denies bedrock-mantle:CreateInference, the creation of an inference is blocked.\n2. **Long-term keys (Service Specific Credentials):** These were able to bypass SCP \"Deny\" statements, and actions like creating inferences were still allowed even if the actions were blocked.\n\nHow I set this up:\n\n* I applied an SCP to a member account that explicitly denied `bedrock-mantle:*` to all users.\n* As an IAM user in that member account, I generated a Service Specific Credential for Bedrock.\n* When using that credential with the Bedrock Mantle API, the SCP was ignored, and I was able to perform inferences despite the global deny.\n\nThis issue was common to all bedrock-mantle permissions.\n\nThis effectively allowed a \"self-bypass\" of organizational governance. If a security team used an SCP to prevent the use of specific model families or to enforce a region-lock on AI workloads, a developer with `iam:CreateServiceSpecificCredential` permissions could bypass those restrictions entirely by generating and using a long-lived key.\n\n**Disclosure and Current Status**\n\nI reported this to the AWS Security Team. They validated the finding and have since deployed a fix. SCPs are now correctly enforced for bedrock-mantle requests made using Service Specific Credentials.\n\nIf you are currently managing Bedrock permissions, it's worth auditing who has the ability to create ServiceSpecificCredentials and ensuring your IAM policies (not just your SCPs) are as tight as possible.\n\nIs anyone else leveraging long-term API keys in bedrock? They are a bit of an outlier compared to the standard IAM/STS flow, so I'd be curious to know what steps people are taking to keep them and their use secure.\n\n  \n\\-Nigel Sood, Researcher @ Sonrai Security\n\n",
      "is_original_content": false,
      "link_flair_text": "security",
      "permalink": "https://reddit.com/r/aws/comments/1rfbpnj/bypassing_scp_enforcement_with_longlived_api_keys/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7its8p",
          "author": "oneplane",
          "text": "This has pretty much always been the case. RDS Credentials also aren't influenced by IAM policies, SCP or otherwise. Same goes for SSH and RDP, or Simple AD to name some more.\n\nEdit: maybe the new-ish factor here is that they tried to normalise the audit logs as well as the policy language to make the API feel more like a 'real' AWS API? A bit like ECR when you generate non-IAM credentials for pull/push authentication.",
          "score": 7,
          "created_utc": "2026-02-26 14:51:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbw463",
      "title": "Podcast?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rbw463/podcast/",
      "author": "putneyj",
      "created_utc": "2026-02-22 20:10:12",
      "score": 17,
      "num_comments": 10,
      "upvote_ratio": 0.95,
      "text": "So, is the official AWS podcast no longer doing news? Anyone else that used it to get \\*most\\* of your news about new services? I’m honestly a little bummed, but it just feels like the way things are going at AWS.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rbw463/podcast/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6tzvdo",
          "author": "signsots",
          "text": "The latest episode aligns with the recent layoff announcements. Wouldn't be surprised if it was part of it unfortunately.",
          "score": 20,
          "created_utc": "2026-02-22 20:18:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u20m0",
          "author": "coyotefarmer",
          "text": "That was a big part of how I kept up with changes. Such a shame. I don't see how ending something like that can be beneficial in the long run. I listened to it for years.",
          "score": 6,
          "created_utc": "2026-02-22 20:29:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71w9ms",
          "author": "ArchangelAdrian",
          "text": "Well the host Simon Elisha announced on his LinkedIn 2 weeks ago that his time at AWS came to an end.",
          "score": 5,
          "created_utc": "2026-02-24 00:41:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71xjc9",
              "author": "putneyj",
              "text": "Damn, yeah I would imagine that’s pretty much the end then. Not even a regular conversation with Werner can save your job.",
              "score": 3,
              "created_utc": "2026-02-24 00:48:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o740qdd",
                  "author": "ArchangelAdrian",
                  "text": "Sad if you think about it.",
                  "score": 2,
                  "created_utc": "2026-02-24 10:02:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6vz1y4",
          "author": "zenmaster24",
          "text": "Are there community podcasts that do similar?",
          "score": 3,
          "created_utc": "2026-02-23 02:53:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xz12c",
              "author": "wreckuiem48",
              "text": "A cloud podcast I like is called [https://www.thecloudpod.net/](https://www.thecloudpod.net/)",
              "score": 3,
              "created_utc": "2026-02-23 12:51:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6w1zg5",
              "author": "putneyj",
              "text": "I would also like to know this",
              "score": 1,
              "created_utc": "2026-02-23 03:11:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y8fb4",
          "author": "Specific-Art-9149",
          "text": "I built a morning brief email with Claude that scans various AWS RSS feeds along with 3rd party RSS and Reddit for AWS news.   Costs me about 7 cents a day utilizing Claude API, and it is emailed daily.  I'm an SA not a developer, so pretty happy with how it turned out.",
          "score": 2,
          "created_utc": "2026-02-23 13:49:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xz8n2",
          "author": "bot403",
          "text": "Google gemeni can turn anything into a podcast episode. Use it on the main aws RSS feed :D",
          "score": -2,
          "created_utc": "2026-02-23 12:52:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd7p4v",
      "title": "Cloudfront + HTTP Rest API Gateway",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rd7p4v/cloudfront_http_rest_api_gateway/",
      "author": "Alive_Opportunity_14",
      "created_utc": "2026-02-24 05:31:50",
      "score": 14,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "Cloudfront has introduced flat rate pricing with WAF and DDos protection included. I am thinking of adding cloudfront in front of my rest api gateway for benefits mentioned above. Does it make sense from an infra design perspective?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rd7p4v/cloudfront_http_rest_api_gateway/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7372db",
          "author": "Old_Cry1308",
          "text": "makes sense if you need the protection and pricing works for you, otherwise might be overkill.",
          "score": 6,
          "created_utc": "2026-02-24 05:34:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73dx94",
              "author": "Alive_Opportunity_14",
              "text": "From a pricing perspective it seems cheaper because of the flat pricing and added benefits. WAF on rest api costs money while its included in the flat pricing",
              "score": 1,
              "created_utc": "2026-02-24 06:30:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73p4sr",
          "author": "snorberhuis",
          "text": "A WAF is a layer of defense I would generally recommend for most companies. It can help you protect against automated attacks. There are very few exceptions to this recommendation.  ",
          "score": 3,
          "created_utc": "2026-02-24 08:11:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74z0gx",
          "author": "menge101",
          "text": "[Docs](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/flat-rate-pricing-plan.html) for anyone else that needs them\n\n[Pricing sheet](https://aws.amazon.com/cloudfront/pricing/) as well\n\nThere is a free tier as well as a pro tier at $15/month that seems fairly compelling.",
          "score": 3,
          "created_utc": "2026-02-24 14:06:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74fiyc",
          "author": "KayeYess",
          "text": "While AWS WAF2 can be attached directly to Amazon API Gateway, Cloudfront gives additional benefits such as distributed edge delivery, ability to use multiple origins (such as S3 for static content), caching, etc.",
          "score": 1,
          "created_utc": "2026-02-24 12:07:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7drkhi",
              "author": "vppencilsharpening",
              "text": "I'd also add that it leverages the AWS managed backbone for transport from the Edge to the Origin. So if your application is running in a single region you get AWS's team ensuring fast connections from the CloudFront edge to your application instead of relying on the public internet. \n\nIt's not going to make a huge difference, but it's not nothing. \n\nClient -> Public Internet (short distance) -> AWS CloudFront Edge (closest to the client) -> AWS Network (for most of the distance) -> Origin Application\n\nVS\n\nClient -> Public Internet (long distance) -> AWS Network (for a very short distance) -> Origin Application",
              "score": 1,
              "created_utc": "2026-02-25 19:34:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dx975",
                  "author": "KayeYess",
                  "text": "Yes ... that's a general benefit of a CDN. Client reaches CDN edge, and CDN handles the rest.",
                  "score": 1,
                  "created_utc": "2026-02-25 20:00:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o797flg",
          "author": "SilentPugz",
          "text": "Harden your security response header and content security policy for your cloudfront.  \n\nLambda edge for quick validations. Cloudfront managed functions makes some things simple \n\nDon’t forget your tls flow. Where you want to terminate. At the cloudfront , lessen the load on the api.",
          "score": 1,
          "created_utc": "2026-02-25 02:28:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f6tjp",
          "author": "TheDearlyt",
          "text": "The main tradeoff is added complexity so it’s worth it mostly when you actually plan to use WAF rules, caching, or global performance improvements, not just stack services for the sake of it.\n\nPersonally, I ended up using Gcore for a similar setup because I wanted CDN + edge protection in front of APIs without dealing with too much AWS configuration overhead. It felt simpler to manage while still giving the edge security and performance benefits.",
          "score": 1,
          "created_utc": "2026-02-25 23:43:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rc92pf",
      "title": "CDK + CodePipeline: How do you handle existing resources when re-deploying a stack?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rc92pf/cdk_codepipeline_how_do_you_handle_existing/",
      "author": "Hungry_Assistant6753",
      "created_utc": "2026-02-23 05:36:51",
      "score": 13,
      "num_comments": 8,
      "upvote_ratio": 0.88,
      "text": "We have an AWS CDK app deployed via CodePipeline. Our stack manages DynamoDB tables, Lambda functions, S3 buckets, and SageMaker endpoints.\n\n\n\n**Background**: Early on we had to delete and re-create our CloudFormation stack a few times due to deployment issues (misconfigured IAM, bad config, etc). We intentionally kept our DynamoDB tables and S3 buckets alive by setting RemovalPolicy.RETAIN. we didn't want to lose production data just because we needed to nuke the stack.\n\n**The problem**: When we re-deploy the stack after deleting it, CloudFormation tries to CREATE the tables again but they already exist. It fails. So we added a context flag `--context import-existing-tables=true` to our cdk synth command in CodePipeline, which switches the table definitions from new dynamodb.Table(...) to dynamodb.Table.from\\_table\\_name(...). This works fine for existing tables.\n\nNow, we added a new DynamoDB table. It doesn't exist yet anywhere. But the pipeline always passes `--context import-existing-tables=true`, so CDK tries to import a table that doesn't exist yet it just creates a reference to a non-existent table. No error, no table created.\n\n**Current workaround**: We special-cased the new table to always create it regardless of the flag, and leave the old tables under the import flag. But this feels fragile every time we add a new table we have to remember to handle this manually.\n\n**The question**: How do you handle this pattern cleanly in CDK? **Is there an established pattern for \"create if not exists, import if exists\"** that works in a fully automated",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rc92pf/cdk_codepipeline_how_do_you_handle_existing/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6wmzp0",
          "author": "deviled-tux",
          "text": "you leave the code alone and have it create the stuff \n\ncdk import to import them \n\nThis does not seem like something you should be automating because this shouldn’t be happening that often \n",
          "score": 17,
          "created_utc": "2026-02-23 05:41:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6won7x",
          "author": "cachemonet0x0cf6619",
          "text": "1. split stacks by the volatility of the resources. think stateful and stateless. \n\n2. use multiple accounts to split your resources so you’re not developing against production resources. \n\n3. if you can’t split accounts add an environment to the stack name and use that to separate dev and prod. \n\n4. don’t do that auto context thing. that’s a code smell. you shouldn’t need that with the advice above. \n\n5. bonus: i would never use code pipeline. instead i’d use github actions using a self hosted runner in my aws account. way easier and that skill is transferable.",
          "score": 10,
          "created_utc": "2026-02-23 05:54:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wspth",
          "author": "DoINeedChains",
          "text": "The joys of having CDK requirements forced upon you in a heavy singleton datasource environment",
          "score": 2,
          "created_utc": "2026-02-23 06:29:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hapz4",
          "author": "International_Body44",
          "text": "Put your dynamodb in a seperate stack, you can keep it in the cdk app, just define a dynamo stack.\n\nThen when you do your delete, specify the other stacks for your delete, 'cdk destroy <stack_1> <stack_2>'\n\nThis leaves the dynamodb alone. When you want to add a new table or data you should be able to add that to just the dynamodb stack and deploy that 'cdk deploy <dynamo_stack>'\n\n\nItll look a bit like this:\n\nLib/stacks/\n\n        |- dynamodb.ts\n\n        |- otherResources.ts\n\nThen in you bin/app.ts file call the dynamodb stack after your resources stack.\n\nIf you need to pass variables from one stack to the other use parameter store, do not directely pass vars between stacks cause it will cause issues later on.",
          "score": 2,
          "created_utc": "2026-02-26 08:02:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wpxp1",
          "author": "Conscious-Title-226",
          "text": "This is why I don’t like CDK to be honest. It ties you into CloudFormation which is just painful when things like this go wrong.\n\nCF just fundamentally doesn’t provide enough support around state management. You need to engineer around its limitations and/or have immutable infrastructure.\n\nWould you be able to rename your stack resource (so it is a new CF stack with a new s3 and dynamodb resource) and then just migrate the data? That’d be faster if it’s not a lot to migrate/copy\n\nYou might need to modify your stack to allow you to give them new names, there’ll be uniquely named resources in your stack like KMS aliases, s3 bucket, iam role etc but this is a good reason to make sure all that is configureable through your stack props",
          "score": 1,
          "created_utc": "2026-02-23 06:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wmtts",
          "author": "pausethelogic",
          "text": "I’m sure there’s a way, but this is honestly one of the major downsides of cloudformation and CDK - there’s no proper state management\n\nIt’s part of why terraform is a much more popular IaC tool, it actually has state management for resources so it knows what resources were deleted, which still exist, and how to handle them",
          "score": 3,
          "created_utc": "2026-02-23 05:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wq4jx",
              "author": "cachemonet0x0cf6619",
              "text": "“can you fix my Honda’s transmission?”\n\n“nah, mate. but if you’d have gotten a bike you wouldn’t need a new transmission”\n\nthat’s how stupid your reply sounds",
              "score": -8,
              "created_utc": "2026-02-23 06:07:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7b0va0",
          "author": "iamtheconundrum",
          "text": "Naming resources explicitly will result in naming collisions. Also, you should put resources like dynamodb tables in a separate stack so you can nuke other stacks without affecting the persistent parts of your architecture.",
          "score": 1,
          "created_utc": "2026-02-25 10:55:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rey0g0",
      "title": "Confused about how to set up a lambda in a private subnet that should receive events from SQS",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rey0g0/confused_about_how_to_set_up_a_lambda_in_a/",
      "author": "Slight_Scarcity321",
      "created_utc": "2026-02-26 02:31:11",
      "score": 8,
      "num_comments": 34,
      "upvote_ratio": 1.0,
      "text": "In CDK, I've set up a VPC with a public and private with egress subnets.  A private security group allows traffic from the same security group and HTTP traffic from the VPC's CIDR block. I have Postgres running in RDS Aurora in this VPC in the private security group.\n\nI have a lambda that lives in this private security group and is supposed to consume messages from an SQS queue and then write directly to the DB.  However, SQS queue messages aren't reaching the lambda.  I am getting some contradictory answers when I try to google how to do this, so I wanted to see what I need to do.\n\nThe SQS queue set up is very basic:\n\n```\nconst sourceQueue = new sqs.Queue(this, \"sourceQueue\");\n```\n\nThe lambda looks like this\n\n```\n        const myLambda = new NodejsFunction(\n            this,\n            \"myLambda\",\n            {\n                entry: \"path/to/index.js\", \n                handler: \"handler\", \n                runtime: lambda.Runtime.NODEJS_22_X, \n                vpc,\n                securityGroups: [privateSG],\n            },\n        );\n\n        myLambda.addEventSource(\n            new SqsEventSource(sourceQueue),\n        );\n\n        // policies to allow access to all sqs actions\n```\n\nIs it true that I need something like this?\n```\n        const vpcEndpoint = new ec2.InterfaceVpcEndpoint(this, \"VpcEndpoint\", {\n            service: ec2.InterfaceVpcEndpointAwsService.SQS,\n            vpc,\n            securityGroups: [privateSG],\n        });\n```\nWhile it allowed messages to reach my lambda, VPC endpoint are IaaS and I am not allowed to create them directly.  What I want is to prevent just anyone from being able to create a message but allow the lambda to receive queue messages and to communicate directly (i.e. write SQL to) the DB.  I am not sure that doing it with a VPC endpoint is correct from a security standpoint (and that would of course be grounds for denying my request to create one).  What's the right move here?\n\nEDIT:\n\nThe main thing here is that there is a lambda that needs to take in some json data, write it to a db.  There are actually two lambdas which do something similar.  The first lambda handles json for a data structure that has a one-to-many relationship with a second data structure.  The first one has to be processed before the second ones can be, but these messages may appear out of order.  I am also using a dead letter queue to reprocess things that failed the first time.  \n\nI am not married to using SQS and was surprised to learn that it's public.  I had thought that someone with our account credentials (i.e. a coworker) could just invoke aws cli to send messages as he generated them.  If there's a better mechanism to do this, I would appreciate the suggestion.  I would really like to have the action take place in the private subnet.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rey0g0/confused_about_how_to_set_up_a_lambda_in_a/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7g7cso",
          "author": "clintkev251",
          "text": "You do not need any connectivity to SQS from your function in order to use an SQS event source. Just need IAM permission in your function's execution role",
          "score": 11,
          "created_utc": "2026-02-26 03:08:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g529b",
          "author": "aqyno",
          "text": "AWS is a public cloud: meaning most of its services, like S3, API Gateway, Lambda, DynamoDB, and SQS, are accessible over the public internet from anywhere.\n\nThen you’ve got VPC, which is the private side of things, used for resources like EC2 or RDS.\n\nWhen you place a Lambda inside a VPC, it basically moves from the public part into the private part, so it loses access to public services. The usual fix is adding a NAT Gateway or egress gateway so a Lambda in a private subnet can reach the internet or public AWS services. But honestly, that’s not ideal: it’s less secure, costs more, adds latency and bandwidth bottlenecks.\n\nThat’s where VPC endpoints come in. They let private resources talk to public-facing AWS services, but keep all the traffic within AWS’s own network.\n\nFor your specific use case, the only real options are a NAT Gateway, Egress Gateway, or VPC endpoint. That means you either need to set up that infrastructure (IaaS is a different thing) yourself or have it already in place.\n\nMy ideal setup would be a queue locked down with a resource policy that only allows access from a specific VPC endpoint and the Lambda’s IAM role, plus a security group that only permits traffic from the Lambda’s own security group.\n\nAnother option would be to refactor your function so it's not polling the queue in code, just let Lambda receive messages via triggers and consume the body from the event. You could still lock things down with resource policies, but keep in mind, a coworker with broad access could still override your restrictions. That’s why you want to layer in granular permissions.",
          "score": 11,
          "created_utc": "2026-02-26 02:55:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g9v4e",
              "author": "clintkev251",
              "text": "You do not need a VPC endpoint for a function to be triggered by SQS unless you also need to access the SQS service within your code",
              "score": 13,
              "created_utc": "2026-02-26 03:23:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7gaadd",
                  "author": "aqyno",
                  "text": "That's what OP asked. He’s polling the SQS, that's why VPC endpoint configuration fixed the lambda not receiving messages.",
                  "score": 2,
                  "created_utc": "2026-02-26 03:26:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7hy1x8",
          "author": "RecordingForward2690",
          "text": "I have read the other answers, and I think none of them do justice to the complexities of the question. The problem is not so much in receiving the messages from SQS, but what to do in case of failures.\n\n(TL;DR: If you rely exclusively on the SQS-Lambda trigger for fetching and deleting the messages from the queue, you don't have to provide a network path. But if you perform SQS API calls from within your Lambda code, you do.)\n\nFirst, like I said, the reception of the messages is not an issue at all. Given the right IAM policy, it's the SQS-Lambda trigger that does the ReceiveMessage call for you. This trigger has access to SQS - you don't have to provide a network path from your Lambda for this. So there's no need for a NAT, Interface Endpoint or Public IP addresses. That's what the majority of other posts also - rightly - point out.\n\nYou do need to provide a network path, somehow, to your backend database, API or whatever your code delivers the message to. That is also what the majority of other posts also - rightly - point out.\n\nHowever, once the message has been sent to your backend, the message needs to be deleted from SQS. And this is where things might get complicated. Or not. It all depends on how robust you want your code to be against failures.\n\nBy default the SQS-Lambda trigger grabs a batch of messages from SQS and feeds this to your Lambda. If the Lambda succeeds (meaning: it generates a return object of some sort, no timeout, no Exception or something else that indicated failure) then the trigger assumes that all messages were handled properly, and it will delete the messages from the queue. In this case, no explicit network path from your Lambda to SQS needs to be provided.\n\nFrom your Lambda you can also generate a return object which identifies which messages were handled successfully, and which messages failed and need to be retried. Documentation here: [https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html](https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html). In this case it's again the trigger that will perform the DeleteMessage API call for successfully handled messages, and nothing for the messages that failed. Again, no explicit network path required.\n\nHowever, there is a third method which can be used if the two scenarios above don't work for you, for some reason. Your Lambda code can also choose to perform a DeleteMessage API call from the code itself. This is not very common but could be the best solution if you are doing a lot of asynchronous work in your Lambda, and want to delete messages as soon as they are handled, never mind other messages that are still in limbo. In that particular scenario, since the API call originates from the Lambda itself (not from the trigger) you do need to provide a network path to the SQS endpoint. Public IPs, NAT, interface endpoints are just some of the many solutions for that.\n\nThe above assumes that you are using the SQS-Lambda trigger. That's a very common pattern and in most cases the right pattern to use. But recently there was another thread on here where somebody needed to poll an SQS queue and send the data to a backend that was severely rate-limited (external API). He used a different pattern, where the Lambda was called from EventBridge Scheduler, and the Lambda itself performed a (low number of) ReceiveMessage API calls, in order not to overload the backend. Again, in that case there needs to be a network path from the Lambda itself to the SQS endpoint since the API calls are in the code itself, and not handled by the trigger.",
          "score": 4,
          "created_utc": "2026-02-26 11:39:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g2b85",
          "author": "cachemonet0x0cf6619",
          "text": "what youre doing is fine but you might be thinking about it wrong. sqs is secured by iam permission so you don’t need that in a vpc. just don’t give iam permission to create messages on the queue.",
          "score": 3,
          "created_utc": "2026-02-26 02:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ik8p2",
              "author": "icky_cyclist",
              "text": "You’re overthinking it. Amazon sqs is controlled by i am no need to put it in a vpc. Just lock down the permissions properly.",
              "score": 1,
              "created_utc": "2026-02-26 14:01:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7it58n",
                  "author": "cachemonet0x0cf6619",
                  "text": "right. that bit is clear. what isn’t clear is ops subnet config. do they have the lamb in a subnet that has a nat gateway or private link",
                  "score": 1,
                  "created_utc": "2026-02-26 14:48:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7gjdwc",
              "author": "rolandofghent",
              "text": "He needs to be in the VPC to write to the Database.",
              "score": 0,
              "created_utc": "2026-02-26 04:23:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7gqt08",
                  "author": "cachemonet0x0cf6619",
                  "text": "the lambda does, yes. and i think you’re right. I’m making an assumption that the lambda is in a subnet that has either vpc private link or a nat gateway configured",
                  "score": 0,
                  "created_utc": "2026-02-26 05:15:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7g20tl",
          "author": "UltimateLmon",
          "text": "If you set up Queue with appropriate IAM policies to both the queue and encryption key, then you shouldn't have to worry about the VPC (as far as the queue is concerned).\n\n  \nIs what you want restrict access for someone pushing messages into the queue or triggering Lambda?\n\n  \nYou do want Lambda to be in the private subnet with appropriate security group if you trying to hit the database in the same subnet / connected subnet.",
          "score": 2,
          "created_utc": "2026-02-26 02:38:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g5w9m",
              "author": "aqyno",
              "text": "If your Lambda needs to hit both a database inside a VPC and a public service like SQS, it has to be in a private subnet. That’s really the only setup that works.",
              "score": 2,
              "created_utc": "2026-02-26 03:00:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7g7gfu",
                  "author": "UltimateLmon",
                  "text": "Exactly yeah. \n\n\nI'm more wondering what the OP meant by \"anyone being able to create a message\".\n\n\nIt would be question of locking down the IAM policies involved but hard to tell what the entry point the OP wants to deny.",
                  "score": 1,
                  "created_utc": "2026-02-26 03:09:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7gtsmv",
                  "author": "aplarsen",
                  "text": "Private VPC + an egress to the public internet",
                  "score": 1,
                  "created_utc": "2026-02-26 05:38:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7i8gek",
          "author": "solo964",
          "text": "Unclear what you mean by \"I am not married to using SQS and was surprised to learn that it's public\". What is it that you think is public that you're concerned about?\n\nOn \"I had thought that someone with our account credentials (i.e. a coworker) could just invoke aws cli to send messages as he generated them\", yes you can do that. What makes you think this is not possible?",
          "score": 1,
          "created_utc": "2026-02-26 12:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ia6r5",
          "author": "Wide_Commission_1595",
          "text": "So, this is a classic problem of understanding the components, where they live, how they communicate and how to secure it all.\n\nFirstly, SQS will never be \"inside\" your VPC.  It's a global service and as long as IAM permissions on the queue and on the principal trying to send a message all agree, then the queue is secure.\n\nThe lambda function, because of your event source mapping, will be invoked by the poller and this will pass the messages to you.  You can control which ones are deleted as consumed or returned to the queue in the return response.\n\nYou only need to put the lambda in a vpc if your database is also in a vpc.  If it's lambda, then that is also a global service.  If your lambda is in a vpc you need to go e it a security group with access to the database sg, and then then database sg allow ingress from the lambda sg\n\nIf that's all in place, a message in the queue will trigger the function which will write to the DB and then respond saying the messages can be deleted from the queue",
          "score": 1,
          "created_utc": "2026-02-26 13:04:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g2hsy",
          "author": "aplarsen",
          "text": "Why do you have it in a private subnet? What problem does that solve?",
          "score": 0,
          "created_utc": "2026-02-26 02:41:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g5ysr",
              "author": "aqyno",
              "text": "Connecting yo a private DB. Apparently.",
              "score": 2,
              "created_utc": "2026-02-26 03:00:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7hwn0j",
              "author": "Sirwired",
              "text": "In general, things that don't need access to the Internet shouldn't have it. It drives up cost, increases attack surface, and increases the chance of security-breaking mis-configuration.",
              "score": 1,
              "created_utc": "2026-02-26 11:27:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7g2lwl",
          "author": "Prestigious_Pace2782",
          "text": "Yeah you need a vpc endpoint in there for sqs and need to allow https between the lambda and the endpoint.",
          "score": 0,
          "created_utc": "2026-02-26 02:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g7qjn",
              "author": "clintkev251",
              "text": "No you don't. The Lambda service polls SQS, not your function",
              "score": 3,
              "created_utc": "2026-02-26 03:11:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7h5i49",
                  "author": "Prestigious_Pace2782",
                  "text": "If you are in a private vpc with no Nat gateway an are calling sqs via an sdk in your code, my experience is that you need an endpoint. It’s a pattern I use a bit.",
                  "score": 1,
                  "created_utc": "2026-02-26 07:15:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1ralvuk",
      "title": "36hr+ Downtime - Response Required: Your Account is on Hold - Need Help",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1ralvuk/36hr_downtime_response_required_your_account_is/",
      "author": "Realistic-Lab6157",
      "created_utc": "2026-02-21 08:25:28",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 0.75,
      "text": "I received this 2 days ago, and I have seen other posts too about it.  \nThe pain is that I have all the right documents to get this verification done but the communication process is so slow and confusing which keeps delaying this and my services are still down.\n\nI have already created support tickets and cases but there has been no response for the last 12 hours. I am stuck here and need urgent help. u/AWSSupport\n\nI am not even sure if the verification team works on the weekends which might add 2 more days of downtime.\n\nDoes anyone have any idea on how to get this escalated or prioritized?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1ralvuk/36hr_downtime_response_required_your_account_is/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6kqbq3",
          "author": "Sirwired",
          "text": "Which support plan do you have?",
          "score": 5,
          "created_utc": "2026-02-21 09:21:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6l304i",
              "author": "Realistic-Lab6157",
              "text": "It’s basic. I was trying to upgrade it but it’s not allowing me.",
              "score": 2,
              "created_utc": "2026-02-21 11:24:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rb6tvu",
      "title": "Registering Partition Information to Glue Iceberg Tables",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rb6tvu/registering_partition_information_to_glue_iceberg/",
      "author": "mike_get_lean",
      "created_utc": "2026-02-22 00:07:23",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I am creating Glue Iceberg tables using Spark on EMR. After creation, I also write a few records to the table. However, when I do this, Spark does not register any partition information in Glue table metadata.\n\nAs I understand, when we use hive, during writes, spark updates table metadata in Glue such as partition information by invoking UpdatePartition API. And therefore, when we write new partitions in Hive, we can get EventBridge notifications from Glue for events such as `BatchCreatePartition`. Also, when we invoke `GetPartitions`, we can get partition information from Glue Tables.\n\nI understand Iceberg works based on metadata and has a feature for hidden partitioning but I am not sure if this is the sole reason Spark is not registering metadata info with Glue table. This is causing various issues such as not being able to detect data changes in tables, not being able to run Glue Data Quality checks on selected partitions, etc.\n\nIs there a simple way I can get this partition change and update information directly from Glue?\n\nOne of the bad ways to do this will be to create S3 notifications, subscribe to those and then run Glue Crawler on those events, which will create another S3 based Glue table with the correct partition information. And then do DQ checks on this new table. I do not like this approach at all because I will need to setup significant automation to achieve this.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rb6tvu/registering_partition_information_to_glue_iceberg/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6qtnje",
          "author": "freerangetrousers",
          "text": "Glue is quite simply built with Hive in mind. Iceberg is more feature rich, but aws haven't built the whole glue ecosystem around it so there isn't feature parity. \nGlue catalogue is just Hive metastore , but iceberg isn't really designed to have to utilise this. \n\n\nInstead you can check for changes on the metadata.json file , or just at the end of your spark scripts emit an event yourself with the details you want to convey. Like in the olden days. ",
          "score": 1,
          "created_utc": "2026-02-22 09:10:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbfv2w",
      "title": "Cloudwatch alarms mute rules",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rbfv2w/cloudwatch_alarms_mute_rules/",
      "author": "becharaerizk",
      "created_utc": "2026-02-22 07:47:17",
      "score": 6,
      "num_comments": 17,
      "upvote_ratio": 0.88,
      "text": "Hello,\n\nI wanted to implement some kind of maintenance mode on alarms i have setup on my work's awa account. Right before i started i saw WA released alarms mute rules which do exactly what i want.\n\nIt works well using the console but i want to write a function that takes a specific string and created mute rules with all alarms containing this string. (For automated workflows and such)\n\nI noticed that neither the cli nor the python sdk support this yet, when are mute rules supposed to be released for cli or boto3 in python?\n\n  \nFeature I am speaking about: [Configure alarm mute rules](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/alarm-mute-rules-configure.html)\n\n  \nChecking [boto3's latest documentation](https://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch.html): no mention of this feature",
      "is_original_content": false,
      "link_flair_text": "monitoring",
      "permalink": "https://reddit.com/r/aws/comments/1rbfv2w/cloudwatch_alarms_mute_rules/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6qofga",
          "author": "Refwah",
          "text": "\nhttps://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch/client/describe_alarms.html\n\nhttps://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch/client/disable_alarm_actions.html",
          "score": 1,
          "created_utc": "2026-02-22 08:20:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qp3he",
              "author": "becharaerizk",
              "text": "Im not talking about disable alarm actions, this doesnt help my case im speaking about [Configure alarm mute rules](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/alarm-mute-rules-configure.html)",
              "score": 1,
              "created_utc": "2026-02-22 08:26:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6qq2pd",
                  "author": "Refwah",
                  "text": "If you set the state:\n\nhttps://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch/client/set_alarm_state.html\n\nAnd then immediately disable actions \n\nYou effectively get a mute\n\nYou then re-enable actions and you un mute the alarm\n\nYou may want to set them back to not alarming to in order to trigger the action again",
                  "score": 1,
                  "created_utc": "2026-02-22 08:35:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6r9jz0",
          "author": "sandro-_",
          "text": "Isn't that the API: https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_PutAlarmMuteRule.html\n\nTo effectively create a mute rule for a specific time?\n\nAnd in MuteTargets you can target which alarms to mute?",
          "score": 1,
          "created_utc": "2026-02-22 11:41:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rcdfw",
              "author": "becharaerizk",
              "text": "Yes the cli commands provided dont work, they return an error when using then, even when using cloudshell",
              "score": 1,
              "created_utc": "2026-02-22 12:06:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rj9vh",
                  "author": "KayeYess",
                  "text": "Make sure one of the associated IAM policies for the user/role has the required permissions for cloudwatch:PutAlarmMuteRule",
                  "score": 1,
                  "created_utc": "2026-02-22 12:59:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rwdon",
                  "author": "Flakmaster92",
                  "text": "Make sure you’re also running the absolute latest version of the CLI, too many times I’ve seen people saying “it didn’t work” when they’re running a CLI version from before the feature came out",
                  "score": 1,
                  "created_utc": "2026-02-22 14:20:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6sitd6",
          "author": "ruibranco",
          "text": "The native mute rules feature is console-only for now — SDK/CLI support is usually added a few months after console launch. In the meantime, the disable\\_alarm\\_actions + set\\_alarm\\_state approach works but is fragile for automated workflows. Another option worth considering: EventBridge Scheduler to run a Lambda that toggles alarm actions on/off around your maintenance windows, which is more auditable and easier to manage at scale than per-alarm state manipulation.",
          "score": 1,
          "created_utc": "2026-02-22 16:09:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6txbhe",
              "author": "crh23",
              "text": "Months would be surprising - I'd expect days",
              "score": 1,
              "created_utc": "2026-02-22 20:05:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6syb93",
              "author": "becharaerizk",
              "text": "A few months for a simple api call from python or the cli? Any programmer could finish that in a week without the use of ai lol",
              "score": 0,
              "created_utc": "2026-02-22 17:19:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6z9xla",
          "author": "liverdust429",
          "text": "The API exists (PutAlarmMuteRule) but SDK/CLI support always lags behind console launches. Check your CloudShell CLI version with `aws --version`, it's probably outdated. If it's still not there on the latest version you can always hit the API directly with SigV4 signing until boto3 catches up. Annoying but it works.",
          "score": 1,
          "created_utc": "2026-02-23 16:56:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7988j4",
          "author": "crh23",
          "text": "It has now released in [the CLI](https://github.com/aws/aws-cli/blob/v2/CHANGELOG.rst) and [boto3](https://github.com/boto/boto3/blob/develop/CHANGELOG.rst)",
          "score": 1,
          "created_utc": "2026-02-25 02:33:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79mfbr",
              "author": "becharaerizk",
              "text": "Finally lol",
              "score": 2,
              "created_utc": "2026-02-25 03:55:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1reb3q4",
      "title": "AWS Backup Jobs with VSS Errors",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1reb3q4/aws_backup_jobs_with_vss_errors/",
      "author": "Budget-Industry-3125",
      "created_utc": "2026-02-25 11:49:03",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Good morning guys,\n\n  \nI've set up AWS Backup Jobs for many of my EC2 Instances. \n\nThere are 20 VMs enabled for backing up their data to AWS, but somehow 9 of them are presenting the following errors:\n\nWindows VSS Backup Job Error encountered, trying for regular backup\n\nI have tried re-installing the backup agent in the vms and updating, but it doesn't seem to be working out. \n\nUpon connecting to the machines, I'm able to find some VSS providers in failed states. However, after restarting them and verifying that they are OK, the job fails again with the same error message.\n\n  \nHas anyone encountered this behaviour before?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1reb3q4/aws_backup_jobs_with_vss_errors/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7bn6tp",
          "author": "brile_86",
          "text": "  \nCheck the pre-reqs  \n[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/application-consistent-snapshots-prereqs.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/application-consistent-snapshots-prereqs.html)\n\nTLDR:\n\n* Windows Server 2016\n* .NET Framework version 4.6 or later\n* Windows PowerShell major version 3, 4, or 5 with language mode set to FullLanguage\n* AWS Tools for Windows PowerShell version [3.3.48.0](http://3.3.48.0) or later\n* IAM policy AWSEC2VssSnapshotPolicy (or equivalent permissions) attached (make sure you don't have any restrictive SCP or IAM Policy Boundaries blocking it)\n\n  \nAlso some instances are not supported as too small  \n[https://docs.aws.amazon.com/aws-backup/latest/devguide/windows-backups.html](https://docs.aws.amazon.com/aws-backup/latest/devguide/windows-backups.html)\n\n* t3.nano\n* t3.micro\n* t3a.nano\n* t3a.micro\n* t2.nano\n* t2.micro\n\n",
          "score": 3,
          "created_utc": "2026-02-25 13:33:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hhfi0",
              "author": "Budget-Industry-3125",
              "text": "already did. all instances have the same permissions. I've reduced the number of jobs with errors to 2 of them.",
              "score": 1,
              "created_utc": "2026-02-26 09:07:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7bjqdc",
          "author": "ReturnOfNogginboink",
          "text": "This is a Windows issue, not an AWS issue. The error is coming from the Windows volume snapshot service.",
          "score": 0,
          "created_utc": "2026-02-25 13:13:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cm51j",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-02-25 16:26:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7co5m1",
              "author": "gex80",
              "text": "Why would they contact Veeam support?",
              "score": 1,
              "created_utc": "2026-02-25 16:35:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rf6j6v",
      "title": "No P5 instances available in any region?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rf6j6v/no_p5_instances_available_in_any_region/",
      "author": "peanutknight1",
      "created_utc": "2026-02-26 10:18:34",
      "score": 5,
      "num_comments": 22,
      "upvote_ratio": 1.0,
      "text": "Curious, is everyone facing the same issue? We have no service quota issue but we arent able to create any P5 type EC2 to train our models. \n\nIts a little crazy, we checked every single region, is there such a big shortage? \n\nAny recommendations on what we can do? \n\nTrainium instances are not available either! ",
      "is_original_content": false,
      "link_flair_text": "compute",
      "permalink": "https://reddit.com/r/aws/comments/1rf6j6v/no_p5_instances_available_in_any_region/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7houm5",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'compute'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+compute).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-26 10:18:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hrzdo",
          "author": "ENBD",
          "text": "You need to use capacity blocks for ML. That’s probably the only realistic way to get access to these instances in us-east-1. https://aws.amazon.com/ec2/capacityblocks/\n\nTalk to your TAM also. They may be able to come up with some other options.",
          "score": 20,
          "created_utc": "2026-02-26 10:47:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hs6ya",
              "author": "peanutknight1",
              "text": "We dont have a TAM, how do we get one? We only have an account manager.",
              "score": 2,
              "created_utc": "2026-02-26 10:49:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hsr4k",
                  "author": "ENBD",
                  "text": "You get assigned a TAM with enterprise support. Capacity blocks are the answer here. Make sure you can launch the instance in any AZ in the region, so have subnets provisioned in all AZs.",
                  "score": 7,
                  "created_utc": "2026-02-26 10:54:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ktr4n",
                  "author": "Ok_Chocolate8661",
                  "text": "Talk to your account manager.",
                  "score": 1,
                  "created_utc": "2026-02-26 20:27:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7hr0m4",
          "author": "alapha23",
          "text": "At least trainium should be available. For p5 you need to talk to your business development manager in aws",
          "score": 5,
          "created_utc": "2026-02-26 10:38:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hrepj",
              "author": "peanutknight1",
              "text": "We did, but its not an issue of quota allocation, its an issue of capacity. They cant help if there are no P5 servers available.\n\nReally difficult right now, training on g6/ g5 is taking 21hrs for one run.\n\nedit:yes, trn type instances are not available either\n\nMeaning if you go to launch the instance it will say insufficient capacity",
              "score": 1,
              "created_utc": "2026-02-26 10:42:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hwliz",
                  "author": "alapha23",
                  "text": "also tell them to talk to SSO to put you on the waitinglist",
                  "score": 2,
                  "created_utc": "2026-02-26 11:27:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7hwi5v",
                  "author": "alapha23",
                  "text": "Tell them to use baywatch to check which region has availabilities",
                  "score": 1,
                  "created_utc": "2026-02-26 11:26:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7hwpcp",
                  "author": "alapha23",
                  "text": "Also, write a script to race for it, that works surprisingly well as well",
                  "score": 1,
                  "created_utc": "2026-02-26 11:28:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ll55k",
                  "author": "Flakmaster92",
                  "text": "A single G5? Why not do distributed training?",
                  "score": 1,
                  "created_utc": "2026-02-26 22:39:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ibcyb",
                  "author": "daredevil82",
                  "text": "good.  maybe this bullshit you're doing affects you as much if not more than people trying to buy hardware\n\nhttps://arstechnica.com/gadgets/2026/02/ram-now-represents-35-percent-of-bill-of-materials-for-hp-pcs/\n\n>I pulled the receipt for 48GB of DDR5 UDIMMs I bought in mid-2024 for my home server. $245. The slower speed version of that same kit is in stock right now for $945.",
                  "score": -2,
                  "created_utc": "2026-02-26 13:11:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7osfi0",
          "author": "crh23",
          "text": "How long is the training run? Capacity blocks are made for this, or try spot if it's short. Also try to have flexibility on instance type",
          "score": 1,
          "created_utc": "2026-02-27 12:11:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7i5hbl",
          "author": "The_Packeteer",
          "text": "U can get them, u just have to request them and it takes time for them to be allocated",
          "score": 0,
          "created_utc": "2026-02-26 12:34:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd84jb",
      "title": "Quantum-Guided Cluster Algorithms for Combinatorial Optimization",
      "subreddit": "aws",
      "url": "https://aws.amazon.com/blogs/quantum-computing/quantum-guided-cluster-algorithms-for-combinatorial-optimization/",
      "author": "donutloop",
      "created_utc": "2026-02-24 05:55:27",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1rd84jb/quantumguided_cluster_algorithms_for/",
      "domain": "aws.amazon.com",
      "is_self": false,
      "comments": [
        {
          "id": "o73zfzp",
          "author": "nucleustt",
          "text": "What in god's name is this? ELI5",
          "score": 2,
          "created_utc": "2026-02-24 09:50:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rctlrt",
      "title": "How Does Karpenter Handle AMI Updates via SSM Parameters? (Triggering Rollouts, Refresh Timing, Best Practices)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rctlrt/how_does_karpenter_handle_ami_updates_via_ssm/",
      "author": "LemonPartyRequiem",
      "created_utc": "2026-02-23 20:52:45",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I’m trying to configure Karpenter so a `NodePool` uses an `EC2NodeClass` whose AMI is selected via an SSM Parameter that we manage ourselves.\n\nWhat I want to achieve is an automated (and controlled) AMI rollout process:\n\n* Use a Lambda (or another AWS service, if there’s a better fit) to periodically fetch the latest AWS-recommended EKS AMI (per the AWS docs: [https://docs.aws.amazon.com/eks/latest/userguide/retrieve-ami-id.html](https://docs.aws.amazon.com/eks/latest/userguide/retrieve-ami-id.html)).\n* Write that AMI ID into *our own* SSM Parameter Store path.\n* Update the parameter used by our **test** cluster first, let it run for \\~1 week, then update the parameter used by **prod**.\n* Have Karpenter automatically pick up the new AMI from Parameter Store and perform the node replacement/upgrade based on that change.\n\nWhere I’m getting stuck is understanding how `amiSelectorTerms` works when using the `ssmParameter` option (docs I’m referencing: [https://karpenter.sh/docs/concepts/nodeclasses/#specamiselectorterms](https://karpenter.sh/docs/concepts/nodeclasses/#specamiselectorterms)):\n\n* How exactly does Karpenter resolve the AMI from an `ssmParameter` selector term?\n* When does Karpenter re-check that parameter for changes (only at node launch time, periodically, or on some internal resync)?\n* Is there a way to force Karpenter to re-resolve the parameter on a schedule or on demand?\n* What key considerations or pitfalls should I be aware of when trying to implement AMI updates this way (e.g., rollout behavior, node recycling strategy, drift, disruption, caching)?\n\nThe long-term goal is to make AMI updates as simple as updating a single SSM parameter: update test first, validate for a week, then update prod letting Karpenter handle rolling the nodes automatically.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rctlrt/how_does_karpenter_handle_ami_updates_via_ssm/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o72r5s3",
          "author": "sunra",
          "text": "Your best bet is going to be to read the source.\n\nBut my understanding is that it is the Karpenter controller itself which monitors the SSM parameter (not the nodes themselves). When the controller notices that some nodes don't match the parameter it will mark the nodes as \"drifted\", and the replacements will happen according to your node-pool disruption-budget and node-termination-grace-period.\n\nI don't know this for sure - it's my expectation based on how Karpenter handles other changes (like k8s control-plane upgrades).",
          "score": 1,
          "created_utc": "2026-02-24 03:42:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73gyvq",
          "author": "yuriy_yarosh",
          "text": "1. You'll need to enable drift detection so it'll actually resync SSM   \n[https://karpenter.sh/docs/reference/settings/#feature-gates](https://karpenter.sh/docs/reference/settings/#feature-gates)\n\n2. SSM itself is throttled [https://github.com/aws/karpenter-provider-aws/issues/5907](https://github.com/aws/karpenter-provider-aws/issues/5907)  \nResync was 5 min before contributing to CNCF (reconcil cycle period for the whole controller), but now it's hardcoded to start checking only after an hour  \n[https://github.com/kubernetes-sigs/karpenter/blob/main/designs/drift.md](https://github.com/kubernetes-sigs/karpenter/blob/main/designs/drift.md)  \n[https://github.com/kubernetes-sigs/karpenter/blob/main/pkg/controllers/nodeclaim/disruption/drift.go#L93](https://github.com/kubernetes-sigs/karpenter/blob/main/pkg/controllers/nodeclaim/disruption/drift.go#L93)  \n[https://github.com/aws/karpenter-provider-aws/blob/main/pkg/cloudprovider/cloudprovider.go#L281](https://github.com/aws/karpenter-provider-aws/blob/main/pkg/cloudprovider/cloudprovider.go#L281)  \n\n\nSpot instances require SQS interruption queue `--interruption-queue`  \n[https://karpenter.sh/docs/concepts/disruption/#interruption](https://karpenter.sh/docs/concepts/disruption/#interruption)\n\n3. No, on-demand... \n\nYeah, been there, Karpenter is all over the place so wrote a custom cluster autoscaler with a Terraform provider and Kamaji, to keep infra state consistent, synchronized, and in one place.",
          "score": 1,
          "created_utc": "2026-02-24 06:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bbcgk",
          "author": "EcstaticJellyfish225",
          "text": "Consider using TAGs for the AMI selector, you should be able to tag AWS provided (and your own) AMIs.  Then you can pre-test an AMI in your dev account, once you are happy with it, you can tag the same AMI in your prod account and it will become available for karpenter to pick up next time a new node is needed (or if using drift detection at any time your disruption budget allows).\n\nAutomating the test cycle and tagging AMIs that pass the test, is also pretty straight forward. Test in a dev account, if the AMI asses the test, by some means tag the same AMI in your prod account. (Maybe setup an SNSTopic triggering a lambda, or something similar).",
          "score": 1,
          "created_utc": "2026-02-25 12:19:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rckoel",
      "title": "CACs in Workspaces",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rckoel/cacs_in_workspaces/",
      "author": "KrazyMuffin",
      "created_utc": "2026-02-23 15:36:47",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Our current AWS workspace setup uses Simple AD, as I couldn't get AD Connector to work (will work on getting this working another time).\n\n\n\nCurrently a Linux workspace (Rocky Linux 8) can use CACs to authenticate to sites in-session, however, on Windows (Windows Server 2022), it doesn't recognize my computer's CAC reader. I have installed ActivID and InstallRoot, the workspace is DCV (formerly WSP).\n\n\n\nThe documentation all talks about how to setup readers with AD Connector so you can log into the workspace with your CAC, but that's not what we're trying to do, just be able to use the reader inside the instance.\n\n\n\nAny suggestions?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rckoel/cacs_in_workspaces/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6zxsei",
          "author": "fjleon",
          "text": "smart card redirection is disabled in windows by default for both presession and insession. you need to enable via GPO\n\nhttps://docs.aws.amazon.com/workspaces/latest/adminguide/group_policy.html#gp_install_template_wsp",
          "score": 2,
          "created_utc": "2026-02-23 18:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75hoqv",
              "author": "KrazyMuffin",
              "text": "Thank you, that worked, I'm dumb 😅",
              "score": 1,
              "created_utc": "2026-02-24 15:38:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rfxklh",
      "title": "Data security specialist intern?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rfxklh/data_security_specialist_intern/",
      "author": "Next-Garbage9163",
      "created_utc": "2026-02-27 05:04:11",
      "score": 5,
      "num_comments": 5,
      "upvote_ratio": 0.73,
      "text": "if there is anyone who works here, can you tell me what this position entails of. Is it just a glorified security guard? I don't want to intern how to be a security gaurd lol.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rfxklh/data_security_specialist_intern/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7nfssm",
          "author": "playahate",
          "text": "Interning at aws is a really good way to get hired on. Even if it does suck get that name on your resume to differentiate yourself.",
          "score": 2,
          "created_utc": "2026-02-27 05:11:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ng5cy",
              "author": "Next-Garbage9163",
              "text": "Yeah thats what I figured. All I have is retail experience on my resume so AWS Data Security Specialist Intern would look amazing.",
              "score": 0,
              "created_utc": "2026-02-27 05:14:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ozgjj",
          "author": "RecordingForward2690",
          "text": "Any \"security\" role in IT is something that you have to be cut out for. Security is usually assumed (just like backups) so nobody gives you a pat on the back if you do your work properly and no security issues happen. But you're first in line to get blamed if something goes wrong.\n\nHaving said that, a Data Security Specialist role can be very interesting and will take you across all the services that AWS offers, will have you talk with Operations, Developers, Legal, Marketing, Data Analysts and a whole bunch of others across the organisation/solution. You'll need to talk with them about Identity, Authentication, Authorization, technical aspects of getting access, retention times for data, making sure data can be trusted, availability, GDPR and other legislation, runbooks for data breaches and so forth. Both before a project starts, in the design phase, but also afterwards when doing audits. You'll also get intimate knowledge of the tools that are able to help you do your work as an auditor. Heck, maybe the company will also sponsor you in obtaining an \"Ethical Hacker\" certificate.\n\nA very interesting aspect of Data Security today is the proliferation of all sorts of AI, which makes it very hard to put guardrails in place to ensure data doesn't leak out via trained models and such. And AI is sometimes coming up with novel ways to breach guardrails. Plus users come up with novel ways to attach AI to data sources without authorization, causing data leaks all over the place.\n\nSo yeah, an internship in that area can be very interesting in itself, and a very good leg-up into getting into IT/AWS in general.",
          "score": 1,
          "created_utc": "2026-02-27 12:59:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pb4ob",
          "author": "solo964",
          "text": "Do you mean \"data **center** security specialist intern\"? Like this [job posting](https://www.amazon.jobs/en/jobs/3189192/data-center-security-specialist-intern-us).\n\nThe role description:\n\n>As a Data Center Operational Security Specialist Intern, you will be tasked with driving operational security excellence within our Data Centers. You will write reports, create presentations and communicate with management on the status of physical security operations.",
          "score": 1,
          "created_utc": "2026-02-27 14:07:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pdutm",
              "author": "Next-Garbage9163",
              "text": "Yes",
              "score": 1,
              "created_utc": "2026-02-27 14:22:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1raayhv",
      "title": "Help with cognito: Code security resource quotas not enforced?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1raayhv/help_with_cognito_code_security_resource_quotas/",
      "author": "TutorNeat2724",
      "created_utc": "2026-02-20 23:20:47",
      "score": 4,
      "num_comments": 3,
      "upvote_ratio": 0.84,
      "text": "Hi everyone,\nI’ve noticed what seems to be unexpected behavior regarding Cognito User Pools code security resource quotas.\nAccording to the [documented limits](https://docs.aws.amazon.com/cognito/latest/developerguide/quotas.html#resource-quotas), certain operations (e.g. GetUserAttributeVerificationCode) should be rate-limited (for example, max 5 consecutive requests). However, in my tests, I’m able to call GetUserAttributeVerificationCode more than 5 times in a row without receiving any throttling error or limit exception.\nHas anyone experienced the same behavior?\nIs there any additional configuration required to enforce these quotas, or are they applied under specific conditions only?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1raayhv/help_with_cognito_code_security_resource_quotas/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6ipsm5",
          "author": "Skytram_",
          "text": "How long are you calling that API above the max TPS for?",
          "score": 1,
          "created_utc": "2026-02-21 00:16:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k1edw",
          "author": "baever",
          "text": "`GetUserAttributeVerificationCode` has a quota of 5 requests/user/hour. The only thing I can think of is that the bucket is based on hour of day. So if you start at 1:59 and make 4 requests and then at 2:01 you make 4 requests (for a total of 8) it won't block you because those span 2 hour buckets. However, if you make 6 requests in the same hour (say between 2:01 and 2:05) it should block you because it's in 1 hour bucket. Are you never seeing it enforced or are you seeing it intermittently enforced?",
          "score": 1,
          "created_utc": "2026-02-21 05:29:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rb4r7y",
      "title": "Initiating App Studio - Entity Already Exists Error",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rb4r7y/initiating_app_studio_entity_already_exists_error/",
      "author": "haonconstrictor",
      "created_utc": "2026-02-21 22:38:59",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Every time I try to setup App Studio on my new account I receive the “Entity Already Exists” error no matter what I do. I’ve confirmed a group exists in IAM and it’s not duplicated anywhere. Pulling my hair out trying to even get off the ground with AWS and can’t figure this out. I’ve deleted and rebuilt the IAM multiple times, and still nothing. Any advice is appreciated! ",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rb4r7y/initiating_app_studio_entity_already_exists_error/",
      "domain": "self.aws",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rbz0rw",
      "title": "Track Karpenter efficiency of cluster bin-packing over time with kube-binpacking-exporter",
      "subreddit": "aws",
      "url": "https://github.com/sherifabdlnaby/kube-binpacking-exporter",
      "author": "SherifAbdelNaby",
      "created_utc": "2026-02-22 21:59:10",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "technical resource",
      "permalink": "https://reddit.com/r/aws/comments/1rbz0rw/track_karpenter_efficiency_of_cluster_binpacking/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    }
  ]
}