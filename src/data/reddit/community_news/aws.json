{
  "metadata": {
    "last_updated": "2026-02-26 09:09:58",
    "time_filter": "week",
    "subreddit": "aws",
    "total_items": 20,
    "total_comments": 87,
    "file_size_bytes": 117910
  },
  "items": [
    {
      "id": "1raski8",
      "title": "AWS Certificate Manager updates default certificate validity to comply with new guidelines",
      "subreddit": "aws",
      "url": "https://aws.amazon.com/about-aws/whats-new/2026/02/aws-certificate-manager-updates-default/",
      "author": "SudoAlex",
      "created_utc": "2026-02-21 14:32:40",
      "score": 41,
      "num_comments": 8,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "security",
      "permalink": "https://reddit.com/r/aws/comments/1raski8/aws_certificate_manager_updates_default/",
      "domain": "aws.amazon.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6lu1ai",
          "author": "SudoAlex",
          "text": "Posted this mostly due to the future uncertainty from their announcement back in June 2025, with exportable public certificates: https://www.reddit.com/r/aws/comments/1ldpw1d/aws_certificate_manager_introduces_public/ - as it was well known that certificate lifetimes would be limited in the long run.\n\nThey'll be reducing the duration, along with the price:\n\n> We have reduced the pricing for ACM’s exportable public certificates in line with the shorter validity period. 198-day exportable public certificate will now cost $7/Fully Qualified domain name (down from $15) and $79/ wildcard name (down from $149). Please refer to ACM’s pricing page for more details. For more information about ACM, visit the [ACM documentation](https://docs.aws.amazon.com/acm/latest/userguide/acm-certificate-characteristics.html).\n\n~~Although the wildcard pricing feels like a slightly sneaky $4.50 increase (half of $149 is $74.50)~~.\n\nUpdated numbers below.",
          "score": 10,
          "created_utc": "2026-02-21 14:38:53",
          "is_submitter": true,
          "replies": [
            {
              "id": "o6m80fe",
              "author": "Mutjny",
              "text": ">Although the wildcard pricing feels like a slightly sneaky $4.50 increase (half of $149 is $74.50).\n\n198 days is not half of 365.  The new price is exactly equivalent to the old price with the reduced number of days.  Its actually like $1.50 less.",
              "score": 6,
              "created_utc": "2026-02-21 15:53:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6mq2vo",
                  "author": "SudoAlex",
                  "text": "Certificates weren't valid for 365 days - it was 395 days. Got the calculations slightly off, but it's probably more of a price bump:\n\nBefore: 395 day certificate, renews 60 days before expiry, $149 per 335 days  \nAfter: 198 day certificate, renews 45 days before expiry, $79 per 153 days\n\nShould be closer to $68 for it to be the same price.",
                  "score": 2,
                  "created_utc": "2026-02-21 17:23:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m02lx",
          "author": "wlonkly",
          "text": "Ah thanks for this, I was wondering what their plan was (but not enough to ask our TAM I guess, heh).",
          "score": 2,
          "created_utc": "2026-02-21 15:12:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74hhmh",
          "author": "KayeYess",
          "text": "CA Browser forum voted to reduce public CA signed certificate validity to 200 days on March 26, 2026. It will reduce to 100 days in March, 2027 and 47 days in March 2029. Time to start preparing and automate. We automated certificate issuance and rotation over a decade ago (especially for external certs imported into ACM). Also, start adopting quantum safe ciphers. And mask unnecessary NPI/PII in all communications.",
          "score": 1,
          "created_utc": "2026-02-24 12:22:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rata1g",
      "title": "If S3 vectors offer sub second latency, why does AWS say it's designed for infrequent access?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rata1g/if_s3_vectors_offer_sub_second_latency_why_does/",
      "author": "nucleustt",
      "created_utc": "2026-02-21 15:03:03",
      "score": 37,
      "num_comments": 36,
      "upvote_ratio": 0.85,
      "text": "I'm building a customer service agent and need a vector DB for RAG.\n\nNaturally, I gravitated toward S3 vectors because the 90% cost reduction was super attractive.\n\nI'm wondering if I'm making the right choice (even though I see RAG as a use case).\n\nBasically, the chatbot has to answer questions via WhatsApp.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rata1g/if_s3_vectors_offer_sub_second_latency_why_does/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6lylv2",
          "author": "Sirwired",
          "text": "Because \"sub-second\" is super slow vs. the alternatives.",
          "score": 84,
          "created_utc": "2026-02-21 15:04:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6m0aee",
              "author": "nucleustt",
              "text": "wow. Thanks. I can only imagine the throughput of an in-memory Redis vector DB then. \"Near instant\"",
              "score": -12,
              "created_utc": "2026-02-21 15:13:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6m2aog",
                  "author": "CoastRedwood",
                  "text": "How big is the data you’re storing? Bigger the dataset the higher the latency and greater the cpu usage.",
                  "score": 7,
                  "created_utc": "2026-02-21 15:24:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6mfh90",
          "author": "jdanton14",
          "text": "If I had a database that has 900ms of latency, I'd fire my DBA and my storage vendor.",
          "score": 20,
          "created_utc": "2026-02-21 16:30:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6p44mo",
              "author": "coinclink",
              "text": "If you're working at scale though, vector algorithms are extremely taxing on a db engine, especially on huge datasets. 900ms consistently across a huge vector store is a good tradeoff vs spending thousands of dollars scaling out a bunch of read replicas that might still get deadlocked when a flood of requests come in.",
              "score": 3,
              "created_utc": "2026-02-22 01:11:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6mhxcr",
              "author": "nucleustt",
              "text": "lol. I wonder if the whatsapp api supports the \"is typing\" indicator.",
              "score": 1,
              "created_utc": "2026-02-21 16:42:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6m2h5q",
          "author": "barnaclebill22",
          "text": "That's sub- second for each retrieval. You then need to submit the returned text as context to your LLM, so any question response might end up taking a few seconds. \nIt's easy to switch to something like Opensearch (or use it as a fast cache with S3 vectors), so might be worth trying. \nChatting with a human agent on WhatsApp typically takes several minutes per conversational turn (since they're all handling dozens of conversations).",
          "score": 17,
          "created_utc": "2026-02-21 15:25:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6meti1",
              "author": "HiCookieJack",
              "text": "Or PG Vector with Postgres Serverless ",
              "score": 10,
              "created_utc": "2026-02-21 16:27:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6n0ogv",
                  "author": "hergabr",
                  "text": "PgVector on Postgres works surprisingly well for a chatbot",
                  "score": 3,
                  "created_utc": "2026-02-21 18:16:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6mhr80",
              "author": "nucleustt",
              "text": "Thanks, these are all solutions I would test and compare",
              "score": 1,
              "created_utc": "2026-02-21 16:41:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nshiy",
          "author": "Due-Horse-5446",
          "text": "500ms is sub second too..",
          "score": 5,
          "created_utc": "2026-02-21 20:38:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ol5r5",
              "author": "nucleustt",
              "text": "true. But even a second isn't bad. Hopefully someone on chat can wait \\~5 sec for a response... hopefully",
              "score": -1,
              "created_utc": "2026-02-21 23:14:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6qw3no",
                  "author": "Due-Horse-5446",
                  "text": "thats a pretty extreme amount, espetfpr chatbots you should be shaving miliseconds where you can..",
                  "score": 1,
                  "created_utc": "2026-02-22 09:33:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6oabfi",
          "author": "tank_of_happiness",
          "text": "I’m using it for your same use case. The LLM’s thinking takes way longer. I’m happy with the results I’m getting and the cost is almost nothing at my volume. Previously I was using open search and paying a couple hundred a month.",
          "score": 5,
          "created_utc": "2026-02-21 22:13:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6olb1w",
              "author": "nucleustt",
              "text": "wtf, couple hundred a month vs almost nothing!? That's crazy cost reduction!",
              "score": 1,
              "created_utc": "2026-02-21 23:15:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6qlamu",
              "author": "nucleustt",
              "text": "Also, it's reassuring that someone else is using it for my use case.",
              "score": 1,
              "created_utc": "2026-02-22 07:50:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6n2cnh",
          "author": "yarenSC",
          "text": "In S3 terms, infrequent access is a billing term more than anything else",
          "score": 3,
          "created_utc": "2026-02-21 18:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6okz0x",
              "author": "nucleustt",
              "text": "ha. gotcha",
              "score": 1,
              "created_utc": "2026-02-21 23:13:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6suj0a",
          "author": "CrazedBotanist",
          "text": "We are using S3 vectors for a couple projects. Overall, they have worked great for us and are cheap. However, the two things to look out for is the lack of native support for hybrid search (semantic and keyword search) and the latency. Latency has only become any issue for us in Agentic RAG solutions where the agents tend to search multiple times based on search results and the initial question.",
          "score": 1,
          "created_utc": "2026-02-22 17:02:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6uu3b5",
              "author": "nucleustt",
              "text": "Super useful comment. Especially the \"lack of hybrid (semantic and keyword) search\" part that I didn't even consider. That's a huge disadvantage.\n\nThank you",
              "score": 1,
              "created_utc": "2026-02-22 22:53:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6vog30",
                  "author": "CrazedBotanist",
                  "text": "No problem! If you need hybrid search and don’t mind the overhead of using managed open search clusters you can use s3 vectors as the storage engine for some cost savings on storage.",
                  "score": 1,
                  "created_utc": "2026-02-23 01:49:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6trq8l",
          "author": "netik23",
          "text": "You can’t tell me you have so much data you can’t store it on a SSD instance.",
          "score": 1,
          "created_utc": "2026-02-22 19:37:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6uubn3",
              "author": "nucleustt",
              "text": "You're right. I can't. What do you suggest?",
              "score": 1,
              "created_utc": "2026-02-22 22:54:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6mz94n",
          "author": "rexspook",
          "text": "Sub second is infrequent in computing terms",
          "score": -4,
          "created_utc": "2026-02-21 18:09:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9nmsm",
      "title": "When DynamoDB single-table design is the wrong choice (and what to use instead)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r9nmsm/when_dynamodb_singletable_design_is_the_wrong/",
      "author": "tejovanthn",
      "created_utc": "2026-02-20 06:11:35",
      "score": 30,
      "num_comments": 16,
      "upvote_ratio": 0.81,
      "text": "Last week's multi-tenant post hit #1 here. The comments were more useful than the upvotes - three engineers described it as a disaster at their companies.\n\nThey're not wrong. I use single-table design in production and I'm building a tool around it, but there are real situations where it's the wrong call.\n\nThe cases where you shouldn't reach for it:\n\n* Access patterns are still changing (pre-PMF products)\n* No DynamoDB champion on the team - works great until that person leaves\n* Significant reporting or analytics requirements\n* Fewer than 6 access patterns (just use multi-table, it's fine)\n* Multiple teams or bounded contexts sharing the same deployment\n* Per-entity DynamoDB Streams processing (you get one stream per table, not per entity type)\n\nThe \"disaster\" pattern I keep seeing isn't single-table being wrong - it's teams starting with key design before listing access patterns. You design the table to serve access patterns, not the other way around.\n\nFull post covers: the decision framework table, the microservices/team ownership case (probably the most underrated reason to avoid it), and what I actually use for [rasika.life](http://rasika.life) vs what I'd use for a prototype.\n\n→ [https://singletable.dev/blog/when-not-to-use-single-table-design](https://singletable.dev/blog/when-not-to-use-single-table-design)\n\nCurious what situations have pushed your teams away from it - or toward it despite the complexity.",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1r9nmsm/when_dynamodb_singletable_design_is_the_wrong/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6gbip1",
          "author": "menge101",
          "text": "Several of these \"single table design is the wrong choice\" are more \"DynamoDB is the wrong choice\" imo.\n\nParticularly: \"You have serious analytical or reporting requirements\"\n\nIt's an OLTP datastore, not an OLAP, it doesn't matter how many tables you use, you can't do arbitrary queries on the data.\n\nThis one: \"Fewer than 6 access patterns (just use multi-table, it's fine)\", i don't agree with.  It's not about how many access patterns, its about access patterns that capture relationships in the data.\n\nYou can have only one access pattern, but if the table uses the partition key and sort key to create a relationship between records, and then you pull all records for the partition key and have a full representation of the data relationship, you have used single table design correctly with a single access pattern.  (albeit, it would seem unlikely to only have that one access pattern form this multi-faceted related data)",
          "score": 10,
          "created_utc": "2026-02-20 17:06:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gei9r",
              "author": "tejovanthn",
              "text": "Both of these are fair pushback.\n\nOn the analytics point - you're right, and I should have been clearer. That item belongs in a \"DynamoDB is the wrong choice\" list, not a \"single-table is the wrong choice\" list. The underlying problem is using DynamoDB for OLAP workloads at all. The number of tables doesn't change that. I'll fix the framing.\n\nOn the access pattern count - also a fair correction. The \"6 access patterns\" heuristic is too blunt. What I was trying to capture is the case where your data relationships are simple enough that the cognitive overhead of single-table design isn't justified - but you're right that the real signal isn't the count, it's whether your access patterns require traversing relationships within a partition. One access pattern that pulls a full aggregate by partition key is a completely legitimate use of single-table design. I'll reframe that item around relationship complexity rather than access pattern count.",
              "score": 2,
              "created_utc": "2026-02-20 17:20:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6gl5cl",
                  "author": "menge101",
                  "text": "I'm not really pushing back, just discussing the ideas, I think your write up is fine for people who don't really know single table design and/or DynamoDB.\n\nLike you can't be all-use-case encompassingly correct on these things when you are talking in sort of abstract generalizations.",
                  "score": 2,
                  "created_utc": "2026-02-20 17:51:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6dssuw",
          "author": "mehneni",
          "text": "It was a disaster for us as well. Causing 6 digit monthly costs at some point. I guess a problem is always: You have to know what you are doing.\n\nReducing the number of tables compared to a relational design makes sense. But I'd almost limit the single table to something like one aggregate in DDD at most: [https://martinfowler.com/bliki/DDD\\_Aggregate.html](https://martinfowler.com/bliki/DDD_Aggregate.html)\n\nJust putting unrelated data in a complex data model with very different quantities in a single table is a disaster. In our case it was an event store, a production database and a (complex) configuration store. Mixing all of this is an absolutely horrible idea, because the different areas have vastly different requirements and change rates.\n\nScanning the table becomes impossible. This might not matter for day-to-day operations, but if every migration takes a week and is hugely expensive that is a problem and you become very inflexible.\n\nAnd always put an expiry on the data. Because deleting data by scanning the table is just so expensive that is makes more sense to create a new table and drop the old one (which is a stressful thing to do if your backup will take ages to reconstruct the table ;).\n\nThe risk in having more tables is also smaller. In that case you can only mess up one area of the data. So only do single table if you know exactly what you are doing. Merging multiple tables into one is easier than the other way around.",
          "score": 13,
          "created_utc": "2026-02-20 07:09:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ejxn5",
              "author": "tybit",
              "text": "Great point on the DDD aggregate. This is roughly how I’ve viewed it for a while but not had the right term to describe it.\n\nI’ve generally thought of it as only use single table design if you have a well bounded micro-service.",
              "score": 3,
              "created_utc": "2026-02-20 11:19:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6k5d92",
                  "author": "tejovanthn",
                  "text": "\"Well-bounded microservice\" is actually a cleaner way to say it for most developers than \"DDD aggregate\" - same idea, more accessible. The service boundary and the table boundary should roughly align.",
                  "score": 1,
                  "created_utc": "2026-02-21 06:02:26",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6dvg3e",
              "author": "tejovanthn",
              "text": "Six-digit monthly costs is the nightmare scenario - and the root cause you're describing is exactly what I called out in the post: mixing data with vastly different access patterns, quantities, and change rates into one table. An event store and a production database have almost nothing in common operationally. That's not a single-table design problem, that's a data modeling problem that single-table made worse.\n\nThe DDD aggregate framing is actually the clearest heuristic I've heard for where the boundary should be. One aggregate (or tightly related aggregates that are always queried together) per table makes the access patterns predictable and keeps migrations scoped. The moment you're mixing things that evolve independently - your config store vs. your event store - you've lost the main benefit and kept all the costs.\n\nThe TTL point is underrated. Designing for data expiry from day one changes how you think about the whole model. \"How does this data leave the table?\" should be in the access patterns list from the start.\n\nGoing to add the aggregate boundary heuristic to the post - it's a more actionable framing than what I had.",
              "score": 2,
              "created_utc": "2026-02-20 07:33:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ju084",
          "author": "finitepie",
          "text": "You can not talk about DynamoDB vs relational DB without talking about scaling. Scalability is the whole point behind DynamoDB. Everything else is a trade-off to make it scalable. I like DDB. I like it a lot. It's not that one super dooper database to rule them all. It serves a well defined purpose. And sometimes it's just not the right tool for the job. If you understand that in advance, understand how to make the tool work for you and not against you, then DDB is your friend. And it might be 'schema-less', but I want to see you change relational schemas back and forth and not complain about the consequences :D. There is always a schema, a logic, a model, just that it is not explicitly defined and enforced at the DB level like it would be with a relational db. And schema definition for a relational db or data in general can be a lot of work.  It requires a lot of discipline to work it out and to keep it consistent in the long run. Having a good model/schema of your data, is to understand the data. And once you get there, you can also understand the access patterns better. So for me, modelling the data is always the very first step. Every mistake you make in that process will always result in plenty of pain and work. For DDB even more so. ",
          "score": 2,
          "created_utc": "2026-02-21 04:33:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k588j",
              "author": "tejovanthn",
              "text": "Hard agree on the sequencing - model the data first, derive access patterns from that, then design the table. The \"schema-less\" label does a lot of damage because it implies you can skip that step. You can't. The schema just lives in your application code instead of the database, which means when it breaks, it breaks silently.\n\nThe scalability point is the crux of it. People reach for DynamoDB because of the scaling promise, then design it like a relational database and get the worst of both worlds - none of the query flexibility, none of the operational simplicity.",
              "score": 1,
              "created_utc": "2026-02-21 06:01:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gvyvw",
          "author": "AftyOfTheUK",
          "text": ">Last week's multi-tenant post hit #1 here\n\nLink?",
          "score": 1,
          "created_utc": "2026-02-20 18:40:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6junn3",
              "author": "tejovanthn",
              "text": "Post - https://www.reddit.com/r/aws/s/q8V8p4A9ab\nBlog -https://singletable.dev/blog/pattern-saas-multi-tenant\n\nI would love your thoughts too :)",
              "score": 2,
              "created_utc": "2026-02-21 04:37:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6m265x",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-21 15:23:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rgrh3",
              "author": "tejovanthn",
              "text": "Interesting take - I'd actually argue the opposite. Multi-attribute composite keys for GSIs eliminate one of the biggest pain points of single-table design: manually concatenating synthetic keys like \\`gsi1pk = TENANT#<id>\\` and parsing them back out on reads.\n\nWith multi-attribute GSIs, you define up to 4 natural attributes per key and DynamoDB handles the composition. That makes single-table patterns cleaner to implement, not harder. The overloaded GSI section in this article is exactly the kind of thing that gets simpler with this feature.\n\nThat said, I haven't updated the article to use multi-attribute keys yet - it's on my list. Curious what specifically you think it makes worse?",
              "score": 1,
              "created_utc": "2026-02-22 12:41:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dnode",
          "author": "galnar",
          "text": "Thanks for sharing, this is really thought provoking. Would you mind sharing your job title? I’m wondering what role gets this deep in the weeds. Do you have the same level of experience with RDS and/or DocumentDB?",
          "score": -1,
          "created_utc": "2026-02-20 06:23:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6drp4t",
              "author": "tejovanthn",
              "text": "Thank you :) I used to lead engineering teams building on AWS for the better half of the last decade. Decided to step back and focus on some passion projects. :)   \n  \nCurrently I'm an indie developer. Day job is building [rasika.life](https://rasika.life) (a Carnatic music platform) where DynamoDB is the primary datastore — so this isn't theoretical for me, it's the system I'm maintaining.\n\nOn RDS: yes, reasonably deep. I've built on Postgres for years and it's my default for anything with complex reporting, evolving schemas, or strong relational requirements — which is basically why it made the \"when to avoid single-table\" list. DocumentDB I've touched but wouldn't claim expertise.\n\nThe honest answer is that I get deep in DynamoDB specifically because I made the choice to build on it and had to live with the consequences. Nothing sharpens your opinion on a tool like owning it in production.",
              "score": 6,
              "created_utc": "2026-02-20 06:59:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdff6p",
      "title": "Price increase at AWS?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rdff6p/price_increase_at_aws/",
      "author": "servermeta_net",
      "created_utc": "2026-02-24 12:52:15",
      "score": 25,
      "num_comments": 16,
      "upvote_ratio": 0.8,
      "text": "Recently many non hyperscaler providers I use (Hetzner, OVH) increased their prices due to the supply issues we all know. Do you think AWS and other hyperscalers will follow through, or will they shield their customers from the hardware market fluctuations? ",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rdff6p/price_increase_at_aws/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o74p0sd",
          "author": "PaintDrinkingPete",
          "text": "AWS likely has an advantage of already having a massive infrastructure and having priority with manufacturers...and already charging more for their services than many of the smaller budget VPS and cloud computing services providers operating on tighter margins, so they don't have to be as reactionary to the market in cases like this. \n\neventually though, if things keep trending as they are... yes, prices will have to go up.",
          "score": 45,
          "created_utc": "2026-02-24 13:11:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74uxv9",
          "author": "criminalsunrise",
          "text": "AWS tend not to do price rises - I believe it's one of their core principles. What they will do is have new service levels (instance types) that are more expensive and slowly sunset the old ones. We may see that accelerated a bit with the supply crunch.",
          "score": 40,
          "created_utc": "2026-02-24 13:44:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o750bw8",
              "author": "RickySpanishLives",
              "text": "Highly likely. The systems already in the field are just being amortized out. No real point increasing their price because they are already there. New instance types will incur an increased cost since they will cost more and have a BOM with more expensive parts.",
              "score": 11,
              "created_utc": "2026-02-24 14:13:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74mnrx",
          "author": "Old_Cry1308",
          "text": "aws will probably absorb some of it short-term. long-term, they'll pass it on. they’re not running a charity.",
          "score": 39,
          "created_utc": "2026-02-24 12:56:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74nwec",
              "author": "moduspol",
              "text": "They might have bigger longer term deals on hardware like this and haven’t had to eat any higher costs yet.",
              "score": 14,
              "created_utc": "2026-02-24 13:04:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7712h9",
          "author": "FlatCondition6222",
          "text": "Less spot capacity, for example, is also a \"way\" to raise prices.",
          "score": 5,
          "created_utc": "2026-02-24 19:48:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74pe0u",
          "author": "Negative-Cook-5958",
          "text": "There is already about a 5% increase with each EC2 generation change (r6i => r7i => r8i for example), they won't increase pricing for existing SKUs, just make the newer ones expensive when they are available.",
          "score": 11,
          "created_utc": "2026-02-24 13:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74vbkc",
              "author": "davewritescode",
              "text": "The thing is you should be using less compute as new generations usually perform better",
              "score": 10,
              "created_utc": "2026-02-24 13:46:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o750u03",
                  "author": "RickySpanishLives",
                  "text": "It's really the instance shape that determines if that is possible. Sometimes the compute is indeed a better performer but the instance shape gives you more than you need at an increased price requiring you to binpack your computer to cover it. Less a concern if you breathe EKS/ECS - but an issue if you use raw EC2.",
                  "score": 2,
                  "created_utc": "2026-02-24 14:16:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74ps5y",
          "author": "JonnyBravoII",
          "text": "Yes. Many of their prices have been the same for years (EBS, lambda, data transfer) and are obviously cash cows. On the EC2 front, prices started going up after the 6 series.  The t series, while wildly popular, are clearly being sunset as the t4g is 6 years.old at this point. I would expect storage costs will be the first to go up.",
          "score": 4,
          "created_utc": "2026-02-24 13:15:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74u5uo",
          "author": "d70",
          "text": "Who is keeping track? https://aws.amazon.com/blogs/aws/category/price-reduction/\nThat said, I agree that some new instance types are slightly more expensive than older ones but they are different products, no?",
          "score": 3,
          "created_utc": "2026-02-24 13:40:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78ssws",
          "author": "Complex86",
          "text": "I think AWS will pass this on in future generations of instance family or other services they may provide down the line.",
          "score": 2,
          "created_utc": "2026-02-25 01:05:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74yw4o",
          "author": "Shington501",
          "text": "Everything is going up",
          "score": 1,
          "created_utc": "2026-02-24 14:06:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbw463",
      "title": "Podcast?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rbw463/podcast/",
      "author": "putneyj",
      "created_utc": "2026-02-22 20:10:12",
      "score": 18,
      "num_comments": 10,
      "upvote_ratio": 0.95,
      "text": "So, is the official AWS podcast no longer doing news? Anyone else that used it to get \\*most\\* of your news about new services? I’m honestly a little bummed, but it just feels like the way things are going at AWS.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rbw463/podcast/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6tzvdo",
          "author": "signsots",
          "text": "The latest episode aligns with the recent layoff announcements. Wouldn't be surprised if it was part of it unfortunately.",
          "score": 20,
          "created_utc": "2026-02-22 20:18:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u20m0",
          "author": "coyotefarmer",
          "text": "That was a big part of how I kept up with changes. Such a shame. I don't see how ending something like that can be beneficial in the long run. I listened to it for years.",
          "score": 6,
          "created_utc": "2026-02-22 20:29:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71w9ms",
          "author": "ArchangelAdrian",
          "text": "Well the host Simon Elisha announced on his LinkedIn 2 weeks ago that his time at AWS came to an end.",
          "score": 4,
          "created_utc": "2026-02-24 00:41:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71xjc9",
              "author": "putneyj",
              "text": "Damn, yeah I would imagine that’s pretty much the end then. Not even a regular conversation with Werner can save your job.",
              "score": 3,
              "created_utc": "2026-02-24 00:48:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o740qdd",
                  "author": "ArchangelAdrian",
                  "text": "Sad if you think about it.",
                  "score": 2,
                  "created_utc": "2026-02-24 10:02:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6vz1y4",
          "author": "zenmaster24",
          "text": "Are there community podcasts that do similar?",
          "score": 3,
          "created_utc": "2026-02-23 02:53:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xz12c",
              "author": "wreckuiem48",
              "text": "A cloud podcast I like is called [https://www.thecloudpod.net/](https://www.thecloudpod.net/)",
              "score": 3,
              "created_utc": "2026-02-23 12:51:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6w1zg5",
              "author": "putneyj",
              "text": "I would also like to know this",
              "score": 1,
              "created_utc": "2026-02-23 03:11:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y8fb4",
          "author": "Specific-Art-9149",
          "text": "I built a morning brief email with Claude that scans various AWS RSS feeds along with 3rd party RSS and Reddit for AWS news.   Costs me about 7 cents a day utilizing Claude API, and it is emailed daily.  I'm an SA not a developer, so pretty happy with how it turned out.",
          "score": 2,
          "created_utc": "2026-02-23 13:49:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xz8n2",
          "author": "bot403",
          "text": "Google gemeni can turn anything into a podcast episode. Use it on the main aws RSS feed :D",
          "score": -4,
          "created_utc": "2026-02-23 12:52:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd7p4v",
      "title": "Cloudfront + HTTP Rest API Gateway",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rd7p4v/cloudfront_http_rest_api_gateway/",
      "author": "Alive_Opportunity_14",
      "created_utc": "2026-02-24 05:31:50",
      "score": 14,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "Cloudfront has introduced flat rate pricing with WAF and DDos protection included. I am thinking of adding cloudfront in front of my rest api gateway for benefits mentioned above. Does it make sense from an infra design perspective?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rd7p4v/cloudfront_http_rest_api_gateway/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7372db",
          "author": "Old_Cry1308",
          "text": "makes sense if you need the protection and pricing works for you, otherwise might be overkill.",
          "score": 5,
          "created_utc": "2026-02-24 05:34:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73dx94",
              "author": "Alive_Opportunity_14",
              "text": "From a pricing perspective it seems cheaper because of the flat pricing and added benefits. WAF on rest api costs money while its included in the flat pricing",
              "score": 1,
              "created_utc": "2026-02-24 06:30:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73p4sr",
          "author": "snorberhuis",
          "text": "A WAF is a layer of defense I would generally recommend for most companies. It can help you protect against automated attacks. There are very few exceptions to this recommendation.  ",
          "score": 4,
          "created_utc": "2026-02-24 08:11:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74z0gx",
          "author": "menge101",
          "text": "[Docs](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/flat-rate-pricing-plan.html) for anyone else that needs them\n\n[Pricing sheet](https://aws.amazon.com/cloudfront/pricing/) as well\n\nThere is a free tier as well as a pro tier at $15/month that seems fairly compelling.",
          "score": 3,
          "created_utc": "2026-02-24 14:06:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74fiyc",
          "author": "KayeYess",
          "text": "While AWS WAF2 can be attached directly to Amazon API Gateway, Cloudfront gives additional benefits such as distributed edge delivery, ability to use multiple origins (such as S3 for static content), caching, etc.",
          "score": 1,
          "created_utc": "2026-02-24 12:07:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7drkhi",
              "author": "vppencilsharpening",
              "text": "I'd also add that it leverages the AWS managed backbone for transport from the Edge to the Origin. So if your application is running in a single region you get AWS's team ensuring fast connections from the CloudFront edge to your application instead of relying on the public internet. \n\nIt's not going to make a huge difference, but it's not nothing. \n\nClient -> Public Internet (short distance) -> AWS CloudFront Edge (closest to the client) -> AWS Network (for most of the distance) -> Origin Application\n\nVS\n\nClient -> Public Internet (long distance) -> AWS Network (for a very short distance) -> Origin Application",
              "score": 1,
              "created_utc": "2026-02-25 19:34:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dx975",
                  "author": "KayeYess",
                  "text": "Yes ... that's a general benefit of a CDN. Client reaches CDN edge, and CDN handles the rest.",
                  "score": 1,
                  "created_utc": "2026-02-25 20:00:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o797flg",
          "author": "SilentPugz",
          "text": "Harden your security response header and content security policy for your cloudfront.  \n\nLambda edge for quick validations. Cloudfront managed functions makes some things simple \n\nDon’t forget your tls flow. Where you want to terminate. At the cloudfront , lessen the load on the api.",
          "score": 1,
          "created_utc": "2026-02-25 02:28:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f6tjp",
          "author": "TheDearlyt",
          "text": "The main tradeoff is added complexity so it’s worth it mostly when you actually plan to use WAF rules, caching, or global performance improvements, not just stack services for the sake of it.\n\nPersonally, I ended up using Gcore for a similar setup because I wanted CDN + edge protection in front of APIs without dealing with too much AWS configuration overhead. It felt simpler to manage while still giving the edge security and performance benefits.",
          "score": 1,
          "created_utc": "2026-02-25 23:43:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rc92pf",
      "title": "CDK + CodePipeline: How do you handle existing resources when re-deploying a stack?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rc92pf/cdk_codepipeline_how_do_you_handle_existing/",
      "author": "Hungry_Assistant6753",
      "created_utc": "2026-02-23 05:36:51",
      "score": 10,
      "num_comments": 8,
      "upvote_ratio": 0.86,
      "text": "We have an AWS CDK app deployed via CodePipeline. Our stack manages DynamoDB tables, Lambda functions, S3 buckets, and SageMaker endpoints.\n\n\n\n**Background**: Early on we had to delete and re-create our CloudFormation stack a few times due to deployment issues (misconfigured IAM, bad config, etc). We intentionally kept our DynamoDB tables and S3 buckets alive by setting RemovalPolicy.RETAIN. we didn't want to lose production data just because we needed to nuke the stack.\n\n**The problem**: When we re-deploy the stack after deleting it, CloudFormation tries to CREATE the tables again but they already exist. It fails. So we added a context flag `--context import-existing-tables=true` to our cdk synth command in CodePipeline, which switches the table definitions from new dynamodb.Table(...) to dynamodb.Table.from\\_table\\_name(...). This works fine for existing tables.\n\nNow, we added a new DynamoDB table. It doesn't exist yet anywhere. But the pipeline always passes `--context import-existing-tables=true`, so CDK tries to import a table that doesn't exist yet it just creates a reference to a non-existent table. No error, no table created.\n\n**Current workaround**: We special-cased the new table to always create it regardless of the flag, and leave the old tables under the import flag. But this feels fragile every time we add a new table we have to remember to handle this manually.\n\n**The question**: How do you handle this pattern cleanly in CDK? **Is there an established pattern for \"create if not exists, import if exists\"** that works in a fully automated",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rc92pf/cdk_codepipeline_how_do_you_handle_existing/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6wmzp0",
          "author": "deviled-tux",
          "text": "you leave the code alone and have it create the stuff \n\ncdk import to import them \n\nThis does not seem like something you should be automating because this shouldn’t be happening that often \n",
          "score": 16,
          "created_utc": "2026-02-23 05:41:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6won7x",
          "author": "cachemonet0x0cf6619",
          "text": "1. split stacks by the volatility of the resources. think stateful and stateless. \n\n2. use multiple accounts to split your resources so you’re not developing against production resources. \n\n3. if you can’t split accounts add an environment to the stack name and use that to separate dev and prod. \n\n4. don’t do that auto context thing. that’s a code smell. you shouldn’t need that with the advice above. \n\n5. bonus: i would never use code pipeline. instead i’d use github actions using a self hosted runner in my aws account. way easier and that skill is transferable.",
          "score": 11,
          "created_utc": "2026-02-23 05:54:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wspth",
          "author": "DoINeedChains",
          "text": "The joys of having CDK requirements forced upon you in a heavy singleton datasource environment",
          "score": 2,
          "created_utc": "2026-02-23 06:29:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wpxp1",
          "author": "Conscious-Title-226",
          "text": "This is why I don’t like CDK to be honest. It ties you into CloudFormation which is just painful when things like this go wrong.\n\nCF just fundamentally doesn’t provide enough support around state management. You need to engineer around its limitations and/or have immutable infrastructure.\n\nWould you be able to rename your stack resource (so it is a new CF stack with a new s3 and dynamodb resource) and then just migrate the data? That’d be faster if it’s not a lot to migrate/copy\n\nYou might need to modify your stack to allow you to give them new names, there’ll be uniquely named resources in your stack like KMS aliases, s3 bucket, iam role etc but this is a good reason to make sure all that is configureable through your stack props",
          "score": 5,
          "created_utc": "2026-02-23 06:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wmtts",
          "author": "pausethelogic",
          "text": "I’m sure there’s a way, but this is honestly one of the major downsides of cloudformation and CDK - there’s no proper state management\n\nIt’s part of why terraform is a much more popular IaC tool, it actually has state management for resources so it knows what resources were deleted, which still exist, and how to handle them",
          "score": 3,
          "created_utc": "2026-02-23 05:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wq4jx",
              "author": "cachemonet0x0cf6619",
              "text": "“can you fix my Honda’s transmission?”\n\n“nah, mate. but if you’d have gotten a bike you wouldn’t need a new transmission”\n\nthat’s how stupid your reply sounds",
              "score": -8,
              "created_utc": "2026-02-23 06:07:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7b0va0",
          "author": "iamtheconundrum",
          "text": "Naming resources explicitly will result in naming collisions. Also, you should put resources like dynamodb tables in a separate stack so you can nuke other stacks without affecting the persistent parts of your architecture.",
          "score": 1,
          "created_utc": "2026-02-25 10:55:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hapz4",
          "author": "International_Body44",
          "text": "Put your dynamodb in a seperate stack, you can keep it in the cdk app, just define a dynamo stack.\n\nThen when you do your delete, specify the other stacks for your delete, 'cdk destroy <stack_1> <stack_2>'\n\nThis leaves the dynamodb alone. When you want to add a new table or data you should be able to add that to just the dynamodb stack and deploy that 'cdk deploy <dynamo_stack>'\n\n\nItll look a bit like this:\n\nLib/stacks/\n\n        |- dynamodb.ts\n\n        |- otherResources.ts\n\nThen in you bin/app.ts file call the dynamodb stack after your resources stack.\n\nIf you need to pass variables from one stack to the other use parameter store, do not directely pass vars between stacks cause it will cause issues later on.",
          "score": 1,
          "created_utc": "2026-02-26 08:02:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbfv2w",
      "title": "Cloudwatch alarms mute rules",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rbfv2w/cloudwatch_alarms_mute_rules/",
      "author": "becharaerizk",
      "created_utc": "2026-02-22 07:47:17",
      "score": 8,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "Hello,\n\nI wanted to implement some kind of maintenance mode on alarms i have setup on my work's awa account. Right before i started i saw WA released alarms mute rules which do exactly what i want.\n\nIt works well using the console but i want to write a function that takes a specific string and created mute rules with all alarms containing this string. (For automated workflows and such)\n\nI noticed that neither the cli nor the python sdk support this yet, when are mute rules supposed to be released for cli or boto3 in python?\n\n  \nFeature I am speaking about: [Configure alarm mute rules](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/alarm-mute-rules-configure.html)\n\n  \nChecking [boto3's latest documentation](https://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch.html): no mention of this feature",
      "is_original_content": false,
      "link_flair_text": "monitoring",
      "permalink": "https://reddit.com/r/aws/comments/1rbfv2w/cloudwatch_alarms_mute_rules/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6qofga",
          "author": "Refwah",
          "text": "\nhttps://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch/client/describe_alarms.html\n\nhttps://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch/client/disable_alarm_actions.html",
          "score": 1,
          "created_utc": "2026-02-22 08:20:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qp3he",
              "author": "becharaerizk",
              "text": "Im not talking about disable alarm actions, this doesnt help my case im speaking about [Configure alarm mute rules](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/alarm-mute-rules-configure.html)",
              "score": 1,
              "created_utc": "2026-02-22 08:26:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6qq2pd",
                  "author": "Refwah",
                  "text": "If you set the state:\n\nhttps://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch/client/set_alarm_state.html\n\nAnd then immediately disable actions \n\nYou effectively get a mute\n\nYou then re-enable actions and you un mute the alarm\n\nYou may want to set them back to not alarming to in order to trigger the action again",
                  "score": 1,
                  "created_utc": "2026-02-22 08:35:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6r9jz0",
          "author": "sandro-_",
          "text": "Isn't that the API: https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_PutAlarmMuteRule.html\n\nTo effectively create a mute rule for a specific time?\n\nAnd in MuteTargets you can target which alarms to mute?",
          "score": 1,
          "created_utc": "2026-02-22 11:41:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rcdfw",
              "author": "becharaerizk",
              "text": "Yes the cli commands provided dont work, they return an error when using then, even when using cloudshell",
              "score": 1,
              "created_utc": "2026-02-22 12:06:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rj9vh",
                  "author": "KayeYess",
                  "text": "Make sure one of the associated IAM policies for the user/role has the required permissions for cloudwatch:PutAlarmMuteRule",
                  "score": 1,
                  "created_utc": "2026-02-22 12:59:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rwdon",
                  "author": "Flakmaster92",
                  "text": "Make sure you’re also running the absolute latest version of the CLI, too many times I’ve seen people saying “it didn’t work” when they’re running a CLI version from before the feature came out",
                  "score": 1,
                  "created_utc": "2026-02-22 14:20:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6sitd6",
          "author": "ruibranco",
          "text": "The native mute rules feature is console-only for now — SDK/CLI support is usually added a few months after console launch. In the meantime, the disable\\_alarm\\_actions + set\\_alarm\\_state approach works but is fragile for automated workflows. Another option worth considering: EventBridge Scheduler to run a Lambda that toggles alarm actions on/off around your maintenance windows, which is more auditable and easier to manage at scale than per-alarm state manipulation.",
          "score": 1,
          "created_utc": "2026-02-22 16:09:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6txbhe",
              "author": "crh23",
              "text": "Months would be surprising - I'd expect days",
              "score": 1,
              "created_utc": "2026-02-22 20:05:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6syb93",
              "author": "becharaerizk",
              "text": "A few months for a simple api call from python or the cli? Any programmer could finish that in a week without the use of ai lol",
              "score": 0,
              "created_utc": "2026-02-22 17:19:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6z9xla",
          "author": "liverdust429",
          "text": "The API exists (PutAlarmMuteRule) but SDK/CLI support always lags behind console launches. Check your CloudShell CLI version with `aws --version`, it's probably outdated. If it's still not there on the latest version you can always hit the API directly with SigV4 signing until boto3 catches up. Annoying but it works.",
          "score": 1,
          "created_utc": "2026-02-23 16:56:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7988j4",
          "author": "crh23",
          "text": "It has now released in [the CLI](https://github.com/aws/aws-cli/blob/v2/CHANGELOG.rst) and [boto3](https://github.com/boto/boto3/blob/develop/CHANGELOG.rst)",
          "score": 1,
          "created_utc": "2026-02-25 02:33:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79mfbr",
              "author": "becharaerizk",
              "text": "Finally lol",
              "score": 2,
              "created_utc": "2026-02-25 03:55:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9r4il",
      "title": "How to guarantee consistency when deleting items from dynamodb?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r9r4il/how_to_guarantee_consistency_when_deleting_items/",
      "author": "Select_Extenson",
      "created_utc": "2026-02-20 09:45:29",
      "score": 7,
      "num_comments": 26,
      "upvote_ratio": 1.0,
      "text": "Let's say I want to delete 100000 items from dynamodb, what is the best approach to delete \"all-or-nothing\", TransactWriteItems only support 100 items, so I don't want to cause inconsistency in my data if for some reason the delete function fails alongs the way.\n\nAnd in my case, I simply couldn't find a solution to implement it with GSI, so the only solution for me is to delete them manually.",
      "is_original_content": false,
      "link_flair_text": "database",
      "permalink": "https://reddit.com/r/aws/comments/1r9r4il/how_to_guarantee_consistency_when_deleting_items/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6e9dq3",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'database'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+database).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-20 09:45:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ee2vq",
          "author": "pint",
          "text": "this is a problem you should not have. the requirement itself screams badly that something is really wrong there, and you should seriously reconsider.\n\nwithout knowing more about the problem, here is a theoretical solution.\n\n1. add a new data field e.g. \"obsolete\" to the records, optionally add a ttl too\n1. modify the software to obey that field\n1. deploy the software, which means at an instant all the records are now \"gone\"\n1. let the ttl delete the records, or delete them manually at your convenience\n\nstep 2 is the most problematic, because the \"software\" might be a dozen different systems, and they might rely heavily on the assumption that queries will return rows in a timely manner, which is now not guaranteed.\n\nsuch operations have to be considered *in advance* with dynamodb.",
          "score": 12,
          "created_utc": "2026-02-20 10:28:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6eeyt9",
              "author": "Select_Extenson",
              "text": "I think my mistake is I shouldn't use dynamodb and use relational databases instead, the project I'm working on contains a lot of related data and I need to gunaratne consistency across them.\n\nIt was my first time using it, can you please tell me your opinion on this? is it actually a bad choice to use dynamodb when you have a project with a lot of related that or is it just me that I didn't design my database properly? but I don't think I did design it poorly, I tried my best to design in the most optimal way but it misses flexibility when it comes to querying and manipulating related data.",
              "score": 1,
              "created_utc": "2026-02-20 10:36:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6eg08o",
                  "author": "xtraman122",
                  "text": "There’s a way to do just about any pattern with Dynamo, but it often requires lots of careful planning around indexes and keys. What’s really rough with Dynamo is having  changes to the access patterns and relationships down the road. \n\nLots of people end up in the same situation as you, choosing a NoSQL DB because they think it’s cool, solves all their problems, or just heard about it too much in blog posts and conferences (Like your CTO probably did…). Unless you have very high throughout in both reads and writes that a traditional relational DB can’t handle, it’s likely not actually necessary for your use case.",
                  "score": 6,
                  "created_utc": "2026-02-20 10:45:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6eglzf",
                  "author": "RecordingForward2690",
                  "text": "Without knowing the details, but just going on what you say here, I agree that your problem screams \"Relational Database\" to me. DynamoDB is simply not designed or intended for your problem.\n\nFor all practical purposes, if you use DynamoDB for something like you describe, you will be writing a custom layer that tries to give you Relational Database functionality (multi-table queries, ACID compliance across multi-table operations and such) on top of DynamoDB. That has already been done, and it's called a Relational Database.\n\nDynamoDB shines when you just have a handful of tables, need extremely high performance at any scale, and are able to live with the loss of ACID compliance - or are willing to write additional code to get some of that ACID compliance back.",
                  "score": 5,
                  "created_utc": "2026-02-20 10:51:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6egq5d",
                  "author": "pint",
                  "text": "to be honest, relying rdbms referential integrity for such huge operations is also not recommended. it is bad design there too, even if at least possible.\n\ni advocate for separation of data. in the old days, we just dumped everything in \"the database\", because where else data would go, right? so different types of data ended up there, configuration, users and privileges, transactions, logs, web sessions, temporary data. all these data types have very different usage patterns, and probably shouldn't be in the same database.\n\none nice pattern is to keep operational data in dynamodb, and use dynamodb streams to deliver historic data to s3 or a rdbms for statistical analysis. meanwhile, keep configuration in ssm, user data maybe in whatever authentication tool you are using, logs in cloudwatch.\n\nrely more on program logic when aggregating data from different sources (as opposed to sql).\n\nwhen it comes to referential integrity, ask yourself the question: can we somehow get away without it? can be employ a little bit of cleverness or extra code to not have to deal with it?",
                  "score": 4,
                  "created_utc": "2026-02-20 10:52:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6eioux",
                  "author": "SonOfSofaman",
                  "text": "I think almost everyone goes through what you're going through. It's sort of a rite of passage with DynamoDB.\n\nDynamoDB is, as you know, very different from relational databases. With DynamoDB it is imperative that you fully understand all of your access patterns ahead of time, then model the table accordingly. If your access patterns change, then you may need to remodel your table.\n\nWith relational databases, you don't need to fully understand the access patterns ahead of time. It helps to know your access patterns ahead of time, but relational databases are flexible and adaptable.\n\nYou can do what you want with DynamoDB, but the table in its current form wasn't designed to support this \"bulk delete\" access pattern.\n\nI think you're at a point where you need to do a bit of redesign. Some of the other comments have practical solutions that may be helpful.",
                  "score": 1,
                  "created_utc": "2026-02-20 11:09:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6eg7sl",
          "author": "RecordingForward2690",
          "text": "I had a somewhat similar problem, where DynamoDB would collect millions of transactions, and at some point in time all transactions (within a Partition Key set) older than a particular timestamp needed to be deleted/invalidated/ignored as a whole. That timestamp was not known in advance - it depended on a user action - so I could not use the TTL mechanism. Like you noticed, there is no way to do that directly in a consistent manner.\n\nSome of the other proposals, where you update each item with a deleted=true or other attribute, suffer from the same problem: You need to either update or delete a large number of items in bulk, and the bulk operations in the DynamoDB are simply too limited in the number of items they can handle in one go, and in an atomic way.\n\nIn my case, my transactions fortunately were timestamped, and the transactions that needed to be deleted from the table were all done before a particular timestamp - the time of that user action. So I added an additional table \"DeleteMarkers\". As soon as the user event happened, I entered the timestamp of the Delete event into the DeleteMarkers, with the same partition key as the transaction table. Now, instead of doing one query to the TransactionsTable, I had to do two queries:\n\n1. Query to the DeleteMarkers table to get the up-to-date DeleteMarker\n2. Query to the TransactionTable to get the transactions, with the limitation that the results returned should be newer than the DeleteMarker.\n\nDynamoDB is quick enough that the additional query time did not impact latency.\n\nAfter this, I could delete the old transactions from the TransactionTable in the background. I used the TTL mechanism for that, but you can also do a query or even a scan if you want to. This is not time-critical or transaction-critical anymore.\n\nFrom a design point of view, DeleteMarkers has the SessionID as my Partition Key, no Sort Key (so when queried it returns one item only), and a field \"Timestamp\". TransactionTable has the SessionID as my Partition Key, and the Timestamp as the Sort Key.\n\nIn my application I did not need to do any other queries so I did not need any GSIs.\n\nFor your application, if you are able to formulate a SQL-query-like-thing that would be able to delete the \\~1M items, then you can also put the variables of that query in a similar DeleteMarkers table, and use those fields in your query to your TransactionTable. It's a bit more complex than the Timestamps I used, but not impossible.",
          "score": 3,
          "created_utc": "2026-02-20 10:47:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ebxoi",
          "author": "safeinitdotcom",
          "text": "DynamoDB just doesn't support this natively at that scale. You can either:\n\n\\- add a`deleted=true` attribute or smth like that, filter it in queries and clean up later.\n\n\\- BatchWriteItems but log which batches completed somewhere, on failure you resume instead of starting over.\n\nIf you need true all-or-nothing for 100k records, that's a relational DB problem. DynamoDB isn't designed for it.",
          "score": 3,
          "created_utc": "2026-02-20 10:08:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6echoq",
              "author": "Select_Extenson",
              "text": ">If you need true all-or-nothing for 100k records, that's a relational DB problem. DynamoDB isn't designed for it.\n\nYeah, that's the a mistake, I got told to use Dynamodb by our CTO, I had no experience with it so I didn't know the props and downs for it, after months of struggling trying to build our project that is mostly contains a lot of related data in dynamodb, it became really pain in the ass to maintain it",
              "score": 3,
              "created_utc": "2026-02-20 10:14:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6eykkr",
                  "author": "KainMassadin",
                  "text": "> I got told to use Dynamodb by our CTO\n\nBeen there, sucks.",
                  "score": 4,
                  "created_utc": "2026-02-20 13:03:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mqa1u",
                  "author": "csharpwarrior",
                  "text": "A lot of people just hear how cool DynamoDB is and they don’t spend enough time understanding the use cases it solves. And more importantly understanding the use cases it is bad at.",
                  "score": 1,
                  "created_utc": "2026-02-21 17:24:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6eb1vf",
          "author": "ebykka",
          "text": "This is one of the reasons why, after six years of using DynamoDB, we decided to give it up and migrate to RDS Aurora.\n\nWhile it was great for prototyping, maintenance, usage and consistency slowly became increasingly problematic.",
          "score": 1,
          "created_utc": "2026-02-20 10:00:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6elidc",
          "author": "SonOfSofaman",
          "text": "Are you able to ignore the unwanted items at read-time instead of deleting them? If those unwanted items have some common value upon which you can filter when you perform a read, to your consumer the items will be as good as deleted.\n\nThen you can casually delete the items in the background in batches or one by one at your leisure.",
          "score": 1,
          "created_utc": "2026-02-20 11:32:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f3con",
          "author": "garrettj100",
          "text": "You don’t want carpet, you want an area rug.  And when I say “carpet” and “area rug” I mean “DynamoDB” and “RDS”.\n\nYou’re asking how to do a transaction and that means a relational database.  That’s why they exist, because 40 years ago banks needed transactions.\n",
          "score": 1,
          "created_utc": "2026-02-20 13:31:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ft8jd",
              "author": "SpecialistMode3131",
              "text": "Maybe.  If you have 100 use cases for a table that are ideal for nosql (different schemas etc etc), and one use case requiring a transaction, do you instantly reject nosql?  Or do you hack around that one case?  Opinions and outcomes vary.\n\nOP didn't provide nearly enough business context to seriously decide one way or another.  \"The CTO told me to do it this way\" can be taken either way.",
              "score": 1,
              "created_utc": "2026-02-20 15:43:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fbk2e",
          "author": "solo964",
          "text": "How do you currently identify the items to be deleted? For example, are they collections of items where each item in a given collection has a common primary key? If you can identify them simply e.g. all items with a PK in the set { customer#12, customer#479, customer#90210 } then you could soft delete them by writing these PKs to control records e.g. an item with (PK = \"customer#12\", SK = \"control\", status = \"deleted\") and then modify the consumers of the table to first check if the given PK/control item was present with status=\"deleted\". Independently, have an async process that slowly deletes the actual items in batches, eventually deleting the control item.",
          "score": 1,
          "created_utc": "2026-02-20 14:15:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6l7rwi",
          "author": "GeorgeMaheiress",
          "text": "If the delete fails, retry until it succeeds. This doesn't have to be a problem.",
          "score": 1,
          "created_utc": "2026-02-21 12:07:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6oewe0",
          "author": "rexspook",
          "text": "I’m so curious about the use case where 100k records need to be deleted in an all or nothing approach. I think you’ve already got a lot of good answers here. My first thought is soft delete for that portion and then real deletes in some batched cleanup job",
          "score": 1,
          "created_utc": "2026-02-21 22:38:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fseud",
          "author": "SpecialistMode3131",
          "text": "Leaving aside the debate over RDBMS, one option:\n\n1. mark the rows in the main table (A) with a deleted\\_id value (a new attribute, added to all elements to be deleted)\n2. When you're 100% sure you have them all marked, you have achieved transactional consistency the Stone Age way.  Now have a small additional table B you atomically add a row to, saying \"rows in A with this deleted\\_id are to be treated as deleted\".\n3. Modify your code to filter those out/never use them as valid rows (check B and if a row has that attribute, filter it).  (do this step before adding the row to B, if you want a true transactional experience).\n4. Delete the rows from A at leisure.  Then remove the row from B.\n\nNot pretty, but it will easily achieve what you want, and you can keep it around as a management mechanism.",
          "score": 0,
          "created_utc": "2026-02-20 15:39:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e9dpe",
          "author": "AutoModerator",
          "text": "Here are a few handy links you can try:\n\n- https://aws.amazon.com/products/databases/\n- https://aws.amazon.com/rds/\n- https://aws.amazon.com/dynamodb/\n- https://aws.amazon.com/aurora/\n- https://aws.amazon.com/redshift/\n- https://aws.amazon.com/documentdb/\n- https://aws.amazon.com/neptune/\n\nTry [this search](https://www.reddit.com/r/aws/search?q=flair%3A'database'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+database).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": -1,
          "created_utc": "2026-02-20 09:45:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ralvuk",
      "title": "36hr+ Downtime - Response Required: Your Account is on Hold - Need Help",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1ralvuk/36hr_downtime_response_required_your_account_is/",
      "author": "Realistic-Lab6157",
      "created_utc": "2026-02-21 08:25:28",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 0.72,
      "text": "I received this 2 days ago, and I have seen other posts too about it.  \nThe pain is that I have all the right documents to get this verification done but the communication process is so slow and confusing which keeps delaying this and my services are still down.\n\nI have already created support tickets and cases but there has been no response for the last 12 hours. I am stuck here and need urgent help. u/AWSSupport\n\nI am not even sure if the verification team works on the weekends which might add 2 more days of downtime.\n\nDoes anyone have any idea on how to get this escalated or prioritized?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1ralvuk/36hr_downtime_response_required_your_account_is/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6kqbq3",
          "author": "Sirwired",
          "text": "Which support plan do you have?",
          "score": 4,
          "created_utc": "2026-02-21 09:21:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6l304i",
              "author": "Realistic-Lab6157",
              "text": "It’s basic. I was trying to upgrade it but it’s not allowing me.",
              "score": 2,
              "created_utc": "2026-02-21 11:24:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rb6tvu",
      "title": "Registering Partition Information to Glue Iceberg Tables",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rb6tvu/registering_partition_information_to_glue_iceberg/",
      "author": "mike_get_lean",
      "created_utc": "2026-02-22 00:07:23",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "I am creating Glue Iceberg tables using Spark on EMR. After creation, I also write a few records to the table. However, when I do this, Spark does not register any partition information in Glue table metadata.\n\nAs I understand, when we use hive, during writes, spark updates table metadata in Glue such as partition information by invoking UpdatePartition API. And therefore, when we write new partitions in Hive, we can get EventBridge notifications from Glue for events such as `BatchCreatePartition`. Also, when we invoke `GetPartitions`, we can get partition information from Glue Tables.\n\nI understand Iceberg works based on metadata and has a feature for hidden partitioning but I am not sure if this is the sole reason Spark is not registering metadata info with Glue table. This is causing various issues such as not being able to detect data changes in tables, not being able to run Glue Data Quality checks on selected partitions, etc.\n\nIs there a simple way I can get this partition change and update information directly from Glue?\n\nOne of the bad ways to do this will be to create S3 notifications, subscribe to those and then run Glue Crawler on those events, which will create another S3 based Glue table with the correct partition information. And then do DQ checks on this new table. I do not like this approach at all because I will need to setup significant automation to achieve this.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rb6tvu/registering_partition_information_to_glue_iceberg/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6qtnje",
          "author": "freerangetrousers",
          "text": "Glue is quite simply built with Hive in mind. Iceberg is more feature rich, but aws haven't built the whole glue ecosystem around it so there isn't feature parity. \nGlue catalogue is just Hive metastore , but iceberg isn't really designed to have to utilise this. \n\n\nInstead you can check for changes on the metadata.json file , or just at the end of your spark scripts emit an event yourself with the details you want to convey. Like in the olden days. ",
          "score": 1,
          "created_utc": "2026-02-22 09:10:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1raayhv",
      "title": "Help with cognito: Code security resource quotas not enforced?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1raayhv/help_with_cognito_code_security_resource_quotas/",
      "author": "TutorNeat2724",
      "created_utc": "2026-02-20 23:20:47",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 0.86,
      "text": "Hi everyone,\nI’ve noticed what seems to be unexpected behavior regarding Cognito User Pools code security resource quotas.\nAccording to the [documented limits](https://docs.aws.amazon.com/cognito/latest/developerguide/quotas.html#resource-quotas), certain operations (e.g. GetUserAttributeVerificationCode) should be rate-limited (for example, max 5 consecutive requests). However, in my tests, I’m able to call GetUserAttributeVerificationCode more than 5 times in a row without receiving any throttling error or limit exception.\nHas anyone experienced the same behavior?\nIs there any additional configuration required to enforce these quotas, or are they applied under specific conditions only?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1raayhv/help_with_cognito_code_security_resource_quotas/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6ipsm5",
          "author": "Skytram_",
          "text": "How long are you calling that API above the max TPS for?",
          "score": 1,
          "created_utc": "2026-02-21 00:16:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k1edw",
          "author": "baever",
          "text": "`GetUserAttributeVerificationCode` has a quota of 5 requests/user/hour. The only thing I can think of is that the bucket is based on hour of day. So if you start at 1:59 and make 4 requests and then at 2:01 you make 4 requests (for a total of 8) it won't block you because those span 2 hour buckets. However, if you make 6 requests in the same hour (say between 2:01 and 2:05) it should block you because it's in 1 hour bucket. Are you never seeing it enforced or are you seeing it intermittently enforced?",
          "score": 1,
          "created_utc": "2026-02-21 05:29:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rctlrt",
      "title": "How Does Karpenter Handle AMI Updates via SSM Parameters? (Triggering Rollouts, Refresh Timing, Best Practices)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rctlrt/how_does_karpenter_handle_ami_updates_via_ssm/",
      "author": "LemonPartyRequiem",
      "created_utc": "2026-02-23 20:52:45",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I’m trying to configure Karpenter so a `NodePool` uses an `EC2NodeClass` whose AMI is selected via an SSM Parameter that we manage ourselves.\n\nWhat I want to achieve is an automated (and controlled) AMI rollout process:\n\n* Use a Lambda (or another AWS service, if there’s a better fit) to periodically fetch the latest AWS-recommended EKS AMI (per the AWS docs: [https://docs.aws.amazon.com/eks/latest/userguide/retrieve-ami-id.html](https://docs.aws.amazon.com/eks/latest/userguide/retrieve-ami-id.html)).\n* Write that AMI ID into *our own* SSM Parameter Store path.\n* Update the parameter used by our **test** cluster first, let it run for \\~1 week, then update the parameter used by **prod**.\n* Have Karpenter automatically pick up the new AMI from Parameter Store and perform the node replacement/upgrade based on that change.\n\nWhere I’m getting stuck is understanding how `amiSelectorTerms` works when using the `ssmParameter` option (docs I’m referencing: [https://karpenter.sh/docs/concepts/nodeclasses/#specamiselectorterms](https://karpenter.sh/docs/concepts/nodeclasses/#specamiselectorterms)):\n\n* How exactly does Karpenter resolve the AMI from an `ssmParameter` selector term?\n* When does Karpenter re-check that parameter for changes (only at node launch time, periodically, or on some internal resync)?\n* Is there a way to force Karpenter to re-resolve the parameter on a schedule or on demand?\n* What key considerations or pitfalls should I be aware of when trying to implement AMI updates this way (e.g., rollout behavior, node recycling strategy, drift, disruption, caching)?\n\nThe long-term goal is to make AMI updates as simple as updating a single SSM parameter: update test first, validate for a week, then update prod letting Karpenter handle rolling the nodes automatically.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rctlrt/how_does_karpenter_handle_ami_updates_via_ssm/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o72r5s3",
          "author": "sunra",
          "text": "Your best bet is going to be to read the source.\n\nBut my understanding is that it is the Karpenter controller itself which monitors the SSM parameter (not the nodes themselves). When the controller notices that some nodes don't match the parameter it will mark the nodes as \"drifted\", and the replacements will happen according to your node-pool disruption-budget and node-termination-grace-period.\n\nI don't know this for sure - it's my expectation based on how Karpenter handles other changes (like k8s control-plane upgrades).",
          "score": 1,
          "created_utc": "2026-02-24 03:42:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73gyvq",
          "author": "yuriy_yarosh",
          "text": "1. You'll need to enable drift detection so it'll actually resync SSM   \n[https://karpenter.sh/docs/reference/settings/#feature-gates](https://karpenter.sh/docs/reference/settings/#feature-gates)\n\n2. SSM itself is throttled [https://github.com/aws/karpenter-provider-aws/issues/5907](https://github.com/aws/karpenter-provider-aws/issues/5907)  \nResync was 5 min before contributing to CNCF (reconcil cycle period for the whole controller), but now it's hardcoded to start checking only after an hour  \n[https://github.com/kubernetes-sigs/karpenter/blob/main/designs/drift.md](https://github.com/kubernetes-sigs/karpenter/blob/main/designs/drift.md)  \n[https://github.com/kubernetes-sigs/karpenter/blob/main/pkg/controllers/nodeclaim/disruption/drift.go#L93](https://github.com/kubernetes-sigs/karpenter/blob/main/pkg/controllers/nodeclaim/disruption/drift.go#L93)  \n[https://github.com/aws/karpenter-provider-aws/blob/main/pkg/cloudprovider/cloudprovider.go#L281](https://github.com/aws/karpenter-provider-aws/blob/main/pkg/cloudprovider/cloudprovider.go#L281)  \n\n\nSpot instances require SQS interruption queue `--interruption-queue`  \n[https://karpenter.sh/docs/concepts/disruption/#interruption](https://karpenter.sh/docs/concepts/disruption/#interruption)\n\n3. No, on-demand... \n\nYeah, been there, Karpenter is all over the place so wrote a custom cluster autoscaler with a Terraform provider and Kamaji, to keep infra state consistent, synchronized, and in one place.",
          "score": 1,
          "created_utc": "2026-02-24 06:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bbcgk",
          "author": "EcstaticJellyfish225",
          "text": "Consider using TAGs for the AMI selector, you should be able to tag AWS provided (and your own) AMIs.  Then you can pre-test an AMI in your dev account, once you are happy with it, you can tag the same AMI in your prod account and it will become available for karpenter to pick up next time a new node is needed (or if using drift detection at any time your disruption budget allows).\n\nAutomating the test cycle and tagging AMIs that pass the test, is also pretty straight forward. Test in a dev account, if the AMI asses the test, by some means tag the same AMI in your prod account. (Maybe setup an SNSTopic triggering a lambda, or something similar).",
          "score": 1,
          "created_utc": "2026-02-25 12:19:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rb4r7y",
      "title": "Initiating App Studio - Entity Already Exists Error",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rb4r7y/initiating_app_studio_entity_already_exists_error/",
      "author": "haonconstrictor",
      "created_utc": "2026-02-21 22:38:59",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Every time I try to setup App Studio on my new account I receive the “Entity Already Exists” error no matter what I do. I’ve confirmed a group exists in IAM and it’s not duplicated anywhere. Pulling my hair out trying to even get off the ground with AWS and can’t figure this out. I’ve deleted and rebuilt the IAM multiple times, and still nothing. Any advice is appreciated! ",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rb4r7y/initiating_app_studio_entity_already_exists_error/",
      "domain": "self.aws",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rckoel",
      "title": "CACs in Workspaces",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rckoel/cacs_in_workspaces/",
      "author": "KrazyMuffin",
      "created_utc": "2026-02-23 15:36:47",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Our current AWS workspace setup uses Simple AD, as I couldn't get AD Connector to work (will work on getting this working another time).\n\n\n\nCurrently a Linux workspace (Rocky Linux 8) can use CACs to authenticate to sites in-session, however, on Windows (Windows Server 2022), it doesn't recognize my computer's CAC reader. I have installed ActivID and InstallRoot, the workspace is DCV (formerly WSP).\n\n\n\nThe documentation all talks about how to setup readers with AD Connector so you can log into the workspace with your CAC, but that's not what we're trying to do, just be able to use the reader inside the instance.\n\n\n\nAny suggestions?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rckoel/cacs_in_workspaces/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6zxsei",
          "author": "fjleon",
          "text": "smart card redirection is disabled in windows by default for both presession and insession. you need to enable via GPO\n\nhttps://docs.aws.amazon.com/workspaces/latest/adminguide/group_policy.html#gp_install_template_wsp",
          "score": 2,
          "created_utc": "2026-02-23 18:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75hoqv",
              "author": "KrazyMuffin",
              "text": "Thank you, that worked, I'm dumb 😅",
              "score": 1,
              "created_utc": "2026-02-24 15:38:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rd84jb",
      "title": "Quantum-Guided Cluster Algorithms for Combinatorial Optimization",
      "subreddit": "aws",
      "url": "https://aws.amazon.com/blogs/quantum-computing/quantum-guided-cluster-algorithms-for-combinatorial-optimization/",
      "author": "donutloop",
      "created_utc": "2026-02-24 05:55:27",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1rd84jb/quantumguided_cluster_algorithms_for/",
      "domain": "aws.amazon.com",
      "is_self": false,
      "comments": [
        {
          "id": "o73zfzp",
          "author": "nucleustt",
          "text": "What in god's name is this? ELI5",
          "score": 2,
          "created_utc": "2026-02-24 09:50:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbz0rw",
      "title": "Track Karpenter efficiency of cluster bin-packing over time with kube-binpacking-exporter",
      "subreddit": "aws",
      "url": "https://github.com/sherifabdlnaby/kube-binpacking-exporter",
      "author": "SherifAbdelNaby",
      "created_utc": "2026-02-22 21:59:10",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "technical resource",
      "permalink": "https://reddit.com/r/aws/comments/1rbz0rw/track_karpenter_efficiency_of_cluster_binpacking/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1reb3q4",
      "title": "AWS Backup Jobs with VSS Errors",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1reb3q4/aws_backup_jobs_with_vss_errors/",
      "author": "Budget-Industry-3125",
      "created_utc": "2026-02-25 11:49:03",
      "score": 4,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Good morning guys,\n\n  \nI've set up AWS Backup Jobs for many of my EC2 Instances. \n\nThere are 20 VMs enabled for backing up their data to AWS, but somehow 9 of them are presenting the following errors:\n\nWindows VSS Backup Job Error encountered, trying for regular backup\n\nI have tried re-installing the backup agent in the vms and updating, but it doesn't seem to be working out. \n\nUpon connecting to the machines, I'm able to find some VSS providers in failed states. However, after restarting them and verifying that they are OK, the job fails again with the same error message.\n\n  \nHas anyone encountered this behaviour before?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1reb3q4/aws_backup_jobs_with_vss_errors/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7bn6tp",
          "author": "brile_86",
          "text": "  \nCheck the pre-reqs  \n[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/application-consistent-snapshots-prereqs.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/application-consistent-snapshots-prereqs.html)\n\nTLDR:\n\n* Windows Server 2016\n* .NET Framework version 4.6 or later\n* Windows PowerShell major version 3, 4, or 5 with language mode set to FullLanguage\n* AWS Tools for Windows PowerShell version [3.3.48.0](http://3.3.48.0) or later\n* IAM policy AWSEC2VssSnapshotPolicy (or equivalent permissions) attached (make sure you don't have any restrictive SCP or IAM Policy Boundaries blocking it)\n\n  \nAlso some instances are not supported as too small  \n[https://docs.aws.amazon.com/aws-backup/latest/devguide/windows-backups.html](https://docs.aws.amazon.com/aws-backup/latest/devguide/windows-backups.html)\n\n* t3.nano\n* t3.micro\n* t3a.nano\n* t3a.micro\n* t2.nano\n* t2.micro\n\n",
          "score": 3,
          "created_utc": "2026-02-25 13:33:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bjqdc",
          "author": "ReturnOfNogginboink",
          "text": "This is a Windows issue, not an AWS issue. The error is coming from the Windows volume snapshot service.",
          "score": 1,
          "created_utc": "2026-02-25 13:13:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cm51j",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-02-25 16:26:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7co5m1",
              "author": "gex80",
              "text": "Why would they contact Veeam support?",
              "score": 1,
              "created_utc": "2026-02-25 16:35:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rey0g0",
      "title": "Confused about how to set up a lambda in a private subnet that should receive events from SQS",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rey0g0/confused_about_how_to_set_up_a_lambda_in_a/",
      "author": "Slight_Scarcity321",
      "created_utc": "2026-02-26 02:31:11",
      "score": 4,
      "num_comments": 22,
      "upvote_ratio": 1.0,
      "text": "In CDK, I've set up a VPC with a public and private with egress subnets.  A private security group allows traffic from the same security group and HTTP traffic from the VPC's CIDR block. I have Postgres running in RDS Aurora in this VPC in the private security group.\n\nI have a lambda that lives in this private security group and is supposed to consume messages from an SQS queue and then write directly to the DB.  However, SQS queue messages aren't reaching the lambda.  I am getting some contradictory answers when I try to google how to do this, so I wanted to see what I need to do.\n\nThe SQS queue set up is very basic:\n\n```\nconst sourceQueue = new sqs.Queue(this, \"sourceQueue\");\n```\n\nThe lambda looks like this\n\n```\n        const myLambda = new NodejsFunction(\n            this,\n            \"myLambda\",\n            {\n                entry: \"path/to/index.js\", \n                handler: \"handler\", \n                runtime: lambda.Runtime.NODEJS_22_X, \n                vpc,\n                securityGroups: [privateSG],\n            },\n        );\n\n        myLambda.addEventSource(\n            new SqsEventSource(sourceQueue),\n        );\n\n        // policies to allow access to all sqs actions\n```\n\nIs it true that I need something like this?\n```\n        const vpcEndpoint = new ec2.InterfaceVpcEndpoint(this, \"VpcEndpoint\", {\n            service: ec2.InterfaceVpcEndpointAwsService.SQS,\n            vpc,\n            securityGroups: [privateSG],\n        });\n```\nWhile it allowed messages to reach my lambda, VPC endpoint are IaaS and I am not allowed to create them directly.  What I want is to prevent just anyone from being able to create a message but allow the lambda to receive queue messages and to communicate directly (i.e. write SQL to) the DB.  I am not sure that doing it with a VPC endpoint is correct from a security standpoint (and that would of course be grounds for denying my request to create one).  What's the right move here?\n\nEDIT:\n\nThe main thing here is that there is a lambda that needs to take in some json data, write it to a db.  There are actually two lambdas which do something similar.  The first lambda handles json for a data structure that has a one-to-many relationship with a second data structure.  The first one has to be processed before the second ones can be, but these messages may appear out of order.  I am also using a dead letter queue to reprocess things that failed the first time.  \n\nI am not married to using SQS and was surprised to learn that it's public.  I had thought that someone with our account credentials (i.e. a coworker) could just invoke aws cli to send messages as he generated them.  If there's a better mechanism to do this, I would appreciate the suggestion.  I would really like to have the action take place in the private subnet.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rey0g0/confused_about_how_to_set_up_a_lambda_in_a/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7g529b",
          "author": "aqyno",
          "text": "AWS is a public cloud: meaning most of its services, like S3, API Gateway, Lambda, DynamoDB, and SQS, are accessible over the public internet from anywhere.\n\nThen you’ve got VPC, which is the private side of things, used for resources like EC2 or RDS.\n\nWhen you place a Lambda inside a VPC, it basically moves from the public part into the private part, so it loses access to public services. The usual fix is adding a NAT Gateway or egress gateway so a Lambda in a private subnet can reach the internet or public AWS services. But honestly, that’s not ideal: it’s less secure, costs more, adds latency and bandwidth bottlenecks.\n\nThat’s where VPC endpoints come in. They let private resources talk to public-facing AWS services, but keep all the traffic within AWS’s own network.\n\nFor your specific use case, the only real options are a NAT Gateway, Egress Gateway, or VPC endpoint. That means you either need to set up that infrastructure (IaaS is a different thing) yourself or have it already in place.\n\nMy ideal setup would be a queue locked down with a resource policy that only allows access from a specific VPC endpoint and the Lambda’s IAM role, plus a security group that only permits traffic from the Lambda’s own security group.\n\nAnother option would be to refactor your function so it's not polling the queue in code, just let Lambda receive messages via triggers and consume the body from the event. You could still lock things down with resource policies, but keep in mind, a coworker with broad access could still override your restrictions. That’s why you want to layer in granular permissions.",
          "score": 9,
          "created_utc": "2026-02-26 02:55:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g9v4e",
              "author": "clintkev251",
              "text": "You do not need a VPC endpoint for a function to be triggered by SQS unless you also need to access the SQS service within your code",
              "score": 9,
              "created_utc": "2026-02-26 03:23:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7gaadd",
                  "author": "aqyno",
                  "text": "That's what OP asked. He’s polling the SQS, that's why VPC endpoint configuration fixed the lambda not receiving messages.",
                  "score": 2,
                  "created_utc": "2026-02-26 03:26:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7g7cso",
          "author": "clintkev251",
          "text": "You do not need any connectivity to SQS from your function in order to use an SQS event source. Just need IAM permission in your function's execution role",
          "score": 5,
          "created_utc": "2026-02-26 03:08:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g2b85",
          "author": "cachemonet0x0cf6619",
          "text": "what youre doing is fine but you might be thinking about it wrong. sqs is secured by iam permission so you don’t need that in a vpc. just don’t give iam permission to create messages on the queue.",
          "score": 3,
          "created_utc": "2026-02-26 02:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gjdwc",
              "author": "rolandofghent",
              "text": "He needs to be in the VPC to write to the Database.",
              "score": 0,
              "created_utc": "2026-02-26 04:23:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7gqt08",
                  "author": "cachemonet0x0cf6619",
                  "text": "the lambda does, yes. and i think you’re right. I’m making an assumption that the lambda is in a subnet that has either vpc private link or a nat gateway configured",
                  "score": 0,
                  "created_utc": "2026-02-26 05:15:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7g20tl",
          "author": "UltimateLmon",
          "text": "If you set up Queue with appropriate IAM policies to both the queue and encryption key, then you shouldn't have to worry about the VPC (as far as the queue is concerned).\n\n  \nIs what you want restrict access for someone pushing messages into the queue or triggering Lambda?\n\n  \nYou do want Lambda to be in the private subnet with appropriate security group if you trying to hit the database in the same subnet / connected subnet.",
          "score": 2,
          "created_utc": "2026-02-26 02:38:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g5w9m",
              "author": "aqyno",
              "text": "If your Lambda needs to hit both a database inside a VPC and a public service like SQS, it has to be in a private subnet. That’s really the only setup that works.",
              "score": 2,
              "created_utc": "2026-02-26 03:00:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7g7gfu",
                  "author": "UltimateLmon",
                  "text": "Exactly yeah. \n\n\nI'm more wondering what the OP meant by \"anyone being able to create a message\".\n\n\nIt would be question of locking down the IAM policies involved but hard to tell what the entry point the OP wants to deny.",
                  "score": 1,
                  "created_utc": "2026-02-26 03:09:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7gtsmv",
                  "author": "aplarsen",
                  "text": "Private VPC + an egress to the public internet",
                  "score": 1,
                  "created_utc": "2026-02-26 05:38:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7g2lwl",
          "author": "Prestigious_Pace2782",
          "text": "Yeah you need a vpc endpoint in there for sqs and need to allow https between the lambda and the endpoint.",
          "score": 1,
          "created_utc": "2026-02-26 02:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g7qjn",
              "author": "clintkev251",
              "text": "No you don't. The Lambda service polls SQS, not your function",
              "score": 2,
              "created_utc": "2026-02-26 03:11:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7h5i49",
                  "author": "Prestigious_Pace2782",
                  "text": "If you are in a private vpc with no Nat gateway an are calling sqs via an sdk in your code, my experience is that you need an endpoint. It’s a pattern I use a bit.",
                  "score": 1,
                  "created_utc": "2026-02-26 07:15:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7g2hsy",
          "author": "aplarsen",
          "text": "Why do you have it in a private subnet? What problem does that solve?",
          "score": 1,
          "created_utc": "2026-02-26 02:41:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g5ysr",
              "author": "aqyno",
              "text": "Connecting yo a private DB. Apparently.",
              "score": 2,
              "created_utc": "2026-02-26 03:00:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ra6gcu",
      "title": "Help — Can’t delete CloudFront distribution and I’m scared of a huge bill",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1ra6gcu/help_cant_delete_cloudfront_distribution_and_im/",
      "author": "SignalDrive3667",
      "created_utc": "2026-02-20 20:25:35",
      "score": 3,
      "num_comments": 7,
      "upvote_ratio": 0.67,
      "text": "I created a CloudFront distribution for learning purposes. I’m now trying to clean up my AWS account and delete all resources, but CloudFront won’t let me delete the distribution.\n\nError message:\n“Failed to delete distribution: You can't delete this distribution while it's subscribed to a pricing plan. After you cancel the pricing plan, you can delete the distribution at the end of the monthly billing cycle.”\n\n**The confusing part:** I selected the Free Tier flat-rate plan, so I don’t understand what pricing plan it’s referring to.\n\nWhy am I unable to delete it? Is there something else I need to disable or cancel first?\n\nI’m worried about unexpected charges and want to make sure everything is fully removed. Any guidance on how to safely delete the distribution would be appreciated.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1ra6gcu/help_cant_delete_cloudfront_distribution_and_im/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6hir8f",
          "author": "pausethelogic",
          "text": "Reach out to AWS account and billing support. They’ll be able to see what’s in your account and how to remove it so you’re not charged",
          "score": 2,
          "created_utc": "2026-02-20 20:28:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hjv0v",
          "author": "Opening-Concert826",
          "text": "See the cancel pricing plan section here: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/flat-rate-pricing-plan.html#manage-your-pricing-plans",
          "score": 2,
          "created_utc": "2026-02-20 20:34:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hrtdz",
          "author": "vppencilsharpening",
          "text": "The flat-rate plans are somewhat new, so I'd expect that there is not a lot of great information if your searching the internet.\n\nu/Opening-Concert826 linked the AWS doc that explains how to remove the pricing plan. However there is a chance that you still may need to wait until the end of the billing cycle for the \\[free\\] flat-rate plan to be removed.\n\nTo ensure that the distribution is NOT used (and not incurring costs), you should be able to disable it from the Distributions list in the console. Just check the box next to the distribution and click \\[Disable\\]. Do this in addition to removing the flat-rate plan. \n\nIf you created this to play around with and never publicly advertised the endpoint then it's unlikely you are going to get hit with much usage and it will all likely fall under the free tier.",
          "score": 2,
          "created_utc": "2026-02-20 21:13:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jxsy8",
          "author": "WhoseThatUsername",
          "text": "Pretty sure the 'Free Tier flat-rate plan' is considered a pricing plan, though unclear as to how you'd remove it.",
          "score": 1,
          "created_utc": "2026-02-21 05:01:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ke6fb",
          "author": "SilentPugz",
          "text": "Just a thought, to delete cloudfront you must perform the step to change the stage before the delete. Disabled must be in state.\n\nEdit : state*  # replace stage.",
          "score": 1,
          "created_utc": "2026-02-21 07:21:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6izc92",
          "author": "llima1987",
          "text": "In addition to the other answers, the safest way to kill your spending on AWS is to close the entire account. You can always create a new one.",
          "score": 1,
          "created_utc": "2026-02-21 01:12:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}