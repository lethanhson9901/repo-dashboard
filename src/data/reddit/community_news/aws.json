{
  "metadata": {
    "last_updated": "2026-02-26 03:06:08",
    "time_filter": "week",
    "subreddit": "aws",
    "total_items": 20,
    "total_comments": 128,
    "file_size_bytes": 147919
  },
  "items": [
    {
      "id": "1raski8",
      "title": "AWS Certificate Manager updates default certificate validity to comply with new guidelines",
      "subreddit": "aws",
      "url": "https://aws.amazon.com/about-aws/whats-new/2026/02/aws-certificate-manager-updates-default/",
      "author": "SudoAlex",
      "created_utc": "2026-02-21 14:32:40",
      "score": 48,
      "num_comments": 8,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "security",
      "permalink": "https://reddit.com/r/aws/comments/1raski8/aws_certificate_manager_updates_default/",
      "domain": "aws.amazon.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6lu1ai",
          "author": "SudoAlex",
          "text": "Posted this mostly due to the future uncertainty from their announcement back in June 2025, with exportable public certificates: https://www.reddit.com/r/aws/comments/1ldpw1d/aws_certificate_manager_introduces_public/ - as it was well known that certificate lifetimes would be limited in the long run.\n\nThey'll be reducing the duration, along with the price:\n\n> We have reduced the pricing for ACM’s exportable public certificates in line with the shorter validity period. 198-day exportable public certificate will now cost $7/Fully Qualified domain name (down from $15) and $79/ wildcard name (down from $149). Please refer to ACM’s pricing page for more details. For more information about ACM, visit the [ACM documentation](https://docs.aws.amazon.com/acm/latest/userguide/acm-certificate-characteristics.html).\n\n~~Although the wildcard pricing feels like a slightly sneaky $4.50 increase (half of $149 is $74.50)~~.\n\nUpdated numbers below.",
          "score": 9,
          "created_utc": "2026-02-21 14:38:53",
          "is_submitter": true,
          "replies": [
            {
              "id": "o6m80fe",
              "author": "Mutjny",
              "text": ">Although the wildcard pricing feels like a slightly sneaky $4.50 increase (half of $149 is $74.50).\n\n198 days is not half of 365.  The new price is exactly equivalent to the old price with the reduced number of days.  Its actually like $1.50 less.",
              "score": 6,
              "created_utc": "2026-02-21 15:53:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6mq2vo",
                  "author": "SudoAlex",
                  "text": "Certificates weren't valid for 365 days - it was 395 days. Got the calculations slightly off, but it's probably more of a price bump:\n\nBefore: 395 day certificate, renews 60 days before expiry, $149 per 335 days  \nAfter: 198 day certificate, renews 45 days before expiry, $79 per 153 days\n\nShould be closer to $68 for it to be the same price.",
                  "score": 2,
                  "created_utc": "2026-02-21 17:23:42",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m02lx",
          "author": "wlonkly",
          "text": "Ah thanks for this, I was wondering what their plan was (but not enough to ask our TAM I guess, heh).",
          "score": 2,
          "created_utc": "2026-02-21 15:12:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74hhmh",
          "author": "KayeYess",
          "text": "CA Browser forum voted to reduce public CA signed certificate validity to 200 days on March 26, 2026. It will reduce to 100 days in March, 2027 and 47 days in March 2029. Time to start preparing and automate. We automated certificate issuance and rotation over a decade ago (especially for external certs imported into ACM). Also, start adopting quantum safe ciphers. And mask unnecessary NPI/PII in all communications.",
          "score": 1,
          "created_utc": "2026-02-24 12:22:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rata1g",
      "title": "If S3 vectors offer sub second latency, why does AWS say it's designed for infrequent access?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rata1g/if_s3_vectors_offer_sub_second_latency_why_does/",
      "author": "nucleustt",
      "created_utc": "2026-02-21 15:03:03",
      "score": 37,
      "num_comments": 36,
      "upvote_ratio": 0.85,
      "text": "I'm building a customer service agent and need a vector DB for RAG.\n\nNaturally, I gravitated toward S3 vectors because the 90% cost reduction was super attractive.\n\nI'm wondering if I'm making the right choice (even though I see RAG as a use case).\n\nBasically, the chatbot has to answer questions via WhatsApp.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rata1g/if_s3_vectors_offer_sub_second_latency_why_does/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6lylv2",
          "author": "Sirwired",
          "text": "Because \"sub-second\" is super slow vs. the alternatives.",
          "score": 80,
          "created_utc": "2026-02-21 15:04:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6m0aee",
              "author": "nucleustt",
              "text": "wow. Thanks. I can only imagine the throughput of an in-memory Redis vector DB then. \"Near instant\"",
              "score": -13,
              "created_utc": "2026-02-21 15:13:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6m2aog",
                  "author": "CoastRedwood",
                  "text": "How big is the data you’re storing? Bigger the dataset the higher the latency and greater the cpu usage.",
                  "score": 7,
                  "created_utc": "2026-02-21 15:24:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6mfh90",
          "author": "jdanton14",
          "text": "If I had a database that has 900ms of latency, I'd fire my DBA and my storage vendor.",
          "score": 21,
          "created_utc": "2026-02-21 16:30:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6p44mo",
              "author": "coinclink",
              "text": "If you're working at scale though, vector algorithms are extremely taxing on a db engine, especially on huge datasets. 900ms consistently across a huge vector store is a good tradeoff vs spending thousands of dollars scaling out a bunch of read replicas that might still get deadlocked when a flood of requests come in.",
              "score": 3,
              "created_utc": "2026-02-22 01:11:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6mhxcr",
              "author": "nucleustt",
              "text": "lol. I wonder if the whatsapp api supports the \"is typing\" indicator.",
              "score": 1,
              "created_utc": "2026-02-21 16:42:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6m2h5q",
          "author": "barnaclebill22",
          "text": "That's sub- second for each retrieval. You then need to submit the returned text as context to your LLM, so any question response might end up taking a few seconds. \nIt's easy to switch to something like Opensearch (or use it as a fast cache with S3 vectors), so might be worth trying. \nChatting with a human agent on WhatsApp typically takes several minutes per conversational turn (since they're all handling dozens of conversations).",
          "score": 17,
          "created_utc": "2026-02-21 15:25:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6meti1",
              "author": "HiCookieJack",
              "text": "Or PG Vector with Postgres Serverless ",
              "score": 9,
              "created_utc": "2026-02-21 16:27:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6n0ogv",
                  "author": "hergabr",
                  "text": "PgVector on Postgres works surprisingly well for a chatbot",
                  "score": 3,
                  "created_utc": "2026-02-21 18:16:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6mhr80",
              "author": "nucleustt",
              "text": "Thanks, these are all solutions I would test and compare",
              "score": 1,
              "created_utc": "2026-02-21 16:41:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nshiy",
          "author": "Due-Horse-5446",
          "text": "500ms is sub second too..",
          "score": 7,
          "created_utc": "2026-02-21 20:38:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ol5r5",
              "author": "nucleustt",
              "text": "true. But even a second isn't bad. Hopefully someone on chat can wait \\~5 sec for a response... hopefully",
              "score": -1,
              "created_utc": "2026-02-21 23:14:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6qw3no",
                  "author": "Due-Horse-5446",
                  "text": "thats a pretty extreme amount, espetfpr chatbots you should be shaving miliseconds where you can..",
                  "score": 1,
                  "created_utc": "2026-02-22 09:33:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6oabfi",
          "author": "tank_of_happiness",
          "text": "I’m using it for your same use case. The LLM’s thinking takes way longer. I’m happy with the results I’m getting and the cost is almost nothing at my volume. Previously I was using open search and paying a couple hundred a month.",
          "score": 6,
          "created_utc": "2026-02-21 22:13:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6olb1w",
              "author": "nucleustt",
              "text": "wtf, couple hundred a month vs almost nothing!? That's crazy cost reduction!",
              "score": 1,
              "created_utc": "2026-02-21 23:15:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6qlamu",
              "author": "nucleustt",
              "text": "Also, it's reassuring that someone else is using it for my use case.",
              "score": 1,
              "created_utc": "2026-02-22 07:50:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6n2cnh",
          "author": "yarenSC",
          "text": "In S3 terms, infrequent access is a billing term more than anything else",
          "score": 4,
          "created_utc": "2026-02-21 18:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6okz0x",
              "author": "nucleustt",
              "text": "ha. gotcha",
              "score": 1,
              "created_utc": "2026-02-21 23:13:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6suj0a",
          "author": "CrazedBotanist",
          "text": "We are using S3 vectors for a couple projects. Overall, they have worked great for us and are cheap. However, the two things to look out for is the lack of native support for hybrid search (semantic and keyword search) and the latency. Latency has only become any issue for us in Agentic RAG solutions where the agents tend to search multiple times based on search results and the initial question.",
          "score": 1,
          "created_utc": "2026-02-22 17:02:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6uu3b5",
              "author": "nucleustt",
              "text": "Super useful comment. Especially the \"lack of hybrid (semantic and keyword) search\" part that I didn't even consider. That's a huge disadvantage.\n\nThank you",
              "score": 1,
              "created_utc": "2026-02-22 22:53:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6vog30",
                  "author": "CrazedBotanist",
                  "text": "No problem! If you need hybrid search and don’t mind the overhead of using managed open search clusters you can use s3 vectors as the storage engine for some cost savings on storage.",
                  "score": 1,
                  "created_utc": "2026-02-23 01:49:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6trq8l",
          "author": "netik23",
          "text": "You can’t tell me you have so much data you can’t store it on a SSD instance.",
          "score": 1,
          "created_utc": "2026-02-22 19:37:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6uubn3",
              "author": "nucleustt",
              "text": "You're right. I can't. What do you suggest?",
              "score": 1,
              "created_utc": "2026-02-22 22:54:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6mz94n",
          "author": "rexspook",
          "text": "Sub second is infrequent in computing terms",
          "score": -4,
          "created_utc": "2026-02-21 18:09:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9nmsm",
      "title": "When DynamoDB single-table design is the wrong choice (and what to use instead)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r9nmsm/when_dynamodb_singletable_design_is_the_wrong/",
      "author": "tejovanthn",
      "created_utc": "2026-02-20 06:11:35",
      "score": 32,
      "num_comments": 16,
      "upvote_ratio": 0.81,
      "text": "Last week's multi-tenant post hit #1 here. The comments were more useful than the upvotes - three engineers described it as a disaster at their companies.\n\nThey're not wrong. I use single-table design in production and I'm building a tool around it, but there are real situations where it's the wrong call.\n\nThe cases where you shouldn't reach for it:\n\n* Access patterns are still changing (pre-PMF products)\n* No DynamoDB champion on the team - works great until that person leaves\n* Significant reporting or analytics requirements\n* Fewer than 6 access patterns (just use multi-table, it's fine)\n* Multiple teams or bounded contexts sharing the same deployment\n* Per-entity DynamoDB Streams processing (you get one stream per table, not per entity type)\n\nThe \"disaster\" pattern I keep seeing isn't single-table being wrong - it's teams starting with key design before listing access patterns. You design the table to serve access patterns, not the other way around.\n\nFull post covers: the decision framework table, the microservices/team ownership case (probably the most underrated reason to avoid it), and what I actually use for [rasika.life](http://rasika.life) vs what I'd use for a prototype.\n\n→ [https://singletable.dev/blog/when-not-to-use-single-table-design](https://singletable.dev/blog/when-not-to-use-single-table-design)\n\nCurious what situations have pushed your teams away from it - or toward it despite the complexity.",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1r9nmsm/when_dynamodb_singletable_design_is_the_wrong/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6gbip1",
          "author": "menge101",
          "text": "Several of these \"single table design is the wrong choice\" are more \"DynamoDB is the wrong choice\" imo.\n\nParticularly: \"You have serious analytical or reporting requirements\"\n\nIt's an OLTP datastore, not an OLAP, it doesn't matter how many tables you use, you can't do arbitrary queries on the data.\n\nThis one: \"Fewer than 6 access patterns (just use multi-table, it's fine)\", i don't agree with.  It's not about how many access patterns, its about access patterns that capture relationships in the data.\n\nYou can have only one access pattern, but if the table uses the partition key and sort key to create a relationship between records, and then you pull all records for the partition key and have a full representation of the data relationship, you have used single table design correctly with a single access pattern.  (albeit, it would seem unlikely to only have that one access pattern form this multi-faceted related data)",
          "score": 9,
          "created_utc": "2026-02-20 17:06:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gei9r",
              "author": "tejovanthn",
              "text": "Both of these are fair pushback.\n\nOn the analytics point - you're right, and I should have been clearer. That item belongs in a \"DynamoDB is the wrong choice\" list, not a \"single-table is the wrong choice\" list. The underlying problem is using DynamoDB for OLAP workloads at all. The number of tables doesn't change that. I'll fix the framing.\n\nOn the access pattern count - also a fair correction. The \"6 access patterns\" heuristic is too blunt. What I was trying to capture is the case where your data relationships are simple enough that the cognitive overhead of single-table design isn't justified - but you're right that the real signal isn't the count, it's whether your access patterns require traversing relationships within a partition. One access pattern that pulls a full aggregate by partition key is a completely legitimate use of single-table design. I'll reframe that item around relationship complexity rather than access pattern count.",
              "score": 2,
              "created_utc": "2026-02-20 17:20:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6gl5cl",
                  "author": "menge101",
                  "text": "I'm not really pushing back, just discussing the ideas, I think your write up is fine for people who don't really know single table design and/or DynamoDB.\n\nLike you can't be all-use-case encompassingly correct on these things when you are talking in sort of abstract generalizations.",
                  "score": 2,
                  "created_utc": "2026-02-20 17:51:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6dssuw",
          "author": "mehneni",
          "text": "It was a disaster for us as well. Causing 6 digit monthly costs at some point. I guess a problem is always: You have to know what you are doing.\n\nReducing the number of tables compared to a relational design makes sense. But I'd almost limit the single table to something like one aggregate in DDD at most: [https://martinfowler.com/bliki/DDD\\_Aggregate.html](https://martinfowler.com/bliki/DDD_Aggregate.html)\n\nJust putting unrelated data in a complex data model with very different quantities in a single table is a disaster. In our case it was an event store, a production database and a (complex) configuration store. Mixing all of this is an absolutely horrible idea, because the different areas have vastly different requirements and change rates.\n\nScanning the table becomes impossible. This might not matter for day-to-day operations, but if every migration takes a week and is hugely expensive that is a problem and you become very inflexible.\n\nAnd always put an expiry on the data. Because deleting data by scanning the table is just so expensive that is makes more sense to create a new table and drop the old one (which is a stressful thing to do if your backup will take ages to reconstruct the table ;).\n\nThe risk in having more tables is also smaller. In that case you can only mess up one area of the data. So only do single table if you know exactly what you are doing. Merging multiple tables into one is easier than the other way around.",
          "score": 14,
          "created_utc": "2026-02-20 07:09:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ejxn5",
              "author": "tybit",
              "text": "Great point on the DDD aggregate. This is roughly how I’ve viewed it for a while but not had the right term to describe it.\n\nI’ve generally thought of it as only use single table design if you have a well bounded micro-service.",
              "score": 3,
              "created_utc": "2026-02-20 11:19:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6k5d92",
                  "author": "tejovanthn",
                  "text": "\"Well-bounded microservice\" is actually a cleaner way to say it for most developers than \"DDD aggregate\" - same idea, more accessible. The service boundary and the table boundary should roughly align.",
                  "score": 1,
                  "created_utc": "2026-02-21 06:02:26",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6dvg3e",
              "author": "tejovanthn",
              "text": "Six-digit monthly costs is the nightmare scenario - and the root cause you're describing is exactly what I called out in the post: mixing data with vastly different access patterns, quantities, and change rates into one table. An event store and a production database have almost nothing in common operationally. That's not a single-table design problem, that's a data modeling problem that single-table made worse.\n\nThe DDD aggregate framing is actually the clearest heuristic I've heard for where the boundary should be. One aggregate (or tightly related aggregates that are always queried together) per table makes the access patterns predictable and keeps migrations scoped. The moment you're mixing things that evolve independently - your config store vs. your event store - you've lost the main benefit and kept all the costs.\n\nThe TTL point is underrated. Designing for data expiry from day one changes how you think about the whole model. \"How does this data leave the table?\" should be in the access patterns list from the start.\n\nGoing to add the aggregate boundary heuristic to the post - it's a more actionable framing than what I had.",
              "score": 3,
              "created_utc": "2026-02-20 07:33:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ju084",
          "author": "finitepie",
          "text": "You can not talk about DynamoDB vs relational DB without talking about scaling. Scalability is the whole point behind DynamoDB. Everything else is a trade-off to make it scalable. I like DDB. I like it a lot. It's not that one super dooper database to rule them all. It serves a well defined purpose. And sometimes it's just not the right tool for the job. If you understand that in advance, understand how to make the tool work for you and not against you, then DDB is your friend. And it might be 'schema-less', but I want to see you change relational schemas back and forth and not complain about the consequences :D. There is always a schema, a logic, a model, just that it is not explicitly defined and enforced at the DB level like it would be with a relational db. And schema definition for a relational db or data in general can be a lot of work.  It requires a lot of discipline to work it out and to keep it consistent in the long run. Having a good model/schema of your data, is to understand the data. And once you get there, you can also understand the access patterns better. So for me, modelling the data is always the very first step. Every mistake you make in that process will always result in plenty of pain and work. For DDB even more so. ",
          "score": 2,
          "created_utc": "2026-02-21 04:33:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k588j",
              "author": "tejovanthn",
              "text": "Hard agree on the sequencing - model the data first, derive access patterns from that, then design the table. The \"schema-less\" label does a lot of damage because it implies you can skip that step. You can't. The schema just lives in your application code instead of the database, which means when it breaks, it breaks silently.\n\nThe scalability point is the crux of it. People reach for DynamoDB because of the scaling promise, then design it like a relational database and get the worst of both worlds - none of the query flexibility, none of the operational simplicity.",
              "score": 1,
              "created_utc": "2026-02-21 06:01:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gvyvw",
          "author": "AftyOfTheUK",
          "text": ">Last week's multi-tenant post hit #1 here\n\nLink?",
          "score": 1,
          "created_utc": "2026-02-20 18:40:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6junn3",
              "author": "tejovanthn",
              "text": "Post - https://www.reddit.com/r/aws/s/q8V8p4A9ab\nBlog -https://singletable.dev/blog/pattern-saas-multi-tenant\n\nI would love your thoughts too :)",
              "score": 2,
              "created_utc": "2026-02-21 04:37:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6m265x",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-21 15:23:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rgrh3",
              "author": "tejovanthn",
              "text": "Interesting take - I'd actually argue the opposite. Multi-attribute composite keys for GSIs eliminate one of the biggest pain points of single-table design: manually concatenating synthetic keys like \\`gsi1pk = TENANT#<id>\\` and parsing them back out on reads.\n\nWith multi-attribute GSIs, you define up to 4 natural attributes per key and DynamoDB handles the composition. That makes single-table patterns cleaner to implement, not harder. The overloaded GSI section in this article is exactly the kind of thing that gets simpler with this feature.\n\nThat said, I haven't updated the article to use multi-attribute keys yet - it's on my list. Curious what specifically you think it makes worse?",
              "score": 1,
              "created_utc": "2026-02-22 12:41:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dnode",
          "author": "galnar",
          "text": "Thanks for sharing, this is really thought provoking. Would you mind sharing your job title? I’m wondering what role gets this deep in the weeds. Do you have the same level of experience with RDS and/or DocumentDB?",
          "score": -1,
          "created_utc": "2026-02-20 06:23:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6drp4t",
              "author": "tejovanthn",
              "text": "Thank you :) I used to lead engineering teams building on AWS for the better half of the last decade. Decided to step back and focus on some passion projects. :)   \n  \nCurrently I'm an indie developer. Day job is building [rasika.life](https://rasika.life) (a Carnatic music platform) where DynamoDB is the primary datastore — so this isn't theoretical for me, it's the system I'm maintaining.\n\nOn RDS: yes, reasonably deep. I've built on Postgres for years and it's my default for anything with complex reporting, evolving schemas, or strong relational requirements — which is basically why it made the \"when to avoid single-table\" list. DocumentDB I've touched but wouldn't claim expertise.\n\nThe honest answer is that I get deep in DynamoDB specifically because I made the choice to build on it and had to live with the consequences. Nothing sharpens your opinion on a tool like owning it in production.",
              "score": 6,
              "created_utc": "2026-02-20 06:59:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r8s1yy",
      "title": "how bad is it to launch without a proper cloud architecture plan?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r8s1yy/how_bad_is_it_to_launch_without_a_proper_cloud/",
      "author": "These_Run_7070",
      "created_utc": "2026-02-19 06:40:35",
      "score": 29,
      "num_comments": 76,
      "upvote_ratio": 0.73,
      "text": "We are 8 months from launch and honestly we have just been spinning up services as we need them. no real architecture doc, just \"lets use this because it works.\" our AWS bill went from 2k to 8k in 3 months and we're not even at scale yet though.   \nmy co founder keeps saying we'll \"fix it after launch\" but I'm getting nervous. what if we hit product market fit and the whole thing falls apart because we built on sand?\n\nIs it crazy to pause feature dev for a month to actually design this properly? or do most startups just figure it out as they grow?\n\n",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1r8s1yy/how_bad_is_it_to_launch_without_a_proper_cloud/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o679a5q",
          "author": "sobeitharry",
          "text": "We are battling mistakes from 10 years ago.  A penny saved is a penny earned.",
          "score": 91,
          "created_utc": "2026-02-19 06:46:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67kc6k",
              "author": "tumes",
              "text": "The number of conference talks I have been to in the last year or two that are just people confidently post mortem-ing, in public no less, the same fucking mistakes every startup makes and has made for the last 15 years in the same order is just… too magnificent for words. I’m begging you, we are gross and old but hire one reliable greybeard who has seen some shit, they will save you one funding round worth of flailing and churn by shooting down your galaxy brain bullshit that relies on one non-load balanced server for ingress or a terminally unindexed database column or whatever.",
              "score": 46,
              "created_utc": "2026-02-19 08:26:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o69l4i8",
                  "author": "sobeitharry",
                  "text": "Yet we refuse to bring in outside expertise even when our in house experts recommend.  Just get it done,  yesterday.",
                  "score": 7,
                  "created_utc": "2026-02-19 16:36:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o68s2i1",
              "author": "SpecialistMode3131",
              "text": "You're still alive from 10 years ago.  People seriously underrate survival.  You \\*get\\* to fix the mistakes now because you're still in business.",
              "score": 16,
              "created_utc": "2026-02-19 14:09:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6lq6ho",
              "author": "Dabnician",
              "text": "I was asked to \"just move it to ecs\", not knowing shit about docker, kubernetes, fargate, ecs or any of that shit.\n\nI took 2 months to launch my first task in ecs cause i was worried about doing it right, or at least in a way where im not spending $300+/month for 1 containers before usage came into play.\n\nI avoided so many \"oopsies\" because I refused to rush.",
              "score": 2,
              "created_utc": "2026-02-21 14:16:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6gwu0b",
              "author": "Useful-Process9033",
              "text": "10 years of compounding tech debt is no joke. The teams I've seen recover from this always start with observability first, you can't fix what you can't see. At minimum get cost alerts and incident tracking in place before launch so you know where the fires are.",
              "score": 1,
              "created_utc": "2026-02-20 18:44:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gypb2",
                  "author": "sobeitharry",
                  "text": "Our company decided to just start building new products and is trying to phase out the old ones.  Of course the people building the new stuff are making the same mistakes,  but i just work here.  We like to repeat our mistakes.",
                  "score": 2,
                  "created_utc": "2026-02-20 18:52:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o679ntm",
          "author": "suddenly_kitties",
          "text": "Push your AWS account team to access whatever startup/Activate credits they are willing to throw at you, and ask for a meeting with a Solutions Architect. You might want to consider getting a contractor/consultant/partner involved at a point, technical debt and your bill will keep growing.",
          "score": 40,
          "created_utc": "2026-02-19 06:49:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i3jhz",
              "author": "Donnelding0",
              "text": "Ditto, there is money you may be leaving on the table that you are entitled to.",
              "score": 1,
              "created_utc": "2026-02-20 22:12:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o679dve",
          "author": "seany1212",
          "text": "I’d love to know what product you were working on that would allow for not being ready for release in 8 months. Also, how are you spending 8k with no throughput? Just what services at what sizing are you actually running?",
          "score": 45,
          "created_utc": "2026-02-19 06:47:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68h8py",
              "author": "ImNewHere05",
              "text": "Yeah, these numbers are wild",
              "score": 17,
              "created_utc": "2026-02-19 13:06:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6a5hy9",
              "author": "nemec",
              "text": "I hate these AI posts because I can't even trust that the numbers they're giving weren't just made up by an LLM.",
              "score": 3,
              "created_utc": "2026-02-19 18:13:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6boy0q",
                  "author": "Wrectal",
                  "text": "What's the giveaway to you that you believe this is an AI post?",
                  "score": 2,
                  "created_utc": "2026-02-19 22:44:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67ebao",
          "author": "spicypixel",
          "text": "Never seen a company successfully defer good decisions and reclaim that lost ground later.\n\nSome things just fuck you forever.",
          "score": 9,
          "created_utc": "2026-02-19 07:30:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gipxy",
              "author": "mezbot",
              "text": "I have, but it results in at least 10x as much work to retrofit, and it’s chaos to manage until they finally bite the bullet.  It ends up costing a small fortune.",
              "score": 2,
              "created_utc": "2026-02-20 17:40:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o69j8j6",
          "author": "Ok_Study3236",
          "text": "you're at $96k annual without a product yet? Lol, yes, you need to fix that now. That's a people problem not a technical one",
          "score": 8,
          "created_utc": "2026-02-19 16:27:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67bmz5",
          "author": "behusbwj",
          "text": "There are production services that cost a fraction of that at scale… what did you do?\n\nCloud architecture “plans” are a scam. Do design reviews with cost analyses. Use services properly with reasonable configurations for your needs. I have no idea what you could be doing with zero traffic that costs thousands already. Did you just spin up a bunch of random ec2 instances and call it a day?",
          "score": 15,
          "created_utc": "2026-02-19 07:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67szg9",
          "author": "mdivan",
          "text": "8k pre launch is huge, you are absolutely correct to be worried how that would scale.",
          "score": 13,
          "created_utc": "2026-02-19 09:52:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67w52e",
              "author": "mamaBiskothu",
              "text": "Also 8 MONTHS from launch it seems. Like what are you doing training the next GPT?",
              "score": 6,
              "created_utc": "2026-02-19 10:22:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67lx04",
          "author": "Sudoplays",
          "text": "Can you give some more insight into what services you’re using, why and the cost of each one?\n\n“Fix it after launch” - this is highly unlikely to happen as new features are needed, bugs get squashed and the focus shifts away from architecture.\n\nPlanning your cloud infrastructure is important, and should be done before deploying any resources. Infrastructure as code isn’t required, but it’ll make everything easier down the road when you need to have separate dev/rc/prod environments.\n\n8k is a huge bill for something that isn’t even launched yet, even 3k is quite high.",
          "score": 5,
          "created_utc": "2026-02-19 08:42:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67cvli",
          "author": "watergoesdownhill",
          "text": "This whole thing reeks. Anything that takes more than six months typically doesn’t ship at all. \n\nI have a feeling you guys are battling customer requirements. While also doing some sort of grand infrastructure.",
          "score": 9,
          "created_utc": "2026-02-19 07:17:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68glwb",
          "author": "never-starting-over",
          "text": "I don't know what your product is but I built a video marketplace like Netflix + Youtube with Kubernetes and analytics for stakeholders and it cost 500/month with two copies of the environment running (prod, staging)\n\n3k sounds like a lot. Jumping from 3k to 8k is insane. Spending this much with zero revenue (pre launch) sounds like suicide\n\nEver consider getting someone to review whats going on there?",
          "score": 2,
          "created_utc": "2026-02-19 13:02:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67aohd",
          "author": "notospez",
          "text": "At this stage it depends on your runway. Do you have or expect to have millions available for growth? Then 8k/month in infra is peanuts and you can hire a bunch of people to reengineer next year once you know which product features resonate with customers and have actual usage data.",
          "score": 3,
          "created_utc": "2026-02-19 06:58:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67fiz6",
          "author": "dariusbiggs",
          "text": "Without infrastructure as code from the start you are shooting yourself in the foot. you need to understand what you are using and why you are using it. Then you can see what your minimum spec is and how you can scale it.\n\nYour description is a clear example of not understanding things clearly (your product, cloud infra, and your customer needs) . This is indicated by your costs and are very likely caused by premature over engineering.\n\nCut it down, simplify, and only build scaling when you have metrics demonstrating the need (or the upcoming need due to observable trends).\n\nWe over engineered from the start due to misunderstood requirements and cut down those costs to the point where our entire prod, staging, and dev environments combined cost about what your costs are. Those resources are all running at the minimum specifications for the various components but provide capacity for another 2000% user increase. (Constraints are caused by memory minimums and db connection numbers, whilst the compute is only running at 1-2% load and growth of users has minimal effects on memory usage and the number of DB connections, it's all compute and network IO based).\n\nAnd to cut down costs, the various non-prod environments are turned off when not needed for extended periods of time (like the entire month of the Christmas break period). That is achievable with proper use of IaC, turn it off. You don't need dev or test environments afterhours or on weekends, so kill them if you can.",
          "score": 2,
          "created_utc": "2026-02-19 07:41:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o679whl",
          "author": "Ready-Trick-8228",
          "text": "honestly we threw infros in just to see what was actually eating credits. kind of low effort, low stress way to keep an eye on things without overthinking it.",
          "score": 1,
          "created_utc": "2026-02-19 06:51:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67d50f",
          "author": "mitch3x3",
          "text": "Feel free to dm me. Happy to help if it’s in my wheelhouse",
          "score": 1,
          "created_utc": "2026-02-19 07:19:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67d8qr",
          "author": "Alsmack",
          "text": "Wall of text incoming; tl;dr: balance your efforts with the realities of time and money. Don't invest too early in things that don't help you prove product market fit. Do invest in things that let you iterate quickly without digging a deep tech debt hole, as being able to deliver changes reliably and quickly lets you get to that product market fit faster.  \n  \nThere is a balance to be had. As a startup there's a few things that really matter. Acquiring/validating product market fit, and the length of your runway (cash on hand / cost per month to run the business = runway in months) are the two that come to mind with this question.\n\nIt's very easy to do things well enough. It's also very easy to make a completely unmaintainable and overpriced mess. What you don't want to do is spend a month making things better than well enough only to launch a product that has no market or can't find it's feet in the market, even if one exists. This shortens your effective runway to solve those problems.\n\nPlatform/tech/infra/arch serve the operations and maintainability of a software product and business. It means nothing if you don't have a product. If what you are doing today is \\*honestly\\* getting in the way of delivering a product (reducing your runway to have a successful (growing) product) or costing too much money (again, increasing burn rate and reducing runway, or too much time due to inconsistency/bad dev flow/whatever) so that you won't hit your deadlines with adequate reserves, then it sounds like there is a business decision to made to solve those pain points.\n\nInvestment in platform is a business decision. You are limited by time and money. Any of those you spend on platform are not being spent on product. Yes, there are tradeoffs - if you don't spend enough time/money on solid foundations, you'll probably have a less reliable product. There's not a magic bullet here, you must evaluate your honest business needs and make a decision based on the facts at hand.\n\nIf you hit product market fit, you have a good problem on your hands. In theory, this means revenue that lowers your burn rate increasing your runway, or more opportunity for investment for more capital which increases your runway, both of which give you more options to get resourcing on stability/scalability/efficiency that you caused by not prematurely spending time on whatever that bottleneck is.\n\nAs an platform decision maker myself, 90% of what I do doesn't matter pre product launch. Make sure we're using IAC that's fast to iterate on, make sure we're documenting decisions, make sure we have reasonable deployment pipelines that are consistent, boring, and reliable, and have any choice for observability and alerting. That's the most I would care about pre launch. Post launch once running a live service, that's a whole different ball game. But, if I've got those 4 things in place we have consistency in provisioning, in deployment, we aren't blind, and we have documented business context. That's a solid foundation for working through operational challenges and pushing for reliability. And you don't need all of them right away. o11y can get expensive fast, for example.\n\nLastly, check with your AWS account rep about AWS startup credits or other programs you can get into. Your infra costs sound way too high for a pre-launch early stage startup, they often give credits for adopting new services and things like that. Depending on your level of support, you may be able to get some help from an aws solutions architect or similar to help manage those costs, though that typically comes with the super pricey stuff.",
          "score": 1,
          "created_utc": "2026-02-19 07:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67f19q",
          "author": "metaphorm",
          "text": "if you can't keep the scaling of operational expenditure below the scaling of revenue growth, the economics of the project are broken. infrastructure cost is a major component of operational expenditure. get this under control now.\n\nyou might reasonably decide to burn extra cash when the scaling cost problems aren't yet critical. you can do this for a short time, but if the curve isn't bent in the right direction, the more you scale the worse it gets, and you'll burn the whole runway and have to raise more money way too soon, probably under poor terms. don't let this one get away from you.",
          "score": 1,
          "created_utc": "2026-02-19 07:36:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g8orw",
              "author": "MaxMcregor",
              "text": "This is such an underrated point if the unit economics do not work, growth just accelerates losses instead of value. Scaling only helps when each new customer actually improves the business, not strains.",
              "score": 1,
              "created_utc": "2026-02-20 16:53:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67ifs9",
          "author": "sunny6333",
          "text": "this is very normal for start up culture because many VCs dont give a shit about how much money u burn, only about revenue growth\n\nyes this will almost guarantee bite u in the ass in the future, but with how frequently roadmaps and decisions change in a start up it's very difficult to nail it immediately",
          "score": 1,
          "created_utc": "2026-02-19 08:08:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67jg8m",
          "author": "Wide_Commission_1595",
          "text": "Honestly if you can afford it, don't worry too much.  Everything can be fixed later.\n\nThat said, a little work to set up a good org structure and a few guardrails isn't too hard.  It causes some initial pain because devs have to follow a plan, but it doesn't need to be horrible",
          "score": 1,
          "created_utc": "2026-02-19 08:18:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67wedo",
          "author": "pint",
          "text": "there are two types of startups i see. one is the garage type, short on resources, fueled by enthusiasm. the other is the investment burning type, which somehow lays a hand on huge sums of angel money, and a new round when the previous runs out.\n\nif you are the second type, why would you care?\n\nif you are the first type, you really need to consider how will you realize enough revenue to cover the 10k, 20k, whatever expense just on infra, and to recover the sunk cost too.\n\nin my experience, the fix it later phase never comes. you always have new things to do, bugs to fix, features to add. nothing lasts longer than a temporary solution.",
          "score": 1,
          "created_utc": "2026-02-19 10:25:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67xtv7",
          "author": "SikhGamer",
          "text": "> my co founder keeps saying we'll \"fix it after launch\"\n\nThis approach is fine, if you _actually_ do it. Otherwise trust your gut.",
          "score": 1,
          "created_utc": "2026-02-19 10:38:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o680pda",
          "author": "256BitChris",
          "text": "Your biggest risk is not getting marketing fit, not whether you'll be able to scale or not.\n\nScaling is a solved problem with well known paths forward - building a successful product isn't.    \n  \nFix it after launch, if it starts to break.  Losing a month to polish tech debt, before reaching PMF could easily kill your startup, especially in today's hyper fast world of AI.",
          "score": 1,
          "created_utc": "2026-02-19 11:03:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68vaz9",
              "author": "AndyWhiteman",
              "text": "Launching without a solid setup can definitely be risky, but a lot of founder get away with it at first and then run into bigger issues later. Having a clear plan for how things will scale from the start can save a lot of headaches down the line.",
              "score": 0,
              "created_utc": "2026-02-19 14:27:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6g9aky",
                  "author": "MaxMcregor",
                  "text": "Totally agree shipping fast is fine, but you need a path to harden things before real scale hits. Early shortcuts are survivable, scaling on top of them without fixing the foundation is where things usually break.",
                  "score": 1,
                  "created_utc": "2026-02-20 16:56:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6839r0",
          "author": "bot403",
          "text": "I was at a B2B business that had 8 figures of top line revenue with 8k/mo spend...\n\n\nWhy are you spending so much with no customers? Scale it down the scale it back up when you know what the bottlenecks are.",
          "score": 1,
          "created_utc": "2026-02-19 11:25:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o688ulo",
          "author": "requiem33",
          "text": "Way to familiar... trying to win the race to the bottom I see.  I've been around the block a few times (grey beard) and the classic mistake of \"it's only temporary\" and \"we'll fix it later\" are the two phrases that have killed more startups than those few that make it to buy out. ",
          "score": 1,
          "created_utc": "2026-02-19 12:09:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68klza",
          "author": "japanthrowaway",
          "text": "What services are you using that led to such a dramatic increase?",
          "score": 1,
          "created_utc": "2026-02-19 13:27:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68qw4t",
          "author": "rwodave",
          "text": "Technical debt is no joke.",
          "score": 1,
          "created_utc": "2026-02-19 14:02:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68sbsy",
          "author": "SpecialistMode3131",
          "text": "Figure out what your burn rate will be under different scenarios and make a call from that. If scaling up will bankrupt you, then yes.  If scaling up will just mean you have ongoing problems, then probably no - because you'll actually be alive to try and solve them.  Scaling up means you have market fit, and you can move on to series A.\n\nYou eat an elephant one bite at a time.",
          "score": 1,
          "created_utc": "2026-02-19 14:10:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68yir5",
          "author": "TheGutterBall",
          "text": "Based on the description, it almost sounds like you don’t have the skill set in house to fix it. If you had someone with experience or architectural knowledge, they should have raised the alarm (loudly) by now with these numbers. An architectural doc isn’t going to solve anything; find a professional, hire them, come up with a plan to get this fully managed, execute on it",
          "score": 1,
          "created_utc": "2026-02-19 14:44:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6965q7",
          "author": "CyclonusRIP",
          "text": "It's more about understanding your constraints.  If you guys have a ton of funding and $8K per month isn't a big deal then maybe you don't need to worry about it.  If you guys are bootstrapping or don't have a ton of runway then you should probably try to minimize hosting cost.  Creating a lot of services can be expensive from a hosting perspective and time consuming to manage that architecture for a small team.  Early on you probably want to minimize the number of services and technologies you use as much as possible.",
          "score": 1,
          "created_utc": "2026-02-19 15:23:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ala8f",
          "author": "ManBearHybrid",
          "text": "There's nothing wrong with a little technical debt... as long as you're realistic about paying it back, and you know both sides of the trade-off. Just blindly deploying stuff without a plan is, frankly, insane. This is before all the possible other concerns - security, reliability, durability, disaster recovery, etc. \n\nAlso, you're probably duplicating a lot of work - doing, undoing and then re-doing a lot of stuff because you're stumbling around in the dark. Taking a breath to put together a plan will give you a proper north star to steer towards. You'll be able to put together a proper road map to get there, and you'll know if you're falling behind. ",
          "score": 1,
          "created_utc": "2026-02-19 19:28:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b4ucr",
          "author": "Own-Manufacturer-640",
          "text": "As an AWS consultant i can assume the same mistakes made by every startup.\n\nDev uat qa resources running 24/7, \nOver provisioned resources because devs have the access to create resource, \nMight be default vpc and public ips, \nZombie resources, \nSnapshot sprawl, \nMulti region resources, \nOr too much NAT Gw usage, \nOver provisioned EBS, \nUsing services that are not useful for your startup,  \nZero ownership of resources etc etc etc. \n\nIf you guys are not using Sagemaker then i think 8k pre launch is too much.",
          "score": 1,
          "created_utc": "2026-02-19 21:03:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c7129",
          "author": "raptorraptor",
          "text": "I'd start looking for another job. Unless you're the most senior engineer, then I'd look for another line of work.",
          "score": 1,
          "created_utc": "2026-02-20 00:28:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ctsbl",
          "author": "DrollAntic",
          "text": "Foundations matter. If you don't have one, you'll struggle to scale.",
          "score": 1,
          "created_utc": "2026-02-20 02:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dy2w8",
          "author": "curiouscrustacean",
          "text": "Sounds like you guys need an experienced DevOps to unfuck the fucked things and do things as you go.\n\nI'm currently doing this for a country level application.",
          "score": 1,
          "created_utc": "2026-02-20 07:58:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ey3gh",
          "author": "RhoOfFeh",
          "text": "Fixing it after launch means never fixing it until a massive re-engineering effort which will be too expensive and too late.",
          "score": 1,
          "created_utc": "2026-02-20 13:00:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fjhk3",
          "author": "grsftw",
          "text": "Yes, do design. No, don't go too deep yet. If you are pre-revenue then your #1 goal is reaching revenue. This to me is a very bad idea: \"Is it crazy to pause feature dev for a month to actually design this properly?\" \n\nSide note, are you saying you are up to $8k/mo for AWS? Unless you are well-funded, yes, that is a bad idea. I'd do some lite design and also a monthly audit and decomm anything not essential.",
          "score": 1,
          "created_utc": "2026-02-20 14:56:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gdcuk",
          "author": "liverdust429",
          "text": "Don't pause for a month but don't wait until after launch either. Spend a week: tag everything, lock down IAM, turn on CloudTrail, set a billing alarm. That alone keeps 8k from silently becoming 20k and prevents the \"oh shit\" moment when your first enterprise customer asks for a security review.",
          "score": 1,
          "created_utc": "2026-02-20 17:15:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hkltb",
          "author": "AftyOfTheUK",
          "text": "The best scenario is the one in which you planned ahead, architected well, kept costs down, got to market with a good product fit, and then succeeded\n\nThe next best scenario is one in which you have to spend quite a lot of money to re-architect at some point your journey, but still achieve the above. DM if you like, that's what I do, and I will be looking for a new role in the summer.\n\nThe worst scenario is one in which you slow down to optimize, miss your window (or spend too much on architecture instead of demonstrable features), and fail completely.\n\nThere's almost nothing you can't architect your way out of, it's just a matter of cost and time. It may be better to allocate five guys to it in a year after a big funding round, than allocate one guy to it today, who would otherwise have been building critical features",
          "score": 1,
          "created_utc": "2026-02-20 20:37:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hzjxu",
          "author": "Neves_Space_Corps",
          "text": "Incremental phased builds in test, with CDK deploy / destroy cycles. Intelligently scaling infrastructure based on test findings. These will help. \n\nWithout a user load, there is no reason to keep infrastructure running all the time.\n\nIaC is your friend here.\n\n$8k/month pre launch sounds like something is hugely amiss.",
          "score": 1,
          "created_utc": "2026-02-20 21:51:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ie6dl",
          "author": "revanthmatha",
          "text": "don’t listen to any of the guys here unless they have practical experience. the solution is actually really simple. \n\ngo into cursor or ide of choice and literally give it a service account with full system admin api key. have it start going through all the resources you have with your code base. \n\nhave it write an as is document of everything. then prompt it to optimize. read the plan it comes up with and execute. \n\nthis is like a 1-2 days task depending on how many resources you have to consolidate. \n\nfor example if you have many different databases, why not just 1 db with many tables etc.",
          "score": 1,
          "created_utc": "2026-02-20 23:09:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6iz7wd",
          "author": "linux_n00by",
          "text": "OP you have at least a VPC configured?",
          "score": 1,
          "created_utc": "2026-02-21 01:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jzi46",
          "author": "AlphaToBe",
          "text": "Just want to add the practical side because I have been in your exact position and it helped me to just SEE the numbers first before making any big decisions.\n\n\nRun this and it will show you your top 5 money burners right now:\n\n\n```\naws ce get-cost-and-usage \\\n  --time-period Start=2026-02-01,End=2026-02-21 \\\n  --granularity DAILY \\\n  --metrics UnblendedCost \\\n  --group-by Type=SERVICE \\\n  --query 'ResultsByTime[-1].Groups | sort_by(@, &Metrics.UnblendedCost.Amount) | [-5:]'\n```\n\n\nOnce you see that list, I bet at least one of these will jump out:\n\n\n**NAT Gateway** , $0.045 per GB processed. Sounds like nothing but a chatty microservice pushing logs externally can burn $2k/mo through a single NAT. I have seen this catch people off guard more than once.\n\n\n**RDS** , running a db.r5.large 24/7 in dev? Thats ~$260/mo sitting idle. Three environments? $800/mo on databases nobody is querying. Easy to miss.\n\n\n**Public IPv4 addresses** , since Feb 2024 AWS charges $3.65/mo PER public IP. Got 20 EIPs sitting around from \"I was just testing something\"? Thats $73/mo for literally nothing.\n\n\nThese two commands helped me find the orphans in my own account:\n\n\n```\naws ec2 describe-addresses \\\n  --query 'Addresses[?AssociationId==`null`].[PublicIp,AllocationId]' \\\n  --output table\n```\n\n\n```\naws ec2 describe-volumes \\\n  --filters Name=status,Values=available \\\n  --query 'Volumes[].[VolumeId,Size,CreateTime]' \\\n  --output table\n```\n\n\nAnd honestly the one thing I wish I had done from day one is just set a budget alert. Takes 30 seconds and it would have saved me a lot of surprises:\n\n\n```\naws budgets create-budget --account-id YOUR_ACCOUNT \\\n  --budget '{\"BudgetName\":\"dont-surprise-me\",\"BudgetLimit\":{\"Amount\":\"5000\",\"Unit\":\"USD\"},\"TimeUnit\":\"MONTHLY\",\"BudgetType\":\"COST\"}'\n```\n\n\nYou dont need a perfect architecture to start. Just spend one afternoon with those commands and you will probably find $2-3k in quick wins. That buys you time to plan properly without bleeding money.",
          "score": 1,
          "created_utc": "2026-02-21 05:14:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k09p6",
          "author": "Jin-Bru",
          "text": "It all depends on your appetite for risk.\n\nYou could just spin up services and hack them together.  Your maintenance cost go up, your security is non existent, your feature deployment success rate goes down.  Constantly chasing bugs and metrics and looking for a reason why.\n\nOr do it your way, with architecture.  Design it first then build it.  Or design it while you build it.  But it ahould have an architect design something for you.\n\nAt least use what you have to get some IaC mappings and use an IaC (Terraform, CF, Ansible) to build for you.   In this way it will always be documented.   The other way is fraught with sleepless nights.\n\n\nIts a very bad idea.  The day you go public facing is the day you become a target.  It's not just the good guys who will visit your sites.... the bad guys are always looking for a new soft target.",
          "score": 1,
          "created_utc": "2026-02-21 05:20:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kwv4w",
          "author": "PokeRestock",
          "text": "I rushed an Elastic Cache adoption for better user experience and estimated the cost incorrectly. The cost was 1.5x more due to other necessary infra and I was already losing money with the move.\n\nIf I spent a day or two planning I could have had the same solution with S3, SQS, and Lambda at fraction of cost but I rushed for UX. Then I rushed to redesign and redefine when costs were too much. \n\nId say take the time to measure twice as bad design decisions take twice as much time and sometimes money to fix",
          "score": 1,
          "created_utc": "2026-02-21 10:25:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kx2jd",
          "author": "PokeRestock",
          "text": "Ill also add that sometimes infra optimization saves time later for amendments and recreation. Ideally you should have your entire stack defined in CDK and have a scaled down version for test, stage, and then production. Doing everything in production is a mistake that will cost you when live.",
          "score": 1,
          "created_utc": "2026-02-21 10:28:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vlqae",
          "author": "Aggressive-Squash-28",
          "text": "A few months of Claude Code could probably fix this with ease. Not 100% perfect but I’ve been doing devops adjacent work rapidly with it on GCP. The latest versions of Claude are pretty unreal.",
          "score": 1,
          "created_utc": "2026-02-23 01:32:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vufqk",
          "author": "dom_optimus_maximus",
          "text": "Im pre launch and manage dev and prod for crud with lambdas, api gateway and \"splurged\" on RDS DBs :).  My spend is 80 buckaroos a month which gives me 3 years on the AWS activate credits we got through our bank. :) You are in the proverbial shit my guy.",
          "score": 1,
          "created_utc": "2026-02-23 02:25:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67onqb",
          "author": "snorberhuis",
          "text": "Most startups figure it out as they grow, but they don't spend 8k every month. They stay below the $ 1000-a-month threshold. \n\nSpending already 8k a month is ridiculously high. I would expect that this is not necessary. You can have some very bad decisions built into your AWS Architecture that lock in your base spend. The migration costs keep being too low versus the other business opportunities, but they keep eating away at your runway/profit every month. In the end, you can still end up with running out of money.\n\nLast week, I ran a quick scan of the company's architecture, and they were spending $400k a year on AWS because of these bad choices. They could reduce their bill by $100k a year if they had built it properly from the start. The expectation was that, with the forecasted exponential growth of their company, this would grow at the same rate.\n\nIt is not crazy if you are already at 8K a month. Dm me if you want me to take a quick look.\n\n  \n",
          "score": 0,
          "created_utc": "2026-02-19 09:09:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67kkiw",
          "author": "EconomistAnxious5913",
          "text": "8 months? launch quicker in today's times - thats the first thing, launch beta's 1,2,3 but generally launch sooner trather than later.\n\naws bill 8 k isn't a worry.\n\nsand castles crashing isn't a worry. \n\nyes, you can fix later, if you don't have time now. it doesnt matter. it may result in un-ideal spikes and failures, but your target is to get customers, especially on a new product launch, that is your first priority.\n\ncustomers ayenge to tehy will be happy, to sab easily fix and handle ho jayega. yes firgure it out as they grow.\n\n",
          "score": -1,
          "created_utc": "2026-02-19 08:29:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ar3iw",
              "author": "Garetht",
              "text": "I hope you are able to recover from your stroke quickly and easily.",
              "score": 3,
              "created_utc": "2026-02-19 19:55:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6c1np8",
                  "author": "beluga-fart2",
                  "text": "It’s not a stroke, it’s Hindi my bro.  He said everything gets figured out eventually , which is generally true.  Even after you go out of business :)",
                  "score": 1,
                  "created_utc": "2026-02-19 23:57:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rdff6p",
      "title": "Price increase at AWS?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rdff6p/price_increase_at_aws/",
      "author": "servermeta_net",
      "created_utc": "2026-02-24 12:52:15",
      "score": 21,
      "num_comments": 16,
      "upvote_ratio": 0.78,
      "text": "Recently many non hyperscaler providers I use (Hetzner, OVH) increased their prices due to the supply issues we all know. Do you think AWS and other hyperscalers will follow through, or will they shield their customers from the hardware market fluctuations? ",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rdff6p/price_increase_at_aws/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o74p0sd",
          "author": "PaintDrinkingPete",
          "text": "AWS likely has an advantage of already having a massive infrastructure and having priority with manufacturers...and already charging more for their services than many of the smaller budget VPS and cloud computing services providers operating on tighter margins, so they don't have to be as reactionary to the market in cases like this. \n\neventually though, if things keep trending as they are... yes, prices will have to go up.",
          "score": 43,
          "created_utc": "2026-02-24 13:11:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74uxv9",
          "author": "criminalsunrise",
          "text": "AWS tend not to do price rises - I believe it's one of their core principles. What they will do is have new service levels (instance types) that are more expensive and slowly sunset the old ones. We may see that accelerated a bit with the supply crunch.",
          "score": 36,
          "created_utc": "2026-02-24 13:44:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o750bw8",
              "author": "RickySpanishLives",
              "text": "Highly likely. The systems already in the field are just being amortized out. No real point increasing their price because they are already there. New instance types will incur an increased cost since they will cost more and have a BOM with more expensive parts.",
              "score": 11,
              "created_utc": "2026-02-24 14:13:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74mnrx",
          "author": "Old_Cry1308",
          "text": "aws will probably absorb some of it short-term. long-term, they'll pass it on. they’re not running a charity.",
          "score": 39,
          "created_utc": "2026-02-24 12:56:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74nwec",
              "author": "moduspol",
              "text": "They might have bigger longer term deals on hardware like this and haven’t had to eat any higher costs yet.",
              "score": 12,
              "created_utc": "2026-02-24 13:04:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7712h9",
          "author": "FlatCondition6222",
          "text": "Less spot capacity, for example, is also a \"way\" to raise prices.",
          "score": 3,
          "created_utc": "2026-02-24 19:48:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74pe0u",
          "author": "Negative-Cook-5958",
          "text": "There is already about a 5% increase with each EC2 generation change (r6i => r7i => r8i for example), they won't increase pricing for existing SKUs, just make the newer ones expensive when they are available.",
          "score": 10,
          "created_utc": "2026-02-24 13:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74vbkc",
              "author": "davewritescode",
              "text": "The thing is you should be using less compute as new generations usually perform better",
              "score": 7,
              "created_utc": "2026-02-24 13:46:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o750u03",
                  "author": "RickySpanishLives",
                  "text": "It's really the instance shape that determines if that is possible. Sometimes the compute is indeed a better performer but the instance shape gives you more than you need at an increased price requiring you to binpack your computer to cover it. Less a concern if you breathe EKS/ECS - but an issue if you use raw EC2.",
                  "score": 2,
                  "created_utc": "2026-02-24 14:16:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74ps5y",
          "author": "JonnyBravoII",
          "text": "Yes. Many of their prices have been the same for years (EBS, lambda, data transfer) and are obviously cash cows. On the EC2 front, prices started going up after the 6 series.  The t series, while wildly popular, are clearly being sunset as the t4g is 6 years.old at this point. I would expect storage costs will be the first to go up.",
          "score": 4,
          "created_utc": "2026-02-24 13:15:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74u5uo",
          "author": "d70",
          "text": "Who is keeping track? https://aws.amazon.com/blogs/aws/category/price-reduction/\nThat said, I agree that some new instance types are slightly more expensive than older ones but they are different products, no?",
          "score": 4,
          "created_utc": "2026-02-24 13:40:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78ssws",
          "author": "Complex86",
          "text": "I think AWS will pass this on in future generations of instance family or other services they may provide down the line.",
          "score": 1,
          "created_utc": "2026-02-25 01:05:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74yw4o",
          "author": "Shington501",
          "text": "Everything is going up",
          "score": 1,
          "created_utc": "2026-02-24 14:06:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbw463",
      "title": "Podcast?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rbw463/podcast/",
      "author": "putneyj",
      "created_utc": "2026-02-22 20:10:12",
      "score": 17,
      "num_comments": 10,
      "upvote_ratio": 0.95,
      "text": "So, is the official AWS podcast no longer doing news? Anyone else that used it to get \\*most\\* of your news about new services? I’m honestly a little bummed, but it just feels like the way things are going at AWS.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rbw463/podcast/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6tzvdo",
          "author": "signsots",
          "text": "The latest episode aligns with the recent layoff announcements. Wouldn't be surprised if it was part of it unfortunately.",
          "score": 19,
          "created_utc": "2026-02-22 20:18:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u20m0",
          "author": "coyotefarmer",
          "text": "That was a big part of how I kept up with changes. Such a shame. I don't see how ending something like that can be beneficial in the long run. I listened to it for years.",
          "score": 6,
          "created_utc": "2026-02-22 20:29:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71w9ms",
          "author": "ArchangelAdrian",
          "text": "Well the host Simon Elisha announced on his LinkedIn 2 weeks ago that his time at AWS came to an end.",
          "score": 4,
          "created_utc": "2026-02-24 00:41:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71xjc9",
              "author": "putneyj",
              "text": "Damn, yeah I would imagine that’s pretty much the end then. Not even a regular conversation with Werner can save your job.",
              "score": 3,
              "created_utc": "2026-02-24 00:48:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o740qdd",
                  "author": "ArchangelAdrian",
                  "text": "Sad if you think about it.",
                  "score": 2,
                  "created_utc": "2026-02-24 10:02:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6vz1y4",
          "author": "zenmaster24",
          "text": "Are there community podcasts that do similar?",
          "score": 3,
          "created_utc": "2026-02-23 02:53:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xz12c",
              "author": "wreckuiem48",
              "text": "A cloud podcast I like is called [https://www.thecloudpod.net/](https://www.thecloudpod.net/)",
              "score": 3,
              "created_utc": "2026-02-23 12:51:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6w1zg5",
              "author": "putneyj",
              "text": "I would also like to know this",
              "score": 1,
              "created_utc": "2026-02-23 03:11:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y8fb4",
          "author": "Specific-Art-9149",
          "text": "I built a morning brief email with Claude that scans various AWS RSS feeds along with 3rd party RSS and Reddit for AWS news.   Costs me about 7 cents a day utilizing Claude API, and it is emailed daily.  I'm an SA not a developer, so pretty happy with how it turned out.",
          "score": 2,
          "created_utc": "2026-02-23 13:49:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xz8n2",
          "author": "bot403",
          "text": "Google gemeni can turn anything into a podcast episode. Use it on the main aws RSS feed :D",
          "score": -3,
          "created_utc": "2026-02-23 12:52:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd7p4v",
      "title": "Cloudfront + HTTP Rest API Gateway",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rd7p4v/cloudfront_http_rest_api_gateway/",
      "author": "Alive_Opportunity_14",
      "created_utc": "2026-02-24 05:31:50",
      "score": 13,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "Cloudfront has introduced flat rate pricing with WAF and DDos protection included. I am thinking of adding cloudfront in front of my rest api gateway for benefits mentioned above. Does it make sense from an infra design perspective?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rd7p4v/cloudfront_http_rest_api_gateway/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7372db",
          "author": "Old_Cry1308",
          "text": "makes sense if you need the protection and pricing works for you, otherwise might be overkill.",
          "score": 5,
          "created_utc": "2026-02-24 05:34:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73dx94",
              "author": "Alive_Opportunity_14",
              "text": "From a pricing perspective it seems cheaper because of the flat pricing and added benefits. WAF on rest api costs money while its included in the flat pricing",
              "score": 1,
              "created_utc": "2026-02-24 06:30:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73p4sr",
          "author": "snorberhuis",
          "text": "A WAF is a layer of defense I would generally recommend for most companies. It can help you protect against automated attacks. There are very few exceptions to this recommendation.  ",
          "score": 3,
          "created_utc": "2026-02-24 08:11:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74z0gx",
          "author": "menge101",
          "text": "[Docs](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/flat-rate-pricing-plan.html) for anyone else that needs them\n\n[Pricing sheet](https://aws.amazon.com/cloudfront/pricing/) as well\n\nThere is a free tier as well as a pro tier at $15/month that seems fairly compelling.",
          "score": 2,
          "created_utc": "2026-02-24 14:06:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74fiyc",
          "author": "KayeYess",
          "text": "While AWS WAF2 can be attached directly to Amazon API Gateway, Cloudfront gives additional benefits such as distributed edge delivery, ability to use multiple origins (such as S3 for static content), caching, etc.",
          "score": 1,
          "created_utc": "2026-02-24 12:07:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7drkhi",
              "author": "vppencilsharpening",
              "text": "I'd also add that it leverages the AWS managed backbone for transport from the Edge to the Origin. So if your application is running in a single region you get AWS's team ensuring fast connections from the CloudFront edge to your application instead of relying on the public internet. \n\nIt's not going to make a huge difference, but it's not nothing. \n\nClient -> Public Internet (short distance) -> AWS CloudFront Edge (closest to the client) -> AWS Network (for most of the distance) -> Origin Application\n\nVS\n\nClient -> Public Internet (long distance) -> AWS Network (for a very short distance) -> Origin Application",
              "score": 1,
              "created_utc": "2026-02-25 19:34:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dx975",
                  "author": "KayeYess",
                  "text": "Yes ... that's a general benefit of a CDN. Client reaches CDN edge, and CDN handles the rest.",
                  "score": 1,
                  "created_utc": "2026-02-25 20:00:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o797flg",
          "author": "SilentPugz",
          "text": "Harden your security response header and content security policy for your cloudfront.  \n\nLambda edge for quick validations. Cloudfront managed functions makes some things simple \n\nDon’t forget your tls flow. Where you want to terminate. At the cloudfront , lessen the load on the api.",
          "score": 1,
          "created_utc": "2026-02-25 02:28:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f6tjp",
          "author": "TheDearlyt",
          "text": "The main tradeoff is added complexity so it’s worth it mostly when you actually plan to use WAF rules, caching, or global performance improvements, not just stack services for the sake of it.\n\nPersonally, I ended up using Gcore for a similar setup because I wanted CDN + edge protection in front of APIs without dealing with too much AWS configuration overhead. It felt simpler to manage while still giving the edge security and performance benefits.",
          "score": 1,
          "created_utc": "2026-02-25 23:43:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rc92pf",
      "title": "CDK + CodePipeline: How do you handle existing resources when re-deploying a stack?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rc92pf/cdk_codepipeline_how_do_you_handle_existing/",
      "author": "Hungry_Assistant6753",
      "created_utc": "2026-02-23 05:36:51",
      "score": 10,
      "num_comments": 7,
      "upvote_ratio": 0.86,
      "text": "We have an AWS CDK app deployed via CodePipeline. Our stack manages DynamoDB tables, Lambda functions, S3 buckets, and SageMaker endpoints.\n\n\n\n**Background**: Early on we had to delete and re-create our CloudFormation stack a few times due to deployment issues (misconfigured IAM, bad config, etc). We intentionally kept our DynamoDB tables and S3 buckets alive by setting RemovalPolicy.RETAIN. we didn't want to lose production data just because we needed to nuke the stack.\n\n**The problem**: When we re-deploy the stack after deleting it, CloudFormation tries to CREATE the tables again but they already exist. It fails. So we added a context flag `--context import-existing-tables=true` to our cdk synth command in CodePipeline, which switches the table definitions from new dynamodb.Table(...) to dynamodb.Table.from\\_table\\_name(...). This works fine for existing tables.\n\nNow, we added a new DynamoDB table. It doesn't exist yet anywhere. But the pipeline always passes `--context import-existing-tables=true`, so CDK tries to import a table that doesn't exist yet it just creates a reference to a non-existent table. No error, no table created.\n\n**Current workaround**: We special-cased the new table to always create it regardless of the flag, and leave the old tables under the import flag. But this feels fragile every time we add a new table we have to remember to handle this manually.\n\n**The question**: How do you handle this pattern cleanly in CDK? **Is there an established pattern for \"create if not exists, import if exists\"** that works in a fully automated",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rc92pf/cdk_codepipeline_how_do_you_handle_existing/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6wmzp0",
          "author": "deviled-tux",
          "text": "you leave the code alone and have it create the stuff \n\ncdk import to import them \n\nThis does not seem like something you should be automating because this shouldn’t be happening that often \n",
          "score": 17,
          "created_utc": "2026-02-23 05:41:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6won7x",
          "author": "cachemonet0x0cf6619",
          "text": "1. split stacks by the volatility of the resources. think stateful and stateless. \n\n2. use multiple accounts to split your resources so you’re not developing against production resources. \n\n3. if you can’t split accounts add an environment to the stack name and use that to separate dev and prod. \n\n4. don’t do that auto context thing. that’s a code smell. you shouldn’t need that with the advice above. \n\n5. bonus: i would never use code pipeline. instead i’d use github actions using a self hosted runner in my aws account. way easier and that skill is transferable.",
          "score": 10,
          "created_utc": "2026-02-23 05:54:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wspth",
          "author": "DoINeedChains",
          "text": "The joys of having CDK requirements forced upon you in a heavy singleton datasource environment",
          "score": 2,
          "created_utc": "2026-02-23 06:29:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wpxp1",
          "author": "Conscious-Title-226",
          "text": "This is why I don’t like CDK to be honest. It ties you into CloudFormation which is just painful when things like this go wrong.\n\nCF just fundamentally doesn’t provide enough support around state management. You need to engineer around its limitations and/or have immutable infrastructure.\n\nWould you be able to rename your stack resource (so it is a new CF stack with a new s3 and dynamodb resource) and then just migrate the data? That’d be faster if it’s not a lot to migrate/copy\n\nYou might need to modify your stack to allow you to give them new names, there’ll be uniquely named resources in your stack like KMS aliases, s3 bucket, iam role etc but this is a good reason to make sure all that is configureable through your stack props",
          "score": 3,
          "created_utc": "2026-02-23 06:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wmtts",
          "author": "pausethelogic",
          "text": "I’m sure there’s a way, but this is honestly one of the major downsides of cloudformation and CDK - there’s no proper state management\n\nIt’s part of why terraform is a much more popular IaC tool, it actually has state management for resources so it knows what resources were deleted, which still exist, and how to handle them",
          "score": 3,
          "created_utc": "2026-02-23 05:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wq4jx",
              "author": "cachemonet0x0cf6619",
              "text": "“can you fix my Honda’s transmission?”\n\n“nah, mate. but if you’d have gotten a bike you wouldn’t need a new transmission”\n\nthat’s how stupid your reply sounds",
              "score": -7,
              "created_utc": "2026-02-23 06:07:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7b0va0",
          "author": "iamtheconundrum",
          "text": "Naming resources explicitly will result in naming collisions. Also, you should put resources like dynamodb tables in a separate stack so you can nuke other stacks without affecting the persistent parts of your architecture.",
          "score": 1,
          "created_utc": "2026-02-25 10:55:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbfv2w",
      "title": "Cloudwatch alarms mute rules",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rbfv2w/cloudwatch_alarms_mute_rules/",
      "author": "becharaerizk",
      "created_utc": "2026-02-22 07:47:17",
      "score": 8,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "Hello,\n\nI wanted to implement some kind of maintenance mode on alarms i have setup on my work's awa account. Right before i started i saw WA released alarms mute rules which do exactly what i want.\n\nIt works well using the console but i want to write a function that takes a specific string and created mute rules with all alarms containing this string. (For automated workflows and such)\n\nI noticed that neither the cli nor the python sdk support this yet, when are mute rules supposed to be released for cli or boto3 in python?\n\n  \nFeature I am speaking about: [Configure alarm mute rules](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/alarm-mute-rules-configure.html)\n\n  \nChecking [boto3's latest documentation](https://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch.html): no mention of this feature",
      "is_original_content": false,
      "link_flair_text": "monitoring",
      "permalink": "https://reddit.com/r/aws/comments/1rbfv2w/cloudwatch_alarms_mute_rules/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6qofga",
          "author": "Refwah",
          "text": "\nhttps://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch/client/describe_alarms.html\n\nhttps://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch/client/disable_alarm_actions.html",
          "score": 1,
          "created_utc": "2026-02-22 08:20:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qp3he",
              "author": "becharaerizk",
              "text": "Im not talking about disable alarm actions, this doesnt help my case im speaking about [Configure alarm mute rules](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/alarm-mute-rules-configure.html)",
              "score": 1,
              "created_utc": "2026-02-22 08:26:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6qq2pd",
                  "author": "Refwah",
                  "text": "If you set the state:\n\nhttps://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch/client/set_alarm_state.html\n\nAnd then immediately disable actions \n\nYou effectively get a mute\n\nYou then re-enable actions and you un mute the alarm\n\nYou may want to set them back to not alarming to in order to trigger the action again",
                  "score": 1,
                  "created_utc": "2026-02-22 08:35:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6r9jz0",
          "author": "sandro-_",
          "text": "Isn't that the API: https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_PutAlarmMuteRule.html\n\nTo effectively create a mute rule for a specific time?\n\nAnd in MuteTargets you can target which alarms to mute?",
          "score": 1,
          "created_utc": "2026-02-22 11:41:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rcdfw",
              "author": "becharaerizk",
              "text": "Yes the cli commands provided dont work, they return an error when using then, even when using cloudshell",
              "score": 1,
              "created_utc": "2026-02-22 12:06:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rj9vh",
                  "author": "KayeYess",
                  "text": "Make sure one of the associated IAM policies for the user/role has the required permissions for cloudwatch:PutAlarmMuteRule",
                  "score": 1,
                  "created_utc": "2026-02-22 12:59:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rwdon",
                  "author": "Flakmaster92",
                  "text": "Make sure you’re also running the absolute latest version of the CLI, too many times I’ve seen people saying “it didn’t work” when they’re running a CLI version from before the feature came out",
                  "score": 1,
                  "created_utc": "2026-02-22 14:20:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6sitd6",
          "author": "ruibranco",
          "text": "The native mute rules feature is console-only for now — SDK/CLI support is usually added a few months after console launch. In the meantime, the disable\\_alarm\\_actions + set\\_alarm\\_state approach works but is fragile for automated workflows. Another option worth considering: EventBridge Scheduler to run a Lambda that toggles alarm actions on/off around your maintenance windows, which is more auditable and easier to manage at scale than per-alarm state manipulation.",
          "score": 1,
          "created_utc": "2026-02-22 16:09:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6txbhe",
              "author": "crh23",
              "text": "Months would be surprising - I'd expect days",
              "score": 1,
              "created_utc": "2026-02-22 20:05:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6syb93",
              "author": "becharaerizk",
              "text": "A few months for a simple api call from python or the cli? Any programmer could finish that in a week without the use of ai lol",
              "score": 0,
              "created_utc": "2026-02-22 17:19:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6z9xla",
          "author": "liverdust429",
          "text": "The API exists (PutAlarmMuteRule) but SDK/CLI support always lags behind console launches. Check your CloudShell CLI version with `aws --version`, it's probably outdated. If it's still not there on the latest version you can always hit the API directly with SigV4 signing until boto3 catches up. Annoying but it works.",
          "score": 1,
          "created_utc": "2026-02-23 16:56:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7988j4",
          "author": "crh23",
          "text": "It has now released in [the CLI](https://github.com/aws/aws-cli/blob/v2/CHANGELOG.rst) and [boto3](https://github.com/boto/boto3/blob/develop/CHANGELOG.rst)",
          "score": 1,
          "created_utc": "2026-02-25 02:33:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79mfbr",
              "author": "becharaerizk",
              "text": "Finally lol",
              "score": 2,
              "created_utc": "2026-02-25 03:55:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9r4il",
      "title": "How to guarantee consistency when deleting items from dynamodb?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1r9r4il/how_to_guarantee_consistency_when_deleting_items/",
      "author": "Select_Extenson",
      "created_utc": "2026-02-20 09:45:29",
      "score": 7,
      "num_comments": 26,
      "upvote_ratio": 0.9,
      "text": "Let's say I want to delete 100000 items from dynamodb, what is the best approach to delete \"all-or-nothing\", TransactWriteItems only support 100 items, so I don't want to cause inconsistency in my data if for some reason the delete function fails alongs the way.\n\nAnd in my case, I simply couldn't find a solution to implement it with GSI, so the only solution for me is to delete them manually.",
      "is_original_content": false,
      "link_flair_text": "database",
      "permalink": "https://reddit.com/r/aws/comments/1r9r4il/how_to_guarantee_consistency_when_deleting_items/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6e9dq3",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'database'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+database).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-20 09:45:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ee2vq",
          "author": "pint",
          "text": "this is a problem you should not have. the requirement itself screams badly that something is really wrong there, and you should seriously reconsider.\n\nwithout knowing more about the problem, here is a theoretical solution.\n\n1. add a new data field e.g. \"obsolete\" to the records, optionally add a ttl too\n1. modify the software to obey that field\n1. deploy the software, which means at an instant all the records are now \"gone\"\n1. let the ttl delete the records, or delete them manually at your convenience\n\nstep 2 is the most problematic, because the \"software\" might be a dozen different systems, and they might rely heavily on the assumption that queries will return rows in a timely manner, which is now not guaranteed.\n\nsuch operations have to be considered *in advance* with dynamodb.",
          "score": 13,
          "created_utc": "2026-02-20 10:28:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6eeyt9",
              "author": "Select_Extenson",
              "text": "I think my mistake is I shouldn't use dynamodb and use relational databases instead, the project I'm working on contains a lot of related data and I need to gunaratne consistency across them.\n\nIt was my first time using it, can you please tell me your opinion on this? is it actually a bad choice to use dynamodb when you have a project with a lot of related that or is it just me that I didn't design my database properly? but I don't think I did design it poorly, I tried my best to design in the most optimal way but it misses flexibility when it comes to querying and manipulating related data.",
              "score": 1,
              "created_utc": "2026-02-20 10:36:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6eg08o",
                  "author": "xtraman122",
                  "text": "There’s a way to do just about any pattern with Dynamo, but it often requires lots of careful planning around indexes and keys. What’s really rough with Dynamo is having  changes to the access patterns and relationships down the road. \n\nLots of people end up in the same situation as you, choosing a NoSQL DB because they think it’s cool, solves all their problems, or just heard about it too much in blog posts and conferences (Like your CTO probably did…). Unless you have very high throughout in both reads and writes that a traditional relational DB can’t handle, it’s likely not actually necessary for your use case.",
                  "score": 7,
                  "created_utc": "2026-02-20 10:45:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6eglzf",
                  "author": "RecordingForward2690",
                  "text": "Without knowing the details, but just going on what you say here, I agree that your problem screams \"Relational Database\" to me. DynamoDB is simply not designed or intended for your problem.\n\nFor all practical purposes, if you use DynamoDB for something like you describe, you will be writing a custom layer that tries to give you Relational Database functionality (multi-table queries, ACID compliance across multi-table operations and such) on top of DynamoDB. That has already been done, and it's called a Relational Database.\n\nDynamoDB shines when you just have a handful of tables, need extremely high performance at any scale, and are able to live with the loss of ACID compliance - or are willing to write additional code to get some of that ACID compliance back.",
                  "score": 6,
                  "created_utc": "2026-02-20 10:51:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6egq5d",
                  "author": "pint",
                  "text": "to be honest, relying rdbms referential integrity for such huge operations is also not recommended. it is bad design there too, even if at least possible.\n\ni advocate for separation of data. in the old days, we just dumped everything in \"the database\", because where else data would go, right? so different types of data ended up there, configuration, users and privileges, transactions, logs, web sessions, temporary data. all these data types have very different usage patterns, and probably shouldn't be in the same database.\n\none nice pattern is to keep operational data in dynamodb, and use dynamodb streams to deliver historic data to s3 or a rdbms for statistical analysis. meanwhile, keep configuration in ssm, user data maybe in whatever authentication tool you are using, logs in cloudwatch.\n\nrely more on program logic when aggregating data from different sources (as opposed to sql).\n\nwhen it comes to referential integrity, ask yourself the question: can we somehow get away without it? can be employ a little bit of cleverness or extra code to not have to deal with it?",
                  "score": 4,
                  "created_utc": "2026-02-20 10:52:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6eioux",
                  "author": "SonOfSofaman",
                  "text": "I think almost everyone goes through what you're going through. It's sort of a rite of passage with DynamoDB.\n\nDynamoDB is, as you know, very different from relational databases. With DynamoDB it is imperative that you fully understand all of your access patterns ahead of time, then model the table accordingly. If your access patterns change, then you may need to remodel your table.\n\nWith relational databases, you don't need to fully understand the access patterns ahead of time. It helps to know your access patterns ahead of time, but relational databases are flexible and adaptable.\n\nYou can do what you want with DynamoDB, but the table in its current form wasn't designed to support this \"bulk delete\" access pattern.\n\nI think you're at a point where you need to do a bit of redesign. Some of the other comments have practical solutions that may be helpful.",
                  "score": 1,
                  "created_utc": "2026-02-20 11:09:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6eg7sl",
          "author": "RecordingForward2690",
          "text": "I had a somewhat similar problem, where DynamoDB would collect millions of transactions, and at some point in time all transactions (within a Partition Key set) older than a particular timestamp needed to be deleted/invalidated/ignored as a whole. That timestamp was not known in advance - it depended on a user action - so I could not use the TTL mechanism. Like you noticed, there is no way to do that directly in a consistent manner.\n\nSome of the other proposals, where you update each item with a deleted=true or other attribute, suffer from the same problem: You need to either update or delete a large number of items in bulk, and the bulk operations in the DynamoDB are simply too limited in the number of items they can handle in one go, and in an atomic way.\n\nIn my case, my transactions fortunately were timestamped, and the transactions that needed to be deleted from the table were all done before a particular timestamp - the time of that user action. So I added an additional table \"DeleteMarkers\". As soon as the user event happened, I entered the timestamp of the Delete event into the DeleteMarkers, with the same partition key as the transaction table. Now, instead of doing one query to the TransactionsTable, I had to do two queries:\n\n1. Query to the DeleteMarkers table to get the up-to-date DeleteMarker\n2. Query to the TransactionTable to get the transactions, with the limitation that the results returned should be newer than the DeleteMarker.\n\nDynamoDB is quick enough that the additional query time did not impact latency.\n\nAfter this, I could delete the old transactions from the TransactionTable in the background. I used the TTL mechanism for that, but you can also do a query or even a scan if you want to. This is not time-critical or transaction-critical anymore.\n\nFrom a design point of view, DeleteMarkers has the SessionID as my Partition Key, no Sort Key (so when queried it returns one item only), and a field \"Timestamp\". TransactionTable has the SessionID as my Partition Key, and the Timestamp as the Sort Key.\n\nIn my application I did not need to do any other queries so I did not need any GSIs.\n\nFor your application, if you are able to formulate a SQL-query-like-thing that would be able to delete the \\~1M items, then you can also put the variables of that query in a similar DeleteMarkers table, and use those fields in your query to your TransactionTable. It's a bit more complex than the Timestamps I used, but not impossible.",
          "score": 3,
          "created_utc": "2026-02-20 10:47:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ebxoi",
          "author": "safeinitdotcom",
          "text": "DynamoDB just doesn't support this natively at that scale. You can either:\n\n\\- add a`deleted=true` attribute or smth like that, filter it in queries and clean up later.\n\n\\- BatchWriteItems but log which batches completed somewhere, on failure you resume instead of starting over.\n\nIf you need true all-or-nothing for 100k records, that's a relational DB problem. DynamoDB isn't designed for it.",
          "score": 2,
          "created_utc": "2026-02-20 10:08:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6echoq",
              "author": "Select_Extenson",
              "text": ">If you need true all-or-nothing for 100k records, that's a relational DB problem. DynamoDB isn't designed for it.\n\nYeah, that's the a mistake, I got told to use Dynamodb by our CTO, I had no experience with it so I didn't know the props and downs for it, after months of struggling trying to build our project that is mostly contains a lot of related data in dynamodb, it became really pain in the ass to maintain it",
              "score": 3,
              "created_utc": "2026-02-20 10:14:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6eykkr",
                  "author": "KainMassadin",
                  "text": "> I got told to use Dynamodb by our CTO\n\nBeen there, sucks.",
                  "score": 5,
                  "created_utc": "2026-02-20 13:03:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mqa1u",
                  "author": "csharpwarrior",
                  "text": "A lot of people just hear how cool DynamoDB is and they don’t spend enough time understanding the use cases it solves. And more importantly understanding the use cases it is bad at.",
                  "score": 1,
                  "created_utc": "2026-02-21 17:24:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6eb1vf",
          "author": "ebykka",
          "text": "This is one of the reasons why, after six years of using DynamoDB, we decided to give it up and migrate to RDS Aurora.\n\nWhile it was great for prototyping, maintenance, usage and consistency slowly became increasingly problematic.",
          "score": 1,
          "created_utc": "2026-02-20 10:00:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6elidc",
          "author": "SonOfSofaman",
          "text": "Are you able to ignore the unwanted items at read-time instead of deleting them? If those unwanted items have some common value upon which you can filter when you perform a read, to your consumer the items will be as good as deleted.\n\nThen you can casually delete the items in the background in batches or one by one at your leisure.",
          "score": 1,
          "created_utc": "2026-02-20 11:32:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f3con",
          "author": "garrettj100",
          "text": "You don’t want carpet, you want an area rug.  And when I say “carpet” and “area rug” I mean “DynamoDB” and “RDS”.\n\nYou’re asking how to do a transaction and that means a relational database.  That’s why they exist, because 40 years ago banks needed transactions.\n",
          "score": 1,
          "created_utc": "2026-02-20 13:31:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ft8jd",
              "author": "SpecialistMode3131",
              "text": "Maybe.  If you have 100 use cases for a table that are ideal for nosql (different schemas etc etc), and one use case requiring a transaction, do you instantly reject nosql?  Or do you hack around that one case?  Opinions and outcomes vary.\n\nOP didn't provide nearly enough business context to seriously decide one way or another.  \"The CTO told me to do it this way\" can be taken either way.",
              "score": 1,
              "created_utc": "2026-02-20 15:43:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fbk2e",
          "author": "solo964",
          "text": "How do you currently identify the items to be deleted? For example, are they collections of items where each item in a given collection has a common primary key? If you can identify them simply e.g. all items with a PK in the set { customer#12, customer#479, customer#90210 } then you could soft delete them by writing these PKs to control records e.g. an item with (PK = \"customer#12\", SK = \"control\", status = \"deleted\") and then modify the consumers of the table to first check if the given PK/control item was present with status=\"deleted\". Independently, have an async process that slowly deletes the actual items in batches, eventually deleting the control item.",
          "score": 1,
          "created_utc": "2026-02-20 14:15:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6l7rwi",
          "author": "GeorgeMaheiress",
          "text": "If the delete fails, retry until it succeeds. This doesn't have to be a problem.",
          "score": 1,
          "created_utc": "2026-02-21 12:07:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6oewe0",
          "author": "rexspook",
          "text": "I’m so curious about the use case where 100k records need to be deleted in an all or nothing approach. I think you’ve already got a lot of good answers here. My first thought is soft delete for that portion and then real deletes in some batched cleanup job",
          "score": 1,
          "created_utc": "2026-02-21 22:38:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fseud",
          "author": "SpecialistMode3131",
          "text": "Leaving aside the debate over RDBMS, one option:\n\n1. mark the rows in the main table (A) with a deleted\\_id value (a new attribute, added to all elements to be deleted)\n2. When you're 100% sure you have them all marked, you have achieved transactional consistency the Stone Age way.  Now have a small additional table B you atomically add a row to, saying \"rows in A with this deleted\\_id are to be treated as deleted\".\n3. Modify your code to filter those out/never use them as valid rows (check B and if a row has that attribute, filter it).  (do this step before adding the row to B, if you want a true transactional experience).\n4. Delete the rows from A at leisure.  Then remove the row from B.\n\nNot pretty, but it will easily achieve what you want, and you can keep it around as a management mechanism.",
          "score": 0,
          "created_utc": "2026-02-20 15:39:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e9dpe",
          "author": "AutoModerator",
          "text": "Here are a few handy links you can try:\n\n- https://aws.amazon.com/products/databases/\n- https://aws.amazon.com/rds/\n- https://aws.amazon.com/dynamodb/\n- https://aws.amazon.com/aurora/\n- https://aws.amazon.com/redshift/\n- https://aws.amazon.com/documentdb/\n- https://aws.amazon.com/neptune/\n\nTry [this search](https://www.reddit.com/r/aws/search?q=flair%3A'database'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+database).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": -1,
          "created_utc": "2026-02-20 09:45:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rb6tvu",
      "title": "Registering Partition Information to Glue Iceberg Tables",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rb6tvu/registering_partition_information_to_glue_iceberg/",
      "author": "mike_get_lean",
      "created_utc": "2026-02-22 00:07:23",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I am creating Glue Iceberg tables using Spark on EMR. After creation, I also write a few records to the table. However, when I do this, Spark does not register any partition information in Glue table metadata.\n\nAs I understand, when we use hive, during writes, spark updates table metadata in Glue such as partition information by invoking UpdatePartition API. And therefore, when we write new partitions in Hive, we can get EventBridge notifications from Glue for events such as `BatchCreatePartition`. Also, when we invoke `GetPartitions`, we can get partition information from Glue Tables.\n\nI understand Iceberg works based on metadata and has a feature for hidden partitioning but I am not sure if this is the sole reason Spark is not registering metadata info with Glue table. This is causing various issues such as not being able to detect data changes in tables, not being able to run Glue Data Quality checks on selected partitions, etc.\n\nIs there a simple way I can get this partition change and update information directly from Glue?\n\nOne of the bad ways to do this will be to create S3 notifications, subscribe to those and then run Glue Crawler on those events, which will create another S3 based Glue table with the correct partition information. And then do DQ checks on this new table. I do not like this approach at all because I will need to setup significant automation to achieve this.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rb6tvu/registering_partition_information_to_glue_iceberg/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6qtnje",
          "author": "freerangetrousers",
          "text": "Glue is quite simply built with Hive in mind. Iceberg is more feature rich, but aws haven't built the whole glue ecosystem around it so there isn't feature parity. \nGlue catalogue is just Hive metastore , but iceberg isn't really designed to have to utilise this. \n\n\nInstead you can check for changes on the metadata.json file , or just at the end of your spark scripts emit an event yourself with the details you want to convey. Like in the olden days. ",
          "score": 1,
          "created_utc": "2026-02-22 09:10:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ralvuk",
      "title": "36hr+ Downtime - Response Required: Your Account is on Hold - Need Help",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1ralvuk/36hr_downtime_response_required_your_account_is/",
      "author": "Realistic-Lab6157",
      "created_utc": "2026-02-21 08:25:28",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 0.67,
      "text": "I received this 2 days ago, and I have seen other posts too about it.  \nThe pain is that I have all the right documents to get this verification done but the communication process is so slow and confusing which keeps delaying this and my services are still down.\n\nI have already created support tickets and cases but there has been no response for the last 12 hours. I am stuck here and need urgent help. u/AWSSupport\n\nI am not even sure if the verification team works on the weekends which might add 2 more days of downtime.\n\nDoes anyone have any idea on how to get this escalated or prioritized?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1ralvuk/36hr_downtime_response_required_your_account_is/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6kqbq3",
          "author": "Sirwired",
          "text": "Which support plan do you have?",
          "score": 3,
          "created_utc": "2026-02-21 09:21:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6l304i",
              "author": "Realistic-Lab6157",
              "text": "It’s basic. I was trying to upgrade it but it’s not allowing me.",
              "score": 2,
              "created_utc": "2026-02-21 11:24:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1raayhv",
      "title": "Help with cognito: Code security resource quotas not enforced?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1raayhv/help_with_cognito_code_security_resource_quotas/",
      "author": "TutorNeat2724",
      "created_utc": "2026-02-20 23:20:47",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 0.86,
      "text": "Hi everyone,\nI’ve noticed what seems to be unexpected behavior regarding Cognito User Pools code security resource quotas.\nAccording to the [documented limits](https://docs.aws.amazon.com/cognito/latest/developerguide/quotas.html#resource-quotas), certain operations (e.g. GetUserAttributeVerificationCode) should be rate-limited (for example, max 5 consecutive requests). However, in my tests, I’m able to call GetUserAttributeVerificationCode more than 5 times in a row without receiving any throttling error or limit exception.\nHas anyone experienced the same behavior?\nIs there any additional configuration required to enforce these quotas, or are they applied under specific conditions only?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1raayhv/help_with_cognito_code_security_resource_quotas/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6ipsm5",
          "author": "Skytram_",
          "text": "How long are you calling that API above the max TPS for?",
          "score": 1,
          "created_utc": "2026-02-21 00:16:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k1edw",
          "author": "baever",
          "text": "`GetUserAttributeVerificationCode` has a quota of 5 requests/user/hour. The only thing I can think of is that the bucket is based on hour of day. So if you start at 1:59 and make 4 requests and then at 2:01 you make 4 requests (for a total of 8) it won't block you because those span 2 hour buckets. However, if you make 6 requests in the same hour (say between 2:01 and 2:05) it should block you because it's in 1 hour bucket. Are you never seeing it enforced or are you seeing it intermittently enforced?",
          "score": 1,
          "created_utc": "2026-02-21 05:29:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rckoel",
      "title": "CACs in Workspaces",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rckoel/cacs_in_workspaces/",
      "author": "KrazyMuffin",
      "created_utc": "2026-02-23 15:36:47",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Our current AWS workspace setup uses Simple AD, as I couldn't get AD Connector to work (will work on getting this working another time).\n\n\n\nCurrently a Linux workspace (Rocky Linux 8) can use CACs to authenticate to sites in-session, however, on Windows (Windows Server 2022), it doesn't recognize my computer's CAC reader. I have installed ActivID and InstallRoot, the workspace is DCV (formerly WSP).\n\n\n\nThe documentation all talks about how to setup readers with AD Connector so you can log into the workspace with your CAC, but that's not what we're trying to do, just be able to use the reader inside the instance.\n\n\n\nAny suggestions?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rckoel/cacs_in_workspaces/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6zxsei",
          "author": "fjleon",
          "text": "smart card redirection is disabled in windows by default for both presession and insession. you need to enable via GPO\n\nhttps://docs.aws.amazon.com/workspaces/latest/adminguide/group_policy.html#gp_install_template_wsp",
          "score": 2,
          "created_utc": "2026-02-23 18:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75hoqv",
              "author": "KrazyMuffin",
              "text": "Thank you, that worked, I'm dumb 😅",
              "score": 1,
              "created_utc": "2026-02-24 15:38:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rd84jb",
      "title": "Quantum-Guided Cluster Algorithms for Combinatorial Optimization",
      "subreddit": "aws",
      "url": "https://aws.amazon.com/blogs/quantum-computing/quantum-guided-cluster-algorithms-for-combinatorial-optimization/",
      "author": "donutloop",
      "created_utc": "2026-02-24 05:55:27",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1rd84jb/quantumguided_cluster_algorithms_for/",
      "domain": "aws.amazon.com",
      "is_self": false,
      "comments": [
        {
          "id": "o73zfzp",
          "author": "nucleustt",
          "text": "What in god's name is this? ELI5",
          "score": 2,
          "created_utc": "2026-02-24 09:50:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rctlrt",
      "title": "How Does Karpenter Handle AMI Updates via SSM Parameters? (Triggering Rollouts, Refresh Timing, Best Practices)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rctlrt/how_does_karpenter_handle_ami_updates_via_ssm/",
      "author": "LemonPartyRequiem",
      "created_utc": "2026-02-23 20:52:45",
      "score": 4,
      "num_comments": 3,
      "upvote_ratio": 0.84,
      "text": "I’m trying to configure Karpenter so a `NodePool` uses an `EC2NodeClass` whose AMI is selected via an SSM Parameter that we manage ourselves.\n\nWhat I want to achieve is an automated (and controlled) AMI rollout process:\n\n* Use a Lambda (or another AWS service, if there’s a better fit) to periodically fetch the latest AWS-recommended EKS AMI (per the AWS docs: [https://docs.aws.amazon.com/eks/latest/userguide/retrieve-ami-id.html](https://docs.aws.amazon.com/eks/latest/userguide/retrieve-ami-id.html)).\n* Write that AMI ID into *our own* SSM Parameter Store path.\n* Update the parameter used by our **test** cluster first, let it run for \\~1 week, then update the parameter used by **prod**.\n* Have Karpenter automatically pick up the new AMI from Parameter Store and perform the node replacement/upgrade based on that change.\n\nWhere I’m getting stuck is understanding how `amiSelectorTerms` works when using the `ssmParameter` option (docs I’m referencing: [https://karpenter.sh/docs/concepts/nodeclasses/#specamiselectorterms](https://karpenter.sh/docs/concepts/nodeclasses/#specamiselectorterms)):\n\n* How exactly does Karpenter resolve the AMI from an `ssmParameter` selector term?\n* When does Karpenter re-check that parameter for changes (only at node launch time, periodically, or on some internal resync)?\n* Is there a way to force Karpenter to re-resolve the parameter on a schedule or on demand?\n* What key considerations or pitfalls should I be aware of when trying to implement AMI updates this way (e.g., rollout behavior, node recycling strategy, drift, disruption, caching)?\n\nThe long-term goal is to make AMI updates as simple as updating a single SSM parameter: update test first, validate for a week, then update prod letting Karpenter handle rolling the nodes automatically.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rctlrt/how_does_karpenter_handle_ami_updates_via_ssm/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o72r5s3",
          "author": "sunra",
          "text": "Your best bet is going to be to read the source.\n\nBut my understanding is that it is the Karpenter controller itself which monitors the SSM parameter (not the nodes themselves). When the controller notices that some nodes don't match the parameter it will mark the nodes as \"drifted\", and the replacements will happen according to your node-pool disruption-budget and node-termination-grace-period.\n\nI don't know this for sure - it's my expectation based on how Karpenter handles other changes (like k8s control-plane upgrades).",
          "score": 1,
          "created_utc": "2026-02-24 03:42:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73gyvq",
          "author": "yuriy_yarosh",
          "text": "1. You'll need to enable drift detection so it'll actually resync SSM   \n[https://karpenter.sh/docs/reference/settings/#feature-gates](https://karpenter.sh/docs/reference/settings/#feature-gates)\n\n2. SSM itself is throttled [https://github.com/aws/karpenter-provider-aws/issues/5907](https://github.com/aws/karpenter-provider-aws/issues/5907)  \nResync was 5 min before contributing to CNCF (reconcil cycle period for the whole controller), but now it's hardcoded to start checking only after an hour  \n[https://github.com/kubernetes-sigs/karpenter/blob/main/designs/drift.md](https://github.com/kubernetes-sigs/karpenter/blob/main/designs/drift.md)  \n[https://github.com/kubernetes-sigs/karpenter/blob/main/pkg/controllers/nodeclaim/disruption/drift.go#L93](https://github.com/kubernetes-sigs/karpenter/blob/main/pkg/controllers/nodeclaim/disruption/drift.go#L93)  \n[https://github.com/aws/karpenter-provider-aws/blob/main/pkg/cloudprovider/cloudprovider.go#L281](https://github.com/aws/karpenter-provider-aws/blob/main/pkg/cloudprovider/cloudprovider.go#L281)  \n\n\nSpot instances require SQS interruption queue `--interruption-queue`  \n[https://karpenter.sh/docs/concepts/disruption/#interruption](https://karpenter.sh/docs/concepts/disruption/#interruption)\n\n3. No, on-demand... \n\nYeah, been there, Karpenter is all over the place so wrote a custom cluster autoscaler with a Terraform provider and Kamaji, to keep infra state consistent, synchronized, and in one place.",
          "score": 1,
          "created_utc": "2026-02-24 06:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bbcgk",
          "author": "EcstaticJellyfish225",
          "text": "Consider using TAGs for the AMI selector, you should be able to tag AWS provided (and your own) AMIs.  Then you can pre-test an AMI in your dev account, once you are happy with it, you can tag the same AMI in your prod account and it will become available for karpenter to pick up next time a new node is needed (or if using drift detection at any time your disruption budget allows).\n\nAutomating the test cycle and tagging AMIs that pass the test, is also pretty straight forward. Test in a dev account, if the AMI asses the test, by some means tag the same AMI in your prod account. (Maybe setup an SNSTopic triggering a lambda, or something similar).",
          "score": 1,
          "created_utc": "2026-02-25 12:19:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rb4r7y",
      "title": "Initiating App Studio - Entity Already Exists Error",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rb4r7y/initiating_app_studio_entity_already_exists_error/",
      "author": "haonconstrictor",
      "created_utc": "2026-02-21 22:38:59",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Every time I try to setup App Studio on my new account I receive the “Entity Already Exists” error no matter what I do. I’ve confirmed a group exists in IAM and it’s not duplicated anywhere. Pulling my hair out trying to even get off the ground with AWS and can’t figure this out. I’ve deleted and rebuilt the IAM multiple times, and still nothing. Any advice is appreciated! ",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rb4r7y/initiating_app_studio_entity_already_exists_error/",
      "domain": "self.aws",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rbz0rw",
      "title": "Track Karpenter efficiency of cluster bin-packing over time with kube-binpacking-exporter",
      "subreddit": "aws",
      "url": "https://github.com/sherifabdlnaby/kube-binpacking-exporter",
      "author": "SherifAbdelNaby",
      "created_utc": "2026-02-22 21:59:10",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "technical resource",
      "permalink": "https://reddit.com/r/aws/comments/1rbz0rw/track_karpenter_efficiency_of_cluster_binpacking/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1ra6gcu",
      "title": "Help — Can’t delete CloudFront distribution and I’m scared of a huge bill",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1ra6gcu/help_cant_delete_cloudfront_distribution_and_im/",
      "author": "SignalDrive3667",
      "created_utc": "2026-02-20 20:25:35",
      "score": 4,
      "num_comments": 7,
      "upvote_ratio": 0.75,
      "text": "I created a CloudFront distribution for learning purposes. I’m now trying to clean up my AWS account and delete all resources, but CloudFront won’t let me delete the distribution.\n\nError message:\n“Failed to delete distribution: You can't delete this distribution while it's subscribed to a pricing plan. After you cancel the pricing plan, you can delete the distribution at the end of the monthly billing cycle.”\n\n**The confusing part:** I selected the Free Tier flat-rate plan, so I don’t understand what pricing plan it’s referring to.\n\nWhy am I unable to delete it? Is there something else I need to disable or cancel first?\n\nI’m worried about unexpected charges and want to make sure everything is fully removed. Any guidance on how to safely delete the distribution would be appreciated.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1ra6gcu/help_cant_delete_cloudfront_distribution_and_im/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6hir8f",
          "author": "pausethelogic",
          "text": "Reach out to AWS account and billing support. They’ll be able to see what’s in your account and how to remove it so you’re not charged",
          "score": 2,
          "created_utc": "2026-02-20 20:28:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hjv0v",
          "author": "Opening-Concert826",
          "text": "See the cancel pricing plan section here: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/flat-rate-pricing-plan.html#manage-your-pricing-plans",
          "score": 2,
          "created_utc": "2026-02-20 20:34:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hrtdz",
          "author": "vppencilsharpening",
          "text": "The flat-rate plans are somewhat new, so I'd expect that there is not a lot of great information if your searching the internet.\n\nu/Opening-Concert826 linked the AWS doc that explains how to remove the pricing plan. However there is a chance that you still may need to wait until the end of the billing cycle for the \\[free\\] flat-rate plan to be removed.\n\nTo ensure that the distribution is NOT used (and not incurring costs), you should be able to disable it from the Distributions list in the console. Just check the box next to the distribution and click \\[Disable\\]. Do this in addition to removing the flat-rate plan. \n\nIf you created this to play around with and never publicly advertised the endpoint then it's unlikely you are going to get hit with much usage and it will all likely fall under the free tier.",
          "score": 2,
          "created_utc": "2026-02-20 21:13:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jxsy8",
          "author": "WhoseThatUsername",
          "text": "Pretty sure the 'Free Tier flat-rate plan' is considered a pricing plan, though unclear as to how you'd remove it.",
          "score": 1,
          "created_utc": "2026-02-21 05:01:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ke6fb",
          "author": "SilentPugz",
          "text": "Just a thought, to delete cloudfront you must perform the step to change the stage before the delete. Disabled must be in state.\n\nEdit : state*  # replace stage.",
          "score": 1,
          "created_utc": "2026-02-21 07:21:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6izc92",
          "author": "llima1987",
          "text": "In addition to the other answers, the safest way to kill your spending on AWS is to close the entire account. You can always create a new one.",
          "score": 1,
          "created_utc": "2026-02-21 01:12:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1reb3q4",
      "title": "AWS Backup Jobs with VSS Errors",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1reb3q4/aws_backup_jobs_with_vss_errors/",
      "author": "Budget-Industry-3125",
      "created_utc": "2026-02-25 11:49:03",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Good morning guys,\n\n  \nI've set up AWS Backup Jobs for many of my EC2 Instances. \n\nThere are 20 VMs enabled for backing up their data to AWS, but somehow 9 of them are presenting the following errors:\n\nWindows VSS Backup Job Error encountered, trying for regular backup\n\nI have tried re-installing the backup agent in the vms and updating, but it doesn't seem to be working out. \n\nUpon connecting to the machines, I'm able to find some VSS providers in failed states. However, after restarting them and verifying that they are OK, the job fails again with the same error message.\n\n  \nHas anyone encountered this behaviour before?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1reb3q4/aws_backup_jobs_with_vss_errors/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7bn6tp",
          "author": "brile_86",
          "text": "  \nCheck the pre-reqs  \n[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/application-consistent-snapshots-prereqs.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/application-consistent-snapshots-prereqs.html)\n\nTLDR:\n\n* Windows Server 2016\n* .NET Framework version 4.6 or later\n* Windows PowerShell major version 3, 4, or 5 with language mode set to FullLanguage\n* AWS Tools for Windows PowerShell version [3.3.48.0](http://3.3.48.0) or later\n* IAM policy AWSEC2VssSnapshotPolicy (or equivalent permissions) attached (make sure you don't have any restrictive SCP or IAM Policy Boundaries blocking it)\n\n  \nAlso some instances are not supported as too small  \n[https://docs.aws.amazon.com/aws-backup/latest/devguide/windows-backups.html](https://docs.aws.amazon.com/aws-backup/latest/devguide/windows-backups.html)\n\n* t3.nano\n* t3.micro\n* t3a.nano\n* t3a.micro\n* t2.nano\n* t2.micro\n\n",
          "score": 3,
          "created_utc": "2026-02-25 13:33:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bjqdc",
          "author": "ReturnOfNogginboink",
          "text": "This is a Windows issue, not an AWS issue. The error is coming from the Windows volume snapshot service.",
          "score": 1,
          "created_utc": "2026-02-25 13:13:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cm51j",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-02-25 16:26:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7co5m1",
              "author": "gex80",
              "text": "Why would they contact Veeam support?",
              "score": 1,
              "created_utc": "2026-02-25 16:35:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}