{
  "metadata": {
    "last_updated": "2026-01-01 10:29:57",
    "time_filter": "week",
    "subreddit": "aws",
    "total_items": 35,
    "total_comments": 255,
    "file_size_bytes": 317900
  },
  "items": [
    {
      "id": "1pupr14",
      "title": "Do you feel terraform is quicker than cdk?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pupr14/do_you_feel_terraform_is_quicker_than_cdk/",
      "author": "rafaturtle",
      "created_utc": "2025-12-24 15:20:44",
      "score": 78,
      "num_comments": 123,
      "upvote_ratio": 0.92,
      "text": "I'm onboarding a new developer and he noticed our pipeline was taking a bit longer he would expect. He than mentioned terraform would have been quicker? Any known explanation?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pupr14/do_you_feel_terraform_is_quicker_than_cdk/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nvqcgyo",
          "author": "KHANDev",
          "text": "There is an architectural difference between CDK and terraform that makes CDK slower. The CDK code you write has to be synthesized to a static cloudformation template before it is executed. Where as terraform is simply executing the changes by calling the api's at apply time.\n\nCDK creates a static cloudformation template. The cloud formation is treated as a transaction and has stronger roll back guarentees. Terraform is far more flexible with what you can express, it is incremental deployment which is why it is often noticeably quicker",
          "score": 120,
          "created_utc": "2025-12-24 15:41:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsh8my",
              "author": "wunderspud7575",
              "text": "Cloudformation is actually the slow part of CDK deployments.\n\nAnd oh how I wish those roll back guarantees were worth anything.",
              "score": 43,
              "created_utc": "2025-12-24 22:59:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvskdhl",
                  "author": "rafaturtle",
                  "text": "This thing of being transactional is key. Yes. May take longer. But for me is a show stopper. \nSo if you deploy a stack in TF and something at the end fails, the state is left inconsistent?",
                  "score": 3,
                  "created_utc": "2025-12-24 23:20:54",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvsc0o4",
              "author": "witty82",
              "text": "This is very highly voted but synthesizing doesn't take very long in my experience. The more relevant difference, I think, is that the underlying technology, i.e. CloudFormation isn't that quick, and TF tends to bw somewhat quicker.",
              "score": 18,
              "created_utc": "2025-12-24 22:25:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvu99ct",
                  "author": "LaSalsiccione",
                  "text": "Agreed. The slow part is not the synthesis. The slow part is always Cloudfront and if you don‚Äôt have Cloudfront in your stack it‚Äôs probably just punishing you for not needing Cloudfront.",
                  "score": 1,
                  "created_utc": "2025-12-25 07:27:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvulfye",
                  "author": "CSI_Tech_Dept",
                  "text": "Yeah, TF in addition to calling the API directly, it also spins up multiple threads (and you can increase their number) to execute operations.\n\nCF was provided by AWS so they cared more about not hammering their own services. So it is much less aggressive.",
                  "score": 1,
                  "created_utc": "2025-12-25 09:39:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvr3z8w",
              "author": "HgnX",
              "text": "I hate CF but at least I don‚Äôt need to manage the state myself",
              "score": -4,
              "created_utc": "2025-12-24 18:09:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvr4vi0",
                  "author": "notospez",
                  "text": "You don't have to manage it with Terraform either.",
                  "score": 22,
                  "created_utc": "2025-12-24 18:14:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvqa2yl",
          "author": "Capable_Dingo_493",
          "text": "It is quicker no question about it. Is it better? That‚Äôs another topic and depends more on your preferences",
          "score": 52,
          "created_utc": "2025-12-24 15:28:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqr7xh",
              "author": "booi",
              "text": "But the answer is yes, it‚Äôs better.",
              "score": 32,
              "created_utc": "2025-12-24 17:00:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvrg4vw",
                  "author": "bman654",
                  "text": "Well it actually depends. Sometimes the answer is hell yes.",
                  "score": 20,
                  "created_utc": "2025-12-24 19:15:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvwg93c",
              "author": "kilobrew",
              "text": "It depends. If you are only in AWS,  CDK has TONS of L2 and L3 objects that terraform doesn‚Äôt. \n\nFor example declaring nodejs lambda in CDK is a one liner. In terraform is like 80 lines.",
              "score": 2,
              "created_utc": "2025-12-25 18:08:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvs5172",
              "author": "vincentdesmet",
              "text": "you can have both with https://terraconstructs.dev\n\naltho CFN does a ‚Äúall or nothing‚Äù approach with automated rollback (which can get stuck) and CFN comes with runners (hosted by AWS)\n\nI find TF speed and ability to manage state yourself quite powerful when combined with AWSCDK level L2 constructs",
              "score": -3,
              "created_utc": "2025-12-24 21:41:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvvg6tj",
              "author": "Sensitive-Ad1098",
              "text": "If you enjoy dealing with stacks stuck in a weird state, you can't really fix yourself (besides deleting the whole stack, which is not always possible right away), than CloudFormation is definitely better",
              "score": 0,
              "created_utc": "2025-12-25 14:25:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvqrz3c",
          "author": "Davidhessler",
          "text": "Quicker in which sense? In terms of onboarding, it depends more on how clean and complex the code base is, the documentation you have, and the processes you‚Äôve built.\n\nIn terms of time to deploy, terraform wins every time. Terraform doesn‚Äôt perform parameter or state checks before it tries to call provider apis. This makes terraform apply really fast. But if something breaks during deployment, it‚Äôs slower to troubleshoot. And if your state breaks‚Ä¶\n\nIt terms of development velocity, CDK probably wins most of the time. CDK‚Äôs abstractions reduces the number of lines of code, complexity, and number of resources you need to configure. Of course I‚Äôm sure someone got some codebase in terraform that is so elegant or the deployments are so simple that CDK isn‚Äôt faster.",
          "score": 11,
          "created_utc": "2025-12-24 17:04:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvuk3ap",
              "author": "CSYVR",
              "text": "Mostly agree, though 2 points:\n\n\\- If something breaks during TF deployment, of course it takes time to troubleshoot. However that time is only a fraction of the time you will be spending on fixing broken stacks. The tools for TF to fix (import, rm) your state are very good and well documented. Once you know how, TF is a very clear winner.  \n\\- Development velocity of CDK; sure. It's the fastest way to deploy common *application* patterns. However, it sucks for infrastructure, partly because of CFN's limitations. Cross-account/region no go. CFN support is limited for some things so you'll have to live with 92 Lambda functions (that you own and are responsible for). Compliance nightmare.",
              "score": 3,
              "created_utc": "2025-12-25 09:24:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvvi2a1",
              "author": "Sensitive-Ad1098",
              "text": ">But if something breaks during deployment, it‚Äôs slower to troubleshoot. And if your state breaks‚Ä¶\n\nCloudFormation stacks are a pain to troubleshoot because they're slow and tend to end up in a completely broken state. Or do you imply that with CDK abstraction added on top of CloudFormation, it actually works well?\n\n>CDK‚Äôs abstractions reduces the number of lines of code, complexity, and number of resources you need to configure\n\nCDK does not reduce complexity. The imperative nature of CDK gives developers a way to add complex logic for creating the infra. That can very easily blow up the complexity.  \nWith terraform, you are quite limited in that regard. It's true that you can end up with more lines of code, but that is also manageable by using modules and loops.  \nWith both Terraform and CDK there are plenty of opportunities to write terrible code. But in terms of complexity, CDK has no limits and it's very easy to quickly get to unreadable mess",
              "score": 1,
              "created_utc": "2025-12-25 14:39:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvqhfzu",
          "author": "craig1f",
          "text": "Cloud formation is trash. It‚Äôs slow and and everything gets rolled back if you make a mistake. Imagine a huge stack where you made a mistake at the end with r53. Now you have to wait for a 30 minute rollback before you can try again.¬†\n\nCDK is a great evolution of CFN but it‚Äôs still CFN under the hood. So it‚Äôs mostly trash. It is very appealing to devs who dabble in ops.¬†\n\nTerraform is fast, preferred by people who do cloud, and mature. I‚Äôve done CFN, serverless framework, CDK, ansible, and Terraform. I would never use anything but terraform again EXCEPT in specific use cases.¬†\n\n1. When providing a stack for others to use as a service or something. You can‚Äôt assume customers always use terraform or CDK and will have things installed. CFN makes sense in these scenarios.¬†\n\n2. Ansible makes sense when the you need to be able to modify a deployed stack on the fly, after it‚Äôs deployed.¬†\n\nEdit: another thing to point out is that AWS teams are not as cohesive as you‚Äôd imagine. Cloudformation support for each service lays behind API support. Terraform uses the API and tends to often be ahead of CFN on supporting AWS service features.¬†\n\nThis doesn‚Äôt come up much, but it does come up.",
          "score": 40,
          "created_utc": "2025-12-24 16:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqix7d",
              "author": "crh23",
              "text": "`aws cloudformation deploy --disable-rollback`",
              "score": 36,
              "created_utc": "2025-12-24 16:16:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvuo127",
                  "author": "AttentionIsAllINeed",
                  "text": "Love it when people trash talk with\n\n>¬†preferred by people who do cloud\n\n>¬†devs who dabble in ops\n\nBut don‚Äôt even know the tool they dislike at all. First he puts everything in one stack, then complains that rollbacks go in fact rollback. Wtf",
                  "score": 5,
                  "created_utc": "2025-12-25 10:07:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvqqcjp",
              "author": "FunkyMonk92",
              "text": "I've never used terraform before. What happens when a resource fails to create/update? Does it give you options to be able to fix it right away?",
              "score": 4,
              "created_utc": "2025-12-24 16:55:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvqs4w6",
                  "author": "craig1f",
                  "text": "It just stops at that point, maintaining the current state. It fails very quickly, allowing you to fix things very quickly.¬†\n\nThere are only two pain points in terraform that bother me.\n\n1. If you lose your connection for whatever reason, while spinning up RDS, it might not add RDS to the state. This is very frustrating. But it‚Äôs like one line to import the RDS instance into your state, which isn‚Äôt possible with CFN. Being able to remove and import existing resources into and out of your stack is so great.¬†\n\n2. Terraform doesn‚Äôt handle conditionals well. If you want to have a variable to enable or disable a resource, you have to set that resource as an array, with length 0 or 1. It‚Äôs kind of janky. This is my largest gripe.¬†",
                  "score": 8,
                  "created_utc": "2025-12-24 17:05:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvr0qqo",
                  "author": "Right_Note1305",
                  "text": "No it just stops in whatever broken ass state lol.",
                  "score": 5,
                  "created_utc": "2025-12-24 17:52:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvqi3a1",
              "author": "connormcwood",
              "text": "If you released a change where the end result is r53 resource why would you want everything but the failed resource to be deployed? An example is ecs service deployed if that‚Äôs failed you wouldn‚Äôt want to expose the alb via r53 record would you?",
              "score": 5,
              "created_utc": "2025-12-24 16:11:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvqit47",
                  "author": "craig1f",
                  "text": "If I make a mistake with a deployment, I want to fix it quickly. I don‚Äôt want to wait fifteen minutes to get control back to try to fix it.¬†",
                  "score": 7,
                  "created_utc": "2025-12-24 16:15:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvumzey",
              "author": "AttentionIsAllINeed",
              "text": ">It is very appealing to devs who dabble in ops.¬†\n> ¬†\n>\n> Terraform is fast, preferred by people who do cloud, and mature\n\nSource: trust me bro. And in the next post proclaiming that fast manual fixes are better than consistent stack rollbacks. Erm",
              "score": 1,
              "created_utc": "2025-12-25 09:56:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvqtfuy",
              "author": "WellYoureWrongThere",
              "text": "Have you tried Pulumi at all?",
              "score": 1,
              "created_utc": "2025-12-24 17:12:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvqvdiz",
                  "author": "craig1f",
                  "text": "No. I would really need a reason to drop TF at this point. IBM buying the company is a good start, but not enough on its own.¬†\n\nLooking at it real quickly here are my thoughts. As a full stack dev, cdk was really attractive to me. But if you look at career Ops people, they prefer TF and declarative libraries like that, over code.¬†\n\nI liked writing CDK, but found it very difficult to read and interpret quickly. If you‚Äôre doing Ops, you gotta think like a you‚Äôre ops. If you‚Äôre doing frontend, think frontend. If you‚Äôre doing backend, think backend. Don‚Äôt do OPs like a frontend person. Don‚Äôt do frontend like a backend person, etc. they are all different mindsets.¬†\n\nInfrastructure for programmers is a trap, because you aren‚Äôt really programming. You‚Äôre adding complexity to your ability to read and interpret your infrastructure.¬†\n\nSo I‚Äôd be wary of Pulumi for one of the same reasons I stopped liking CDK after learning Terraform.¬†",
                  "score": -1,
                  "created_utc": "2025-12-24 17:23:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvrqhjk",
          "author": "Prestigious_Pace2782",
          "text": "Yeah CDK runs on cloudformation, which has always been slower. There are some good reasons behind this, the ability to rollback being one of them. I still much prefer it and find it much faster to write. But it‚Äôs always going to be slower to deploy.",
          "score": 3,
          "created_utc": "2025-12-24 20:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsfggh",
          "author": "zenmaster24",
          "text": "Terraform is usually many times quicker due to no (less?) retries - love it",
          "score": 3,
          "created_utc": "2025-12-24 22:47:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtr7eb",
          "author": "PhatOofxD",
          "text": "CDK compiles to CloudFormation and CloudFormation is slower yes.\n\nThere are tradeoffs though, CDK does some things better \n\nAnd CDK being slow can be mitigated by splitting things up.... But yeah it's slower.",
          "score": 3,
          "created_utc": "2025-12-25 04:42:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqupt8",
          "author": "timvw74",
          "text": "While you're at it,¬† look at Pulumi. It's Terraform in code.",
          "score": 6,
          "created_utc": "2025-12-24 17:19:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvjo4h",
              "author": "Sensitive-Ad1098",
              "text": "Actual Terraform in code exists - CDKTF.  \nImperative infra is a very tempting idea (especially for someone who started as a developer). But it adds complexity, and I'd think twice if I really need actual code instead of easy to read .tf files",
              "score": 1,
              "created_utc": "2025-12-25 14:50:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyka9m",
                  "author": "reubendevries",
                  "text": "CDKTF was deprecated earlier this month, so while it‚Äôs still around, it won‚Äôt be for much longer.",
                  "score": 2,
                  "created_utc": "2025-12-26 02:03:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvrnrdj",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -1,
              "created_utc": "2025-12-24 19:59:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvrq098",
                  "author": "ivanyaru",
                  "text": "Pulumi is available in other languages too, so...",
                  "score": 2,
                  "created_utc": "2025-12-24 20:12:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvqao71",
          "author": "oneplane",
          "text": "Yes, it tends to be quicker, but it usually isn't as one-dimensional as that. Use what fits your use case, internal capacity (knowledge, interoperability, time). \n\nIf you are currently in a position where you don't have that much existing systems built up, this event could be a good trigger to review what you have, what works, and what might need adjustment.",
          "score": 2,
          "created_utc": "2025-12-24 15:32:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwtm8b",
          "author": "MavZA",
          "text": "My personal take is on how you like to interact with your templates. CDK is way more flexible on syntax and expressing your architecture, which many prefer and find that their velocity higher, the trade off is CloudFormation. Then there‚Äôs HCL which is certainly something, but you get its direct API driven deployments. Not everyone will agree with my take, I just sharing my opinion having worked on both ends, in teams that have worked both ends. It‚Äôs pretty much why I‚Äôm on SST with Pulumi now, because it‚Äôs a great middle ground. Needs more time to get the other providers working perfectly but at least my AWS workloads are singing and I find them more readable.",
          "score": 2,
          "created_utc": "2025-12-25 19:26:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxozhe",
          "author": "RecordingForward2690",
          "text": "Another thing, not mentioned yet, is that it also depends on how you organize your CDK to synthesise its CloudFormation templates. Within a single CloudFormation template, CloudFormation will try to do as much work in parallel as possible. But if your CDK produces multiple CFN templates, then these CFN templates will be executed in series. There is a --concurrency option but that will still only work if complete templates are not dependent on each other.\n\nFor fastest CDK deployments, it's therefore best to let CDK synthesise everything into one monster CFN template and deploy that. But for readability and maintainability, you may still want to split things up a bit and accept that the deployment (in particular the initial deployment, or changes that touch all your synthesised templates) are slower.",
          "score": 2,
          "created_utc": "2025-12-25 22:38:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzxf0j",
          "author": "dataflow_mapper",
          "text": "In my experience the pipeline time difference usually isn‚Äôt about Terraform vs CDK in isolation. It‚Äôs more about what‚Äôs happening under the hood. CDK has a synth step and often runs asset bundling or uploads, which can add noticeable time compared to a straight Terraform plan and apply.\n\nTerraform can feel faster if your modules are simple and you‚Äôre not constantly refreshing a huge state. CDK shines when you need more abstraction or logic, but you pay a bit for that flexibility. If pipeline speed is the concern, I‚Äôd first look at asset builds, context lookups, and how often stacks are being redeployed rather than switching tools outright.",
          "score": 2,
          "created_utc": "2025-12-26 08:47:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqbftn",
          "author": "SquiffSquiff",
          "text": "Cloudformation is objectively slower than Terraform/Tofu even before you get in to the whole rollback situation. You're working with an API to a system that then runs its own stuff against different APIs. Anything that builds on Cloudformation would then be slower again.\n\nI would not recommend CDK in 2025. If you want a 'real code' solution then check out Pulumi which is comparable to Terraform.",
          "score": 9,
          "created_utc": "2025-12-24 15:36:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrpl99",
              "author": "ivanyaru",
              "text": "Pulumi uses Terraform under the hood. Probably the biggest reason why Hashicorp had to do license shenanigans a couple of years ago.",
              "score": 1,
              "created_utc": "2025-12-24 20:09:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzj7u5",
                  "author": "cnunciato",
                  "text": "It doesn‚Äôt use Terraform, it has its own engine and its providers are based on Terraform provider schemas, but it doesn‚Äôt actually use Terraform anywhere. Common misconception.",
                  "score": 1,
                  "created_utc": "2025-12-26 06:27:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvqs0e1",
          "author": "mrlikrsh",
          "text": "Yes it is, since the api calls are made directly. When using CDK, it synthesises to cloudformation template and a stack is created. CFN will invoke respective resource handlers which then makes the api call to create update resources.",
          "score": 1,
          "created_utc": "2025-12-24 17:04:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsy985",
          "author": "Kitchen-Location-373",
          "text": "interesting thread. I'd caution you not to have the strong opinions displayed here. CloudFormation is an API. it's not a client. there are a lot of client applications to interact with the CloudFormation API, including Terraform. Terraform is solely a client.\n\nthere seems to be a theme of \"managing cloud as a developer is bad\" here. I've never experienced that before. it comes off as cope. any sufficiently advanced operation will be managing their entire infrastructure operation programmatically - either with a framework like Pulumi or CDK or by writing your own application in-house. I've written many apps in-house that interact with AWS APIs to manage infrastructure programmatically via code (Python or Go).\n\nTerraform doesn't have its own client libraries. it uses the official Go SDKs any other project could would use. it's extremely simple. even despite that, it's certainly not \"fast\" because it has to reconcile state with every operation. if you want the fastest possible operation, just run something against the AWS API directly and don't track state at all. you'd simply have to tag the resources properly for service discovery. I've done it at massive scale.\n\nthat might be too complicated to get started with. usually environments I manage \"evolve\" to more complex/sophisticated orchestration. but one thing I can promise you: you'll sound like a damned fool if you go around in this industry acting like Terraform is the end-all, be-all of IaC. it's better than no IaC and great for \"hello world\" and \"getting started\" scale. but it's a mess even once you're dealing with a couple dozen AWS accounts, it's untenable once you're working with 100-200 AWS accounts where thousands of deployments need to \"just work\" for a team of ~5 people managing it all.",
          "score": 1,
          "created_utc": "2025-12-25 00:58:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvbqfi",
          "author": "yuriy_yarosh",
          "text": "CDK creates implicit resources, and gets translated to CloudFormation.\n\nIn terms of state management - Terraform itself has inherent design flaw in deferred provider initialization. [https://discuss.hashicorp.com/t/depends-on-in-providers/42632](https://discuss.hashicorp.com/t/depends-on-in-providers/42632)\n\nWhich is partially resolved through terraform stacks [https://developer.hashicorp.com/terraform/language/stacks](https://developer.hashicorp.com/terraform/language/stacks) and terragrunt stacks for OpenTofu [https://terragrunt.gruntwork.io/docs/features/stacks/](https://terragrunt.gruntwork.io/docs/features/stacks/)\n\nThus making state itself fragmented, but deployments parallelizable and atomic.\n\nCloudFormation and CDK itself does not fully support Landing Zone stacks in form of StackSets [https://github.com/cdklabs/cdk-stacksets](https://github.com/cdklabs/cdk-stacksets) which makes it inherently non-parallelizable, in the same sense.\n\nBoth CDK and Terraform have their own flaws, but Terraform has more spikes and workarounds, driven by enter-pricy greed and complacency.",
          "score": 1,
          "created_utc": "2025-12-25 13:52:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxdedh",
          "author": "ConstructionSoft7584",
          "text": "Generally yes, but It depends on scale and resources types and how they work.\n\nAt work we have a cdk project that governs 5 ecs services in the same cluster, and a massive terraform state of 30+ clusters (we've taken it apart since then) and the terraform takes half the time to apply.\n\nI think the reason for that is when governing ecs clusters (for example) terraform does not wait for the deployment to finish, unlike cdk (cloudformation) that awaits the SUCCESS event for each resource - whichever it would be. Finish deployment, finish provisioning a Redshift cluster (don't use redshift), whatever. If terraform would wait for all the events cdk awaits, they would have the same speed. It's just api calls to create resources, and cdk just awaits more events.\n\nMy guess is that if we would've added all the hooks and await all the events cdk does, our terraform would be much more stable but much slower.",
          "score": 1,
          "created_utc": "2025-12-25 21:27:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0c8pa",
          "author": "Previous_Box_6127",
          "text": "In my experience, it‚Äôs less about Terraform vs CDK and more about how the stacks are structured. Synthesis time, stack size, and dependency graphs in CDK can add overhead, while Terraform plans feel faster for simpler setups. Both can be quick or slow depending on design.",
          "score": 1,
          "created_utc": "2025-12-26 11:20:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsl7sr",
          "author": "Beastwood5",
          "text": "Hell yeah!!",
          "score": 1,
          "created_utc": "2025-12-30 20:33:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvq8xdx",
          "author": "JoshSmeda",
          "text": "Yes terraform uses AWS APIs. CDK uses Cloudformation which is trash",
          "score": -9,
          "created_utc": "2025-12-24 15:22:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvq924h",
              "author": "Dull_Caterpillar_642",
              "text": "However I think the syntax is a lot nicer and easier to reason about, which to me is worth an extra minute or two in the deploy pipeline.",
              "score": 9,
              "created_utc": "2025-12-24 15:23:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvq9b6l",
              "author": "lowfox",
              "text": "Why is CF trash?",
              "score": 2,
              "created_utc": "2025-12-24 15:24:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvqazo4",
                  "author": "JoshSmeda",
                  "text": "Mostly slow, but in my opinion the yaml manifests look bloated, and messy (due to it being imperative). \n\nAlso, wait till it tries to delete something. For example, on a VPC attached Lambda, CF deletes the Lambda before the ENI can be removed so then it gets stuck in a state it can‚Äôt recover from if you also try delete the VPC because there‚Äôs a security group and a subnet still attached, to the ENI.\n\nAt the end of the day, you need to use whatever you (and the business) is comfortable with.",
                  "score": 4,
                  "created_utc": "2025-12-24 15:33:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvqap69",
                  "author": "mr_mgs11",
                  "text": "It's way slower and deletion of resources can be a massive pain in the ass. I have not used CF in years, but every time I did I would end up having orphaned resources kicking around after the fact. These are from AWS provided stacks to. The only time I used the CDK was at a few re:Invent workshops and my last org had our TAMS do a presentation on using the CDK to bootstrap new accounts.",
                  "score": 9,
                  "created_utc": "2025-12-24 15:32:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvq9cfi",
          "author": "CircularCircumstance",
          "text": "Very different use cases",
          "score": -2,
          "created_utc": "2025-12-24 15:24:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqbc04",
          "author": "Old-Sweet7661",
          "text": "Yes ofc cdk is based on cloudformation and cloudformation was developed by aws closed source -> Trash",
          "score": -11,
          "created_utc": "2025-12-24 15:35:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvq9yry",
          "author": "Icy_Start_1653",
          "text": "That‚Äôs an obvious answer.",
          "score": -9,
          "created_utc": "2025-12-24 15:28:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pub5gi",
      "title": "Made an open-source AWS Free Tier reference - updated for the July 2025 changes",
      "subreddit": "aws",
      "url": "https://i.redd.it/647tg6ws129g1.png",
      "author": "building_costgoat",
      "created_utc": "2025-12-24 01:39:48",
      "score": 45,
      "num_comments": 3,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "technical resource",
      "permalink": "https://reddit.com/r/aws/comments/1pub5gi/made_an_opensource_aws_free_tier_reference/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvooc5y",
          "author": "VPav",
          "text": "Nice work! Free tier is very confusing for a lot of people and my issue was always its interaction with other accounts in the organization.\n\nIt used to be that only one account can benefit from it, but there are different types of free tier. GuardDuty is a good example where you will have 30 days free on every account/region. I personally would find it very helpful if you could provide that separation.\n\nAlso, it's missig the new CloudFront free plan - not sure if you would like to include it.",
          "score": 3,
          "created_utc": "2025-12-24 07:51:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvoukn7",
              "author": "building_costgoat",
              "text": "Thanks! Both great points.\n\nJust added an Organizations section covering this - how Always Free/12-month tiers are shared across the org, but short-term trials (like GuardDuty) are per-account and per-region. Would appreciate if you could sanity-check it.\n\nCloudFront is in the Always Free section (1 TB out + 10M requests/mo) - were you thinking of something else that's missing?",
              "score": 2,
              "created_utc": "2025-12-24 08:52:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvp8nsm",
                  "author": "VPav",
                  "text": "No, I was thinking the new CloudFront pricing - the free plan that exists and is limited to 3 per account.\n\nRef here: https://aws.amazon.com/cloudfront/pricing/",
                  "score": 1,
                  "created_utc": "2025-12-24 11:10:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pwbq5k",
      "title": "Support: How to bypass Artificial Idiot and get a Human Being on wire?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pwbq5k/support_how_to_bypass_artificial_idiot_and_get_a/",
      "author": "dim13",
      "created_utc": "2025-12-26 18:49:05",
      "score": 41,
      "num_comments": 21,
      "upvote_ratio": 0.83,
      "text": "A bit of rant: We have paid support. Nevertheless, we are stuck in a loop with AI bullshit responses on our issue. It is probably a 5th back and forth over past few weeks already.\n\n> Thank you for writing back to us. Since assisting you is my highest priority, I thought of calling you to discuss this issue over a live medium and address any additional queries you might have. However, due to us being in different time zones, I couldn't call you as it was too early to call as per time zone and I didn't want to disturb you outside business hours. Rest assured, all my research is mentioned below for your reference.\n> ‚Ä¶\n\nIs there any magic keyword to summon a Human Being and get past this AI BS? Or is this ship already sailed? :(",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pwbq5k/support_how_to_bypass_artificial_idiot_and_get_a/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nw2d92r",
          "author": "Remifex",
          "text": "Have you tried saying ‚Äúcall me at any time regardless of the time zone‚Äù?\n\nI‚Äôm also not convinced this is agentic AI. This may be a button click response from a human.",
          "score": 49,
          "created_utc": "2025-12-26 18:54:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2feos",
              "author": "dim13",
              "text": "Hmm, worth to try probably. However all responses come from different names. And they ask for in one of previous responses:\n\n```\nTo proceed we will please need the root user of this account to give us permission to proceed. They can do so on this existing case. We will also need the details below:\n\nCompany name:\nContact name:\nPrimary contact email address:\nAdditional contact email addresses:\nAddress:\nCity:\nState:\nPostal code:\nCountry:\nContact phone number:\n```\n\nOn a f*cking PAID official support request! They have all this information already!",
              "score": 2,
              "created_utc": "2025-12-26 19:05:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw2gkbk",
                  "author": "Remifex",
                  "text": "Is the root user of the account making this request?\n\nJust to give some perspective, AWS takes the responsibility of account access incredibly seriously.",
                  "score": 15,
                  "created_utc": "2025-12-26 19:11:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw2f9sz",
          "author": "ryanrem",
          "text": "I would reply with either a chat or call, but most start with chat, then transfer over to Chime/Soom. \n\nWhen you are making your reply with the AWS Support Console you'll have the option for either Web, Chat or Call. Web sends a message to the engineer you are actively working with, while Chat or Call will route you to the next available engineer and start a live session with someone.\n\nI know they did recently make some changes so if you don't have the option for a chat or call, you might need to revert back to the legacy experience.\n\nLegacy experience: Creating support cases and case management - AWS Support https://docs.aws.amazon.com/awssupport/latest/user/case-management-legacy.html#creating-a-support-case-legacy",
          "score": 8,
          "created_utc": "2025-12-26 19:04:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2epdy",
          "author": "AWSSupport",
          "text": "Hello,\n\nSorry to hear about the frustration. If you could send us your case ID via chat, I'd be happy to take a look.\n\n\\- Doug S.",
          "score": 8,
          "created_utc": "2025-12-26 19:01:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2gz9b",
              "author": "dim13",
              "text": "Thanks! Sent a DM.",
              "score": 2,
              "created_utc": "2025-12-26 19:13:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw2jdri",
                  "author": "AWSSupport",
                  "text": " Hello,\n\nWe've received your chat message and will be responding shortly.\n\n\\- Doug S.",
                  "score": 4,
                  "created_utc": "2025-12-26 19:26:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw2ohl1",
          "author": "DreadStarX",
          "text": "Hold up, AWS actually has employees checking Reddit?! Wild.... \n\nHope you get it fixed OP. I always hate seeing people have issues with AWS, feels like a jab at my pride being an Amazonian..",
          "score": 7,
          "created_utc": "2025-12-26 19:54:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4juro",
              "author": "AntDracula",
              "text": ">¬†feels like a jab at my pride being an Amazonian..\n\nYou‚Äôre very unlikely to be the root cause of these issues. But the quality of support has absolutely taken a nosedive since Andy went full CEO ‚ÄúAI is going to replace everything and everyone‚Äù",
              "score": 6,
              "created_utc": "2025-12-27 02:27:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgkq3q",
                  "author": "DreadStarX",
                  "text": "Oh, I know I'm not to blame for it. I take pride in knowing that my work for AWS is allowing others to do theirs. Feels like this push for AI has made us lose our focus on what's important: the customer.\n\nI love working at AWS, it's allowed me to learn, grow, and to deal with my mental health issues. I do not think any other company would be as tolerant with someone like me. I have ADHD, ASD, and Social Anxiety.",
                  "score": 1,
                  "created_utc": "2025-12-29 00:35:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw2j6h7",
          "author": "TheGingerDog",
          "text": "almost like you need a magic keyword to trigger escalation to a human .... shibboleet etc ... [https://xkcd.com/806/](https://xkcd.com/806/)",
          "score": 8,
          "created_utc": "2025-12-26 19:25:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4fs0b",
          "author": "emperorOfTheUniverse",
          "text": "In my experience, not paying your bill for 6 months will do it.",
          "score": 3,
          "created_utc": "2025-12-27 02:01:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcd01w",
          "author": "latent_signalcraft",
          "text": "you are not alone this comes up a lot. the irony is that support bots are optimized for ticket deflection not resolution so once your issue falls outside a known pattern you just loop. what usually breaks it is reframing the ticket around business impact or risk not technical symptoms because that tends to trigger escalation paths that are still human owned. it is less a magic keyword and more forcing the system out of FAQ mode.",
          "score": 2,
          "created_utc": "2025-12-28 10:19:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxmx0q",
      "title": "Memory spikes killing my workersüíÄ  need scaling advice",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pxmx0q/memory_spikes_killing_my_workers_need_scaling/",
      "author": "tiln7",
      "created_utc": "2025-12-28 09:03:41",
      "score": 33,
      "num_comments": 29,
      "upvote_ratio": 0.72,
      "text": "So I've got this Node.js SaaS that's processing way more data than I originally planned for and my infrastructure is starting to crack...\n\n**Current setup (hosted on 1 EC2):**\n\n* Main API container (duplicated, behind load balancer)\n* Separate worker container handling background tasks\n\n**The problem:** Critical tasks are not executed fast enough + memory spikes making my worker container being restarted 6-7x per day.\n\n**What the workers handle:**\n\n* API calls to external services (some slow/unpredictable)\n* Heavy data processing and parsing\n* Document generation\n* Analysis tasks that crunch through datasets\n\n\n\nSome jobs are time-critical (like onboardings) and others can take hours.\n\n\n\n**What I'm considering:**\n\n1. Managed Redis (AWS ElastiCache) \n2. Switching to SQS \n\n  \nWhat approach should I take and why? How should I scale my workers based on the workload?   \n\n\nThanks üôè",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pxmx0q/memory_spikes_killing_my_workers_need_scaling/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwclk3e",
          "author": "Dave3of5",
          "text": "Try to figure out the memory spikes / crashing first before doing anything arch / design related. There could be a problem there and then you are just shifting your problem around.",
          "score": 11,
          "created_utc": "2025-12-28 11:40:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcmchg",
              "author": "tiln7",
              "text": "will try!",
              "score": 2,
              "created_utc": "2025-12-28 11:47:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwca7im",
          "author": "MortTheLemur23",
          "text": "For a starter I would recommend separating your API from your workers by placing them on separate instances. And for more resilience dockerize both and use an AWS managed service like ECS or Elastic Beanstalk (simple setup). That way a single api/worker failure won't tear down your entire app.\n\nAnd for the workers, I would (like you said) look into SQS or Bullmq to queue your worker jobs. That enables you to better manage your jobs and see what causes your memory spikes.",
          "score": 22,
          "created_utc": "2025-12-28 09:53:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwie5zm",
              "author": "UltimateLmon",
              "text": "Honestly though for OP's particular problem, Op should really investigate the memory spikes.\n\n\nThough everything you've said should still be implemented because it's just generally good to have more resilient architecture - costs pending.\n\n\nOP might even consider moving analysis tasks to AWS Batch while at it. Can't tell about doc processing and data processing without further details.",
              "score": 4,
              "created_utc": "2025-12-29 07:46:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwc87zt",
          "author": "seanv507",
          "text": "Its unclear what your question is. \n\nAre you saying you need more compute resources (bigger instance/ aws batch serverless ,...) so handle memory spikes\n\nor a better multiprocessing (so important quick tasks are performed in a timely way) eg a separate queue\n\n...",
          "score": 9,
          "created_utc": "2025-12-28 09:33:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcno6e",
          "author": "hornetmadness79",
          "text": "You underestimated the necessary resources required to run your apps. Not uncommon really, but your reaction is. If an app is crashing 7 times a day, you can either fix the code, provision a larger node, or build better infra to handle the capacity problem. Throwing more cloudy solutions to fix a problem of your own making is what AWS is betting on.",
          "score": 9,
          "created_utc": "2025-12-28 11:59:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc6rg3",
          "author": "xzaramurd",
          "text": "For background tasks it sounds like you could use AWS Batch with Fargate or EC2 Spot instances. For your API, if sounds like you already have an ELB? Do you not have an AutoScalingGroup with it so it can spin up more instances if needed?",
          "score": 10,
          "created_utc": "2025-12-28 09:19:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc8q4g",
          "author": "mr_q_ukcs",
          "text": "What was the decision behind hosting it on a single ec2 ?\n\nThe memory spikes could be a number of application level  issues, do they happen at a set timeframe or are they random?\n\nAre your api calls asynchronous ? Do you have timeouts on them? Do you have back offs? All these things can cause a constraint on cpu resource.\n\nI would look at moving your service to ECS Fargate if it‚Äôs handling tasks that are critical, a single ec2 isn‚Äôt going to give you the resilience you need.",
          "score": 5,
          "created_utc": "2025-12-28 09:38:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcu9i5",
          "author": "FlamboyantKoala",
          "text": "Are you generating files in memory? ¬†90% of the time I‚Äôve had to fix issues with memory it‚Äôs caused by that.¬†",
          "score": 2,
          "created_utc": "2025-12-28 12:54:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwd32a8",
          "author": "cachemonet0x0cf6619",
          "text": "i don‚Äôt think using another cloud service is going to fix your underlying issue of a poorly designed application. if you dm me we can talk about the shift aspect of lift and shift but i‚Äôd need to see the details",
          "score": 1,
          "created_utc": "2025-12-28 13:54:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj070q",
          "author": "dataflow_mapper",
          "text": "This smells like a workload separation problem more than just a queue choice. Mixing time critical jobs and long running memory heavy tasks in the same worker almost guarantees spikes and restarts. I would split workers by job class first, even if they still share infra, so the heavy batch stuff cannot starve or crash onboarding jobs.\n\nSQS plus autoscaled workers is usually a good baseline because it forces you to think in smaller, bounded jobs and gives you backpressure for free. Redis can work, but it is easier to accidentally turn it into a memory footgun. Also worth looking at streaming large payloads instead of loading everything into memory at once. Most Node memory issues I see come from buffering way more than expected.",
          "score": 1,
          "created_utc": "2025-12-29 11:11:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcugwz",
          "author": "Wide_Commission_1595",
          "text": "So generally it seems like you've got a lot right! Separate API and workers is going to make scaling this an awful lot easier! \n\nFirst of consider moving over to Fargate.  If have the API auto scale based on CPU/memory to make sure it stays responsive.  I would go with the SQS option for queueing jobs, because you can auto scale your workers based on queue length, and that way you keep background job processing responsive.\n\nIt's not clear exactly how the background jobs work, i.e. is it always one type of job, or are there multiple different jobs?  Basically I would go with one queue per job type, and have dedicated workers assigned to their own queue, but if that creates extra work, a single queue/worker combo will do fine\n\nLonger term, depending on how the whole system works I might be tempted to convert the API to API gateway.  You can use it to validate requests without code and place items direct into SQS.  Other endpoints can use lambda functions.  For the reports, save them S3 and then in the API return a redirect to a pre-signed S3 url so user can download easily without exposing the bucket itself.  These are future suggestions though, for now just moving away from EC2 and into Fargate with SQS buffering should take you an awfully long way",
          "score": 1,
          "created_utc": "2025-12-28 12:55:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwd3xsu",
              "author": "nekokattt",
              "text": "Fargate is going to be more expensive and you still have to configure it to scale. While it removes the overhead of managing EC2, it does not remove the issue for OP.",
              "score": 1,
              "created_utc": "2025-12-28 14:00:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwi9xpr",
                  "author": "Wide_Commission_1595",
                  "text": "The OP is already running containers, do why use EC2 when ECS is specifically for scaling. \n\nYour right, you do have to configure scaling, I mean, that's kind of how AWS works.  You configure services to work the way you want them to?\n\nAnd yes, it's going to cost more.  Scaling up kinda does that.  Again, it's how AWS works.\n\nThe OP could attempt to fix memory leaks, which could take months, may never fix the issue, and all the while of customers off, or just scale using services designed for the task and keep customers happy and paying.....",
                  "score": 0,
                  "created_utc": "2025-12-29 07:09:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdjncf",
          "author": "AnomalyNexus",
          "text": "node.js is a poor choice for time-critical heavy data processing\n\nYou can try to patch over that by throwing more hardware at it to buy time but if the SaaS keeps growing you'll hit a wall eventually forcing a rewrite of at least the core logic in something more suitable",
          "score": 1,
          "created_utc": "2025-12-28 15:32:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwh8agd",
              "author": "ducki666",
              "text": "Lol",
              "score": 1,
              "created_utc": "2025-12-29 02:49:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwe5ckk",
          "author": "mlhpdx",
          "text": "Use Lambda. You already know Node.js and can keep your web server since it probably won‚Äôt need scaling for a bit if you put the workers elsewhere.¬†\n\nIdeally, have the web sever put messages for workers in a queue (or multiple queues if you have different priority levels) and use Lambda to work it/them. This isn‚Äôt an ideal setup, but it‚Äôs an easy step from where you are and allows workers to scale from zero to whatever you need when spikes happen.",
          "score": 0,
          "created_utc": "2025-12-28 17:21:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwennm6",
              "author": "tiln7",
              "text": "Noted! Migrating everything to lambdas would require serious refactoring and probably I would need to open DB?",
              "score": 1,
              "created_utc": "2025-12-28 18:49:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nweubk7",
                  "author": "mlhpdx",
                  "text": "Yeah, don‚Äôt migrate everything ‚Äî just the worker code that runs in the background. The Lambda can connect to the same VPC that contains the DB, so it doesn‚Äôt need to be ‚Äúopen‚Äù in that sense.",
                  "score": 0,
                  "created_utc": "2025-12-28 19:19:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwfkeyq",
          "author": "TechDebtSommelier",
          "text": "Put the heavy work behind SQS and let your API just enqueue jobs, because Redis queues fall over easily when memory spikes or workers crash. Split workers by job type so fast, time sensitive tasks are not stuck behind long running jobs, and autoscale workers based on SQS queue depth. Keep EC2 or ECS for predictable heavy jobs and consider Lambda only for short bursty tasks. This setup absorbs spikes cleanly and stops one bad job from taking everything down.",
          "score": 0,
          "created_utc": "2025-12-28 21:27:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwibgyw",
          "author": "could_be_any_person",
          "text": "I also have an app hosted on AWS with an API container and long running tasks. You need to split your tasks between different workers. Have a dedicated worker container for long running tasks and one for short tasks. Create an auto scaling group and policy for each worker. I have mine set to scale based on the size of my task queues, but you can also set it to scale based on memory, cpu, etc. For example, you can create an auto scaling policy for your short tasks to create more worker containers whenever tasks are backed up. That way, there's always a worker container available to process short tasks in case there's a spike in user activity (or migrate short tasks to a lambda function).  You can also set up predictive scaling policies so that your cluster scales automatically ahead of predicted demand patterns. \n\nEither use redis + a lambda function + the EventBridge scheduler, AWS SQS, or a managed solution like Temporal to handle delegation of queued tasks to your worker containers.\n\nFigure out the memory problem first. Your program shouldn't be crashing regardless of spike in user activity. It should be queuing any jobs your container can't handle. I recommend SQS for that. Also make sure you're not saving large files to memory. Use a temp folder to save any large files that you need to return.",
          "score": 0,
          "created_utc": "2025-12-29 07:22:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc7vjc",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -14,
          "created_utc": "2025-12-28 09:30:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwd0ns8",
              "author": "Dangerous-Sale3243",
              "text": "OP is unclear, but it doesn‚Äôt sound like there‚Äôs a memory leak, there‚Äôs just either ‚Äúbad‚Äù programming (holding entire documents in memory perhaps too long) or just lack of scaling in the design.\n\nOP could spend a lot of time rewriting to Rust but it wouldn‚Äôt solve the underlying mathematical problem that requests/sec * avg request time * avg document size is close to the memory limit.\n\nI do think rewriting to Rust could be helpful though, but it‚Äôs further down the list of priorities. To me, #1 is stopping the bleeding, give everything more RAM to stop dropping requests. #2 is to switch to passing documents as s3 URIs and then migrate the background workers to Lambda.",
              "score": 2,
              "created_utc": "2025-12-28 13:39:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nweqd7r",
              "author": "bot403",
              "text": "As a software engineer I'd rather spin up a second instance in minutes to hours than a week or weeks doing a risky language rewrite of my core application.",
              "score": 1,
              "created_utc": "2025-12-28 19:01:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q05hib",
      "title": "Why do I need 5 different services just to run a function on HTTP trigger?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1q05hib/why_do_i_need_5_different_services_just_to_run_a/",
      "author": "Sadhvik1998",
      "created_utc": "2025-12-31 05:56:29",
      "score": 31,
      "num_comments": 47,
      "upvote_ratio": 0.69,
      "text": "Genuine question‚Äîam I missing something, or is this just how the cloud works?\n\nWhat I'm trying to do:\n\n\\- Simple thing - HTTP request comes in, runs some code async and pushes a message to broker.\n\nWhat am I using to do this (AWS example):\n\n1. API Gateway for the HTTP endpoint\n2. Lambda for running code\n3. EventBridge for routing the event\n4. SQS for queue and retries\n5. CloudWatch for logs\n6. I am to connect everything\n\nSame story on Azure/GCP, just different service names.\n\nTwo problems I'm facing:\n\n1. Cost is crazy: Each service bills separately. One request = 5 billing charges (API Gateway + Lambda + EventBridge + SQS + CloudWatch). When traffic grows, I'm paying more for connecting services than actual compute.\n2. Too many moving parts: 6 different dashboards to check. Retries are configured in 3 places. Debugging needs checking multiple services. Each service has its own limits.\n\n\n\nFor one simple \"run code on HTTP request,\" I'm managing half a dozen services.\n\nMy question:\n\nIs this normal? Do you just accept this complexity? Or is there a simpler way that I'm missing?\n\nI see people either deal with it or go back to old-style EC2 apps. Is there any middle path?\n\nWhat do you guys do?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1q05hib/why_do_i_need_5_different_services_just_to_run_a/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwvduf9",
          "author": "The-Wizard-of-AWS",
          "text": "Sounds like you‚Äôre making things way too complicated. API Gateway -> Lambda is all you need if you just need to trigger a function. If you need something long running or need to have it handle scale with SQS then it‚Äôs API Gateway-> SQS -> Lambda. If this wasn‚Äôt in the cloud it wouldn‚Äôt be much different if you wanted the same scale and resiliency. You‚Äôd have something like Load Balancer-> compute -> queue/stream (e.g., RabbitMQ, Kafka) -> compute. In the cloud you don‚Äôt have to manage most of those things.",
          "score": 96,
          "created_utc": "2025-12-31 06:07:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxeimj",
              "author": "aboothe726",
              "text": "You can also use a lambda function URL (without API gateway) if you just need to be able to access/trigger the function publicly and don‚Äôt care about the URL.",
              "score": 17,
              "created_utc": "2025-12-31 15:31:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxeymp",
                  "author": "SodaAnt",
                  "text": "Lambda function URLs support IAM auth.",
                  "score": 10,
                  "created_utc": "2025-12-31 15:34:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nww7sxs",
              "author": "Crossroads86",
              "text": "IAM and Cloudwatch is still needed.",
              "score": 5,
              "created_utc": "2025-12-31 10:41:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx01kpi",
                  "author": "The-Wizard-of-AWS",
                  "text": "Technically CloudWatch is optional. If you don‚Äôt grant permissions and don‚Äôt create a log group it will still work. And, like the other things, this isn‚Äôt really different than if you run on prem. You log it somewhere and have to have a way to view the logs. \n\nYou‚Äôre right about IAM, which is needed for everything.  It‚Äôs the one difference from on prem for most things (unless you‚Äôve done zero trust). That said, it often is in place of networking requirements you‚Äôd have. You may need to have firewall rules in the cloud, but in its simplest form you don‚Äôt have to think about networking at all. It‚Äôs been a while since I‚Äôve even used a security group.",
                  "score": 2,
                  "created_utc": "2025-12-31 23:56:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nww93bx",
              "author": "BredFromAbove",
              "text": "Always have a sqs in between, no? Best for retries in case of error / timeout?",
              "score": 1,
              "created_utc": "2025-12-31 10:53:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwx6cm8",
                  "author": "landon912",
                  "text": "For event handlers? Yes. For an API? No. \n\nThe caller is responsible for handling the queueing / retries of required.",
                  "score": 16,
                  "created_utc": "2025-12-31 14:48:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwx5ynp",
                  "author": "Veuxdo",
                  "text": "No? Without a very specific reason, adding another thing between APIG and Lambda would create more problems than it solves.",
                  "score": 4,
                  "created_utc": "2025-12-31 14:46:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nww99td",
                  "author": "tr666tr",
                  "text": "Depends if you need a synchronous response from the Lambda",
                  "score": 1,
                  "created_utc": "2025-12-31 10:55:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwzbvgs",
                  "author": "chalbersma",
                  "text": "Depends on the use case. If failure is acceptable then no.¬†",
                  "score": 1,
                  "created_utc": "2025-12-31 21:26:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvfpll",
          "author": "MmmmmmJava",
          "text": "I understand where you‚Äôre coming from but I‚Äôll nitpick your example. Technically you don‚Äôt need SQS, Event Bridge, or event CW logs (but you‚Äôll want logs) or possibly event API GW for a minimal serverless HTTP endpoint (if you use lambda function URLs). \n\nThe event bridge, API GW, and SQS layers are managed services acting as functional add-ons based on your specific use case. Each of these services are powerful building blocks that you can pick and choose to use. \n\nAlso, what type of routing are you doing with event bridge? Can that Event Bridge routing component be eliminated by adding a little extra code/routing logic in your Lambda?",
          "score": 24,
          "created_utc": "2025-12-31 06:22:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvgkkk",
          "author": "lulu1993cooly",
          "text": "Aws is really meant to be an ecosystem of interconnected services, not a one-click solution. \n\nYou seem to want SaaS not PaaS mate",
          "score": 43,
          "created_utc": "2025-12-31 06:29:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy9lfo",
              "author": "Mysterious_Rub_224",
              "text": "This.",
              "score": 0,
              "created_utc": "2025-12-31 18:05:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvenmo",
          "author": "Beginning-Swim-1249",
          "text": "You could just invoke the lambda directly if it‚Äôs not business critical or production. However, if you want the extra stuff‚Ä¶ well you need the extra stuff. You‚Äôre not going to get a gateway without a gateway, logs with logs etc. \n\nThere probably is a cloud formation stack that has all this ready for you so there‚Äôs not really much in the way of manual configuration you need to repeat. Or you could use some IaC that has a template for it.\n\nAlso as you scale up in complexity you won‚Äôt necessarily need to have another of everything, you‚Äôll probably share some of the service, especially the more expensive ones.\n\nI‚Äôm pretty sure AWS allows you to make custom dashboards as well so you can aggregate all your metrics in one ‚Äúplace‚Äù. It‚Äôs been a while since I‚Äôve used it",
          "score": 13,
          "created_utc": "2025-12-31 06:14:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvdypx",
          "author": "dunkah",
          "text": "Your simple thing has all of those components. Either do them yourself or use a managed service. Sometimes it is for sure cheaper and simpler.",
          "score": 11,
          "created_utc": "2025-12-31 06:08:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvdxc6",
          "author": "magnetik79",
          "text": "Steps 1& 2 can be combined, using Lambda functions URLs.",
          "score": 15,
          "created_utc": "2025-12-31 06:08:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvkbl2",
              "author": "itdoesntmatteranyway",
              "text": "As long as you‚Äôre okay without the allow/denylist and throttling API gateway provides.",
              "score": 15,
              "created_utc": "2025-12-31 07:01:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwvt159",
              "author": "behusbwj",
              "text": "Function url‚Äôs are an anti-pattern",
              "score": -8,
              "created_utc": "2025-12-31 08:21:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwveyuv",
          "author": "kobumaister",
          "text": "You don't NEED all that, you need it if you want to implement it using aws services. You could use knative with a rabbitmq and a kafka server, for example.\n\nA lot of context is missing in your post. Why do you want to implement using solely aws services? It's a requirement? Why not use a simple python service in an ec2 instance with celery?\n\nIf you have the expertise, building that is quite straightforward and generalizing it to build future services too.\n\nAbout the billing part, those services are quite cheap, if you can't monetize your service to cover that cost, you need to review what you're offering (or your infra).\n\nAnd about the monitoring, you don't need monitoring dashboards for each step, you monitor the part that you can act on, in this case, the lambda. Other services are monitored by aws. Maybe you'll build a dashboard with business metrics about how many requests are going through, but you don't monitor the uptime of event bridge. The approach that worked better for me is an up-bottom monitoring.",
          "score": 5,
          "created_utc": "2025-12-31 06:16:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwznns",
              "author": "marx2k",
              "text": ">Why not use a simple python service in an ec2 instance with celery?\n\nTell me more about celery",
              "score": 1,
              "created_utc": "2025-12-31 14:10:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxd0l4",
                  "author": "kobumaister",
                  "text": "https://letmegooglethat.com/?q=Celery+python+",
                  "score": 0,
                  "created_utc": "2025-12-31 15:24:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwezyb",
          "author": "pint",
          "text": "i don't understand eventbridge in your architecture, but for the other elements: those are not what you have to use, but what you want to use, right? you yourself added rationale for them. you can easily skip sqs, but then you risk losing messages. you don't need logs, but you want for auditing/debugging. it is not aws that is complex, but your requirements.\n\nthis is pretty typical. i often end up having 10-15 objects in my cloudformation templates even for the simplest applications.\n\nabout costs: if you look at your bill, or do any calculations, you often see a very lopsided picture. one or two services will dominate, while most of them will be single digit percents. however it is, this is the price. take it or leave it.",
          "score": 2,
          "created_utc": "2025-12-31 11:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwx8wa7",
              "author": "germoo0",
              "text": "usually the main compute and the main storage. f.e. EC2 and RDS",
              "score": 1,
              "created_utc": "2025-12-31 15:02:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwhaw4",
          "author": "RecordingForward2690",
          "text": "One more thing to add - you don't need six CW Dashboards. Just grab all the metrics from all the components you need, and add them to a custom dashboard. Everything in one view.\n\nOther than that - you forgot three essential services in your list. Make sure you throw in a WAF at your public endpoint (CloudFront, API GW or ALB) for DDoS/XSS and other protections. And you'll want to use ACM and Route53 to get a proper X.509 cert so you can do https:// instead of http://.\n\nAWS is all about building blocks, that each do one thing and do one thing well, and can be combined in a lot of different ways. And that's a good thing, IMHO. Consider the opposite. Suppose that AWS would have a ready-to-go solution that would allow you to make an https:// API call (using a custom domain name), that would invoke code. Now you're managing code in that solution. Then there would be another complete solution that would accept events from some sort of queue and run code in response, with retries upon failure. Now you need to manage code in that solution as well, with slightly different configuration parameters and capabilities. Before you know it, AWS would have to manage 100s of environments where users can run code. Better to have one all-singing-all-dancing solution for running code that can be integrated everywhere: Lambda.\n\n(In reality, AWS already has half a dozen solutions to run code in a serverless fashion. It's not just Lambda, but also Lambda@Edge, CloudFront Functions, SSM Documents and a few others. When for instance a new Python runtime is launched, we always find that support for that runtime in these solutions is added on a different schedule. So we could go to Python 3.13 in Lambda, but not yet in our SSM documents. Grrr.)",
          "score": 2,
          "created_utc": "2025-12-31 12:05:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx5vwv",
          "author": "oneplane",
          "text": "\\> Why do I need 5 different services just to run a function on HTTP trigger?\n\nBecause 'run a function on http trigger' is 5 different things. It can even be 50 different things depending on how granular you want to get.\n\nYou can pay someone to make the 5 different things seem like 1 thing, or you could us a service that doesn't have 5 things but only has 1 thing that just happens to be the thing you need.\n\nDoing this yourself on EC2 is also 5 things (if you just pick 5 random things). But EC2 itself is also more than 5 things (Instance, ENI, IP, AMI, EBS, Subnet, VPC, SG, AWS Account) so it's never really 'just the few things you wanted' if you don't specify your scope.\n\nIt can be 'one thing' if you scope it to a single Git repo, but that repo would need all the config to make that 1 thing happen (by doing 5 things in its configuration).\n\nPerhaps the issue here is how you perceive 'things' or 'complexity'; depending on your perspective merely including a standard library for a function that needs a runtime to open a socket and read and write on it is already too complex, but there isn't going to be a solution that doesn't do those things. But is that a problem? Not really, you're probably not even taking those into account because they are just there.\n\nThe same goes for the services and granularity of the components you consume. The complexity doesn't go away, it's just packaged and abstracted differently. Keep in mind that complex ‚â† hard (or difficult), it merely means that a thing is composed out of multiple other things.",
          "score": 2,
          "created_utc": "2025-12-31 14:46:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvdv7a",
          "author": "cmills2000",
          "text": "Start small using a monolith deployed to an ec2 instance or an app service (Elastic Beanstalk, ECS).  The AWS way of doing things is often overkill.  Start with as little as you need at first, and if you find in the future that you need to do it the AWS way due to scale, you cross that bridge when you get there.",
          "score": 4,
          "created_utc": "2025-12-31 06:07:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvnsde",
              "author": "HydrA-",
              "text": "Eh, not sure I agree. With IaaC and AI its never been easier to set things up correctly from the start. I wouldn‚Äôt risk the double work and then have something that isn‚Äôt scalable or can scale-to-zero, or needs patching and firewall rules/nat gateway, etc. etc. Going the VM route will not necessarily simplify things. Before you know it you‚Äôre relatively locked in to Beanstalk or what have you",
              "score": 3,
              "created_utc": "2025-12-31 07:32:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvgfp9",
          "author": "PhilipJayFry1077",
          "text": "Those services (minus cloud watch) are super cheap. Like millions of invocations and it's dirt cheap. What kind of traffic are you expecting?",
          "score": 2,
          "created_utc": "2025-12-31 06:28:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvtjj7",
          "author": "behusbwj",
          "text": "EC2 is an option (or running your own server), but you‚Äôre not gonna do that for obvious reasons I think.\n\nCloud is not simple. The second you start using serverless tech you‚Äôre dealing with a distributed system and the cognitive load that comes with that. The problem it solves is scalability and no infra management. If you would rather manage the infra, well go ahead and spin up a server. Infra management costs money because it usually saves you time, which also happens to be money.\n\nIf your concern is monitoring, you can create a single cloudwatch dashboard that combines metrics from anything in your account. The standard practice is to create one operational dashboard for monitoring critical services and a detailed dashboard for monitoring everything else. Thats just for catching anomalies. You should also be configuring alarms that email or page you.",
          "score": 1,
          "created_utc": "2025-12-31 08:26:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxc426",
          "author": "TechDebtSommelier",
          "text": "This is normal for cloud-native serverless architectures, but many teams avoid the complexity and cost by switching to a small EC2-based service: a single always-on application handles the HTTP request, runs the async logic, and publishes to a broker in one place, giving you fewer moving parts, simpler debugging, and more predictable costs at the expense of some managed scaling and infrastructure responsibility.",
          "score": 1,
          "created_utc": "2025-12-31 15:19:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxk03g",
          "author": "KayeYess",
          "text": "All you need is API Gateway (listener) and compute (Lambda). SQS is good for async/retries. EventBridge is another way of invoking Lambda. Not sure why you are using it if you only need to support web based triggers. Unless you are doing something special, Cloudwatch for most services is more or less out of the box. Also, you are managing nothing here. These are all \"managed\" services. You are essentially configuring them as per your requirements. If you were to build and manage similar services on your own, it would be far more complicated.and expensive¬†\n\nIf you find your existing setup expensive and complicated, you (or a qualified architect) should revisit the Architecture/Design.",
          "score": 1,
          "created_utc": "2025-12-31 15:59:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0q59u",
          "author": "_smartin",
          "text": "You could simplify architecture based on requirements. Why have an event broker AND a distributed queue? Also, if all lambda is doing is mapping and invoking another AWS service, cut it out. You can use an API Gateway AWS Integration type and use VTL to map requests and responses directly to and from the underlying AWS service you *ACTUALLY* need. Most people don‚Äôt know this because AWS and partners propagate a message of ‚Äúuse lambda‚Äù through all the examples.",
          "score": 1,
          "created_utc": "2026-01-01 02:32:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1kmth",
          "author": "poptimus_rhyme",
          "text": "Wouldn't something like n8n make this workflow easier?",
          "score": 1,
          "created_utc": "2026-01-01 06:23:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1lsmv",
          "author": "jagster247",
          "text": "I recommend SST",
          "score": 1,
          "created_utc": "2026-01-01 06:34:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvgdhv",
          "author": "NoMoreVillains",
          "text": "You really only need APIGW and lambda. If you don't want logs nothing is making you use CloudWatch. I'm not sure what the message and broker you're using are but if that's not needed then cut out SQS. Also you only pay for the traffic/duration/messages sent so why does it matter?",
          "score": 1,
          "created_utc": "2025-12-31 06:28:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvu3uf",
          "author": "r2yxe",
          "text": "You can get rid of the Eventbridge layer and maybe use lambda function urls or directly invoke lambda depending on the auth scenario. \n\nBut without some additional context its hard to say.",
          "score": 1,
          "created_utc": "2025-12-31 08:31:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwi73r",
          "author": "owengo1",
          "text": "For the complexity side, you can just  ask your favorite llm to generate a terraform for your project. You will have a full working PoC in one go, IaC, and you can maintain it with an llm. It will connect all the services and you will have a small bunch of file to manage your infra.\n\nFor the cost side it's another issue: clearly your costs will grow linearly with your traffic, so it will be very cheap as long you have very low traffic, and quickly something completely unaffordable with high traffic. Once again, you can ask an LLM to modelize the costs of your architecture and estimate the threshold at which you have to find something else.\n\n\"Something else\" could be cloudflare workers, or ALB + ECS instead of Gateway + Lambda, or a cheap graviton instance, ... Clearly you will have other constraints to manage but it's very likely you can save a significant amout of money with a less \"fully-managed\" solution.",
          "score": 1,
          "created_utc": "2025-12-31 12:13:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvm97q",
          "author": "Human-Possession135",
          "text": "I use AWS lightsail containers. One instance is $7 per month and offers up to 10 containers. I have a fastapi + redis + worker node. \n\nCovers the same usecase: a quick endpoint + task queue + retry logic + message broker\n\nFew months ago we got 25k signups in < 1 hour and it held up aside from the queue filling up",
          "score": 0,
          "created_utc": "2025-12-31 07:18:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvri5a",
          "author": "soundman32",
          "text": "Costs are crazy?  How much traffic are you expecting?\n\nAws first year is probably free, even for the services you mention.\n\nAlternatively, host your app yourself on your own server. Traffic will be minimal, you wont need a load balancer and multiple instances.  All you need is a domain name, a static ip address, and a server.\n\nWhen you NEED to scale, then start looking at cloud.",
          "score": 0,
          "created_utc": "2025-12-31 08:07:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvdtrr",
          "author": "serpix",
          "text": "Use terraform or cdk to set this up at least.\nI'd like to correct that you'll be getting the largest bill from Cloudwatch alone üëç",
          "score": -1,
          "created_utc": "2025-12-31 06:07:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwe4x8",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -2,
          "created_utc": "2025-12-31 11:39:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwzj0w",
              "author": "marx2k",
              "text": "ALB to Cloudfront you say?",
              "score": 1,
              "created_utc": "2025-12-31 14:10:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxd8kh",
          "author": "flyontimeapp",
          "text": "Because AWS is 15% of Amazon's revenue but 60% of its profit. More complexity is better for them because it locks you in.",
          "score": -2,
          "created_utc": "2025-12-31 15:25:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1puowoy",
      "title": "Ec2 Server Backup",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1puowoy/ec2_server_backup/",
      "author": "DARKSTAIN",
      "created_utc": "2025-12-24 14:42:21",
      "score": 24,
      "num_comments": 24,
      "upvote_ratio": 0.91,
      "text": "Hello Team, \n\nI have a file server in EC2 that I need to be able to backup and have the ability to recover individual files from at any given time. What solution is everyone using? I tried Druva, but I am not happy with how long it takes to spin up an image/mount it/ etc... Also, their support or at least the person I was working with seemed very novice. Please help. Here are the specs:\n\n\\* 1 Server - 4TB in size\n\n\\* Need to have a backup of 7 years \n\n\\* Need to be able to access the backup fairy quickly in order to restore individual files. \n\n\n\nThanks  ",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1puowoy/ec2_server_backup/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nvq2sa8",
          "author": "error-99999",
          "text": "Can you just use EBS volume snapshots/AWS Backup?",
          "score": 29,
          "created_utc": "2025-12-24 14:48:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvq3rin",
          "author": "Nearby-Middle-8991",
          "text": "First thing comes to mind is that might be mixing *operational* backup with archiving.\n\nFor operational backups (\"ec2 is down, need to create a new one\", maybe in another region), then AWS Backup.\n\nFor *archival*, I'd just dump to S3, turn on tiering, call it a day. cron job to sync, ensure you have error alerting for that job turned on (preferably a report of activities as well, even when it works so you know *how much* is working) and done. S3 versioning or just different prefixes...",
          "score": 22,
          "created_utc": "2025-12-24 14:53:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqkug4",
              "author": "pixeladdie",
              "text": "The same thought occurred to me.\n\nOP, what do you think about just running something like [restic](https://restic.readthedocs.io/en/latest/010_introduction.html) and pushing that to S3? Be warned - This may not be a good solution if you're going to leverage Deep Archive but since you mentioned needing quick restores, maybe you're not going that cold anyway.",
              "score": 2,
              "created_utc": "2025-12-24 16:26:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvq5ums",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 9,
          "created_utc": "2025-12-24 15:05:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqfeez",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 3,
              "created_utc": "2025-12-24 15:57:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvq3sth",
          "author": "oneplane",
          "text": "Can you kindly do the needful?",
          "score": 8,
          "created_utc": "2025-12-24 14:53:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvr6hwn",
              "author": "[deleted]",
              "text": "[removed]",
              "score": -1,
              "created_utc": "2025-12-24 18:23:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvrbhqp",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 0,
                  "created_utc": "2025-12-24 18:50:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvqenoa",
          "author": "tijiez",
          "text": "AWS Backup with item-level recovery for EBS (enable indexing)",
          "score": 3,
          "created_utc": "2025-12-24 15:53:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqygi6",
          "author": "vppencilsharpening",
          "text": "Depending on what your requirements are, it might be worth looking at Amazon Storage Gateway and replacing your current system with that. \n\nIt's an appliance that presents file shares that are primarily stored in S3, with local storage (EBS or whatever on-prem) used for caching. \n\nThe advantage for your use case is that every change to a file is automatically sync'd to S3 and with versioning enabled, you can pick and choose the version you need to restore. \n\nBecause you would run it in AWS you would have really fast access to S3 so could reduce the size of the EBS volumes significantly as they are only used for caching files on the appliance. With a little tuning you probably can get it down fairly small reducing your EBS cost (that shit is expensive). \n\nIndividual file restores are trivial, but require access to S3. Bulk file restores (like after a crypto attack) are a bit harder as you need to touch every file so a script to find and restore versions based on timestamp is probably necessary. \n\nYou also need to manage S3 lifecycle policies, which are a different than normal backup retention settings that we are used to. \n\nOn the plus side, if the virtual appliance ever gets corrupted, you just replace it. No need to try and fix it or restore it from a backup. \n\nIf you are paranoid you can copy the entire S3 bucket to another region, sync it to another cloud provider or both.",
          "score": 2,
          "created_utc": "2025-12-24 17:40:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvu4e3x",
          "author": "ohad1282",
          "text": "By the fact that you mention 7 years retention, looks like you need compliance-grade solution, not just copies of data. You need to ensure that the data is stored for 7 years, immutable (this is crucial!), ideally logically airgapped on another account and even better - organization. You need to have monitoring and notifications to ensure this is not accidentally changed by another admin in your company, etc.  \n  \nSo you need a real backup solution, not just snapshots of your EBS. Druva, Commvault, Rubrik, AWS Backup, Eon and others are the solutions you should take a look at. Some offer quick and granular file restore (such as Eon), and some do not.\n\nDisclaimer - I work for Eon. Happy to help about what I know about the market in general and of course Eon specific questions.",
          "score": 2,
          "created_utc": "2025-12-25 06:38:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzgqhx",
          "author": "ThigleBeagleMingle",
          "text": "Lots of regulated companies use AWS Fsx/EFS for this use case.",
          "score": 1,
          "created_utc": "2025-12-26 06:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvq3jv1",
          "author": "RecordingForward2690",
          "text": "AWS doesn't really do file-level backups of EC2 instances. They use volume-level backups instead (through EBS snapshots, if necessary orchestrated by AWS Backup.) It is certainly possible to restore such a snapshot to a new volume, mount the volume and restore individual files from those, but that's probably just as awkward as your Druva solution.\n\nFor an AWS native file-level solution, you can use \"aws s3 sync\" from a cron/Task Scheduler job to sync the content of your user directories to an S3 bucket. If the S3 bucket is then versioned and has a lifecycle policy to delete old versions after seven years, you've achieved your goals. You might also want to enable Intelligent Tiering to save on storage costs. And add an S3 Gateway Endpoint to your VPC to save NAT/Egress costs to the public S3 Endpoints.\n\nIf you want an enterprise-grade solution, look at products like Tivoli Storage Manager or any of the other backup tools you can find in any corporate data center. If your EC2 is Linux, you could even use the old UNIX backup tool to make incremental backups.\n\nHaving said this, why do you run a file server in an EC2 in the first place? If you're serving files to other EC2s in AWS, then wouldn't EFS/FSx be a much better solution (and one that is integrated with AWS Backup)? And if you're serving files to systems outside of AWS, why a fileserver in AWS and not something like AWS Storage Gateway with an S3 backend?",
          "score": 1,
          "created_utc": "2025-12-24 14:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqjsgn",
          "author": "barnaclebill22",
          "text": "Backup typically stores data in S3. Druva compresses and dedupes data before storing, so your consumption is a lot lower over time. But as you've noted, it can take longer to restore as the deduplication needs to be \"rehydrated\".\nDirect s3 snapshots can be restored faster, but consume a lot more space. The initial snap of an EBS volume containing 4TB of data will be 4TB. Subsequent snaps will usually be smaller unless you overwrite everything.\nIf restore from s3 isn't fast enough, you can use DRS. This is significantly higher cost but enables near-instantaneous recovery.",
          "score": 1,
          "created_utc": "2025-12-24 16:20:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqbylm",
          "author": "182RG",
          "text": "Use S3 Sync with versioning and maintenance rules on the EBS folders.\n\nUse a cron/Scheduler job.  Sync as often as you need to stay current.\n\nUse Backup for full recovery.",
          "score": 0,
          "created_utc": "2025-12-24 15:39:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqwybh",
              "author": "vppencilsharpening",
              "text": "If your going to go this way, it might be worth looking at S3 Storage Gateway. Then S3 becomes the primary storage with EBS volumes used for caching. \n\nLower EBS volume cost and no need to manually run S3 Sync as the storage gateway handles pushing it to S3.",
              "score": 3,
              "created_utc": "2025-12-24 17:31:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvw6s2e",
                  "author": "182RG",
                  "text": "Latency on S3 is too high.  We have some middleware (ERP) apps that process file data.  Most of it needs to be on EBS for performance.",
                  "score": 1,
                  "created_utc": "2025-12-25 17:12:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvqsdcb",
          "author": "think-flux",
          "text": "I like using rsync.net + Borg.\n\n\nIts deduped, encrypted and there are 0 ingress/egress fees.\n\n\nAlso, there is a special pricing tier for Borg users that do not use rsync.net snapshots.",
          "score": 0,
          "created_utc": "2025-12-24 17:06:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvq3vmw",
          "author": "mezbot",
          "text": "Not sure if it would meet your needs, but if it‚Äôs Windows I normally just enable VSS to capture versions that can be restored instantly (and capture changes between backups) and do an EBS backups/snapshot daily.  For individual file restores I just create a volume from the individual snapshot (from whatever day) and mount it as another drive letter, restore the files, then delete the volume.  You can do the same if you back it up as an AMI too (restore individual volumes).  The benefit of the AMI backup is that it can be made VSS aware.\n\nIt‚Äôs funny because as I write this I am doing Druva restore testing, Druva is horrible.",
          "score": -2,
          "created_utc": "2025-12-24 14:54:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pu35sw",
      "title": "End of 2025 state of Serverless Framework question",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pu35sw/end_of_2025_state_of_serverless_framework_question/",
      "author": "jaredce",
      "created_utc": "2025-12-23 19:45:16",
      "score": 21,
      "num_comments": 33,
      "upvote_ratio": 0.84,
      "text": "It's nearly the end of 2025 and I'm wondering how many people are still using Serverless Framework and how many are making plans to move off of it in 2026.\n\nMy company has about 40 microservices with maybe a 1/3rd of them using or moved to CDK and the rest of them still using a version of Serverless Framework 3.xx.  \n\nI still quite like Serverless Framework, and it's a shame they had to start charging for v4, but I can understand why they went that route and don't begrudge them. If they do make money from it, more power to them. \n\nMy colleague has been busy creating a CLI that will make generating new CDK baked API gateway and lambda based APIs slightly easier, though he was complimenting how the Serverless people had managed to wrangle some of the intricacies of CDK. \n\nI have created one nice plugin for the Serverless Framework that helps with OpenAPI definitions, and must admit I'm a little unsure how I'll port that/make something similar for CDK. I'm also in the middle of creating an Arazzo plugin for Serverless Framework. One thing they did really well was building out a decent plugin system. \n\nServerless Framework 3 is pretty much EOL now, so unless you're willing to pay for 4, what are your plans for something similar? \n\n",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pu35sw/end_of_2025_state_of_serverless_framework_question/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nvloef8",
          "author": "Thommasc",
          "text": "Sticking to v3. No plan to move away from it.\n\n\nIt has too many useful features with its extra plugins for me to replace it by any alternative at the moment.\n\n\nI also use it for bref.sh",
          "score": 9,
          "created_utc": "2025-12-23 20:14:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvopume",
              "author": "kcbh711",
              "text": "https://github.com/oss-serverless/serverless\n\n\na drop in replacement for v3 with some updates",
              "score": 2,
              "created_utc": "2025-12-24 08:05:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvp0ph4",
                  "author": "Thommasc",
                  "text": "Thanks! This is great!",
                  "score": 1,
                  "created_utc": "2025-12-24 09:53:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvljh5i",
          "author": "jZma",
          "text": "We'll start moving stuff to CDK in next year - gg",
          "score": 14,
          "created_utc": "2025-12-23 19:48:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs3e3r",
              "author": "LordWitness",
              "text": "This is the way\n\nI believe that many people who still use Serverless Framework do so because they don't know nothing about CDK.\n\nAPI Gateway + S3 + DynamoDB + Route53 + 3 Lambdas with Layers. \n\nAll this in less than 100 lines of Python code. I prefer CDK even more than Terraform, depending on where in the system I'm building my IaC.",
              "score": 3,
              "created_utc": "2025-12-24 21:32:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3douz",
                  "author": "Cautious-Tiger8211",
                  "text": "With serverless framework this is a lot less than 100 lines though.",
                  "score": 2,
                  "created_utc": "2025-12-26 22:11:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvmfg47",
          "author": "Quinnypig",
          "text": "I did a full rip and replace earlier this year in favor of CDK. Was dragging my feet on it, but Claude Code made it a lot less painful.",
          "score": 7,
          "created_utc": "2025-12-23 22:37:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvn9hi7",
          "author": "suinp",
          "text": "We moved to sst, it's serving us well",
          "score": 3,
          "created_utc": "2025-12-24 01:37:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmp3af",
          "author": "Syrbor493",
          "text": "I have started the move to manage the infra with terraform and have in house scripts to manage local setups and deployment.\n\nIt integrates very well with the rest of our infra in tf and we did not even need advanced features that come with something like SAM.",
          "score": 2,
          "created_utc": "2025-12-23 23:33:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvndzsn",
          "author": "Perryfl",
          "text": "our app was origionally built 100% serverless. 2 years ago we decided it was kinda pointless. we have since combined the lambdas and rewrote then into a few different serivices which we migrafed to EC2, the later off the cloud all together... we are oaying about 20% of what we were paying aws and our oerformance has skyrocketed... we moved to bare metal whixh your not in some virtualized envireonment sharing a physical node with god knows how many other users",
          "score": 2,
          "created_utc": "2025-12-24 02:06:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw82sk6",
          "author": "ExplanationHot4568",
          "text": "> My colleague has been busy creating a CLI that will make generating new CDK baked API gateway and lambda based APIs slightly easier\n\nYou should stop your colleague right now. Creating additional layers of abstraction around CDK is understandable, that's how we developers think after all. It's one of the most common mistakes you can make with CDK though.\n\nCDK is already clean and clear as it is. Adding these layers will make you feel very clever and save you some LoC but add so much mental overload that it is never worth it. Onboarding new colleagues (especially the ones that already know CDK) will become a lot harder. The consulting firms you have to involve because you fucked up big time will love it that they can charge so many extra hours.\n\nCDK is the best when it comes to IaC because code is always superior to declarative forms when it comes to describing infrastructure. You need things like \"this ID has to be some combination of other IDs\" all the time. Code beats anything else here. The heavy downside is: it's code. Ask ten developers and you'll get twelve solutions.\n\ntl;dr: Don't create additional abstractions around CDK. You're digging your own grave. Write business constructs, not functional ones. Do not fear some extra LoCs, CDK is very readable on it's own.",
          "score": 2,
          "created_utc": "2025-12-27 17:50:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq5ea1",
          "author": "bot403",
          "text": "Serverless V4 is like $15/mo for our company making seven figures in revenue on our app. So we did the break even analysis and we're fine paying for a few centuries of serverless V4 vs the dev work to move our app off it.",
          "score": 2,
          "created_utc": "2025-12-30 13:21:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvml3l6",
          "author": "extraandre",
          "text": "We completely moved to CDK at the start of this year and definitely don't regret it. At the beginning we created some common constructs that we use in all projects then.",
          "score": 2,
          "created_utc": "2025-12-23 23:09:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvol614",
          "author": "mathilda-scott",
          "text": "We‚Äôre in a similar spot. v3 still works, but with EOL it‚Äôs hard to justify long-term. Most teams I see are standardizing on CDK or Terraform for new services and only migrating Serverless apps when there‚Äôs active work. CDK‚Äôs verbosity is the tradeoff, but native AWS support and no framework lock-in usually win out. A gradual ‚Äúnew stuff in CDK, old stuff maintained as-is‚Äù approach seems to be the least painful.",
          "score": 1,
          "created_utc": "2025-12-24 07:21:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3e151",
              "author": "Cautious-Tiger8211",
              "text": "Just use the fork¬†https://github.com/oss-serverless/serverless",
              "score": 2,
              "created_utc": "2025-12-26 22:13:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvomxgm",
          "author": "teroa",
          "text": "You guys who moved to CDK, do you it with SAM for local invoke or how do you develop? I really like CDK, but current team is big fan of local development and therefore we migrated to SST.\n\nDevelopment experience with SST is awesome, but I'm slightly worried committing to SST because of small core team developing the framework. \n\nIn previous company we used CDK, but there we had less Lambdas and more Fargate. If we would have something similar to `sst dev` CDK would be almost perfect.",
          "score": 1,
          "created_utc": "2025-12-24 07:38:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtfgqt",
          "author": "cjrun",
          "text": ":::cries in Terraform:::",
          "score": 1,
          "created_utc": "2025-12-25 03:09:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2pptn",
          "author": "donkanator",
          "text": "SAM should have been discontinued the moment CDK came out. The product that promised to help a little with CFN, but really required additional learning, CLI, build resources, was made to look like a mistake in timeline that took to get from cfn to cdk",
          "score": 1,
          "created_utc": "2025-12-26 20:01:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3dhfl",
          "author": "Cautious-Tiger8211",
          "text": "Just use https://github.com/oss-serverless/serverless which is a drop-in replacement of v3.\n\n\nContacted Serverless Inc multiple times to discuss Enterprise pricing but they just didn't bother to reply. Now they get nothing from me :)",
          "score": 1,
          "created_utc": "2025-12-26 22:10:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3gzxp",
              "author": "AWSSupport",
              "text": "Hi there, \n\nWe'd like to look into this for you. Chat message us your case number and any further details. \n\n\\- Gee J.",
              "score": 1,
              "created_utc": "2025-12-26 22:30:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5skxn",
                  "author": "Cautious-Tiger8211",
                  "text": "Since when is Serverless Inc part of AWS?",
                  "score": 1,
                  "created_utc": "2025-12-27 08:14:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvljtdz",
          "author": "uncleguru",
          "text": "We're still using it. I'd like to migrate to SAM but the inability to import API Gateways into our templates makes it impossible. \nI've not checked in a while, maybe they support it now. Until then we're planning to stay on serverless 3.x.",
          "score": 1,
          "created_utc": "2025-12-23 19:50:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3dryl",
              "author": "Cautious-Tiger8211",
              "text": "Use the fork¬†https://github.com/oss-serverless/serverless",
              "score": 2,
              "created_utc": "2025-12-26 22:12:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvlvsn8",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -5,
              "created_utc": "2025-12-23 20:53:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvlyc4r",
                  "author": "uncleguru",
                  "text": "Why?",
                  "score": 1,
                  "created_utc": "2025-12-23 21:07:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvljobm",
          "author": "cranberrie_sauce",
          "text": "people still use it? every single time I see serverless its a sh\\*t show old app\n\nhard to iterate on and unfit for llm coding",
          "score": 0,
          "created_utc": "2025-12-23 19:49:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvlk7am",
              "author": "spicypixel",
              "text": "The era of the monolith that Claude can go wild with is here.",
              "score": 9,
              "created_utc": "2025-12-23 19:52:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvlm5j7",
                  "author": "cranberrie_sauce",
                  "text": "modular monoliths ftw. code colocality, not delayed by who owns aws account.  many benefits.\n\nMy sense is - most of the people that were bringing server-less to companies are juniors who are now not in great demand.\n\nI declare the end of the serverless days  \n\nedit: aws sees writing on the wall. new lambda product (basically the \"invention\" of always on server as a lambda by AWS) - is pretty telling: [https://aws.amazon.com/blogs/aws/introducing-aws-lambda-managed-instances-serverless-simplicity-with-ec2-flexibility/](https://aws.amazon.com/blogs/aws/introducing-aws-lambda-managed-instances-serverless-simplicity-with-ec2-flexibility/)",
                  "score": -9,
                  "created_utc": "2025-12-23 20:02:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvlqs0j",
          "author": "HelpfulFriend0",
          "text": "Serverless is just a billing/perf model. If you have fast, non io bounded work, that doesn't mind cold start problems, it's great and cheap. If your workload doesn't match these characteristics, it's probably not a good workload for serverless to save you money",
          "score": -15,
          "created_utc": "2025-12-23 20:26:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvlr5hj",
              "author": "jaredce",
              "text": "What?",
              "score": 3,
              "created_utc": "2025-12-23 20:28:50",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvm0g9s",
              "author": "uncleguru",
              "text": "Op is talking about the serverless framework",
              "score": 2,
              "created_utc": "2025-12-23 21:18:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwe505",
      "title": "Console Hanging",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pwe505/console_hanging/",
      "author": "The-Wizard-of-AWS",
      "created_utc": "2025-12-26 20:28:50",
      "score": 19,
      "num_comments": 17,
      "upvote_ratio": 0.95,
      "text": "Is it just me, or are others running into the console hanging lately. I mostly run into it when I‚Äôm in CloudWatch. It‚Äôs so bad that I have to kill my browser to recover. Multiple computers, different accounts. ",
      "is_original_content": false,
      "link_flair_text": "console",
      "permalink": "https://reddit.com/r/aws/comments/1pwe505/console_hanging/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nw2v944",
          "author": "rocketbunny77",
          "text": "Yep. I've noticed it when a tab is in the background and I come back to it later",
          "score": 10,
          "created_utc": "2025-12-26 20:31:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2ymig",
              "author": "Psych76",
              "text": "This is when I see it basically, like if I had a few open and my system goes to sleep or they‚Äôre in the background low process wise, I come back and they‚Äôre hung and even a refresh of the page fails to load or only some bits load.  Killing that tab and retrying in a fresh one works.",
              "score": 4,
              "created_utc": "2025-12-26 20:50:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw30pim",
                  "author": "rocketbunny77",
                  "text": "Yes that is the detailed explanation of my problem",
                  "score": 2,
                  "created_utc": "2025-12-26 21:01:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw4c7ye",
              "author": "yesman_85",
              "text": "This happens very often, basically always have to refresh after coming back to a tab.¬†",
              "score": 1,
              "created_utc": "2025-12-27 01:38:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2y0bx",
          "author": "abofh",
          "text": "Yeah, open in New tab in Firefox fails regularly with some other aws console tab has eaten the CPU error",
          "score": 2,
          "created_utc": "2025-12-26 20:46:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2yc9a",
          "author": "cunninglingers",
          "text": "Yep, anecdotally have noticed the same recently. We use MS Edge in my company. Can't say I've noticed any pattern other than it started happening some time in the last month and it's when you come back to a tab after a little while. Probably something to do with the sleep feature of tabs in Chromium not playing nicely with the console for some reason.",
          "score": 2,
          "created_utc": "2025-12-26 20:48:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw30wvc",
          "author": "dragonnfr",
          "text": "CloudWatch hangs are killing productivity. I've had to switch to CLI for metrics when the web UI freezes. AWS needs to fix this.",
          "score": 2,
          "created_utc": "2025-12-26 21:02:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2xckc",
          "author": "AWSSupport",
          "text": "Hello,\n\nAppreciate you sharing your thoughts on this. I've passed it along for our team to review. \n\nIf you've any further suggestions, this option is available: http://go.aws/feedback. \n\n\\- Elle G.",
          "score": 1,
          "created_utc": "2025-12-26 20:43:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2z49r",
              "author": "anotherNarom",
              "text": "I've filled out the console feedback form daily, and on every single occasion it has crashed. \n\nIt's browser agnostic and has to be affecting your own staff.",
              "score": 5,
              "created_utc": "2025-12-26 20:52:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw32oqj",
                  "author": "AWSSupport",
                  "text": "Thanks for sharing this.\n\nI've forwarded this to our team. It would be best to also open a Support case for this matter, so we can investigate the issue further. You can begin the process of creating one here.: http://go.aws/support-center.\n\n\\- Elle G.",
                  "score": 5,
                  "created_utc": "2025-12-26 21:12:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw881y3",
          "author": "dxlee90",
          "text": "Mostly happens for me with bigger DynamoDB tables, when I switch back to the tab after a while",
          "score": 1,
          "created_utc": "2025-12-27 18:16:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbxwht",
          "author": "advanderveer",
          "text": "What fixed it for me was enabling functional cookies for the console.",
          "score": 1,
          "created_utc": "2025-12-28 07:53:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcj53x",
          "author": "hyperInTheDiaper",
          "text": "Same here. Even the console homepage/athena freeze quite often these days",
          "score": 1,
          "created_utc": "2025-12-28 11:18:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxtlwv",
      "title": "AWS Support Nightmare",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pxtlwv/aws_support_nightmare/",
      "author": "theHephestus",
      "created_utc": "2025-12-28 15:12:11",
      "score": 16,
      "num_comments": 11,
      "upvote_ratio": 0.7,
      "text": "I am a long time lurker, I always read about AWS support horror stories here and I did not think it was that bad until a few days ago its still ongoing.  TLDR AWS support sucks ass.\n\nI have AWS Business Support +. AWS restricted my account after a security alert. I complied with all the remediation needed, even had to explain that CI/CD activity from GitHub Actions IP != human sign-in location.\n\nNow support is repeatedly insisting I delete EKS node group IAM roles that are actively in use, required for node groups to operate, and properly scoped standard EKS worker/ECR/CNI policies.\n\nThey haven‚Äôt provided any concrete justification beyond generic shared responsibility text and a link to how to delete a role. \n\nAnyone been through this? How did you escalate to get an actual security rationale or get restrictions lifted? Any success getting service credits for the delay?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pxtlwv/aws_support_nightmare/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwdpitm",
          "author": "clintkev251",
          "text": "If the roles were created or modified around the time of the compromise, they probably have these flagged as suspicious. You need to explain to them a few things, 1. The roles were created by you and have been validated as not suspicious, 2. They are currently in use for an active production workload and you will not be deleting them. If they don't listen, reiterate, request they escalate, remind them you consider this to be a false positive.",
          "score": 20,
          "created_utc": "2025-12-28 16:02:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwds2eb",
          "author": "cachemonet0x0cf6619",
          "text": "what‚Äôs stopping you from creating a new role with the same policies and swapping to that?",
          "score": 26,
          "created_utc": "2025-12-28 16:15:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdru7o",
          "author": "pipesed",
          "text": "What are the trust policies for these roles, and can you determine in cloudtrail what assumed these roles in the past?",
          "score": 9,
          "created_utc": "2025-12-28 16:14:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdoutw",
          "author": "AWSSupport",
          "text": "Hello,\n\nApologies for any frustrations you've encountered.\n\nYou can share your case ID via chat message, so we can pass along your concerns.\n\n\\- Elle G.",
          "score": 12,
          "created_utc": "2025-12-28 15:59:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe0oj7",
          "author": "Nearby-Middle-8991",
          "text": "Support is always a crapshoot, and AWS isn't the only one. It's a large pool of people, some are experienced, some are not. Some turn off their brains and follow the script blindly.\n\nFor AWS specifically, what worked for me was to check the working hours of whomever had the ticket, and then raise a chat *outside* of those, so I'd get someone new.  Alternatively, depending on your level of support, go light a fire under the TAM. But TAM experience was about as much as crapshoot for me, so idk...\n\nThe script being followed there probably assumes you roles were compromised. That's easy to check on cloudtrail, but can't really be sure without the details. Swapping roles shouldn't be that much trouble...",
          "score": 6,
          "created_utc": "2025-12-28 16:58:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfp8qf",
          "author": "pausethelogic",
          "text": "‚ÄúAWS support sucks ass because I don‚Äôt understand what they‚Äôre telling me to do‚Äù lol\n\nAWS takes security seriously, so they‚Äôre not going to be super flexible about you continuing to use a role that they‚Äôve marked as compromised\n\nAre you claiming that they‚Äôre wrong and the role wasn‚Äôt compromised or part of a security incident?",
          "score": 6,
          "created_utc": "2025-12-28 21:50:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg56yu",
          "author": "coinclink",
          "text": "I understand from your perspective that this seems like they are being dumb, but they are actually doing exactly what they are supposed to do. They haven't been assured that these roles weren't modified to allow the attacker to assume them during your compromise. All they see on their end is timestamps showing that the roles WERE created or modified during your compromise.\n\nDon't be upset with AWS because of your perceived superiority of knowledge. You should have a little humility instead. YOU (or your team) fat fingered and leaked a key and you shouldn't be upset with AWS taking drastic measures to ensure infrastructure security. (it's ok, we've all done it once or twice, I'm not trying to shame you, BUT YOU SHOULD BE ASHAMED, that's what stops you from doing it again!)",
          "score": 3,
          "created_utc": "2025-12-28 23:12:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwib6qq",
          "author": "IridescentKoala",
          "text": "Why won't you remove the roles?",
          "score": 1,
          "created_utc": "2025-12-29 07:20:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwivkls",
          "author": "devguyrun",
          "text": "products/services aside, AWS support is best in class in my opinion, most if not all in the support org are highly technical folks and know what they are talking about, when they don't , they always check in with others that do.\n\none should be smart enough to know when they are not being so....",
          "score": 1,
          "created_utc": "2025-12-29 10:29:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg3e72",
          "author": "isoAntti",
          "text": "They never mention these cases when they say go cloud/aws",
          "score": -2,
          "created_utc": "2025-12-28 23:03:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pu3itd",
      "title": "About to start as an AWS L5 SA - how should I maximise the onboarding period?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pu3itd/about_to_start_as_an_aws_l5_sa_how_should_i/",
      "author": "Estatic_moose_875",
      "created_utc": "2025-12-23 20:00:08",
      "score": 15,
      "num_comments": 28,
      "upvote_ratio": 0.76,
      "text": "I‚Äôm joining AWS as an L5 Solutions Architect in the ISV team and would really value some advice from current or former AWS SAs.\n\nI‚Äôve been told to expect a 3 month onboarding period, but beyond that I don‚Äôt yet have much insight into what the first 3‚Äì6 months looks like.\n\nI‚Äôd love to hear:  \n‚Ä¢ What your first 3‚Äì6 months looked like  \n‚Ä¢ What you wish you‚Äôd focused on more (or less) during onboarding  \n‚Ä¢ What tends to differentiate strong SAs early vs people who struggle  \n‚Ä¢ Any common mistakes you see new SAs make  \n‚Ä¢ What good performance realistically looks like at L5 in the first 6 months\n\nAny advice would be hugely appreciated - thank you!",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pu3itd/about_to_start_as_an_aws_l5_sa_how_should_i/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nvm3ty5",
          "author": "cloudnavig8r",
          "text": "The biggest mistake I‚Äôve seen, is rushing to get through your embark plan and be on your own with customers.\n\nYou may feel pressure from your account teams and maybe even your manager or peers.  Protect that time, you will never get it back!\n\nImposter syndrome is real.  You were hired for a reason, and that means that your interviewers believed in you being able to be performing better than half your team within 3-6 months.  So believe in yourself.\n\nLearn how to work with each different account manger- they are your biggest internal stakeholder.",
          "score": 42,
          "created_utc": "2025-12-23 21:35:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp8rgj",
              "author": "Estatic_moose_875",
              "text": "Thank you for the insight and the encouragement!\n\nWhat is the embark plan, and when will you know if you're actually ready?",
              "score": 2,
              "created_utc": "2025-12-24 11:11:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvrzmya",
                  "author": "cloudnavig8r",
                  "text": "Embark will be a 90-day list of things that your manager has you do to on-board.  Like watching all the compliance videos, meeting people, getting certifications and complete SA Onboarding (whatever it is called now).\n\nThe SA training usually includes a mock customer engagement that is evaluated by other SAs.  You will need to pass this process as part of your on boarding.\n\nYou might be over confident, or lack confidence.  But these type of ‚Äúcheck rides‚Äù are intended to provide feedback - not personal. I know several people that did not pass their first attempts, it is not career limiting.  It‚Äôs all part of learning.",
                  "score": 5,
                  "created_utc": "2025-12-24 21:09:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvlm6ef",
          "author": "Doormatty",
          "text": "You'll be learning to breath underwater for the first 3-6 months.\n\nJust go with the flow, and learn as much as you can about the process.\n\nBy the time you're starting to get a handle on things, check \"Old Fart\" on the intranet, and be scared about how senior you are now.",
          "score": 14,
          "created_utc": "2025-12-23 20:02:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvlopbz",
              "author": "Active-Isopod-3656",
              "text": "Learning to drink from the firehose...",
              "score": 3,
              "created_utc": "2025-12-23 20:15:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvn47pl",
          "author": "Sirwired",
          "text": "- Build and learn relentlessly.  \n- Learn from your account manager(s) what services and features your customers use most and concentrate your efforts.  \n- Use your Awsome Builder project to stretch your limits, but not too much that it takes too much time away from everything else.\n- Get a Mac, unless you are really comfortable with WSL, or your manager instructs you to get Windows.\n- If you don't have SAA yet, start studying now, because you'll have little time to study after your onboarding is over.\n- Don't let your AM's rope you into doing useful work during onboarding.\n- Learn Isengard (the internal account vending system) ASAP and take advantage of the fact that you can spin up almost whatever you want without worrying overmuch about the bill. Treat accounts like the disposable objects they are so you don't worry so much about leaving things running. My manager said that unless I was spending over $1,200 or so a month in AWS charges, I didn't even need to bother telling him. And if I was spending over it, I just needed a reason for it. (It's not really that much money, but the list-price bill is easier to find than the one that says how much it all actually costs.)\n\nYour first six months mean you got your Embark done, you are building a network of people to help you, you've established a good rapport with your AM's and your customer, and you are starting to deliver at least some value. (Running technical meetings solo, presenting demos, working out solutions)",
          "score": 9,
          "created_utc": "2025-12-24 01:04:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp6f5l",
              "author": "Estatic_moose_875",
              "text": "This is so helpful - thank you! I really like the tip of learning from the AM's to share the most commonly used services. I was conscious about the sheer quantity of learning that would be required but this eases that somewhat. \n\nWhat's the Awesome Builder project?",
              "score": 1,
              "created_utc": "2025-12-24 10:49:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvu5wqj",
                  "author": "tnstaafsb",
                  "text": "AWSome builder is the major activity to establish your readiness to be in front of customers.  You get a set of requirements from a mock customer, and you go through three stages.  First stage is typically a presentation, second is a whiteboarding session, and third is a demo you'll build out.  It can be pretty stressful, but remember everyone wants you to pass it and most are willing to help you learn.  It's also fine to fail the first time as long as you're showing progress.",
                  "score": 2,
                  "created_utc": "2025-12-25 06:53:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvlv3vz",
          "author": "IbuHatela92",
          "text": "Learn how to sell AWS in a much better way. Because that‚Äôs what you will be doing once you join. Hahahaha",
          "score": 17,
          "created_utc": "2025-12-23 20:50:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmf75b",
              "author": "Quinnypig",
              "text": "Yeah, I'm really struggling to see how moving the SA org under sales isn't going to be a disaster for customers, and by extension the company longer term.",
              "score": 4,
              "created_utc": "2025-12-23 22:36:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvn2dq0",
                  "author": "Sirwired",
                  "text": "Errr.. SA has always been a pre-sales position, hasn't it?",
                  "score": 13,
                  "created_utc": "2025-12-24 00:53:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvoarq9",
                  "author": "Harsha_7697",
                  "text": "Well it has always been under sales. SAs are pre-sales. TAMs are post sales",
                  "score": 3,
                  "created_utc": "2025-12-24 05:49:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvmtg38",
                  "author": "jacksbox",
                  "text": "Wait wasn't SA always under sales? They're present on all the pre sales stuff. I thought they had spend targets too.",
                  "score": 5,
                  "created_utc": "2025-12-23 23:59:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw57ri1",
              "author": "jumpstart_999",
              "text": "Thats a myopic and amateur view. Wont be surprised if you lasted a even a year (in case you ever worked for them). All the aws sa‚Äôs i have met have some of their own expertise. Being able to solve business challenges using your own product is what everyone is looking for.",
              "score": 1,
              "created_utc": "2025-12-27 05:11:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw57yxb",
                  "author": "IbuHatela92",
                  "text": "Solve business challenges ONLY using AWS stack xD. Yeah big challenge I see.",
                  "score": 1,
                  "created_utc": "2025-12-27 05:13:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvmwiy7",
          "author": "classicrock40",
          "text": "First, you should spend the time going through embark and the tasks presented.  You'll learn culture, standard messaging, bit of product, maybe build a team project. \n\nSecond, use that time to expand your network. If you know your AM, at least get to know them a bit, maybe get a brief on the customers. You shouldn't have to take meetings nor get engaged in opportunities,  but if you have time it doesn't hurt to listen in. \n\nWhile embark is supposed to last 3 months, make sure you are completing tasks per your managers expectations.",
          "score": 5,
          "created_utc": "2025-12-24 00:18:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp7za9",
              "author": "Estatic_moose_875",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2025-12-24 11:04:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvmoc1t",
          "author": "Opposite_Date_1790",
          "text": "The way you maximize it is by not taking it for granted. Build solutions in your sandbox, fill in knowledge gaps you have, and learn how to be a customer advocate in addition to a salesperson. Do not rush out of embark.",
          "score": 2,
          "created_utc": "2025-12-23 23:28:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmvk4t",
          "author": "antimoto",
          "text": "Build, build and build. ISV customers tend to be more advanced and more nimble in roadmap than enterprise/pubsec. If you want any chance of building trust with customers (which will affect your influence and therefore performance) you need to run ahead of customers' issues and have practical knowledge/experience. The embark material is a bunch of baseline LP and ways of working stuff, some useful, some not.",
          "score": 1,
          "created_utc": "2025-12-24 00:12:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvov19s",
          "author": "dataflow_mapper",
          "text": "Congrats, that‚Äôs a solid role. From what I‚Äôve seen, the people who do well early focus less on memorizing every service and more on understanding how AWS actually makes decisions with customers and partners. Spend time learning how successful SAs tell stories, not just architectures. Shadow as many calls as you can and ask why certain tradeoffs were made. A common mistake is trying to prove technical depth too fast instead of building trust with account teams. At L5 in the first six months, being dependable, prepared, and easy to work with matters more than being the smartest person in the room.",
          "score": 1,
          "created_utc": "2025-12-24 08:56:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp98nt",
              "author": "Estatic_moose_875",
              "text": "Thank you! \n\nI like that you've come at it from a different angle - where i'm mostly concerned about the services and depth, I need to remember it's not just the tech that matters.",
              "score": 2,
              "created_utc": "2025-12-24 11:16:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvqgczf",
          "author": "forsgren123",
          "text": "Read the SA Role Guide to see what's expected at L5.",
          "score": 1,
          "created_utc": "2025-12-24 16:02:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvrzpp9",
          "author": "Stock_Dependent_6208",
          "text": "Ask questions!",
          "score": 1,
          "created_utc": "2025-12-24 21:10:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmcfa0",
          "author": "clandestine-sherpa",
          "text": "Be careful we have layoffs at the end of January.",
          "score": -3,
          "created_utc": "2025-12-23 22:21:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pwtnty",
      "title": "Update: I added \"Ghost\" EKS filtering and Tag Suppression to my AWS Garbage Collector (v1.2.5) based on your feedback.",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pwtnty/update_i_added_ghost_eks_filtering_and_tag/",
      "author": "DrSkyle",
      "created_utc": "2025-12-27 09:20:23",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "I posted my \"Forensic Cloud Accountant\" for AWS here last week and the feedback was honestly super helpful.  I  did some updates on the detection engine to be less aggressive and smarter about false positives.\n\nThe big changes in v1.2.5:\n\n**first , EKS Ghost Detection** Standard autoscalers often keep Node Groups active solely to run daemonsets (like `kube-proxy` or `aws-node`), even when no user applications are running. The tool now filters out this system noise. If a Node Group is burning cash but only serving system pods, it gets flagged as a \"Ghost.\" This also includes a check for \"Zombie Control Planes\" (clusters idling with 0 nodes for >7 days).\n\n**second , trap door analysis** This feature targets configuration drift. Specifically, it detects Fargate profiles that are targeting namespaces that have been deleted. The tool validates profiles against the current cluster state to flag these broken links/config debt.\n\nand also Safety Tags (Thanks u/pint) for pointing out \"Idle\" doesn't always mean \"Abandoned.\" I didn't want people accidentally nuking a dev spike, so I added a simple tag override. You can now tag any AWS resource with cloudslash:ignore to whitelist it. You can even set it to expire (e.g., 2026-01-01) or base it on cost (cost<15).\n\n\n\nPricing/Repo A few people asked about the business model. I‚Äôm keeping the Pro remediation as a one-time $49 license (lifetime). I really dislike subscriptions for local CLI tools, so I'm not doing that. The core scanner is still AGPL and free to use.\n\nRepo:[https://github.com/DrSkyle/CloudSlash](https://github.com/DrSkyle/CloudSlash)\n\n(P.S. To u/bqw74 \\- I finally fixed that annoying [install.sh](http://install.sh) bug, sorry about the mess).\n\nLet me know if this version feels a bit smarter on your clusters and what else i should add to  make cloudslash more helpful for your specific workflow.",
      "is_original_content": false,
      "link_flair_text": "monitoring",
      "permalink": "https://reddit.com/r/aws/comments/1pwtnty/update_i_added_ghost_eks_filtering_and_tag/",
      "domain": "self.aws",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pywi5h",
      "title": "What FinOps practices worked for you in 2025?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pywi5h/what_finops_practices_worked_for_you_in_2025/",
      "author": "TehWeezle",
      "created_utc": "2025-12-29 20:02:08",
      "score": 10,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "Am stuck here prepping budget reviews and am thinking what worked and what flopped this year. Lets share our experiences here. I would love to hear the numbers.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pywi5h/what_finops_practices_worked_for_you_in_2025/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwlsakr",
          "author": "Old-Astronomer3995",
          "text": "The simplest and the most popular:\n- Still migrating clients from gp2 to gp3\n- Proper backup policies and reviewing snapshots \n- EBS volumes for EKS review\n- Teaching developers that they can resize disks \n- Reducing number of load balancers\n- Reducing number of ipv4",
          "score": 6,
          "created_utc": "2025-12-29 20:14:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nws6e09",
              "author": "Dangerous-Sale3243",
              "text": "What kind of development requires resizing disks these days?",
              "score": 1,
              "created_utc": "2025-12-30 19:22:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwsblx8",
                  "author": "Old-Astronomer3995",
                  "text": "Databases, storage for AI models (in my cases)",
                  "score": 1,
                  "created_utc": "2025-12-30 19:47:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwoe87m",
          "author": "inevitable_hunk",
          "text": "Migrating from t3 to t4g",
          "score": 5,
          "created_utc": "2025-12-30 04:40:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwmef28",
          "author": "vladputenesca",
          "text": "Implementing cloud custodian to shutdown instances at weekends and evenings in non prod environments.",
          "score": 2,
          "created_utc": "2025-12-29 22:02:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxklpy",
          "author": "Sadhvik1998",
          "text": "Used Yeedu.io. Saved more than 60% costs compared with Databricks for spark computes\n\nFrom almost 50k $ spends per month to less than 20$ per month..",
          "score": 1,
          "created_utc": "2025-12-31 16:02:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq8vgs",
          "author": "FinOps_4ever",
          "text": "\\* Graviton everywhere all at once  \n\\* Selecting the best storage class for DDB tables, S3, EFS, etc based on observed patterns of usage  \n\\* Right size provisioned IOPS for EBS - measured the p100 and adjusted each volume accordingly  \n\\* Lots and lots of small RI and SP purchases to give us more granular layers to manage vs. having coverage be a large step function  \n\\* Automate operational hygiene functions like cleaning up unused resources that can't impact production via cloud custodian  \n\\* Promote turning off resources when not in use in non-prod environments...even if it is only for a few hours",
          "score": 0,
          "created_utc": "2025-12-30 13:42:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyc5zb",
      "title": "Denial of Wallet Via Route 53?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pyc5zb/denial_of_wallet_via_route_53/",
      "author": "Extra-Moose4828",
      "created_utc": "2025-12-29 04:08:43",
      "score": 9,
      "num_comments": 7,
      "upvote_ratio": 0.8,
      "text": "I am wondering if anyone knows if a Denial of Wallet attack via Route 53 is possible??\n\n  \nThe pricing for Route 53 is $0.40 per million queries per month.\n\n  \nI know that this can be avoided by pointing the DNS records to an AWS resource (as described here: [https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/configuring-route53-for-cost-protection-from-nxdomain-attacks.html](https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/configuring-route53-for-cost-protection-from-nxdomain-attacks.html) ).\n\n  \nBut let's say that's not an option. Is it even feasible for an attacker to send enough DNS queries to rack up a substantial (>$100) bill?? O  \nMy napkin math tells me that to get to >$100, they would need to send 250 million requests in a month. Which I think sounds possible?? \n\n  \nHas anyone ever witnessed such an attack?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pyc5zb/denial_of_wallet_via_route_53/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwj3ww5",
          "author": "Sirwired",
          "text": "If someone wants to rack up your AWS bill, they are definiteily likely to choose other routes besides R53.  (In the big scheme of things, a $100/mo bill isn't considered \"substantial\" at all.)",
          "score": 27,
          "created_utc": "2025-12-29 11:43:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiymmo",
          "author": "jmkgreen",
          "text": "They could. Question is why. You wouldn‚Äôt likely be causing a DoS and it‚Äôs probably the case AWS would raise their shields faster than you would notice.\n\nThat‚Äôs not to say something nasty has never happened, I just don‚Äôt recall this vector being mentioned as an attack likely to inflict damage when hosted on modern infrastructure.",
          "score": 12,
          "created_utc": "2025-12-29 10:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj4dzj",
          "author": "RecordingForward2690",
          "text": "AWS hosts millions of public domains, but all these domains are hosted on a smaller number of DNS servers (still a large number of servers, because there's multiple servers in each of the 250+ POPs, but probably thousands instead of millions).\n\nDespite things like Shuffle Sharding (see the Builders Library for an explanation), there is a risk that a DoS/DoW attack on one domain impacts other domains. That's why AWS  protects its own infrastructure against these types of attacks by default. AWS calls this protection Shield Standard. \n\n>All AWS customers benefit from the automatic protection of Shield Standard, at no additional charge. Shield Standard defends against the most common, frequently occurring network and transport layer DDoS attacks that target your website or applications. While Shield Standard helps protect all AWS customers, you get particular benefit with Amazon Route¬†53 hosted zones, Amazon CloudFront distributions, and AWS Global Accelerator standard accelerators. These resources receive comprehensive availability protection against all known network and transport layer attacks.\n\n[https://docs.aws.amazon.com/waf/latest/developerguide/ddos-standard-summary.html](https://docs.aws.amazon.com/waf/latest/developerguide/ddos-standard-summary.html)",
          "score": 7,
          "created_utc": "2025-12-29 11:47:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjb9dw",
          "author": "Big-Minimum6368",
          "text": "In order to even get to $1000 would take 2.5 billion requests per month. Someone check my math.\n\nThis would not be a worthwhile attack vector in my mind. Our AWS bills we wouldn't notice that outside of an audit",
          "score": 5,
          "created_utc": "2025-12-29 12:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwizwth",
          "author": "Wilbo007",
          "text": "In a month thats just under 100 queries per second, for a month certainly possible",
          "score": 1,
          "created_utc": "2025-12-29 11:09:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkkant",
          "author": "eggwhiteontoast",
          "text": "Any query to your domain doesn‚Äôt always end up in Route53 or wherever your domain is hosted, DNS records are often cached by numerous DNS servers on the way for faster performance. Unless your TTL is very low, query to your domain will most likely be answered by one of the intermediary DNS servers for eg ISPs DNS.",
          "score": 1,
          "created_utc": "2025-12-29 16:47:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnmbwa",
              "author": "Extra-Moose4828",
              "text": "Correct, however that doesn't stop an attacker from directly querying AWS's authoritative server directly.",
              "score": 3,
              "created_utc": "2025-12-30 01:59:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzj2kg",
      "title": "How does RDS use NVMe instance store?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pzj2kg/how_does_rds_use_nvme_instance_store/",
      "author": "RecordingForward2690",
      "created_utc": "2025-12-30 14:10:20",
      "score": 7,
      "num_comments": 6,
      "upvote_ratio": 0.82,
      "text": "I have a transactional MSSQL DB that currently runs on a db.z1d.2xlarge RDS instance. From the metrics we know that this database is overprovisioned, and we are looking at smaller (cheaper) instances, possibly a db.r7i.xlarge.\n\n(Note that there is a discrepancy in the documentation: [This page](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.Concepts.General.InstanceClasses.html) claims that MSSQL SE supports a db.r7i.xlarge, while [this page](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.Support.html) claims it doesn't.)\n\nBased on the CW Metrics and DB Insights I can pretty much predict how the DB will behave regarding CPU, memory, network and EBS I/O when switching instance types. However, the z1d.2xlarge also has 300 GB of NVMe SSD instance store, and I have no clue whether this is used, what for, and whether this will impact performance if I switch to an instance type without instance store. It doesn't seem like there are CW Metrics available for starters, and I also can't find any documentation on it. Does anybody know of a way to understand what's going on with this storage?\n\nThe problem is also that this is a production database that runs 24/7. Due to it being Multi-AZ, switching instance types requires quite a bit of downtime that we have to schedule in advance. This severely limits the ability to experiment. I do have a test environment but I don't have a mock load generator that is representative of the workload.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pzj2kg/how_does_rds_use_nvme_instance_store/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwqjvwg",
          "author": "earl_of_angus",
          "text": "eta: I originally read MSSQL as MySQL - optimized reads doesn't apply.  Instead, with MS SQL you can check where the tmpdb is stored to see if it's on the nvme drive or ebs.\n\nFor uses of nvme, out of the box defaults will enable Optimized Reads: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-optimized-reads.html#rds-optimized-reads-use-cases (TL;DR: temp tables etc)\n\nFor CloudWatch, take a look at the *LocalStorage metrics e.g., ReadThroughputLocalStorage - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-metrics.html (ctrl-f LocalStorage)",
          "score": 3,
          "created_utc": "2025-12-30 14:45:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqyi8k",
              "author": "RecordingForward2690",
              "text": "Thanks. Your response however, throws up more questions.\n\nFirst, I don't have any \\*LocalStorage metrics in CloudWatch. At all. Not even the FreeLocalStorage or FreeLocalStoragePercent, which I would expect to be there even if instance store was not used at all. Any idea?\n\nSecond, [this documentation](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.InstanceStore.html) suggests that instance store is automatically used for tempdb usage, but only on db.m5d, db.r5d and db.x2iedn instances. db.z1d instances are not listed. Would that be an error in the documentation, or does that mean that I would actually need to put in extra work (which I didn't do) to start using the instance store? I'm not familiar with MSSQL at all. Is the disk location of the tempdb something that I can view using SQL Studio or something?",
              "score": 1,
              "created_utc": "2025-12-30 15:58:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwr28wn",
                  "author": "earl_of_angus",
                  "text": "> Is the disk location of the tempdb something that I can view using SQL Studio or something?\n\nThe tempdb is an object in SQL Server Management Studio (under system databases) that can be right-clicked & then view properties, or you can run a query like:\n\n\n    SELECT name AS file_name, physical_name AS physical_location\n    FROM sys.master_files\n    WHERE database_id = DB_ID(N'tempdb');",
                  "score": 2,
                  "created_utc": "2025-12-30 16:16:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwrncnn",
          "author": "Competitive_Two6205",
          "text": "RDS for SQL Server uses instance store to host the TempDB. You should monitor your TempDB usage to assess the impact to move it to off NVMe. For example if your databases are using Read Committed Snapshot Isolation (RCSI) your performances will likely be affected. I'm happy to provide more details",
          "score": 2,
          "created_utc": "2025-12-30 17:54:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqoky9",
          "author": "InterestedBalboa",
          "text": "Do yourself a favour and put together a plan to get off MSSQL, it‚Äôs not cloud native, expensive and has limitations the competition doesn‚Äôt have.\n\nIf you‚Äôre a ‚ÄúMicrosoft shop‚Äù then you have bigger problems to worry about and you can disregard said advice.",
          "score": 1,
          "created_utc": "2025-12-30 15:10:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwquxnc",
              "author": "RecordingForward2690",
              "text": "Wish I could. This is a 3rd party product that requires 3 MSSQL databases, and something like 69 Windows EC2 instances (with MS Service Fabric) to run properly in our Prod environment, with our workload. All authentication is done through AD. Ripping out the DB technology and replacing it with something else is not going to happen.\n\nThis is also the first Microsoft-based workload that we're hosting in AWS. All other workloads so far were either developed in-house (serverless) or Linux-based. So we're learning a lot about Microsoft technology as we go along.\n\nThe good news is that all the integrations that we built around this product are all AWS-native, with DynamoDB, SQS/SNS, EventBridge, Lambda, ECS and whatnot.",
              "score": 3,
              "created_utc": "2025-12-30 15:41:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyj2w6",
      "title": "Identify workspaces with no user connections in x days",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pyj2w6/identify_workspaces_with_no_user_connections_in_x/",
      "author": "-Xuby-",
      "created_utc": "2025-12-29 10:35:40",
      "score": 6,
      "num_comments": 12,
      "upvote_ratio": 0.81,
      "text": "Hi team,\n\nI've come from a strictly MS world and just been given access to AWS, we have around 700 always on workspaces that users connect to as their 'desktop'. \n\nI suspect we have over 100 not logged into in the last 30days.\n\nI've got access to the workspaces node and cloudwatch. The AD attribute for last login is inaccurate (suspect a service account periodically connecting). \n\nLooking for simple way to generate a list of machines where no users have connected in say 30days.\n\nIve been going in circles trying to see when UserConnected=0 for >30days. (Combining with max/min) \n\nKeep hitting 500 metric limit. \n\nFrom the workspaces node side it's the \"User last active\" field I'm interested in. \n\nFrom a windows /powershell point of view I'd just iterate & dump computer name and user last active. Surely there must be an equivalent!\n\nApologies if I'm being dim but this seems like it would be a common report for people to want so must exist somewhere! \n\nThanks! ",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pyj2w6/identify_workspaces_with_no_user_connections_in_x/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwjci48",
          "author": "circalight",
          "text": "We check \"last active\" info for workspaces through our dev portal Port. Doesn't touch the limit. There's open source options (Backstage), but you'd need a big team to make that functional.",
          "score": 6,
          "created_utc": "2025-12-29 12:50:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlcn51",
          "author": "TheNaPalmer",
          "text": "deploy this and check monthly for recommendations \n\nhttps://aws.amazon.com/solutions/implementations/cost-optimizer-for-amazon-workspaces/",
          "score": 4,
          "created_utc": "2025-12-29 18:59:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwmnrxr",
              "author": "lurkerloo29",
              "text": "This.",
              "score": 1,
              "created_utc": "2025-12-29 22:50:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkflqz",
          "author": "PelosiCapitalMgmnt",
          "text": "If you have SAML authentication for your workspaces, I would just pull your logs from your IDP and see how long ago each user signed into their workspace",
          "score": 3,
          "created_utc": "2025-12-29 16:25:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwktsbj",
          "author": "-Xuby-",
          "text": "I've brute forced it, ugly and working > slick and broken\n\n\nCloudWatch ‚ÄúUserConnected‚Äù Activity Extraction ‚Äì Four‚ÄëWeek Report\n\nLog into CloudWatch.\n\nGo to the Metrics tab.\n\nSelect All Metrics.\n\nClick Browse, \nWorkspaces\nBy Workspace ID\n(might need to click all to get out of breadcrumbs) \n\nIn search box search for ‚Äúuserconnected‚Äù.\n\nAdd that metric as a filter.\n\nAt the top of the screen, set the time range to Last 4 Weeks.\n\nChange the graph view to Data Table.\n\nOpen the Graphed Metrics tab.\n\nSet Statistic to Maximum.\n\nSet Period to 1 Day.\n\nBecause the metric is binary (0/1), a 1 indicates at least one connection that day.\n\nClick Actions ‚Üí Export to CSV.\n\nDeselect the first 100 rows, move to page 2, export the next 100.\n\nRepeat until all rows are exported.\n\nCombine the CSV files.\n\nAdd your own MAX() row at the bottom to show whether each user had any login in the four‚Äëweek window.\n\nPour beer. Sob quietly.",
          "score": 3,
          "created_utc": "2025-12-29 17:32:26",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwj2rde",
          "author": "SammichAffectionate",
          "text": "We would get this data from our RMM",
          "score": 2,
          "created_utc": "2025-12-29 11:34:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj57ru",
          "author": "-Xuby-",
          "text": "Thanks guys, small org so no Claude and no CLi rights (or sadly paid for RMM) so need something in either workspaces node or cloudwatch.\n\nP",
          "score": 1,
          "created_utc": "2025-12-29 11:54:19",
          "is_submitter": true,
          "replies": [
            {
              "id": "nwl83sw",
              "author": "JohnnyMiskatonic",
              "text": "When you say 'no CLI rights' does that include Cloudshell?",
              "score": 1,
              "created_utc": "2025-12-29 18:38:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwlrnpt",
                  "author": "-Xuby-",
                  "text": "Yes sadly!",
                  "score": 1,
                  "created_utc": "2025-12-29 20:11:16",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwx633s",
          "author": "moullas",
          "text": "you can use the aws cli or boto api to access the last connected detail from each workspace. That‚Äôs what we do, packaged in a lambda that auto-deprovisions machines once inactive for 60 days",
          "score": 1,
          "created_utc": "2025-12-31 14:47:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj0f7b",
          "author": "Hartep",
          "text": "Just use Claude for a cloud shell script. Did that, you can then either export as CSV or directly stop/terminate the ws.",
          "score": 0,
          "created_utc": "2025-12-29 11:13:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pvgj3k",
      "title": "A Little Lost: What tool to use in AWS",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pvgj3k/a_little_lost_what_tool_to_use_in_aws/",
      "author": "Idi_Amin_Haha",
      "created_utc": "2025-12-25 16:11:06",
      "score": 6,
      "num_comments": 15,
      "upvote_ratio": 0.71,
      "text": "Hi there, total noob here trying to host my first hobby project on AWS.   \nIt's a web app game with a NextJS frontend and NestJS backend and I'm looking for information on how best to host it on AWS. \n\n**Short Description**:   \n\\- It's a text based simulation game in which millions of entities enter a dungeon and events happen. Players can then influence these entities by gearing them, helping them and guiding them inside the dungeon without actually deciding or influencing events directly. E.g. an entity can be influenced to take the 'Grind' or 'Scout' action, but the outcome of that action is simulated based on factors about the environment, skills, time inside the dungeon, etc... The player has no direct influence over that result.   \n\\- Players can follow up on their favorite entities like a sort of Tamagochi.   \n\\- For some 'Legendary' events, an LLM integration (direct from the backend to Claude API's) writes a bigger story for added flavor.   \n\n\n**Technically**: There's a NextJS frontend web application in which the player can do some actions. This is connected to the NestJs Backend API that is linked to a PostgreSQL db.   \nThere's also a concurrent NestJS worker cron job that acts as the simulation. It loops over all alive entities and simulates actions on it. Every entity generates an Action Log with possible Combat Log records for every action, so there's hundreds of millions if not billions of expected records generated. \n\n**Current State:**  \nSo after struggling with Vercel and Railway (both cost and couldn't manage the worker properly) I tried hosting it on AWS directly. After reading some docs and googling a bit I started experimenting with the different tools. Currently I'm using **Amplify for the frontend** and **Elastic Beanstalk** for the backend API. The database is running on RDS and I'm using CloudFront too. The worker cron job however, is **not running on AWS yet**. \n\n**Some questions**:   \n\\- What would be the preferred tool to use for the worker? Should I host that on Elastic Beanstalk too? It does work with the same backend code as the API so that should be easy enough...  \n\\- Is my current setup correct for the type of game / web app? If not, what other tools could be recommended?   \n\\- What would be some pitfalls or common mistakes I should learn about knowing that this is my first app on AWS and I don't have a lot of experience with stuff like this?   \n\\- How could I estimate my total costs for running this app? I'm on the Free plan right now and it's estimating around 40$ monthly. This is with it running for about a month, but without other players. Just me and an additional tester. (See screenshot)\n\nhttps://preview.redd.it/ea11eepbjd9g1.png?width=376&format=png&auto=webp&s=8db1be3ff3db433c1fa746a45e328533df45d648\n\nAny other help or guidance or references to great docs or tutorials is greatly appreciated. \n\nRegards",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pvgj3k/a_little_lost_what_tool_to_use_in_aws/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nvvz0g5",
          "author": "256BitChris",
          "text": "You might want to look into lightsail for this - it's basically a pre configured ec2 but without all these complexities.   Also, generous free data transfer out.",
          "score": 14,
          "created_utc": "2025-12-25 16:26:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvzqhh",
              "author": "Idi_Amin_Haha",
              "text": "Thanks for the suggestion!   \nI assume you mean for the whole setup right? Not just the worker?   \nHaven't taken a look into Lightsail yet as it is not included in the free plan.   \nDo you think it would be cheaper in the long run too?",
              "score": 2,
              "created_utc": "2025-12-25 16:30:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvw59fw",
                  "author": "256BitChris",
                  "text": "I haven't looked at recent AWS Free plan but my understanding is they've shifted to be more like a set of credits (like 125/250) for the year.\n\nI believe with Lightsail, you don't need to manage any of the VPC elements, and I think you can provision a database to use with it as well.   That will make your life much easier and you an focus on just making the product.\n\nWhen you move to EC2, you have to use the VPC, and VPC has a bunch of hidden costs, like NAT Gateways, Public IPs, load balancer, egress, etc - not the mention the complexity of managing security groups, route tables, subnets, etc.\n\nLong run, I think you can scale pretty well with Lightsail, I believe they have load balancers too - and so honestly, it will be cheap now and then you can circle back in the future and reassess based on how your app does (ie. don't plan for scale on a new product, just plan for speed of iteration).\n\nAlso, not to throw a wrench in this, but I'd also look at CloudFlare and workers too - you can roll out next js apps really easy there with their wrangler tool, plus egress free costs and a pretty fixed low cost for start out apps.",
                  "score": 3,
                  "created_utc": "2025-12-25 17:03:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvw0hou",
          "author": "tholmes4005",
          "text": "The game sounds really interesting. To answer your questions. \n- your setup is fine for now, but I would get off of beanstalk as soon as possible. I would at least containerize your application and run it in ECS Fargate so it is easy to scale. I would front the ECS containers with a load balancer and remove the cloud front for now until you really need a global distribution. For the database make sure you r using Aurora vs standard.\n- The Cron Job can be replaced by using an Eventbridge Schedule Rule which triggers the action. The target of the Eventbridge Rule can be a Lambda Function or it can send a message to a queue, or API endpoint.\n- Amplify is fine, but I have heard it has some issue for larger applications specifically if you are using Cognito for authentication.\n- For the Database make sure you optimize your code to write to the DB write instances and read from the read instances, or better yet use the RDS Proxy. I think it takes care of a lot DB issues.\n\n- Cost: your biggest constant cost will be your database because it will be constantly up and running no matter the number of users. ECS should scale up and down as needed, but the DB will be constant.\n\nEdit: I would also spend a little time and learn CDK for Infrastructure as Code. It should be too hard if you already know JavaScript. Just makes sure all your environments are the same and easy to deploy",
          "score": 8,
          "created_utc": "2025-12-25 16:35:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvw20ke",
              "author": "Idi_Amin_Haha",
              "text": "Thanks a lot! A lot of topics to dig into! \nCould you expand a little on why I need to get off've beanstalk asap? Doesn't it scale properly like Fargate?",
              "score": 1,
              "created_utc": "2025-12-25 16:44:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvxfge8",
                  "author": "tholmes4005",
                  "text": "I should have emphasized more, but beanstalk is good for what u r doing now, but the next level of performance and control is kind of what I described. But I would not worry about it until the game starts to take off.",
                  "score": 1,
                  "created_utc": "2025-12-25 21:40:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvvyejo",
          "author": "Peterjgrainger",
          "text": "Your tool choice is great for scaling and maintenance but is a little complex for a hobby project.\n\nyou could sign up for the cloudfront free flat fee tier and aggressively cache your front end and GET routes of your api.\n\nYou could host your front end app and your backend and have an sql lite database on an EC2 instance. \n\nMore maintenance but simpler and cheaper ( could be free depending on instance size )",
          "score": 3,
          "created_utc": "2025-12-25 16:22:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvye5tm",
          "author": "aviboy2006",
          "text": "If you're new to AWS, you're actually on a decent path üëç and others suggested similar tips and here is some tips from me:\n\n\\- Worker: Don't put the cron inside Beanstalk. Run it as an ECS Fargate scheduled task triggered by EventBridge Scheduler. Same NestJS code, no servers to manage, and much easier to scale later.\n\n\\- Architecture: Amplify for Next.js + RDS for Postgres is fine. If you're adding workers, consider moving the API to ECS Fargate too, so API + worker share the same deployment model.\n\n\\- Big pitfall: Billions of action/combat logs will hurt Postgres fast. Keep game state in RDS, but stream append-only logs to S3 (Firehose works well). This alone can save you a lot.\n\n\\- Highly recommended action\\*\\* Costs: Set up AWS Budgets + Cost Anomaly Detection now (email alerts). Also watch CloudWatch Logs and NAT Gateway traffic and those surprise people the most.\n\n\\- Rule of thumb: Start simple, but design workers to scale horizontally (queue + multiple workers) so you don't have to rewrite everything later.\n\nYou're doing the right thing by thinking about this early and asking questions. Starting thinking now what worse can happen and design such way.",
          "score": 2,
          "created_utc": "2025-12-26 01:21:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw08jso",
          "author": "flyontimeapp",
          "text": "As far as I can tell you're just running a standard frontend + backend + task. If I were you I would honestly just do it all on a VPS. The complexity of using multiple services together with all the associated permission rules sounds really annoying. I would be willing to bet the complexity of doing it all on a VPS with a well-appointed Makefile would be less than or equal to the complexity associated with all the clicking and navigating the AWS dashboard. Really just boils down to whether you prefer the text interface or the AWS web console.\n\nAlso do you have an old computer sitting around your house? You could host it there too. That's how I hosted FlyOnTime for a long time. And for the scalability, if you're running it on a single computer in your house then you can run it on a single computer in the cloud. \n\nLast thing, how much egress of the worker simulations will there be? The word \"millions\" gives me pause...millions of anything will add up your egress costs p quickly.",
          "score": 2,
          "created_utc": "2025-12-26 10:43:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1bgts",
          "author": "TechDebtSommelier",
          "text": "Your setup is fine for a hobby app, but don‚Äôt run the simulation worker inside Elastic Beanstalk. The clean path is to containerize it and run it as an ECS Fargate task triggered by EventBridge Scheduler so it can run long jobs without server babysitting. Biggest pitfalls are mixing cron logic into your API and underestimating DB write volume, which will dominate cost long before compute does.",
          "score": 2,
          "created_utc": "2025-12-26 15:34:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwtuho",
          "author": "richard5mith",
          "text": "There is no way that this on Fargate is going to cheaper than the same containers on Railway. \n\nI can‚Äôt imagine any reason why I‚Äôd put this on AWS. And I‚Äôve been an AWS engineer for a couple of decades. I‚Äôd love to know why you think Railway is going to be more expensive.",
          "score": 1,
          "created_utc": "2025-12-25 19:27:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwz5g3",
              "author": "Idi_Amin_Haha",
              "text": "I must admit Vercel was the one that would cost the most.   \nThe implementation I had back then could not process the Worker Cron Job in the allowed amount of time on Vercel ( I think it's like 12 minutes or something?). Processing those millions of entities can take upwards of 20 mins...   \nI never did get the worker running on Railway in the end and from reading the docs I found that it would also be quite expensive to run the worker continuously. \n\nIf you could please provide more info on how you would host it on Railway, that would be very much appreciated too! I'd be more than happy to learn!\n\nThanks!\n\nPs: as an AWS engineer for a couple of decades, what would be your approach on a project like this?",
              "score": 1,
              "created_utc": "2025-12-25 19:59:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvx2zh6",
                  "author": "richard5mith",
                  "text": "Railway is just an easy way to host containers. So if it‚Äôs a bunch of those; then they‚Äôll just charge you for the usage. So a container can be put on a cron schedule and then shut down when done, and they‚Äôll just charge you for the time it runs. No limit on time. Then you‚Äôd have a DB container, front and back end container. Again, you‚Äôll pay based on the CPU and memory they actually use. eg. If you‚Äôre efficient; it‚Äôll cost less.  \n\nWith Fargate you‚Äôll be paying for a set size of container all the time for the back and front end ones. So if you occasionally spike to 4GB RAM, you‚Äôll need to pay for that amount all the time. But on Railway you‚Äôd only pay for that much during your spike. \n\nOn AWS the question is about how resilience, versus ease of management, versus cost. The cheapest would be just an EC2 or Litesail that you install some docker orchestration onto and run everything on there. Including your DB. Install it all on a separately mounted EBS so the instance can go down and come back up, put the instance in an autoscaling group of 1. You have resilience because the storage and instance are separated, but not much in way of AZ resilience or scaling ability. And it‚Äôd likely be cheaper with Hetzner etc. \n\nIf you don‚Äôt want to manage a DB, use RDS. But that comes with cost. And if you don‚Äôt want to manage orchestration, use ECS on EC2. And if you don‚Äôt want to manage servers at all, use Fargate. Scheduled tasks are supported by ECS, so you can do the scheduled job on there, but you will pay for the max amount of RAM and CPU you need throughout the whole job (unlike Railway). \n\nCodebuild is an alternative way to run a container on a schedule which is often simpler than ECS. But if you have the rest of the setup on that anyway then not as much of an advantage. \n\nRun things in autoscaling groups on ECS, tied to your EC2 autoscaling if you don‚Äôt use Fargate, separate storage from compute, put it in two AZe with a load balancer and RDS and you can fully enjoy a resilient, secure, scalable, easy to manage deployment. And you can pay the bill every month to match. \n\nMy personal projects go on Railway now. AWS is becoming increasingly hard to justify for hobby projects or those who aren‚Äôt already fully aware of what they‚Äôre letting themselves into.",
                  "score": 1,
                  "created_utc": "2025-12-25 20:22:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvx3vvy",
          "author": "bearposters",
          "text": "Why can‚Äôt you just use AWS Amplify and a GitHub repo",
          "score": 1,
          "created_utc": "2025-12-25 20:28:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pykiyl",
      "title": "Audio AWS Learn Resources?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pykiyl/audio_aws_learn_resources/",
      "author": "wreckuiem48",
      "created_utc": "2025-12-29 11:59:55",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.8,
      "text": "What good audio resources are there for keeping up and deepening your AWS knowledge? I know there are several AWS branded podcasts, but to be honest I dont find these hosts particularily engaging. \n\n  \nVisual tools obviously are very helpful when learning, I just have a lot of time where my eyes and hands are busy that I would like to utilize. \n\n  \nThanks!",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pykiyl/audio_aws_learn_resources/",
      "domain": "self.aws",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pzty6b",
      "title": "Is there a public AWS Health Status JSON API?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pzty6b/is_there_a_public_aws_health_status_json_api/",
      "author": "Dapper-Inspector-675",
      "created_utc": "2025-12-30 21:13:42",
      "score": 6,
      "num_comments": 20,
      "upvote_ratio": 0.8,
      "text": "Hi,\n\nSo lately I've been making all sorts of status checks via JSON API to services I rely on daily via uptime-kuma (selfhosted), which is a status monitor.\n\nSo far many popular sites had some sort of status page, which in the background scraped a json api all couple seconds, so those were pretty easy to find, some also hid in html code.\n\nBut at aws I only found this one: [https://health.aws.amazon.com/health/status](https://health.aws.amazon.com/health/status)\n\nBut I could not find any json api with some sort of summary of their uptime status, that I could use to check if AWS has an outage or not, this does not need to be detailed.\n\nI just can't believe that the big and great AWS does not have a json api for their status page?\n\nDoes anyone know if something like this exists?\n\n",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pzty6b/is_there_a_public_aws_health_status_json_api/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwt3rdf",
          "author": "AWSSupport",
          "text": "Hi there, \n\nThis feature does exist, but it's currently only available to our Premium Support subscribers: https://go.aws/44Or9gC. All AWS customers can receive Health Dashboard updates through Amazon EventBridge: https://go.aws/45lmwuE. I've forwarded your feedback about this to our internal team for review. \n\n\\- Gee J.",
          "score": 7,
          "created_utc": "2025-12-30 22:01:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt5ihm",
              "author": "Dapper-Inspector-675",
              "text": "Wooooow, I did not expect a reply from AWS officially, and this fast!\n\nThanks a lot!",
              "score": 3,
              "created_utc": "2025-12-30 22:10:01",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwt6phd",
              "author": "Dapper-Inspector-675",
              "text": "Quick question,\n\nquting the second link:\n\"All customers can use the AWS Health Dashboard, powered by the AWS Health API. The dashboard requires no setup, and it's ready to use for authenticated AWS users\"\n\nSo this means to get any data I have to be an aws customer?\n\nI'm a homelabber and don't really have anygthing on aws nor am I customer, but I am interested in their uptime status as many other services rely on AWS.",
              "score": 3,
              "created_utc": "2025-12-30 22:15:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwt80ds",
                  "author": "nemec",
                  "text": "> So this means to get any data I have to be an aws customer?\n\nCorrect, other than the [public health dashboard](https://health.aws.amazon.com/health/status). Or ask the sites you are a customer of (who use AWS) to provide their own status updates as relate to their use of the services.\n\nYou can likely rig up a lambda acting on the free EventBridge health events which stays under the free tier, but that requires you to become a customer.",
                  "score": 2,
                  "created_utc": "2025-12-30 22:22:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwijb8",
          "author": "RecordingForward2690",
          "text": "Another tip is to open up the Developer console in your browser, activate the network trace and then go to the public health dashboard. By far the majority of dashboards of AWS consists of a static page and a bunch of JS that performs API calls to AWS to retrieve the information. So the network trace shows the actual API calls that were made. It's very rare that the HTML is generated server-side.\n\nThis has sometimes helped me figure out why my code was failing when performing an API call, while it was perfectly fine in a browser.",
          "score": 2,
          "created_utc": "2025-12-31 12:15:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsw50c",
          "author": "PelosiCapitalMgmnt",
          "text": "AWS surfaces health alerts through eventbridge, which is the better way to handle things where you push events elsewhere. By for example sending them to an event bus which sends them to an SNS Topic or SQS queue",
          "score": 1,
          "created_utc": "2025-12-30 21:25:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsx2vy",
              "author": "Dapper-Inspector-675",
              "text": "okay I see, thanks for the info.\n\nWhat does that mean in practice? Is there any way to scrape them using either html keyword find (works only if it's not a dynamic webapp) or via JSONata scraping.",
              "score": 0,
              "created_utc": "2025-12-30 21:30:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwt4mrv",
                  "author": "BeasleyMusic",
                  "text": "It means they provide you with an event driven way to get notified for health events so you can trigger some automation when a health event occurs. \n\nSo say there‚Äôs a health event, you‚Äôd get an event and then could trigger a message to a slack channel or something like that \n\nIMO this is the better way than scraping website or polling an API and triggering something based on the json response. \n\nAWS even has a repo full of tools for inspiration:\n\nhttps://github.com/aws/aws-health-tools",
                  "score": 1,
                  "created_utc": "2025-12-30 22:05:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwt6l6g",
                  "author": "PelosiCapitalMgmnt",
                  "text": "If you have premium support yes, but honestly the best way is to just do it in an event driven manner so health events are pushed to you and alerted that way rather than constantly grabbing events which do happen rarely.",
                  "score": 1,
                  "created_utc": "2025-12-30 22:15:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwt3pcb",
          "author": "Padresoba",
          "text": "AWS has a Health API that can be queried \nhttps://docs.aws.amazon.com/health/latest/ug/health-api.html \n\nSame as the other comment, Eventbridge might be a better solution so you can filter down Health events to your account and regarding services and regions you actually care about. Trying to scrape all of AWS Health and determine uptime is wasted effort because of how big it is",
          "score": 1,
          "created_utc": "2025-12-30 22:01:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt5ock",
              "author": "Dapper-Inspector-675",
              "text": "Sounds reasonable, I was already thinking it will be hell of an effort to filter it :D",
              "score": 1,
              "created_utc": "2025-12-30 22:10:48",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwt64nx",
              "author": "Dapper-Inspector-675",
              "text": "Though this is the one aws support meant, which was premium only, right?",
              "score": 1,
              "created_utc": "2025-12-30 22:13:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwt6taa",
                  "author": "Padresoba",
                  "text": "Yep that's right. Can you elaborate on what problem you're trying to solve? The community here can help you better then",
                  "score": 1,
                  "created_utc": "2025-12-30 22:16:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx06wdy",
          "author": "onefivesix156",
          "text": "You may find the data on Updog (from datadog) helpful in some way. https://updog.ai/\n\nAs others have said, the health notifications via Event Bridge connected through a Lambda or Express Step Function for filtering and delivery to _somewhere_ is an excellent solution.  If this matters it is actually a decent way to learn some of the basics of AWS services and should be doable in the free tier.  That said, I understand not wanting to go that route.",
          "score": 1,
          "created_utc": "2026-01-01 00:28:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pze3rs",
      "title": "Using a presigned url in 2025 to upload file is a good enough solution to protect from malware files and allowing only images?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pze3rs/using_a_presigned_url_in_2025_to_upload_file_is_a/",
      "author": "xSypRo",
      "created_utc": "2025-12-30 09:49:02",
      "score": 6,
      "num_comments": 18,
      "upvote_ratio": 0.71,
      "text": "Hi,\n\nI was looking for the answer online and came across this - [https://www.reddit.com/r/aws/comments/zmbw4h/enforce\\_content\\_type\\_during\\_upload\\_with\\_s3\\_signed/](https://www.reddit.com/r/aws/comments/zmbw4h/enforce_content_type_during_upload_with_s3_signed/)\n\n  \nThis post from 3 years ago, and the answer was no. 3 years ago AWS S3 only allowed to enforce content type header, which is a joke for a serious attacker.\n\n3 years later, is there a solution?\n\nI am working on an app of my own that allows users to upload file, verifying the files are legit is a big overhead that I want to take off my mind. Presigned url is an easy solution or should I skip it and do it on my server?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pze3rs/using_a_presigned_url_in_2025_to_upload_file_is_a/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwpfvzj",
          "author": "The_Startup_CTO",
          "text": "AWS GuardDuty can scan files for Malware, and you can configure the bucket with a deny policy for any file that doesn't have the tag from GuardDuty that it's clean.",
          "score": 48,
          "created_utc": "2025-12-30 09:56:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwro2cu",
              "author": "thegeniunearticle",
              "text": "But - GD ain't cheap.\n\nS3 bucket scanning adds up.",
              "score": 3,
              "created_utc": "2025-12-30 17:57:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwslvgh",
                  "author": "Zenin",
                  "text": "Have you checked the pricing recently on GD for this?  It dropped 85% last Feb, making it *much* more reasonable.\n\n[https://aws.amazon.com/about-aws/whats-new/2025/02/amazon-guardduty-malware-protection-s3-price-reduction/](https://aws.amazon.com/about-aws/whats-new/2025/02/amazon-guardduty-malware-protection-s3-price-reduction/)",
                  "score": 3,
                  "created_utc": "2025-12-30 20:36:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwrqqte",
                  "author": "The_Startup_CTO",
                  "text": "There are alternatives, though I would recommend to do the math. So far, at none of the companies I've worked with, the cost for the malware scan was anywhere near something that would justify spending the working hours of engineers to find a cheaper solution.",
                  "score": 3,
                  "created_utc": "2025-12-30 18:10:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwroc07",
                  "author": "techypaul",
                  "text": "There are self made options, or one from the marketplace which is pretty cheap (Sophos based).",
                  "score": 1,
                  "created_utc": "2025-12-30 17:59:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpg2xs",
          "author": "steveoderocker",
          "text": "The best way to do it is to use something like event notifications and use lambda to actually verify the file contents. Eg https://devsecopssourav.hashnode.dev/content-type-validation-during-file-uploads-to-an-aws-s3-bucket",
          "score": 18,
          "created_utc": "2025-12-30 09:58:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwphbp7",
              "author": "RecordingForward2690",
              "text": "This was my idea as well. And you can combine it with the tip from u/The_Startup_CTO: After the Lambda has verified that the file contents matches the Content-Type header AND the Content-Type header is allowed, you add a tag that indicates the file is clean. The S3 bucket then has a resource policy with a Deny on any GetObject API call when the tag is not present. Simple, elegant, minimal code changes necessary.",
              "score": 15,
              "created_utc": "2025-12-30 10:09:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwq4w7r",
              "author": "The_Startup_CTO",
              "text": "I don't think that this would work. You can already limit the presigned url to a specific content-type, so verifying with the lambda afterwards doesn't give you anything extra. The actual danger isn't the content-type, though, that's just metadata that hints to the browser how it should try to open the file. You can still upload any combination of bytes and just give it whatever content-type will be accepted. But the file will still have the same malware in it.\n\nThat's why the check via GuardDuty is so important: It checks the actual content of the file, not just some metadata associated with it.",
              "score": 2,
              "created_utc": "2025-12-30 13:18:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwqjigu",
                  "author": "RecordingForward2690",
                  "text": "Read the blog post to the end. The example uses python-magic to actually determine the type of file from the content. This is then compared to the Content-Type header. So it will even detect the situation where a hacker uploads a file in an allowed content format, but uses a different (but also allowed) Content-Type header.",
                  "score": 1,
                  "created_utc": "2025-12-30 14:43:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpg57w",
          "author": "nemec",
          "text": "nothing's changed. AWS doesn't validate your content. You can't trust what the client uploads, all you can do is validate after the upload is finished, before the new content is available to others.\n\ne.g. virus scanning: https://docs.aws.amazon.com/guardduty/latest/ug/gdu-malware-protection-s3.html",
          "score": 13,
          "created_utc": "2025-12-30 09:58:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwppxls",
          "author": "texxelate",
          "text": "Presigned URLs are not intended to solve this problem. So,  no. You‚Äôll need to analyse the file after it has been uploaded.\n\nYou can do this easily by configuring an SQS queue which invokes a Lambda, or something which others have suggested.",
          "score": 4,
          "created_utc": "2025-12-30 11:27:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpu857",
          "author": "raja4net",
          "text": "If you don‚Äôt want to use GuardDuty, you can use [ClamAV](https://aws.amazon.com/blogs/developer/virus-scan-s3-buckets-with-a-serverless-clamav-based-cdk-construct/) to scan for malware. No built-in solution to restrict upload of images only.",
          "score": 4,
          "created_utc": "2025-12-30 12:02:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpudqw",
          "author": "martinbean",
          "text": "A pre-signed URL just lets a user upload a file to your S3 bucket for a short period of time. It does absolutely **zero** checks on the type of file being uploaded.",
          "score": 4,
          "created_utc": "2025-12-30 12:04:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpok5e",
          "author": "magnetik79",
          "text": "You have to programmatically verify the content of the upload. The presigned S3 URL system is solid, what you do with the content beyond that is your job to implement.",
          "score": 1,
          "created_utc": "2025-12-30 11:15:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq7qyi",
          "author": "Toastyproduct",
          "text": "No built in solutions. Best method is to build this into the backend. If the files are going to be limited in size and number then uploading to your backend and then placing in the bucket is still best. For images I recommend sanitizing using a reformat with some tool to a common file type and wiping out metadata. \n\nIf the files are more frequent or bigger then a ‚Äúinbox‚Äù location and then a lambda might be better than directly through backend. Just depends on your scale and resource sizing.",
          "score": 1,
          "created_utc": "2025-12-30 13:36:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsn86a",
          "author": "Zenin",
          "text": "You can't trust any data you (or a service you control) has actually seen.  There's no client-side solution to this problem; You *have* to accept the data first and then scan it however you choose.  And you have to scan *all* of it, not just a handful of bytes at the start to workout the file magic number type.\n\nGuardDuty is the obvious choice and after Feb's price cuts is very reasonably priced.  Anything else you do is going to almost certainly cost you more.  More resources, more man hours, more risks, more support issues, more outages.",
          "score": 1,
          "created_utc": "2025-12-30 20:43:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pv8asz",
      "title": "Building MCP-Powered Agents with AWS Strands",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pv8asz/building_mcppowered_agents_with_aws_strands/",
      "author": "Arindam_200",
      "created_utc": "2025-12-25 07:47:59",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.58,
      "text": "Most MCP examples stop at ‚Äúhere‚Äôs a server‚Äù and never show how it fits into real agents.\n\nIn Part 4 of my Strands series, I walk through building **MCP-powered agents** in AWS Strands, starting with a single MCP server and then scaling to agents that work with multiple MCP servers.\n\nHere‚Äôs what I cover:\n\n* What MCP is and how it fits into the Strands\n* How to build agents backed by **one MCP server**\n* How to build agents that coordinate across **multiple MCP servers**\n* When to use single-MCP vs multi-MCP agent designs\n* Real use cases for each pattern in production-style workflows\n\nIf you‚Äôve used tool-driven agents in frameworks like LangGraph, this should feel familiar, but the focus here is on how Strands makes MCP integration more modular and explicit. Here's the¬†[Full Tutorial](https://www.youtube.com/watch?v=glR4XwuqfYY).\n\nAlso, You can find all code snippets here:¬†[Github Repo](https://github.com/Arindam200/awesome-ai-apps/tree/main/course/aws_strands)\n\nWould love feedback from anyone building MCP-based or multi-agent systems in Strands.",
      "is_original_content": false,
      "link_flair_text": "technical resource",
      "permalink": "https://reddit.com/r/aws/comments/1pv8asz/building_mcppowered_agents_with_aws_strands/",
      "domain": "self.aws",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pu3nkk",
      "title": "Extracting Landing Zone Accelerator (LZA): total rebuild vs. surgical removal?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pu3nkk/extracting_landing_zone_accelerator_lza_total/",
      "author": "dereksurfs",
      "created_utc": "2025-12-23 20:05:13",
      "score": 5,
      "num_comments": 7,
      "upvote_ratio": 0.78,
      "text": "Our customer wants to move completely away from LZA in their enterprise multi-tenant system. They want to go with a Terraform replacement for IaC, account vending, etc... I'm curious to hear from those who have divested completely from LZA in an enterprise environment.   \n  \nDid you standup a net new environment to migrate to or try to surgically remove it from the existing environment? Think Strangler Pattern. While surgical removal initially sounds more cost effective, I also realize how deeply embedded LZA is across all accounts which ProServe built out via CloudFormation IaC and LZA. That is not an easy extraction. I have visions of Alien or Walking Dead zombie surgery.   \n  \nBTW, please do not chime in with why LZA is so great or why this customer should keep it. That is not the ask.\n\nThanks,\n\nDerek   \n  \n ",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pu3nkk/extracting_landing_zone_accelerator_lza_total/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nvlrkqh",
          "author": "inarush0",
          "text": "I agree in general with Rusty-Swashplate, but be mindful of the extra machinery present in the LZA CDK and CodePipelines, it won‚Äôt be a simple 1:1 resource replacement with Terraform. Just be prepared to implement extra lambdas etc on your own. Check out their repo for references https://github.com/awslabs/landing-zone-accelerator-on-aws\n\nPersonally, if I were the decision maker, I‚Äôd start net new, but in my experience management rarely has the appetite for tossing everything out and starting over. \n\nGood luck!",
          "score": 5,
          "created_utc": "2025-12-23 20:31:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmc6wx",
          "author": "toopz10",
          "text": "My question is why they want to move away from LZA? \nIs it the time it takes for changes to be applied? \n\nThat would answer how you need to slice up your Terraform State strategy and if you would actually get any benefit from the switch",
          "score": 1,
          "created_utc": "2025-12-23 22:19:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmkpn2",
              "author": "dereksurfs",
              "text": "I'm really trying to avoid getting into an LZA holy war for those diehard fans which detracts from the best ways to remove it. But if you must know, there are many reasons and I have seen multiple customers frustrated with LZA, not just this one.\n\nFirst of all, our large customer runs in multi-cloud environment where standardization of cloud agnostic tools is very important. This lends itself to better overall long-term support and maintenance where cloud teams can work across these CSPs more seamlessly. Terraform is their enterprise tool of choice.\n\nProServe designed LZA to make their jobs easier when deploying common architectures across many client environments. They have a bunch of canned IaC scripts for this purpose. So, it does that initial work well. Maintaining it however, once they leave, is an entirely different matter. And this is even with prior AWS staff who have worked with LZA for years. It's known to be very quirky,  hard to troubleshoot, maintain and manage operationally after initial deploys.\n\nLZA is error prone and fragile with second and third order affects when it breaks that can be even more difficult to fix. If it breaks other critical services, backing those changes out are not always doable since it touches so many things automagically behind the scenes. We've had  folks pulling all nighters fixing it. And these are very experienced AWS cloud engineers. We've even brought in AWS Enterprise Support LZA SMEs and they couldn't figure out how it broke at times.\n\nLZA is extremely slow when running and iterating changes through it. The logging of errors are not always helpful when looking for root cause. Documentation is lacking. The dependency on Node is a pain and sometimes not backward compatible. For all the things it automates, it causes headaches that in the end don't make it worth using from an operational perspective. The list goes on and on. In the end life was better without it.\n\nWe're already using Terraform more for IaC deployments of tenant account cloud services once stood up via LZA. Most dev teams come in with pre-existing Terraform scripts. So, that's already a step in the right direction away from LZA/CF.",
              "score": 3,
              "created_utc": "2025-12-23 23:07:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvmky2v",
          "author": "bungfarmer",
          "text": "I‚Äôve done this a few times both via new CT + Org with Account migration and in-place swap New CT + Org is definitely the way to go if you have production workloads in the current LZA Org (assuming you have root access). Migrating accounts is pretty straightforward but be sure to read docs carefully about unenrolling in the current org before leaving / accepting invite. There a couple other gotcha‚Äôs like SageMaker domains only being able to connect one identity center instance - so I‚Äôd open a support ticket to talk through that first.",
          "score": 1,
          "created_utc": "2025-12-23 23:08:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvq7vzn",
          "author": "RecordingForward2690",
          "text": "We moved away from ADF and went to Control Tower + CfCT a while ago. But we still occasionally see remains of ADF in our infrastructure.\n\nTherefore the only tip that I can give you is this: You need to understand exactly what your old solution is doing, where it leaves resources including roles & policies, RAM shares and whatnot, for cross-account management purposes. If you don't have that picture 100% complete then a complete surgical removal won't be possible, and you'll still be mopping up the pieces years from now.\n\nStarting from new, with a new root account, is unfortunately not always an alternative. I did that a while ago to my five-account personal test-environment and found that it takes about 30 minutes per account to move it over. Just because there are so many steps involved:\n\n1. Role-switch from the old root account into the to-be-moved account so you can reset the root password - this first of all requires that you have access to the inbox of that email address.\n2. Login as root, go through the wizard to accept the license terms, put in your credit card details etc.\n3. Unlink from the existing org\n4. Get an invite to link with the new org, accept the invite.\n\nAll this assumes that the account is not a delegated admin of some sort for something (like SecurityHub), that there are no RAM shares (like Transit Gateway) preventing the unlink, or shared CloudWatch/CloudTrails or other cross-account stuff blocking this process.\n\nOnly then can you let Control Tower/CfCT/Terraform do its magic and properly onboard the account into the new org.",
          "score": 1,
          "created_utc": "2025-12-24 15:16:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvlphvk",
          "author": "Rusty-Swashplate",
          "text": "I have not divested from LZA (never used it), but when re-creating something new, I have the old stuff, I learned what I liked and what was not-so-great or even bad, and re-creating it never was a huge problem.\n\nIt's much easier to re-build something you know quite well. Since you have the CF scripts and a working AWS environment, you can look up everything and re-create this via TF. Try with the easiest part, and if your process works, repeat until it's all done. \n\nOf course if your enterprise environment is thousands and thousands of rather complex AWS products, this is probably easier said than done.",
          "score": 0,
          "created_utc": "2025-12-23 20:20:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvm942s",
          "author": "TurboPigCartRacer",
          "text": "I've been through the LZA deployment process for clients and honestly wasn't thrilled with it either. The dependency on GitHub issues for bug fixes and limited customization options made it frustrating to work with.\n\nTo answer your question directly: **surgical removal is actually more straightforward than you might think.** Here's what I'd recommend:\n\n1. Follow the [official AWS uninstall instructions](https://docs.aws.amazon.com/solutions/latest/landing-zone-accelerator-on-aws/uninstall-the-solution.html) to delete the pipeline, buckets, and stacks from the management account\n2. Once removed, your org structure, SCPs, and configurations remain untouched as LZA is essentially just a wrapper around AWS Control Tower managing those resources.\n\nHowever if your customer wants to also ditch Control Tower entirely (which it sounds like they might, given the Terraform direction), then I'd lean toward the total rebuild approach. Start up a new management account, build out your Terraform-based account vending and guardrails, then migrate workload accounts by inviting them to the new org.\n\nFor what it's worth, I ended up developing [my own CDK-based solution](https://github.com/towardsthecloud/aws-cdk-landing-zone-roadmap) after dealing with LZA's limitations. Native CDK with StackSets integration gives you way more control for multi-account provisioning and is actually maintainable when you need to extend it. Might be worth considering if Terraform doesn't pan out.\n\nGood luck with the extraction!",
          "score": 0,
          "created_utc": "2025-12-23 22:03:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pwhf5e",
      "title": "Lambdas and external rate limits",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pwhf5e/lambdas_and_external_rate_limits/",
      "author": "DespoticLlama",
      "created_utc": "2025-12-26 22:48:43",
      "score": 4,
      "num_comments": 12,
      "upvote_ratio": 0.84,
      "text": "We have a burst operation (runs ad-hoc maybe once or twice a month) that pushes 10000s of messages onto a queue that we then process using a lambda function that posts data to a 3rd party. API errors were either retried or the message returned back to the queue and retried later, finally ending in the DLQ.\n\nRecently this party has introduced rate limiting and has has said we have to live with the number imposed on us - we are not big enough users of their API I suppose. When we run we burn that rate limit in 5 mins or less. So now we need to look into a way of handling the rate limit and waiting up to an hour before retrying the message as our current strategy isn't working for us. I've tinkered with concurrency numbers and visibility time-outs and had some mitigation success but frankly I don't like it and prefer something more controllable.\n\nWould step-functions be a solution to this, I've never used them before and feeling a little unsure if it is a path worth pursuing? I've tried searching but probably not using the right terms.\n\nAny guidance appreciated. Meanwhile I'll be back to monitoring the DLQ and redriving.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pwhf5e/lambdas_and_external_rate_limits/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nw3mu68",
          "author": "smutje187",
          "text": "You could handle the rate limiting case differently - instead of writing to a DLQ don‚Äôt delete the message from the queue (Lambdas support that via partial batch response) and to make sure the message isn‚Äôt immediately processed by another Lambda set its visibility timeout to 1h. Then, the message stays on the queue and will be re-tried after 1h.\n\nAlternatively, read all messages from SQS into an intermediate data store and run an hourly EventBridge schedule that consumes X messages from that data store, similar to the outbox pattern.",
          "score": 7,
          "created_utc": "2025-12-26 23:04:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4649i",
              "author": "otterley",
              "text": "If you go the visibility timeout route, be aware of the limit of 120,000 messages in flight.",
              "score": 2,
              "created_utc": "2025-12-27 01:00:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3p7mf",
              "author": "DespoticLlama",
              "text": "I am currently using ReportBatchItemFailures to try a message 3x before dumping to the DLQ, is it possible to change the visibility time-out on the messages returned back to the queue? \n\nI'll have a look into the outbox pattern.",
              "score": 1,
              "created_utc": "2025-12-26 23:18:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw3t33w",
                  "author": "smutje187",
                  "text": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html",
                  "score": 1,
                  "created_utc": "2025-12-26 23:41:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3qh3m",
          "author": "MysteriousArachnid67",
          "text": "Step Functions is a good fit for your use case. It handles wait/delay states natively and works well with stateful retry logic..exactly what you need for rate-limited retries. cost wise they are $0.025 for thousand transistions \\[[https://aws.amazon.com/step-functions/pricing/\\]](https://aws.amazon.com/step-functions/pricing/])",
          "score": 2,
          "created_utc": "2025-12-26 23:25:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3ur07",
          "author": "gioppo",
          "text": "There‚Äôs a blog that discusses a strategy just for that: https://aws.amazon.com/blogs/infrastructure-and-automation/optimize-message-delivery-to-third-party-services-using-aws-lambda-and-aws-step-functions/",
          "score": 2,
          "created_utc": "2025-12-26 23:51:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3x3nk",
              "author": "DespoticLlama",
              "text": "Thanks, I'll have a good look at this.",
              "score": 0,
              "created_utc": "2025-12-27 00:06:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw3tz95",
          "author": "agarc08",
          "text": "You could leverage a queue processing fargate service that‚Äôs configured to scale up/down based on messages visible in the queue. It would allow you to more centrally manage the rate limit vs concurrent lambdas. Depending on your handler code you may have to make some adjustments to leverage threads/parallelism.  \n\nThis would also enable you to have more control over message visibility timeouts as you need to pull the messages yourself through the sqs client.\n\nhttps://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_ecs_patterns.QueueProcessingFargateService.html",
          "score": 1,
          "created_utc": "2025-12-26 23:47:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4x5x8",
          "author": "moofox",
          "text": "Step Functions is an ideal solution for this. You can set a concurrency limit of anywhere between 1-10000 when invoking your Lambda function.",
          "score": 1,
          "created_utc": "2025-12-27 03:55:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw68i1l",
          "author": "RecordingForward2690",
          "text": "Your main problem is twofold here: The SQS/Lambda trigger tries to process messages as fast as possible, and Lambda has a 15-minute runtime limit. If you insist on a Lambda solution, you need to work around those two limitations.\n\nThere's a neat trick you can use to help you work around problem number one: There is a DelaySeconds attribute in the SQS SendMessage API call. This allows you to put messages in the queue, but they'll only become visible after that delay. Unfortunately the maximum delay you can introduce this way is 900 seconds.\n\nIf that 900 seconds is enough to prevent hitting the backend rate limit, then you only need to modify the logic that dumps the 10K messages in the queue, but no changes to your existing logic would be required elsewhere.\n\nIf you need to spread out the work even further, beyond 15 minutes, here's what I would do: Dump the 10K messages in queue #1 just like you do now. Have a scheduled Lambda run every 15 minutes, have it grab a number of messages (using explicit ReceiveMessage/DeleteMessage API calls) that you can process within 15 minutes and dump them in queue #2, with a DelaySeconds so that they are released evenly across a 15-minute period. Then have a regular SQS/Lambda trigger on the queue #2 which processes the messages like normal. This way, no changes to your existing code will be required, just add one queue and the intermediate logic.\n\nAnother approach may need a little bit of architectural change, but could, in the end, be easier overall. Have a CW Alarm that triggers if the #messages in your queue is >0. This fires up a one-time job in ECS/Fargate. The job pulls messages from the SQS queue one by one and processes these at a rate that doesn't trigger the backend rate limits. The job then shuts itself down once the queue is empty. Going the ECS/Fargate route means that you don't have to work around the two problems I mentioned earlier, which will eventually lead to a cleaner solution.\n\nLast thing: At Re:Invent this year AWS launched Lambda Durable Functions. Think of it as a sort of Step-Functions-lite, integrated in Lambda. I have not worked with this yet, but it could possibly be a solution for your problem as well.",
          "score": 1,
          "created_utc": "2025-12-27 10:50:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8bbdh",
              "author": "DespoticLlama",
              "text": "Thanks for the suggestions.\n\nThe durable functions look ideal for this problem, the docs say the wait is up to a year so 1 hour should be no problem. And you don't pay whilst idling which is even better. This feels the simplest approach to initially try with the code as it currently is.",
              "score": 1,
              "created_utc": "2025-12-27 18:33:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw9n9r2",
                  "author": "RecordingForward2690",
                  "text": "Good luck. Let us know if it worked.\n\nAnyway, whatever you do, don't go bouncing messages back to the queue when you hit the backend throttle. If you bounce messages back often enough, not only will this increase your cost, but it also eventually ensures these messages end up in the DLQ. Sure, you can run without a DLQ, or with really high redrive policies, but that also means that messages that are legitimately wrong, for instance due to a syntax error, are also being handled indefinitely.",
                  "score": 1,
                  "created_utc": "2025-12-27 22:49:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pylndu",
      "title": "Looking for some clarification on the new Amazon EKS ‚ÄúArgo CD capability‚Äù",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pylndu/looking_for_some_clarification_on_the_new_amazon/",
      "author": "PiccoloSlight5907",
      "created_utc": "2025-12-29 12:58:17",
      "score": 4,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Looking for some clarification on the new Amazon EKS ‚ÄúArgo CD capability‚Äù and how its namespace support works.\n\nIn the docs, under the comparison between the EKS Argo CD capability and self-managed Argo CD, it says something like:\n\n‚ÄúNamespace support: The capability initially supports deploying applications to a single namespace, which you specify when creating the capability. Support for applications in multiple namespaces may be added in future releases.‚Äù\n\n\n\nWhat does this mean ?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pylndu/looking_for_some_clarification_on_the_new_amazon/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwjlhjj",
          "author": "frnzle",
          "text": "It is most likely referring to where you deploy the ArgoCd application spec. Typically its in the ArgoCd namespace https://argo-cd.readthedocs.io/en/latest/operator-manual/app-any-namespace/",
          "score": 4,
          "created_utc": "2025-12-29 13:48:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjv4zw",
              "author": "PiccoloSlight5907",
              "text": "yes that is correct, but does that mean that we cannot deploy the actual resource to a desired namespace of our choice typically argocd applications are defined in argocd namespace.",
              "score": 1,
              "created_utc": "2025-12-29 14:43:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwk5wkp",
                  "author": "frnzle",
                  "text": "No the actual resources can still be in any namespace you want",
                  "score": 1,
                  "created_utc": "2025-12-29 15:38:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwkci0l",
                  "author": "SelfDestructSep2020",
                  "text": "It just means where the App custom resource has to be placed. Its so that EKS doesn't need to  change the argocd configs that allow it to read Apps/AppSet definitions from across the cluster. Sounds like they will add that later. The App still should specify a target namespace where the actual app resources will create.",
                  "score": 1,
                  "created_utc": "2025-12-29 16:10:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pwvkx9",
      "title": "Does SES need email warming?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pwvkx9/does_ses_need_email_warming/",
      "author": "Hemanthmrv",
      "created_utc": "2025-12-27 11:21:55",
      "score": 4,
      "num_comments": 8,
      "upvote_ratio": 0.67,
      "text": "I am using SES for sending campaigns to new emails. So, I wanted to know whether I need to warm my email, or will SES emails won't go to spam as AWS verifies it.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pwvkx9/does_ses_need_email_warming/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nw6d4u0",
          "author": "itzjustinn",
          "text": "Are you using shared or dedicated IPs?  SES automatically warms up dedicated IPs by splitting your traffic with the shared pools and shifting more traffic to your dedicated IPs over time.  Shared IP pools dont need any additional warming.\n\nhttps://docs.aws.amazon.com/ses/latest/dg/dedicated-ip-warming.html",
          "score": 3,
          "created_utc": "2025-12-27 11:34:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6iqhd",
              "author": "Hemanthmrv",
              "text": "Currently using shared.  \nSo, if I don't use dedicated, I don't need to worry as it uses shared IP?",
              "score": 1,
              "created_utc": "2025-12-27 12:24:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7g8er",
          "author": "omeganon",
          "text": "Since early 2024, DKIM domains hold more reputation than IP addresses and need warming if you‚Äôre sending more than 5000 per day. A DKIM domain sending high volumes of messages with no reputation looks like a spammer pump and dump run and will be rate limited and/or rejected until recipients provide enough signals about the desirability of the mail stream to begin building reputation. Those throttles would be relaxed over days or weeks as positive signals accumulate, or further blocked as negative signals accumulate.  This is regardless of the sending reputation of the shared IP pool. Spammers used to love shared pools because it gave them some amount of cover for their traffic. That isn‚Äôt the case any longer. It‚Äôs all about the DKIM domain used. \n\nIf this is the first significant email use for this sending domain, you‚Äôll need to warm it up. \n\nAlso note that spam folder placement is highly personal and your personal experience is no indication of the experience of other recipients.",
          "score": 2,
          "created_utc": "2025-12-27 15:56:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6cg3z",
          "author": "nekokattt",
          "text": "have you tried it to see if it goes to spam?",
          "score": 1,
          "created_utc": "2025-12-27 11:28:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6if57",
              "author": "Hemanthmrv",
              "text": "yeah, it didn't go",
              "score": 1,
              "created_utc": "2025-12-27 12:22:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw6sru0",
                  "author": "nekokattt",
                  "text": "you're probably fine then\n\nif there are concerns, reach out to AWS support.",
                  "score": 1,
                  "created_utc": "2025-12-27 13:39:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwfkl5y",
          "author": "TechDebtSommelier",
          "text": "You should be good without warming them.",
          "score": 1,
          "created_utc": "2025-12-28 21:27:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtj2gs",
          "author": "Normal_Toe5346",
          "text": "For most cases should be fine. I have seen new domains could end up landing into spam if you are literally blasting emails and they are getting into spams, bounced or reported.  \nI try do a warmup for like 2 weeks where i ramp up 1.5x per day (from last day) before sending out at larger capacity say (1000+ per day) on a newer domain.",
          "score": 1,
          "created_utc": "2025-12-30 23:19:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxez6t",
      "title": "Looking for pointers. Was invited to AWS Customer Solutions Manager phone screen",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pxez6t/looking_for_pointers_was_invited_to_aws_customer/",
      "author": "WhiskeySaigon",
      "created_utc": "2025-12-28 01:53:21",
      "score": 3,
      "num_comments": 8,
      "upvote_ratio": 0.6,
      "text": "I am looking for interview pointers as I test the job market. So passively searching I guess is what they call it. I applied for an AWS CSM ISV role. I was surprised I was asked to interview. The role fits nicely with what I do today, but slightly different tech stack (AI vs Regulated industry what I do today). For more context, I had a brief call with the recruiter, who basically coached me for the 60 min phone screen. \"Do that, cover this, Im going to put you through to the 60 min phonee screen\", that type of discussion.\n\nSo Im looking for any customer solutions manager specific or ISV insight. I am comfortable with my level  of understanding of the generic AWS interview process. Lots of information available about that.  I am structuring my prep around that. But I wanted to see if there is anyone with customer solutions manager specific context? The JD was fairly generic but closely aligns with what I do today. I am particularly concerned this might be more of a customer success role by a different name. Thats not really my cup of tea. But the job description makes it sound more like customer-facing TPM role, which is what I do today. Either way, Im treating this serioiusly and want to use the opportunity as interview practice. \n\nWould appreciate if anyone has insights or suggestions on this role or industry segment specifically (ISV). If it matters I am deeply experienced and currently employed in big tech and work for an ISV but on the cloud stack side as opposed to AI. But I guess you could make an argument it is AI adjacent.\n\n\nThanks in advance for any and all responses. If this question is better suited somewhere else, I would appreciate that feeback as well.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pxez6t/looking_for_pointers_was_invited_to_aws_customer/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwi5hen",
          "author": "Zephpyr",
          "text": "From what I‚Äôve seen, CSM at cloud vendors often skews TPM-ish for ISVs: aligning stakeholders, unblocking execution, and driving outcomes, with a lighter ‚Äúrenewals/health scores‚Äù vibe than classic customer success. Are you leaning toward the TPM flavor over the pure CS motion? I‚Äôd prep two STAR stories: one on cross-org alignment with an ISV partner and one on a hairy escalation where you created a clear mechanism and metrics. Keep answers \\~90 seconds. I‚Äôll pull a few prompts from the IQB interview question bank and do a timed mock with Beyz interview assistant to tighten phrasing and test how I frame tradeoffs. Sounds like solid practice either way.",
          "score": 1,
          "created_utc": "2025-12-29 06:30:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi9ip7",
              "author": "WhiskeySaigon",
              "text": "Thanks so much. I definitely lean more TPM than customer success. Im already performing in this role for an ISV in big tech so this would be a lateral move with maybe some equity upside. But in this job market, hard to say. Ive interviewed with FAANG in the past but Im a little rusty so going into this with more of a practice mindset. Your suggestions are great!",
              "score": 1,
              "created_utc": "2025-12-29 07:05:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwl0hxj",
          "author": "WhiskeySaigon",
          "text": "Thanks everyone for the suggestions and encouragement. Phone screen is next week so still prepping. \n\nKeep those ideas and suggestions coming. This community is so helpful.",
          "score": 1,
          "created_utc": "2025-12-29 18:03:37",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwplbp1",
          "author": "Zephpyr",
          "text": "Nice invite, tbh. From what I‚Äôve seen, CSM at cloud vendors often skews TPM-ish for ISVs: aligning stakeholders, unblocking execution, and driving outcomes, with a lighter ‚Äúrenewals/health scores‚Äù vibe than classic customer success. Are you leaning toward the TPM flavor over the pure CS motion? I‚Äôd prep two STAR stories: one on cross-org alignment with an ISV partner and one on a hairy escalation where you created a clear mechanism and metrics. Keep answers \\~90 seconds. I‚Äôll pull a few prompts from the IQB interview question bank and do a timed mock with Beyz interview assistant to tighten phrasing and test how I frame tradeoffs. Sounds like solid practice either way.",
          "score": 1,
          "created_utc": "2025-12-30 10:46:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwatqmh",
          "author": "TheBrianiac",
          "text": "Yeah it‚Äôs basically a customer-facing TPM, you coordinate different stakeholders and internal teams to solve customer problems or help close opportunities",
          "score": 1,
          "created_utc": "2025-12-28 02:54:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwd6qs9",
              "author": "WhiskeySaigon",
              "text": "Thats great to know! Thanks for the assist.",
              "score": 1,
              "created_utc": "2025-12-28 14:18:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwav01t",
          "author": "independant_786",
          "text": "You can DM me any specific questions you have. Happy to help.",
          "score": 0,
          "created_utc": "2025-12-28 03:02:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwfbzj8",
              "author": "WhiskeySaigon",
              "text": "Ok.. DM'd. Thx",
              "score": 1,
              "created_utc": "2025-12-28 20:45:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pydrbh",
      "title": "How long should quota increase requests take?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pydrbh/how_long_should_quota_increase_requests_take/",
      "author": "Embarrassed-Toe-7115",
      "created_utc": "2025-12-29 05:27:19",
      "score": 3,
      "num_comments": 9,
      "upvote_ratio": 0.8,
      "text": "I submitted a request to increase Running On-Demand G and VT from 0 to 4 so that I could run g6e.xlarge. I got declined (automatically I assume) 2 hours later, and it said provide more information to appeal, so I replied with my use case, and it says Customer Action Complete. This was over 4 days ago, and I haven't heard back. Should it take this long for a increase request to process?",
      "is_original_content": false,
      "link_flair_text": "general aws",
      "permalink": "https://reddit.com/r/aws/comments/1pydrbh/how_long_should_quota_increase_requests_take/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwhyq33",
          "author": "AWSSupport",
          "text": "Hi there,\n\nSorry for the delay. Our cases are handled in the order received and response times vary, depending on our volumes.\n\nSend us a private chat with your case ID and we'll check from our end that it's been routed correctly.\n\n\\- Reece W.",
          "score": 7,
          "created_utc": "2025-12-29 05:36:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi20oj",
          "author": "mattjmj",
          "text": "For g-series and similar, it's a manual review process through service teams (who likely have lots of people off at the moment with the holidays) rather than just support team, as these instance types are much more in-demand and limited. I normally advise people with well established AWS relationships to account for about 2 weeks for a first GPU quota request. If you've not got much history with AWS (or newer accounts) expect a few more rounds or involvement from sales.",
          "score": 7,
          "created_utc": "2025-12-29 06:02:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwihrz1",
          "author": "perciva",
          "text": "It shouldn't take that long, but sometimes it does.\n\nBut a bigger issue than the latency of quota increase requests is the correctness of them.  I find that about 25% of my requests get mishandled somehow -- issues ranging from the wrong quota being increased to the wrong *account* having its limit increased\\[0\\].  There's clearly a lack of tooling here; quota increase requests are going into a general ticketing system as unstructured text and people are screwing up in ways which would not be possible if AWS had proper tooling which effected requests upon the support person clicking a \"request approved\" button.\n\n\\[0\\] To be specific, my quota got increased for a GovCloud account with the same email address.  For a region which isn't part of GovCloud.",
          "score": 2,
          "created_utc": "2025-12-29 08:20:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwji918",
          "author": "182RG",
          "text": "It took over a week for G quota increase, even after I escalated the case.  Existing business customer.  They are extremely tight on these.",
          "score": 2,
          "created_utc": "2025-12-29 13:29:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjoqw6",
          "author": "benpakal",
          "text": "Really depends on your relationship. I helped one of the biggest AWS spenders in a geography and quota increases would happen over a teams call.",
          "score": 1,
          "created_utc": "2025-12-29 14:07:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwoqoix",
          "author": "krishopper",
          "text": "Over the past month, my quota requests which were not automatic have been taking at least 10 days to be addressed. It‚Äôs been very slow.",
          "score": 1,
          "created_utc": "2025-12-30 06:09:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpllek",
          "author": "TomRiha",
          "text": "Availability of GPUs is very regionally dependent. Some regions have very limited stock.\n\nUsing the sport instance placement score is a great proxy for finding what regions have higher stock. The higher the placement score the higher the stock.\n\nNote it‚Äôs only a proxy. Placement score zero doesn‚Äôt necessarily mean there are none on demand. Though it means the stock isn‚Äôt deep enough for them to get into the spot pool.",
          "score": 1,
          "created_utc": "2025-12-30 10:48:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi12pl",
          "author": "gooner4ever19",
          "text": "Rate the initial respone you got 1 star, you might get a faster reply",
          "score": 1,
          "created_utc": "2025-12-29 05:55:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi1r16",
              "author": "kei_ichi",
              "text": "lol dirty move but this is true",
              "score": 3,
              "created_utc": "2025-12-29 06:00:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1px7e41",
      "title": "Multi-tenant QuickSight migration: Reusing datasets or speeding up dashboard creation?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1px7e41/multitenant_quicksight_migration_reusing_datasets/",
      "author": "NewTrouble6245",
      "created_utc": "2025-12-27 20:14:02",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm in the middle of migrating an existing **Looker / LookML + PostgreSQL** analytics setup to **Amazon QuickSight** for a **multi-tenant SaaS application** (\\~10 tenants, each with its own database schema).\n\nIn Looker, models and dashboards are largely reusable. During the QuickSight migration, however, the most straightforward approach appears to require **creating separate datasets, analyses, and dashboards per tenant**, which makes the **initial migration and setup significantly slower**. I‚Äôm also translating LookML dimensions and SQL logic into QuickSight calculated fields.\n\nMy main questions are focused on **migration and initial creation**:\n\n* Is it possible to **reuse a dataset across tenants** in QuickSight while enforcing tenant isolation (e.g., via RLS or similar)?\n* If reuse isn‚Äôt feasible, are there **recommended patterns or tooling** to make dataset, analysis, and dashboard creation **faster during migration** (APIs, templates, CloudFormation, embedding, parameterization, etc.)?\n\nIf you‚Äôve migrated analytics for a **multi-tenant application into QuickSight**, I‚Äôd really appreciate hearing what approaches worked in practice.\n\nThanks in advance.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1px7e41/multitenant_quicksight_migration_reusing_datasets/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nw8ws4d",
          "author": "Dangerous-Sale3243",
          "text": "You can script it with cdk to reuse assets. Ive done that. It‚Äôs not easy but it‚Äôs not a nightmare either.",
          "score": 2,
          "created_utc": "2025-12-27 20:25:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8y5ei",
              "author": "NewTrouble6245",
              "text": "Thanks, that‚Äôs helpful.  \n  \nWhen you did this with CDK, did you export an existing ‚Äúgolden‚Äù dataset/analysis and parameterize it per tenant, or did you define the QuickSight assets entirely in code from scratch?",
              "score": 1,
              "created_utc": "2025-12-27 20:32:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw98r8m",
                  "author": "rafaturtle",
                  "text": "I did this by simply building the dataset directly in cdk. \n\nIt would be possible to export but you will need to create tokens and do string replace before importing. \n\nDepending on the dataset complexity it's not hard to build them via cdk. You define a sql query, a list of columns with data type and maybe some transformations like column renaming or calculations.",
                  "score": 1,
                  "created_utc": "2025-12-27 21:30:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pz7atb",
      "title": "ECS) Diff between Container Insight vs ECS Service utilization?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pz7atb/ecs_diff_between_container_insight_vs_ecs_service/",
      "author": "Think_Director_9010",
      "created_utc": "2025-12-30 03:33:11",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "https://preview.redd.it/rtgk5ye8h9ag1.png?width=383&format=png&auto=webp&s=6bfdcb42084c0f7e032035c505fffc541a069f6b\n\n\\# When you Get into ECS > cluster > service > monitoring\n\n  \nYou will see two monitoring menu\n\n1) ECS \n\n2) Container insight\n\n# I really do not have what are the diff between them, because at the same time\n\nAt 1)ECS monitoring, max CPU utilization reached 99%\n\nBut 2) Container insight, max CPU utilization reached 33%",
      "is_original_content": false,
      "link_flair_text": "containers",
      "permalink": "https://reddit.com/r/aws/comments/1pz7atb/ecs_diff_between_container_insight_vs_ecs_service/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwt9vqc",
          "author": "Advanced_Bag_5995",
          "text": "Container Insights metrics provide more information than standard Amazon ECS metrics, such as network, storage, ephemeral storage metrics, etc. The screenshot you shared has a link to learn more, so I‚Äôd recommend you go through that documentation\n\n> https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-metrics-ECS.html\n\nKeep in mind that Container Insights metrics are paid, whereas standard ECS metrics are free, but they are very basic - they are different from Container Insights metrics.",
          "score": 1,
          "created_utc": "2025-12-30 22:31:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuq7dg",
          "author": "aviboy2006",
          "text": "Lets understand this in different way and why the numbers are different:\n\nECS monitoring (Van view):- \n\nIn this view only cares about the space you asked for.\n\nIf you put 10 boxes in that \"van,\" ECS Monitoring says: \"You are at 100% capacity!\" \\* It is telling you that you are out of room in the space you officially reserved.\n\nwhere as container insights (Warehouse view):-\n\nin this view looks at the actual hardware (vCPU). It sees your 10 boxes sitting in that giant warehouse that has room for 30.\n\nso, It calculates: 10 / 30 = 33%\n\nIts show you  \"You are only using 33% of what the hardware can actually handle.\"\n\nWhich one matters most?\n\nThe 99% (ECS Monitoring) is the one you need to worry about. Even though the Warehouse (the hardware) has plenty of room, AWS will throttle (slow down) your application because you only paid for/requested the Small Van size. Your app is trying to go faster, but it is hitting the ceiling you set in your configuration.\n\nHow to fix this ?\n\nTo tackle the 99% spike, you need to go into your task definition and increase the CPU units. Once you give the van more space, that 99% number will drop.",
          "score": 1,
          "created_utc": "2025-12-31 03:25:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyqw0c",
      "title": "Tax Exemption",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pyqw0c/tax_exemption/",
      "author": "Pbj0308",
      "created_utc": "2025-12-29 16:35:23",
      "score": 3,
      "num_comments": 9,
      "upvote_ratio": 0.67,
      "text": "I had an issue with submitting a tax exemption for a client. I opened a case, I was able to fix what I needed. I submitted my form on 12/18. It says to give support 24 hours to review it. It has been well over that. I reopened the previous case to see if I can get an answer as to why it‚Äôs still unassigned. \n\nHas anybody else‚Äôs taken this long? I‚Äôve been seeing other threads saying support has gone down hill but this is truly terrible.",
      "is_original_content": false,
      "link_flair_text": "general aws",
      "permalink": "https://reddit.com/r/aws/comments/1pyqw0c/tax_exemption/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwkkdkc",
          "author": "oneplane",
          "text": "Give it a few days.",
          "score": 1,
          "created_utc": "2025-12-29 16:47:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkkoaa",
              "author": "Pbj0308",
              "text": "I think giving it 11 days was more than enough.",
              "score": 2,
              "created_utc": "2025-12-29 16:49:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwklt7v",
                  "author": "oneplane",
                  "text": "True, but I was aiming at the re-opened thing, or was that 24h after the initial request? (so about 10 days?)",
                  "score": 1,
                  "created_utc": "2025-12-29 16:54:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwkkfo0",
          "author": "AWSSupport",
          "text": "Hi there, \n\nThis sounds like a frustrating experience. Please chat message us the case ID, and we'll take a look into this for you. \n\n \\- Gee J.",
          "score": 0,
          "created_utc": "2025-12-29 16:48:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkln4w",
              "author": "Pbj0308",
              "text": "Greatly appreciated!",
              "score": 1,
              "created_utc": "2025-12-29 16:53:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwkvb7q",
                  "author": "AWSSupport",
                  "text": "Hi there, \n\nThanks! Check your inbox for further information. \n\n \\- Gee J.",
                  "score": 1,
                  "created_utc": "2025-12-29 17:39:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwl60uh",
                  "author": "AWSSupport",
                  "text": "Hi there, \n\nI am glad we could help with this!\n\n\\- Gee J.",
                  "score": 1,
                  "created_utc": "2025-12-29 18:28:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pv7i4n",
      "title": "Conversation route token usage - Amplify AI kit",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pv7i4n/conversation_route_token_usage_amplify_ai_kit/",
      "author": "Mean-Engineer-7220",
      "created_utc": "2025-12-25 06:53:43",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 0.8,
      "text": "I‚Äôm using Amplify AI kit (conversation route). How can track token usage of the conversations in it?\n\nWhen you call bedrock directly it gives token in meta data response but how to do it with conversation route?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pv7i4n/conversation_route_token_usage_amplify_ai_kit/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nvzqivp",
          "author": "latent_signalcraft",
          "text": "if amplify is wrapping the bedrock call the token metadata usually is not exposed. the practical options are bedrock invocation logging or adding a thin proxy so you can capture the raw response before amplify normalizes it. otherwise you are left estimating from input and output length.",
          "score": 2,
          "created_utc": "2025-12-26 07:36:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzs1uz",
              "author": "Mean-Engineer-7220",
              "text": "How to add thin proxy ?",
              "score": 1,
              "created_utc": "2025-12-26 07:51:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvu8bw8",
          "author": "okaysystems",
          "text": "pretty sure convo routes don‚Äôt expose token counts yet like direct bedrock calls do. ran into same thing a bit ago\n\ni ended up estimating via cloudwatch + request size, or just bypassing convo route and calling bedrock directly when i *really* needed token metrics. would be nice if amplify surfaced that tho üò¨\n\ncurious if anyone‚Äôs found a cleaner way bc docs are kinda vague here‚Ä¶",
          "score": 1,
          "created_utc": "2025-12-25 07:17:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzp5sn",
      "title": "Policy as JSON (A Rego alternative idea)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pzp5sn/policy_as_json_a_rego_alternative_idea/",
      "author": "Yersyas",
      "created_utc": "2025-12-30 18:08:14",
      "score": 3,
      "num_comments": 9,
      "upvote_ratio": 0.72,
      "text": "I have came across many posts talking about OPA Rego being to complicated and overkill for policies. So I'm thinking to build a cli or GitHub Actions tool to integrate a self-defined \\`policy.json\\` file which can scan through your .tf file whether it passes the policy.\n\nHere is one of the examples I'm thinking right now for the \\`policy.json\\`.\n\n**Block public S3 buckets**\n\n    {\n      \"id\": \"s3_no_public\",\n      \"description\": \"Block creation of public S3 buckets\",\n      \"effect\": \"deny\",\n      \"actions\": [\"aws:s3:CreateBucket\"],\n      \"resources\": [\"aws.s3.bucket\"],\n      \"conditions\": [{\n        \"field\": \"resource.acl\",\n        \"operator\": \"in\",\n        \"value\": [\"public-read\", \"public-read-write\"]\n      }]\n    }\n\nWould like to hear your feedback. Thanks!",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pzp5sn/policy_as_json_a_rego_alternative_idea/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwtyl54",
          "author": "chalbersma",
          "text": "I think the issue is that eventually you're going to end up with a Schema as complex as Rego. Also [relevant xkcd](https://xkcd.com/927/).",
          "score": 3,
          "created_utc": "2025-12-31 00:44:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwu0q3m",
              "author": "Yersyas",
              "text": "What if I switch to Kyverno-like YAML style file? Where security scanner can detect it as well.",
              "score": 1,
              "created_utc": "2025-12-31 00:56:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwu19mp",
                  "author": "chalbersma",
                  "text": "That's definitely better. But you still probably don't want to design your own schema. YAML and JSON are interchangeable. So it's not neccessarily a JSON vs. YAML thing either.",
                  "score": 1,
                  "created_utc": "2025-12-31 00:59:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsebln",
          "author": "pausethelogic",
          "text": "I‚Äôm confused. What‚Äôs the purpose of this?",
          "score": 2,
          "created_utc": "2025-12-30 20:00:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtpodu",
              "author": "Yersyas",
              "text": "To replace Rego",
              "score": 1,
              "created_utc": "2025-12-30 23:56:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwymty",
                  "author": "pausethelogic",
                  "text": "But AWS policies don‚Äôt use rego?",
                  "score": 1,
                  "created_utc": "2025-12-31 14:04:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwy65rt",
                  "author": "shisnotbash",
                  "text": "Rego is actually a language. What you describe is a schema. You still have to parse that schema somehow. Writing application logic to parse it would leave a lot to be desired, both in performance, complexity and security. This is one reason why Rego is an actual language and not just a markdown. So, in the end, you will end up with another Rego-like language, just with your own abstraction on top for inputs.",
                  "score": 1,
                  "created_utc": "2025-12-31 17:48:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nws2tdq",
          "author": "Iliketrucks2",
          "text": "Following!",
          "score": 1,
          "created_utc": "2025-12-30 19:05:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy4b0e",
          "author": "shisnotbash",
          "text": "There are tools, such as Checkov, that already do this.  Tfsec (part of Trivvy now) also supports YAML and JSON. \n\nHonestly I would write a lexer/parser in an easy language like Python (using Sly) before I would try implementing it as JSON. I love what Checkov gives me but writing complex logic as YAML is freaking horrible IMO.\n\nEdit: I‚Äôm not sure that anyone complaining about rego complexity is going to do any better with a complex schema based config. Basic use of rego is really simple IMHO, users don t have to implement complex statements, there‚Äôs a ton of documentation and tools and you can run the actual policy engine as an API. Food for thought.",
          "score": 1,
          "created_utc": "2025-12-31 17:39:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pwb3uq",
      "title": "Fastest way to get request from mobile app to amazon EC2 (via https)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pwb3uq/fastest_way_to_get_request_from_mobile_app_to/",
      "author": "Turbulent_Pool9167",
      "created_utc": "2025-12-26 18:23:34",
      "score": 2,
      "num_comments": 20,
      "upvote_ratio": 0.67,
      "text": "Hi,  \nI am using **Cloudflare to redirect the API calls to my domain to EC2**, by adding records in DNS (with proxy on), I have also turned on SSL for the domain.  \nUsing Cloudflare in the free tier with almost no traffic.\n\nIt is getting solved if I remove the proxy, but that doesn't seem right. What can I do?\n\n**The server is taking up to 1.5 seconds to send data to the frontend mobile app.**  \nIs this normal? How can I debug and fix it without compromising on security?\n\nWhat's the fastest way to get a request from the frontend to the backend?\n\n\nUpdate: Apparently in free tier, cloudfare requests were routing through far away pop. Using aws cloudfront worked",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pwb3uq/fastest_way_to_get_request_from_mobile_app_to/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nw28bvq",
          "author": "Dismal-Sort-1081",
          "text": "Kind of a very vague questions with too many things in the middle, how big is your payload, what is the delay from ec2 to ur code? Can u trace where the req is taking the longest?",
          "score": 5,
          "created_utc": "2025-12-26 18:28:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw290x5",
              "author": "Turbulent_Pool9167",
              "text": "Payload is small, some KBs max, generally in bytes.\n\nIf I directly hit Ec2. Latency gets solved.\n\nBut when I use cloudfare, it becomes slow.\nCan't trace it further, why is it slow in cloudfare.\nCan you please tell me how to do that?",
              "score": -2,
              "created_utc": "2025-12-26 18:32:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw2ak09",
                  "author": "Dismal-Sort-1081",
                  "text": "can u try this\n\n     curl -w \"\\n\\nTiming Breakdown:\\n\\\n      DNS Lookup:        %{time_namelookup}s\\n\\\n      TCP Connection:    %{time_connect}s\\n\\\n      TLS Handshake:     %{time_appconnect}s\\n\\\n      Pre-Transfer:      %{time_pretransfer}s\\n\\\n      Start Transfer:    %{time_starttransfer}s\\n\\\n      Total Time:        %{time_total}s\\n\\\n      ---\\n\\\n      Redirect Time:     %{time_redirect}s\\n\\\n      Size Downloaded:   %{size_download} bytes\\n\\\n      Speed:             %{speed_download} bytes/sec\\n\" \\\n      -o /dev/null -s \\\n      https://your-api.com",
                  "score": 4,
                  "created_utc": "2025-12-26 18:40:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw2a0oe",
                  "author": "Dismal-Sort-1081",
                  "text": "not sure honestly, if cdn is the issue try checking if cdn is working for ur partcular region(not sure about cloudflare but aws cloudnfront has like tiers targetting diff regions), also i would check for reults consistency, looking up same url may cache results locally making u thnk ec2 s faster, u should try checking both with caching off, full reload etc   \nu can also try tracing the req to the final dest, there must be some tool that can show what part takes long, for e.g. local - >cdn server -> ec2 etc etc",
                  "score": 1,
                  "created_utc": "2025-12-26 18:37:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw28czb",
          "author": "nekokattt",
          "text": "if you want full speed, use AWS native services in the closest region to the user rather than using CloudFlare.",
          "score": 2,
          "created_utc": "2025-12-26 18:28:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw294lj",
              "author": "Turbulent_Pool9167",
              "text": "But shouldn't cloudfare also work?\nUsers and ec2 both are in same region",
              "score": 2,
              "created_utc": "2025-12-26 18:32:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw29h0x",
                  "author": "nekokattt",
                  "text": "unless the cloudflare infra is hosted on AWS in the exact same region, then no, it will be slower. You've added an extra hop in and it depends where that cloudflare tunnel is being hosted.",
                  "score": 1,
                  "created_utc": "2025-12-26 18:34:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw2okd5",
          "author": "MiikaH",
          "text": "Have you tried if also cached results are slow. That would confirm whether the issue is on CF side because then there is no connection made to your EC2 instance at all.\n\nI have seen couple of times recently that I get multi second response times, then test direct connection to EC2 and it works fast. Further debugging reveals that also cached results (cache hit in headers) are also slow. And then I tried other websites that use Cloudflare (verified from response headers) and those are also loading slowly.\n\nIn both cases the issue went away after 12h on its own, so my best guess is that a local CF cache server near my ISP was overloaded and they eventually rebooted or fixed it otherwise.",
          "score": 1,
          "created_utc": "2025-12-26 19:54:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7460j",
              "author": "Turbulent_Pool9167",
              "text": "Actually I dont need caching as these are api calls.\nApparently in free tier, cloudfare requests were routing through far away pop.\nSo using aws cloudfront worked.",
              "score": 1,
              "created_utc": "2025-12-27 14:50:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pz61fy",
      "title": "Where should I start, as a future software engineer?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pz61fy/where_should_i_start_as_a_future_software_engineer/",
      "author": "Happy_Philosophy5600",
      "created_utc": "2025-12-30 02:35:07",
      "score": 2,
      "num_comments": 15,
      "upvote_ratio": 0.54,
      "text": "Hi, I am about a year out from graduating from university, and I have a software engineering internship in the upcoming summer. The company uses cloud computing, in most teams, and one of the current software engineers that I spoke with told me that it would be good to come in to the company with some familiarity with AWS.\n\nI am a little overwhelmed by how many services there are, and by the sheer number of learning resources available. Where should I start to begin building familiarity with AWS?\n\nI plan to spend around 30 minutes per day studying AWS, along with my studies, to give myself a base understanding of the tools, which I can then build on in my internship. I am fine spending money on a good course (maybe less than $100 USD). One issue I have is that I don't currently have personal projects in mind.\n\n  \nThanks! Let me know if you have any questions.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pz61fy/where_should_i_start_as_a_future_software_engineer/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwnuc4k",
          "author": "BeasleyMusic",
          "text": "IMO don‚Äôt ‚Äústudy aws‚Äù build something with AWS if you want to really learn. You won‚Äôt learn much reading books, build a web app with lambda for the backend, dynamodb for the db, maybe ec2 for front end? Idk but really the best way to learn something is to build it.\n\nLearn how to deploy everything with terraform too",
          "score": 10,
          "created_utc": "2025-12-30 02:43:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnvzto",
              "author": "davasaurus",
              "text": "Agreed. Build something and solve real problems, then study afterwards. \n\nOne caveat: make sure you know how to block public access to your resources; but don‚Äôt try to learn that upfront. Learn it as you go. \n\nHave fun!",
              "score": 1,
              "created_utc": "2025-12-30 02:52:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwny85x",
              "author": "Happy_Philosophy5600",
              "text": "I think that makes sense because I've made most of my progress in CS by building my own projects, and learning as I go. I have so little knowledge of AWS or how to use it, so I think I'm going to start with a smaller course on the basics, building small things as I go, so that I can feel comfortable with starting a project, and learning the rest as I go. Thank you!",
              "score": 1,
              "created_utc": "2025-12-30 03:04:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwq5pc0",
                  "author": "BeasleyMusic",
                  "text": "BILLING ALERTS!!! Only thing you should look up how to do first is setting up a billing alert so you don‚Äôt accidentally charge too much.",
                  "score": 1,
                  "created_utc": "2025-12-30 13:23:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo3j75",
          "author": "classicrock40",
          "text": "Start with understanding the basics- compute, networking and storage",
          "score": 2,
          "created_utc": "2025-12-30 03:35:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwntnpg",
          "author": "balu2gani",
          "text": "AWS Skill builder. Free version comes with good starter courses. Sign up for a free tier AWS account and play around. Once you get familiar with services, try to build and deploy same apps (check AWS sample apps repo for reference).",
          "score": 1,
          "created_utc": "2025-12-30 02:39:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnycgg",
              "author": "Happy_Philosophy5600",
              "text": "Thanks, this is exactly what I was looking for!",
              "score": 1,
              "created_utc": "2025-12-30 03:05:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwntrkf",
          "author": "rrrhys",
          "text": "One of these is what you want -> https://aws.amazon.com/certification/certified-cloud-practitioner/\n\nThe training guides to learn for the exam will give you a ton of practical experience in AWS UI & cli.",
          "score": 1,
          "created_utc": "2025-12-30 02:40:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnybi3",
              "author": "Happy_Philosophy5600",
              "text": "Thank you, this is great. I was looking through skill builder, and felt really overwhelmed with where to start, but there are a lot of great guides there. Thank you!",
              "score": 1,
              "created_utc": "2025-12-30 03:05:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwo5lw2",
          "author": "SonOfSofaman",
          "text": "As you point out, there is too much to learn so narrowing things down a bit might help focus your energy.\n\nYou might ask if the company uses serverless technologies within AWS, then focus your efforts on that if that's the way they lean. \"Serverless\" includes things like Lambda, API Gateway, Step Functions, DynamoDB, SQS, SNS, S3, EventBridge ... the list goes on. If they are a more traditional shop then you can focus on VPC, EC2, ECS, etc.\n\nSome places use both so this might not narrow things down, but it is worth asking.",
          "score": 1,
          "created_utc": "2025-12-30 03:47:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwovddd",
          "author": "HiCookieJack",
          "text": "\n\nmaybe get the cloud practitioner - but in the end nothing beats hands on.¬†\n\n\nGet comfortable with aws iam, since it's at the core of everything, roles, users, sessions, temporary credentials, instance credentials, assuming roles and such.¬†\n\n\nGet comfortable with security groups, they are also pretty essential\n\n\nLambda is also pretty essential, but it has so many different shades depending what you want to do.¬†\n\n\n\nAfter that check what your company is doing and try to get into that.\n\n\nHow I did it: I just started and made mistakes. Got reviews from colleagues, discussed and talked.\n\n\nAt some point I took all the essential exams¬†",
          "score": 1,
          "created_utc": "2025-12-30 06:48:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwp9hhg",
          "author": "mathilda-scott",
          "text": "With 30 minutes a day, keep it focused and practical. Start with core services you‚Äôll actually see as a software engineer: IAM (basics), EC2, S3, RDS, and Lambda. Follow a beginner AWS course that walks through small hands-on demos rather than just theory. You don‚Äôt need big personal projects yet - simple things like deploying a small API or hosting a static site are enough to build familiarity before your internship.",
          "score": 1,
          "created_utc": "2025-12-30 08:56:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpi72h",
          "author": "RecordingForward2690",
          "text": "**Disregard all the other answers for now.** Start with setting up an AWS account **AND PUT IN BILLING CONTROLS.**\n\nIt seems like every other day we have a student in here complaining that they setup an AWS account, did some experiments and two months later were confronted with a huge bill. Don't be one of those students.\n\nHere's what you need to study and understand first, before going any further:\n\n* General AWS billing principles - what is billable and what's not.\n* Where to find pricing information, how to calculate expected costs.\n* Free tier and its limitations.\n* How to obtain credits, and their limitations.\n* How to use Cost Explorer.\n* How to setup Billing Alerts.\n\nOnly then read the rest of the posts in this thread - there's good suggestions in there.",
          "score": 1,
          "created_utc": "2025-12-30 10:17:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq2frc",
          "author": "bot403",
          "text": "Study guide:¬†\n\n\nhttps://roadmap.sh/aws",
          "score": 1,
          "created_utc": "2025-12-30 13:03:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwntk8r",
          "author": "bboysoulcn",
          "text": "begin with the network",
          "score": 0,
          "created_utc": "2025-12-30 02:39:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pw4dyp",
      "title": "ECS Terraform vs Code Pipeline",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pw4dyp/ecs_terraform_vs_code_pipeline/",
      "author": "Mander95",
      "created_utc": "2025-12-26 13:32:57",
      "score": 2,
      "num_comments": 23,
      "upvote_ratio": 0.67,
      "text": "I current have terraform setup with ECS and all my ECS task definitions. I haven't found any answers online to this issue, but how do you consolidate the terraform task definition with code deployments?\n\nMy code pipeline builds the docker images, tags it with the commit hash, and then pushes it to ECR, creates a new task definition from the latest version, and only updates the container\\_definitions image property in each updated container. But then in the terraform file the image tag is static, so if I want to go back and update some cpu allocation for example, in one of the containers, I have to apply the changes with the static image. Is there a more efficient way to hold the task definition somewhere like S3 as the source of truth, and have terraform apply from it as well as have the code pipeline update it? Or what is the best way to do this?\n\nRight now I have it setup where my ecs service in terraform ignores the task definition, so if I update my TD, it creates a new revision but doesn't deploy becuase the docker image specified is not usable, then my code pipeline finds the latest revision (the one terraform made), compares it with TD currently used by the service, and creates a new revision that combines the container images (for the containers that didn't update) from the currently active TD, then the config from the LATEST TD (the terraform one), and the container images from the current deployment.\n\nBut this seems inefficient and is causing confusion. What is the best way to handle ECS in this regard? Thank you.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pw4dyp/ecs_terraform_vs_code_pipeline/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nw0yrm3",
          "author": "the_pwnererXx",
          "text": "You don't touch the definition in terraform. It's just used to initialize. You keep the task definition in one place of truth - your pipeline with your code",
          "score": 6,
          "created_utc": "2025-12-26 14:18:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw10kr7",
              "author": "burlyginger",
              "text": "The tradeoff being that you have to hard code values like DB hosts etc, or how do you handle that?",
              "score": 2,
              "created_utc": "2025-12-26 14:30:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw12t5r",
                  "author": "the_pwnererXx",
                  "text": "You put all of those in secrets",
                  "score": 3,
                  "created_utc": "2025-12-26 14:44:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw22shp",
              "author": "rarecold733",
              "text": "Yes, we do the same, however it is a little annoying our script needs to create a new task definition for the container revision and also change the ECS service to point to that new task definition number. And even with ignore_changes we end up with drift warnings in TF.",
              "score": 2,
              "created_utc": "2025-12-26 18:00:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2sho6",
                  "author": "the_pwnererXx",
                  "text": "Yes we have a custom script to do that but I think there are also actions that can do it for you. We don't get any drift though so probably something misaligned in your tf ignores",
                  "score": 2,
                  "created_utc": "2025-12-26 20:16:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw0uz3o",
          "author": "burlyginger",
          "text": "That's probably your simplest solution. \n\nIn my deployment system we export all the important bits from Terraform as SSM Parameters and use a Lambda to generate the task def contents. \n\nThen we build an artifact that contained the app spec and task def and send it to the code pipeline codedeploy action. \n\nI can get more details when at a machine if you want. This is going from memory and I built this a year ago (or more?).\n\nIt has more parts but it works well.",
          "score": 1,
          "created_utc": "2025-12-26 13:53:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1m8x6",
              "author": "Street_Smart_Phone",
              "text": "Interesting. Curious why you have to use lambda and not embed it in the Terraform?",
              "score": 1,
              "created_utc": "2025-12-26 16:32:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1z81e",
                  "author": "burlyginger",
                  "text": "We do it this way because we don't want to see Terraform changes on our PRs and we have a lot of central logic in the lambda that applies to multiple deployment types. \n\nWe are also able to enforce a number of things (sidecar containers) on every deployment instead of requiring various projects to update modules or config. \n\nSo it adds complication but gives us a lot of controls.",
                  "score": 3,
                  "created_utc": "2025-12-26 17:41:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw18w2v",
          "author": "Yoliocaust93",
          "text": "Terraform data resource, searching latest image by date, terraform task definition uses that data's output and you can update it out of Terraform without seeing any drift",
          "score": 1,
          "created_utc": "2025-12-26 15:20:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1be05",
          "author": "Unscene",
          "text": "I have CICD build and push images to ECR with two tags including \"lastest\", terraform task definition image is set to \"latest\". This way my CICD is separate from terraform, and will only use terraform to update infrastructure.",
          "score": 1,
          "created_utc": "2025-12-26 15:34:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1pfcp",
              "author": "greenstake",
              "text": "But then you lose rollback and I think some ECS things like circuit breaker don't work then.",
              "score": 2,
              "created_utc": "2025-12-26 16:49:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1s0iq",
                  "author": "Unscene",
                  "text": "Yeah true we leverage our CICD pipeline for rollbacks as well.",
                  "score": 1,
                  "created_utc": "2025-12-26 17:03:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3qsth",
          "author": "Zenin",
          "text": "It's been a while since I built this, but if I'm not forgetting something what I ended up doing was build the task in terraform with a tag of \"bootstrap\" and a \"bootstrap\" placeholder image that was basically \"hello world\" with a working /health endpoint for the AsLB health checks.\n\nECR push -> EventBridge -> EvenBridge rule -> Lambda deployment looks for the \"bootstrap\" tagged task and uses that to build the new task from the immutable image ID coming from ECR.\n\nWhen it's just an image change the rest of the task is unchanged from \"bootstrap\" so ECS does its lightweight update.  Terraform for its part is set to ignore changes for the service task definition, so it'll update (create a new) tagged bootstrap task w/o kicking off an ECS change (that would cause it to reload \"hello world\"...woops).  The next ECR push (CICD) pulls the new bootstrap definition to bring in any non-image updates (CPU, memory, etc).\n\nI'm missing something there I think, I'll need to look at that old code.  But that's the jist of it.",
          "score": 1,
          "created_utc": "2025-12-26 23:27:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7fc3w",
          "author": "mrlikrsh",
          "text": "Terraform variable with data source as ssm and update the image tag into this ssm parameter?",
          "score": 1,
          "created_utc": "2025-12-27 15:51:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8i8q0",
          "author": "no1bullshitguy",
          "text": "Your Task Definition & Service Definition file should not be part of Terraform code. It should be versioned along with application  code.",
          "score": 1,
          "created_utc": "2025-12-27 19:07:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwevpf9",
          "author": "widowhanzo",
          "text": "We prepared initial task definition with Terraform then used lifetime to ignore changes to it, then Github Actions updates the task definition during deploy with the latest image and any additional variables like version etc.\n\nIf we need to add any new env variables to the task, we update terraform, update the task definition with Terraform, then run GitHub actions again. It's not great, but it's rare enough that it's also not a major pain.",
          "score": 1,
          "created_utc": "2025-12-28 19:26:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjx4jv",
          "author": "Honest-Associate-485",
          "text": "I use a github action pipelines that updates the task definition and deploy new deployment of ecs service",
          "score": 1,
          "created_utc": "2025-12-29 14:54:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0vr12",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -3,
          "created_utc": "2025-12-26 13:58:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3khfp",
              "author": "Zenin",
              "text": ">Honestly I don't want to start a whole debate about what IaC you should use, but all of this stuff is automatically covered by using CDK instead of Terraform.\n\nYet here you are, arguing in favor of dumping the top IaC in the industry for the cheap lipstick (CDK) on the old pig that is CloudFormation.\n\nBecause it's always a great idea to put the unreliable CloudFormation service in your critical path for every single line of code change when all you actually want/need to do is update a container image.  Neat.",
              "score": -1,
              "created_utc": "2025-12-26 22:50:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1i2fr",
          "author": "HosseinKakavand",
          "text": "This is a really common ECS pain point, especially once pipelines and Terraform both start mutating runtime definitions. You are not alone in hitting confusion around versioning, drift, and long running services. A pattern that tends to hold up is separating infrastructure concerns from orchestration logic, and making process versions explicit so in flight executions are not affected. There are some concrete examples and patterns related to this here:  \n[https://dev.luthersystems.com/product](https://dev.luthersystems.com/product)  \nFor deeper, practical discussions and real world patterns around versioned workflows and orchestration at scale, there are also ongoing threads here:  \n[https://www.reddit.com/r/luthersystems/](https://www.reddit.com/r/luthersystems/)",
          "score": -4,
          "created_utc": "2025-12-26 16:10:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pw9pwd",
      "title": "Serverless Lambda Functions with 3rd party Python libraries",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pw9pwd/serverless_lambda_functions_with_3rd_party_python/",
      "author": "AsparagusKlutzy1817",
      "created_utc": "2025-12-26 17:26:18",
      "score": 2,
      "num_comments": 32,
      "upvote_ratio": 0.67,
      "text": "I am currently working quite a lot with AWS which is not my home turf to be honest. We are using heavily Lambda functions as mean to implement serverless features to avoid containers where possible.\n\nThis works so far but a pain point for me is the limit of custom lambda layers you can create. I know there is the possibility to dump additional 3rd party libraries to an EFS network drive and then let the lambda import its runtime libraries from there.\n\nWhile this seems to work technically, this looks extremely overcomplicated too me. Also hacking the system path of a lambda function to point/import libraries from an EFS looks more like a \"don't do that\" than a best practice.\n\nI am lacking quite some experience in this area. Are there really no other ways of installing 3rd party libraries. In particular in Python with the AI tooling which explodes at the moment you easily run into issues here. Needles to say that maintaining such a library list in an network drive is error prone and tedious.  \nI can avoid in many situations running containers but I would need a way to add a slowly increasing number of Python libraries to my AWS custom lambda layer stack....\n\nI would appreciate insights or some hints what else would work - the objective is to stay serverless.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pw9pwd/serverless_lambda_functions_with_3rd_party_python/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nw20ntu",
          "author": "katatondzsentri",
          "text": "Package your stuff in docker images",
          "score": 29,
          "created_utc": "2025-12-26 17:49:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2bs23",
              "author": "reichardtim",
              "text": "This comment.  You can use docker with lambda and even install massive 3rd party libs like pytorch.",
              "score": 3,
              "created_utc": "2025-12-26 18:46:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw21vga",
          "author": "nekokattt",
          "text": "Lambda Containers are a better fit here.",
          "score": 7,
          "created_utc": "2025-12-26 17:55:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw23i1d",
              "author": "AsparagusKlutzy1817",
              "text": "What exactly is a lambda container ? Installing the dependencies you need into this provided AWS layer? This is what I am doing but there is a maximum size limit which is not very high. What do you do if you hit this limit?",
              "score": 1,
              "created_utc": "2025-12-26 18:03:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw2692h",
                  "author": "Decent-Economics-693",
                  "text": "You literally take a ‚Äúcontainer image‚Äù like you‚Äôd build it to run in Docker and package it for Lambda",
                  "score": 3,
                  "created_utc": "2025-12-26 18:18:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw7cchn",
                  "author": "ManBearHybrid",
                  "text": "[https://docs.aws.amazon.com/lambda/latest/dg/images-create.html](https://docs.aws.amazon.com/lambda/latest/dg/images-create.html)",
                  "score": 1,
                  "created_utc": "2025-12-27 15:36:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw22sfy",
          "author": "Crossroads86",
          "text": "If your packages are beyond what a hand full of layers can handle, chances are its not a good lambda usecase.\n\nThere are some exceptions like sdks that are just bloated beyond good snd evil (looking at you microsoft).\n\nBut in general it should be a hint to reevalue wether a container or an ec2 might be the better option.",
          "score": 2,
          "created_utc": "2025-12-26 18:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw24mrg",
              "author": "AsparagusKlutzy1817",
              "text": "They would not have this maximum size limitation for sure. They come with additional costs as they would be running permanently. Most task fit nicely into this 15 minute window I have. I usually have more than one lambda. You can probably argue that they may become own features which may justify separation in an own account but somewhat this AWS size limit seems like a huge design flaw?",
              "score": 1,
              "created_utc": "2025-12-26 18:09:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw295r4",
          "author": "Glucosquidic",
          "text": "As others have stated, it seems like simply using a container is the best route. \n\nIn our architecture, we purely use lambdas via containers. You can work building the image into any CICD pipeline, then use terraform to push the image to ECR and associate it with the lambda. \n\nThis will also provide a more robust system for testing and reproducibility. \n\nI have not read this fully, but it seems useful:\n\n[Example](https://medium.com/akava/deploying-containerized-aws-lambda-functions-with-terraform-7147b9815599)",
          "score": 2,
          "created_utc": "2025-12-26 18:33:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw20nb6",
          "author": "Cocoa_Pug",
          "text": "You can build customer lambda runtimes too. I personally have preferred using amazonlinux runtime for the AWS cli vs pythons‚Äôs boto3 api.",
          "score": 1,
          "created_utc": "2025-12-26 17:48:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw23pe9",
              "author": "AsparagusKlutzy1817",
              "text": "What do you do once the size limit is hit and this custom lambda runtime/layer becomes too large?",
              "score": 1,
              "created_utc": "2025-12-26 18:04:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw25xvv",
                  "author": "Decent-Economics-693",
                  "text": "Container image for Lambda max size is... 10GB",
                  "score": 2,
                  "created_utc": "2025-12-26 18:16:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw24vpe",
          "author": "CyberKiller40",
          "text": "Lots of popular libraries are already pre built by the community as simple drop in layers. You can use those to ease your time with writing your own stuff.\n\nYou can find those easily enough on GitHub. They provide arn links to include.",
          "score": 1,
          "created_utc": "2025-12-26 18:10:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2733p",
              "author": "Decent-Economics-693",
              "text": "One cannot add more that 5 layers to a function. Given that you‚Äôd better use Powertools, it is actually 4. Unless the community pre-packages a layer with a bunch of stuff, you can‚Äôt treat layers and your ‚Äúcloud-based package manager‚Äù. Also, a total _unzipped_ size of the Lamba package and _all it's layers_ has a limit of 250MB, while container image size limit is 10GB",
              "score": 1,
              "created_utc": "2025-12-26 18:22:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw26hfc",
          "author": "MikkyTikky",
          "text": "How about adding the packages to the zip file, upload it to S3, and add it from there. \n\nIt has it advantages and disadvantages, but I think this approach allows for a larger package size.",
          "score": 1,
          "created_utc": "2025-12-26 18:19:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw27ght",
          "author": "Ok-Data9207",
          "text": "If cold start is not an issue just go with docker images. If cold starts can be a concern you should focus on layers or just zip, both have same code size limit.",
          "score": 1,
          "created_utc": "2025-12-26 18:24:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw28r60",
          "author": "shisnotbash",
          "text": "Use containers instead of zips.",
          "score": 1,
          "created_utc": "2025-12-26 18:31:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2l26u",
          "author": "Nearby-Middle-8991",
          "text": "ok, from the text, I'm suspecting there's more to it.\n\nFrom memory:\n\n1. Grab a machine/venv with the same python version (you can \"cross\" compile, but I don't remember the details),\n2. pip3 install -r req.txt --target ./<package>  #this will put all 3rd party in the folder\n3. zip the <package> folder and use that zip on the lambda.\n\n\\*if\\* you blow over the .zip limit, which is fairly large (only happened to me with things like oracle client), then docker image.\n\nDon't mess with lambda layers because governance for that is a pain. If you update lambda layers, their version changes, previous version disappears. Which means any lambdas you update after that can't rollback easily to that version. It's one of those features that sound better on paper than reality.",
          "score": 1,
          "created_utc": "2025-12-26 19:35:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4rd4o",
              "author": "aplarsen",
              "text": "Previous lambda layer versions should be around forever. They don't get replaced when you update the layer code.",
              "score": 1,
              "created_utc": "2025-12-27 03:16:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6qesl",
                  "author": "Nearby-Middle-8991",
                  "text": "They are kept as long as something uses it. But an \"older\" version can't be used in a new deployment. Which is how cf does rollbacks.",
                  "score": 1,
                  "created_utc": "2025-12-27 13:23:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6a2u8",
          "author": "RecordingForward2690",
          "text": "Loads of people have recommended Lambda Containers and I agree. But to expand on that answer just a bit more:\n\nA Docker container is a Docker container as far as the packaging is concerned. It doesn't matter whether the container will eventually run in EKS, ECS, Fargate or Lambda. You create a Dockerfile that pulls a base image, adds your code and dependencies, and lists an entry point. After the docker build you push the container to a repository (like ECR).\n\nBut that's where the commonality stops. When you have a Docker container that's intended to run in an EKS/ECS/Fargate environment, the container is normally expected to run 24/7, and usually needs to listen to some sort of TCP port for incoming traffic. So on the last line of your Dockerfile you start a daemon/service that is long-running, and you may need to add some graceful container-stop handling code as well (trapping various signals and such.)\n\nIn contrast, a Lambda container still runs in an event-driven environment, with 15-minute runtime limits, cold/warm start issues and everything. So the last line of your Dockerfile should not refer to a 24/7 daemon, but to the event handler code, similar to a regular Lambda. And in order to make it work in a Lambda environment, the Docker container needs some extra bits - but these are all included if you use the base container images AWS makes available for this. Your Dockerfile can really be as simple as this:\n\n    FROM public.ecr.aws/lambda/python:3.12\n    \n    # Copy requirements.txt\n    COPY requirements.txt ${LAMBDA_TASK_ROOT}\n    \n    # Install the specified packages\n    RUN pip install -r requirements.txt\n    \n    # Copy function code\n    COPY lambda_function.py ${LAMBDA_TASK_ROOT}\n    \n    # Set the CMD to your handler (could also be done as a parameter override outside of the Dockerfile)\n    CMD [ \"lambda_function.handler\" ]\n\nIn addition, you need to setup an ECR for your container. But if you've ever done Docker containers before, you can probably have your first Lambda container up and running inside of 15 minutes.\n\n[https://docs.aws.amazon.com/lambda/latest/dg/images-create.html#runtimes-images-lp](https://docs.aws.amazon.com/lambda/latest/dg/images-create.html#runtimes-images-lp)\n\nFor complicated projects with loads of dependencies, this makes things a lot easier because you're not working with a combination of zip files and layers anymore - everything you need is just packaged into that single container. I also found the docker build process to be more reliable, because it doesn't matter if I do the docker build on my Silicon Mac, in a Cloud9 environment or in a CodeBuild environment - it just works.\n\nAnd for you, another advantage could be this: The total size of your deployment package (zip + layers) using the zip+layers method is 250MB. But a Docker container image for Lambda can be up to 10 GB. Makes all the difference for complicated projects.\n\nAnd yes, the pricing model for Lambda containers is the same as for everything else in Lambda: You pay for the runtime, not for idle.",
          "score": 1,
          "created_utc": "2025-12-27 11:05:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwko5dc",
          "author": "PelosiCapitalMgmnt",
          "text": "Honestly if you need a lot of libraries, I‚Äôd consider looking at ECS fargate and using that instead of lambda. Lambda is great for quick tiny jobs that need to do a little bit of work then die. But if you have that many dependencies that you‚Äôre running into limits, it might be time to use something that‚Äôs more suited for bigger jobs",
          "score": 1,
          "created_utc": "2025-12-29 17:05:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw21foa",
          "author": "DeathByWater",
          "text": "You'd only need to mess around with layers if your total package size exceeds the limit for the lambda - in most cases, I've been available to avoid that.\n\n\nHow are you deploying them? If you want a \"this kinda just works without thinking about it\" check out the serverless framework with the (I think) serverless-python-requirements plugin",
          "score": 1,
          "created_utc": "2025-12-26 17:53:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw235zy",
              "author": "AsparagusKlutzy1817",
              "text": "I build my custom layer via Docker and then zip them. The zip is added to my Git where I define from terraform a lambda layer and attach it then to the functions which need it. This works for leaner libraries.  \nSome of the various AI tools are quite a bit too large for this to work. I start to run into this maximum size limit you mentioned. I tried to delete certain libraries from my zipped layer for instance boto3 or so which occasionally happen to be installed and delete them manually to make the zips leaner. Works but not ideal. I still come to situations where I cannot add any more libraries in some accounts. This is frustrating",
              "score": 1,
              "created_utc": "2025-12-26 18:02:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw22mcd",
          "author": "Hey-buuuddy",
          "text": "Uh did you use Lambda layers? This is standard AWS practice. If you are using GitHub, store the lib contents there and have your build script or Terraform zip them, which then will be layer resources to your lambda.",
          "score": -2,
          "created_utc": "2025-12-26 17:59:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw242vi",
              "author": "AsparagusKlutzy1817",
              "text": "Yes, this is what I am doing now. There is a fixed value of MB which the self-added layer cannot exceed. How do you work around it?",
              "score": 1,
              "created_utc": "2025-12-26 18:06:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw2ueur",
                  "author": "Hey-buuuddy",
                  "text": "Lambda container images, but first I‚Äôd reevaluate any way to can trim those libs down. You may also want to consider segmenting your lambda into several and use a Step Function.",
                  "score": -1,
                  "created_utc": "2025-12-26 20:26:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}