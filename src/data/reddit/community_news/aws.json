{
  "metadata": {
    "last_updated": "2026-01-04 05:29:58",
    "time_filter": "week",
    "subreddit": "aws",
    "total_items": 19,
    "total_comments": 130,
    "file_size_bytes": 162512
  },
  "items": [
    {
      "id": "1q05hib",
      "title": "Why do I need 5 different services just to run a function on HTTP trigger?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1q05hib/why_do_i_need_5_different_services_just_to_run_a/",
      "author": "Sadhvik1998",
      "created_utc": "2025-12-31 05:56:29",
      "score": 41,
      "num_comments": 49,
      "upvote_ratio": 0.72,
      "text": "Genuine question‚Äîam I missing something, or is this just how the cloud works?\n\nWhat I'm trying to do:\n\n\\- Simple thing - HTTP request comes in, runs some code async and pushes a message to broker.\n\nWhat am I using to do this (AWS example):\n\n1. API Gateway for the HTTP endpoint\n2. Lambda for running code\n3. EventBridge for routing the event\n4. SQS for queue and retries\n5. CloudWatch for logs\n6. I am to connect everything\n\nSame story on Azure/GCP, just different service names.\n\nTwo problems I'm facing:\n\n1. Cost is crazy: Each service bills separately. One request = 5 billing charges (API Gateway + Lambda + EventBridge + SQS + CloudWatch). When traffic grows, I'm paying more for connecting services than actual compute.\n2. Too many moving parts: 6 different dashboards to check. Retries are configured in 3 places. Debugging needs checking multiple services. Each service has its own limits.\n\n\n\nFor one simple \"run code on HTTP request,\" I'm managing half a dozen services.\n\nMy question:\n\nIs this normal? Do you just accept this complexity? Or is there a simpler way that I'm missing?\n\nI see people either deal with it or go back to old-style EC2 apps. Is there any middle path?\n\nWhat do you guys do?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1q05hib/why_do_i_need_5_different_services_just_to_run_a/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwvduf9",
          "author": "The-Wizard-of-AWS",
          "text": "Sounds like you‚Äôre making things way too complicated. API Gateway -> Lambda is all you need if you just need to trigger a function. If you need something long running or need to have it handle scale with SQS then it‚Äôs API Gateway-> SQS -> Lambda. If this wasn‚Äôt in the cloud it wouldn‚Äôt be much different if you wanted the same scale and resiliency. You‚Äôd have something like Load Balancer-> compute -> queue/stream (e.g., RabbitMQ, Kafka) -> compute. In the cloud you don‚Äôt have to manage most of those things.",
          "score": 107,
          "created_utc": "2025-12-31 06:07:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxeimj",
              "author": "aboothe726",
              "text": "You can also use a lambda function URL (without API gateway) if you just need to be able to access/trigger the function publicly and don‚Äôt care about the URL.",
              "score": 21,
              "created_utc": "2025-12-31 15:31:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxeymp",
                  "author": "SodaAnt",
                  "text": "Lambda function URLs support IAM auth.",
                  "score": 11,
                  "created_utc": "2025-12-31 15:34:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nww7sxs",
              "author": "Crossroads86",
              "text": "IAM and Cloudwatch is still needed.",
              "score": 5,
              "created_utc": "2025-12-31 10:41:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx01kpi",
                  "author": "The-Wizard-of-AWS",
                  "text": "Technically CloudWatch is optional. If you don‚Äôt grant permissions and don‚Äôt create a log group it will still work. And, like the other things, this isn‚Äôt really different than if you run on prem. You log it somewhere and have to have a way to view the logs. \n\nYou‚Äôre right about IAM, which is needed for everything.  It‚Äôs the one difference from on prem for most things (unless you‚Äôve done zero trust). That said, it often is in place of networking requirements you‚Äôd have. You may need to have firewall rules in the cloud, but in its simplest form you don‚Äôt have to think about networking at all. It‚Äôs been a while since I‚Äôve even used a security group.",
                  "score": 2,
                  "created_utc": "2025-12-31 23:56:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nww93bx",
              "author": "BredFromAbove",
              "text": "Always have a sqs in between, no? Best for retries in case of error / timeout?",
              "score": -1,
              "created_utc": "2025-12-31 10:53:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwx6cm8",
                  "author": "landon912",
                  "text": "For event handlers? Yes. For an API? No. \n\nThe caller is responsible for handling the queueing / retries of required.",
                  "score": 19,
                  "created_utc": "2025-12-31 14:48:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwx5ynp",
                  "author": "Veuxdo",
                  "text": "No? Without a very specific reason, adding another thing between APIG and Lambda would create more problems than it solves.",
                  "score": 6,
                  "created_utc": "2025-12-31 14:46:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nww99td",
                  "author": "tr666tr",
                  "text": "Depends if you need a synchronous response from the Lambda",
                  "score": 1,
                  "created_utc": "2025-12-31 10:55:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwzbvgs",
                  "author": "chalbersma",
                  "text": "Depends on the use case. If failure is acceptable then no.¬†",
                  "score": 1,
                  "created_utc": "2025-12-31 21:26:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvfpll",
          "author": "MmmmmmJava",
          "text": "I understand where you‚Äôre coming from but I‚Äôll nitpick your example. Technically you don‚Äôt need SQS, Event Bridge, or event CW logs (but you‚Äôll want logs) or possibly event API GW for a minimal serverless HTTP endpoint (if you use lambda function URLs). \n\nThe event bridge, API GW, and SQS layers are managed services acting as functional add-ons based on your specific use case. Each of these services are powerful building blocks that you can pick and choose to use. \n\nAlso, what type of routing are you doing with event bridge? Can that Event Bridge routing component be eliminated by adding a little extra code/routing logic in your Lambda?",
          "score": 23,
          "created_utc": "2025-12-31 06:22:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvgkkk",
          "author": "lulu1993cooly",
          "text": "Aws is really meant to be an ecosystem of interconnected services, not a one-click solution. \n\nYou seem to want SaaS not PaaS mate",
          "score": 46,
          "created_utc": "2025-12-31 06:29:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy9lfo",
              "author": "Mysterious_Rub_224",
              "text": "This.",
              "score": 1,
              "created_utc": "2025-12-31 18:05:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvenmo",
          "author": "Beginning-Swim-1249",
          "text": "You could just invoke the lambda directly if it‚Äôs not business critical or production. However, if you want the extra stuff‚Ä¶ well you need the extra stuff. You‚Äôre not going to get a gateway without a gateway, logs with logs etc. \n\nThere probably is a cloud formation stack that has all this ready for you so there‚Äôs not really much in the way of manual configuration you need to repeat. Or you could use some IaC that has a template for it.\n\nAlso as you scale up in complexity you won‚Äôt necessarily need to have another of everything, you‚Äôll probably share some of the service, especially the more expensive ones.\n\nI‚Äôm pretty sure AWS allows you to make custom dashboards as well so you can aggregate all your metrics in one ‚Äúplace‚Äù. It‚Äôs been a while since I‚Äôve used it",
          "score": 12,
          "created_utc": "2025-12-31 06:14:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvdypx",
          "author": "dunkah",
          "text": "Your simple thing has all of those components. Either do them yourself or use a managed service. Sometimes it is for sure cheaper and simpler.",
          "score": 10,
          "created_utc": "2025-12-31 06:08:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvdxc6",
          "author": "magnetik79",
          "text": "Steps 1& 2 can be combined, using Lambda functions URLs.",
          "score": 14,
          "created_utc": "2025-12-31 06:08:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvkbl2",
              "author": "itdoesntmatteranyway",
              "text": "As long as you‚Äôre okay without the allow/denylist and throttling API gateway provides.",
              "score": 15,
              "created_utc": "2025-12-31 07:01:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwvt159",
              "author": "behusbwj",
              "text": "Function url‚Äôs are an anti-pattern",
              "score": -9,
              "created_utc": "2025-12-31 08:21:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwveyuv",
          "author": "kobumaister",
          "text": "You don't NEED all that, you need it if you want to implement it using aws services. You could use knative with a rabbitmq and a kafka server, for example.\n\nA lot of context is missing in your post. Why do you want to implement using solely aws services? It's a requirement? Why not use a simple python service in an ec2 instance with celery?\n\nIf you have the expertise, building that is quite straightforward and generalizing it to build future services too.\n\nAbout the billing part, those services are quite cheap, if you can't monetize your service to cover that cost, you need to review what you're offering (or your infra).\n\nAnd about the monitoring, you don't need monitoring dashboards for each step, you monitor the part that you can act on, in this case, the lambda. Other services are monitored by aws. Maybe you'll build a dashboard with business metrics about how many requests are going through, but you don't monitor the uptime of event bridge. The approach that worked better for me is an up-bottom monitoring.",
          "score": 6,
          "created_utc": "2025-12-31 06:16:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwznns",
              "author": "marx2k",
              "text": ">Why not use a simple python service in an ec2 instance with celery?\n\nTell me more about celery",
              "score": 0,
              "created_utc": "2025-12-31 14:10:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxd0l4",
                  "author": "kobumaister",
                  "text": "https://letmegooglethat.com/?q=Celery+python+",
                  "score": 1,
                  "created_utc": "2025-12-31 15:24:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwezyb",
          "author": "pint",
          "text": "i don't understand eventbridge in your architecture, but for the other elements: those are not what you have to use, but what you want to use, right? you yourself added rationale for them. you can easily skip sqs, but then you risk losing messages. you don't need logs, but you want for auditing/debugging. it is not aws that is complex, but your requirements.\n\nthis is pretty typical. i often end up having 10-15 objects in my cloudformation templates even for the simplest applications.\n\nabout costs: if you look at your bill, or do any calculations, you often see a very lopsided picture. one or two services will dominate, while most of them will be single digit percents. however it is, this is the price. take it or leave it.",
          "score": 2,
          "created_utc": "2025-12-31 11:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwx8wa7",
              "author": "germoo0",
              "text": "usually the main compute and the main storage. f.e. EC2 and RDS",
              "score": 1,
              "created_utc": "2025-12-31 15:02:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwhaw4",
          "author": "RecordingForward2690",
          "text": "One more thing to add - you don't need six CW Dashboards. Just grab all the metrics from all the components you need, and add them to a custom dashboard. Everything in one view.\n\nOther than that - you forgot three essential services in your list. Make sure you throw in a WAF at your public endpoint (CloudFront, API GW or ALB) for DDoS/XSS and other protections. And you'll want to use ACM and Route53 to get a proper X.509 cert so you can do https:// instead of http://.\n\nAWS is all about building blocks, that each do one thing and do one thing well, and can be combined in a lot of different ways. And that's a good thing, IMHO. Consider the opposite. Suppose that AWS would have a ready-to-go solution that would allow you to make an https:// API call (using a custom domain name), that would invoke code. Now you're managing code in that solution. Then there would be another complete solution that would accept events from some sort of queue and run code in response, with retries upon failure. Now you need to manage code in that solution as well, with slightly different configuration parameters and capabilities. Before you know it, AWS would have to manage 100s of environments where users can run code. Better to have one all-singing-all-dancing solution for running code that can be integrated everywhere: Lambda.\n\n(In reality, AWS already has half a dozen solutions to run code in a serverless fashion. It's not just Lambda, but also Lambda@Edge, CloudFront Functions, SSM Documents and a few others. When for instance a new Python runtime is launched, we always find that support for that runtime in these solutions is added on a different schedule. So we could go to Python 3.13 in Lambda, but not yet in our SSM documents. Grrr.)",
          "score": 2,
          "created_utc": "2025-12-31 12:05:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx5vwv",
          "author": "oneplane",
          "text": "\\> Why do I need 5 different services just to run a function on HTTP trigger?\n\nBecause 'run a function on http trigger' is 5 different things. It can even be 50 different things depending on how granular you want to get.\n\nYou can pay someone to make the 5 different things seem like 1 thing, or you could us a service that doesn't have 5 things but only has 1 thing that just happens to be the thing you need.\n\nDoing this yourself on EC2 is also 5 things (if you just pick 5 random things). But EC2 itself is also more than 5 things (Instance, ENI, IP, AMI, EBS, Subnet, VPC, SG, AWS Account) so it's never really 'just the few things you wanted' if you don't specify your scope.\n\nIt can be 'one thing' if you scope it to a single Git repo, but that repo would need all the config to make that 1 thing happen (by doing 5 things in its configuration).\n\nPerhaps the issue here is how you perceive 'things' or 'complexity'; depending on your perspective merely including a standard library for a function that needs a runtime to open a socket and read and write on it is already too complex, but there isn't going to be a solution that doesn't do those things. But is that a problem? Not really, you're probably not even taking those into account because they are just there.\n\nThe same goes for the services and granularity of the components you consume. The complexity doesn't go away, it's just packaged and abstracted differently. Keep in mind that complex ‚â† hard (or difficult), it merely means that a thing is composed out of multiple other things.",
          "score": 2,
          "created_utc": "2025-12-31 14:46:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvdv7a",
          "author": "cmills2000",
          "text": "Start small using a monolith deployed to an ec2 instance or an app service (Elastic Beanstalk, ECS).  The AWS way of doing things is often overkill.  Start with as little as you need at first, and if you find in the future that you need to do it the AWS way due to scale, you cross that bridge when you get there.",
          "score": 4,
          "created_utc": "2025-12-31 06:07:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvnsde",
              "author": "HydrA-",
              "text": "Eh, not sure I agree. With IaaC and AI its never been easier to set things up correctly from the start. I wouldn‚Äôt risk the double work and then have something that isn‚Äôt scalable or can scale-to-zero, or needs patching and firewall rules/nat gateway, etc. etc. Going the VM route will not necessarily simplify things. Before you know it you‚Äôre relatively locked in to Beanstalk or what have you",
              "score": 3,
              "created_utc": "2025-12-31 07:32:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvgfp9",
          "author": "PhilipJayFry1077",
          "text": "Those services (minus cloud watch) are super cheap. Like millions of invocations and it's dirt cheap. What kind of traffic are you expecting?",
          "score": 2,
          "created_utc": "2025-12-31 06:28:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxc426",
          "author": "TechDebtSommelier",
          "text": "This is normal for cloud-native serverless architectures, but many teams avoid the complexity and cost by switching to a small EC2-based service: a single always-on application handles the HTTP request, runs the async logic, and publishes to a broker in one place, giving you fewer moving parts, simpler debugging, and more predictable costs at the expense of some managed scaling and infrastructure responsibility.",
          "score": 1,
          "created_utc": "2025-12-31 15:19:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxk03g",
          "author": "KayeYess",
          "text": "All you need is API Gateway (listener) and compute (Lambda). SQS is good for async/retries. EventBridge is another way of invoking Lambda. Not sure why you are using it if you only need to support web based triggers. Unless you are doing something special, Cloudwatch for most services is more or less out of the box. Also, you are managing nothing here. These are all \"managed\" services. You are essentially configuring them as per your requirements. If you were to build and manage similar services on your own, it would be far more complicated.and expensive¬†\n\nIf you find your existing setup expensive and complicated, you (or a qualified architect) should revisit the Architecture/Design.",
          "score": 1,
          "created_utc": "2025-12-31 15:59:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0q59u",
          "author": "_smartin",
          "text": "You could simplify architecture based on requirements. Why have an event broker AND a distributed queue? Also, if all lambda is doing is mapping and invoking another AWS service, cut it out. You can use an API Gateway AWS Integration type and use VTL to map requests and responses directly to and from the underlying AWS service you *ACTUALLY* need. Most people don‚Äôt know this because AWS and partners propagate a message of ‚Äúuse lambda‚Äù through all the examples.",
          "score": 1,
          "created_utc": "2026-01-01 02:32:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1kmth",
          "author": "poptimus_rhyme",
          "text": "Wouldn't something like n8n make this workflow easier?",
          "score": 1,
          "created_utc": "2026-01-01 06:23:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1lsmv",
          "author": "jagster247",
          "text": "I recommend SST",
          "score": 1,
          "created_utc": "2026-01-01 06:34:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx41mel",
          "author": "austerul",
          "text": "I think you may be doing something wrong.\n1. If your use case is \"run lambda on http trigger\" then all you need is lambda and some http gateway  (api gateway is expensive unless you really need all the stuff it brings for running a rest api)\n2. I don't see why you need eventbridge for the stuff you listed. Eventbridge is largely useful to have your code react to events or for scheduled jobs. It's not mandatory for http triggers or to process sqs messages. Some people use it to schedule jobs to ensure the lambda is warmed up which you may or may not care about. Have you measured your cold start penalty? Is it worth the cost?\n3. If you need retries then yes, you need sqs. You can do some limited retries inside your lambda function with the risk of prolonging the running time when the retry would not work.\n\nBut realistically, nothing you mention here is really particular to cloud except the concept of lambda.\n\nInstead of lambda with event bridge warmup you'd need a permanent service up.\n\nInstead of sqs/sns you would need some queueing system for retries (rabbitmq, redis) - another server to manage\n\nInstead of api gateway you'd need some reverse proxy (maybe depending on your platform, if using Go or Rust I wouldn't have a problem exposing directly).\n\nIf you care about logs or metrics, you'd need your own log collector/metrics backend and instrument your service to use that + ensure that your infrastructure is monitored (server, app, queue, reverse proxy)",
          "score": 1,
          "created_utc": "2026-01-01 17:57:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvgdhv",
          "author": "NoMoreVillains",
          "text": "You really only need APIGW and lambda. If you don't want logs nothing is making you use CloudWatch. I'm not sure what the message and broker you're using are but if that's not needed then cut out SQS. Also you only pay for the traffic/duration/messages sent so why does it matter?",
          "score": 1,
          "created_utc": "2025-12-31 06:28:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvri5a",
          "author": "soundman32",
          "text": "Costs are crazy?  How much traffic are you expecting?\n\nAws first year is probably free, even for the services you mention.\n\nAlternatively, host your app yourself on your own server. Traffic will be minimal, you wont need a load balancer and multiple instances.  All you need is a domain name, a static ip address, and a server.\n\nWhen you NEED to scale, then start looking at cloud.",
          "score": 1,
          "created_utc": "2025-12-31 08:07:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvu3uf",
          "author": "r2yxe",
          "text": "You can get rid of the Eventbridge layer and maybe use lambda function urls or directly invoke lambda depending on the auth scenario. \n\nBut without some additional context its hard to say.",
          "score": 1,
          "created_utc": "2025-12-31 08:31:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwi73r",
          "author": "owengo1",
          "text": "For the complexity side, you can just  ask your favorite llm to generate a terraform for your project. You will have a full working PoC in one go, IaC, and you can maintain it with an llm. It will connect all the services and you will have a small bunch of file to manage your infra.\n\nFor the cost side it's another issue: clearly your costs will grow linearly with your traffic, so it will be very cheap as long you have very low traffic, and quickly something completely unaffordable with high traffic. Once again, you can ask an LLM to modelize the costs of your architecture and estimate the threshold at which you have to find something else.\n\n\"Something else\" could be cloudflare workers, or ALB + ECS instead of Gateway + Lambda, or a cheap graviton instance, ... Clearly you will have other constraints to manage but it's very likely you can save a significant amout of money with a less \"fully-managed\" solution.",
          "score": 1,
          "created_utc": "2025-12-31 12:13:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxd8kh",
          "author": "flyontimeapp",
          "text": "Because AWS is 15% of Amazon's revenue but 60% of its profit. More complexity is better for them because it locks you in.",
          "score": -1,
          "created_utc": "2025-12-31 15:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvm97q",
          "author": "Human-Possession135",
          "text": "I use AWS lightsail containers. One instance is $7 per month and offers up to 10 containers. I have a fastapi + redis + worker node. \n\nCovers the same usecase: a quick endpoint + task queue + retry logic + message broker\n\nFew months ago we got 25k signups in < 1 hour and it held up aside from the queue filling up",
          "score": 0,
          "created_utc": "2025-12-31 07:18:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvdtrr",
          "author": "serpix",
          "text": "Use terraform or cdk to set this up at least.\nI'd like to correct that you'll be getting the largest bill from Cloudwatch alone üëç",
          "score": -3,
          "created_utc": "2025-12-31 06:07:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwe4x8",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -2,
          "created_utc": "2025-12-31 11:39:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwzj0w",
              "author": "marx2k",
              "text": "ALB to Cloudfront you say?",
              "score": 1,
              "created_utc": "2025-12-31 14:10:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2pr9a",
          "author": "Sadhvik1998",
          "text": "‚ÄòYeedu‚Äô solved my problem!! I request you to try this out",
          "score": -2,
          "created_utc": "2026-01-01 13:15:39",
          "is_submitter": true,
          "replies": [
            {
              "id": "nx9z5rc",
              "author": "LilRagnarLothbrok",
              "text": "Yeedu is scam and the support sucks, they billed me additional charges and they have a lot of downtime issues",
              "score": 1,
              "created_utc": "2026-01-02 16:32:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxmx0q",
      "title": "Memory spikes killing my workersüíÄ  need scaling advice",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pxmx0q/memory_spikes_killing_my_workers_need_scaling/",
      "author": "tiln7",
      "created_utc": "2025-12-28 09:03:41",
      "score": 34,
      "num_comments": 29,
      "upvote_ratio": 0.73,
      "text": "So I've got this Node.js SaaS that's processing way more data than I originally planned for and my infrastructure is starting to crack...\n\n**Current setup (hosted on 1 EC2):**\n\n* Main API container (duplicated, behind load balancer)\n* Separate worker container handling background tasks\n\n**The problem:** Critical tasks are not executed fast enough + memory spikes making my worker container being restarted 6-7x per day.\n\n**What the workers handle:**\n\n* API calls to external services (some slow/unpredictable)\n* Heavy data processing and parsing\n* Document generation\n* Analysis tasks that crunch through datasets\n\n\n\nSome jobs are time-critical (like onboardings) and others can take hours.\n\n\n\n**What I'm considering:**\n\n1. Managed Redis (AWS ElastiCache) \n2. Switching to SQS \n\n  \nWhat approach should I take and why? How should I scale my workers based on the workload?   \n\n\nThanks üôè",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pxmx0q/memory_spikes_killing_my_workers_need_scaling/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwclk3e",
          "author": "Dave3of5",
          "text": "Try to figure out the memory spikes / crashing first before doing anything arch / design related. There could be a problem there and then you are just shifting your problem around.",
          "score": 13,
          "created_utc": "2025-12-28 11:40:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcmchg",
              "author": "tiln7",
              "text": "will try!",
              "score": 2,
              "created_utc": "2025-12-28 11:47:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwca7im",
          "author": "MortTheLemur23",
          "text": "For a starter I would recommend separating your API from your workers by placing them on separate instances. And for more resilience dockerize both and use an AWS managed service like ECS or Elastic Beanstalk (simple setup). That way a single api/worker failure won't tear down your entire app.\n\nAnd for the workers, I would (like you said) look into SQS or Bullmq to queue your worker jobs. That enables you to better manage your jobs and see what causes your memory spikes.",
          "score": 25,
          "created_utc": "2025-12-28 09:53:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwie5zm",
              "author": "UltimateLmon",
              "text": "Honestly though for OP's particular problem, Op should really investigate the memory spikes.\n\n\nThough everything you've said should still be implemented because it's just generally good to have more resilient architecture - costs pending.\n\n\nOP might even consider moving analysis tasks to AWS Batch while at it. Can't tell about doc processing and data processing without further details.",
              "score": 5,
              "created_utc": "2025-12-29 07:46:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwc87zt",
          "author": "seanv507",
          "text": "Its unclear what your question is. \n\nAre you saying you need more compute resources (bigger instance/ aws batch serverless ,...) so handle memory spikes\n\nor a better multiprocessing (so important quick tasks are performed in a timely way) eg a separate queue\n\n...",
          "score": 9,
          "created_utc": "2025-12-28 09:33:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcno6e",
          "author": "hornetmadness79",
          "text": "You underestimated the necessary resources required to run your apps. Not uncommon really, but your reaction is. If an app is crashing 7 times a day, you can either fix the code, provision a larger node, or build better infra to handle the capacity problem. Throwing more cloudy solutions to fix a problem of your own making is what AWS is betting on.",
          "score": 7,
          "created_utc": "2025-12-28 11:59:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc6rg3",
          "author": "xzaramurd",
          "text": "For background tasks it sounds like you could use AWS Batch with Fargate or EC2 Spot instances. For your API, if sounds like you already have an ELB? Do you not have an AutoScalingGroup with it so it can spin up more instances if needed?",
          "score": 10,
          "created_utc": "2025-12-28 09:19:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc8q4g",
          "author": "mr_q_ukcs",
          "text": "What was the decision behind hosting it on a single ec2 ?\n\nThe memory spikes could be a number of application level  issues, do they happen at a set timeframe or are they random?\n\nAre your api calls asynchronous ? Do you have timeouts on them? Do you have back offs? All these things can cause a constraint on cpu resource.\n\nI would look at moving your service to ECS Fargate if it‚Äôs handling tasks that are critical, a single ec2 isn‚Äôt going to give you the resilience you need.",
          "score": 4,
          "created_utc": "2025-12-28 09:38:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcu9i5",
          "author": "FlamboyantKoala",
          "text": "Are you generating files in memory? ¬†90% of the time I‚Äôve had to fix issues with memory it‚Äôs caused by that.¬†",
          "score": 2,
          "created_utc": "2025-12-28 12:54:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwd32a8",
          "author": "cachemonet0x0cf6619",
          "text": "i don‚Äôt think using another cloud service is going to fix your underlying issue of a poorly designed application. if you dm me we can talk about the shift aspect of lift and shift but i‚Äôd need to see the details",
          "score": 1,
          "created_utc": "2025-12-28 13:54:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj070q",
          "author": "dataflow_mapper",
          "text": "This smells like a workload separation problem more than just a queue choice. Mixing time critical jobs and long running memory heavy tasks in the same worker almost guarantees spikes and restarts. I would split workers by job class first, even if they still share infra, so the heavy batch stuff cannot starve or crash onboarding jobs.\n\nSQS plus autoscaled workers is usually a good baseline because it forces you to think in smaller, bounded jobs and gives you backpressure for free. Redis can work, but it is easier to accidentally turn it into a memory footgun. Also worth looking at streaming large payloads instead of loading everything into memory at once. Most Node memory issues I see come from buffering way more than expected.",
          "score": 1,
          "created_utc": "2025-12-29 11:11:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcugwz",
          "author": "Wide_Commission_1595",
          "text": "So generally it seems like you've got a lot right! Separate API and workers is going to make scaling this an awful lot easier! \n\nFirst of consider moving over to Fargate.  If have the API auto scale based on CPU/memory to make sure it stays responsive.  I would go with the SQS option for queueing jobs, because you can auto scale your workers based on queue length, and that way you keep background job processing responsive.\n\nIt's not clear exactly how the background jobs work, i.e. is it always one type of job, or are there multiple different jobs?  Basically I would go with one queue per job type, and have dedicated workers assigned to their own queue, but if that creates extra work, a single queue/worker combo will do fine\n\nLonger term, depending on how the whole system works I might be tempted to convert the API to API gateway.  You can use it to validate requests without code and place items direct into SQS.  Other endpoints can use lambda functions.  For the reports, save them S3 and then in the API return a redirect to a pre-signed S3 url so user can download easily without exposing the bucket itself.  These are future suggestions though, for now just moving away from EC2 and into Fargate with SQS buffering should take you an awfully long way",
          "score": 1,
          "created_utc": "2025-12-28 12:55:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwd3xsu",
              "author": "nekokattt",
              "text": "Fargate is going to be more expensive and you still have to configure it to scale. While it removes the overhead of managing EC2, it does not remove the issue for OP.",
              "score": 3,
              "created_utc": "2025-12-28 14:00:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwi9xpr",
                  "author": "Wide_Commission_1595",
                  "text": "The OP is already running containers, do why use EC2 when ECS is specifically for scaling. \n\nYour right, you do have to configure scaling, I mean, that's kind of how AWS works.  You configure services to work the way you want them to?\n\nAnd yes, it's going to cost more.  Scaling up kinda does that.  Again, it's how AWS works.\n\nThe OP could attempt to fix memory leaks, which could take months, may never fix the issue, and all the while of customers off, or just scale using services designed for the task and keep customers happy and paying.....",
                  "score": 0,
                  "created_utc": "2025-12-29 07:09:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdjncf",
          "author": "AnomalyNexus",
          "text": "node.js is a poor choice for time-critical heavy data processing\n\nYou can try to patch over that by throwing more hardware at it to buy time but if the SaaS keeps growing you'll hit a wall eventually forcing a rewrite of at least the core logic in something more suitable",
          "score": 1,
          "created_utc": "2025-12-28 15:32:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwh8agd",
              "author": "ducki666",
              "text": "Lol",
              "score": 1,
              "created_utc": "2025-12-29 02:49:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwe5ckk",
          "author": "mlhpdx",
          "text": "Use Lambda. You already know Node.js and can keep your web server since it probably won‚Äôt need scaling for a bit if you put the workers elsewhere.¬†\n\nIdeally, have the web sever put messages for workers in a queue (or multiple queues if you have different priority levels) and use Lambda to work it/them. This isn‚Äôt an ideal setup, but it‚Äôs an easy step from where you are and allows workers to scale from zero to whatever you need when spikes happen.",
          "score": 0,
          "created_utc": "2025-12-28 17:21:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwennm6",
              "author": "tiln7",
              "text": "Noted! Migrating everything to lambdas would require serious refactoring and probably I would need to open DB?",
              "score": 1,
              "created_utc": "2025-12-28 18:49:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nweubk7",
                  "author": "mlhpdx",
                  "text": "Yeah, don‚Äôt migrate everything ‚Äî just the worker code that runs in the background. The Lambda can connect to the same VPC that contains the DB, so it doesn‚Äôt need to be ‚Äúopen‚Äù in that sense.",
                  "score": 0,
                  "created_utc": "2025-12-28 19:19:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwfkeyq",
          "author": "TechDebtSommelier",
          "text": "Put the heavy work behind SQS and let your API just enqueue jobs, because Redis queues fall over easily when memory spikes or workers crash. Split workers by job type so fast, time sensitive tasks are not stuck behind long running jobs, and autoscale workers based on SQS queue depth. Keep EC2 or ECS for predictable heavy jobs and consider Lambda only for short bursty tasks. This setup absorbs spikes cleanly and stops one bad job from taking everything down.",
          "score": 0,
          "created_utc": "2025-12-28 21:27:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwibgyw",
          "author": "could_be_any_person",
          "text": "I also have an app hosted on AWS with an API container and long running tasks. You need to split your tasks between different workers. Have a dedicated worker container for long running tasks and one for short tasks. Create an auto scaling group and policy for each worker. I have mine set to scale based on the size of my task queues, but you can also set it to scale based on memory, cpu, etc. For example, you can create an auto scaling policy for your short tasks to create more worker containers whenever tasks are backed up. That way, there's always a worker container available to process short tasks in case there's a spike in user activity (or migrate short tasks to a lambda function).  You can also set up predictive scaling policies so that your cluster scales automatically ahead of predicted demand patterns. \n\nEither use redis + a lambda function + the EventBridge scheduler, AWS SQS, or a managed solution like Temporal to handle delegation of queued tasks to your worker containers.\n\nFigure out the memory problem first. Your program shouldn't be crashing regardless of spike in user activity. It should be queuing any jobs your container can't handle. I recommend SQS for that. Also make sure you're not saving large files to memory. Use a temp folder to save any large files that you need to return.",
          "score": 0,
          "created_utc": "2025-12-29 07:22:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc7vjc",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -17,
          "created_utc": "2025-12-28 09:30:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwd0ns8",
              "author": "Dangerous-Sale3243",
              "text": "OP is unclear, but it doesn‚Äôt sound like there‚Äôs a memory leak, there‚Äôs just either ‚Äúbad‚Äù programming (holding entire documents in memory perhaps too long) or just lack of scaling in the design.\n\nOP could spend a lot of time rewriting to Rust but it wouldn‚Äôt solve the underlying mathematical problem that requests/sec * avg request time * avg document size is close to the memory limit.\n\nI do think rewriting to Rust could be helpful though, but it‚Äôs further down the list of priorities. To me, #1 is stopping the bleeding, give everything more RAM to stop dropping requests. #2 is to switch to passing documents as s3 URIs and then migrate the background workers to Lambda.",
              "score": 2,
              "created_utc": "2025-12-28 13:39:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nweqd7r",
              "author": "bot403",
              "text": "As a software engineer I'd rather spin up a second instance in minutes to hours than a week or weeks doing a risky language rewrite of my core application.",
              "score": 1,
              "created_utc": "2025-12-28 19:01:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q2mf1g",
      "title": "Scaling 'Mark All as Read' in DynamoDB: Avoiding the 1MB limit without 100k background writes.",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1q2mf1g/scaling_mark_all_as_read_in_dynamodb_avoiding_the/",
      "author": "guts1866",
      "created_utc": "2026-01-03 05:39:26",
      "score": 31,
      "num_comments": 14,
      "upvote_ratio": 0.92,
      "text": "Building a notification system (Missed calls, alerts, etc.) and I've run into the classic DynamoDB 1MB response limit.\n\nBasically, my users can have thousands of notifications. I need to be able to \"Mark All as Read\" instantly.\n\n Currently, my \"unread\" query is returning way too much data because I can't effectively update every single row in the DB without the cost being insane.\n\nI tried using a timestamp in Redis to filter the results in my backend, but I‚Äôm still paying for the \"Read\" units in Dynamo for items that are technically already read. It feels like I'm fighting the database.\n\nIf you‚Äôve built a notification feed,  how did you handle the \"Mark All\" feature? Did you use a \"watermark\" timestamp, or did you find a clever way to batch update?\n\nAppreciate any tips or war stories!",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1q2mf1g/scaling_mark_all_as_read_in_dynamodb_avoiding_the/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nxg1rbw",
          "author": "Mishoniko",
          "text": ">I tried using a timestamp in Redis to filter the results in my backend, but I‚Äôm still paying for the \"Read\" units in Dynamo for items that are technically already read. It feels like I'm fighting the database.\n\nYou are because your schema design is faulty. Putting a read marker in the message when you can expect users to generate millions of notifications was never going to scale.\n\nIndex notifications by Sargable value (timestamp, etc.). Put a watermark in the user record. One row update to mark read all. \n\nIf that's not doable then build a SQS/Lambda pipeline to run the marking async and batch queries by the size limit.",
          "score": 53,
          "created_utc": "2026-01-03 14:46:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhinz6",
              "author": "IntermediateSwimmer",
              "text": "I was a solutions architect for 6 years at AWS OP, u/Mishoniko is exactly correct here",
              "score": 7,
              "created_utc": "2026-01-03 18:56:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxg76xp",
          "author": "Limp-Maximum9662",
          "text": "Add a version field. When all marked as read inc by 1 in one global user config. When reading the notifications, read only latest version",
          "score": 16,
          "created_utc": "2026-01-03 15:15:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxg6q4u",
          "author": "darvink",
          "text": "Move the read/unread marker to the user record rather than having a property of read/unread in the notification record.",
          "score": 11,
          "created_utc": "2026-01-03 15:12:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgjtrd",
              "author": "Robodude",
              "text": "What do you track in the user record? NotificationId and timestamp?",
              "score": 3,
              "created_utc": "2026-01-03 16:16:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgrsif",
                  "author": "darvink",
                  "text": "Depending on its access pattern, but if it is just a simple read/unread marker, you can maybe add a set of unread notification ids.\n\nEmpty this set to indicate that everything is in read status.\n\nEdit: note that you need to know exactly what your access pattern is. If you do it this way and you have a lot of notifications you might hit the max entry size limit.\n\nWe always make decisions based on trade off, and doing it this way is probably one of the simplest way, if your access pattern allows it.",
                  "score": 0,
                  "created_utc": "2026-01-03 16:54:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxh6kdh",
          "author": "alex_bilbie",
          "text": "Store a lastReadTimestamp property on the user record.\n\nWhen the user marks all as read, update that property, then you can easily determine if notifications added after that time are new.",
          "score": 7,
          "created_utc": "2026-01-03 18:02:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxg6v9q",
          "author": "ReturnOfNogginboink",
          "text": "Create a secondary index with just the users ID and the 'hasbeenread' flag. Use the secondary index to query and mark items read.",
          "score": 5,
          "created_utc": "2026-01-03 15:13:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhewwu",
          "author": "bittrance",
          "text": "As others have said. you put the read markers in the user record. However, that is only half the answer, since simply putting a list of ids there will not scale forever. What you want to do is encode the read marks as a list of ranges. For simplicity, if we assume the ids are ints, you will have `[(1,5),(7,15),(17,17)]` meaning messages 1-5,7-15 and 17 have been marked as read. (If we assume that all messages will eventually be marked read, it can be easier to view the list as unread message ranges instead.) In practice, such a list is unlikely to ever grow large. Typically, you cache these ranges in memory and use them to inform queries as needed.\n\nI have not done this in DynamoDB, but I imagine that if you use timebased UUIDv6/7 ids, it should work just fine.",
          "score": 1,
          "created_utc": "2026-01-03 18:40:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxi3j2w",
          "author": "RecordingForward2690",
          "text": "I've had a somewhat similar problem, where I was doing a multi-user (shared) whiteboard. All draw operations went into one DDB table (with the session ID as the partition key, and the timestamp as the sort key), and users that were attaching to an existing session did a query for all operations within a particular session ID to get the latest whiteboard status. But... What if somebody pressed the \"clear whiteboard\" button? There was no way to do a query for all operations since a particular operation happened, within a single table setup.\n\nSo in the end I implemented a second table with just the timestamp of the last clear operation of a session (partition key session ID, no sort key). So users would first query that second table with their session ID, get the timestamp of the last clear operation, and then query the operations table, but only for operations with that session ID and since that timestamp. Works like a charm. You don't even need a secondary index or something, since the timestamp was used as the sort key anyway, which gives an implicit index.\n\nA background query/scan operation is then used to purge the first table of stale entries every now and then.\n\nYou can do the same, by setting up an additional table where you store the timestamp of the \"Mark all Unread\" operations, with your user ID as the partition key. Your code should query that first, and then use that timestamp to only request data for that user, that is newer than that timestamp. If the timestamp is your sort key, then things are very easy. But if your timestamp is a non-indexed field, it may help to put a local secondary index on it. (It doesn't have to be a global secondary index in this case, as you won't be querying by specific timestamps.)\n\nThis also means that you don't have to update all your notifications when a user clicks on \"Mark all Read\". The only thing you need to think about - maybe - is the situation where a user clicks on \"Mark all Read\" and then wants to flag certain notifications as Unread anyway.",
          "score": 1,
          "created_utc": "2026-01-03 20:36:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxg2hjl",
          "author": "elchicodeallado",
          "text": "https://dev.to/epilot/scaling-notification-systems-how-a-single-timestamp-improved-our-dynamodb-performance-5c84\n\nhad the same issue and found a very smart solution. DM me if you need more help.",
          "score": 1,
          "created_utc": "2026-01-03 14:50:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxh1eod",
          "author": "alienangel2",
          "text": "Are you ok with some duplication and storage costs scaling with number of unreads? if so the approach is pretty simple: \n\n- for each user track message ids (or if the size is comparable, the whole message) that is unread. Depending on size this could be a per-user record, a per-user ddb table or (probably best) an s3 blob. You could use SQS if you were willing to let notificatuons expire within ~4 days\n- each time a new notification comes, update this per-user structure. Make sure it has metadata for a counter so you can efficiently display the notification count without parsing the whole thing \n- if the user hits \"mark all as read\", delete their user-specific record/table/s3 blob or purge their sqs \n\nThe duplication means you can do the delete in one operation without losing the original notifications (otherwise you could considering having a similar per-user structure for \"read notifications\" but at that point you've abandoned you (poorly thought out) original ddb schema for notifications) \n\nPushing it to s3 keeps it scalable. Even if a user legitimately has thousands of unread notifications they can't humanly read all of them at once anyway, so persisting them individually isn't really required. If they want to read a particular past notification (read or unread) that you let them addtess, they can just go directly to your original ddb table and read it by id.",
          "score": 0,
          "created_utc": "2026-01-03 17:39:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxh78sj",
          "author": "DaScoobyShuffle",
          "text": "Each message can have a boolean for read/unread.  Make sure they're fetched by timestamp.  When a user marks it as read, you make the boolean true.  Make sure this record can be indexed by timestamp.\n\nYou also store (immutable) \"master\" records.  These records have a timestamp and a flag for read/unread.  When a user marks everything as read, you create one of these.\n\nWhen reading individual messages later, you also query for the latest \"master\" record.  If the message is older than the master record, then you ignore the read/unread from the message record and use the master's.  Otherwise you use the message's value.\n\nThis way, it doesn't matter how many messages there are.",
          "score": 0,
          "created_utc": "2026-01-03 18:05:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q20o8j",
      "title": "AWS CloudFormation Diagrams",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1q20o8j/aws_cloudformation_diagrams/",
      "author": "Philippe_Merle",
      "created_utc": "2026-01-02 14:56:22",
      "score": 26,
      "num_comments": 5,
      "upvote_ratio": 0.81,
      "text": "[AWS CloudFormation Diagrams](https://github.com/philippemerle/AWS-CloudFormation-Diagrams) is a simple CLI script to generate AWS architecture diagrams from AWS CloudFormation templates. It parses both YAML and JSON AWS CloudFormation templates, supports 140 AWS resource types and any custom resource types, generates DOT, GIF, JPEG, PDF, PNG, SVG, and TIFF diagrams, and provides 126 generated diagram examples. Following illustrates a generated diagram example\n\nhttps://preview.redd.it/nzbkvn4q9yag1.png?width=4899&format=png&auto=webp&s=99771623c2d4e43240950e7f7d398ac0ef0104bc\n\n",
      "is_original_content": false,
      "link_flair_text": "technical resource",
      "permalink": "https://reddit.com/r/aws/comments/1q20o8j/aws_cloudformation_diagrams/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nxa33d2",
          "author": "Veuxdo",
          "text": "Nice work. Some ideas for improvement after just looking at the sample:\n\n- The sample diagram appears to be combining *run-time* relations and *permission* relations. An example a of *run-time* relation is APIG -> Lambda -> Dynamo Table, while *permission* relations include the IAM roles and permissions. It would probably be best to split these concerns up into two separate diagrams (and possibly a third: static Cloudfront configuration). [\"Master\" diagrams are generally bad](https://www.ilograph.com/blog/posts/breaking-up-the-master-diagram/).\n\n- The arrows really need labels to tell the viewer what the relations are.",
          "score": 11,
          "created_utc": "2026-01-02 16:50:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf9ajk",
              "author": "Philippe_Merle",
              "text": "Thank for these improvement ideas.",
              "score": 1,
              "created_utc": "2026-01-03 11:39:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxanayz",
          "author": "bruderj15",
          "text": "This looks great! I will give it a try!",
          "score": 2,
          "created_utc": "2026-01-02 18:24:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxc1n5w",
          "author": "imbecominginsane",
          "text": "Thanks for sharing! Will give it a try",
          "score": 2,
          "created_utc": "2026-01-02 22:27:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxehs0f",
          "author": "ThigleBeagleMingle",
          "text": "And‚Ä¶‚Ä¶? \n\nProblem \n\nChallenge/Opportunity\n\nApproach/capabilities\n\nImplementation\n\nResults/scope/impact statement",
          "score": 0,
          "created_utc": "2026-01-03 07:43:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxtlwv",
      "title": "AWS Support Nightmare",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pxtlwv/aws_support_nightmare/",
      "author": "theHephestus",
      "created_utc": "2025-12-28 15:12:11",
      "score": 15,
      "num_comments": 15,
      "upvote_ratio": 0.69,
      "text": "I am a long time lurker, I always read about AWS support horror stories here and I did not think it was that bad until a few days ago its still ongoing.  TLDR AWS support sucks ass.\n\nI have AWS Business Support +. AWS restricted my account after a security alert. I complied with all the remediation needed, even had to explain that CI/CD activity from GitHub Actions IP != human sign-in location.\n\nNow support is repeatedly insisting I delete EKS node group IAM roles that are actively in use, required for node groups to operate, and properly scoped standard EKS worker/ECR/CNI policies.\n\nThey haven‚Äôt provided any concrete justification beyond generic shared responsibility text and a link to how to delete a role. \n\nAnyone been through this? How did you escalate to get an actual security rationale or get restrictions lifted? Any success getting service credits for the delay?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pxtlwv/aws_support_nightmare/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwdpitm",
          "author": "clintkev251",
          "text": "If the roles were created or modified around the time of the compromise, they probably have these flagged as suspicious. You need to explain to them a few things, 1. The roles were created by you and have been validated as not suspicious, 2. They are currently in use for an active production workload and you will not be deleting them. If they don't listen, reiterate, request they escalate, remind them you consider this to be a false positive.",
          "score": 19,
          "created_utc": "2025-12-28 16:02:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwds2eb",
          "author": "cachemonet0x0cf6619",
          "text": "what‚Äôs stopping you from creating a new role with the same policies and swapping to that?",
          "score": 27,
          "created_utc": "2025-12-28 16:15:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdru7o",
          "author": "pipesed",
          "text": "What are the trust policies for these roles, and can you determine in cloudtrail what assumed these roles in the past?",
          "score": 8,
          "created_utc": "2025-12-28 16:14:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdoutw",
          "author": "AWSSupport",
          "text": "Hello,\n\nApologies for any frustrations you've encountered.\n\nYou can share your case ID via chat message, so we can pass along your concerns.\n\n\\- Elle G.",
          "score": 13,
          "created_utc": "2025-12-28 15:59:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe0oj7",
          "author": "Nearby-Middle-8991",
          "text": "Support is always a crapshoot, and AWS isn't the only one. It's a large pool of people, some are experienced, some are not. Some turn off their brains and follow the script blindly.\n\nFor AWS specifically, what worked for me was to check the working hours of whomever had the ticket, and then raise a chat *outside* of those, so I'd get someone new.  Alternatively, depending on your level of support, go light a fire under the TAM. But TAM experience was about as much as crapshoot for me, so idk...\n\nThe script being followed there probably assumes you roles were compromised. That's easy to check on cloudtrail, but can't really be sure without the details. Swapping roles shouldn't be that much trouble...",
          "score": 6,
          "created_utc": "2025-12-28 16:58:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfp8qf",
          "author": "pausethelogic",
          "text": "‚ÄúAWS support sucks ass because I don‚Äôt understand what they‚Äôre telling me to do‚Äù lol\n\nAWS takes security seriously, so they‚Äôre not going to be super flexible about you continuing to use a role that they‚Äôve marked as compromised\n\nAre you claiming that they‚Äôre wrong and the role wasn‚Äôt compromised or part of a security incident?",
          "score": 6,
          "created_utc": "2025-12-28 21:50:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8nes2",
              "author": "owengo1",
              "text": "Ok but what's the value of a support which can't explain why the action must done, and how to do it so that production workloads are not impacted?  \nI mean, if support is just: \"reset to factory defauts\" ( or whatever scripted instructions list ), you're better off with chatgpt \\[ which will probably ask to see the potentially compromised role and explain why it has to be replaced and how to replace it as safely as possible and how to monitor the replacement went ok or not>.  \n The real question is the value of the support, not if the request they make could be pertinent. Any dumbass can say \"wipe out\", \"block all traffic\", \"revoke all privileges\" etc.",
              "score": 0,
              "created_utc": "2026-01-02 11:53:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwg56yu",
          "author": "coinclink",
          "text": "I understand from your perspective that this seems like they are being dumb, but they are actually doing exactly what they are supposed to do. They haven't been assured that these roles weren't modified to allow the attacker to assume them during your compromise. All they see on their end is timestamps showing that the roles WERE created or modified during your compromise.\n\nDon't be upset with AWS because of your perceived superiority of knowledge. You should have a little humility instead. YOU (or your team) fat fingered and leaked a key and you shouldn't be upset with AWS taking drastic measures to ensure infrastructure security. (it's ok, we've all done it once or twice, I'm not trying to shame you, BUT YOU SHOULD BE ASHAMED, that's what stops you from doing it again!)",
          "score": 4,
          "created_utc": "2025-12-28 23:12:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8o6l2",
              "author": "owengo1",
              "text": "If support was really \"support\", and not just a scripted todo list, they would explain why there is a problem, the likely cause, and how to remedy it safely. It's very clear OP does not fully understand what happened and it's also very clear support made 0 effort to explain anything, they just had an automated security alert, sent an automated email with hard coded instructions, and they just say \"this must be done\". Even if yes, the action must be done, they prove they have 0 value versus a fully automated script.",
              "score": 1,
              "created_utc": "2026-01-02 11:59:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxal674",
                  "author": "coinclink",
                  "text": "I guarantee they did explain it. Every org has had this happen and they offer very clear and comprehensive explanations in their shared responsibility documentation pages that they link to.",
                  "score": 1,
                  "created_utc": "2026-01-02 18:14:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwib6qq",
          "author": "IridescentKoala",
          "text": "Why won't you remove the roles?",
          "score": 1,
          "created_utc": "2025-12-29 07:20:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwivkls",
          "author": "devguyrun",
          "text": "products/services aside, AWS support is best in class in my opinion, most if not all in the support org are highly technical folks and know what they are talking about, when they don't , they always check in with others that do.\n\none should be smart enough to know when they are not being so....",
          "score": 1,
          "created_utc": "2025-12-29 10:29:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8mk6v",
          "author": "owengo1",
          "text": "This is \"nova\" support I guess",
          "score": 1,
          "created_utc": "2026-01-02 11:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg3e72",
          "author": "isoAntti",
          "text": "They never mention these cases when they say go cloud/aws",
          "score": -2,
          "created_utc": "2025-12-28 23:03:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2epx1",
      "title": "How do you monitor async (lambda -> sqs -> lambda..) workflows when correlation Ids fall apart?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1q2epx1/how_do_you_monitor_async_lambda_sqs_lambda/",
      "author": "bl4ckmagik",
      "created_utc": "2026-01-02 23:49:49",
      "score": 15,
      "num_comments": 21,
      "upvote_ratio": 0.89,
      "text": "Hi guys,   \n\nI have experienced issues related to async workflows such as the flow not completing, or not even being triggered when there are multiple hops involved (API gateway -> lambda -> sqs -> lambda...) and things breaking silently.   \n\nI was wondering if you guys have faced similar issues such as not knowing if a flow completed as expected. Especially, at scale when there are 1000s of flows being run in parallel.   \n\nOne example being, I have an EOD workflow that had failed because of a bug in a calculation which decides next steps, and it never sent the message to the queue because of the bug miscalcuting. Therefore it never even threw an error or alert. I only got to know about this a few days later.   \n\nYou can always retrospectively look at logs and try to figure out what went wrong but that would require you knowing that a workflow failed or never got triggered in the first place.   \n\nAre there any tools you use to monitor async workflows and surface these issues? Like track the expected and actual flow? ",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1q2epx1/how_do_you_monitor_async_lambda_sqs_lambda/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nxd4qfz",
          "author": "TampaStartupGuy",
          "text": "Not sure this will solve your issue, but here‚Äôs a solution that I use. \n\nTrack the workflow state directly in DynamoDB instead of relying on logs. The core issue is you‚Äôre depending on error logs to flag failures, but in this case the bug prevented the message from being sent at all, so there was nothing to log.\n\nInstead, make the expected workflow state something you can query. Set up a job tracking table in Ddb where each step is recorded with a job ID, step name, status like pending, in progress, completed, or failed with a timestamp. Then you query for steps that never completed, not just the ones that failed. \n\nYou can invoke the lambda whenever the task is done.  It queries for jobs where the status isn‚Äôt marked as completed and sends an alert if anything‚Äôs missing.\n\nThe key idea is to track what should have happened and alert when something doesn‚Äôt show up, rather than waiting for an error to tell you something broke. DLQs help if your handler crashes. State tracking catches the silent failures where no message was sent at all.",
          "score": 13,
          "created_utc": "2026-01-03 02:03:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxeurw0",
              "author": "Willkuer__",
              "text": "I currently have to deal with this setup and I hate it tbh. I still have to find the correct logs to debug the issue. Instead of writing your own process manager in ddb I find stepfunction executions much easier to use and they give me all the information needed: input, output, error for each individual step.\n\nWe also have the issue that we have some relatively long running jobs that can fail within ninutes but also run succesfully for days. The jobs health is just so hard to debug with a ddb record and loads of cloudwatch logs.",
              "score": 5,
              "created_utc": "2026-01-03 09:37:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxckrk6",
          "author": "smutje187",
          "text": "You should never consciously let a process fail silently - issues with AWS itself can always happen but your Lambda code should never ignore errors or exceptions (depending on your programming language) and instead raise CloudWatch alarms or other kind of events that trigger someone to take a look.",
          "score": 10,
          "created_utc": "2026-01-03 00:10:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxctw4e",
              "author": "bl4ckmagik",
              "text": "Agreed. I'm very much in the fail loud camp as well. Maybe I should have phrased my post better. Sorry, English isn't my first language.   \nThe tricky cases I've experienced are not really exceptions or errors inside lambdas but situations where they don't run at all causing workflow to finish halfway. Like an event ridge rule not matching, sns filters dropping messages... Etc.    \n\nIn those scenarios there's no event to trigger an alarm. You only notice it because a downstream effect never happens.   \n\nAny thoughts on catching non-events like that early?",
              "score": 2,
              "created_utc": "2026-01-03 01:01:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxeu37j",
          "author": "Willkuer__",
          "text": "I am currently rebuilding a POC based on sqs+lambda flows using stepfunctions because of such issues. I've worked with both in the past and find stepfunctions much easier on the observability side if the workflow gets more complex/has more steps.\n\nStepfunctions comes with a significant overhead in engineering I'd say but it is worth the hassle.\n\nThis obviously makes only sense of you have a job-like structure: a trigger, start/end of a job. If you are more interested in kind of real-time streaming it's maybe too rigid depending on your usecase.",
          "score": 5,
          "created_utc": "2026-01-03 09:31:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxchc86",
          "author": "Iliketrucks2",
          "text": "Can you add cray tracing easily?",
          "score": 1,
          "created_utc": "2026-01-02 23:51:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxchha2",
              "author": "bl4ckmagik",
              "text": "Sorry, do you mean X-ray tracing?",
              "score": 1,
              "created_utc": "2026-01-02 23:52:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxci13g",
                  "author": "Iliketrucks2",
                  "text": "Lol yeah sorry. Autocorrect got me.  I think you can fairly quickly and easily instrument your code to get better insights into what‚Äôs going on",
                  "score": 1,
                  "created_utc": "2026-01-02 23:55:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxe5pmg",
          "author": "commentShark",
          "text": "I currently have every error thrown raise an alarm which emails me, then I investigate that issue by pasting the email to Claude to debug it (which has a doc of Instructions on how to query etc). It can fix the issue quite fast. \n\nWorks for my low traffic site, I‚Äôve plugged lots of holes and edge cases this way. Could work with higher error thresholds or sampling.",
          "score": 1,
          "created_utc": "2026-01-03 06:03:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q09vkg",
      "title": "Dynamodb local support for multi-attribute GSI",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1q09vkg/dynamodb_local_support_for_multiattribute_gsi/",
      "author": "quantumfy",
      "created_utc": "2025-12-31 10:25:57",
      "score": 12,
      "num_comments": 7,
      "upvote_ratio": 0.94,
      "text": "Dear u/aws ,  \nWhen will support for multi-attribute GSI be available in Dynamodb-local?",
      "is_original_content": false,
      "link_flair_text": "database",
      "permalink": "https://reddit.com/r/aws/comments/1q09vkg/dynamodb_local_support_for_multiattribute_gsi/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwyh2kh",
          "author": "soundman32",
          "text": "For testing purposes?",
          "score": 1,
          "created_utc": "2025-12-31 18:43:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwyja5g",
              "author": "quantumfy",
              "text": "Both testing and development, I have been using it on my local environment for a while, and it was working fine. Now that this new feature has been used, it no longer functions on local",
              "score": 2,
              "created_utc": "2025-12-31 18:54:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwyl99w",
          "author": "AWSSupport",
          "text": "Hi there,\n\nI found the following docs regarding multi-attribute GSI in Dynamodb-local: https://go.aws/4q2zaav & https://go.aws/3N8uwJc.\n\nIf this isn't quite it, send us a chat message with further details, so we can pass it along to our team.\n\n\\- Elle G.",
          "score": -1,
          "created_utc": "2025-12-31 19:04:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwyy5jh",
              "author": "Jolly-Phone8982",
              "text": "Hello, \n\nI believe what OP is referring to is the fact that the dynamo-db local docker image hasn‚Äôt been updated to support the new multi-attribute GSI feature. \n\nThe resources you sent are correct, however, the dynamo db image on docker hub was last updated 4 months ago and fails to create a table using the new gsi system. \n\nLooks like the dynamo team need to push the latest image to docker so we can start testing and migrating our dbs",
              "score": 6,
              "created_utc": "2025-12-31 20:12:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx0pssl",
                  "author": "kei_ichi",
                  "text": "Unfortunately, I think we will get new ‚ÄúAI‚Äù features before we get that image updated‚Ä¶",
                  "score": 1,
                  "created_utc": "2026-01-01 02:30:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2keo7",
              "author": "redditor_tx",
              "text": "u/AWSSupport Elle, can you please ping the team? It's worrying that the Docker image hasn't been updated in 4 months. I also found [https://github.com/aws/aws-sdk-net/issues/4179](https://github.com/aws/aws-sdk-net/issues/4179)",
              "score": 1,
              "created_utc": "2026-01-01 12:30:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2nz4q",
                  "author": "AWSSupport",
                  "text": "Hi there,\n\nI've passed along feedback to our devs team for review. \n\n\\- Kay B.",
                  "score": 2,
                  "created_utc": "2026-01-01 13:01:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q0zatf",
      "title": "How is the SA market in 2025?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1q0zatf/how_is_the_sa_market_in_2025/",
      "author": "No_Mood4637",
      "created_utc": "2026-01-01 08:18:12",
      "score": 10,
      "num_comments": 14,
      "upvote_ratio": 0.71,
      "text": "I'm a Senior Dev who has thinking about jumping to a SA role for the past few years. I did the SAA cert in 2023 and have been building with AWS since 10 years. Europe based.\n\nMy job has become more about managing AI agents now, and it's less fulfilling. In fact even our CDK has become mostly AI driven.\n\nHow do you feel about the future of the SA role in terms of job safety and satisfaction? \n\nThanks",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1q0zatf/how_is_the_sa_market_in_2025/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nx1zcwr",
          "author": "mathilda-scott",
          "text": "From what I‚Äôm seeing, SA roles in 2025 are still solid in Europe, but the shape of the job has changed. The pure ‚Äúdesign architectures‚Äù part is more commoditized now, especially with AI-assisted IaC.\n\nStrong SAs are the ones who can translate messy business problems into constraints, trade-offs, security, cost, and org impact - not just draw diagrams. If you enjoy customer-facing work, influencing decisions, and guiding teams (vs. hands-on coding all day), SA can still be satisfying and fairly safe. If you want deep build work again, it may feel like a lateral move rather than an escape from AI-driven workflows.",
          "score": 22,
          "created_utc": "2026-01-01 08:52:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3befq",
          "author": "nope_nope_nope_yep_",
          "text": "SA here.\n\nIt‚Äôs still a growing and important area, AI can help, but it doesn‚Äôt really help with all of the actual sales and finding out what the customer really wants, some customers just don‚Äôt know what they really want and need a human to make the decision if what they want is really the right fit for their business needs.",
          "score": 3,
          "created_utc": "2026-01-01 15:38:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1wvfb",
          "author": "Old_Cry1308",
          "text": "sa still exists but way more pre sales and powerpoint than deep aws. fun depends on company. and yeah hiring is rough lately everywhere",
          "score": 9,
          "created_utc": "2026-01-01 08:25:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1xkqu",
              "author": "No_Mood4637",
              "text": "My intuition is that the SA market would be heavily saturated because of AI lowering the skill requirement of doing architecture work, so what do we need highly paid SA. Same with consultancies in general. I think the big4 have all had big layoffs recently... I'm also thinking about the way AWS in heading, building their own AIs specifically designed for architecture planning which are very easy to use. Is the golden age of SA behind us?",
              "score": -6,
              "created_utc": "2026-01-01 08:33:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2di4s",
                  "author": "forsgren123",
                  "text": "It's a common mistake to think that the SA role is only about tech, while in reality the role is customer-facing and requires good soft skills, business acumen, decision maker relationship building, navigating customer org, sales pipeline, identifying and developing opportunities, connecting customers to service teams, public speaking, leading meetings and workshops, etc.\n\nIf you're *only* technical, you will end up being a glorified tech support for the customers' developers - which is being commoditized because developers can ask those technical questions directly from the AI instead.\n\nFrom personal experience I see AI tools only boosting SA work, because you can do research and build demos/pocs much faster now. And who's better at commanding AI than the SA who often has 20 years of experience in the industry, understands the business challenge and constraints, and knows the platform inside out.",
                  "score": 11,
                  "created_utc": "2026-01-01 11:22:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx23iwe",
                  "author": "Jin-Bru",
                  "text": "As a senior Enterprise architect with over 30 years of design experience AI might have broad impact at the development coding level but architecture is often quite specific within the organisation. \n\nThe layers may be fairly constant but the deployment of architecture needs to be customised for each organisation\n\n\nI'd say architecture is still a good place to be but employers are digging deeper into experience than output.",
                  "score": 3,
                  "created_utc": "2026-01-01 09:37:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx32xde",
                  "author": "CoolBoi6Pack",
                  "text": "Not the case for sure. Most architecture work can't be done by AI because it's too difficult to explain all the business constraints and impacts that we're targeting.",
                  "score": 1,
                  "created_utc": "2026-01-01 14:48:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1za28",
          "author": "TomRiha",
          "text": "SA role has been watered out over the years. Used to be very senior people who had built and accomplished things in their careers, wanting to help customers. \n\nToday it‚Äôs kids chasing promotions, period.",
          "score": 5,
          "created_utc": "2026-01-01 08:51:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2niql",
              "author": "Sirwired",
              "text": "In my SA Launch Group, there was not a single \"kid chasing promotions.\" We had a Sr. RDS dev, a vet coming from 15 yrs in Army IT, myself (25 yrs in IT infrastructure), and a seasoned K8s/EKS admin.\n\nAnd yeah, I use the hell out of AI tools when slapping together CDK proof-of-concepts, but you can be assured that nothing is going in front of a customer until I understand the why and how of every box on that diagram.",
              "score": 7,
              "created_utc": "2026-01-01 12:57:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx388dh",
                  "author": "TomRiha",
                  "text": "Good for the customers in your Geo in that case.\n\nThough, unless your in a greenfield segment 9 out of 10 customers are gonna be deeper in in their competence then IaC PoCs. With those customers the value is Specialist SA engagements. Generalist/Account SAs need to be able to gain trust of the customers senior technical management to move the needle. This is not done by IaC PoCs. \n\nThis is done by deeply understanding customers domain and challenges. This is something today‚Äôs SAs are not equipped to do, because they are too junior. It‚Äôs also hard to make or deepen those relationships. In the past SAs had fewer customers and spent tons of time onsite with customers. Today it‚Äôs more accounts, distributed teams and customers working hybrid making it really difficult.\n\nAll of the above is why SAs are turning more into presales. Partially because in presales PoCs you can get away with IaC PoCs and simple things like that. But also because the customers have changed. Either customers are super deep or not in cloud. The not in cloud ones need to be sold to, they are not self served customers. All those are in cloud already.\n\nWith all the above how do associate SAs make an impact? Well they hunt for stories to their promo doc. Their managers cheer them on because they have KPIs on leveling employees. So Day 2.",
                  "score": 4,
                  "created_utc": "2026-01-01 15:20:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdm0uj",
                  "author": "mountainlifa",
                  "text": "\"SA launch\". The stuff of nightmares.",
                  "score": 1,
                  "created_utc": "2026-01-03 03:47:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2sm07",
              "author": "cjrun",
              "text": "And this is why its important that an SA portrays an aura of some technical credibility.",
              "score": 1,
              "created_utc": "2026-01-01 13:37:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdmdhl",
          "author": "mountainlifa",
          "text": "With the impending layoffs predicted to hit AWS I can't imagine it's a good place to be. I served from 2017-2022 and it was cutthroat then despite no threat from layoffs, I can't imagine now. I would suspect it's highly sales focused, you'd be better off switching to account executive, at least you get commission.",
          "score": 1,
          "created_utc": "2026-01-03 03:49:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyc5zb",
      "title": "Denial of Wallet Via Route 53?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pyc5zb/denial_of_wallet_via_route_53/",
      "author": "Extra-Moose4828",
      "created_utc": "2025-12-29 04:08:43",
      "score": 9,
      "num_comments": 8,
      "upvote_ratio": 0.8,
      "text": "I am wondering if anyone knows if a Denial of Wallet attack via Route 53 is possible??\n\n  \nThe pricing for Route 53 is $0.40 per million queries per month.\n\n  \nI know that this can be avoided by pointing the DNS records to an AWS resource (as described here: [https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/configuring-route53-for-cost-protection-from-nxdomain-attacks.html](https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/configuring-route53-for-cost-protection-from-nxdomain-attacks.html) ).\n\n  \nBut let's say that's not an option. Is it even feasible for an attacker to send enough DNS queries to rack up a substantial (>$100) bill?? O  \nMy napkin math tells me that to get to >$100, they would need to send 250 million requests in a month. Which I think sounds possible?? \n\n  \nHas anyone ever witnessed such an attack?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pyc5zb/denial_of_wallet_via_route_53/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwj3ww5",
          "author": "Sirwired",
          "text": "If someone wants to rack up your AWS bill, they are definiteily likely to choose other routes besides R53.  (In the big scheme of things, a $100/mo bill isn't considered \"substantial\" at all.)",
          "score": 28,
          "created_utc": "2025-12-29 11:43:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiymmo",
          "author": "jmkgreen",
          "text": "They could. Question is why. You wouldn‚Äôt likely be causing a DoS and it‚Äôs probably the case AWS would raise their shields faster than you would notice.\n\nThat‚Äôs not to say something nasty has never happened, I just don‚Äôt recall this vector being mentioned as an attack likely to inflict damage when hosted on modern infrastructure.",
          "score": 11,
          "created_utc": "2025-12-29 10:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj4dzj",
          "author": "RecordingForward2690",
          "text": "AWS hosts millions of public domains, but all these domains are hosted on a smaller number of DNS servers (still a large number of servers, because there's multiple servers in each of the 250+ POPs, but probably thousands instead of millions).\n\nDespite things like Shuffle Sharding (see the Builders Library for an explanation), there is a risk that a DoS/DoW attack on one domain impacts other domains. That's why AWS  protects its own infrastructure against these types of attacks by default. AWS calls this protection Shield Standard. \n\n>All AWS customers benefit from the automatic protection of Shield Standard, at no additional charge. Shield Standard defends against the most common, frequently occurring network and transport layer DDoS attacks that target your website or applications. While Shield Standard helps protect all AWS customers, you get particular benefit with Amazon Route¬†53 hosted zones, Amazon CloudFront distributions, and AWS Global Accelerator standard accelerators. These resources receive comprehensive availability protection against all known network and transport layer attacks.\n\n[https://docs.aws.amazon.com/waf/latest/developerguide/ddos-standard-summary.html](https://docs.aws.amazon.com/waf/latest/developerguide/ddos-standard-summary.html)",
          "score": 8,
          "created_utc": "2025-12-29 11:47:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjb9dw",
          "author": "Big-Minimum6368",
          "text": "In order to even get to $1000 would take 2.5 billion requests per month. Someone check my math.\n\nThis would not be a worthwhile attack vector in my mind. Our AWS bills we wouldn't notice that outside of an audit",
          "score": 7,
          "created_utc": "2025-12-29 12:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwizwth",
          "author": "Wilbo007",
          "text": "In a month thats just under 100 queries per second, for a month certainly possible",
          "score": 1,
          "created_utc": "2025-12-29 11:09:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3wa93",
          "author": "dub_starr",
          "text": "my company was hit with an NXDOMAIN attack a few years ago. Not AWS DNS provider, but it is a possible thing. if the DNS company didnt work with us, we would have been on the hook for over a million dollars (pretty high profile sites and company). one of the problems we had is that we had a very large legacy backlinking posture, which was very helpful for google ranking at the time. We had some wildcard DNS entries for a lot of the older links that would direct them to a landing page of sorts. That wildcard record was the set of sites that were attacked. With a small botnet, its really easy to get the request counts up high",
          "score": 1,
          "created_utc": "2026-01-01 17:30:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkkant",
          "author": "eggwhiteontoast",
          "text": "Any query to your domain doesn‚Äôt always end up in Route53 or wherever your domain is hosted, DNS records are often cached by numerous DNS servers on the way for faster performance. Unless your TTL is very low, query to your domain will most likely be answered by one of the intermediary DNS servers for eg ISPs DNS.",
          "score": 1,
          "created_utc": "2025-12-29 16:47:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnmbwa",
              "author": "Extra-Moose4828",
              "text": "Correct, however that doesn't stop an attacker from directly querying AWS's authoritative server directly.",
              "score": 5,
              "created_utc": "2025-12-30 01:59:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzj2kg",
      "title": "How does RDS use NVMe instance store?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pzj2kg/how_does_rds_use_nvme_instance_store/",
      "author": "RecordingForward2690",
      "created_utc": "2025-12-30 14:10:20",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 0.91,
      "text": "I have a transactional MSSQL DB that currently runs on a db.z1d.2xlarge RDS instance. From the metrics we know that this database is overprovisioned, and we are looking at smaller (cheaper) instances, possibly a db.r7i.xlarge.\n\n(Note that there is a discrepancy in the documentation: [This page](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.Concepts.General.InstanceClasses.html) claims that MSSQL SE supports a db.r7i.xlarge, while [this page](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.Support.html) claims it doesn't.)\n\nBased on the CW Metrics and DB Insights I can pretty much predict how the DB will behave regarding CPU, memory, network and EBS I/O when switching instance types. However, the z1d.2xlarge also has 300 GB of NVMe SSD instance store, and I have no clue whether this is used, what for, and whether this will impact performance if I switch to an instance type without instance store. It doesn't seem like there are CW Metrics available for starters, and I also can't find any documentation on it. Does anybody know of a way to understand what's going on with this storage?\n\nThe problem is also that this is a production database that runs 24/7. Due to it being Multi-AZ, switching instance types requires quite a bit of downtime that we have to schedule in advance. This severely limits the ability to experiment. I do have a test environment but I don't have a mock load generator that is representative of the workload.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pzj2kg/how_does_rds_use_nvme_instance_store/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwqjvwg",
          "author": "earl_of_angus",
          "text": "eta: I originally read MSSQL as MySQL - optimized reads doesn't apply.  Instead, with MS SQL you can check where the tmpdb is stored to see if it's on the nvme drive or ebs.\n\nFor uses of nvme, out of the box defaults will enable Optimized Reads: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-optimized-reads.html#rds-optimized-reads-use-cases (TL;DR: temp tables etc)\n\nFor CloudWatch, take a look at the *LocalStorage metrics e.g., ReadThroughputLocalStorage - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-metrics.html (ctrl-f LocalStorage)",
          "score": 3,
          "created_utc": "2025-12-30 14:45:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqyi8k",
              "author": "RecordingForward2690",
              "text": "Thanks. Your response however, throws up more questions.\n\nFirst, I don't have any \\*LocalStorage metrics in CloudWatch. At all. Not even the FreeLocalStorage or FreeLocalStoragePercent, which I would expect to be there even if instance store was not used at all. Any idea?\n\nSecond, [this documentation](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.InstanceStore.html) suggests that instance store is automatically used for tempdb usage, but only on db.m5d, db.r5d and db.x2iedn instances. db.z1d instances are not listed. Would that be an error in the documentation, or does that mean that I would actually need to put in extra work (which I didn't do) to start using the instance store? I'm not familiar with MSSQL at all. Is the disk location of the tempdb something that I can view using SQL Studio or something?",
              "score": 1,
              "created_utc": "2025-12-30 15:58:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwr28wn",
                  "author": "earl_of_angus",
                  "text": "> Is the disk location of the tempdb something that I can view using SQL Studio or something?\n\nThe tempdb is an object in SQL Server Management Studio (under system databases) that can be right-clicked & then view properties, or you can run a query like:\n\n\n    SELECT name AS file_name, physical_name AS physical_location\n    FROM sys.master_files\n    WHERE database_id = DB_ID(N'tempdb');",
                  "score": 2,
                  "created_utc": "2025-12-30 16:16:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwrncnn",
          "author": "Competitive_Two6205",
          "text": "RDS for SQL Server uses instance store to host the TempDB. You should monitor your TempDB usage to assess the impact to move it to off NVMe. For example if your databases are using Read Committed Snapshot Isolation (RCSI) your performances will likely be affected. I'm happy to provide more details",
          "score": 2,
          "created_utc": "2025-12-30 17:54:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqoky9",
          "author": "InterestedBalboa",
          "text": "Do yourself a favour and put together a plan to get off MSSQL, it‚Äôs not cloud native, expensive and has limitations the competition doesn‚Äôt have.\n\nIf you‚Äôre a ‚ÄúMicrosoft shop‚Äù then you have bigger problems to worry about and you can disregard said advice.",
          "score": 0,
          "created_utc": "2025-12-30 15:10:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwquxnc",
              "author": "RecordingForward2690",
              "text": "Wish I could. This is a 3rd party product that requires 3 MSSQL databases, and something like 69 Windows EC2 instances (with MS Service Fabric) to run properly in our Prod environment, with our workload. All authentication is done through AD. Ripping out the DB technology and replacing it with something else is not going to happen.\n\nThis is also the first Microsoft-based workload that we're hosting in AWS. All other workloads so far were either developed in-house (serverless) or Linux-based. So we're learning a lot about Microsoft technology as we go along.\n\nThe good news is that all the integrations that we built around this product are all AWS-native, with DynamoDB, SQS/SNS, EventBridge, Lambda, ECS and whatnot.",
              "score": 3,
              "created_utc": "2025-12-30 15:41:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pze3rs",
      "title": "Using a presigned url in 2025 to upload file is a good enough solution to protect from malware files and allowing only images?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pze3rs/using_a_presigned_url_in_2025_to_upload_file_is_a/",
      "author": "xSypRo",
      "created_utc": "2025-12-30 09:49:02",
      "score": 9,
      "num_comments": 24,
      "upvote_ratio": 0.76,
      "text": "Hi,\n\nI was looking for the answer online and came across this - [https://www.reddit.com/r/aws/comments/zmbw4h/enforce\\_content\\_type\\_during\\_upload\\_with\\_s3\\_signed/](https://www.reddit.com/r/aws/comments/zmbw4h/enforce_content_type_during_upload_with_s3_signed/)\n\n  \nThis post from 3 years ago, and the answer was no. 3 years ago AWS S3 only allowed to enforce content type header, which is a joke for a serious attacker.\n\n3 years later, is there a solution?\n\nI am working on an app of my own that allows users to upload file, verifying the files are legit is a big overhead that I want to take off my mind. Presigned url is an easy solution or should I skip it and do it on my server?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pze3rs/using_a_presigned_url_in_2025_to_upload_file_is_a/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwpfvzj",
          "author": "The_Startup_CTO",
          "text": "AWS GuardDuty can scan files for Malware, and you can configure the bucket with a deny policy for any file that doesn't have the tag from GuardDuty that it's clean.",
          "score": 51,
          "created_utc": "2025-12-30 09:56:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwro2cu",
              "author": "thegeniunearticle",
              "text": "But - GD ain't cheap.\n\nS3 bucket scanning adds up.",
              "score": 4,
              "created_utc": "2025-12-30 17:57:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwslvgh",
                  "author": "Zenin",
                  "text": "Have you checked the pricing recently on GD for this?  It dropped 85% last Feb, making it *much* more reasonable.\n\n[https://aws.amazon.com/about-aws/whats-new/2025/02/amazon-guardduty-malware-protection-s3-price-reduction/](https://aws.amazon.com/about-aws/whats-new/2025/02/amazon-guardduty-malware-protection-s3-price-reduction/)",
                  "score": 6,
                  "created_utc": "2025-12-30 20:36:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwrqqte",
                  "author": "The_Startup_CTO",
                  "text": "There are alternatives, though I would recommend to do the math. So far, at none of the companies I've worked with, the cost for the malware scan was anywhere near something that would justify spending the working hours of engineers to find a cheaper solution.",
                  "score": 3,
                  "created_utc": "2025-12-30 18:10:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwroc07",
                  "author": "techypaul",
                  "text": "There are self made options, or one from the marketplace which is pretty cheap (Sophos based).",
                  "score": 1,
                  "created_utc": "2025-12-30 17:59:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpg2xs",
          "author": "steveoderocker",
          "text": "The best way to do it is to use something like event notifications and use lambda to actually verify the file contents. Eg https://devsecopssourav.hashnode.dev/content-type-validation-during-file-uploads-to-an-aws-s3-bucket",
          "score": 18,
          "created_utc": "2025-12-30 09:58:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwphbp7",
              "author": "RecordingForward2690",
              "text": "This was my idea as well. And you can combine it with the tip from u/The_Startup_CTO: After the Lambda has verified that the file contents matches the Content-Type header AND the Content-Type header is allowed, you add a tag that indicates the file is clean. The S3 bucket then has a resource policy with a Deny on any GetObject API call when the tag is not present. Simple, elegant, minimal code changes necessary.",
              "score": 16,
              "created_utc": "2025-12-30 10:09:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwq4w7r",
              "author": "The_Startup_CTO",
              "text": "I don't think that this would work. You can already limit the presigned url to a specific content-type, so verifying with the lambda afterwards doesn't give you anything extra. The actual danger isn't the content-type, though, that's just metadata that hints to the browser how it should try to open the file. You can still upload any combination of bytes and just give it whatever content-type will be accepted. But the file will still have the same malware in it.\n\nThat's why the check via GuardDuty is so important: It checks the actual content of the file, not just some metadata associated with it.",
              "score": 2,
              "created_utc": "2025-12-30 13:18:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwqjigu",
                  "author": "RecordingForward2690",
                  "text": "Read the blog post to the end. The example uses python-magic to actually determine the type of file from the content. This is then compared to the Content-Type header. So it will even detect the situation where a hacker uploads a file in an allowed content format, but uses a different (but also allowed) Content-Type header.",
                  "score": 2,
                  "created_utc": "2025-12-30 14:43:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpg57w",
          "author": "nemec",
          "text": "nothing's changed. AWS doesn't validate your content. You can't trust what the client uploads, all you can do is validate after the upload is finished, before the new content is available to others.\n\ne.g. virus scanning: https://docs.aws.amazon.com/guardduty/latest/ug/gdu-malware-protection-s3.html",
          "score": 13,
          "created_utc": "2025-12-30 09:58:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwppxls",
          "author": "texxelate",
          "text": "Presigned URLs are not intended to solve this problem. So,  no. You‚Äôll need to analyse the file after it has been uploaded.\n\nYou can do this easily by configuring an SQS queue which invokes a Lambda, or something which others have suggested.",
          "score": 5,
          "created_utc": "2025-12-30 11:27:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpudqw",
          "author": "martinbean",
          "text": "A pre-signed URL just lets a user upload a file to your S3 bucket for a short period of time. It does absolutely **zero** checks on the type of file being uploaded.",
          "score": 6,
          "created_utc": "2025-12-30 12:04:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpu857",
          "author": "raja4net",
          "text": "If you don‚Äôt want to use GuardDuty, you can use [ClamAV](https://aws.amazon.com/blogs/developer/virus-scan-s3-buckets-with-a-serverless-clamav-based-cdk-construct/) to scan for malware. No built-in solution to restrict upload of images only.",
          "score": 3,
          "created_utc": "2025-12-30 12:02:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2stqu",
          "author": "TopSwagCode",
          "text": "Presigned urls isnt protection for anything. Its just a easy way to upload files without them touching your server first. If you want to scan items you either need to scan them before upload on server or scan afterwards in bucket. There are tons of options to do so.",
          "score": 2,
          "created_utc": "2026-01-01 13:39:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpok5e",
          "author": "magnetik79",
          "text": "You have to programmatically verify the content of the upload. The presigned S3 URL system is solid, what you do with the content beyond that is your job to implement.",
          "score": 1,
          "created_utc": "2025-12-30 11:15:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq7qyi",
          "author": "Toastyproduct",
          "text": "No built in solutions. Best method is to build this into the backend. If the files are going to be limited in size and number then uploading to your backend and then placing in the bucket is still best. For images I recommend sanitizing using a reformat with some tool to a common file type and wiping out metadata. \n\nIf the files are more frequent or bigger then a ‚Äúinbox‚Äù location and then a lambda might be better than directly through backend. Just depends on your scale and resource sizing.",
          "score": 1,
          "created_utc": "2025-12-30 13:36:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsn86a",
          "author": "Zenin",
          "text": "You can't trust any data you (or a service you control) has actually seen.  There's no client-side solution to this problem; You *have* to accept the data first and then scan it however you choose.  And you have to scan *all* of it, not just a handful of bytes at the start to workout the file magic number type.\n\nGuardDuty is the obvious choice and after Feb's price cuts is very reasonably priced.  Anything else you do is going to almost certainly cost you more.  More resources, more man hours, more risks, more support issues, more outages.",
          "score": 1,
          "created_utc": "2025-12-30 20:43:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q17obr",
      "title": "Learning path for AWS Certified Solutions Architect",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1q17obr/learning_path_for_aws_certified_solutions/",
      "author": "Oredreim",
      "created_utc": "2026-01-01 16:08:45",
      "score": 8,
      "num_comments": 9,
      "upvote_ratio": 0.84,
      "text": "Hi! I'm a cybersecurity Engineer (more for red team) that wants to be certified with AWS Certified Solutions Architect, and I'm here to ask for videos or documentations or anything that could help me learn to approve this Certification.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1q17obr/learning_path_for_aws_certified_solutions/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nx3iggy",
          "author": "AWSSupport",
          "text": "Hi there,\n\nThese materials can help you study for our AWS Certified Solutions Architect exam: https://go.aws/3YkgyGI & https://go.aws/3Lm0YaB.\n\n\\- Elle G.",
          "score": 13,
          "created_utc": "2026-01-01 16:17:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3i406",
          "author": "abofh",
          "text": "Take the sample tests, if you clear them easy, you'll do fine, if you struggle, read the docs more.\n\n\nAlso: red team, not read team",
          "score": 7,
          "created_utc": "2026-01-01 16:15:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3qvml",
              "author": "Oredreim",
              "text": "The autocorrect Xd",
              "score": 1,
              "created_utc": "2026-01-01 17:01:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4r07d",
          "author": "Tiny_Durian_5650",
          "text": "I passed my SA Pro exam on the first try mostly by watching video tutorials, I think they were from Linux Academy. The teacher's name was Adrian Cantrill, he sells them on his own site now: https://learn.cantrill.io/",
          "score": 5,
          "created_utc": "2026-01-01 20:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3j1ni",
          "author": "sad-whale",
          "text": "r/awscertifications FAQ",
          "score": 3,
          "created_utc": "2026-01-01 16:20:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3lcjm",
          "author": "xxwetdogxx",
          "text": "Practice tests are a must, I'm a tutorialsdojo guy myself",
          "score": 4,
          "created_utc": "2026-01-01 16:32:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3n7jz",
              "author": "t90090",
              "text": "TJ is a must for practice exams and their cheat sheets.",
              "score": 3,
              "created_utc": "2026-01-01 16:42:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8esr2",
          "author": "skillbubble",
          "text": "If you‚Äôre coming from a red team / security background, the biggest shift is thinking in terms of **architecture and trade-offs**, not just services.\n\nI‚Äôd recommend starting with:\n\n* **AWS Skill Builder**¬†(official, mapped directly to the exam)\n* **AWS Well-Architected Framework**¬†‚Äî especially the Security and Reliability pillars\n* Adrian Cantrill‚Äôs Solutions Architect course (very architecture-focused)\n\nWhen studying, try to map each service to¬†*why*¬†you‚Äôd choose it, not just¬†*how*¬†it works. That mindset helps a lot on the exam.",
          "score": 1,
          "created_utc": "2026-01-02 10:37:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxj5pmx",
          "author": "garathk",
          "text": "Udemy Stephan Maarek course. I used that plus some practice tests and passed by a pretty solid margin. The udemy courses go on sale very frequently. I'm pretty sure I've never paid more than 10 bucks for one. Good platform.",
          "score": 1,
          "created_utc": "2026-01-03 23:48:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyj2w6",
      "title": "Identify workspaces with no user connections in x days",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pyj2w6/identify_workspaces_with_no_user_connections_in_x/",
      "author": "-Xuby-",
      "created_utc": "2025-12-29 10:35:40",
      "score": 8,
      "num_comments": 12,
      "upvote_ratio": 0.9,
      "text": "Hi team,\n\nI've come from a strictly MS world and just been given access to AWS, we have around 700 always on workspaces that users connect to as their 'desktop'. \n\nI suspect we have over 100 not logged into in the last 30days.\n\nI've got access to the workspaces node and cloudwatch. The AD attribute for last login is inaccurate (suspect a service account periodically connecting). \n\nLooking for simple way to generate a list of machines where no users have connected in say 30days.\n\nIve been going in circles trying to see when UserConnected=0 for >30days. (Combining with max/min) \n\nKeep hitting 500 metric limit. \n\nFrom the workspaces node side it's the \"User last active\" field I'm interested in. \n\nFrom a windows /powershell point of view I'd just iterate & dump computer name and user last active. Surely there must be an equivalent!\n\nApologies if I'm being dim but this seems like it would be a common report for people to want so must exist somewhere! \n\nThanks! ",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pyj2w6/identify_workspaces_with_no_user_connections_in_x/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwjci48",
          "author": "circalight",
          "text": "We check \"last active\" info for workspaces through our dev portal Port. Doesn't touch the limit. There's open source options (Backstage), but you'd need a big team to make that functional.",
          "score": 6,
          "created_utc": "2025-12-29 12:50:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlcn51",
          "author": "TheNaPalmer",
          "text": "deploy this and check monthly for recommendations \n\nhttps://aws.amazon.com/solutions/implementations/cost-optimizer-for-amazon-workspaces/",
          "score": 4,
          "created_utc": "2025-12-29 18:59:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwmnrxr",
              "author": "lurkerloo29",
              "text": "This.",
              "score": 1,
              "created_utc": "2025-12-29 22:50:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkflqz",
          "author": "PelosiCapitalMgmnt",
          "text": "If you have SAML authentication for your workspaces, I would just pull your logs from your IDP and see how long ago each user signed into their workspace",
          "score": 3,
          "created_utc": "2025-12-29 16:25:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwktsbj",
          "author": "-Xuby-",
          "text": "I've brute forced it, ugly and working > slick and broken\n\n\nCloudWatch ‚ÄúUserConnected‚Äù Activity Extraction ‚Äì Four‚ÄëWeek Report\n\nLog into CloudWatch.\n\nGo to the Metrics tab.\n\nSelect All Metrics.\n\nClick Browse, \nWorkspaces\nBy Workspace ID\n(might need to click all to get out of breadcrumbs) \n\nIn search box search for ‚Äúuserconnected‚Äù.\n\nAdd that metric as a filter.\n\nAt the top of the screen, set the time range to Last 4 Weeks.\n\nChange the graph view to Data Table.\n\nOpen the Graphed Metrics tab.\n\nSet Statistic to Maximum.\n\nSet Period to 1 Day.\n\nBecause the metric is binary (0/1), a 1 indicates at least one connection that day.\n\nClick Actions ‚Üí Export to CSV.\n\nDeselect the first 100 rows, move to page 2, export the next 100.\n\nRepeat until all rows are exported.\n\nCombine the CSV files.\n\nAdd your own MAX() row at the bottom to show whether each user had any login in the four‚Äëweek window.\n\nPour beer. Sob quietly.",
          "score": 3,
          "created_utc": "2025-12-29 17:32:26",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwj2rde",
          "author": "SammichAffectionate",
          "text": "We would get this data from our RMM",
          "score": 2,
          "created_utc": "2025-12-29 11:34:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj57ru",
          "author": "-Xuby-",
          "text": "Thanks guys, small org so no Claude and no CLi rights (or sadly paid for RMM) so need something in either workspaces node or cloudwatch.\n\nP",
          "score": 1,
          "created_utc": "2025-12-29 11:54:19",
          "is_submitter": true,
          "replies": [
            {
              "id": "nwl83sw",
              "author": "JohnnyMiskatonic",
              "text": "When you say 'no CLI rights' does that include Cloudshell?",
              "score": 1,
              "created_utc": "2025-12-29 18:38:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwlrnpt",
                  "author": "-Xuby-",
                  "text": "Yes sadly!",
                  "score": 1,
                  "created_utc": "2025-12-29 20:11:16",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwx633s",
          "author": "moullas",
          "text": "you can use the aws cli or boto api to access the last connected detail from each workspace. That‚Äôs what we do, packaged in a lambda that auto-deprovisions machines once inactive for 60 days",
          "score": 1,
          "created_utc": "2025-12-31 14:47:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj0f7b",
          "author": "Hartep",
          "text": "Just use Claude for a cloud shell script. Did that, you can then either export as CSV or directly stop/terminate the ws.",
          "score": 1,
          "created_utc": "2025-12-29 11:13:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1j3c1",
      "title": "Tools for bulk discovery/ diagram AWS and Azure.",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1q1j3c1/tools_for_bulk_discovery_diagram_aws_and_azure/",
      "author": "Iconically_Lost",
      "created_utc": "2026-01-01 23:51:32",
      "score": 6,
      "num_comments": 5,
      "upvote_ratio": 0.88,
      "text": "Hey are there any decent tools or scripts that can be used to do a bulk discovery of an AWS account/ Azure tenant for all the objects, the relative configurations/ logical connections (ie DNS name->NLB->TG->ECS)/ links and dump it out to a CSV. If it can do a diagram of all of this, would be a plus.\n\n  \nI did look at cloudcraft, but it only does AWS and does not export to CSV/excel, Hava was meh and cloudockit seems to be very $.\n\n  \nThe ultimate goal is to have a total export of all the objects so this could be manually analyzed for relevance in prep for migrations/audit.\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1q1j3c1/tools_for_bulk_discovery_diagram_aws_and_azure/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nx60d7y",
          "author": "Old_Cry1308",
          "text": "cloudockit is pricey but works. maybe try lucidchart, no csv though.",
          "score": 1,
          "created_utc": "2026-01-02 00:04:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx62fu7",
              "author": "Iconically_Lost",
              "text": "Lucid doesn't support Azure, no csv/excel and if I am paying for a tool. I would prefer 1 that does both (AWS+Azure and CSV).",
              "score": 1,
              "created_utc": "2026-01-02 00:16:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx66s8t",
          "author": "Veuxdo",
          "text": "[Resource explorer](https://aws.amazon.com/resourceexplorer/) can dump all of your resources in an account to CSV. This can be a *lot* because it includes IAM and default EC2 resources.\n\nWhat it doesn't do is export relations and interactions between the resources. The tools you mentioned can (ostensibly) do this, but as you noted they are either expensive, limited, or both.\n\nThat said, exporting your resources to CSV and then importing those into a diagram tool is at least half the battle. With those imported resources you can use your understanding of the system to create relevant/interesting perspectives of it. [This article](https://www.ilograph.com/blog/posts/generate-aws-diagrams-with-resource-explorer-and-ilograph/) has a few more details on what that might look like.",
          "score": 1,
          "created_utc": "2026-01-02 00:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6sunt",
              "author": "extra_specticles",
              "text": "Throw that CSV into an LLM with the awslabs diagramming MCP, and it will likely do a great job.\n\nI'll have a try with kiro-cli when I get back to work next week.",
              "score": 1,
              "created_utc": "2026-01-02 02:53:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx6yf7k",
              "author": "Iconically_Lost",
              "text": "Will have another look, but last time i looked it needed some setup and i was hoping for just something more turn key. Give it access, or use my account and get CSV. \n\nI just need the raw content, diagramming is a optional.",
              "score": 1,
              "created_utc": "2026-01-02 03:28:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzty6b",
      "title": "Is there a public AWS Health Status JSON API?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pzty6b/is_there_a_public_aws_health_status_json_api/",
      "author": "Dapper-Inspector-675",
      "created_utc": "2025-12-30 21:13:42",
      "score": 6,
      "num_comments": 21,
      "upvote_ratio": 0.75,
      "text": "Hi,\n\nSo lately I've been making all sorts of status checks via JSON API to services I rely on daily via uptime-kuma (selfhosted), which is a status monitor.\n\nSo far many popular sites had some sort of status page, which in the background scraped a json api all couple seconds, so those were pretty easy to find, some also hid in html code.\n\nBut at aws I only found this one: [https://health.aws.amazon.com/health/status](https://health.aws.amazon.com/health/status)\n\nBut I could not find any json api with some sort of summary of their uptime status, that I could use to check if AWS has an outage or not, this does not need to be detailed.\n\nI just can't believe that the big and great AWS does not have a json api for their status page?\n\nDoes anyone know if something like this exists?\n\n",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1pzty6b/is_there_a_public_aws_health_status_json_api/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwt3rdf",
          "author": "AWSSupport",
          "text": "Hi there, \n\nThis feature does exist, but it's currently only available to our Premium Support subscribers: https://go.aws/44Or9gC. All AWS customers can receive Health Dashboard updates through Amazon EventBridge: https://go.aws/45lmwuE. I've forwarded your feedback about this to our internal team for review. \n\n\\- Gee J.",
          "score": 8,
          "created_utc": "2025-12-30 22:01:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt5ihm",
              "author": "Dapper-Inspector-675",
              "text": "Wooooow, I did not expect a reply from AWS officially, and this fast!\n\nThanks a lot!",
              "score": 4,
              "created_utc": "2025-12-30 22:10:01",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwt6phd",
              "author": "Dapper-Inspector-675",
              "text": "Quick question,\n\nquting the second link:\n\"All customers can use the AWS Health Dashboard, powered by the AWS Health API. The dashboard requires no setup, and it's ready to use for authenticated AWS users\"\n\nSo this means to get any data I have to be an aws customer?\n\nI'm a homelabber and don't really have anygthing on aws nor am I customer, but I am interested in their uptime status as many other services rely on AWS.",
              "score": 3,
              "created_utc": "2025-12-30 22:15:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwt80ds",
                  "author": "nemec",
                  "text": "> So this means to get any data I have to be an aws customer?\n\nCorrect, other than the [public health dashboard](https://health.aws.amazon.com/health/status). Or ask the sites you are a customer of (who use AWS) to provide their own status updates as relate to their use of the services.\n\nYou can likely rig up a lambda acting on the free EventBridge health events which stays under the free tier, but that requires you to become a customer.",
                  "score": 2,
                  "created_utc": "2025-12-30 22:22:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwijb8",
          "author": "RecordingForward2690",
          "text": "Another tip is to open up the Developer console in your browser, activate the network trace and then go to the public health dashboard. By far the majority of dashboards of AWS consists of a static page and a bunch of JS that performs API calls to AWS to retrieve the information. So the network trace shows the actual API calls that were made. It's very rare that the HTML is generated server-side.\n\nThis has sometimes helped me figure out why my code was failing when performing an API call, while it was perfectly fine in a browser.",
          "score": 2,
          "created_utc": "2025-12-31 12:15:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx06wdy",
          "author": "onefivesix156",
          "text": "You may find the data on Updog (from datadog) helpful in some way. https://updog.ai/\n\nAs others have said, the health notifications via Event Bridge connected through a Lambda or Express Step Function for filtering and delivery to _somewhere_ is an excellent solution.  If this matters it is actually a decent way to learn some of the basics of AWS services and should be doable in the free tier.  That said, I understand not wanting to go that route.",
          "score": 2,
          "created_utc": "2026-01-01 00:28:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2l9d9",
              "author": "Dapper-Inspector-675",
              "text": "thanks!\n\nJust checked via browser console, seems like it needs an api key",
              "score": 1,
              "created_utc": "2026-01-01 12:38:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsw50c",
          "author": "PelosiCapitalMgmnt",
          "text": "AWS surfaces health alerts through eventbridge, which is the better way to handle things where you push events elsewhere. By for example sending them to an event bus which sends them to an SNS Topic or SQS queue",
          "score": 1,
          "created_utc": "2025-12-30 21:25:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwsx2vy",
              "author": "Dapper-Inspector-675",
              "text": "okay I see, thanks for the info.\n\nWhat does that mean in practice? Is there any way to scrape them using either html keyword find (works only if it's not a dynamic webapp) or via JSONata scraping.",
              "score": 0,
              "created_utc": "2025-12-30 21:30:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwt4mrv",
                  "author": "BeasleyMusic",
                  "text": "It means they provide you with an event driven way to get notified for health events so you can trigger some automation when a health event occurs. \n\nSo say there‚Äôs a health event, you‚Äôd get an event and then could trigger a message to a slack channel or something like that \n\nIMO this is the better way than scraping website or polling an API and triggering something based on the json response. \n\nAWS even has a repo full of tools for inspiration:\n\nhttps://github.com/aws/aws-health-tools",
                  "score": 1,
                  "created_utc": "2025-12-30 22:05:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwt6l6g",
                  "author": "PelosiCapitalMgmnt",
                  "text": "If you have premium support yes, but honestly the best way is to just do it in an event driven manner so health events are pushed to you and alerted that way rather than constantly grabbing events which do happen rarely.",
                  "score": 1,
                  "created_utc": "2025-12-30 22:15:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwt3pcb",
          "author": "Padresoba",
          "text": "AWS has a Health API that can be queried \nhttps://docs.aws.amazon.com/health/latest/ug/health-api.html \n\nSame as the other comment, Eventbridge might be a better solution so you can filter down Health events to your account and regarding services and regions you actually care about. Trying to scrape all of AWS Health and determine uptime is wasted effort because of how big it is",
          "score": 1,
          "created_utc": "2025-12-30 22:01:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt5ock",
              "author": "Dapper-Inspector-675",
              "text": "Sounds reasonable, I was already thinking it will be hell of an effort to filter it :D",
              "score": 1,
              "created_utc": "2025-12-30 22:10:48",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwt64nx",
              "author": "Dapper-Inspector-675",
              "text": "Though this is the one aws support meant, which was premium only, right?",
              "score": 1,
              "created_utc": "2025-12-30 22:13:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwt6taa",
                  "author": "Padresoba",
                  "text": "Yep that's right. Can you elaborate on what problem you're trying to solve? The community here can help you better then",
                  "score": 1,
                  "created_utc": "2025-12-30 22:16:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pykiyl",
      "title": "Audio AWS Learn Resources?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pykiyl/audio_aws_learn_resources/",
      "author": "wreckuiem48",
      "created_utc": "2025-12-29 11:59:55",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.78,
      "text": "What good audio resources are there for keeping up and deepening your AWS knowledge? I know there are several AWS branded podcasts, but to be honest I dont find these hosts particularily engaging. \n\n  \nVisual tools obviously are very helpful when learning, I just have a lot of time where my eyes and hands are busy that I would like to utilize. \n\n  \nThanks!",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pykiyl/audio_aws_learn_resources/",
      "domain": "self.aws",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pzp5sn",
      "title": "Policy as JSON (A Rego alternative idea)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pzp5sn/policy_as_json_a_rego_alternative_idea/",
      "author": "Yersyas",
      "created_utc": "2025-12-30 18:08:14",
      "score": 4,
      "num_comments": 9,
      "upvote_ratio": 0.83,
      "text": "I have came across many posts talking about OPA Rego being to complicated and overkill for policies. So I'm thinking to build a cli or GitHub Actions tool to integrate a self-defined \\`policy.json\\` file which can scan through your .tf file whether it passes the policy.\n\nHere is one of the examples I'm thinking right now for the \\`policy.json\\`.\n\n**Block public S3 buckets**\n\n    {\n      \"id\": \"s3_no_public\",\n      \"description\": \"Block creation of public S3 buckets\",\n      \"effect\": \"deny\",\n      \"actions\": [\"aws:s3:CreateBucket\"],\n      \"resources\": [\"aws.s3.bucket\"],\n      \"conditions\": [{\n        \"field\": \"resource.acl\",\n        \"operator\": \"in\",\n        \"value\": [\"public-read\", \"public-read-write\"]\n      }]\n    }\n\nWould like to hear your feedback. Thanks!",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pzp5sn/policy_as_json_a_rego_alternative_idea/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwtyl54",
          "author": "chalbersma",
          "text": "I think the issue is that eventually you're going to end up with a Schema as complex as Rego. Also [relevant xkcd](https://xkcd.com/927/).",
          "score": 3,
          "created_utc": "2025-12-31 00:44:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwu0q3m",
              "author": "Yersyas",
              "text": "What if I switch to Kyverno-like YAML style file? Where security scanner can detect it as well.",
              "score": 1,
              "created_utc": "2025-12-31 00:56:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwu19mp",
                  "author": "chalbersma",
                  "text": "That's definitely better. But you still probably don't want to design your own schema. YAML and JSON are interchangeable. So it's not neccessarily a JSON vs. YAML thing either.",
                  "score": 1,
                  "created_utc": "2025-12-31 00:59:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsebln",
          "author": "pausethelogic",
          "text": "I‚Äôm confused. What‚Äôs the purpose of this?",
          "score": 2,
          "created_utc": "2025-12-30 20:00:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtpodu",
              "author": "Yersyas",
              "text": "To replace Rego",
              "score": 1,
              "created_utc": "2025-12-30 23:56:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwymty",
                  "author": "pausethelogic",
                  "text": "But AWS policies don‚Äôt use rego?",
                  "score": 1,
                  "created_utc": "2025-12-31 14:04:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwy65rt",
                  "author": "shisnotbash",
                  "text": "Rego is actually a language. What you describe is a schema. You still have to parse that schema somehow. Writing application logic to parse it would leave a lot to be desired, both in performance, complexity and security. This is one reason why Rego is an actual language and not just a markdown. So, in the end, you will end up with another Rego-like language, just with your own abstraction on top for inputs.",
                  "score": 1,
                  "created_utc": "2025-12-31 17:48:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nws2tdq",
          "author": "Iliketrucks2",
          "text": "Following!",
          "score": 1,
          "created_utc": "2025-12-30 19:05:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy4b0e",
          "author": "shisnotbash",
          "text": "There are tools, such as Checkov, that already do this.  Tfsec (part of Trivvy now) also supports YAML and JSON. \n\nHonestly I would write a lexer/parser in an easy language like Python (using Sly) before I would try implementing it as JSON. I love what Checkov gives me but writing complex logic as YAML is freaking horrible IMO.\n\nEdit: I‚Äôm not sure that anyone complaining about rego complexity is going to do any better with a complex schema based config. Basic use of rego is really simple IMHO, users don t have to implement complex statements, there‚Äôs a ton of documentation and tools and you can run the actual policy engine as an API. Food for thought.",
          "score": 1,
          "created_utc": "2025-12-31 17:39:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pylndu",
      "title": "Looking for some clarification on the new Amazon EKS ‚ÄúArgo CD capability‚Äù",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1pylndu/looking_for_some_clarification_on_the_new_amazon/",
      "author": "PiccoloSlight5907",
      "created_utc": "2025-12-29 12:58:17",
      "score": 4,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Looking for some clarification on the new Amazon EKS ‚ÄúArgo CD capability‚Äù and how its namespace support works.\n\nIn the docs, under the comparison between the EKS Argo CD capability and self-managed Argo CD, it says something like:\n\n‚ÄúNamespace support: The capability initially supports deploying applications to a single namespace, which you specify when creating the capability. Support for applications in multiple namespaces may be added in future releases.‚Äù\n\n\n\nWhat does this mean ?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1pylndu/looking_for_some_clarification_on_the_new_amazon/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nwjlhjj",
          "author": "frnzle",
          "text": "It is most likely referring to where you deploy the ArgoCd application spec. Typically its in the ArgoCd namespace https://argo-cd.readthedocs.io/en/latest/operator-manual/app-any-namespace/",
          "score": 4,
          "created_utc": "2025-12-29 13:48:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjv4zw",
              "author": "PiccoloSlight5907",
              "text": "yes that is correct, but does that mean that we cannot deploy the actual resource to a desired namespace of our choice typically argocd applications are defined in argocd namespace.",
              "score": 1,
              "created_utc": "2025-12-29 14:43:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwk5wkp",
                  "author": "frnzle",
                  "text": "No the actual resources can still be in any namespace you want",
                  "score": 1,
                  "created_utc": "2025-12-29 15:38:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwkci0l",
                  "author": "SelfDestructSep2020",
                  "text": "It just means where the App custom resource has to be placed. Its so that EKS doesn't need to  change the argocd configs that allow it to read Apps/AppSet definitions from across the cluster. Sounds like they will add that later. The App still should specify a target namespace where the actual app resources will create.",
                  "score": 1,
                  "created_utc": "2025-12-29 16:10:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q1uc1b",
      "title": "AWS Firewall FQDN filtering with suricata rules",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1q1uc1b/aws_firewall_fqdn_filtering_with_suricata_rules/",
      "author": "rozanw",
      "created_utc": "2026-01-02 09:30:52",
      "score": 3,
      "num_comments": 8,
      "upvote_ratio": 0.72,
      "text": "0\n\nHello, I've configured AWS firewall based on suricate rules, but I am having some major issues. I'm not 100% sure if I am correct, but from the CloudWatch logs it seems that some requests are either not sending the TLS\\_SNI information, or AWS firewall is not able to pick it up.\n\nAs an example, when I do a curl test on¬†[https://registry.terraform.io](https://registry.terraform.io/), I get a nice HTTP/200 response. However, when I tried to initialize Terraform, I ran into an error:\n\nhttps://preview.redd.it/cli4f0w3lwag1.png?width=860&format=png&auto=webp&s=f8fafd3ec79effe811dd8b85da1b9c5bcc90e509\n\nLooking at the CloudWatch logs, some entries don't have the TLS\\_SNI and the result is a timeout, or a drop. Bu every curl request I do has the SNI included:\n\nhttps://preview.redd.it/w355vxd5lwag1.png?width=1214&format=png&auto=webp&s=b5487b6c1e0b58f31f2ba96872e1ee30501c657a\n\nI also don't understand why some packets time out and some are outright rejected by the firewall. Perhaps this is some indicator.\n\nBelow is an example of how I configure my rules:\n\n    # Bootstrap: allow only the early packets so TLS can be inspected\n    pass tcp $HOME_NET any -> any 443 (flow:not_established,to_server; sid:7100001; rev:1;)\n    \n    # Allow ALL outbound HTTPS traffic from the VHP PRD VNET\n    alert tls $HOME_NET  any -> any 443 (msg:\"Log all outbound HTTPS from HOME_NET \"; ssl_state:client_hello; flow:to_server,established; sid:7100002; rev:2;)\n    \n    pass tls $HOME_NET  any -> any 443 (msg:\"Log all outbound HTTPS from HOME_NET \"; ssl_state:client_hello; flow:to_server,established; sid:7100003; rev:2;)\n    \n    \n\nThough the rule above could be replaced with a TCP 443 rule, some of our networks need FQDN based filtering, and for that I need the SNI. An example of the rule is below:\n\n    pass tls $ISO_NET any -> any 443 (ssl_state:client_hello; msg:\"Allow HTTPS access to *.letsencrypt.org\"; tls.sni; content:\"letsencrypt.org\"; endswith; nocase; flow: to_server; sid:6100060; rev:1;)\n    \n\nThis problem affects not only terraform, but that's an example I can easily reproduce. I have our Partners trying to reach different services, for example AWS IAM, with similar results.\n\nI would appreciate any help on this matter, as I'm struggling with this for weeks now and haven't been able to find a solution.\n\nThanks in advance.\n\nWojciech",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1q1uc1b/aws_firewall_fqdn_filtering_with_suricata_rules/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "nx8hhp4",
          "author": "RecordingForward2690",
          "text": "Getting the NWFW to check for SNI, with a default-drop configuration, is seriously hard due to the way Suricata drops packets. The TCP connection itself is setup using three packets (SYN, SYN+ACK, ACK) that are allowed when you use the \"flow\" keyword or something. That's default Suricata behaviour. But then the very first TCP data packet comes in, which is the (still plaintext) packet that tries to negotiate the TLS connection. This very first packet has to have the SNI in it. If not, Suricata now sees an established connection but has no rule to allow it. So it drops it, and any subsequent packets (that would have the SNI in it) as well.\n\nThis stuff gets even more complicated if you do not want to filter on SNI, but on stuff that's further down in the TLS negotiation, like the server certificate - I needed this for a few connections that did not have a SNI in it.\n\nI wrote a set of rules that use the x-bits to allow all HTTPS (port 443) connections to be passed until TLS negotiation is finished. That means I can add rules reliably to allow connections based on SNI, certificate and other TLS characteristics. Only when the TLS negotiation is finished and there's no rule that allows the connection to continue, will the x-bits be reset and the connection dropped.\n\nI don't have access to that ruleset right now, but if you're interested I can post them later.\n\nAnother thing that has no definitive solution yet, is TLS 1.3. With TLS 1.3 the SNI is (deliberately) obfuscated so you can't test on SNI anymore. At least, that's the very short summary of a very long and complicated discussion.",
          "score": 5,
          "created_utc": "2026-01-02 11:02:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxay5bz",
              "author": "Moresty",
              "text": "We had that issue as well (flow established but first packet after that does not have the SNI, due to fragmentation I think).\nWe used flowbits to \"tag\" the connection to avoid blocking until we have the SNI. I'm far from an expert, so idk if x-bits are better",
              "score": 1,
              "created_utc": "2026-01-02 19:14:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxbalhn",
              "author": "Larryjkl_42",
              "text": "Do you have a link to the long and complicated discussion? I had thought that the SNI could be obfuscated with TLS 1.3 but currently isn't very often due to the complication of setting up the DNS for it. But would like to confirm and/or learn more.",
              "score": 1,
              "created_utc": "2026-01-02 20:14:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxfbnzr",
              "author": "rozanw",
              "text": "Hi. If you could find some time to share your approach, I would really appreciate.\n\nI chose the Suricata way because I also had serious issues trying to get this to work via standard stateful rules. When I finally thought I cracked Suricata, because all my curls tests worked, I ran into this issue...\n\nI always want to go with a cloud native solution because I'm not a true network expert and setting up an NVA (Checkpoint, Cisco etc.) is not something I would be able to do and properly maintain.\n\nI even tried reproducing this issue in our other environment, Azure, with the usage of Azure firewall. Same terraform version, and I had zero issues getting it to work. Of course Azure Firewall does not use Suricata rules...",
              "score": 1,
              "created_utc": "2026-01-03 11:58:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8boqh",
          "author": "brile_86",
          "text": "This is what we are using (that's jinja to get a allowed-domains list and source-IPs list as input), but you can extrapolate the logic:\n\n    %{for url in i.allowed-domains}\n    pass tls [${i.source-IPs}] any -> any 443 (\n    tls.sni; \n    dotprefix; \n    content:\"${url}\"; \n    nocase; \n    endswith; \n    ssl_state:client_hello; \n    msg:\"matching ${i.source-IPs} TLS allowlisted FQDNs\" ; \n    flow:to_server, \n    established; \n    alert; \n    sid:1${format(\"%04d\", index)}${format(\"%04d\", index(i.allowed-domains, url))};\n    rev:1;)\n\nChecking your code you probably need the dotprefix (optional but recommended) and the \"established\" clause, which only considers the established connections. \n\nLet me know if that sorts it out, we've got this setup for a while and never had issues",
          "score": 2,
          "created_utc": "2026-01-02 10:08:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfazif",
              "author": "rozanw",
              "text": "Hi. I really like your approach. I'll need to try something like this. Assuming I finally get this to work...\n\nI have adjusted the code, and this is how the relevant rules look like (the first two rules were suggested by ChatGPT)\n\n    # Bootstrap: allow only the early packets so TLS can be inspected\n    pass tcp $SHAREDSERVICES_NET any -> any 443 (flow:not_established,to_server; sid:7100001; rev:1;)\n\n    # 2. PLUMBING: Allow TCP ACKs / Keep-alives (CRITICAL FIX)\n    # This allows the 3rd packet of the handshake (ACK) because it has 0 data.\n    # It does NOT allow SSH/Netcat data because they have dsize > 0.\n    pass tcp $SHAREDSERVICES_NET any -> any 443 (msg:\"Allow TCP ACK/Plumbing\"; flow:established,to_server; dsize:0; sid:7000002; rev:1;)\n\n    # Allow outbound HTTPS traffic\n    pass tls $SHAREDSERVICES_NET any -> any 443 (tls.sni; dotprefix; content:\"terraform.io\"; nocase; endswith; ssl_state:client_hello; msg:\"allowing terraform.io\"; flow:to_server, established; alert; sid:7000009; rev:1;)\n\nThe error unfortunately is still there:\n\n    [ssm-user@ip-10-221-10-19 home]$ terraform init\n    Initializing the backend...\n    Initializing provider plugins...\n    - Reusing previous version of hashicorp/aws from the dependency lock file\n    - Reusing previous version of hashicorp/local from the dependency lock file\n    ‚ï∑\n    ‚îÇ Error: Failed to query available provider packages\n    ‚îÇ\n    ‚îÇ Could not retrieve the list of available versions for provider hashicorp/aws: could not connect to registry.terraform.io: failed to request discovery document: Get \"https://registry.terraform.io/.well-known/terraform.json\": read\n    ‚îÇ tcp 10.221.10.19:60540->18.245.60.45:443: read: connection reset by peer\n    ‚îÇ\n    ‚îÇ To see which modules are currently depending on hashicorp/aws and what versions are specified, run the following command:\n    ‚îÇ     terraform providers\n\n  \nBut what I really cannot understand is that curl works like a charm:\n\n    [ssm-user@ip-10-221-10-19 home]$ curl -lI https://registry.terraform.io/.well-known/terraform.json\n    HTTP/2 200\n    content-type: application/json\n\n  \nAnd, of course, everything works fine when I use this rule:\n\n    # pass tcp $SHAREDSERVICES_NET any -> 18.245.60.0/24 443 (msg:\"ALLOW outbound TCP 443 to terraform servers\"; flow:to_server,established; sid:71000017; rev:1;)\n\nWhich is which I am ruling out any routing issues, which I have been suspecting initially due to the timeouts seen in the CloudWatch logs.\n\nSo, the struggle continues...",
              "score": 1,
              "created_utc": "2026-01-03 11:52:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx936xh",
          "author": "Jazzlike_Object_9464",
          "text": "If I remember well, you shouldn‚Äôt put the * and instead, start with the dot like ‚Äú.letsencrypt.org‚Äù. Another point is that you have to add dotprefix. If I remember well, we need to add two rules to allow the TCP handshake, one for each direction.",
          "score": 1,
          "created_utc": "2026-01-02 13:45:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxf2h1b",
          "author": "mindurbiz15",
          "text": "Have a look at this page: \nhttps://aws.github.io/aws-security-services-best-practices/guides/network-firewall/#use-custom-suricata-rules-instead-of-ui-generated-rules\n\nMake sure the default drop action is none (since the default action in the example is to drop all traffic at the end of the example) \n\nI found the above page really helpful as a getting started with nfw",
          "score": 1,
          "created_utc": "2026-01-03 10:42:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}