{
  "metadata": {
    "last_updated": "2026-03-02 09:15:17",
    "time_filter": "week",
    "subreddit": "aws",
    "total_items": 20,
    "total_comments": 87,
    "file_size_bytes": 114427
  },
  "items": [
    {
      "id": "1ri51kf",
      "title": "me-central-1 AZ mec1-az2 down due to power outage/fire",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1ri51kf/mecentral1_az_mec1az2_down_due_to_power_outagefire/",
      "author": "KJKingJ",
      "created_utc": "2026-03-01 18:35:12",
      "score": 127,
      "num_comments": 24,
      "upvote_ratio": 0.99,
      "text": "Surprised to not see any posts about this yet, but I guess it's not a heavily used region. One of me-central-1's AZs has been [down for a few hours](https://health.aws.amazon.com/health/status?eventID=arn:aws:health:me-central-1::event/MULTIPLE_SERVICES/AWS_MULTIPLE_SERVICES_OPERATIONAL_ISSUE/AWS_MULTIPLE_SERVICES_OPERATIONAL_ISSUE_5E6B8_EF2498889B5) after it was \"impacted by objects that struck the data center, creating sparks and fire\". We saw things gradually drop offline from around 12:30 UTC.\n\nNaturally, there's zero capacity in the other AZs, and we're seeing impact to some supposedly multi-AZ managed services too. Anyone else dealing with the consequences from this too?",
      "is_original_content": false,
      "link_flair_text": "general aws",
      "permalink": "https://reddit.com/r/aws/comments/1ri51kf/mecentral1_az_mec1az2_down_due_to_power_outagefire/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o83xnt1",
          "author": "i_am_voldemort",
          "text": "Is this the first time military action disrupted a major cloud provider?",
          "score": 41,
          "created_utc": "2026-03-01 20:00:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o8440av",
              "author": "Drugba",
              "text": "Confirmed, yeah, it’s the only one I can remember.\n\nIn 2024 or 2025 though some of the undersea cables in the Red Sea were cut which I think cause a partial Azure outage and rumors were that it was related to Russia, but that was never confirmed.",
              "score": 24,
              "created_utc": "2026-03-01 20:33:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o86t8z8",
                  "author": "Piyh",
                  "text": "Compute vs transmission lines is a definite escalation",
                  "score": 1,
                  "created_utc": "2026-03-02 06:37:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o8453mg",
          "author": "cloudnavig8r",
          "text": "When I was a TAM, some people would ask me “what happens if a whole region goes down”.\n\nThe reality is for a whole region to go down, it would take an act of war; and if that were to happen- people would be worried about more than just heir computer a data.\n\nBut, the more likely scenario would be something impacting (no pun intended) a single data centre or Availability Zone.\n\nA proper DR plan should allow a customer to migrate to another AZ in the same region; but 2 AZs will not have the capacity of 3 AZs- meaning it is a race to get the limited capacity.\n\nSo, having a multi-region DR for critical workloads is really prudent.  However, due to data sovereignty laws- not always possible.  \n\nThese types of events are very rare, and as much as we are technicians and like to contemplate the implications for workloads- remember there are humans involved.\n\nIf an act of war, or nature, is powerful enough to significantly impact a data center capacity- I ask: what about the people that live and work in these areas?",
          "score": 65,
          "created_utc": "2026-03-01 20:38:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o84k3fr",
              "author": "general-noob",
              "text": "Or… someone messing with DNS in us-east-1",
              "score": 23,
              "created_utc": "2026-03-01 21:55:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o84zkx2",
                  "author": "OstrichLive8440",
                  "text": "I still have flashbacks about the “enactor”",
                  "score": 4,
                  "created_utc": "2026-03-01 23:20:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o84sihq",
              "author": "NoForm5443",
              "text": "That was my spiel when I was teaching and they asked us to put DR in the syllabus. If we can't come to the school, we'll continue online, if the school systems are down, hit my Gmail account for info, and if the school and Gmail are down, who cares about the class ;)",
              "score": 8,
              "created_utc": "2026-03-01 22:41:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o84wwnz",
              "author": "KJKingJ",
              "text": "Certainly don't mean to make light of the folks living/working out there. Spending the weekend on an incident call isn't fun, but it's still a million times better than having to worry about your safety.\n\nWe design around the loss of a zone as and absolute minimum (albeit with the expectation that we'd be operating in a degraded fashion due to capacity), and in many cases we design around the loss of a region too. The latter becomes quite a bit harder when you have the combination of strict data residency and single-region countries. \n\nThe failure of the region-level routing APIs are what appear to have bitten some of the multi-AZ managed services we use. Lots of US-based AWS folks getting paged as a result, or at least that's what our TAM is claiming!",
              "score": 3,
              "created_utc": "2026-03-01 23:05:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o83lhn6",
          "author": "peanutknight1",
          "text": "damn, one of my customers was using that region for a solution we had deployed. I should probably check on them tomorrow. \n\n\nCurious, so what happens if all 3 zones are hit in a region? Complete data loss?",
          "score": 17,
          "created_utc": "2026-03-01 18:59:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o83lxhs",
              "author": "NutterzUK",
              "text": "That’s possible, sure. It’s up to you to decide how much resilience you need. Multi region is an expensive but available option.",
              "score": 17,
              "created_utc": "2026-03-01 19:02:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o83s2j7",
              "author": "wlonkly",
              "text": "wellllll that depends on what hits it.\n\nBut Amazon isn't doing anything to move your data into another region just in case they permanently lose all AZs in the region, that's on you.",
              "score": 12,
              "created_utc": "2026-03-01 19:32:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o83okrg",
              "author": "Complex86",
              "text": "that would probably depends on if hit or totally destroyed. I don't think this has happened before, hopefully for the customers impacted there is no lasting impact.",
              "score": 9,
              "created_utc": "2026-03-01 19:14:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o84npki",
              "author": "profmonocle",
              "text": "A big reason these smaller regions exist is so customers can ensure their data is staying within a given country. If they automatically backed up to another region, that would defeat the purpose. (considering there isn't another region in the UAE)\n\nSo I would say customers would have a bad time if the other AZs go down.",
              "score": 4,
              "created_utc": "2026-03-01 22:15:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o842l46",
          "author": "EroeNarrante",
          "text": "\"struck by an object creating sparks and fire\" just smacks of the times when they described total ec2 outages as \"elevated error rates\"...",
          "score": 17,
          "created_utc": "2026-03-01 20:25:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85bp7o",
          "author": "powderp",
          "text": "I can't imagine getting paged for \"AZ disruption due to missile strikes\".",
          "score": 10,
          "created_utc": "2026-03-02 00:31:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o86n1rl",
              "author": "TackleInfinite1728",
              "text": "that was me this morning - surreal",
              "score": 3,
              "created_utc": "2026-03-02 05:45:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o84e1bb",
          "author": "Miasodasto13",
          "text": "Reading the status page mentioning objects hitting the dc... feels surreal..",
          "score": 5,
          "created_utc": "2026-03-01 21:24:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o84hfcs",
          "author": "PutinIsASheethole",
          "text": "we lost a few instances, took forever to try and relaunch in other AZ. I would imagine the other 2 AZ are now at capacity. Copying RDS backups to another region to be safe.  \nOne thing that took a while to realise was load balancers. uncheck the affected subnets that are in AZ2\n\nEdit - another AZ is now down. this is crazy.",
          "score": 6,
          "created_utc": "2026-03-01 21:42:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o83h42m",
          "author": "tfn105",
          "text": "Niche region I guess",
          "score": 12,
          "created_utc": "2026-03-01 18:39:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o83omb2",
              "author": "SpecialistMode3131",
              "text": "Given the location, it might just have gotten blown up.",
              "score": 28,
              "created_utc": "2026-03-01 19:15:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o85g3w3",
          "author": "playahate",
          "text": "Impacted by objects that hit the data center, sounds like a missile strike with recent events.",
          "score": 3,
          "created_utc": "2026-03-02 00:57:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o86mv8a",
          "author": "TackleInfinite1728",
          "text": "can confirm",
          "score": 1,
          "created_utc": "2026-03-02 05:43:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rihu1d",
      "title": "Amazon's cloud unit reports fire after objects hit UAE data center",
      "subreddit": "aws",
      "url": "https://www.reuters.com/world/middle-east/amazons-cloud-unit-reports-fire-after-objects-hit-uae-data-center-2026-03-01/",
      "author": "Prickly_Brain",
      "created_utc": "2026-03-02 03:22:15",
      "score": 90,
      "num_comments": 11,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rihu1d/amazons_cloud_unit_reports_fire_after_objects_hit/",
      "domain": "reuters.com",
      "is_self": false,
      "comments": [
        {
          "id": "o8641lk",
          "author": "teo-tsirpanis",
          "text": "\"objects\"",
          "score": 37,
          "created_utc": "2026-03-02 03:26:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o8663zr",
              "author": "EngineeringExpress79",
              "text": "Technically a ballistic missiles is an object. ",
              "score": 23,
              "created_utc": "2026-03-02 03:40:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o868hws",
                  "author": "water_bottle_goggles",
                  "text": "S3 Object",
                  "score": 22,
                  "created_utc": "2026-03-02 03:56:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o86bcv4",
          "author": "kingslayerer",
          "text": "Oh no. Not my precious.",
          "score": 8,
          "created_utc": "2026-03-02 04:16:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o86ijlz",
          "author": "nocommentsno",
          "text": "Pen test",
          "score": 7,
          "created_utc": "2026-03-02 05:09:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o86n2n0",
          "author": "moebaca",
          "text": "This is going to become a new meme in the AWS community, isn't it?",
          "score": 4,
          "created_utc": "2026-03-02 05:45:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o872wy7",
          "author": "PutinIsASheethole",
          "text": "second AZ now down. AWS say \"For customers that can, we recommend failing away to another AWS Region at this time\"",
          "score": 2,
          "created_utc": "2026-03-02 08:07:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o874wso",
              "author": "boffbowsh",
              "text": "But probably not to \\`me-south-1\\` which also has one AZ down now",
              "score": 1,
              "created_utc": "2026-03-02 08:26:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdff6p",
      "title": "Price increase at AWS?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rdff6p/price_increase_at_aws/",
      "author": "servermeta_net",
      "created_utc": "2026-02-24 12:52:15",
      "score": 26,
      "num_comments": 19,
      "upvote_ratio": 0.81,
      "text": "Recently many non hyperscaler providers I use (Hetzner, OVH) increased their prices due to the supply issues we all know. Do you think AWS and other hyperscalers will follow through, or will they shield their customers from the hardware market fluctuations? ",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rdff6p/price_increase_at_aws/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o74p0sd",
          "author": "PaintDrinkingPete",
          "text": "AWS likely has an advantage of already having a massive infrastructure and having priority with manufacturers...and already charging more for their services than many of the smaller budget VPS and cloud computing services providers operating on tighter margins, so they don't have to be as reactionary to the market in cases like this. \n\neventually though, if things keep trending as they are... yes, prices will have to go up.",
          "score": 51,
          "created_utc": "2026-02-24 13:11:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jui2i",
              "author": "cranberrie_sauce",
              "text": "even with increased prices - comparable hetzner/ovh compute is 60-70% cheaper than aws.  \n\naws increased prices long in advance of anticipated shortages.",
              "score": 2,
              "created_utc": "2026-02-26 17:42:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74uxv9",
          "author": "criminalsunrise",
          "text": "AWS tend not to do price rises - I believe it's one of their core principles. What they will do is have new service levels (instance types) that are more expensive and slowly sunset the old ones. We may see that accelerated a bit with the supply crunch.",
          "score": 44,
          "created_utc": "2026-02-24 13:44:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o750bw8",
              "author": "RickySpanishLives",
              "text": "Highly likely. The systems already in the field are just being amortized out. No real point increasing their price because they are already there. New instance types will incur an increased cost since they will cost more and have a BOM with more expensive parts.",
              "score": 13,
              "created_utc": "2026-02-24 14:13:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74mnrx",
          "author": "Old_Cry1308",
          "text": "aws will probably absorb some of it short-term. long-term, they'll pass it on. they’re not running a charity.",
          "score": 39,
          "created_utc": "2026-02-24 12:56:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74nwec",
              "author": "moduspol",
              "text": "They might have bigger longer term deals on hardware like this and haven’t had to eat any higher costs yet.",
              "score": 14,
              "created_utc": "2026-02-24 13:04:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7712h9",
          "author": "FlatCondition6222",
          "text": "Less spot capacity, for example, is also a \"way\" to raise prices.",
          "score": 5,
          "created_utc": "2026-02-24 19:48:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7q5huq",
              "author": "classicrock40",
              "text": "only if you don't believe spot=excess capacity. ",
              "score": 1,
              "created_utc": "2026-02-27 16:37:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74pe0u",
          "author": "Negative-Cook-5958",
          "text": "There is already about a 5% increase with each EC2 generation change (r6i => r7i => r8i for example), they won't increase pricing for existing SKUs, just make the newer ones expensive when they are available.",
          "score": 9,
          "created_utc": "2026-02-24 13:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74vbkc",
              "author": "davewritescode",
              "text": "The thing is you should be using less compute as new generations usually perform better",
              "score": 12,
              "created_utc": "2026-02-24 13:46:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o750u03",
                  "author": "RickySpanishLives",
                  "text": "It's really the instance shape that determines if that is possible. Sometimes the compute is indeed a better performer but the instance shape gives you more than you need at an increased price requiring you to binpack your computer to cover it. Less a concern if you breathe EKS/ECS - but an issue if you use raw EC2.",
                  "score": 4,
                  "created_utc": "2026-02-24 14:16:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74ps5y",
          "author": "JonnyBravoII",
          "text": "Yes. Many of their prices have been the same for years (EBS, lambda, data transfer) and are obviously cash cows. On the EC2 front, prices started going up after the 6 series.  The t series, while wildly popular, are clearly being sunset as the t4g is 6 years.old at this point. I would expect storage costs will be the first to go up.",
          "score": 6,
          "created_utc": "2026-02-24 13:15:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78ssws",
          "author": "Complex86",
          "text": "I think AWS will pass this on in future generations of instance family or other services they may provide down the line.",
          "score": 2,
          "created_utc": "2026-02-25 01:05:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ousiy",
          "author": "nucleustt",
          "text": "Meanwhile, a jackass on my feed keeps posting AI slop images of him riding capybaras.\n\nI'm clenching my fists, saying, this is why RAM prices are increasing.",
          "score": 2,
          "created_utc": "2026-02-27 12:28:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74u5uo",
          "author": "d70",
          "text": "Who is keeping track? https://aws.amazon.com/blogs/aws/category/price-reduction/\nThat said, I agree that some new instance types are slightly more expensive than older ones but they are different products, no?",
          "score": 2,
          "created_utc": "2026-02-24 13:40:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74yw4o",
          "author": "Shington501",
          "text": "Everything is going up",
          "score": 1,
          "created_utc": "2026-02-24 14:06:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ricsvf",
      "title": "Visualizing VPC Flow Logs",
      "subreddit": "aws",
      "url": "https://github.com/jbhoorasingh/aws-vpc-flow-logs-visualizer",
      "author": "EyeCodeAtNight",
      "created_utc": "2026-03-01 23:33:51",
      "score": 25,
      "num_comments": 11,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "technical resource",
      "permalink": "https://reddit.com/r/aws/comments/1ricsvf/visualizing_vpc_flow_logs/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o854qd6",
          "author": "kopi-luwak123",
          "text": "I am very interested in this, currently looking at building something similar by loading the data into a graph DB. I will try to deploy this and check. \n\nHow are you handling the naming of things ? Not every IP will have a name, and some IPs will keep changing. \n\nAnd how much of data it can potentially handle with an external database with reasonable performance?  My environment generates around 2TB of flowlogs daily. \n\nFrom a quick look, I didn't see how to load the actual flowlogs data from s3 (I could be wrong )",
          "score": 3,
          "created_utc": "2026-03-01 23:51:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o8554r4",
              "author": "EyeCodeAtNight",
              "text": "Sorry - I still had the repo private. I wanted to ensure I didn’t have any keys or secrets in there :) it’s public now.",
              "score": 1,
              "created_utc": "2026-03-01 23:53:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o855v16",
                  "author": "kopi-luwak123",
                  "text": "I can see now and edited to add few comments",
                  "score": 1,
                  "created_utc": "2026-03-01 23:57:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o85g2az",
          "author": "SPBLuke",
          "text": "This is cool, will give it a go and report back",
          "score": 1,
          "created_utc": "2026-03-02 00:57:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o85h3c8",
              "author": "EyeCodeAtNight",
              "text": "Thank you, happy cake day!",
              "score": 1,
              "created_utc": "2026-03-02 01:03:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o85j3oa",
          "author": "HarbingerXXIV",
          "text": "I had totally thought about creating something like this. Glad someone else did it first, will give it a try!",
          "score": 1,
          "created_utc": "2026-03-02 01:16:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfbpnj",
      "title": "Bypassing SCP Enforcement with Long-Lived API Keys in Bedrock",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rfbpnj/bypassing_scp_enforcement_with_longlived_api_keys/",
      "author": "SonraiSecurity",
      "created_utc": "2026-02-26 14:33:21",
      "score": 22,
      "num_comments": 1,
      "upvote_ratio": 0.89,
      "text": "I wanted to share a finding regarding an SCP (Service Control Policy) bypass I discovered in Amazon Bedrock. For those of us using SCPs as the sort of final guardrail in a multi-account setup, this was a surprising edge case where a specific type of credential completely ignored SCP \"Deny\" statements.\n\nMost of us interact with Bedrock via standard IAM users/roles. However, Bedrock also supports Short-Term and Long-Term API Keys. Long-term Bedrock API keys are actually backed by Service Specific Credentials - an ad-hoc authentication mechanism also used in AWS CodeCommit and Amazon Keyspaces.\n\n**The Vulnerability: SCP Bypass**\n\nWhen using permissions in the bedrock IAM namespace, SCPs are properly enforced no matter the authentication mechanism. When testing permissions in the bedrock-mantle namespace however, I found a discrepancy in how Bedrock evaluated these three credential types against Organization-level policies:\n\n1. **SigV4 (IAM Authentication) & Short-term keys:** Behave as expected. If an SCP denies bedrock-mantle:CreateInference, the creation of an inference is blocked.\n2. **Long-term keys (Service Specific Credentials):** These were able to bypass SCP \"Deny\" statements, and actions like creating inferences were still allowed even if the actions were blocked.\n\nHow I set this up:\n\n* I applied an SCP to a member account that explicitly denied `bedrock-mantle:*` to all users.\n* As an IAM user in that member account, I generated a Service Specific Credential for Bedrock.\n* When using that credential with the Bedrock Mantle API, the SCP was ignored, and I was able to perform inferences despite the global deny.\n\nThis issue was common to all bedrock-mantle permissions.\n\nThis effectively allowed a \"self-bypass\" of organizational governance. If a security team used an SCP to prevent the use of specific model families or to enforce a region-lock on AI workloads, a developer with `iam:CreateServiceSpecificCredential` permissions could bypass those restrictions entirely by generating and using a long-lived key.\n\n**Disclosure and Current Status**\n\nI reported this to the AWS Security Team. They validated the finding and have since deployed a fix. SCPs are now correctly enforced for bedrock-mantle requests made using Service Specific Credentials.\n\nIf you are currently managing Bedrock permissions, it's worth auditing who has the ability to create ServiceSpecificCredentials and ensuring your IAM policies (not just your SCPs) are as tight as possible.\n\nIs anyone else leveraging long-term API keys in bedrock? They are a bit of an outlier compared to the standard IAM/STS flow, so I'd be curious to know what steps people are taking to keep them and their use secure.\n\n  \n\\-Nigel Sood, Researcher @ Sonrai Security\n\n",
      "is_original_content": false,
      "link_flair_text": "security",
      "permalink": "https://reddit.com/r/aws/comments/1rfbpnj/bypassing_scp_enforcement_with_longlived_api_keys/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7its8p",
          "author": "oneplane",
          "text": "This has pretty much always been the case. RDS Credentials also aren't influenced by IAM policies, SCP or otherwise. Same goes for SSH and RDP, or Simple AD to name some more.\n\nEdit: maybe the new-ish factor here is that they tried to normalise the audit logs as well as the policy language to make the API feel more like a 'real' AWS API? A bit like ECR when you generate non-IAM credentials for pull/push authentication.",
          "score": 7,
          "created_utc": "2026-02-26 14:51:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd7p4v",
      "title": "Cloudfront + HTTP Rest API Gateway",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rd7p4v/cloudfront_http_rest_api_gateway/",
      "author": "Alive_Opportunity_14",
      "created_utc": "2026-02-24 05:31:50",
      "score": 15,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "Cloudfront has introduced flat rate pricing with WAF and DDos protection included. I am thinking of adding cloudfront in front of my rest api gateway for benefits mentioned above. Does it make sense from an infra design perspective?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rd7p4v/cloudfront_http_rest_api_gateway/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7372db",
          "author": "Old_Cry1308",
          "text": "makes sense if you need the protection and pricing works for you, otherwise might be overkill.",
          "score": 4,
          "created_utc": "2026-02-24 05:34:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73dx94",
              "author": "Alive_Opportunity_14",
              "text": "From a pricing perspective it seems cheaper because of the flat pricing and added benefits. WAF on rest api costs money while its included in the flat pricing",
              "score": 1,
              "created_utc": "2026-02-24 06:30:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73p4sr",
          "author": "snorberhuis",
          "text": "A WAF is a layer of defense I would generally recommend for most companies. It can help you protect against automated attacks. There are very few exceptions to this recommendation.  ",
          "score": 3,
          "created_utc": "2026-02-24 08:11:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74z0gx",
          "author": "menge101",
          "text": "[Docs](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/flat-rate-pricing-plan.html) for anyone else that needs them\n\n[Pricing sheet](https://aws.amazon.com/cloudfront/pricing/) as well\n\nThere is a free tier as well as a pro tier at $15/month that seems fairly compelling.",
          "score": 3,
          "created_utc": "2026-02-24 14:06:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74fiyc",
          "author": "KayeYess",
          "text": "While AWS WAF2 can be attached directly to Amazon API Gateway, Cloudfront gives additional benefits such as distributed edge delivery, ability to use multiple origins (such as S3 for static content), caching, etc.",
          "score": 1,
          "created_utc": "2026-02-24 12:07:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7drkhi",
              "author": "vppencilsharpening",
              "text": "I'd also add that it leverages the AWS managed backbone for transport from the Edge to the Origin. So if your application is running in a single region you get AWS's team ensuring fast connections from the CloudFront edge to your application instead of relying on the public internet. \n\nIt's not going to make a huge difference, but it's not nothing. \n\nClient -> Public Internet (short distance) -> AWS CloudFront Edge (closest to the client) -> AWS Network (for most of the distance) -> Origin Application\n\nVS\n\nClient -> Public Internet (long distance) -> AWS Network (for a very short distance) -> Origin Application",
              "score": 1,
              "created_utc": "2026-02-25 19:34:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dx975",
                  "author": "KayeYess",
                  "text": "Yes ... that's a general benefit of a CDN. Client reaches CDN edge, and CDN handles the rest.",
                  "score": 1,
                  "created_utc": "2026-02-25 20:00:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o797flg",
          "author": "SilentPugz",
          "text": "Harden your security response header and content security policy for your cloudfront.  \n\nLambda edge for quick validations. Cloudfront managed functions makes some things simple \n\nDon’t forget your tls flow. Where you want to terminate. At the cloudfront , lessen the load on the api.",
          "score": 1,
          "created_utc": "2026-02-25 02:28:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f6tjp",
          "author": "TheDearlyt",
          "text": "The main tradeoff is added complexity so it’s worth it mostly when you actually plan to use WAF rules, caching, or global performance improvements, not just stack services for the sake of it.\n\nPersonally, I ended up using Gcore for a similar setup because I wanted CDN + edge protection in front of APIs without dealing with too much AWS configuration overhead. It felt simpler to manage while still giving the edge security and performance benefits.",
          "score": 1,
          "created_utc": "2026-02-25 23:43:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhopwc",
      "title": "AWS KMS best practices",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rhopwc/aws_kms_best_practices/",
      "author": "DopeyMcDouble",
      "created_utc": "2026-03-01 05:15:46",
      "score": 10,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "We currently use one CMK key that is used with all our data services such as Redshift, RDS, EBS, and Redis.\n\n  \nI know this is not the most sufficient practice but wanted our data at-rest at the very least. This is why I'm wondering if this is a good starting point to be SOC2 compliant? We are looking to break apart our CMKs for the specific AWS services into the following:\n\n* prod-redshift-cmk\n* prod-rds-cmk\n* prod-ebs-cmk\n* prod-cache-cmk\n\n  \nOr is keeping one prod-data-cmk ideal?",
      "is_original_content": false,
      "link_flair_text": "security",
      "permalink": "https://reddit.com/r/aws/comments/1rhopwc/aws_kms_best_practices/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o80a8z7",
          "author": "liverdust429",
          "text": "One CMK is fine for SOC 2. Auditors care that encryption at rest exists and you control the key; not how many keys you have.\n\n  \nSplitting is still worth it though, just not for compliance:\n\n* **blast radius.** one key goes down, everything goes down. separate keys isolate the damage\n* **key policy scoping.** one shared key means every service role gets access to everything. separate keys = actual least privilege\n* **cloudtrail readability.** all your decrypt events hitting one key ARN is useless noise. separate keys make filtering trivial\n* **rotation is less scary.** rotating one shared key means testing everything. rotating prod-redshift-cmk means testing redshift\n* **cost visibility.** KMS charges per API call. separate keys with tags and you can actually see which service is burning through decrypt calls\n* **access control later.** when you need to give one team access to redshift but not RDS, separate key policies make that easy instead of a refactoring project\n\n  \nYour proposed split is a good pattern. I'd add S3 and secrets manager if you use them. I wouldn't go per-region per-environment crazy though. `prod-redshift-cmk` good. `prod-us-east-1-redshift-primary-cmk-v2` bad.\n\nOn top of that, I would monitor CMKs/AWS keys and what goes in and out of your environment. There are tools out there that can do that out of the box.",
          "score": 23,
          "created_utc": "2026-03-01 05:36:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o82kms2",
              "author": "iamaperson3133",
              "text": "SOC2 is a framework where the company publishes confidential details about their practices to customers and then auditors assess if the company follows it's own policy. It says nothing in general about how many keys you're supposed to have or any other specifics.",
              "score": 6,
              "created_utc": "2026-03-01 16:05:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o80k8hu",
              "author": "DopeyMcDouble",
              "text": "That sounds very reasonable. Currently we just need at-rest encryption which is where I went with the `prod-data-cmk`. As we grow it would be best to break them apart. (We also transfer snapshots cross-region which entails using a CMK.)\n\nHowever, this helps very much for our SOC2 compliance thanks for the response.",
              "score": 2,
              "created_utc": "2026-03-01 07:01:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o8096yf",
          "author": "yourparadigm",
          "text": "Maintaining multiple keys is probably more hassle than it's worth. The real value from having multiple CMKs is having hyper-specific policies on them to focus access to resources like SecretsManager entries and S3 objects where you might want to is different KMS keys with different access in conjunction with other resources or IAM policies.",
          "score": 2,
          "created_utc": "2026-03-01 05:28:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80hefb",
          "author": "Key-Cricket9256",
          "text": "What about managing keys via policy vs IAM or IAM and key policy",
          "score": 1,
          "created_utc": "2026-03-01 06:36:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o80okfl",
              "author": "oalfonso",
              "text": "Both. We keep the kms policy for basic rules like “cannot be used outside this account, cannot be used for services different than xyz…” and iam policies for who uses it.",
              "score": 2,
              "created_utc": "2026-03-01 07:41:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o80w4us",
          "author": "Mammoth_Ad_7089",
          "text": "The SOC2 framing is a bit of a distraction here. Auditors will want evidence that encryption at rest exists and that key lifecycle controls are documented — a single shared CMK can satisfy that just fine. The real argument for splitting is blast radius and key policy scoping. With one prod-data-cmk, your Redis service role and your Redshift snapshot role share the same key, so any over-permissioned IAM policy in one service bleeds access into the others.\n\n\n\nSplitting by service lets you write tight key policies per workload. Your EBS key policy only grants access to the EC2 instance profile, not your Redshift cluster roles. That separation also produces cleaner audit evidence when SOC2 reviewers ask to see least-privilege controls on encrypted data, because you can point to discrete key policies scoped to specific service principals.\n\n\n\nThe cross-region snapshot piece is worth thinking through before you split, since you will need replica keys in each destination region with explicit grants back to the source account. How is access to the current CMK structured today, through key policies, IAM policies, or a mix of both?",
          "score": 1,
          "created_utc": "2026-03-01 08:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o82s3oe",
          "author": "ruibranco",
          "text": "One thing worth flagging on the cross-region snapshot point - each KMS key is region-bound, so any cross-region copy already forces a re-encrypt with a destination region key. If you share one CMK for everything, that re-encryption step touches a single key policy and any rotation event cascades across all services. Separate keys keep that blast radius scoped per service even across regions.",
          "score": 1,
          "created_utc": "2026-03-01 16:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o82w2rn",
          "author": "KayeYess",
          "text": "More keys alone necessarily doesn't mean better security. Ideally, there should be some type of vertical segmentation (by tenant, by product, etc). We go with one key per tenant.",
          "score": 1,
          "created_utc": "2026-03-01 17:00:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rey0g0",
      "title": "Confused about how to set up a lambda in a private subnet that should receive events from SQS",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rey0g0/confused_about_how_to_set_up_a_lambda_in_a/",
      "author": "Slight_Scarcity321",
      "created_utc": "2026-02-26 02:31:11",
      "score": 9,
      "num_comments": 34,
      "upvote_ratio": 1.0,
      "text": "In CDK, I've set up a VPC with a public and private with egress subnets.  A private security group allows traffic from the same security group and HTTP traffic from the VPC's CIDR block. I have Postgres running in RDS Aurora in this VPC in the private security group.\n\nI have a lambda that lives in this private security group and is supposed to consume messages from an SQS queue and then write directly to the DB.  However, SQS queue messages aren't reaching the lambda.  I am getting some contradictory answers when I try to google how to do this, so I wanted to see what I need to do.\n\nThe SQS queue set up is very basic:\n\n```\nconst sourceQueue = new sqs.Queue(this, \"sourceQueue\");\n```\n\nThe lambda looks like this\n\n```\n        const myLambda = new NodejsFunction(\n            this,\n            \"myLambda\",\n            {\n                entry: \"path/to/index.js\", \n                handler: \"handler\", \n                runtime: lambda.Runtime.NODEJS_22_X, \n                vpc,\n                securityGroups: [privateSG],\n            },\n        );\n\n        myLambda.addEventSource(\n            new SqsEventSource(sourceQueue),\n        );\n\n        // policies to allow access to all sqs actions\n```\n\nIs it true that I need something like this?\n```\n        const vpcEndpoint = new ec2.InterfaceVpcEndpoint(this, \"VpcEndpoint\", {\n            service: ec2.InterfaceVpcEndpointAwsService.SQS,\n            vpc,\n            securityGroups: [privateSG],\n        });\n```\nWhile it allowed messages to reach my lambda, VPC endpoint are IaaS and I am not allowed to create them directly.  What I want is to prevent just anyone from being able to create a message but allow the lambda to receive queue messages and to communicate directly (i.e. write SQL to) the DB.  I am not sure that doing it with a VPC endpoint is correct from a security standpoint (and that would of course be grounds for denying my request to create one).  What's the right move here?\n\nEDIT:\n\nThe main thing here is that there is a lambda that needs to take in some json data, write it to a db.  There are actually two lambdas which do something similar.  The first lambda handles json for a data structure that has a one-to-many relationship with a second data structure.  The first one has to be processed before the second ones can be, but these messages may appear out of order.  I am also using a dead letter queue to reprocess things that failed the first time.  \n\nI am not married to using SQS and was surprised to learn that it's public.  I had thought that someone with our account credentials (i.e. a coworker) could just invoke aws cli to send messages as he generated them.  If there's a better mechanism to do this, I would appreciate the suggestion.  I would really like to have the action take place in the private subnet.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rey0g0/confused_about_how_to_set_up_a_lambda_in_a/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7g7cso",
          "author": "clintkev251",
          "text": "You do not need any connectivity to SQS from your function in order to use an SQS event source. Just need IAM permission in your function's execution role",
          "score": 12,
          "created_utc": "2026-02-26 03:08:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g529b",
          "author": "aqyno",
          "text": "AWS is a public cloud: meaning most of its services, like S3, API Gateway, Lambda, DynamoDB, and SQS, are accessible over the public internet from anywhere.\n\nThen you’ve got VPC, which is the private side of things, used for resources like EC2 or RDS.\n\nWhen you place a Lambda inside a VPC, it basically moves from the public part into the private part, so it loses access to public services. The usual fix is adding a NAT Gateway or egress gateway so a Lambda in a private subnet can reach the internet or public AWS services. But honestly, that’s not ideal: it’s less secure, costs more, adds latency and bandwidth bottlenecks.\n\nThat’s where VPC endpoints come in. They let private resources talk to public-facing AWS services, but keep all the traffic within AWS’s own network.\n\nFor your specific use case, the only real options are a NAT Gateway, Egress Gateway, or VPC endpoint. That means you either need to set up that infrastructure (IaaS is a different thing) yourself or have it already in place.\n\nMy ideal setup would be a queue locked down with a resource policy that only allows access from a specific VPC endpoint and the Lambda’s IAM role, plus a security group that only permits traffic from the Lambda’s own security group.\n\nAnother option would be to refactor your function so it's not polling the queue in code, just let Lambda receive messages via triggers and consume the body from the event. You could still lock things down with resource policies, but keep in mind, a coworker with broad access could still override your restrictions. That’s why you want to layer in granular permissions.",
          "score": 12,
          "created_utc": "2026-02-26 02:55:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g9v4e",
              "author": "clintkev251",
              "text": "You do not need a VPC endpoint for a function to be triggered by SQS unless you also need to access the SQS service within your code",
              "score": 11,
              "created_utc": "2026-02-26 03:23:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7gaadd",
                  "author": "aqyno",
                  "text": "That's what OP asked. He’s polling the SQS, that's why VPC endpoint configuration fixed the lambda not receiving messages.",
                  "score": 1,
                  "created_utc": "2026-02-26 03:26:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7hy1x8",
          "author": "RecordingForward2690",
          "text": "I have read the other answers, and I think none of them do justice to the complexities of the question. The problem is not so much in receiving the messages from SQS, but what to do in case of failures.\n\n(TL;DR: If you rely exclusively on the SQS-Lambda trigger for fetching and deleting the messages from the queue, you don't have to provide a network path. But if you perform SQS API calls from within your Lambda code, you do.)\n\nFirst, like I said, the reception of the messages is not an issue at all. Given the right IAM policy, it's the SQS-Lambda trigger that does the ReceiveMessage call for you. This trigger has access to SQS - you don't have to provide a network path from your Lambda for this. So there's no need for a NAT, Interface Endpoint or Public IP addresses. That's what the majority of other posts also - rightly - point out.\n\nYou do need to provide a network path, somehow, to your backend database, API or whatever your code delivers the message to. That is also what the majority of other posts also - rightly - point out.\n\nHowever, once the message has been sent to your backend, the message needs to be deleted from SQS. And this is where things might get complicated. Or not. It all depends on how robust you want your code to be against failures.\n\nBy default the SQS-Lambda trigger grabs a batch of messages from SQS and feeds this to your Lambda. If the Lambda succeeds (meaning: it generates a return object of some sort, no timeout, no Exception or something else that indicated failure) then the trigger assumes that all messages were handled properly, and it will delete the messages from the queue. In this case, no explicit network path from your Lambda to SQS needs to be provided.\n\nFrom your Lambda you can also generate a return object which identifies which messages were handled successfully, and which messages failed and need to be retried. Documentation here: [https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html](https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html). In this case it's again the trigger that will perform the DeleteMessage API call for successfully handled messages, and nothing for the messages that failed. Again, no explicit network path required.\n\nHowever, there is a third method which can be used if the two scenarios above don't work for you, for some reason. Your Lambda code can also choose to perform a DeleteMessage API call from the code itself. This is not very common but could be the best solution if you are doing a lot of asynchronous work in your Lambda, and want to delete messages as soon as they are handled, never mind other messages that are still in limbo. In that particular scenario, since the API call originates from the Lambda itself (not from the trigger) you do need to provide a network path to the SQS endpoint. Public IPs, NAT, interface endpoints are just some of the many solutions for that.\n\nThe above assumes that you are using the SQS-Lambda trigger. That's a very common pattern and in most cases the right pattern to use. But recently there was another thread on here where somebody needed to poll an SQS queue and send the data to a backend that was severely rate-limited (external API). He used a different pattern, where the Lambda was called from EventBridge Scheduler, and the Lambda itself performed a (low number of) ReceiveMessage API calls, in order not to overload the backend. Again, in that case there needs to be a network path from the Lambda itself to the SQS endpoint since the API calls are in the code itself, and not handled by the trigger.",
          "score": 4,
          "created_utc": "2026-02-26 11:39:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g2b85",
          "author": "cachemonet0x0cf6619",
          "text": "what youre doing is fine but you might be thinking about it wrong. sqs is secured by iam permission so you don’t need that in a vpc. just don’t give iam permission to create messages on the queue.",
          "score": 4,
          "created_utc": "2026-02-26 02:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ik8p2",
              "author": "icky_cyclist",
              "text": "You’re overthinking it. Amazon sqs is controlled by i am no need to put it in a vpc. Just lock down the permissions properly.",
              "score": 1,
              "created_utc": "2026-02-26 14:01:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7it58n",
                  "author": "cachemonet0x0cf6619",
                  "text": "right. that bit is clear. what isn’t clear is ops subnet config. do they have the lamb in a subnet that has a nat gateway or private link",
                  "score": 1,
                  "created_utc": "2026-02-26 14:48:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7gjdwc",
              "author": "rolandofghent",
              "text": "He needs to be in the VPC to write to the Database.",
              "score": 0,
              "created_utc": "2026-02-26 04:23:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7gqt08",
                  "author": "cachemonet0x0cf6619",
                  "text": "the lambda does, yes. and i think you’re right. I’m making an assumption that the lambda is in a subnet that has either vpc private link or a nat gateway configured",
                  "score": 0,
                  "created_utc": "2026-02-26 05:15:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7g20tl",
          "author": "UltimateLmon",
          "text": "If you set up Queue with appropriate IAM policies to both the queue and encryption key, then you shouldn't have to worry about the VPC (as far as the queue is concerned).\n\n  \nIs what you want restrict access for someone pushing messages into the queue or triggering Lambda?\n\n  \nYou do want Lambda to be in the private subnet with appropriate security group if you trying to hit the database in the same subnet / connected subnet.",
          "score": 2,
          "created_utc": "2026-02-26 02:38:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g5w9m",
              "author": "aqyno",
              "text": "If your Lambda needs to hit both a database inside a VPC and a public service like SQS, it has to be in a private subnet. That’s really the only setup that works.",
              "score": 2,
              "created_utc": "2026-02-26 03:00:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7g7gfu",
                  "author": "UltimateLmon",
                  "text": "Exactly yeah. \n\n\nI'm more wondering what the OP meant by \"anyone being able to create a message\".\n\n\nIt would be question of locking down the IAM policies involved but hard to tell what the entry point the OP wants to deny.",
                  "score": 1,
                  "created_utc": "2026-02-26 03:09:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7gtsmv",
                  "author": "aplarsen",
                  "text": "Private VPC + an egress to the public internet",
                  "score": 1,
                  "created_utc": "2026-02-26 05:38:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7i8gek",
          "author": "solo964",
          "text": "Unclear what you mean by \"I am not married to using SQS and was surprised to learn that it's public\". What is it that you think is public that you're concerned about?\n\nOn \"I had thought that someone with our account credentials (i.e. a coworker) could just invoke aws cli to send messages as he generated them\", yes you can do that. What makes you think this is not possible?",
          "score": 1,
          "created_utc": "2026-02-26 12:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ia6r5",
          "author": "Wide_Commission_1595",
          "text": "So, this is a classic problem of understanding the components, where they live, how they communicate and how to secure it all.\n\nFirstly, SQS will never be \"inside\" your VPC.  It's a global service and as long as IAM permissions on the queue and on the principal trying to send a message all agree, then the queue is secure.\n\nThe lambda function, because of your event source mapping, will be invoked by the poller and this will pass the messages to you.  You can control which ones are deleted as consumed or returned to the queue in the return response.\n\nYou only need to put the lambda in a vpc if your database is also in a vpc.  If it's lambda, then that is also a global service.  If your lambda is in a vpc you need to go e it a security group with access to the database sg, and then then database sg allow ingress from the lambda sg\n\nIf that's all in place, a message in the queue will trigger the function which will write to the DB and then respond saying the messages can be deleted from the queue",
          "score": 1,
          "created_utc": "2026-02-26 13:04:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g2hsy",
          "author": "aplarsen",
          "text": "Why do you have it in a private subnet? What problem does that solve?",
          "score": 0,
          "created_utc": "2026-02-26 02:41:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g5ysr",
              "author": "aqyno",
              "text": "Connecting yo a private DB. Apparently.",
              "score": 2,
              "created_utc": "2026-02-26 03:00:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7hwn0j",
              "author": "Sirwired",
              "text": "In general, things that don't need access to the Internet shouldn't have it. It drives up cost, increases attack surface, and increases the chance of security-breaking mis-configuration.",
              "score": 1,
              "created_utc": "2026-02-26 11:27:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7g2lwl",
          "author": "Prestigious_Pace2782",
          "text": "Yeah you need a vpc endpoint in there for sqs and need to allow https between the lambda and the endpoint.",
          "score": 0,
          "created_utc": "2026-02-26 02:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g7qjn",
              "author": "clintkev251",
              "text": "No you don't. The Lambda service polls SQS, not your function",
              "score": 3,
              "created_utc": "2026-02-26 03:11:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7h5i49",
                  "author": "Prestigious_Pace2782",
                  "text": "If you are in a private vpc with no Nat gateway an are calling sqs via an sdk in your code, my experience is that you need an endpoint. It’s a pattern I use a bit.",
                  "score": 1,
                  "created_utc": "2026-02-26 07:15:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rf6j6v",
      "title": "No P5 instances available in any region?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rf6j6v/no_p5_instances_available_in_any_region/",
      "author": "peanutknight1",
      "created_utc": "2026-02-26 10:18:34",
      "score": 5,
      "num_comments": 24,
      "upvote_ratio": 1.0,
      "text": "Curious, is everyone facing the same issue? We have no service quota issue but we arent able to create any P5 type EC2 to train our models. \n\nIts a little crazy, we checked every single region, is there such a big shortage? \n\nAny recommendations on what we can do? \n\nTrainium instances are not available either! ",
      "is_original_content": false,
      "link_flair_text": "compute",
      "permalink": "https://reddit.com/r/aws/comments/1rf6j6v/no_p5_instances_available_in_any_region/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7houm5",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'compute'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+compute).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-26 10:18:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hrzdo",
          "author": "ENBD",
          "text": "You need to use capacity blocks for ML. That’s probably the only realistic way to get access to these instances in us-east-1. https://aws.amazon.com/ec2/capacityblocks/\n\nTalk to your TAM also. They may be able to come up with some other options.",
          "score": 20,
          "created_utc": "2026-02-26 10:47:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hs6ya",
              "author": "peanutknight1",
              "text": "We dont have a TAM, how do we get one? We only have an account manager.",
              "score": 2,
              "created_utc": "2026-02-26 10:49:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hsr4k",
                  "author": "ENBD",
                  "text": "You get assigned a TAM with enterprise support. Capacity blocks are the answer here. Make sure you can launch the instance in any AZ in the region, so have subnets provisioned in all AZs.",
                  "score": 7,
                  "created_utc": "2026-02-26 10:54:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ktr4n",
                  "author": "Ok_Chocolate8661",
                  "text": "Talk to your account manager.",
                  "score": 1,
                  "created_utc": "2026-02-26 20:27:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7hr0m4",
          "author": "alapha23",
          "text": "At least trainium should be available. For p5 you need to talk to your business development manager in aws",
          "score": 4,
          "created_utc": "2026-02-26 10:38:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hrepj",
              "author": "peanutknight1",
              "text": "We did, but its not an issue of quota allocation, its an issue of capacity. They cant help if there are no P5 servers available.\n\nReally difficult right now, training on g6/ g5 is taking 21hrs for one run.\n\nedit:yes, trn type instances are not available either\n\nMeaning if you go to launch the instance it will say insufficient capacity",
              "score": 1,
              "created_utc": "2026-02-26 10:42:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hwliz",
                  "author": "alapha23",
                  "text": "also tell them to talk to SSO to put you on the waitinglist",
                  "score": 2,
                  "created_utc": "2026-02-26 11:27:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7hwi5v",
                  "author": "alapha23",
                  "text": "Tell them to use baywatch to check which region has availabilities",
                  "score": 2,
                  "created_utc": "2026-02-26 11:26:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7hwpcp",
                  "author": "alapha23",
                  "text": "Also, write a script to race for it, that works surprisingly well as well",
                  "score": 1,
                  "created_utc": "2026-02-26 11:28:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ll55k",
                  "author": "Flakmaster92",
                  "text": "A single G5? Why not do distributed training?",
                  "score": 1,
                  "created_utc": "2026-02-26 22:39:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7s3oim",
                  "author": "The_Tree_Branch",
                  "text": "P5's are only available at scale for On Demand capacity in 4 regions (see this blog: https://aws.amazon.com/blogs/aws/announcing-up-to-45-price-reduction-for-amazon-ec2-nvidia-gpu-accelerated-instances/):\n\n- Asia Pacific (Mumbai)\n- Asia Pacific (Tokyo)\n- Asia Pacific (Jakarta)\n- South America (São Paulo)\n\nThat said, I would expect actual demand to still outweigh available capacity and echo other recommendations to use Capacity Blocks for ML (EC2) or SageMaker Training Plans. \n\nOnly other option is to work with your account team (would be the TAM if you had Enterprise Support).",
                  "score": 1,
                  "created_utc": "2026-02-27 22:20:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ibcyb",
                  "author": "daredevil82",
                  "text": "good.  maybe this bullshit you're doing affects you as much if not more than people trying to buy hardware\n\nhttps://arstechnica.com/gadgets/2026/02/ram-now-represents-35-percent-of-bill-of-materials-for-hp-pcs/\n\n>I pulled the receipt for 48GB of DDR5 UDIMMs I bought in mid-2024 for my home server. $245. The slower speed version of that same kit is in stock right now for $945.",
                  "score": -1,
                  "created_utc": "2026-02-26 13:11:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7osfi0",
          "author": "crh23",
          "text": "How long is the training run? Capacity blocks are made for this, or try spot if it's short. Also try to have flexibility on instance type",
          "score": 1,
          "created_utc": "2026-02-27 12:11:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7i5hbl",
          "author": "The_Packeteer",
          "text": "U can get them, u just have to request them and it takes time for them to be allocated",
          "score": 1,
          "created_utc": "2026-02-26 12:34:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdkgms",
      "title": "Database downtime under 5 seconds… real or marketing?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rdkgms/database_downtime_under_5_seconds_real_or/",
      "author": "shagul998",
      "created_utc": "2026-02-24 16:12:43",
      "score": 5,
      "num_comments": 20,
      "upvote_ratio": 0.62,
      "text": "AWS says new RDS Blue/Green switchovers can reduce downtime to around **5 seconds or less**.\n\n**In theory:**\n\nProduction DB (Blue)\n\n⬇\n\nClone + test (Green)\n\n⬇\n\nInstant switch\n\nBut in real systems we have:\n\n* connections\n* transactions\n* caching\n* DNS\n\nSo curious:\n\nHas anyone tried this in production?\n\nSource: [Amazon RDS Blue/Green Deployments reduces downtime to under five seconds](https://aws.amazon.com/about-aws/whats-new/2026/01/amazon-rds-blue-green-deployments-reduces-downtime/)",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rdkgms/database_downtime_under_5_seconds_real_or/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o75qohm",
          "author": "booi",
          "text": "We are able to do this in our own environment so I don’t see why you wouldn’t be able to do it in AWS.\n\nIf you use a database proxy, this could even be as low as your longest running query",
          "score": 34,
          "created_utc": "2026-02-24 16:19:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76tkeb",
              "author": "rafaturtle",
              "text": "How do you do it with a proxy? As far as I understand current blue green doesn't support db proxy but I could be wrong",
              "score": 7,
              "created_utc": "2026-02-24 19:13:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75x0vc",
          "author": "ElectricSpice",
          "text": "Not quite 5s, but I saw <30s when I migrated from Postgres 12 -> 16. I was pretty happy with that.",
          "score": 10,
          "created_utc": "2026-02-24 16:47:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7u3r9u",
              "author": "minirova",
              "text": "Looking at making that jump soon on some “maintenance mode” app DBs. Did you have to make many code changes to support the move?",
              "score": 1,
              "created_utc": "2026-02-28 05:56:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75vpk3",
          "author": "Psych76",
          "text": "In some cases it works like that, in other cases the moment of “switch over” marks the original database instance as read only and all your active connections persist there and fail on writes.  \n\nPossibly in cases where connections are not persisted this is avoided.\n\nGreat for lower envs though, where you can bounce the workloads to reconnect after switchover.",
          "score": 6,
          "created_utc": "2026-02-24 16:42:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7622c9",
          "author": "oaga_strizzi",
          "text": "Not sure if it really had been 5 seconds, but yes, it is pretty painless and quick if you do everything right. ",
          "score": 6,
          "created_utc": "2026-02-24 17:10:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76krwv",
          "author": "inphinitfx",
          "text": "Provided you design for it, yes.",
          "score": 3,
          "created_utc": "2026-02-24 18:34:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o787bss",
          "author": "if2159",
          "text": "Used this when upgrading MySQL versions and was under 30 seconds for most services. Some services did have issues with maintaining connections to the old DB, but was easily solved by bouncing the services.",
          "score": 3,
          "created_utc": "2026-02-24 23:08:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75r1wp",
          "author": "mightybob4611",
          "text": "Just be careful. You can’t rollback shit once you switch over.",
          "score": 6,
          "created_utc": "2026-02-24 16:21:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76ade7",
          "author": "cachemonet0x0cf6619",
          "text": "real. have done it. works great when it works. had to have aws on the phone for one instance but that was really early on in the release cycle for blue green",
          "score": 5,
          "created_utc": "2026-02-24 17:48:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o794gzl",
          "author": "coinclink",
          "text": "It's important to do a test first in a non-prod environment. Other than that though, it is pretty smooth. The one I did, it actually detected a problem during the blue-green switch and rolled back to blue, also with only 5s of downtime. So I'd say they have the automated process down pretty well.",
          "score": 2,
          "created_utc": "2026-02-25 02:12:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7df0yl",
          "author": "just_a_pyro",
          "text": "Yes, with serverless auroras accessed only with data API, for engine version upgrade, wasn't <5s, more like 30s to 1 min",
          "score": 2,
          "created_utc": "2026-02-25 18:37:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7duhoq",
          "author": "gooserider",
          "text": "Reliably cutting the connections and forcing DNS to update is tough but solvable on the app side. On RDS itself, we've seen 15-30s failovers.",
          "score": 2,
          "created_utc": "2026-02-25 19:47:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7fb6gv",
              "author": "IridescentKoala",
              "text": "What dns updates?",
              "score": 2,
              "created_utc": "2026-02-26 00:07:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7fdxed",
                  "author": "gooserider",
                  "text": "We run an internal hostname like db.vector which is a CNAME to the RDS endpoint. We do that to make it easy to switch the live RDS, ex rollback a snapshot or switch regions.\n\n\nBut our Java services cache the IP addresses associated with the hostname. So if you failover an RDS, you need to make sure the DNS updates.",
                  "score": 2,
                  "created_utc": "2026-02-26 00:22:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75r5h3",
          "author": "Old_Cry1308",
          "text": "never trust marketing claims. real world always adds complexity they don't account for.",
          "score": 3,
          "created_utc": "2026-02-24 16:21:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75tdav",
              "author": "rdubya",
              "text": "Not sure why you are being downvoted but for sure test with your workloads, we ran into issues with the logical replication that blue/green requires due to DDL. Ended up upgrading postgres in-place with an outage window as we ran out of time to figure out the replications issues.",
              "score": 6,
              "created_utc": "2026-02-24 16:31:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o794q8n",
              "author": "coinclink",
              "text": "I mean, yeah, you should test before doing anything, but AWS doesn't just put out marketing fluff, especially around something as core, pivotal to production product like a database. Everything in RDS is very solid. The B-G deploy will even detect failures during the switchover and roll back to blue automatically. It's really well designed.",
              "score": 1,
              "created_utc": "2026-02-25 02:13:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rctlrt",
      "title": "How Does Karpenter Handle AMI Updates via SSM Parameters? (Triggering Rollouts, Refresh Timing, Best Practices)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rctlrt/how_does_karpenter_handle_ami_updates_via_ssm/",
      "author": "LemonPartyRequiem",
      "created_utc": "2026-02-23 20:52:45",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I’m trying to configure Karpenter so a `NodePool` uses an `EC2NodeClass` whose AMI is selected via an SSM Parameter that we manage ourselves.\n\nWhat I want to achieve is an automated (and controlled) AMI rollout process:\n\n* Use a Lambda (or another AWS service, if there’s a better fit) to periodically fetch the latest AWS-recommended EKS AMI (per the AWS docs: [https://docs.aws.amazon.com/eks/latest/userguide/retrieve-ami-id.html](https://docs.aws.amazon.com/eks/latest/userguide/retrieve-ami-id.html)).\n* Write that AMI ID into *our own* SSM Parameter Store path.\n* Update the parameter used by our **test** cluster first, let it run for \\~1 week, then update the parameter used by **prod**.\n* Have Karpenter automatically pick up the new AMI from Parameter Store and perform the node replacement/upgrade based on that change.\n\nWhere I’m getting stuck is understanding how `amiSelectorTerms` works when using the `ssmParameter` option (docs I’m referencing: [https://karpenter.sh/docs/concepts/nodeclasses/#specamiselectorterms](https://karpenter.sh/docs/concepts/nodeclasses/#specamiselectorterms)):\n\n* How exactly does Karpenter resolve the AMI from an `ssmParameter` selector term?\n* When does Karpenter re-check that parameter for changes (only at node launch time, periodically, or on some internal resync)?\n* Is there a way to force Karpenter to re-resolve the parameter on a schedule or on demand?\n* What key considerations or pitfalls should I be aware of when trying to implement AMI updates this way (e.g., rollout behavior, node recycling strategy, drift, disruption, caching)?\n\nThe long-term goal is to make AMI updates as simple as updating a single SSM parameter: update test first, validate for a week, then update prod letting Karpenter handle rolling the nodes automatically.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rctlrt/how_does_karpenter_handle_ami_updates_via_ssm/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o72r5s3",
          "author": "sunra",
          "text": "Your best bet is going to be to read the source.\n\nBut my understanding is that it is the Karpenter controller itself which monitors the SSM parameter (not the nodes themselves). When the controller notices that some nodes don't match the parameter it will mark the nodes as \"drifted\", and the replacements will happen according to your node-pool disruption-budget and node-termination-grace-period.\n\nI don't know this for sure - it's my expectation based on how Karpenter handles other changes (like k8s control-plane upgrades).",
          "score": 1,
          "created_utc": "2026-02-24 03:42:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73gyvq",
          "author": "yuriy_yarosh",
          "text": "1. You'll need to enable drift detection so it'll actually resync SSM   \n[https://karpenter.sh/docs/reference/settings/#feature-gates](https://karpenter.sh/docs/reference/settings/#feature-gates)\n\n2. SSM itself is throttled [https://github.com/aws/karpenter-provider-aws/issues/5907](https://github.com/aws/karpenter-provider-aws/issues/5907)  \nResync was 5 min before contributing to CNCF (reconcil cycle period for the whole controller), but now it's hardcoded to start checking only after an hour  \n[https://github.com/kubernetes-sigs/karpenter/blob/main/designs/drift.md](https://github.com/kubernetes-sigs/karpenter/blob/main/designs/drift.md)  \n[https://github.com/kubernetes-sigs/karpenter/blob/main/pkg/controllers/nodeclaim/disruption/drift.go#L93](https://github.com/kubernetes-sigs/karpenter/blob/main/pkg/controllers/nodeclaim/disruption/drift.go#L93)  \n[https://github.com/aws/karpenter-provider-aws/blob/main/pkg/cloudprovider/cloudprovider.go#L281](https://github.com/aws/karpenter-provider-aws/blob/main/pkg/cloudprovider/cloudprovider.go#L281)  \n\n\nSpot instances require SQS interruption queue `--interruption-queue`  \n[https://karpenter.sh/docs/concepts/disruption/#interruption](https://karpenter.sh/docs/concepts/disruption/#interruption)\n\n3. No, on-demand... \n\nYeah, been there, Karpenter is all over the place so wrote a custom cluster autoscaler with a Terraform provider and Kamaji, to keep infra state consistent, synchronized, and in one place.",
          "score": 1,
          "created_utc": "2026-02-24 06:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bbcgk",
          "author": "EcstaticJellyfish225",
          "text": "Consider using TAGs for the AMI selector, you should be able to tag AWS provided (and your own) AMIs.  Then you can pre-test an AMI in your dev account, once you are happy with it, you can tag the same AMI in your prod account and it will become available for karpenter to pick up next time a new node is needed (or if using drift detection at any time your disruption budget allows).\n\nAutomating the test cycle and tagging AMIs that pass the test, is also pretty straight forward. Test in a dev account, if the AMI asses the test, by some means tag the same AMI in your prod account. (Maybe setup an SNSTopic triggering a lambda, or something similar).",
          "score": 1,
          "created_utc": "2026-02-25 12:19:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd84jb",
      "title": "Quantum-Guided Cluster Algorithms for Combinatorial Optimization",
      "subreddit": "aws",
      "url": "https://aws.amazon.com/blogs/quantum-computing/quantum-guided-cluster-algorithms-for-combinatorial-optimization/",
      "author": "donutloop",
      "created_utc": "2026-02-24 05:55:27",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1rd84jb/quantumguided_cluster_algorithms_for/",
      "domain": "aws.amazon.com",
      "is_self": false,
      "comments": [
        {
          "id": "o73zfzp",
          "author": "nucleustt",
          "text": "What in god's name is this? ELI5",
          "score": 2,
          "created_utc": "2026-02-24 09:50:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfxklh",
      "title": "Data security specialist intern?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rfxklh/data_security_specialist_intern/",
      "author": "Next-Garbage9163",
      "created_utc": "2026-02-27 05:04:11",
      "score": 5,
      "num_comments": 6,
      "upvote_ratio": 0.78,
      "text": "if there is anyone who works here, can you tell me what this position entails of. Is it just a glorified security guard? I don't want to intern how to be a security gaurd lol.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rfxklh/data_security_specialist_intern/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7nfssm",
          "author": "playahate",
          "text": "Interning at aws is a really good way to get hired on. Even if it does suck get that name on your resume to differentiate yourself.",
          "score": 2,
          "created_utc": "2026-02-27 05:11:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ng5cy",
              "author": "Next-Garbage9163",
              "text": "Yeah thats what I figured. All I have is retail experience on my resume so AWS Data Security Specialist Intern would look amazing.",
              "score": 0,
              "created_utc": "2026-02-27 05:14:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ozgjj",
          "author": "RecordingForward2690",
          "text": "Any \"security\" role in IT is something that you have to be cut out for. Security is usually assumed (just like backups) so nobody gives you a pat on the back if you do your work properly and no security issues happen. But you're first in line to get blamed if something goes wrong.\n\nHaving said that, a Data Security Specialist role can be very interesting and will take you across all the services that AWS offers, will have you talk with Operations, Developers, Legal, Marketing, Data Analysts and a whole bunch of others across the organisation/solution. You'll need to talk with them about Identity, Authentication, Authorization, technical aspects of getting access, retention times for data, making sure data can be trusted, availability, GDPR and other legislation, runbooks for data breaches and so forth. Both before a project starts, in the design phase, but also afterwards when doing audits. You'll also get intimate knowledge of the tools that are able to help you do your work as an auditor. Heck, maybe the company will also sponsor you in obtaining an \"Ethical Hacker\" certificate.\n\nA very interesting aspect of Data Security today is the proliferation of all sorts of AI, which makes it very hard to put guardrails in place to ensure data doesn't leak out via trained models and such. And AI is sometimes coming up with novel ways to breach guardrails. Plus users come up with novel ways to attach AI to data sources without authorization, causing data leaks all over the place.\n\nSo yeah, an internship in that area can be very interesting in itself, and a very good leg-up into getting into IT/AWS in general.",
          "score": 1,
          "created_utc": "2026-02-27 12:59:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o805c40",
              "author": "Ok_Mathematician6075",
              "text": "Ok grandpa",
              "score": 0,
              "created_utc": "2026-03-01 04:59:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7pb4ob",
          "author": "solo964",
          "text": "Do you mean \"data **center** security specialist intern\"? Like this [job posting](https://www.amazon.jobs/en/jobs/3189192/data-center-security-specialist-intern-us).\n\nThe role description:\n\n>As a Data Center Operational Security Specialist Intern, you will be tasked with driving operational security excellence within our Data Centers. You will write reports, create presentations and communicate with management on the status of physical security operations.",
          "score": 1,
          "created_utc": "2026-02-27 14:07:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pdutm",
              "author": "Next-Garbage9163",
              "text": "Yes",
              "score": 1,
              "created_utc": "2026-02-27 14:22:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rgy7lw",
      "title": "can i use sagemaker preprocessing together with training?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rgy7lw/can_i_use_sagemaker_preprocessing_together_with/",
      "author": "Sbadabam278",
      "created_utc": "2026-02-28 09:20:12",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hello,\n\n  \nI have a sagemaker training script. The model relies on some global statistics to be computed from the data. Right now this runs as a function before the pytorch training starts, but that's obviously suboptimal (I am paying for gpu time unnecessarily)\n\n  \nSo I was thinking of using sagemaker preprocessing for this. But can I spawn both the preprocessing job and training with the same script invocation, with the training job waiting to schedule until preprocessing is done?\n\n  \nIf I need to instead run two separate commands and wait manually anyway, then perhaps using aws batch is better ?\n\nThank you in advance!",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rgy7lw/can_i_use_sagemaker_preprocessing_together_with/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o86ye44",
          "author": "Nite_Night",
          "text": "Look into sagemaker pipelines: [https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html)\n\n",
          "score": 1,
          "created_utc": "2026-03-02 07:24:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1reb3q4",
      "title": "AWS Backup Jobs with VSS Errors",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1reb3q4/aws_backup_jobs_with_vss_errors/",
      "author": "Budget-Industry-3125",
      "created_utc": "2026-02-25 11:49:03",
      "score": 4,
      "num_comments": 4,
      "upvote_ratio": 0.84,
      "text": "Good morning guys,\n\n  \nI've set up AWS Backup Jobs for many of my EC2 Instances. \n\nThere are 20 VMs enabled for backing up their data to AWS, but somehow 9 of them are presenting the following errors:\n\nWindows VSS Backup Job Error encountered, trying for regular backup\n\nI have tried re-installing the backup agent in the vms and updating, but it doesn't seem to be working out. \n\nUpon connecting to the machines, I'm able to find some VSS providers in failed states. However, after restarting them and verifying that they are OK, the job fails again with the same error message.\n\n  \nHas anyone encountered this behaviour before?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1reb3q4/aws_backup_jobs_with_vss_errors/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7bn6tp",
          "author": "brile_86",
          "text": "  \nCheck the pre-reqs  \n[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/application-consistent-snapshots-prereqs.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/application-consistent-snapshots-prereqs.html)\n\nTLDR:\n\n* Windows Server 2016\n* .NET Framework version 4.6 or later\n* Windows PowerShell major version 3, 4, or 5 with language mode set to FullLanguage\n* AWS Tools for Windows PowerShell version [3.3.48.0](http://3.3.48.0) or later\n* IAM policy AWSEC2VssSnapshotPolicy (or equivalent permissions) attached (make sure you don't have any restrictive SCP or IAM Policy Boundaries blocking it)\n\n  \nAlso some instances are not supported as too small  \n[https://docs.aws.amazon.com/aws-backup/latest/devguide/windows-backups.html](https://docs.aws.amazon.com/aws-backup/latest/devguide/windows-backups.html)\n\n* t3.nano\n* t3.micro\n* t3a.nano\n* t3a.micro\n* t2.nano\n* t2.micro\n\n",
          "score": 3,
          "created_utc": "2026-02-25 13:33:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hhfi0",
              "author": "Budget-Industry-3125",
              "text": "already did. all instances have the same permissions. I've reduced the number of jobs with errors to 2 of them.",
              "score": 1,
              "created_utc": "2026-02-26 09:07:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7bjqdc",
          "author": "ReturnOfNogginboink",
          "text": "This is a Windows issue, not an AWS issue. The error is coming from the Windows volume snapshot service.",
          "score": 0,
          "created_utc": "2026-02-25 13:13:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cm51j",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-02-25 16:26:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7co5m1",
              "author": "gex80",
              "text": "Why would they contact Veeam support?",
              "score": 1,
              "created_utc": "2026-02-25 16:35:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rckoel",
      "title": "CACs in Workspaces",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rckoel/cacs_in_workspaces/",
      "author": "KrazyMuffin",
      "created_utc": "2026-02-23 15:36:47",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 0.84,
      "text": "Our current AWS workspace setup uses Simple AD, as I couldn't get AD Connector to work (will work on getting this working another time).\n\n\n\nCurrently a Linux workspace (Rocky Linux 8) can use CACs to authenticate to sites in-session, however, on Windows (Windows Server 2022), it doesn't recognize my computer's CAC reader. I have installed ActivID and InstallRoot, the workspace is DCV (formerly WSP).\n\n\n\nThe documentation all talks about how to setup readers with AD Connector so you can log into the workspace with your CAC, but that's not what we're trying to do, just be able to use the reader inside the instance.\n\n\n\nAny suggestions?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rckoel/cacs_in_workspaces/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6zxsei",
          "author": "fjleon",
          "text": "smart card redirection is disabled in windows by default for both presession and insession. you need to enable via GPO\n\nhttps://docs.aws.amazon.com/workspaces/latest/adminguide/group_policy.html#gp_install_template_wsp",
          "score": 2,
          "created_utc": "2026-02-23 18:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75hoqv",
              "author": "KrazyMuffin",
              "text": "Thank you, that worked, I'm dumb 😅",
              "score": 1,
              "created_utc": "2026-02-24 15:38:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rgvfbz",
      "title": "AWS DynamoDB / Copilot Studio",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rgvfbz/aws_dynamodb_copilot_studio/",
      "author": "Ja-smine",
      "created_utc": "2026-02-28 06:35:35",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.8,
      "text": "Hello,\n\nWith my colleagues we're trying to create an agent in Copilot Studio for sales teams.\n\nOur data is stored in AWS DynamoDB. We've been trying to find a way to connect to it but in vain...\n\nThe only solution that I could find was CData connector but our company won't pay for it.  At least not at this stage of the project...\n\nDo you know of any way to do it ? Or should we just give up and try to store our data elsewhere ?\n\nThanks !",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rgvfbz/aws_dynamodb_copilot_studio/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7ut47b",
          "author": "SuperScorpion",
          "text": "Making data available to sales teams sounds like you’re looking to make data available for analytical processing? One thing we recently found out is that DynamoDB does not lend itself well to those kinds of applications because it usually has to aggregate lots of data resulting in heavy scan operations.\n\nWe were able to make data available for analytics by storing it in another database as well (or Apache Iceberg in our case) . We use DynamoDB Streams to sync the updates happening to the database to the iceberg tables",
          "score": 1,
          "created_utc": "2026-02-28 09:47:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd81dy",
      "title": "Getting Started with AWS",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rd81dy/getting_started_with_aws/",
      "author": "gokuplayer17",
      "created_utc": "2026-02-24 05:50:41",
      "score": 3,
      "num_comments": 31,
      "upvote_ratio": 0.8,
      "text": "Hello! I recently got hired to work on a solar metric dashboard for a company that uses Arduinos to control their solar systems. I am using Grafana for the dashboard itself but have no way of passing on the data from the Arduino to Grafana without manually copy/pasting the CSV files the Arduino generates. To automate this, I was looking into the best system to send data to from the Arduino to Grafana, and my research brought up AWS. My coworker, who is working on the Arduino side of this, agreed.\n\nBefore getting into AWS, I wanted to confirm with people the services that would be best for me/the company. The general pipeline I saw would be Arduino -> IoT Core -> S3 -> Athena -> Grafana. Does this sound right? The company has around 100 clients, so this seemed pretty cost efficient.\n\nGrafana is hosted as a VPS through Hostinger as well. Let me know if I can provide more context!",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rd81dy/getting_started_with_aws/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o73a82e",
          "author": "therouterguy",
          "text": "How many files are generated and which datasource is used by Grafana? Anyway I would look at parsing the csv files with a Lambda when it is created. Athena can be quite expensive.",
          "score": 3,
          "created_utc": "2026-02-24 06:00:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73bt6r",
              "author": "gokuplayer17",
              "text": "The arduino pushes a CSV every minute with 10 seconds of data each. Our current set up appends this to an existing CSV on another website. That CSV on the other website is like, years of data and the one I've been using is around 60 MB but that might not be necessary? I know the long time period I really need is \"year to date\" and \"last 30 days\" data.\n\nFor the Grafana datasource, I am open to whatever works. I saw Athena is a connection and wasn't sure what specific datasource that would translate to.\n\nI did see parsing with Lambda would probably be a good thing to include, but wasn't certain.\n\nAnother thing to add is Grafana can be a bit laggy with the calculations for something like BTUs, so I wanted to know where that could be done. I think Lambda could do this too?",
              "score": 1,
              "created_utc": "2026-02-24 06:13:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o73hrqu",
                  "author": "therouterguy",
                  "text": "I am not that familiar with Athena but iirc Athena will go over all the data to get you only a small subset. As your datasource is quite small it might not be a lot but it depends on how many queries you make. I would just parse the data with Lambda and push it to some local datastore.",
                  "score": 1,
                  "created_utc": "2026-02-24 07:04:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77aemk",
                  "author": "cachemonet0x0cf6619",
                  "text": "you probably want kinesis to do the calculations but you’re starting to reach beyond the scope of a hobby app and should consider cost. \n\nhere’s an article to help you think about that:\nhttps://aws.amazon.com/blogs/iot/7-patterns-for-iot-data-ingestion-and-visualization-how-to-decide-what-works-best-for-your-use-case/",
                  "score": 1,
                  "created_utc": "2026-02-24 20:31:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73a9uy",
          "author": "Old_Cry1308",
          "text": "aws iot core is a good choice. for data storage, s3 works. athena to query. looks solid. might want to check costs though.",
          "score": 3,
          "created_utc": "2026-02-24 06:00:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73by7s",
              "author": "gokuplayer17",
              "text": "Thank you! Definitely wanna look into costs, I have played with the AWS cost calculator but without knowing exact file sizes, it's been hard to get a sure estimate. I've mainly seen around $30 monthly which isn't bad.",
              "score": 1,
              "created_utc": "2026-02-24 06:14:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73jlab",
          "author": "cycle-nerd",
          "text": "S3 + Athena, while it will technically work, do not seem like the optimal choice here. Look into specialized time series databases like Amazon Timestream for InfluxDB that are purpose-built for this type of use case.",
          "score": 2,
          "created_utc": "2026-02-24 07:20:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73oq9g",
          "author": "snorberhuis",
          "text": "AWS is a good fit if you plan to quickly grow your client base. It will help you easily scale with the number of clients. Better than a VPS.\n\nAfter IoT Core, you can process the data using Lambda functions. Be sure to build the lambdas so they can later be migrated to containers, as containers can become more cost-effective at scale.\n\nThe IoT companies I work with often store large amounts of time-series data. Time Series Influx DB is a better fit for this, but it is not serverless. So I would start with S3 to keep costs down. \n\nBe sure to correctly set up your AWS Account structure. You will not yet need a VPC. But getting this right prevents future migrations.",
          "score": 2,
          "created_utc": "2026-02-24 08:08:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76g6v6",
              "author": "cachemonet0x0cf6619",
              "text": "would you consider using durable functions before containers?",
              "score": 1,
              "created_utc": "2026-02-24 18:14:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7atv3l",
                  "author": "snorberhuis",
                  "text": "Durable functions serve a different purpose than switching to containers. You could actually also use Lambda managed instances for this purpose. They also offer the ability to reduce cold starts and be more cost effective.",
                  "score": 2,
                  "created_utc": "2026-02-25 09:52:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73eqnz",
          "author": "ramdonstring",
          "text": "I would suggest to reconsider AWS for this solution.\n\nMy proposal would be to change the way the Arduinos publish data, or make them dual publish during migration, and start publishing in MQTT (as they should) to an MQTT broker. Then use https://grafana.com/grafana/plugins/grafana-mqtt-datasource/ or Loki and then Grafana.\n\nYou can install everything in the same VPS.\n\nEdit: oh the downvotes! I understand this subreddit is completely against anyone suggesting not using AWS.",
          "score": 2,
          "created_utc": "2026-02-24 06:37:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76f6by",
              "author": "cachemonet0x0cf6619",
              "text": "you don’t choose aws iot for the mqtt you chose aws iot for the certificate management.",
              "score": 1,
              "created_utc": "2026-02-24 18:09:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76gvou",
                  "author": "ramdonstring",
                  "text": "I didn't say anything about using AWS IoT for MQTT, I said don't use AWS at all. It's overkill for the problem and scale.",
                  "score": 0,
                  "created_utc": "2026-02-24 18:17:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o73q8vm",
              "author": "maxlan",
              "text": "I would agree\n\nIf you as a human can get the csv file to copy paste then some automation can get it. Saying there is no way to do that is like admitting you do not understand how computers work. \n\nAnd your solution to not understanding how computers work is an immensely complex solution that still doesn't really answer the question of how do you make it do the copy/paste job. \n\nWhat are your requirements for the solution? What are your non functional requirements? \n\nGoing into this with the information you provided is a recipe for being one of those companies who say \"we were spending 3/month on our IT solution and then we got aws and now we spend 3/minute and it doesn't provide the customer access to the data they want\"",
              "score": 1,
              "created_utc": "2026-02-24 08:22:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76fj7c",
                  "author": "cachemonet0x0cf6619",
                  "text": "anyone that’s done iot and aws knows the answers to this and if you don’t that’s probably an indication that you shouldn’t respond",
                  "score": 0,
                  "created_utc": "2026-02-24 18:11:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74tlb6",
          "author": "TheGutterBall",
          "text": "If this is the pipeline you use (seems like the correct use case) check out the 3 golden rules for using Athena with S3 to help save some money (just ask chatGPT). Reason being, based on your description you will have a lot of small files, all in CSV which will be really expensive for Athena queries. In short, try to consolidate the data into bigger files (maybe daily), set up the S3 keys to partition by date, and lastly add AWS Glue to your pipeline that can convert the CSV to Parquet (columnar) format. Will save quite a bit of money in the long run",
          "score": 1,
          "created_utc": "2026-02-24 13:37:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7u3fea",
          "author": "Soul-Ice-Phoenix",
          "text": "Unable create account",
          "score": 1,
          "created_utc": "2026-02-28 05:53:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhvbeb",
      "title": "Concepts for simple data landing zone",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rhvbeb/concepts_for_simple_data_landing_zone/",
      "author": "BarryTownCouncil",
      "created_utc": "2026-03-01 11:45:08",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I'm looking at building a customer facing server monitoring data collection service which uses a number of identical ecs tasks to receive data, filter it and relay it on to a persistent backend storage. We need a specific task to get a specific clients data by default, but data loss isn't a real concern. \n\nWe can provide customers either one of a couple of different FQDNs to say which their preferred ecs task should be, or something similar in a JWT claim. Either way we want to implement a simple failover mechanism, routing that prefers task A can fail over to task B, then C, whilst B can fail to C and then A.\n\nI can't work out if we are better off fronting this via API Gateway or an IGW to ALB. API Gateway sounds best, and then using Cloud Map with service discovery in some form, but I can't work out if that can actually provide a realistic failover scenario or not.\n\nNLB's don't appear to be any use when it's down to a non-DNS approach of preferred weighting, which ALB can do, and if we continue to walk along this path, it then seems that API Gateway is no longer doing anything that an ALB can't do anyway, so why bother with it...\n\nSo summarising the use case is along the lines of:\n\n  \n1) Client POSTs data to [a.service.com](http://a.service.com) \n\n2) AWS validates request and passes data to ecs task A\n\n2a) If A is unavailable, data should instead reach task B\n\n2b) If B is unavailable, data task C should be used\n\n  \nHow would you implement this in the most generic way? I do have the ability to customise the ecs containers. I could notionally provide a query endpoint on them which could report back which tasks should be used for which fqdn (or jwt claim) in some form. I suppose I could completely code up their service discovery registration logic in python / boto3 and simplify the external architecture a lot, but hoping to stick to the generic AWS side where possible.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rhvbeb/concepts_for_simple_data_landing_zone/",
      "domain": "self.aws",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rhjq0a",
      "title": "Monitoring EFA Performance During Distributed Training with Nsys",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rhjq0a/monitoring_efa_performance_during_distributed/",
      "author": "spiderpower02",
      "created_utc": "2026-03-01 01:10:00",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I'm currently working on analyzing EFA NCCL GIN with DeepEP and found that Nsys now supports EFA analysis, so I wrote a guide following the 2024 re:Invent slides using Megatron Bridge as an example to show how to monitor NCCL and EFA during training.\n\n[https://www.pythonsheets.com/notes/appendix/megatron-efa-monitoring.html](https://www.pythonsheets.com/notes/appendix/megatron-efa-monitoring.html)",
      "is_original_content": false,
      "link_flair_text": "ai/ml",
      "permalink": "https://reddit.com/r/aws/comments/1rhjq0a/monitoring_efa_performance_during_distributed/",
      "domain": "self.aws",
      "is_self": true,
      "comments": []
    }
  ]
}