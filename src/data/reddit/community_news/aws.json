{
  "metadata": {
    "last_updated": "2026-03-01 03:29:57",
    "time_filter": "week",
    "subreddit": "aws",
    "total_items": 20,
    "total_comments": 88,
    "file_size_bytes": 114435
  },
  "items": [
    {
      "id": "1rdff6p",
      "title": "Price increase at AWS?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rdff6p/price_increase_at_aws/",
      "author": "servermeta_net",
      "created_utc": "2026-02-24 12:52:15",
      "score": 26,
      "num_comments": 19,
      "upvote_ratio": 0.81,
      "text": "Recently many non hyperscaler providers I use (Hetzner, OVH) increased their prices due to the supply issues we all know. Do you think AWS and other hyperscalers will follow through, or will they shield their customers from the hardware market fluctuations? ",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rdff6p/price_increase_at_aws/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o74p0sd",
          "author": "PaintDrinkingPete",
          "text": "AWS likely has an advantage of already having a massive infrastructure and having priority with manufacturers...and already charging more for their services than many of the smaller budget VPS and cloud computing services providers operating on tighter margins, so they don't have to be as reactionary to the market in cases like this. \n\neventually though, if things keep trending as they are... yes, prices will have to go up.",
          "score": 49,
          "created_utc": "2026-02-24 13:11:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jui2i",
              "author": "cranberrie_sauce",
              "text": "even with increased prices - comparable hetzner/ovh compute is 60-70% cheaper than aws.  \n\naws increased prices long in advance of anticipated shortages.",
              "score": 2,
              "created_utc": "2026-02-26 17:42:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74uxv9",
          "author": "criminalsunrise",
          "text": "AWS tend not to do price rises - I believe it's one of their core principles. What they will do is have new service levels (instance types) that are more expensive and slowly sunset the old ones. We may see that accelerated a bit with the supply crunch.",
          "score": 42,
          "created_utc": "2026-02-24 13:44:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o750bw8",
              "author": "RickySpanishLives",
              "text": "Highly likely. The systems already in the field are just being amortized out. No real point increasing their price because they are already there. New instance types will incur an increased cost since they will cost more and have a BOM with more expensive parts.",
              "score": 11,
              "created_utc": "2026-02-24 14:13:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74mnrx",
          "author": "Old_Cry1308",
          "text": "aws will probably absorb some of it short-term. long-term, they'll pass it on. they’re not running a charity.",
          "score": 39,
          "created_utc": "2026-02-24 12:56:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74nwec",
              "author": "moduspol",
              "text": "They might have bigger longer term deals on hardware like this and haven’t had to eat any higher costs yet.",
              "score": 14,
              "created_utc": "2026-02-24 13:04:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7712h9",
          "author": "FlatCondition6222",
          "text": "Less spot capacity, for example, is also a \"way\" to raise prices.",
          "score": 5,
          "created_utc": "2026-02-24 19:48:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7q5huq",
              "author": "classicrock40",
              "text": "only if you don't believe spot=excess capacity. ",
              "score": 1,
              "created_utc": "2026-02-27 16:37:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o74pe0u",
          "author": "Negative-Cook-5958",
          "text": "There is already about a 5% increase with each EC2 generation change (r6i => r7i => r8i for example), they won't increase pricing for existing SKUs, just make the newer ones expensive when they are available.",
          "score": 10,
          "created_utc": "2026-02-24 13:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74vbkc",
              "author": "davewritescode",
              "text": "The thing is you should be using less compute as new generations usually perform better",
              "score": 10,
              "created_utc": "2026-02-24 13:46:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o750u03",
                  "author": "RickySpanishLives",
                  "text": "It's really the instance shape that determines if that is possible. Sometimes the compute is indeed a better performer but the instance shape gives you more than you need at an increased price requiring you to binpack your computer to cover it. Less a concern if you breathe EKS/ECS - but an issue if you use raw EC2.",
                  "score": 3,
                  "created_utc": "2026-02-24 14:16:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74ps5y",
          "author": "JonnyBravoII",
          "text": "Yes. Many of their prices have been the same for years (EBS, lambda, data transfer) and are obviously cash cows. On the EC2 front, prices started going up after the 6 series.  The t series, while wildly popular, are clearly being sunset as the t4g is 6 years.old at this point. I would expect storage costs will be the first to go up.",
          "score": 3,
          "created_utc": "2026-02-24 13:15:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74u5uo",
          "author": "d70",
          "text": "Who is keeping track? https://aws.amazon.com/blogs/aws/category/price-reduction/\nThat said, I agree that some new instance types are slightly more expensive than older ones but they are different products, no?",
          "score": 4,
          "created_utc": "2026-02-24 13:40:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78ssws",
          "author": "Complex86",
          "text": "I think AWS will pass this on in future generations of instance family or other services they may provide down the line.",
          "score": 2,
          "created_utc": "2026-02-25 01:05:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ousiy",
          "author": "nucleustt",
          "text": "Meanwhile, a jackass on my feed keeps posting AI slop images of him riding capybaras.\n\nI'm clenching my fists, saying, this is why RAM prices are increasing.",
          "score": 1,
          "created_utc": "2026-02-27 12:28:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74yw4o",
          "author": "Shington501",
          "text": "Everything is going up",
          "score": 1,
          "created_utc": "2026-02-24 14:06:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfbpnj",
      "title": "Bypassing SCP Enforcement with Long-Lived API Keys in Bedrock",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rfbpnj/bypassing_scp_enforcement_with_longlived_api_keys/",
      "author": "SonraiSecurity",
      "created_utc": "2026-02-26 14:33:21",
      "score": 22,
      "num_comments": 1,
      "upvote_ratio": 0.89,
      "text": "I wanted to share a finding regarding an SCP (Service Control Policy) bypass I discovered in Amazon Bedrock. For those of us using SCPs as the sort of final guardrail in a multi-account setup, this was a surprising edge case where a specific type of credential completely ignored SCP \"Deny\" statements.\n\nMost of us interact with Bedrock via standard IAM users/roles. However, Bedrock also supports Short-Term and Long-Term API Keys. Long-term Bedrock API keys are actually backed by Service Specific Credentials - an ad-hoc authentication mechanism also used in AWS CodeCommit and Amazon Keyspaces.\n\n**The Vulnerability: SCP Bypass**\n\nWhen using permissions in the bedrock IAM namespace, SCPs are properly enforced no matter the authentication mechanism. When testing permissions in the bedrock-mantle namespace however, I found a discrepancy in how Bedrock evaluated these three credential types against Organization-level policies:\n\n1. **SigV4 (IAM Authentication) & Short-term keys:** Behave as expected. If an SCP denies bedrock-mantle:CreateInference, the creation of an inference is blocked.\n2. **Long-term keys (Service Specific Credentials):** These were able to bypass SCP \"Deny\" statements, and actions like creating inferences were still allowed even if the actions were blocked.\n\nHow I set this up:\n\n* I applied an SCP to a member account that explicitly denied `bedrock-mantle:*` to all users.\n* As an IAM user in that member account, I generated a Service Specific Credential for Bedrock.\n* When using that credential with the Bedrock Mantle API, the SCP was ignored, and I was able to perform inferences despite the global deny.\n\nThis issue was common to all bedrock-mantle permissions.\n\nThis effectively allowed a \"self-bypass\" of organizational governance. If a security team used an SCP to prevent the use of specific model families or to enforce a region-lock on AI workloads, a developer with `iam:CreateServiceSpecificCredential` permissions could bypass those restrictions entirely by generating and using a long-lived key.\n\n**Disclosure and Current Status**\n\nI reported this to the AWS Security Team. They validated the finding and have since deployed a fix. SCPs are now correctly enforced for bedrock-mantle requests made using Service Specific Credentials.\n\nIf you are currently managing Bedrock permissions, it's worth auditing who has the ability to create ServiceSpecificCredentials and ensuring your IAM policies (not just your SCPs) are as tight as possible.\n\nIs anyone else leveraging long-term API keys in bedrock? They are a bit of an outlier compared to the standard IAM/STS flow, so I'd be curious to know what steps people are taking to keep them and their use secure.\n\n  \n\\-Nigel Sood, Researcher @ Sonrai Security\n\n",
      "is_original_content": false,
      "link_flair_text": "security",
      "permalink": "https://reddit.com/r/aws/comments/1rfbpnj/bypassing_scp_enforcement_with_longlived_api_keys/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7its8p",
          "author": "oneplane",
          "text": "This has pretty much always been the case. RDS Credentials also aren't influenced by IAM policies, SCP or otherwise. Same goes for SSH and RDP, or Simple AD to name some more.\n\nEdit: maybe the new-ish factor here is that they tried to normalise the audit logs as well as the policy language to make the API feel more like a 'real' AWS API? A bit like ECR when you generate non-IAM credentials for pull/push authentication.",
          "score": 7,
          "created_utc": "2026-02-26 14:51:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbw463",
      "title": "Podcast?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rbw463/podcast/",
      "author": "putneyj",
      "created_utc": "2026-02-22 20:10:12",
      "score": 19,
      "num_comments": 10,
      "upvote_ratio": 0.95,
      "text": "So, is the official AWS podcast no longer doing news? Anyone else that used it to get \\*most\\* of your news about new services? I’m honestly a little bummed, but it just feels like the way things are going at AWS.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rbw463/podcast/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6tzvdo",
          "author": "signsots",
          "text": "The latest episode aligns with the recent layoff announcements. Wouldn't be surprised if it was part of it unfortunately.",
          "score": 18,
          "created_utc": "2026-02-22 20:18:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u20m0",
          "author": "coyotefarmer",
          "text": "That was a big part of how I kept up with changes. Such a shame. I don't see how ending something like that can be beneficial in the long run. I listened to it for years.",
          "score": 6,
          "created_utc": "2026-02-22 20:29:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71w9ms",
          "author": "ArchangelAdrian",
          "text": "Well the host Simon Elisha announced on his LinkedIn 2 weeks ago that his time at AWS came to an end.",
          "score": 5,
          "created_utc": "2026-02-24 00:41:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71xjc9",
              "author": "putneyj",
              "text": "Damn, yeah I would imagine that’s pretty much the end then. Not even a regular conversation with Werner can save your job.",
              "score": 3,
              "created_utc": "2026-02-24 00:48:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o740qdd",
                  "author": "ArchangelAdrian",
                  "text": "Sad if you think about it.",
                  "score": 2,
                  "created_utc": "2026-02-24 10:02:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6vz1y4",
          "author": "zenmaster24",
          "text": "Are there community podcasts that do similar?",
          "score": 3,
          "created_utc": "2026-02-23 02:53:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xz12c",
              "author": "wreckuiem48",
              "text": "A cloud podcast I like is called [https://www.thecloudpod.net/](https://www.thecloudpod.net/)",
              "score": 3,
              "created_utc": "2026-02-23 12:51:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6w1zg5",
              "author": "putneyj",
              "text": "I would also like to know this",
              "score": 1,
              "created_utc": "2026-02-23 03:11:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y8fb4",
          "author": "Specific-Art-9149",
          "text": "I built a morning brief email with Claude that scans various AWS RSS feeds along with 3rd party RSS and Reddit for AWS news.   Costs me about 7 cents a day utilizing Claude API, and it is emailed daily.  I'm an SA not a developer, so pretty happy with how it turned out.",
          "score": 2,
          "created_utc": "2026-02-23 13:49:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xz8n2",
          "author": "bot403",
          "text": "Google gemeni can turn anything into a podcast episode. Use it on the main aws RSS feed :D",
          "score": -3,
          "created_utc": "2026-02-23 12:52:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd7p4v",
      "title": "Cloudfront + HTTP Rest API Gateway",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rd7p4v/cloudfront_http_rest_api_gateway/",
      "author": "Alive_Opportunity_14",
      "created_utc": "2026-02-24 05:31:50",
      "score": 14,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "Cloudfront has introduced flat rate pricing with WAF and DDos protection included. I am thinking of adding cloudfront in front of my rest api gateway for benefits mentioned above. Does it make sense from an infra design perspective?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rd7p4v/cloudfront_http_rest_api_gateway/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7372db",
          "author": "Old_Cry1308",
          "text": "makes sense if you need the protection and pricing works for you, otherwise might be overkill.",
          "score": 5,
          "created_utc": "2026-02-24 05:34:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73dx94",
              "author": "Alive_Opportunity_14",
              "text": "From a pricing perspective it seems cheaper because of the flat pricing and added benefits. WAF on rest api costs money while its included in the flat pricing",
              "score": 1,
              "created_utc": "2026-02-24 06:30:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73p4sr",
          "author": "snorberhuis",
          "text": "A WAF is a layer of defense I would generally recommend for most companies. It can help you protect against automated attacks. There are very few exceptions to this recommendation.  ",
          "score": 4,
          "created_utc": "2026-02-24 08:11:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74z0gx",
          "author": "menge101",
          "text": "[Docs](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/flat-rate-pricing-plan.html) for anyone else that needs them\n\n[Pricing sheet](https://aws.amazon.com/cloudfront/pricing/) as well\n\nThere is a free tier as well as a pro tier at $15/month that seems fairly compelling.",
          "score": 3,
          "created_utc": "2026-02-24 14:06:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74fiyc",
          "author": "KayeYess",
          "text": "While AWS WAF2 can be attached directly to Amazon API Gateway, Cloudfront gives additional benefits such as distributed edge delivery, ability to use multiple origins (such as S3 for static content), caching, etc.",
          "score": 1,
          "created_utc": "2026-02-24 12:07:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7drkhi",
              "author": "vppencilsharpening",
              "text": "I'd also add that it leverages the AWS managed backbone for transport from the Edge to the Origin. So if your application is running in a single region you get AWS's team ensuring fast connections from the CloudFront edge to your application instead of relying on the public internet. \n\nIt's not going to make a huge difference, but it's not nothing. \n\nClient -> Public Internet (short distance) -> AWS CloudFront Edge (closest to the client) -> AWS Network (for most of the distance) -> Origin Application\n\nVS\n\nClient -> Public Internet (long distance) -> AWS Network (for a very short distance) -> Origin Application",
              "score": 1,
              "created_utc": "2026-02-25 19:34:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dx975",
                  "author": "KayeYess",
                  "text": "Yes ... that's a general benefit of a CDN. Client reaches CDN edge, and CDN handles the rest.",
                  "score": 1,
                  "created_utc": "2026-02-25 20:00:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o797flg",
          "author": "SilentPugz",
          "text": "Harden your security response header and content security policy for your cloudfront.  \n\nLambda edge for quick validations. Cloudfront managed functions makes some things simple \n\nDon’t forget your tls flow. Where you want to terminate. At the cloudfront , lessen the load on the api.",
          "score": 1,
          "created_utc": "2026-02-25 02:28:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f6tjp",
          "author": "TheDearlyt",
          "text": "The main tradeoff is added complexity so it’s worth it mostly when you actually plan to use WAF rules, caching, or global performance improvements, not just stack services for the sake of it.\n\nPersonally, I ended up using Gcore for a similar setup because I wanted CDN + edge protection in front of APIs without dealing with too much AWS configuration overhead. It felt simpler to manage while still giving the edge security and performance benefits.",
          "score": 1,
          "created_utc": "2026-02-25 23:43:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rc92pf",
      "title": "CDK + CodePipeline: How do you handle existing resources when re-deploying a stack?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rc92pf/cdk_codepipeline_how_do_you_handle_existing/",
      "author": "Hungry_Assistant6753",
      "created_utc": "2026-02-23 05:36:51",
      "score": 12,
      "num_comments": 8,
      "upvote_ratio": 0.88,
      "text": "We have an AWS CDK app deployed via CodePipeline. Our stack manages DynamoDB tables, Lambda functions, S3 buckets, and SageMaker endpoints.\n\n\n\n**Background**: Early on we had to delete and re-create our CloudFormation stack a few times due to deployment issues (misconfigured IAM, bad config, etc). We intentionally kept our DynamoDB tables and S3 buckets alive by setting RemovalPolicy.RETAIN. we didn't want to lose production data just because we needed to nuke the stack.\n\n**The problem**: When we re-deploy the stack after deleting it, CloudFormation tries to CREATE the tables again but they already exist. It fails. So we added a context flag `--context import-existing-tables=true` to our cdk synth command in CodePipeline, which switches the table definitions from new dynamodb.Table(...) to dynamodb.Table.from\\_table\\_name(...). This works fine for existing tables.\n\nNow, we added a new DynamoDB table. It doesn't exist yet anywhere. But the pipeline always passes `--context import-existing-tables=true`, so CDK tries to import a table that doesn't exist yet it just creates a reference to a non-existent table. No error, no table created.\n\n**Current workaround**: We special-cased the new table to always create it regardless of the flag, and leave the old tables under the import flag. But this feels fragile every time we add a new table we have to remember to handle this manually.\n\n**The question**: How do you handle this pattern cleanly in CDK? **Is there an established pattern for \"create if not exists, import if exists\"** that works in a fully automated",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rc92pf/cdk_codepipeline_how_do_you_handle_existing/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6wmzp0",
          "author": "deviled-tux",
          "text": "you leave the code alone and have it create the stuff \n\ncdk import to import them \n\nThis does not seem like something you should be automating because this shouldn’t be happening that often \n",
          "score": 18,
          "created_utc": "2026-02-23 05:41:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6won7x",
          "author": "cachemonet0x0cf6619",
          "text": "1. split stacks by the volatility of the resources. think stateful and stateless. \n\n2. use multiple accounts to split your resources so you’re not developing against production resources. \n\n3. if you can’t split accounts add an environment to the stack name and use that to separate dev and prod. \n\n4. don’t do that auto context thing. that’s a code smell. you shouldn’t need that with the advice above. \n\n5. bonus: i would never use code pipeline. instead i’d use github actions using a self hosted runner in my aws account. way easier and that skill is transferable.",
          "score": 12,
          "created_utc": "2026-02-23 05:54:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wspth",
          "author": "DoINeedChains",
          "text": "The joys of having CDK requirements forced upon you in a heavy singleton datasource environment",
          "score": 2,
          "created_utc": "2026-02-23 06:29:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hapz4",
          "author": "International_Body44",
          "text": "Put your dynamodb in a seperate stack, you can keep it in the cdk app, just define a dynamo stack.\n\nThen when you do your delete, specify the other stacks for your delete, 'cdk destroy <stack_1> <stack_2>'\n\nThis leaves the dynamodb alone. When you want to add a new table or data you should be able to add that to just the dynamodb stack and deploy that 'cdk deploy <dynamo_stack>'\n\n\nItll look a bit like this:\n\nLib/stacks/\n\n        |- dynamodb.ts\n\n        |- otherResources.ts\n\nThen in you bin/app.ts file call the dynamodb stack after your resources stack.\n\nIf you need to pass variables from one stack to the other use parameter store, do not directely pass vars between stacks cause it will cause issues later on.",
          "score": 2,
          "created_utc": "2026-02-26 08:02:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wpxp1",
          "author": "Conscious-Title-226",
          "text": "This is why I don’t like CDK to be honest. It ties you into CloudFormation which is just painful when things like this go wrong.\n\nCF just fundamentally doesn’t provide enough support around state management. You need to engineer around its limitations and/or have immutable infrastructure.\n\nWould you be able to rename your stack resource (so it is a new CF stack with a new s3 and dynamodb resource) and then just migrate the data? That’d be faster if it’s not a lot to migrate/copy\n\nYou might need to modify your stack to allow you to give them new names, there’ll be uniquely named resources in your stack like KMS aliases, s3 bucket, iam role etc but this is a good reason to make sure all that is configureable through your stack props",
          "score": 3,
          "created_utc": "2026-02-23 06:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wmtts",
          "author": "pausethelogic",
          "text": "I’m sure there’s a way, but this is honestly one of the major downsides of cloudformation and CDK - there’s no proper state management\n\nIt’s part of why terraform is a much more popular IaC tool, it actually has state management for resources so it knows what resources were deleted, which still exist, and how to handle them",
          "score": 4,
          "created_utc": "2026-02-23 05:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wq4jx",
              "author": "cachemonet0x0cf6619",
              "text": "“can you fix my Honda’s transmission?”\n\n“nah, mate. but if you’d have gotten a bike you wouldn’t need a new transmission”\n\nthat’s how stupid your reply sounds",
              "score": -7,
              "created_utc": "2026-02-23 06:07:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7b0va0",
          "author": "iamtheconundrum",
          "text": "Naming resources explicitly will result in naming collisions. Also, you should put resources like dynamodb tables in a separate stack so you can nuke other stacks without affecting the persistent parts of your architecture.",
          "score": 1,
          "created_utc": "2026-02-25 10:55:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rey0g0",
      "title": "Confused about how to set up a lambda in a private subnet that should receive events from SQS",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rey0g0/confused_about_how_to_set_up_a_lambda_in_a/",
      "author": "Slight_Scarcity321",
      "created_utc": "2026-02-26 02:31:11",
      "score": 7,
      "num_comments": 34,
      "upvote_ratio": 0.9,
      "text": "In CDK, I've set up a VPC with a public and private with egress subnets.  A private security group allows traffic from the same security group and HTTP traffic from the VPC's CIDR block. I have Postgres running in RDS Aurora in this VPC in the private security group.\n\nI have a lambda that lives in this private security group and is supposed to consume messages from an SQS queue and then write directly to the DB.  However, SQS queue messages aren't reaching the lambda.  I am getting some contradictory answers when I try to google how to do this, so I wanted to see what I need to do.\n\nThe SQS queue set up is very basic:\n\n```\nconst sourceQueue = new sqs.Queue(this, \"sourceQueue\");\n```\n\nThe lambda looks like this\n\n```\n        const myLambda = new NodejsFunction(\n            this,\n            \"myLambda\",\n            {\n                entry: \"path/to/index.js\", \n                handler: \"handler\", \n                runtime: lambda.Runtime.NODEJS_22_X, \n                vpc,\n                securityGroups: [privateSG],\n            },\n        );\n\n        myLambda.addEventSource(\n            new SqsEventSource(sourceQueue),\n        );\n\n        // policies to allow access to all sqs actions\n```\n\nIs it true that I need something like this?\n```\n        const vpcEndpoint = new ec2.InterfaceVpcEndpoint(this, \"VpcEndpoint\", {\n            service: ec2.InterfaceVpcEndpointAwsService.SQS,\n            vpc,\n            securityGroups: [privateSG],\n        });\n```\nWhile it allowed messages to reach my lambda, VPC endpoint are IaaS and I am not allowed to create them directly.  What I want is to prevent just anyone from being able to create a message but allow the lambda to receive queue messages and to communicate directly (i.e. write SQL to) the DB.  I am not sure that doing it with a VPC endpoint is correct from a security standpoint (and that would of course be grounds for denying my request to create one).  What's the right move here?\n\nEDIT:\n\nThe main thing here is that there is a lambda that needs to take in some json data, write it to a db.  There are actually two lambdas which do something similar.  The first lambda handles json for a data structure that has a one-to-many relationship with a second data structure.  The first one has to be processed before the second ones can be, but these messages may appear out of order.  I am also using a dead letter queue to reprocess things that failed the first time.  \n\nI am not married to using SQS and was surprised to learn that it's public.  I had thought that someone with our account credentials (i.e. a coworker) could just invoke aws cli to send messages as he generated them.  If there's a better mechanism to do this, I would appreciate the suggestion.  I would really like to have the action take place in the private subnet.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rey0g0/confused_about_how_to_set_up_a_lambda_in_a/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7g7cso",
          "author": "clintkev251",
          "text": "You do not need any connectivity to SQS from your function in order to use an SQS event source. Just need IAM permission in your function's execution role",
          "score": 13,
          "created_utc": "2026-02-26 03:08:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g529b",
          "author": "aqyno",
          "text": "AWS is a public cloud: meaning most of its services, like S3, API Gateway, Lambda, DynamoDB, and SQS, are accessible over the public internet from anywhere.\n\nThen you’ve got VPC, which is the private side of things, used for resources like EC2 or RDS.\n\nWhen you place a Lambda inside a VPC, it basically moves from the public part into the private part, so it loses access to public services. The usual fix is adding a NAT Gateway or egress gateway so a Lambda in a private subnet can reach the internet or public AWS services. But honestly, that’s not ideal: it’s less secure, costs more, adds latency and bandwidth bottlenecks.\n\nThat’s where VPC endpoints come in. They let private resources talk to public-facing AWS services, but keep all the traffic within AWS’s own network.\n\nFor your specific use case, the only real options are a NAT Gateway, Egress Gateway, or VPC endpoint. That means you either need to set up that infrastructure (IaaS is a different thing) yourself or have it already in place.\n\nMy ideal setup would be a queue locked down with a resource policy that only allows access from a specific VPC endpoint and the Lambda’s IAM role, plus a security group that only permits traffic from the Lambda’s own security group.\n\nAnother option would be to refactor your function so it's not polling the queue in code, just let Lambda receive messages via triggers and consume the body from the event. You could still lock things down with resource policies, but keep in mind, a coworker with broad access could still override your restrictions. That’s why you want to layer in granular permissions.",
          "score": 12,
          "created_utc": "2026-02-26 02:55:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g9v4e",
              "author": "clintkev251",
              "text": "You do not need a VPC endpoint for a function to be triggered by SQS unless you also need to access the SQS service within your code",
              "score": 13,
              "created_utc": "2026-02-26 03:23:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7gaadd",
                  "author": "aqyno",
                  "text": "That's what OP asked. He’s polling the SQS, that's why VPC endpoint configuration fixed the lambda not receiving messages.",
                  "score": 1,
                  "created_utc": "2026-02-26 03:26:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7hy1x8",
          "author": "RecordingForward2690",
          "text": "I have read the other answers, and I think none of them do justice to the complexities of the question. The problem is not so much in receiving the messages from SQS, but what to do in case of failures.\n\n(TL;DR: If you rely exclusively on the SQS-Lambda trigger for fetching and deleting the messages from the queue, you don't have to provide a network path. But if you perform SQS API calls from within your Lambda code, you do.)\n\nFirst, like I said, the reception of the messages is not an issue at all. Given the right IAM policy, it's the SQS-Lambda trigger that does the ReceiveMessage call for you. This trigger has access to SQS - you don't have to provide a network path from your Lambda for this. So there's no need for a NAT, Interface Endpoint or Public IP addresses. That's what the majority of other posts also - rightly - point out.\n\nYou do need to provide a network path, somehow, to your backend database, API or whatever your code delivers the message to. That is also what the majority of other posts also - rightly - point out.\n\nHowever, once the message has been sent to your backend, the message needs to be deleted from SQS. And this is where things might get complicated. Or not. It all depends on how robust you want your code to be against failures.\n\nBy default the SQS-Lambda trigger grabs a batch of messages from SQS and feeds this to your Lambda. If the Lambda succeeds (meaning: it generates a return object of some sort, no timeout, no Exception or something else that indicated failure) then the trigger assumes that all messages were handled properly, and it will delete the messages from the queue. In this case, no explicit network path from your Lambda to SQS needs to be provided.\n\nFrom your Lambda you can also generate a return object which identifies which messages were handled successfully, and which messages failed and need to be retried. Documentation here: [https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html](https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html). In this case it's again the trigger that will perform the DeleteMessage API call for successfully handled messages, and nothing for the messages that failed. Again, no explicit network path required.\n\nHowever, there is a third method which can be used if the two scenarios above don't work for you, for some reason. Your Lambda code can also choose to perform a DeleteMessage API call from the code itself. This is not very common but could be the best solution if you are doing a lot of asynchronous work in your Lambda, and want to delete messages as soon as they are handled, never mind other messages that are still in limbo. In that particular scenario, since the API call originates from the Lambda itself (not from the trigger) you do need to provide a network path to the SQS endpoint. Public IPs, NAT, interface endpoints are just some of the many solutions for that.\n\nThe above assumes that you are using the SQS-Lambda trigger. That's a very common pattern and in most cases the right pattern to use. But recently there was another thread on here where somebody needed to poll an SQS queue and send the data to a backend that was severely rate-limited (external API). He used a different pattern, where the Lambda was called from EventBridge Scheduler, and the Lambda itself performed a (low number of) ReceiveMessage API calls, in order not to overload the backend. Again, in that case there needs to be a network path from the Lambda itself to the SQS endpoint since the API calls are in the code itself, and not handled by the trigger.",
          "score": 3,
          "created_utc": "2026-02-26 11:39:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g2b85",
          "author": "cachemonet0x0cf6619",
          "text": "what youre doing is fine but you might be thinking about it wrong. sqs is secured by iam permission so you don’t need that in a vpc. just don’t give iam permission to create messages on the queue.",
          "score": 4,
          "created_utc": "2026-02-26 02:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ik8p2",
              "author": "icky_cyclist",
              "text": "You’re overthinking it. Amazon sqs is controlled by i am no need to put it in a vpc. Just lock down the permissions properly.",
              "score": 1,
              "created_utc": "2026-02-26 14:01:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7it58n",
                  "author": "cachemonet0x0cf6619",
                  "text": "right. that bit is clear. what isn’t clear is ops subnet config. do they have the lamb in a subnet that has a nat gateway or private link",
                  "score": 1,
                  "created_utc": "2026-02-26 14:48:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7gjdwc",
              "author": "rolandofghent",
              "text": "He needs to be in the VPC to write to the Database.",
              "score": 0,
              "created_utc": "2026-02-26 04:23:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7gqt08",
                  "author": "cachemonet0x0cf6619",
                  "text": "the lambda does, yes. and i think you’re right. I’m making an assumption that the lambda is in a subnet that has either vpc private link or a nat gateway configured",
                  "score": 0,
                  "created_utc": "2026-02-26 05:15:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7g20tl",
          "author": "UltimateLmon",
          "text": "If you set up Queue with appropriate IAM policies to both the queue and encryption key, then you shouldn't have to worry about the VPC (as far as the queue is concerned).\n\n  \nIs what you want restrict access for someone pushing messages into the queue or triggering Lambda?\n\n  \nYou do want Lambda to be in the private subnet with appropriate security group if you trying to hit the database in the same subnet / connected subnet.",
          "score": 2,
          "created_utc": "2026-02-26 02:38:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g5w9m",
              "author": "aqyno",
              "text": "If your Lambda needs to hit both a database inside a VPC and a public service like SQS, it has to be in a private subnet. That’s really the only setup that works.",
              "score": 2,
              "created_utc": "2026-02-26 03:00:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7g7gfu",
                  "author": "UltimateLmon",
                  "text": "Exactly yeah. \n\n\nI'm more wondering what the OP meant by \"anyone being able to create a message\".\n\n\nIt would be question of locking down the IAM policies involved but hard to tell what the entry point the OP wants to deny.",
                  "score": 1,
                  "created_utc": "2026-02-26 03:09:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7gtsmv",
                  "author": "aplarsen",
                  "text": "Private VPC + an egress to the public internet",
                  "score": 1,
                  "created_utc": "2026-02-26 05:38:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7i8gek",
          "author": "solo964",
          "text": "Unclear what you mean by \"I am not married to using SQS and was surprised to learn that it's public\". What is it that you think is public that you're concerned about?\n\nOn \"I had thought that someone with our account credentials (i.e. a coworker) could just invoke aws cli to send messages as he generated them\", yes you can do that. What makes you think this is not possible?",
          "score": 1,
          "created_utc": "2026-02-26 12:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ia6r5",
          "author": "Wide_Commission_1595",
          "text": "So, this is a classic problem of understanding the components, where they live, how they communicate and how to secure it all.\n\nFirstly, SQS will never be \"inside\" your VPC.  It's a global service and as long as IAM permissions on the queue and on the principal trying to send a message all agree, then the queue is secure.\n\nThe lambda function, because of your event source mapping, will be invoked by the poller and this will pass the messages to you.  You can control which ones are deleted as consumed or returned to the queue in the return response.\n\nYou only need to put the lambda in a vpc if your database is also in a vpc.  If it's lambda, then that is also a global service.  If your lambda is in a vpc you need to go e it a security group with access to the database sg, and then then database sg allow ingress from the lambda sg\n\nIf that's all in place, a message in the queue will trigger the function which will write to the DB and then respond saying the messages can be deleted from the queue",
          "score": 1,
          "created_utc": "2026-02-26 13:04:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g2hsy",
          "author": "aplarsen",
          "text": "Why do you have it in a private subnet? What problem does that solve?",
          "score": 0,
          "created_utc": "2026-02-26 02:41:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g5ysr",
              "author": "aqyno",
              "text": "Connecting yo a private DB. Apparently.",
              "score": 2,
              "created_utc": "2026-02-26 03:00:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7hwn0j",
              "author": "Sirwired",
              "text": "In general, things that don't need access to the Internet shouldn't have it. It drives up cost, increases attack surface, and increases the chance of security-breaking mis-configuration.",
              "score": 1,
              "created_utc": "2026-02-26 11:27:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7g2lwl",
          "author": "Prestigious_Pace2782",
          "text": "Yeah you need a vpc endpoint in there for sqs and need to allow https between the lambda and the endpoint.",
          "score": 0,
          "created_utc": "2026-02-26 02:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g7qjn",
              "author": "clintkev251",
              "text": "No you don't. The Lambda service polls SQS, not your function",
              "score": 3,
              "created_utc": "2026-02-26 03:11:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7h5i49",
                  "author": "Prestigious_Pace2782",
                  "text": "If you are in a private vpc with no Nat gateway an are calling sqs via an sdk in your code, my experience is that you need an endpoint. It’s a pattern I use a bit.",
                  "score": 1,
                  "created_utc": "2026-02-26 07:15:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rbfv2w",
      "title": "Cloudwatch alarms mute rules",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rbfv2w/cloudwatch_alarms_mute_rules/",
      "author": "becharaerizk",
      "created_utc": "2026-02-22 07:47:17",
      "score": 6,
      "num_comments": 17,
      "upvote_ratio": 0.88,
      "text": "Hello,\n\nI wanted to implement some kind of maintenance mode on alarms i have setup on my work's awa account. Right before i started i saw WA released alarms mute rules which do exactly what i want.\n\nIt works well using the console but i want to write a function that takes a specific string and created mute rules with all alarms containing this string. (For automated workflows and such)\n\nI noticed that neither the cli nor the python sdk support this yet, when are mute rules supposed to be released for cli or boto3 in python?\n\n  \nFeature I am speaking about: [Configure alarm mute rules](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/alarm-mute-rules-configure.html)\n\n  \nChecking [boto3's latest documentation](https://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch.html): no mention of this feature",
      "is_original_content": false,
      "link_flair_text": "monitoring",
      "permalink": "https://reddit.com/r/aws/comments/1rbfv2w/cloudwatch_alarms_mute_rules/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6qofga",
          "author": "Refwah",
          "text": "\nhttps://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch/client/describe_alarms.html\n\nhttps://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch/client/disable_alarm_actions.html",
          "score": 1,
          "created_utc": "2026-02-22 08:20:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qp3he",
              "author": "becharaerizk",
              "text": "Im not talking about disable alarm actions, this doesnt help my case im speaking about [Configure alarm mute rules](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/alarm-mute-rules-configure.html)",
              "score": 1,
              "created_utc": "2026-02-22 08:26:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6qq2pd",
                  "author": "Refwah",
                  "text": "If you set the state:\n\nhttps://docs.aws.amazon.com/boto3/latest/reference/services/cloudwatch/client/set_alarm_state.html\n\nAnd then immediately disable actions \n\nYou effectively get a mute\n\nYou then re-enable actions and you un mute the alarm\n\nYou may want to set them back to not alarming to in order to trigger the action again",
                  "score": 1,
                  "created_utc": "2026-02-22 08:35:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6r9jz0",
          "author": "sandro-_",
          "text": "Isn't that the API: https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_PutAlarmMuteRule.html\n\nTo effectively create a mute rule for a specific time?\n\nAnd in MuteTargets you can target which alarms to mute?",
          "score": 1,
          "created_utc": "2026-02-22 11:41:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rcdfw",
              "author": "becharaerizk",
              "text": "Yes the cli commands provided dont work, they return an error when using then, even when using cloudshell",
              "score": 1,
              "created_utc": "2026-02-22 12:06:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rj9vh",
                  "author": "KayeYess",
                  "text": "Make sure one of the associated IAM policies for the user/role has the required permissions for cloudwatch:PutAlarmMuteRule",
                  "score": 1,
                  "created_utc": "2026-02-22 12:59:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rwdon",
                  "author": "Flakmaster92",
                  "text": "Make sure you’re also running the absolute latest version of the CLI, too many times I’ve seen people saying “it didn’t work” when they’re running a CLI version from before the feature came out",
                  "score": 1,
                  "created_utc": "2026-02-22 14:20:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6sitd6",
          "author": "ruibranco",
          "text": "The native mute rules feature is console-only for now — SDK/CLI support is usually added a few months after console launch. In the meantime, the disable\\_alarm\\_actions + set\\_alarm\\_state approach works but is fragile for automated workflows. Another option worth considering: EventBridge Scheduler to run a Lambda that toggles alarm actions on/off around your maintenance windows, which is more auditable and easier to manage at scale than per-alarm state manipulation.",
          "score": 1,
          "created_utc": "2026-02-22 16:09:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6txbhe",
              "author": "crh23",
              "text": "Months would be surprising - I'd expect days",
              "score": 1,
              "created_utc": "2026-02-22 20:05:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6syb93",
              "author": "becharaerizk",
              "text": "A few months for a simple api call from python or the cli? Any programmer could finish that in a week without the use of ai lol",
              "score": 0,
              "created_utc": "2026-02-22 17:19:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6z9xla",
          "author": "liverdust429",
          "text": "The API exists (PutAlarmMuteRule) but SDK/CLI support always lags behind console launches. Check your CloudShell CLI version with `aws --version`, it's probably outdated. If it's still not there on the latest version you can always hit the API directly with SigV4 signing until boto3 catches up. Annoying but it works.",
          "score": 1,
          "created_utc": "2026-02-23 16:56:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7988j4",
          "author": "crh23",
          "text": "It has now released in [the CLI](https://github.com/aws/aws-cli/blob/v2/CHANGELOG.rst) and [boto3](https://github.com/boto/boto3/blob/develop/CHANGELOG.rst)",
          "score": 1,
          "created_utc": "2026-02-25 02:33:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79mfbr",
              "author": "becharaerizk",
              "text": "Finally lol",
              "score": 2,
              "created_utc": "2026-02-25 03:55:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rctlrt",
      "title": "How Does Karpenter Handle AMI Updates via SSM Parameters? (Triggering Rollouts, Refresh Timing, Best Practices)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rctlrt/how_does_karpenter_handle_ami_updates_via_ssm/",
      "author": "LemonPartyRequiem",
      "created_utc": "2026-02-23 20:52:45",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I’m trying to configure Karpenter so a `NodePool` uses an `EC2NodeClass` whose AMI is selected via an SSM Parameter that we manage ourselves.\n\nWhat I want to achieve is an automated (and controlled) AMI rollout process:\n\n* Use a Lambda (or another AWS service, if there’s a better fit) to periodically fetch the latest AWS-recommended EKS AMI (per the AWS docs: [https://docs.aws.amazon.com/eks/latest/userguide/retrieve-ami-id.html](https://docs.aws.amazon.com/eks/latest/userguide/retrieve-ami-id.html)).\n* Write that AMI ID into *our own* SSM Parameter Store path.\n* Update the parameter used by our **test** cluster first, let it run for \\~1 week, then update the parameter used by **prod**.\n* Have Karpenter automatically pick up the new AMI from Parameter Store and perform the node replacement/upgrade based on that change.\n\nWhere I’m getting stuck is understanding how `amiSelectorTerms` works when using the `ssmParameter` option (docs I’m referencing: [https://karpenter.sh/docs/concepts/nodeclasses/#specamiselectorterms](https://karpenter.sh/docs/concepts/nodeclasses/#specamiselectorterms)):\n\n* How exactly does Karpenter resolve the AMI from an `ssmParameter` selector term?\n* When does Karpenter re-check that parameter for changes (only at node launch time, periodically, or on some internal resync)?\n* Is there a way to force Karpenter to re-resolve the parameter on a schedule or on demand?\n* What key considerations or pitfalls should I be aware of when trying to implement AMI updates this way (e.g., rollout behavior, node recycling strategy, drift, disruption, caching)?\n\nThe long-term goal is to make AMI updates as simple as updating a single SSM parameter: update test first, validate for a week, then update prod letting Karpenter handle rolling the nodes automatically.",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rctlrt/how_does_karpenter_handle_ami_updates_via_ssm/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o72r5s3",
          "author": "sunra",
          "text": "Your best bet is going to be to read the source.\n\nBut my understanding is that it is the Karpenter controller itself which monitors the SSM parameter (not the nodes themselves). When the controller notices that some nodes don't match the parameter it will mark the nodes as \"drifted\", and the replacements will happen according to your node-pool disruption-budget and node-termination-grace-period.\n\nI don't know this for sure - it's my expectation based on how Karpenter handles other changes (like k8s control-plane upgrades).",
          "score": 1,
          "created_utc": "2026-02-24 03:42:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73gyvq",
          "author": "yuriy_yarosh",
          "text": "1. You'll need to enable drift detection so it'll actually resync SSM   \n[https://karpenter.sh/docs/reference/settings/#feature-gates](https://karpenter.sh/docs/reference/settings/#feature-gates)\n\n2. SSM itself is throttled [https://github.com/aws/karpenter-provider-aws/issues/5907](https://github.com/aws/karpenter-provider-aws/issues/5907)  \nResync was 5 min before contributing to CNCF (reconcil cycle period for the whole controller), but now it's hardcoded to start checking only after an hour  \n[https://github.com/kubernetes-sigs/karpenter/blob/main/designs/drift.md](https://github.com/kubernetes-sigs/karpenter/blob/main/designs/drift.md)  \n[https://github.com/kubernetes-sigs/karpenter/blob/main/pkg/controllers/nodeclaim/disruption/drift.go#L93](https://github.com/kubernetes-sigs/karpenter/blob/main/pkg/controllers/nodeclaim/disruption/drift.go#L93)  \n[https://github.com/aws/karpenter-provider-aws/blob/main/pkg/cloudprovider/cloudprovider.go#L281](https://github.com/aws/karpenter-provider-aws/blob/main/pkg/cloudprovider/cloudprovider.go#L281)  \n\n\nSpot instances require SQS interruption queue `--interruption-queue`  \n[https://karpenter.sh/docs/concepts/disruption/#interruption](https://karpenter.sh/docs/concepts/disruption/#interruption)\n\n3. No, on-demand... \n\nYeah, been there, Karpenter is all over the place so wrote a custom cluster autoscaler with a Terraform provider and Kamaji, to keep infra state consistent, synchronized, and in one place.",
          "score": 1,
          "created_utc": "2026-02-24 06:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bbcgk",
          "author": "EcstaticJellyfish225",
          "text": "Consider using TAGs for the AMI selector, you should be able to tag AWS provided (and your own) AMIs.  Then you can pre-test an AMI in your dev account, once you are happy with it, you can tag the same AMI in your prod account and it will become available for karpenter to pick up next time a new node is needed (or if using drift detection at any time your disruption budget allows).\n\nAutomating the test cycle and tagging AMIs that pass the test, is also pretty straight forward. Test in a dev account, if the AMI asses the test, by some means tag the same AMI in your prod account. (Maybe setup an SNSTopic triggering a lambda, or something similar).",
          "score": 1,
          "created_utc": "2026-02-25 12:19:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgy7lw",
      "title": "can i use sagemaker preprocessing together with training?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rgy7lw/can_i_use_sagemaker_preprocessing_together_with/",
      "author": "Sbadabam278",
      "created_utc": "2026-02-28 09:20:12",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hello,\n\n  \nI have a sagemaker training script. The model relies on some global statistics to be computed from the data. Right now this runs as a function before the pytorch training starts, but that's obviously suboptimal (I am paying for gpu time unnecessarily)\n\n  \nSo I was thinking of using sagemaker preprocessing for this. But can I spawn both the preprocessing job and training with the same script invocation, with the training job waiting to schedule until preprocessing is done?\n\n  \nIf I need to instead run two separate commands and wait manually anyway, then perhaps using aws batch is better ?\n\nThank you in advance!",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rgy7lw/can_i_use_sagemaker_preprocessing_together_with/",
      "domain": "self.aws",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1reb3q4",
      "title": "AWS Backup Jobs with VSS Errors",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1reb3q4/aws_backup_jobs_with_vss_errors/",
      "author": "Budget-Industry-3125",
      "created_utc": "2026-02-25 11:49:03",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Good morning guys,\n\n  \nI've set up AWS Backup Jobs for many of my EC2 Instances. \n\nThere are 20 VMs enabled for backing up their data to AWS, but somehow 9 of them are presenting the following errors:\n\nWindows VSS Backup Job Error encountered, trying for regular backup\n\nI have tried re-installing the backup agent in the vms and updating, but it doesn't seem to be working out. \n\nUpon connecting to the machines, I'm able to find some VSS providers in failed states. However, after restarting them and verifying that they are OK, the job fails again with the same error message.\n\n  \nHas anyone encountered this behaviour before?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1reb3q4/aws_backup_jobs_with_vss_errors/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7bn6tp",
          "author": "brile_86",
          "text": "  \nCheck the pre-reqs  \n[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/application-consistent-snapshots-prereqs.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/application-consistent-snapshots-prereqs.html)\n\nTLDR:\n\n* Windows Server 2016\n* .NET Framework version 4.6 or later\n* Windows PowerShell major version 3, 4, or 5 with language mode set to FullLanguage\n* AWS Tools for Windows PowerShell version [3.3.48.0](http://3.3.48.0) or later\n* IAM policy AWSEC2VssSnapshotPolicy (or equivalent permissions) attached (make sure you don't have any restrictive SCP or IAM Policy Boundaries blocking it)\n\n  \nAlso some instances are not supported as too small  \n[https://docs.aws.amazon.com/aws-backup/latest/devguide/windows-backups.html](https://docs.aws.amazon.com/aws-backup/latest/devguide/windows-backups.html)\n\n* t3.nano\n* t3.micro\n* t3a.nano\n* t3a.micro\n* t2.nano\n* t2.micro\n\n",
          "score": 3,
          "created_utc": "2026-02-25 13:33:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hhfi0",
              "author": "Budget-Industry-3125",
              "text": "already did. all instances have the same permissions. I've reduced the number of jobs with errors to 2 of them.",
              "score": 1,
              "created_utc": "2026-02-26 09:07:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7bjqdc",
          "author": "ReturnOfNogginboink",
          "text": "This is a Windows issue, not an AWS issue. The error is coming from the Windows volume snapshot service.",
          "score": 0,
          "created_utc": "2026-02-25 13:13:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cm51j",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-02-25 16:26:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7co5m1",
              "author": "gex80",
              "text": "Why would they contact Veeam support?",
              "score": 1,
              "created_utc": "2026-02-25 16:35:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rf6j6v",
      "title": "No P5 instances available in any region?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rf6j6v/no_p5_instances_available_in_any_region/",
      "author": "peanutknight1",
      "created_utc": "2026-02-26 10:18:34",
      "score": 5,
      "num_comments": 24,
      "upvote_ratio": 1.0,
      "text": "Curious, is everyone facing the same issue? We have no service quota issue but we arent able to create any P5 type EC2 to train our models. \n\nIts a little crazy, we checked every single region, is there such a big shortage? \n\nAny recommendations on what we can do? \n\nTrainium instances are not available either! ",
      "is_original_content": false,
      "link_flair_text": "compute",
      "permalink": "https://reddit.com/r/aws/comments/1rf6j6v/no_p5_instances_available_in_any_region/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7houm5",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'compute'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+compute).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-26 10:18:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hrzdo",
          "author": "ENBD",
          "text": "You need to use capacity blocks for ML. That’s probably the only realistic way to get access to these instances in us-east-1. https://aws.amazon.com/ec2/capacityblocks/\n\nTalk to your TAM also. They may be able to come up with some other options.",
          "score": 22,
          "created_utc": "2026-02-26 10:47:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hs6ya",
              "author": "peanutknight1",
              "text": "We dont have a TAM, how do we get one? We only have an account manager.",
              "score": 2,
              "created_utc": "2026-02-26 10:49:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hsr4k",
                  "author": "ENBD",
                  "text": "You get assigned a TAM with enterprise support. Capacity blocks are the answer here. Make sure you can launch the instance in any AZ in the region, so have subnets provisioned in all AZs.",
                  "score": 8,
                  "created_utc": "2026-02-26 10:54:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ktr4n",
                  "author": "Ok_Chocolate8661",
                  "text": "Talk to your account manager.",
                  "score": 1,
                  "created_utc": "2026-02-26 20:27:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7hr0m4",
          "author": "alapha23",
          "text": "At least trainium should be available. For p5 you need to talk to your business development manager in aws",
          "score": 4,
          "created_utc": "2026-02-26 10:38:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hrepj",
              "author": "peanutknight1",
              "text": "We did, but its not an issue of quota allocation, its an issue of capacity. They cant help if there are no P5 servers available.\n\nReally difficult right now, training on g6/ g5 is taking 21hrs for one run.\n\nedit:yes, trn type instances are not available either\n\nMeaning if you go to launch the instance it will say insufficient capacity",
              "score": 2,
              "created_utc": "2026-02-26 10:42:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hwliz",
                  "author": "alapha23",
                  "text": "also tell them to talk to SSO to put you on the waitinglist",
                  "score": 2,
                  "created_utc": "2026-02-26 11:27:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7hwi5v",
                  "author": "alapha23",
                  "text": "Tell them to use baywatch to check which region has availabilities",
                  "score": 3,
                  "created_utc": "2026-02-26 11:26:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7hwpcp",
                  "author": "alapha23",
                  "text": "Also, write a script to race for it, that works surprisingly well as well",
                  "score": 1,
                  "created_utc": "2026-02-26 11:28:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ll55k",
                  "author": "Flakmaster92",
                  "text": "A single G5? Why not do distributed training?",
                  "score": 1,
                  "created_utc": "2026-02-26 22:39:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7s3oim",
                  "author": "The_Tree_Branch",
                  "text": "P5's are only available at scale for On Demand capacity in 4 regions (see this blog: https://aws.amazon.com/blogs/aws/announcing-up-to-45-price-reduction-for-amazon-ec2-nvidia-gpu-accelerated-instances/):\n\n- Asia Pacific (Mumbai)\n- Asia Pacific (Tokyo)\n- Asia Pacific (Jakarta)\n- South America (São Paulo)\n\nThat said, I would expect actual demand to still outweigh available capacity and echo other recommendations to use Capacity Blocks for ML (EC2) or SageMaker Training Plans. \n\nOnly other option is to work with your account team (would be the TAM if you had Enterprise Support).",
                  "score": 1,
                  "created_utc": "2026-02-27 22:20:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7ibcyb",
                  "author": "daredevil82",
                  "text": "good.  maybe this bullshit you're doing affects you as much if not more than people trying to buy hardware\n\nhttps://arstechnica.com/gadgets/2026/02/ram-now-represents-35-percent-of-bill-of-materials-for-hp-pcs/\n\n>I pulled the receipt for 48GB of DDR5 UDIMMs I bought in mid-2024 for my home server. $245. The slower speed version of that same kit is in stock right now for $945.",
                  "score": -3,
                  "created_utc": "2026-02-26 13:11:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7osfi0",
          "author": "crh23",
          "text": "How long is the training run? Capacity blocks are made for this, or try spot if it's short. Also try to have flexibility on instance type",
          "score": 1,
          "created_utc": "2026-02-27 12:11:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7i5hbl",
          "author": "The_Packeteer",
          "text": "U can get them, u just have to request them and it takes time for them to be allocated",
          "score": 1,
          "created_utc": "2026-02-26 12:34:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd84jb",
      "title": "Quantum-Guided Cluster Algorithms for Combinatorial Optimization",
      "subreddit": "aws",
      "url": "https://aws.amazon.com/blogs/quantum-computing/quantum-guided-cluster-algorithms-for-combinatorial-optimization/",
      "author": "donutloop",
      "created_utc": "2026-02-24 05:55:27",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1rd84jb/quantumguided_cluster_algorithms_for/",
      "domain": "aws.amazon.com",
      "is_self": false,
      "comments": [
        {
          "id": "o73zfzp",
          "author": "nucleustt",
          "text": "What in god's name is this? ELI5",
          "score": 2,
          "created_utc": "2026-02-24 09:50:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbz0rw",
      "title": "Track Karpenter efficiency of cluster bin-packing over time with kube-binpacking-exporter",
      "subreddit": "aws",
      "url": "https://github.com/sherifabdlnaby/kube-binpacking-exporter",
      "author": "SherifAbdelNaby",
      "created_utc": "2026-02-22 21:59:10",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "technical resource",
      "permalink": "https://reddit.com/r/aws/comments/1rbz0rw/track_karpenter_efficiency_of_cluster_binpacking/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1rfeonb",
      "title": "2 support requests are being ignored",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rfeonb/2_support_requests_are_being_ignored/",
      "author": "l1nux44",
      "created_utc": "2026-02-26 16:25:10",
      "score": 5,
      "num_comments": 7,
      "upvote_ratio": 0.86,
      "text": "Hey guys, \n\nI'm in a bit of a pickle here. I posted a while ago saying that I've been locked out of our company amazon account because of an old email address. The advice I got was to make a new account and reach out to Amazon, so that's what I did. Now I'm logged into the new account, and they won't respond to my original support request, or the new one I've opened asking about it. Has anyone else had to deal with this? We're paying for a service and we can't access our billing information, what happens when we need to update our credit card or something?",
      "is_original_content": false,
      "link_flair_text": "billing",
      "permalink": "https://reddit.com/r/aws/comments/1rfeonb/2_support_requests_are_being_ignored/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7jdrep",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'billing'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+billing).\n\nLooking for more information regarding billing, securing your account or anything related? [Check it out here!](https://www.reddit.com/r/aws/comments/vn4ebe/check_it_first_operating_within_amazon_web/)\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-26 16:25:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jgnbf",
          "author": "AWSSupport",
          "text": "Hello,\n\nApologies for the ongoing frustration regarding your account and support case concerns. We've received your private chat message and will be responding shortly to assist.\n\n\\- Marc O.",
          "score": 5,
          "created_utc": "2026-02-26 16:38:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jezjg",
          "author": "cachemonet0x0cf6619",
          "text": "That was bad advice. If you can’t get into the root account then the account isn’t yours and you should consider it gone. I know this is not what you want to hear but i would suggest that you start migrating into the new account and properly set up the account so that this never happens again. another good tip is to buy domains in third party service instead of route53 so that you don’t lose the domains you control.",
          "score": 6,
          "created_utc": "2026-02-26 16:30:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ljmkj",
          "author": "pixeladdie",
          "text": "What do you mean old address?\n\nMaking some assumptions here, work with your sysadmins to gain access to the old address inbox so you can perform the necessary resets, gain access, and then work to change the root email address to a shared inbox to prevent this in the future.",
          "score": 2,
          "created_utc": "2026-02-26 22:32:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pqott",
              "author": "l1nux44",
              "text": "I am the admin XD I inherited this setup from forever ago.",
              "score": 1,
              "created_utc": "2026-02-27 15:26:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7q14r2",
                  "author": "pixeladdie",
                  "text": "Well, what’s the email infra look like? \n\nDo you have your own domain?\n\nDo you control DNS for it?\n\nDo you control mail box access/configuration?\n\nWork your side to make that old email available again and get access to it so you can receive reset emails and work it that way.",
                  "score": 1,
                  "created_utc": "2026-02-27 16:16:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rckoel",
      "title": "CACs in Workspaces",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rckoel/cacs_in_workspaces/",
      "author": "KrazyMuffin",
      "created_utc": "2026-02-23 15:36:47",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Our current AWS workspace setup uses Simple AD, as I couldn't get AD Connector to work (will work on getting this working another time).\n\n\n\nCurrently a Linux workspace (Rocky Linux 8) can use CACs to authenticate to sites in-session, however, on Windows (Windows Server 2022), it doesn't recognize my computer's CAC reader. I have installed ActivID and InstallRoot, the workspace is DCV (formerly WSP).\n\n\n\nThe documentation all talks about how to setup readers with AD Connector so you can log into the workspace with your CAC, but that's not what we're trying to do, just be able to use the reader inside the instance.\n\n\n\nAny suggestions?",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rckoel/cacs_in_workspaces/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o6zxsei",
          "author": "fjleon",
          "text": "smart card redirection is disabled in windows by default for both presession and insession. you need to enable via GPO\n\nhttps://docs.aws.amazon.com/workspaces/latest/adminguide/group_policy.html#gp_install_template_wsp",
          "score": 2,
          "created_utc": "2026-02-23 18:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75hoqv",
              "author": "KrazyMuffin",
              "text": "Thank you, that worked, I'm dumb 😅",
              "score": 1,
              "created_utc": "2026-02-24 15:38:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rfxklh",
      "title": "Data security specialist intern?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rfxklh/data_security_specialist_intern/",
      "author": "Next-Garbage9163",
      "created_utc": "2026-02-27 05:04:11",
      "score": 4,
      "num_comments": 5,
      "upvote_ratio": 0.7,
      "text": "if there is anyone who works here, can you tell me what this position entails of. Is it just a glorified security guard? I don't want to intern how to be a security gaurd lol.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rfxklh/data_security_specialist_intern/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7nfssm",
          "author": "playahate",
          "text": "Interning at aws is a really good way to get hired on. Even if it does suck get that name on your resume to differentiate yourself.",
          "score": 2,
          "created_utc": "2026-02-27 05:11:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ng5cy",
              "author": "Next-Garbage9163",
              "text": "Yeah thats what I figured. All I have is retail experience on my resume so AWS Data Security Specialist Intern would look amazing.",
              "score": 0,
              "created_utc": "2026-02-27 05:14:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7ozgjj",
          "author": "RecordingForward2690",
          "text": "Any \"security\" role in IT is something that you have to be cut out for. Security is usually assumed (just like backups) so nobody gives you a pat on the back if you do your work properly and no security issues happen. But you're first in line to get blamed if something goes wrong.\n\nHaving said that, a Data Security Specialist role can be very interesting and will take you across all the services that AWS offers, will have you talk with Operations, Developers, Legal, Marketing, Data Analysts and a whole bunch of others across the organisation/solution. You'll need to talk with them about Identity, Authentication, Authorization, technical aspects of getting access, retention times for data, making sure data can be trusted, availability, GDPR and other legislation, runbooks for data breaches and so forth. Both before a project starts, in the design phase, but also afterwards when doing audits. You'll also get intimate knowledge of the tools that are able to help you do your work as an auditor. Heck, maybe the company will also sponsor you in obtaining an \"Ethical Hacker\" certificate.\n\nA very interesting aspect of Data Security today is the proliferation of all sorts of AI, which makes it very hard to put guardrails in place to ensure data doesn't leak out via trained models and such. And AI is sometimes coming up with novel ways to breach guardrails. Plus users come up with novel ways to attach AI to data sources without authorization, causing data leaks all over the place.\n\nSo yeah, an internship in that area can be very interesting in itself, and a very good leg-up into getting into IT/AWS in general.",
          "score": 1,
          "created_utc": "2026-02-27 12:59:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pb4ob",
          "author": "solo964",
          "text": "Do you mean \"data **center** security specialist intern\"? Like this [job posting](https://www.amazon.jobs/en/jobs/3189192/data-center-security-specialist-intern-us).\n\nThe role description:\n\n>As a Data Center Operational Security Specialist Intern, you will be tasked with driving operational security excellence within our Data Centers. You will write reports, create presentations and communicate with management on the status of physical security operations.",
          "score": 1,
          "created_utc": "2026-02-27 14:07:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pdutm",
              "author": "Next-Garbage9163",
              "text": "Yes",
              "score": 1,
              "created_utc": "2026-02-27 14:22:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdkgms",
      "title": "Database downtime under 5 seconds… real or marketing?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rdkgms/database_downtime_under_5_seconds_real_or/",
      "author": "shagul998",
      "created_utc": "2026-02-24 16:12:43",
      "score": 4,
      "num_comments": 20,
      "upvote_ratio": 0.6,
      "text": "AWS says new RDS Blue/Green switchovers can reduce downtime to around **5 seconds or less**.\n\n**In theory:**\n\nProduction DB (Blue)\n\n⬇\n\nClone + test (Green)\n\n⬇\n\nInstant switch\n\nBut in real systems we have:\n\n* connections\n* transactions\n* caching\n* DNS\n\nSo curious:\n\nHas anyone tried this in production?\n\nSource: [Amazon RDS Blue/Green Deployments reduces downtime to under five seconds](https://aws.amazon.com/about-aws/whats-new/2026/01/amazon-rds-blue-green-deployments-reduces-downtime/)",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rdkgms/database_downtime_under_5_seconds_real_or/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o75qohm",
          "author": "booi",
          "text": "We are able to do this in our own environment so I don’t see why you wouldn’t be able to do it in AWS.\n\nIf you use a database proxy, this could even be as low as your longest running query",
          "score": 34,
          "created_utc": "2026-02-24 16:19:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76tkeb",
              "author": "rafaturtle",
              "text": "How do you do it with a proxy? As far as I understand current blue green doesn't support db proxy but I could be wrong",
              "score": 8,
              "created_utc": "2026-02-24 19:13:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75x0vc",
          "author": "ElectricSpice",
          "text": "Not quite 5s, but I saw <30s when I migrated from Postgres 12 -> 16. I was pretty happy with that.",
          "score": 12,
          "created_utc": "2026-02-24 16:47:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7u3r9u",
              "author": "minirova",
              "text": "Looking at making that jump soon on some “maintenance mode” app DBs. Did you have to make many code changes to support the move?",
              "score": 1,
              "created_utc": "2026-02-28 05:56:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o75vpk3",
          "author": "Psych76",
          "text": "In some cases it works like that, in other cases the moment of “switch over” marks the original database instance as read only and all your active connections persist there and fail on writes.  \n\nPossibly in cases where connections are not persisted this is avoided.\n\nGreat for lower envs though, where you can bounce the workloads to reconnect after switchover.",
          "score": 6,
          "created_utc": "2026-02-24 16:42:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7622c9",
          "author": "oaga_strizzi",
          "text": "Not sure if it really had been 5 seconds, but yes, it is pretty painless and quick if you do everything right. ",
          "score": 6,
          "created_utc": "2026-02-24 17:10:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76krwv",
          "author": "inphinitfx",
          "text": "Provided you design for it, yes.",
          "score": 3,
          "created_utc": "2026-02-24 18:34:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o787bss",
          "author": "if2159",
          "text": "Used this when upgrading MySQL versions and was under 30 seconds for most services. Some services did have issues with maintaining connections to the old DB, but was easily solved by bouncing the services.",
          "score": 3,
          "created_utc": "2026-02-24 23:08:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75r1wp",
          "author": "mightybob4611",
          "text": "Just be careful. You can’t rollback shit once you switch over.",
          "score": 7,
          "created_utc": "2026-02-24 16:21:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76ade7",
          "author": "cachemonet0x0cf6619",
          "text": "real. have done it. works great when it works. had to have aws on the phone for one instance but that was really early on in the release cycle for blue green",
          "score": 5,
          "created_utc": "2026-02-24 17:48:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o794gzl",
          "author": "coinclink",
          "text": "It's important to do a test first in a non-prod environment. Other than that though, it is pretty smooth. The one I did, it actually detected a problem during the blue-green switch and rolled back to blue, also with only 5s of downtime. So I'd say they have the automated process down pretty well.",
          "score": 2,
          "created_utc": "2026-02-25 02:12:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7df0yl",
          "author": "just_a_pyro",
          "text": "Yes, with serverless auroras accessed only with data API, for engine version upgrade, wasn't <5s, more like 30s to 1 min",
          "score": 2,
          "created_utc": "2026-02-25 18:37:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7duhoq",
          "author": "gooserider",
          "text": "Reliably cutting the connections and forcing DNS to update is tough but solvable on the app side. On RDS itself, we've seen 15-30s failovers.",
          "score": 2,
          "created_utc": "2026-02-25 19:47:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7fb6gv",
              "author": "IridescentKoala",
              "text": "What dns updates?",
              "score": 2,
              "created_utc": "2026-02-26 00:07:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7fdxed",
                  "author": "gooserider",
                  "text": "We run an internal hostname like db.vector which is a CNAME to the RDS endpoint. We do that to make it easy to switch the live RDS, ex rollback a snapshot or switch regions.\n\n\nBut our Java services cache the IP addresses associated with the hostname. So if you failover an RDS, you need to make sure the DNS updates.",
                  "score": 2,
                  "created_utc": "2026-02-26 00:22:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o75r5h3",
          "author": "Old_Cry1308",
          "text": "never trust marketing claims. real world always adds complexity they don't account for.",
          "score": 3,
          "created_utc": "2026-02-24 16:21:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75tdav",
              "author": "rdubya",
              "text": "Not sure why you are being downvoted but for sure test with your workloads, we ran into issues with the logical replication that blue/green requires due to DDL. Ended up upgrading postgres in-place with an outage window as we ran out of time to figure out the replications issues.",
              "score": 7,
              "created_utc": "2026-02-24 16:31:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o794q8n",
              "author": "coinclink",
              "text": "I mean, yeah, you should test before doing anything, but AWS doesn't just put out marketing fluff, especially around something as core, pivotal to production product like a database. Everything in RDS is very solid. The B-G deploy will even detect failures during the switchover and roll back to blue automatically. It's really well designed.",
              "score": 1,
              "created_utc": "2026-02-25 02:13:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rgvfbz",
      "title": "AWS DynamoDB / Copilot Studio",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rgvfbz/aws_dynamodb_copilot_studio/",
      "author": "Ja-smine",
      "created_utc": "2026-02-28 06:35:35",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.8,
      "text": "Hello,\n\nWith my colleagues we're trying to create an agent in Copilot Studio for sales teams.\n\nOur data is stored in AWS DynamoDB. We've been trying to find a way to connect to it but in vain...\n\nThe only solution that I could find was CData connector but our company won't pay for it.  At least not at this stage of the project...\n\nDo you know of any way to do it ? Or should we just give up and try to store our data elsewhere ?\n\nThanks !",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rgvfbz/aws_dynamodb_copilot_studio/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o7ut47b",
          "author": "SuperScorpion",
          "text": "Making data available to sales teams sounds like you’re looking to make data available for analytical processing? One thing we recently found out is that DynamoDB does not lend itself well to those kinds of applications because it usually has to aggregate lots of data resulting in heavy scan operations.\n\nWe were able to make data available for analytics by storing it in another database as well (or Apache Iceberg in our case) . We use DynamoDB Streams to sync the updates happening to the database to the iceberg tables",
          "score": 1,
          "created_utc": "2026-02-28 09:47:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd81dy",
      "title": "Getting Started with AWS",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rd81dy/getting_started_with_aws/",
      "author": "gokuplayer17",
      "created_utc": "2026-02-24 05:50:41",
      "score": 3,
      "num_comments": 31,
      "upvote_ratio": 0.8,
      "text": "Hello! I recently got hired to work on a solar metric dashboard for a company that uses Arduinos to control their solar systems. I am using Grafana for the dashboard itself but have no way of passing on the data from the Arduino to Grafana without manually copy/pasting the CSV files the Arduino generates. To automate this, I was looking into the best system to send data to from the Arduino to Grafana, and my research brought up AWS. My coworker, who is working on the Arduino side of this, agreed.\n\nBefore getting into AWS, I wanted to confirm with people the services that would be best for me/the company. The general pipeline I saw would be Arduino -> IoT Core -> S3 -> Athena -> Grafana. Does this sound right? The company has around 100 clients, so this seemed pretty cost efficient.\n\nGrafana is hosted as a VPS through Hostinger as well. Let me know if I can provide more context!",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1rd81dy/getting_started_with_aws/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o73a82e",
          "author": "therouterguy",
          "text": "How many files are generated and which datasource is used by Grafana? Anyway I would look at parsing the csv files with a Lambda when it is created. Athena can be quite expensive.",
          "score": 3,
          "created_utc": "2026-02-24 06:00:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73bt6r",
              "author": "gokuplayer17",
              "text": "The arduino pushes a CSV every minute with 10 seconds of data each. Our current set up appends this to an existing CSV on another website. That CSV on the other website is like, years of data and the one I've been using is around 60 MB but that might not be necessary? I know the long time period I really need is \"year to date\" and \"last 30 days\" data.\n\nFor the Grafana datasource, I am open to whatever works. I saw Athena is a connection and wasn't sure what specific datasource that would translate to.\n\nI did see parsing with Lambda would probably be a good thing to include, but wasn't certain.\n\nAnother thing to add is Grafana can be a bit laggy with the calculations for something like BTUs, so I wanted to know where that could be done. I think Lambda could do this too?",
              "score": 1,
              "created_utc": "2026-02-24 06:13:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o73hrqu",
                  "author": "therouterguy",
                  "text": "I am not that familiar with Athena but iirc Athena will go over all the data to get you only a small subset. As your datasource is quite small it might not be a lot but it depends on how many queries you make. I would just parse the data with Lambda and push it to some local datastore.",
                  "score": 1,
                  "created_utc": "2026-02-24 07:04:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o77aemk",
                  "author": "cachemonet0x0cf6619",
                  "text": "you probably want kinesis to do the calculations but you’re starting to reach beyond the scope of a hobby app and should consider cost. \n\nhere’s an article to help you think about that:\nhttps://aws.amazon.com/blogs/iot/7-patterns-for-iot-data-ingestion-and-visualization-how-to-decide-what-works-best-for-your-use-case/",
                  "score": 1,
                  "created_utc": "2026-02-24 20:31:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73a9uy",
          "author": "Old_Cry1308",
          "text": "aws iot core is a good choice. for data storage, s3 works. athena to query. looks solid. might want to check costs though.",
          "score": 3,
          "created_utc": "2026-02-24 06:00:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73by7s",
              "author": "gokuplayer17",
              "text": "Thank you! Definitely wanna look into costs, I have played with the AWS cost calculator but without knowing exact file sizes, it's been hard to get a sure estimate. I've mainly seen around $30 monthly which isn't bad.",
              "score": 1,
              "created_utc": "2026-02-24 06:14:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73jlab",
          "author": "cycle-nerd",
          "text": "S3 + Athena, while it will technically work, do not seem like the optimal choice here. Look into specialized time series databases like Amazon Timestream for InfluxDB that are purpose-built for this type of use case.",
          "score": 2,
          "created_utc": "2026-02-24 07:20:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73oq9g",
          "author": "snorberhuis",
          "text": "AWS is a good fit if you plan to quickly grow your client base. It will help you easily scale with the number of clients. Better than a VPS.\n\nAfter IoT Core, you can process the data using Lambda functions. Be sure to build the lambdas so they can later be migrated to containers, as containers can become more cost-effective at scale.\n\nThe IoT companies I work with often store large amounts of time-series data. Time Series Influx DB is a better fit for this, but it is not serverless. So I would start with S3 to keep costs down. \n\nBe sure to correctly set up your AWS Account structure. You will not yet need a VPC. But getting this right prevents future migrations.",
          "score": 2,
          "created_utc": "2026-02-24 08:08:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76g6v6",
              "author": "cachemonet0x0cf6619",
              "text": "would you consider using durable functions before containers?",
              "score": 1,
              "created_utc": "2026-02-24 18:14:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7atv3l",
                  "author": "snorberhuis",
                  "text": "Durable functions serve a different purpose than switching to containers. You could actually also use Lambda managed instances for this purpose. They also offer the ability to reduce cold starts and be more cost effective.",
                  "score": 2,
                  "created_utc": "2026-02-25 09:52:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73eqnz",
          "author": "ramdonstring",
          "text": "I would suggest to reconsider AWS for this solution.\n\nMy proposal would be to change the way the Arduinos publish data, or make them dual publish during migration, and start publishing in MQTT (as they should) to an MQTT broker. Then use https://grafana.com/grafana/plugins/grafana-mqtt-datasource/ or Loki and then Grafana.\n\nYou can install everything in the same VPS.\n\nEdit: oh the downvotes! I understand this subreddit is completely against anyone suggesting not using AWS.",
          "score": 2,
          "created_utc": "2026-02-24 06:37:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o76f6by",
              "author": "cachemonet0x0cf6619",
              "text": "you don’t choose aws iot for the mqtt you chose aws iot for the certificate management.",
              "score": 1,
              "created_utc": "2026-02-24 18:09:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76gvou",
                  "author": "ramdonstring",
                  "text": "I didn't say anything about using AWS IoT for MQTT, I said don't use AWS at all. It's overkill for the problem and scale.",
                  "score": 0,
                  "created_utc": "2026-02-24 18:17:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o73q8vm",
              "author": "maxlan",
              "text": "I would agree\n\nIf you as a human can get the csv file to copy paste then some automation can get it. Saying there is no way to do that is like admitting you do not understand how computers work. \n\nAnd your solution to not understanding how computers work is an immensely complex solution that still doesn't really answer the question of how do you make it do the copy/paste job. \n\nWhat are your requirements for the solution? What are your non functional requirements? \n\nGoing into this with the information you provided is a recipe for being one of those companies who say \"we were spending 3/month on our IT solution and then we got aws and now we spend 3/minute and it doesn't provide the customer access to the data they want\"",
              "score": 1,
              "created_utc": "2026-02-24 08:22:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76fj7c",
                  "author": "cachemonet0x0cf6619",
                  "text": "anyone that’s done iot and aws knows the answers to this and if you don’t that’s probably an indication that you shouldn’t respond",
                  "score": 0,
                  "created_utc": "2026-02-24 18:11:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74tlb6",
          "author": "TheGutterBall",
          "text": "If this is the pipeline you use (seems like the correct use case) check out the 3 golden rules for using Athena with S3 to help save some money (just ask chatGPT). Reason being, based on your description you will have a lot of small files, all in CSV which will be really expensive for Athena queries. In short, try to consolidate the data into bigger files (maybe daily), set up the S3 keys to partition by date, and lastly add AWS Glue to your pipeline that can convert the CSV to Parquet (columnar) format. Will save quite a bit of money in the long run",
          "score": 1,
          "created_utc": "2026-02-24 13:37:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7u3fea",
          "author": "Soul-Ice-Phoenix",
          "text": "Unable create account",
          "score": 1,
          "created_utc": "2026-02-28 05:53:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdsi12",
      "title": "Shrinking/growing EBS volumes automatically - Datafy vs. ZestyDisk vs. Lucidity - any feedback?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1rdsi12/shrinkinggrowing_ebs_volumes_automatically_datafy/",
      "author": "cornloko",
      "created_utc": "2026-02-24 20:58:30",
      "score": 2,
      "num_comments": 8,
      "upvote_ratio": 0.67,
      "text": "It's really hard to shrink any kind of block storage volumes on-premises or in the cloud but it's everywhere that EC2 is. Autoscaling is great but only in one direction!\n\nI came across these three vendors that do automated EBS volume management but I wanted to see what people were doing besides the normal copy-to-smaller volumes shuffle.\n\n(I know that FSxN has dedupe/thin provisioning - don't want to go down that route)\n\nThere are so many more compute management mechanisms/strategies and so few storage ones so thought to ask!\n\nThanks",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1rdsi12/shrinkinggrowing_ebs_volumes_automatically_datafy/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o77h6bu",
          "author": "oneplane",
          "text": "That would really depend on what you are actually doing. Changing EBS sizes isn't a goal in itself, so you probably have some other problem to solve?\n\nEither way, if you needed online resizing I'd always stick to something like LVM and just adding EBS PVs to a VG (and pvmove+detach) as needed.",
          "score": 4,
          "created_utc": "2026-02-24 21:02:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78jm3p",
              "author": "justin-8",
              "text": "The time spent asking the question let alone paying a vendor for one of their solutions is usually not worth just allocating the disk as needed. I think the number of times I've encountered a system that would need to scale down storage later could be counted on one hand",
              "score": 2,
              "created_utc": "2026-02-25 00:16:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78mlzs",
                  "author": "oneplane",
                  "text": "Yeah, storage being rather cheap makes most of this stuff irrelevant. We have one or two processes that need a lot of storage that goes up and down over 80% every 72 hours, but we don't want it to go offline to reboot as it's ancient software that takes ages to hash, verify and load into memory before it will even touch data.\n\nFor everything else we just nuke the disk, persistence goes into S3 or RDS almost all of the time.",
                  "score": 2,
                  "created_utc": "2026-02-25 00:32:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o77lpk1",
          "author": "steveoderocker",
          "text": "Ebs is so cheap, just provision what’s needed and slowly increment. If you need to shrink disks, copy data to a new volume and swap them out for each other. There is no actual way to decrease a volumes size.",
          "score": 5,
          "created_utc": "2026-02-24 21:23:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hih6b",
          "author": "CryOwn50",
          "text": "Shrinking EBS is always messy most teams either overprovision to stay safe or rely on snapshot + recreate during maintenance windows. I’d definitely test how those tools handle rollback and sudden IO growth before trusting automation fully.That said, in a few environments I’ve seen, the bigger savings didn’t come from shrinking volumes but from identifying dev/test instances and attached EBS running 24/7 unnecessarily. Scheduling non-prod infra down off-hours sometimes had more impact than aggressive right-sizing.",
          "score": 1,
          "created_utc": "2026-02-26 09:17:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7l31eb",
          "author": "vppencilsharpening",
          "text": "Not sure how quickly the app need to access data, but AWS Storage Gateway may be another option to consider. It leverages S3 as the backend, presenting SMB or NFS shares to the network. \n\nMount the shares on your drive and you end up with the Storage Gateway appliance instance costs (including the local/caching EBS volume) and the S3 costs, which is entirely consumption based. \n\nS3 storage plus the I/O may be cheaper than the EBS cost if the application performs well with slower storage. ",
          "score": 1,
          "created_utc": "2026-02-26 21:11:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}