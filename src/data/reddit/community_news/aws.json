{
  "metadata": {
    "last_updated": "2026-02-05 09:15:02",
    "time_filter": "week",
    "subreddit": "aws",
    "total_items": 20,
    "total_comments": 135,
    "file_size_bytes": 167463
  },
  "items": [
    {
      "id": "1qut06h",
      "title": "[Update] AWS suspended my account anyway - production is down",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qut06h/update_aws_suspended_my_account_anyway_production/",
      "author": "charm88_baby",
      "created_utc": "2026-02-03 14:13:47",
      "score": 235,
      "num_comments": 93,
      "upvote_ratio": 0.93,
      "text": "Update to my previous post about verification issues.\n\nAWS just suspended my account. Production is down.\n\nDespite multiple AWS support reps getting involved across Reddit (Roman Z., Reece W.), LinkedIn (Aimee K.), and the support portal (Alondra G., Arturo A.). Despite Executive Escalations (Eric G.) taking over on Feb 2 and coordinating with Trust & Safety.\n\nTimeline: Verification request Jan 29. Submitted docs Jan 30. Asked to resubmit same docs Jan 31, complied. Asked for passport Feb 2, uploaded immediately. Executive Escalations involved since Feb 2.\n\nToday: Suspended anyway. Have until Feb 18 or everything gets deleted.\n\nI'm a Business Support customer. I've submitted bank statements, phone bill, passport, and LLC formation documents. Responded within hours every time. Multiple support reps across every channel confirmed they escalated.\n\nStill got suspended with production serving live customers.\n\nHas anyone recovered from full suspension after this level of compliance and escalation?\n\nCase 176984120700770",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1qut06h/update_aws_suspended_my_account_anyway_production/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o3cogc2",
          "author": "PhilosophyOutside132",
          "text": "Last month my account was also suspended, but it was reinstated 2 days later after I repeatedly escalated the issue. I lost customers and had to pay a contractual penalty for leaving my product offline for 2 days.\nThe justification given was that I was making requests to Bedrock from a different region. Which makes absolutely no sense because that’s exactly why other regions exist. They didn’t ask me to stop doing it. They just blocked me because they thought it was suspicious.",
          "score": 96,
          "created_utc": "2026-02-03 15:04:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d60vj",
              "author": "-_LS_-",
              "text": "That’s bizarre. There must be something more to it? I’m actively doing cross region bedrock and agentcore requests, because N.Virgina was the only one with the functionality at the time. Account manager is well aware as we scoped it out that way!",
              "score": 20,
              "created_utc": "2026-02-03 16:28:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3hsnls",
              "author": "jeneralpain",
              "text": "When legit traffic is more suspicious than half the phishing sites on aws:azure etc…",
              "score": 3,
              "created_utc": "2026-02-04 07:45:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3jnydp",
              "author": "methods2121",
              "text": "You have to be kidding me right?  AWS specifically indicates that in certain instances they will route Bedrock calls to the 'best' available zone... WTF... this is hard to believe.",
              "score": 2,
              "created_utc": "2026-02-04 15:38:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3jt1ba",
              "author": "Horror_Response_1991",
              "text": "They only have so much compute for bedrock and will quickly block smaller accounts so as not to affect their larger customers ",
              "score": 1,
              "created_utc": "2026-02-04 16:01:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ckcbw",
          "author": "yesman_85",
          "text": "Yikes, scare story. Makes you rethink putting all your eggs in 1 basket.\n\nYou can harden your environments as much as you want against hackers, but the biggest threat for us is AWS doing a rugpull.",
          "score": 55,
          "created_utc": "2026-02-03 14:43:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dqjmb",
              "author": "pint",
              "text": "aws is working hard to make cloud agnostic happen",
              "score": 20,
              "created_utc": "2026-02-03 18:02:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3cghk1",
          "author": "peanutknight1",
          "text": "Im sorry you have to go through this. The older AWS would fight tooth and nail to keep a customer, not sure what is going on here. ",
          "score": 117,
          "created_utc": "2026-02-03 14:23:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ck36w",
              "author": "sudoku7",
              "text": "Frugality won over customer obsession.",
              "score": 74,
              "created_utc": "2026-02-03 14:42:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3cym4w",
                  "author": "PeteTinNY",
                  "text": "Frugality went to Frupid - the intersection of Frugal and Stupid.",
                  "score": 23,
                  "created_utc": "2026-02-03 15:53:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3d0wbw",
                  "author": "rdubya",
                  "text": "Ive been using AWS for 8 years or so now in my current employ. The change in that time has been dramatic. Their support agents used to be top notch, go out of their way to help you with things that had nothing to do with their area or even AWS. Now every time we open a ticket its like scraping the bottom of the barrel.   \n  \nAgents giving us inaccurate information from LLMs, constantly demanding you get on calls when you open chat cases purposely because you need to work out of band, gaslighting you with stuff you clearly know is not the case, being told completely different things from one agent to the next.  I could go on and on..",
                  "score": 43,
                  "created_utc": "2026-02-03 16:04:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3cw0yv",
                  "author": "gastroengineer",
                  "text": "    AI won over customer obsession.\n\nFTFY",
                  "score": 23,
                  "created_utc": "2026-02-03 15:40:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ikqav",
                  "author": "BusterOfCherry",
                  "text": "Yup, more cuts, same amount of work, less people. Fun.",
                  "score": 2,
                  "created_utc": "2026-02-04 11:58:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3e2n57",
          "author": "charm88_baby",
          "text": "Update: Just received another email from AWS minutes ago.\n\nNow they're claiming my account is \"related to other previously closed accounts\" and closing it permanently. Won't provide any details due to \"privacy concerns.\"\n\nThis is completely false. We created this account 4 months ago when we launched. Small team of 5 developers. We have never had any other AWS accounts. We've never had any closed accounts. There is no connection because none exists.\n\nThis went from \"verify your identity\" to \"related to closed accounts\" right after this post gained traction. I've submitted passport, bank statements, business formation documents proving we're a legitimate company.\n\nu/AWSSupport \\- Case 176984120700770 - This is factually incorrect and you have all documentation proving it. We have no other accounts. Our entire production infrastructure and database are on this account with no backups.\n\nWe need this resolved now. You have our government ID and business documents. We're a legitimate startup being destroyed based on information that's simply wrong.\n\nHow do we prove we don't have other accounts when you won't tell us what accounts you think we're connected to?",
          "score": 78,
          "created_utc": "2026-02-03 18:56:35",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3f9hrj",
              "author": "mattjmj",
              "text": "Why do you not have any backups of your production databases?",
              "score": 16,
              "created_utc": "2026-02-03 22:16:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fc5ou",
                  "author": "kdegraaf",
                  "text": "Something tells me his next startup will.",
                  "score": 41,
                  "created_utc": "2026-02-03 22:29:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3fqxjf",
                  "author": "sirsavant",
                  "text": "Small startup less than a year in is unlikely to have multi-provider setup. Even large companies that have been around for decades don't go multi-cloud. Having offsite backups is important but just not common as multi-region backups made that easier.",
                  "score": 25,
                  "created_utc": "2026-02-03 23:48:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3imtt8",
                  "author": "SikhGamer",
                  "text": "> Why do you not have any backups of your production databases?\n\nI really hate this line of questioning - do you have everything 100% correct when you launch a business?",
                  "score": 9,
                  "created_utc": "2026-02-04 12:13:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m80hq",
                  "author": "nowsplashattack",
                  "text": "Oh let me just export my backups from AWS Backup... oh right I can't.",
                  "score": 1,
                  "created_utc": "2026-02-04 22:49:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m8hp3",
                  "author": "Mrbucket101",
                  "text": "Backup is probably in an S3 bucket lol",
                  "score": 0,
                  "created_utc": "2026-02-04 22:52:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3l9pip",
              "author": "Natfubar",
              "text": "Maybe submit a request under your privacy legislation for information they have about you to sidestep the privacy concerns blocker. It can force someone higher up to take a look as well and maybe go WTF. ",
              "score": 1,
              "created_utc": "2026-02-04 20:03:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3gtc0x",
              "author": "dog_in_da_park",
              "text": "Sounds like AWS liked your idea and is implementing it internally and using this to stifle competition. I'd caution against using AWS for your next idea, not the first time.",
              "score": -5,
              "created_utc": "2026-02-04 03:23:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3m4xsx",
                  "author": "Legitimate_Put_1653",
                  "text": "I’m not sure why this got downvoted. Is it really far-fetched? What controls exist to keep somebody in AWS security from acting on behalf of somebody who wants to shut down a potential competitor by flagging his account for fraud?",
                  "score": 1,
                  "created_utc": "2026-02-04 22:34:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3cmb1d",
          "author": "gex80",
          "text": "That's a great way to lose a customer.",
          "score": 31,
          "created_utc": "2026-02-03 14:53:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cikds",
          "author": "mightybob4611",
          "text": "Damn man. I would freak in your position. I’ll start doing offsite backups of my database ASAP. This is a nightmare scenario.",
          "score": 45,
          "created_utc": "2026-02-03 14:34:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3cmzsy",
              "author": "Dangle76",
              "text": "If they account is suspended they can’t access the db to do those backups.  I hope OP has at least one",
              "score": 22,
              "created_utc": "2026-02-03 14:56:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3degqr",
                  "author": "mightybob4611",
                  "text": "You can set it up so that you create snapshots in account a, then share them with account b which then copies them so that they are owned by account b. Doing that right now. Also considering using a service like snapshooter as another layer.",
                  "score": 5,
                  "created_utc": "2026-02-03 17:07:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3clcdi",
              "author": "isoAntti",
              "text": "Don't forget to keep DNS somewhere you have always access",
              "score": 10,
              "created_utc": "2026-02-03 14:48:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3de5kb",
                  "author": "mightybob4611",
                  "text": "I don’t use AWS for anything related to my domains.",
                  "score": 4,
                  "created_utc": "2026-02-03 17:05:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3dxonh",
                  "author": "yesman_85",
                  "text": "That's a good point, what's a good professional registar I can at least host my domains so I can change the zone?",
                  "score": 4,
                  "created_utc": "2026-02-03 18:34:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3cgvyo",
          "author": "AWS_Chaos",
          "text": "Following. I've had a few customers with validation issues. This could be any one of us on any given day.",
          "score": 30,
          "created_utc": "2026-02-03 14:25:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3cnqi1",
              "author": "latestagecapitalist",
              "text": "Wait, what\n\nHow does this happen? (I'm just coming back to AWS after a long time away)",
              "score": 9,
              "created_utc": "2026-02-03 15:00:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d569z",
                  "author": "coinclink",
                  "text": "Short answer: I doubt it happens at all for legitimate customers. Typically you find out the reasoning behind most of these cases is multiple red flags that AWS notices. personal prepaid cards being used to pay for a supposed business, questionable user base and activity that seemingly appears overnight in a very new account, odd usage patterns and constantly changing login locations, sketchy business description (likely edging toward scam or fraud)...\n\nMy point is, they don't just suspend accounts with zero reason. We see a lot of these nightmare scenarios posted here, but whenever I've been able to dig deeper, there are legitimate reasons that OP doesn't share about what flagged their account in the first place. The idea it's been escalated so much according to them and they are still suspended is just another clue that there is more to the story.",
                  "score": 20,
                  "created_utc": "2026-02-03 16:24:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3eebyn",
          "author": "dr_batmann",
          "text": "New reason for AWS execs to sell DR solutions: We ourselves will be the disaster",
          "score": 11,
          "created_utc": "2026-02-03 19:51:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3co6x6",
          "author": "AntDracula",
          "text": "Too many layoffs, nobody to work on your case. Except AI, which hallucinated that you don't exist.",
          "score": 32,
          "created_utc": "2026-02-03 15:03:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cnzhn",
          "author": "drool_34",
          "text": "ex-AWS employee here - for T&S to do something so drastic so quickly there has to be some sort of violations against their AUP or gross violations of local laws. [https://aws.amazon.com/aup/](https://aws.amazon.com/aup/)\n\nI'm not taking their side, just saying from experience they do not do this for shits and giggles. I had a customer who does search engines and their crawlers cached CSE images and I made them drop everything to take it seriously when they wanted to nonchalantly brush it off as \"bots gonna bot\". T&S wouldn't even share too much info with me internally to communicate with customers, it's their protocol.\n\nContinue to escalate with your contacts and ask for what else is needed to resolve the issue - if you have a sales Account Rep reach out to them as well.",
          "score": 40,
          "created_utc": "2026-02-03 15:01:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3cpxhc",
              "author": "charm88_baby",
              "text": "I appreciate the context, but I need to clarify what's actually happening here.\n\nMy account was placed on hold on January 29 for verification. This wasn't some quick drastic action. It's been over a week of back and forth document requests. The suspension happened today after I submitted everything they asked for.\n\nEvery single communication from AWS has been about verifying my payment information and identity. They asked for utility bills, then bank statements, then my passport. I provided all of it. There has been zero mention of AUP violations, suspicious activity, or legal issues of any kind.\n\nWe're a legitimate SaaS company building scheduling automation software. Standard web application infrastructure serving enterprise customers. If there were AUP concerns, AWS would have said so. They haven't because this is a verification process issue.\n\nThe frustrating part is complying with everything immediately and still ending up suspended with production down. That's what I'm dealing with here.",
              "score": 38,
              "created_utc": "2026-02-03 15:11:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3d0cwy",
                  "author": "Ok_Chocolate8661",
                  "text": "Do you had an account manager? If yes, get them on the case. If not, do you have a VC investor or part of an accelerator? They usually have dedicated AWS contact person for the bigger investors of each country. They can help to provide support for their portfolio and one of the fastest way to get things resolved",
                  "score": 8,
                  "created_utc": "2026-02-03 16:01:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3efwum",
                  "author": "kn33",
                  "text": "> January 29 \n\n>over a week \n\nThe math isn't mathing, chief",
                  "score": -12,
                  "created_utc": "2026-02-03 19:58:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3cqizf",
              "author": "pint",
              "text": "i don't think that they ask for documents in case of a violation. hey, you are in violation of the law, could you please submit a bank statement?",
              "score": 12,
              "created_utc": "2026-02-03 15:14:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dtdt3",
              "author": "profmonocle",
              "text": "If AI has been introduced into this process, all bets are off. Could just be a hallucination. And they laid off thousands last week - entirely possible that the team who manually reviewed AI decisions got decimated.\n\nDo I have any evidence for this? No, but the fact that Amazon has explicitly bragged about their goal of replacing human workers with buggy AI erodes a lot of trust. Any time you hear about something like this a lot of people's first thought will be that AI fucked up.",
              "score": 8,
              "created_utc": "2026-02-03 18:15:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d614k",
              "author": "AntDracula",
              "text": "> I'm not taking their side, just saying from experience they do not do this for shits and giggles. \n\nIf it's currently being handled by AI, they are literally doing it with shits and giggles.",
              "score": 10,
              "created_utc": "2026-02-03 16:28:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3cgoa8",
          "author": "Little-Sizzle",
          "text": "Good luck my man",
          "score": 5,
          "created_utc": "2026-02-03 14:24:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cu3ok",
          "author": "burnbern",
          "text": "Remember when many of us flocked to the cloud because a colo or two wasn’t resilient enough? Posts like this make me want to run back to my data center.",
          "score": 22,
          "created_utc": "2026-02-03 15:31:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3eksfk",
              "author": "krishopper",
              "text": "After some recent negative AWS support experiences myself, this is the way I’m going. \n\nI used to be die hard AWS and tried to get everyone in the cloud. Now I want out.\n\nI miss when AWS was customer obsessed.",
              "score": 11,
              "created_utc": "2026-02-03 20:21:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dfqdi",
          "author": "TheLordB",
          "text": "Ouch how the mighty have fallen.\n\nLong long ago 10+ish years ago we were testing google cloud vs. amazon cloud.\n\nThe thing that ended the test was gcloud suspended the account 3 times for various reasons and being down for a day or two (1 payment/id issue and 2 fraud alerts/suspensions). It really didn’t like us starting up 20+ of their biggest servers even though we had talked to an account rep, gotten quotas raised etc. and explained our use case (bioinformatics NGS analysis in this case).\n\nI’m fairly sure AWS would have won anyways, but gcloud took themselves out of the equation completely with the poor service.",
          "score": 5,
          "created_utc": "2026-02-03 17:13:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dvcp3",
          "author": "paul_h",
          "text": "Make a proper website and de-anonymize self and your company, then link that back to here.",
          "score": 6,
          "created_utc": "2026-02-03 18:23:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3el5ro",
          "author": "yang2lalang",
          "text": "Run your stuff on proxmox stop using aws for core services and dB, this is exactly how they treat sellers on Amazon seller central, section 3 business agreement bla bla bla account banned, funds frozen business destroyed and this company keeps getting away with this and small businesses destroyed",
          "score": 5,
          "created_utc": "2026-02-03 20:23:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g9s51",
          "author": "calib0rx",
          "text": "Probably won't move the needle, but I posted on LI and tagged Matt Garman. Also privately pinged a few people I know at AWS. I hope there is a resolution for y'all.",
          "score": 4,
          "created_utc": "2026-02-04 01:32:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3chtt3",
          "author": "falling_away_again",
          "text": "Why do we hear this so often about AWS accounts? I don't recall ever reading stories like this about Azure? This must be a nightmare to deal with!",
          "score": 18,
          "created_utc": "2026-02-03 14:30:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d5njj",
              "author": "AntDracula",
              "text": "> I don't recall ever reading stories like this about Azure?\n\nBecause nobody uses it.",
              "score": 40,
              "created_utc": "2026-02-03 16:26:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dew6p",
                  "author": "mightybob4611",
                  "text": "Burn.",
                  "score": 5,
                  "created_utc": "2026-02-03 17:09:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3g40hs",
              "author": "PM__ME__BITCOINS",
              "text": "Lol check out r/oraclecloud, they fuck around even less.",
              "score": 6,
              "created_utc": "2026-02-04 00:59:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d9xew",
          "author": "OutOfAmmO",
          "text": "And this exactly is why all my crap is 100% gitops and on proxmox. Anyone does this to me I will have new VPS’s spinning elsewhere in 5 minutes and be fully deployed in whatever time it takes to have my cluster fully running and stabilized, roughly an hour. Fuck vendor lock-in.",
          "score": 10,
          "created_utc": "2026-02-03 16:46:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dxwcg",
              "author": "yesman_85",
              "text": "Infra is one thing, but restoring backups and making sure no data loss is a whole different story.",
              "score": 9,
              "created_utc": "2026-02-03 18:35:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3e0q14",
                  "author": "OutOfAmmO",
                  "text": "Which is why you gotta test your disaster recovery frequently, which i do for that exact reason, also made easier if you're 100% gitops, it was a nightmare when I still had iac and clickops all mixed up.",
                  "score": 3,
                  "created_utc": "2026-02-03 18:48:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3garbo",
          "author": "AustinLeungCK",
          "text": "Can anyone find Jeff Barr to escalate this?",
          "score": 3,
          "created_utc": "2026-02-04 01:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hptn5",
          "author": "purpletux",
          "text": "Sorry but I believe this is not the whole story. There is something missing.",
          "score": 1,
          "created_utc": "2026-02-04 07:19:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hzkq2",
          "author": "TheGowanus",
          "text": "Your account is one karma-farming photo of Amy Adam’s and this thread.  It is possible something is genuinely going on here, but the lack of detail in the post and account are suspicious.  \n\nIf this is a legitimate company and hasn’t been suspended for TOS violations, you need to be persistent and get in direct contact with a human to understand what triggered the verification.",
          "score": 1,
          "created_utc": "2026-02-04 08:49:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ihdow",
              "author": "DragonfruitOk5707",
              "text": "What the hell does reddit karma have to do with all of that?",
              "score": 2,
              "created_utc": "2026-02-04 11:32:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3clfdt",
          "author": "iheartrms",
          "text": "This is why the cloud is someone else's computer. I'm surprised that so many companies roll the dice like this.",
          "score": -4,
          "created_utc": "2026-02-03 14:49:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jn651",
          "author": "eliwuu",
          "text": "Stop using aws, it's no longer safe for business; it's that simple",
          "score": 0,
          "created_utc": "2026-02-04 15:34:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvqv6w",
      "title": "The actual ways to get AWS credits right now (Feb 2026 updated)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qvqv6w/the_actual_ways_to_get_aws_credits_right_now_feb/",
      "author": "alex_aws_solutions",
      "created_utc": "2026-02-04 15:02:42",
      "score": 54,
      "num_comments": 10,
      "upvote_ratio": 0.89,
      "text": "I keep seeing the same questions about AWS credits, and most of the answers are either outdated or vague. We went through this ourselves last year when building on AWS. So here’s what actually works as of February 2026\n\nNo affliate links, no fluff. Just what’s currently real and worked for us.\n\n\n\n**1. The fintech route - $5K in about 15 minutes**\n\nMost people overlook this one. A few startup banking platforms are official AWS Activate Providers, which means their customers can apply for AWS credits directly through them. Sometimes you will get notified from them if you are eligable.\n\nWe used Brex, but the same logic applies to other fintechs offering benefit. Once your business account is set up, you can apply through the AWS Activate Portfolio tier using the provider’s organization ID (you’ll find it inside their perks or startup programs section).\n\nA few important details:\n\n\\- Your AWS Accouunt should list your fintechs card as default payment method before applying.\n\n\\- The Company needs to be under 10 years old and must not have already received more than $5K in Activate Credits.\n\n\\- Credits usually land in about a week and expire after 12 months.\n\n\\- Your support plan needs be Business Support+. \n\n\n\n**2. AWS Activate Founders - $1K, open to almost anyone**\n\nGot to [https://aws.amazon.com/startups](https://aws.amazon.com/startups) and apply for the Founders tier.\n\nRequirements:\n\n\\- Company founded within the last 10 years\n\n\\- Pre-series B\n\n\\- AWS Account on a paid plan\n\n\\- Real company website (not a placeholder)\n\n\n\nTwo common mistakes:\n\n\\- Don’t use a Gmail or Yahoo adress, use your company domain instead.\n\n\\- Make sure your website hast actual content. Empty sites often get auto-rejected\n\n\n\n**3. AWS Activate Portfolio - $25K to $100K**\n\nIf you are backed by a VC or went through an accelerator, ask them. Most investors are AWS Activate Providers but never proactively mention it.\n\n\n\n**4. The Free Tier changed in mid-2025**\n\nNew Accounts after July 15, 2025 get $100 in credits automatically, plus another $100 unlocked by using core services (e.g. EC2, Lambda, Budgets).\n\n\n\n**5. Accelerator programs - $100K+ if you get in**\n\nY Combinator gives $100K standard and up to $500K for AI startups.\n\nEven YC’s Startup School (free, online, open to anyone) includes $2.5K in AWS credits.\n\n\n\n**6. Nonprofits and researchers - $1K to $5K**\n\nRegistered nonprofits can get $1K - $5K per year through TechSoup’s AWS program.\n\n\n\n**What doesn’t work!**\n\n\\- Buying credits from „brokers“, violates AWS ToS.\n\n\\- Creating multiple accounts to stack Founders credits, AWS tracks at the company level\n\n\\- Using personal AWS account and later converting it to business, just start fresh with a business account.\n\n\\- waiting too long after funding round, the 12-month Portfolio window is hard-coded.\n\n\n\n**TL;DR**\n\nFastest route if you’re an early-stage startup:\n\n1. Open a fintech business account (Brex or Mercury both work)\n\n2. Apply through their AWS Activate partnership -> $5K in credits.\n\n3. Apply for Founders ($1K) seperately.\n\n4. If you have investors, ask for their Activate org ID -> $25K - $100K.\n\n\n\nHappy to answer questions. We’ve gone through most of these paths ourselves.",
      "is_original_content": false,
      "link_flair_text": "article",
      "permalink": "https://reddit.com/r/aws/comments/1qvqv6w/the_actual_ways_to_get_aws_credits_right_now_feb/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o3kq3x9",
          "author": "ruibranco",
          "text": "Solid breakdown. One thing worth adding -- the Business Support plan requirement for the fintech route is a hidden cost people miss. That's $100/month minimum, so factor that into the $5K credit math. Still net positive obviously, but I've seen people surprised by the support plan charge eating into their credits.\n\n  \nAlso for anyone reading this in a non-US context: the fintech route through Brex/Mercury only works if you have a US-incorporated entity. For international startups, the Activate Founders tier is usually the first real option.\n\n  \nOne more gotcha on the Portfolio tier -- some VCs have already used up their Activate allocations for the quarter and won't tell you. If your investor says they're an Activate Provider but the application gets rejected, ask them to check their remaining allocation. They get a fixed number of referrals per period.\n\n  \nThe personal vs business account advice is critical. Starting fresh with a business account from day one saves weeks of back-and-forth with support trying to convert later.",
          "score": 6,
          "created_utc": "2026-02-04 18:33:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kx3ij",
              "author": "alex_aws_solutions",
              "text": "Some great add-ons. Thanks you!\n\nDefinitely worth commenting that the Business Support+ expenses starting at $99/month and still absolutely an advantage using those credits to pay for this plan.\n\nThanks for the input!",
              "score": 1,
              "created_utc": "2026-02-04 19:04:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3mvwwx",
                  "author": "yarenSC",
                  "text": "I always thought credits couldn't be used for support plans?",
                  "score": 1,
                  "created_utc": "2026-02-05 01:01:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3l9bwp",
          "author": "perciva",
          "text": "AWS also has lots of credits for open source software.  Like \"I've never seen anyone not get approved for whatever they ask for\" amounts of credits.\n\nNot relevant for startups, of course.",
          "score": 3,
          "created_utc": "2026-02-04 20:01:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3larmd",
              "author": "alex_aws_solutions",
              "text": "Also a great Input. Just skimmed the blog post in aws about this. Quite interesting. \nTo be eligable for this \"the organisation must have an OSI-approved license and can't be dominated by a single vendor or VC-funded.\"",
              "score": 1,
              "created_utc": "2026-02-04 20:08:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ls2x3",
                  "author": "perciva",
                  "text": "Yeah, that's basically just \"this has to be real open source\".",
                  "score": 4,
                  "created_utc": "2026-02-04 21:31:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qtmnuf",
      "title": "About this sub",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qtmnuf/about_this_sub/",
      "author": "Ok_Whole_1665",
      "created_utc": "2026-02-02 05:42:42",
      "score": 53,
      "num_comments": 10,
      "upvote_ratio": 0.9,
      "text": "I noticed that a previous useful post about the less popular (as in unpopular) AWS services got removed by the mods for no apparent reason.\n\nSearched for a set of rules for this sub but there doesn't seem to be any? And also noting that several of the mods seem to be AWS employees.\n\nWhich begs the question: Is this sub an unofficial AWS-affiliated sub without an overt declaration of the relationship or is it a \"normal\" sub which is not affiliated with AWS in any way?\n\nBoth are fine, I just think it's important to be clear about this.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1qtmnuf/about_this_sub/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o344y9b",
          "author": "TimonAndPumbaAreDead",
          "text": "Disclaimer - I work for AWS but I do not post here as a representative of the company nor am I affiliated with the mod team. I have frequently seen CS reps respond to posts here so they're aware of it and it's at least semi monitored. I am not aware of any sort of official Reddit team internally, and half the time Reddit links are blocked on the work network (even things that would objectively be work-related). I can say my team doesn't monitor this subreddit, although I have searched for posts related to our service just out of curiosity",
          "score": 33,
          "created_utc": "2026-02-02 06:40:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34d7bl",
              "author": "Living_off_coffee",
              "text": "Interestingly, it's Reddit that blocks Amazon's VPN IP range, it's not Amazon that blocks Reddit.\nI assume they've tried to block bots accessing Reddit from EC2 or something, but used a broad range that included our corporate IPs.\n\nIf you disconnect from the VPN it's usually fine on office WiFi.",
              "score": 27,
              "created_utc": "2026-02-02 07:54:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34ude4",
                  "author": "Drumedor",
                  "text": "Usually it is also works fine as long as you are logged in to your Reddit account, if I am logged out I can't access from company network (blocked by Reddit), but if I am logged in I can access without problems.",
                  "score": 6,
                  "created_utc": "2026-02-02 10:39:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o353mfv",
          "author": "Jeoh",
          "text": "Two of the three mods work for AWS. There are frequent posts by AWS's Support Team here.",
          "score": 12,
          "created_utc": "2026-02-02 11:58:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37fbla",
          "author": "thamesr",
          "text": "My comment about elastic beanstalk being not good got deleted before they took the entire post down lol",
          "score": 10,
          "created_utc": "2026-02-02 19:07:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o39dq40",
              "author": "bofkentucky",
              "text": "Were you just shitposting or did you have a legitimate gripe about the product.    It is a PaaS, you have to live in its guardrails, you have to wait for them to bless platform/runtime updates, they're at the mercy of the Amazon Linux team delivering timely updates, etc, et al.  That being said as a happy beanstalk customer, we've learned adapt to it for over a decade at this point.  It does what we ask, when we ask.\n\nNow apprunner, a product they're letting rot publicly, that's worthy of derision.",
              "score": 0,
              "created_utc": "2026-02-03 00:59:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3c7pbg",
                  "author": "thamesr",
                  "text": "I was replying to a comment from a user that was saying Amplify and Beanstalk were bad. I was asking what the alternative to Amplify was because I'm not a frontend engineer and I don't really know what else to use. I ended my post agreeing with the dislike for Beanstalk, so maybe that was it?",
                  "score": 2,
                  "created_utc": "2026-02-03 13:34:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3gdgon",
          "author": "ImCaffeinated_Chris",
          "text": "I am AWS, and I speak for it. I own everything AWS related. \n\nI'm currently going thru your data and turning every use of \"the\" to \"sassy\". \n\nEnjoy sassy fun, with all sassy services.\n\nObviously I'm joking.... I have ai agents changing your data, not me! \n\nEdit: Also Melinda in support ate the last of the doughnuts in the break room, not Steve!",
          "score": 0,
          "created_utc": "2026-02-04 01:52:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qt3gne",
      "title": "AWS Bedrock in production: anyone else finding it a mixed bag?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qt3gne/aws_bedrock_in_production_anyone_else_finding_it/",
      "author": "Different-Use2635",
      "created_utc": "2026-02-01 16:45:45",
      "score": 38,
      "num_comments": 27,
      "upvote_ratio": 0.69,
      "text": "Been using AWS Bedrock for a GenAI project at work for about six months now, and honestly, it's been... interesting. I came across this guide by an Amazon Applied Scientist (Stephen Bridwell, if you're curious) who's built systems processing billions of interactions, and it got me thinking about my own setup.\n\nFirst of, the model access is legit – having Claude, Llama, Titan all in one place is convenient. But man, the quotas... getting increases was such a hassle, and testing in production because nonprod accounts get nada? Feels janky. The guide mentions right-sizing models to save costs, like using Haiku for simple stuff instead of Sonnet for everything, which I totally screwed up early on. Wasted a bunch of credits before I figured that out.\n\nSecurity-wise, Bedrock's VPC endpoints and IAM integration are solid, no complaints there. But the instability... random errors during invocations, especially around that us-east-1 outage period. And the documentation? Sometimes it's just wrong, spent hours debugging only to find the SDK method didn't work as advertised. \n\nHmm, actually, let me backtrack a bit – the Knowledge Bases for RAG are pretty slick once you get the chunking right. But data prep is key, and if your docs are messy, it's gonna suck. Learned that the hard way after a few failed prototypes.\n\nCost optimization tips from the guide were helpful, like using batch mode for non-urgent jobs and prompt caching. Still, monitoring token usage is a pain, and I wish the CloudWatch integration was more intuitive.\n\nWhat's been your experience? Anyone else hit throttling issues or found workarounds for the quotas madness? Or maybe you've had smoother sailing – curious what models you're using and for what projects. \n\nAlso, if you've tried building agents or using Multi-Agent Collaboration, how'd that go? I heard it's janky, but I haven't done in yet.\n\nJust trying to figure out if I'm missing something or if Bedrock's just inherently fiddly for production GenAI.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1qt3gne/aws_bedrock_in_production_anyone_else_finding_it/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o30qoqm",
          "author": "rhit_engineer",
          "text": "Did you actually write this? I've never met a human who would ever write, \"hmm, actually, let me backtrack a bit\" much less in a self contained post. Reeks of AI ",
          "score": 68,
          "created_utc": "2026-02-01 19:06:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30xc1x",
              "author": "nemec",
              "text": "the whole thing was written with AI. Guess he figured out how to use Bedrock after all lol",
              "score": 50,
              "created_utc": "2026-02-01 19:37:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o33u5e5",
              "author": "caughtinthought",
              "text": "The sentence followed by question cadence as well, brutal",
              "score": 6,
              "created_utc": "2026-02-02 05:13:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3a0jzz",
                  "author": "ghostmastergeneral",
                  "text": "Sentence followed by question cadence?",
                  "score": 1,
                  "created_utc": "2026-02-03 03:09:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zwrvg",
          "author": "pyrospade",
          "text": "Looks like most of your criticism comes from limits snd throttling which unfortunately you’ll get from any AI service or cloud provider unless you are a big money spending account because there’s simply not enough capacity for everyone in the world right now with all the LLM madness",
          "score": 52,
          "created_utc": "2026-02-01 16:51:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zyedg",
              "author": "Lendari",
              "text": "I think what would help is more warnings that you need LLM request traffic shaping and high latency tolerance in your agent architecture.\n\nThe frustration is when working prototypes fail when run at scale. Much of the promise of AWS is that they limit this risk of launch failures or scalability problems. \n\nSo customers get surprised.",
              "score": 14,
              "created_utc": "2026-02-01 16:58:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o305jjg",
          "author": "jeekp",
          "text": "I’m building on Bedrock as well. Using Flows is nice for peppering AI inference into otherwise deterministic workloads which keeps costs way down vs having an Agent orchestrate it all. \n\nThe real superstar of AWS RAG for me is the ultra-low cost S3 vector DB. It was a bit disappointing to have to build a custom lambda orchestrator for my agent to utilize multiple knowledge bases with chunk reranking, though. Feels like that should be out-of-the-box.",
          "score": 7,
          "created_utc": "2026-02-01 17:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30081v",
          "author": "iolairemcfadden",
          "text": "I'm very happy with Bedrock Nova Lite for photo descriptions and image captioning.  But thats a very simple use case.",
          "score": 4,
          "created_utc": "2026-02-01 17:07:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o302chg",
          "author": "legendov",
          "text": "Try openrouter if you want a lot of models out of AWS ecosystem",
          "score": 4,
          "created_utc": "2026-02-01 17:16:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31zx58",
              "author": "TheGrich",
              "text": "Replicate is pretty solid in terms of model selection too.  But idk on their capacity.",
              "score": 2,
              "created_utc": "2026-02-01 22:46:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o30fxwe",
          "author": "weirdbrags",
          "text": "when you say bedrock, do you mean bedrock agents (via lambda)? or do you mean bedrock agentcore runtime (or any of the other primitives)?\n\nasking because my team is in the process of migrating production agents from the old to the new and it’s been like night and day going all in on agentcore. it’s not without its bumps considering it’s still pretty early. but for the most part it’s legit. excited to see where they take it as it matures.",
          "score": 2,
          "created_utc": "2026-02-01 18:18:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30j5c2",
          "author": "Sad_Treacle_9307",
          "text": "It sounds like a lot of your pain comes from missing some platform fundamentals, so I’d politely suggest going back to the core AWS Bedrock documentation and especially reading “AWS Bedrock: A Complete Guide from an Amazon Applied Scientist Who Built Production GenAI Systems” by Stephen Bridwell, which covers many of these exact pitfalls and best practices from real production experience.",
          "score": 2,
          "created_utc": "2026-02-01 18:32:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31xgfh",
          "author": "binarystrike",
          "text": "In my personal experience, I have seen people do cool things with it, however the fact that models are based in a variety of regions, it creates all kinds of issues if you limit requests to certain regions with SCPs. We have also found it to be kind expensive and using other AI tools with APIs / tokens to be a far more cost effective way to use AI in AWS.",
          "score": 2,
          "created_utc": "2026-02-01 22:33:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2zy8sr",
          "author": "AWSSupport",
          "text": "Hello,\n\nWe're always looking to improve our services where we can. I've passed your feedback internally to our Bedrock team for review.\n\n\\- Marc O.",
          "score": 3,
          "created_utc": "2026-02-01 16:58:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o305mha",
          "author": "HKChad",
          "text": "I find it best to test locally, work out your cleaning, embedding/chunking issues at a small scale. Then run a slightly more aggressive test in prod before trying to go full scale. There’s a ton of free models you can run on a decent laptop, just setup your test to run overnight. Best to catch issues early before you burn a bunch of credits debugging. Nothing you mentioned is really bedrock specific.",
          "score": 1,
          "created_utc": "2026-02-01 17:31:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30sy8q",
          "author": "IntermediateSwimmer",
          "text": "I work for a large enterprise so the limits, while annoying, do get increased pretty fast. It’s frustrating though, because we don’t have the same issue with OpenAI and Google.\n\nMy biggest issue with bedrock so far has mostly been with the fact that companies like Anthropic are making specific APIs that Bedrock doesn’t support. So it just makes no sense to use Bedrock if I can go direct to Anthropic and get the full feature set.",
          "score": 1,
          "created_utc": "2026-02-01 19:16:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3154hq",
          "author": "wlonkly",
          "text": "I think the primary purpose of Bedrock is to let companies point their software at a vendor with whom they already have POs, discounts, legal agreements, security policies, etc.  (Which is incredibly useful, mind you! But it's not really a _technical_ advantage, especially for one person.)",
          "score": 1,
          "created_utc": "2026-02-01 20:14:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o315yr9",
          "author": "Human-Job2104",
          "text": "Great tool, but I have a few complaints.\n\nThrottling issues are real, and a pain in the rear to get fixed, for us, a better model was released before we were able to get the increase through, so we dropped the case. But, that experience is consistent with many bigger service quote changes, and for regulated stuff like SES and Connect. \n\nDocumentation and examples are lacking heavily, but that's consistent with other tools. Some of the docs are hard to follow, and it always leaves me wanting more. AI has been a huge help there though.\n\nI was disappointed with its limited Terraform support this past year as well.\n\nOverall, it still rocks, and it's going to get better as it matures and chip capacity scales.",
          "score": 1,
          "created_utc": "2026-02-01 20:19:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o327d2b",
          "author": "dustinpdx",
          "text": "My company built a small service that wraps it and all of our products and features just use it via that service. That means we can address any shortcomings or difficulties once and benefit from it in every product. It works very well for us and we have a _huge_ amount of customers using these features on a daily basis.",
          "score": 1,
          "created_utc": "2026-02-01 23:27:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35nn5h",
          "author": "life_like_weeds",
          "text": "Have you looked into inference profiles for monitoring and reporting needs?",
          "score": 1,
          "created_utc": "2026-02-02 14:06:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31wo5n",
          "author": "PhilosophyOutside132",
          "text": "Bedrock is honestly terrible. \n\nI have a 5M TPM quota and still get a ridiculous amount of random errors, even with very low usage (well under 100k TPM).\n\nThe amount of time I waste doing gymnastics to work around things the SDK simply doesn’t support is insanely expensive. I constantly hit agent/human message sequence errors that other providers automatically handle or silently tolerate.\n\nOn top of that, last month AWS locked my account because I used Bedrock in another region to balance token load. Two days later they unlocked it without me changing anything. Just an apology for “the inconvenience” of my product being down for 48 hours.\n\nI have days where 80% of requests fail. The product basically runs on retries.\n\nIt works “fine” while you’re developing and testing. But once you go to production, good luck. AWS won’t help you even if you pay for Premium Support. The answers are always some variation of “that’s our policy” or “turn this undocumented knob and try again.”\n\nUse it at your own risk.",
          "score": 1,
          "created_utc": "2026-02-01 22:29:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o306wdl",
          "author": "ItisGonnaBeAlright",
          "text": "I left AWS all together and moved to Google Cloud. Vertex AI is so much more efficient for my case use. I spent almost a month trying to tweak things incrementally and could just never get the parsing function I wanted to work. Embarrassingly, I looked at Vertex and it was solved in under 15 minutes. 🤦🏻‍♂️",
          "score": 0,
          "created_utc": "2026-02-01 17:37:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2zypsq",
          "author": "jeremy_foobar",
          "text": "I have experienced a bit with AgentCore and S3 Vectors recently.  Here aer some details: [https://github.com/jeremylem/cloud\\_knowledge\\_base](https://github.com/jeremylem/cloud_knowledge_base)",
          "score": -2,
          "created_utc": "2026-02-01 17:00:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o312b0v",
          "author": "HelloVap",
          "text": "Slop",
          "score": -2,
          "created_utc": "2026-02-01 20:01:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsc5xt",
      "title": "EKS Users: What does your \"Day 0\" bootstrap stack look like?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qsc5xt/eks_users_what_does_your_day_0_bootstrap_stack/",
      "author": "srwalker101",
      "created_utc": "2026-01-31 19:44:53",
      "score": 32,
      "num_comments": 12,
      "upvote_ratio": 0.91,
      "text": "Hi everyone,\n\nI’m looking to gather data on what a \"standard\" production EKS setup looks like in 2026 to improve the accuracy of our EKS emulation.\n\n_Disclosure_: I lead a team at LocalStack. We are working on making our EKS emulation accurate enough to support real-world platform engineering workflows, and we want to ensure we prioritise the add-ons and patterns people actually use.\n\nI'd love to know what your \"must-have\" cluster bootstrap looks like. For example:\n\n* **IaC**: Terraform, Pulumi, eksctl, or Crossplane?\n* **Ingress/Network**: AWS Load Balancer Controller, Nginx, Istio, Linkerd?\n* **GitOps**: ArgoCD, Flux, or CI-push?\n* **Critical Add-ons**: ExternalDNS, Cert-Manager, Karpenter, Cluster Autoscaler?\n* **Storage**: EBS CSI, EFS CSI?\n\nEven a quick bulleted list of your \"Day 0\" installs would be incredibly helpful to help us build a better offline testing experience.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "technical question",
      "permalink": "https://reddit.com/r/aws/comments/1qsc5xt/eks_users_what_does_your_day_0_bootstrap_stack/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o2ugvyw",
          "author": "InfiniteAd86",
          "text": "We use TF for provisioning EKS (follow Blue/green deployment during upgrades). Ingress we use ALB controller. Gitops: ArgoCD . HPA (horizontal pod autoscaler) for addons along with default vpc cni etc.. we don’t have storage requirement as we host stateless APIs.",
          "score": 4,
          "created_utc": "2026-01-31 19:59:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ufqww",
          "author": "bryantbiggs",
          "text": "Is that a commonly requested feature? It’s just Kubernetes and there’s already kind and other dev tools",
          "score": 3,
          "created_utc": "2026-01-31 19:54:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uy12n",
              "author": "iamaperson3133",
              "text": "You can say the same thing about QEMU and Open stack EC2 emulation. People want to test the code targeting the exact same apis as in prod, not test against analogues because the code will be different. \n\nFor local stack eks, I imagine that IaM role-based access to supporting aws services is a large area one may want to test not against AWS.",
              "score": 3,
              "created_utc": "2026-01-31 21:24:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2v6f0c",
              "author": "srwalker101",
              "text": "You are right in that for application developers it could be possible to emulate locally with a local k8s distribution (kind, k3d etc.). But for cluster admins there is no real way to emulate the setup and configuration of EKS locally. \n\nWhere we want to focus is any tooling that users install into the cluster that integrate with AWS. For example we have been working on supporting the AWS APIs that the AWS Load Balancer Controller uses to provision ELBv2 load balancers.\n\nAnd as another user has said, emulating IAM integration is a big part of that. We recently implemented support for IRSA and EKS pod identities.",
              "score": 2,
              "created_utc": "2026-01-31 22:05:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zv8v5",
          "author": "DPRegular",
          "text": "Full disclosure, I had Claude write up below summary. It's all correct, though.\n\n**Our Day 0 EKS Bootstrap (Managed K8s Provider)**\n\nWe're a managed services provider running EKS for customers. Here's our Day 0 stack:\n\n**IaC:** OpenTofu + Terragrunt (stack composition pattern with reusable modules)\n\n**CNI/Network:**\n\n* Cilium (eBPF-native routing, ENI mode with prefix delegation)\n* Hubble for network observability\n* Gateway API support\n\n**Ingress:**\n\n* AWS Load Balancer Controller (NLB → Ingress Controller)\n* CloudFront + WAFv2 in front for edge protection\n* Route53 for DNS, ACM for certs\n\n**Management:** Rancher (multi-cluster management)\n\n**Critical Add-ons:**\n\n* Karpenter (node provisioning + consolidation)\n* Cert-Manager\n* External-Secrets (→ AWS Secrets Manager)\n* Kyverno (policy enforcement, Pod Security Standards)\n* KEDA + VPA + Metrics Server\n* node-local-dns (CoreDNS caching)\n\n**Storage:**\n\n* EBS CSI (gp3, KMS-encrypted)\n* EFS CSI (dynamic provisioning with access points)\n\n**Observability (LGTM):**\n\n* Grafana + Loki + Tempo + Mimir\n* OpenTelemetry Operator + Collectors\n* Prometheus (kube-prometheus-stack)\n\n**AWS Services We Depend On:**\n\n* EKS, EC2, ELB/NLB\n* Route53, ACM, CloudFront, WAFv2\n* EBS, EFS, S3\n* KMS, Secrets Manager\n* CloudWatch (logs)\n* VPC Endpoints (ECR, S3, EC2, logs, monitoring)\n* IAM (IRSA via OIDC)\n* SSM (node access)\n* ECR (pull-through cache)\n\nFor LocalStack specifically - the AWS integrations that would matter most to us for testing:\n\n1. **AWS LB Controller** → ELBv2 APIs\n2. **External-Secrets** → Secrets Manager\n3. **Karpenter** → EC2 Fleet/Spot APIs\n4. **IRSA/Pod Identity** (you mentioned you have this)\n5. **EFS/EBS CSI** → Storage APIs\n6. **Route53** → DNS record management",
          "score": 3,
          "created_utc": "2026-02-01 16:44:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2uly6w",
          "author": "doublet00th",
          "text": "* IaC: Terraform\n* Ingress: AWS LBC, Linkerd\n* GitOps: Python scripting that pushes manifests into repo\n* Add-Ons: External DNS, Karpenter, Fluent Bit, Grafana (on cluster)\n* Storage: EBS CSI\n\nA few of my clusters also run Cilium for L7 policy enforcement",
          "score": 2,
          "created_utc": "2026-01-31 20:24:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2vt58g",
          "author": "hexfury",
          "text": "IAC: Terraform + Terragrunt\nAutoscaling: karpenter\nIngress: Istio\nStorage: EBS CNI + EFS CNI\nDNS: Coredns\nTelemetry: Grafana, Tempo, Loki, Prometheus\nOIDC: Keycloak + Gatekeeper",
          "score": 2,
          "created_utc": "2026-02-01 00:07:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31644h",
          "author": "dubl_x",
          "text": "- Terraform, \n- Traefik, \n- Argo, \n- all of those add ons you mentioned plus kyverno, plus a logging/monitoring stack\n- EBS.",
          "score": 2,
          "created_utc": "2026-02-01 20:19:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2w9l2q",
          "author": "Aware_Obligation5330",
          "text": "What about CNI?",
          "score": 1,
          "created_utc": "2026-02-01 01:42:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33fevb",
          "author": "SDplinker",
          "text": "It’s incredible how much shit you have to manage to run EKS - I kind of hate it",
          "score": 1,
          "created_utc": "2026-02-02 03:36:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d4aj1",
              "author": "srwalker101",
              "text": "What would make it easier do you think?",
              "score": 1,
              "created_utc": "2026-02-03 16:20:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qr0w4h",
      "title": "Has anyone noticed a significant slowdown in AWS provisioning recently? (Terraform/RDS)",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qr0w4h/has_anyone_noticed_a_significant_slowdown_in_aws/",
      "author": "Gluhy",
      "created_utc": "2026-01-30 09:31:20",
      "score": 28,
      "num_comments": 10,
      "upvote_ratio": 0.9,
      "text": "Hi everyone,\n\nI'm curious if anyone else has experienced a noticeable degradation in provisioning times on AWS over the last few months.\n\nI've been noticing a trend where resources take significantly longer to spin up compared to about 3 months ago. For example, restoring an RDS database from a snapshot using Terraform used to take consistently around **20 minutes**. Lately, the exact same operation (same configuration, same snapshot size) is taking upwards of **45 minutes**.\n\nIt's not just isolated to RDS either; I'm seeing similar delays across other services during `terraform apply`.\n\n**Context:**\n\n* **IaC:** Terraform\n* **Region:** eu-central-1\n* **Timeframe:** Comparison between \\~3 months ago vs. now.\n\nHas anyone else observed this? I'm trying to figure out if this is an account-specific issue (throttling/quotas?), a specific region issue, or if the control plane performance has actually degraded globally.\n\nThanks",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1qr0w4h/has_anyone_noticed_a_significant_slowdown_in_aws/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o2ku2nj",
          "author": "woodje",
          "text": "For RDS remember a restore has to roll through the transaction log, so size isn’t necessarily the exact deciding factor - it’s number of transactions. So if your DB is busier than it was 3 months ago that may explain it.",
          "score": 24,
          "created_utc": "2026-01-30 10:14:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ktmtd",
          "author": "oneplane",
          "text": "No, not really. Looking at apply metrics from the last year, not much has changed.",
          "score": 11,
          "created_utc": "2026-01-30 10:10:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ku2ou",
          "author": "Lazy_1207",
          "text": "We got a notification that we need to upgrade Aurora MySQL from 3.11.0 to 3.11.1 until 8th of February to solve some issues with database cluster creates and restores taking longer.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraMySQLReleaseNotes/AuroraMySQL.Updates.3111.html\n\nNot sure if this is relevant to RDS also or only Aurora.",
          "score": 5,
          "created_utc": "2026-01-30 10:14:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33q7io",
          "author": "nekokattt",
          "text": "wouldnt surprise me if this is an \"optimisation\".\n\nin the same way lambda hyperplane ENIs take upwards of an hour to deprovision, so good luck destroying that VPC with a Lambda in it before lunch",
          "score": 1,
          "created_utc": "2026-02-02 04:45:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nnosh",
          "author": "znpy",
          "text": "Not sure if it's a slowdown, but: yesterday I applies some patches to a few elasticache clusters (primary/replicas) and i was surprised to see how long did they take: about 30-45 minutes per cluster (three instances total).\n\nPlease keep in mind that i have no persistant data, so the \"update\" is really \"shut down the old version of the software and turn on the new one\". Even deploying a new node from scratch, it's too much time.",
          "score": 1,
          "created_utc": "2026-01-30 19:13:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oaw0f",
          "author": "usernamechecksout8",
          "text": "Did you check the cloud trail for throttling during `terraform apply`?",
          "score": 0,
          "created_utc": "2026-01-30 21:02:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2owpq9",
          "author": "ViolinistRemote8819",
          "text": "Did you try a manual restore and see how the performance looks? If you have an older snapshot, maybe try restoring that and check how long it takes. Also, are you on the latest Terraform version?",
          "score": 0,
          "created_utc": "2026-01-30 22:47:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2wddve",
          "author": "ObjectUsual77",
          "text": "It takes awhile (maybe 10 mins) for transit gateway links to be established and we had a running joke that the api call was actually ringing a bell at the data centre to have a technician run patch cords to setup the link! 😅",
          "score": 0,
          "created_utc": "2026-02-01 02:05:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ojiwl",
          "author": "damnitdad",
          "text": "Great question!",
          "score": -3,
          "created_utc": "2026-01-30 21:42:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1quwq1g",
      "title": "AWS Blogs - What Are Your Favorites?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1quwq1g/aws_blogs_what_are_your_favorites/",
      "author": "gobullz",
      "created_utc": "2026-02-03 16:34:23",
      "score": 19,
      "num_comments": 10,
      "upvote_ratio": 0.91,
      "text": "Hey Everyone, just wanted to see what some of your favorite AWS blogs were that have helped you out? Do you guys like blog posts with deep technical information or higher level architecture focused information?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1quwq1g/aws_blogs_what_are_your_favorites/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o3dw8ii",
          "author": "mountainlifa",
          "text": "Deep technical. Don't need more high level marketing nonsense ",
          "score": 25,
          "created_utc": "2026-02-03 18:27:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dmphh",
          "author": "alex_aws_solutions",
          "text": "All things distributed- from Werner Vogels himself\nLast Week in AWS",
          "score": 20,
          "created_utc": "2026-02-03 17:45:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g0kej",
          "author": "TheGABB",
          "text": "So many of the new blogs are either regurgitating documentation, marketing material, or pure AI slop. A few gems here and there, but they publish SO much now that it’s a needle in a haystack. Heard from our SA that technical roles have constantly increasing KPIs on blog and other technical writing publications, so I’m not surprised",
          "score": 9,
          "created_utc": "2026-02-04 00:40:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3h982y",
              "author": "tnstaafsb",
              "text": "It's true.  They employ thousands of SAs and yet somehow expect all of them to be constantly churning out innovative content regularly.  Helping your customers and helping your sales team isn't enough to even get a satisfactory rating anymore.  If you're not producing several artifacts a year, you're cooked.",
              "score": 3,
              "created_utc": "2026-02-04 05:06:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3g0pws",
              "author": "TheGABB",
              "text": "I used to really enjoy what the SaaS factory team was publishing. Technical and always on point for SaaS developers",
              "score": 2,
              "created_utc": "2026-02-04 00:41:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3hphyc",
              "author": "ThigleBeagleMingle",
              "text": "Yup. I was regular on few of their channels / tech editor on +100 artifact’s. We internally debated if SAs should be measured on blog publications.\n\nBut it’s part of performance metrics that influences compensation and you get what measure.\n\nUnfortunately that was 4 years ago and everything old is new again",
              "score": 1,
              "created_utc": "2026-02-04 07:17:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3fure2",
          "author": "Zenin",
          "text": "You guys have time to read blogs?  I barely have time to listen to Last Week in AWS.",
          "score": 7,
          "created_utc": "2026-02-04 00:09:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jl07d",
              "author": "AWS_Chaos",
              "text": "I read Last Week in AWS in batches. I'm currently 7 weeks behind :)",
              "score": 2,
              "created_utc": "2026-02-04 15:24:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3g5jlb",
          "author": "jed_l",
          "text": "Love this thread. Please keep it coming. If possible please post your role.",
          "score": 1,
          "created_utc": "2026-02-04 01:08:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvid5g",
      "title": "How to evaluate if hybrid AWS GCP setup improves cost and resilience",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qvid5g/how_to_evaluate_if_hybrid_aws_gcp_setup_improves/",
      "author": "SlightReflection4351",
      "created_utc": "2026-02-04 07:42:06",
      "score": 17,
      "num_comments": 16,
      "upvote_ratio": 0.95,
      "text": "spent the last month designing a hybrid AWS/GCP setup that optimizes for cost and resilience. used GCP for our data pipeline and ML workloads, AWS for application hosting and compute. included proper failover, cross region redundancy, the whole thing.  \npresented it yesterday and got the usual questions. \"isn't this too complex?\" \"what if something breaks between clouds?\" \"why not just stay on AWS?\"\n\ni have good answers for all of this but now i'm wondering if i'm overcomplicating things. maybe the single cloud simplicity is worth the vendor lock in and higher costs? or maybe i'm just second guessing myself because i got pushback.  \nhow do you know when multi cloud is actually the right call versus just being architecture for the sake of architecture?",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1qvid5g/how_to_evaluate_if_hybrid_aws_gcp_setup_improves/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o3hvrdw",
          "author": "wy100101",
          "text": "You are probably over complicating things. It sounds like you are probably less robust now since you are now fully dependent on 2 providers instead of one.\n\nA more resilient multi cloud strategy would be where everything keeps working if either go down.",
          "score": 12,
          "created_utc": "2026-02-04 08:13:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ibxon",
              "author": "LemonFishSauce",
              "text": "Totally concur. OP is doubling their risk. Now they are impacted if any one of the two clouds are down.",
              "score": 6,
              "created_utc": "2026-02-04 10:45:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3htzpi",
          "author": "UnluckyTiger5675",
          "text": "Correct me if I’m wrong but you still couldn’t “lose” one cloud, right? Should AWS have a multi-region multi-service outage (again), is your production still not down? It sounds like you’re as resilient in AWS as you can get, and I don’t know your app(s), but what about such an outage at google? How integral to your apps are the ML/datapipe components? Do they have a different RTO/RPO?",
          "score": 10,
          "created_utc": "2026-02-04 07:57:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hxljb",
              "author": "EconomistAnxious5913",
              "text": "In this case a country/or region, could ban a cloud provider from a different geo.\n\ne.g. say the EU could ban US cloud providers, even today there are lot of restrictions on using US cloud providers vs EU based cloud providers for certain EU sponsored projects.\n\nto add, this specific problem isn't going to be solved AWS v/s GCP",
              "score": -4,
              "created_utc": "2026-02-04 08:30:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3hu6i1",
          "author": "behusbwj",
          "text": "Multicloud if:\n* a different platform offers a unique solution that reduces complexity more than multi-cloud increases complexity\n* a different platform offers a price that is lower than the cost of the time spent managing additional complexity (billing, onboarding, maintenance, operations etc will be forever costs).\n\nBoth of those points are rare to fulfill. There are few problems that can’t be solved on either platform.",
          "score": 8,
          "created_utc": "2026-02-04 07:59:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ishom",
          "author": "invidiah",
          "text": "Multicloud is always more expensive because of the maintenance costs. And if you need to comply with some regulation rules or pass any kind of data protection audit that would be a hell.\n\n  \nHave you included already egress costs into your calculations?",
          "score": 3,
          "created_utc": "2026-02-04 12:51:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iuxu2",
          "author": "ReturnOfNogginboink",
          "text": "Agree or disagree, this is worth reading: https://www.lastweekinaws.com/blog/multi-cloud-is-the-worst-practice/",
          "score": 3,
          "created_utc": "2026-02-04 13:06:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3huahd",
          "author": "inphinitfx",
          "text": "Simply, when the real benefits outweigh the real downsides. That may be that there is a capability that one cloud platform provides that is fundamentally better for your use case, while other cloud platforms offer similar advantages in other areas. \n\nFrom your description, I'm assuming you've got different workloads in each cloud platform, rather than a single workload that spans both (although the question of what happens if something breaks between clouds kind of makes this one murky). This can be viable, but you need to ensure you've looked at all the relevant aspects - TCO (including operational overheads form complexity), security impact, etc. I've seen plenty of architects overlook these, and their $50k savings is eaten up by $100k in additional operational overheads to string it together.",
          "score": 2,
          "created_utc": "2026-02-04 08:00:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iwx3v",
          "author": "yeochin",
          "text": "Having been someone who has made these calls before (as the final decision maker) - multi-cloud hybrids make no sense financially, resiliency-wise, and operationally. If you're going to use Azure, AWS, or GCP - stick to one.\n\nI am skeptical if you are truly calculating lower-costs. Usually:\n\n1. The cost analysis underestimates running Engineering costs required for oncall, maintenance and support. Most teams struggle with distributed systems. Observability of systems across clouds and 3P vendors becomes much more complex and painful.\n2. Temporary discounts provided by the 2nd cloud provider are erroneously factored in. You're making a long-term decision that costs significant money to unwind. Naiive business/tech leaders factor in the discounted run-rate which is a fundamental mistake.\n3. Data-transfer costs (realized and opportunity costs). Cloud providers often eat the costs of intra-network data transfer because its folded into the cost of setting up and maintaining compute. They are forced to charge some degree of ingress/egress costs because that is a cost born from another party (namely ISPs). The data-costs add up in both dollars and time (the amount of data that can move between compute nodes is several times greater than traversing the public internet).\n\nFor most businesses - going back to a hybrid on-prem is a wiser business and architectural decision for the same CapEX and OpEX you would need to operate a hybrid cloud. If you're trying to avoid vendor lock-in, going into a hybrid on-prem solution is a better architectural choice.\n\n* You will be forced to modularize your systems in a way that can be executed either on-prem or in-cloud.\n* You'll select vendors that have built solutions that keep you in the drivers-seat with how you use them. You'll find yourself \"owning\" what you purchase instead of \"renting it\" - meaning that you'll start to really assess the cost/value of holding a subscription.\n* You'll be intentional with your data-sovereignty ; leading to less headaches and less likely you get hit with the regulatory schtick which is due to whack a few folks in the upcoming years as the world starts to decouple with the American cloud providers.\n\nOnboarding a second cloud does not structurally reduce your running costs - it increases it. If you're going to increase it - its better to achieve real vendor independence.",
          "score": 2,
          "created_utc": "2026-02-04 13:18:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hy7dz",
          "author": "PokeRestock",
          "text": "What exactly does GCP have that AWS doesnt? For disaster recovery I would think Azure to be better, and if youre looking for VM specifics again Azure could be beneficial. \n\nBut Im not seeing how youre saving money with GCP in the mix. Also not seeing how you would share data between the two where you now have a separated monolith pipeline.",
          "score": 1,
          "created_utc": "2026-02-04 08:36:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jga55",
              "author": "Dull-Engineer-1757",
              "text": "BigQuery is magic ",
              "score": 1,
              "created_utc": "2026-02-04 15:01:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ixd4a",
          "author": "xxwetdogxx",
          "text": "I'd also consider data transfer costs, anything exiting one cloud provider to the other will be charged where typically within a cloud it wouldn't be, unless maybe it's cross-region",
          "score": 1,
          "created_utc": "2026-02-04 13:21:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hyhhx",
          "author": "Timely-Dinner5772",
          "text": "Failover across clouds sounds hypothetical until your primary region becomes the bottleneck.",
          "score": 0,
          "created_utc": "2026-02-04 08:39:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ic7fc",
          "author": "LemonFishSauce",
          "text": "If it’s me, I will read up on which companies were affected by the previous AWS outage. Then I’ll ask myself if my business is bigger than those companies.",
          "score": 0,
          "created_utc": "2026-02-04 10:47:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtrz8d",
      "title": "New APN partner here. What should we actually be doing?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qtrz8d/new_apn_partner_here_what_should_we_actually_be/",
      "author": "imsankettt",
      "created_utc": "2026-02-02 10:56:56",
      "score": 16,
      "num_comments": 10,
      "upvote_ratio": 0.79,
      "text": "My company recently joined the **AWS Partner Network (APN)** and paid the annual **$2,500 subscription fee**. As part of the signup, we linked our company’s AWS account to the APN account.\n\nWe’re a VoIP-based company providing VoIP solutions, and now I’m trying to understand how to actually make use of APN in a meaningful way. I know the high-level goal of APN is to help partners accelerate AWS-related sales, but beyond that, things feel a bit vague.\n\nSome questions I’m hoping the community can help with:\n\n* How do companies typically start using APN after joining?\n* What should we focus on first to get real value out of it?\n* Are there AWS contacts (Partner Managers, programs, etc.) we should be engaging with?\n* Is this something AWS Support helps with, or does it require reaching out through a different channel?\n* For anyone who started APN from scratch, what did your early steps look like?\n\nAny guidance, lessons learned, or pointers to the right AWS teams would be greatly appreciated.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1qtrz8d/new_apn_partner_here_what_should_we_actually_be/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o354n9o",
          "author": "clandestine-sherpa",
          "text": "Reach out to your aws account team they can help you understand all this. If you don’t know who they are throw in a support ticket requesting contact from your support account team.",
          "score": 14,
          "created_utc": "2026-02-02 12:06:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o354tm6",
              "author": "imsankettt",
              "text": "Thank you",
              "score": 3,
              "created_utc": "2026-02-02 12:07:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o356kur",
          "author": "safeinitdotcom",
          "text": "Hey there, congrats on becoming a partner and welcome! We've been a partner since 2022 and I know how overwhelming things might seem.\n\nYou first need to decide which path you want to take - [see here](https://aws.amazon.com/partners/paths/). As a VoIP solutions provider, it seems the software path might be the one to start with. i.e: build your solution on top of AWS and distribute it to customers through AWS Marketplace. Either as an AMI or SaaS offering. Of course, you can branch out to other paths as you grow.\n\nTypically you want to focus on getting as much funding as possible. Each partner paths comes with its own access to various funding programs - POC, MDF, Sandbox, etc. I'd suggest studying the official docs from partner central [here](https://partnercentral.awspartner.com/partnercentral2/s/guides).\n\nNormally, an AWS representative should've made introductions already. However, AWS went through some tough...restructuring exercises and people might be focused on something else. Also, per capita workload has increased tremendously in the past 2 years so there's a good chance you're going to fall through the cracks. If somebody does get through to you, play nice and make your offerings as clear as possible. They have KPIs and need your help to achieve them. If somebody doesn't introduce themselves in the next 7-10 days, reach out to support and ask for an introduction.\n\nIn our early days we were lucky enough to have somebody in AWS who pushed us - \"get to the next partnership tier\", \"get recognized through service delivery certifications and competencies\", \"get your GTM strategy in place\", etc. I hope you get somebody like that as well.\n\nIf you're struggling, feel free to DM me and we can have a chat.\n\nGood luck and enjoy it. It was definitely worth it for us.",
          "score": 10,
          "created_utc": "2026-02-02 12:20:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36b1hm",
              "author": "imsankettt",
              "text": "Thank you for the detailed response, I will reach out to you if needed! :)",
              "score": 3,
              "created_utc": "2026-02-02 16:03:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o36f4x7",
          "author": "CloudForecast",
          "text": "Tthe next steps are pretty vague and confusing - probably on purpose. We had to figure this out the hard way at CloudForecast.\n\nThe $2.5k gets you in the door, but your real next step is becoming a Validated AWS Partner. That's what opens up more opportunities for you and show you're somewhat serious. The main hurdle with that is getting AWS FTR certified, plus you'll need to maintain some ACE opportunity items in the partner network. They've updated the requirements since I went through it, so I can't give you exact details on that part.\n\nEven after you're validated, you need to get into another program called ISV Accelerate to be considered for co-sell. You'll also want your product on AWS Marketplace with proper AWS hosting.\n\n[https://aws.amazon.com/partners/foundational-technical-review/](https://aws.amazon.com/partners/foundational-technical-review/)\n\nIt's basically a series of mini-bosses you have to fight through. And honestly, opportunities don't just drop in your lap after all that. The co-sell motion is pretty bureaucratic and resource-intensive to even start understanding how it works. After you get to that point, the partner team will want to push you towards more specialized certifications relevant to your industry and what you're trying to do.",
          "score": 4,
          "created_utc": "2026-02-02 16:22:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35ivxt",
          "author": "Infamous_Rush601",
          "text": "VoIP- in this case does that you mean you're working with Amazon Connect?",
          "score": 3,
          "created_utc": "2026-02-02 13:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36ams8",
              "author": "imsankettt",
              "text": "Oh yes",
              "score": 3,
              "created_utc": "2026-02-02 16:01:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o37mvgj",
          "author": "Own-Manufacturer-640",
          "text": "Explore following :\n\n1. Competencies\n2. Service Validations\n3. Partner programs\n4. FTR (because you are selling solutions)\n\nIt indeed is overwhelming but give it some time. I work for a partner , i do not have alot of direct APN access but I worked on getting GenAI competency and explored it. \n\nBasically they have given very detailed paths there. Connect first with your Account Manager. \nHe will be your main point of contact.",
          "score": 1,
          "created_utc": "2026-02-02 19:42:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34xiig",
          "author": "askcloudstuff",
          "text": "I don't have the answers you are looking for. However, if your company needs to hire anyone with AWS experience, I love to interview. With or without compensation. I have interned at AWS and hold AWS SA Associate and Pro in addition to CCNA and AI practitioner. I’m keen to keep learning, contribute where I can.",
          "score": 1,
          "created_utc": "2026-02-02 11:07:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34xrnw",
              "author": "imsankettt",
              "text": "Thanks for the response, I will let you know if there are any postions!",
              "score": 1,
              "created_utc": "2026-02-02 11:09:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qw5snt",
      "title": "When using SQS and Lambda, what is the best way to rate limit how many messages the lambda can process per minute?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qw5snt/when_using_sqs_and_lambda_what_is_the_best_way_to/",
      "author": "PuppyLand95",
      "created_utc": "2026-02-05 00:13:49",
      "score": 13,
      "num_comments": 42,
      "upvote_ratio": 0.93,
      "text": "My app allows users to do a bulk import of many products. When the user triggers a bulk import, each product will get enqueued to the sqs queue as a message. There is a lambda worker that will process from the queue. The problem is that in order to import the product I need to call a third party API which is rate limited (using a fixed window, e.g. 5000 api calls per day). Since there could be multiple users that trigger a bulk import at the same time, I was planning to use SQS \"fair\" queues to avoid the noisy neighbor problem.\n\nMy original idea was to create an internal rate limiter that would allow the lambda to process X amount of messages per minute. For example, 3 messages per minute. Once the limit per minute is reached, I was planning to use changeMessageVisibility() for any other messages it picks up until the next one-minute window begins. So for example, if there are 30 seconds left until the next minute window starts, I would make the message invisible for 30 seconds. But I realize now that if some messages are \"unlucky\" and keep getting changeMessageVisibility() called on them, then the receive count will increase and eventually they will be added to the dead letter queue. And for bulk imports, the queue will be quite full, so the lambda would be picking up messages continuously for a period of time.\n\nI'm aware we can use \"maximum concurrency\" on the SQS side and \"reserved concurrency\" on the lambda side, but this doesn't give me the granularity of control on the rate of processing that I am seeking.",
      "is_original_content": false,
      "link_flair_text": "serverless",
      "permalink": "https://reddit.com/r/aws/comments/1qw5snt/when_using_sqs_and_lambda_what_is_the_best_way_to/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o3mnd6j",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'serverless'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+serverless).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-05 00:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mvc7v",
          "author": "_rundude",
          "text": "My suggestion:\nEvent bridge schedule to trigger the lambda\nLambda calls the message queue to process batches.\n\nSet the schedule to the relevant limit you need.\n5000/day = about 1 every 17 seconds. Which falls below the granularity of 1 minute. But that’s easy enough to create 3 or 4 schedules. (Or Eventbridge to sns > multiple lambdas. \n\nKeeps it all serverless.",
          "score": 16,
          "created_utc": "2026-02-05 00:57:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3mydbo",
              "author": "bdavid21wnec",
              "text": "I like this answer, don't need to mess with any crazy lambda settings, just need to think about sqs retention period, your lambda should return an error if it fails to process and might need another dlq with a redrive policy or something",
              "score": 2,
              "created_utc": "2026-02-05 01:15:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3mqpm9",
          "author": "SonOfSofaman",
          "text": "The Fargate solution recommended by u/aromaticfoxsquirrel is probably the \"best\" solution, but another solution might be to use EventBridge Scheduler to fire off the Lambda function instead of using the SQS Event Source Mapping to trigger it. You can control the schedule as precisely as you need. This isn't the best solution because you might (will) end up triggering the Lambda function even when the queue is empty.\n\nWith either of these solutions, you'll end up polling the queue manually from the function instead of letting the Event Source Mapping do that for you.",
          "score": 7,
          "created_utc": "2026-02-05 00:32:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mwccw",
          "author": "Zenin",
          "text": "One possible pattern that's not too hacky, if you can suffer a slight delay in processing your requests:\n\n**Inbox/Outbox Pattern**\n\nTwo SQS, two Lambdas, 1 DynamoDB table for quota tracking.\n\nDynamoDB: 1 row per hour storing an int for a count, configured with a TTL to auto-expire after 24 hours.  This is how we'll support a rolling daily rate limit check.  If you have a hard daily limit that rolls over at x time daily, just switch this to a single daily row.\n\nSenders -> Inbox SQS\n\nLambda A on cron such as \"rate(5 minutes)\": Reads all items from DynamoDB newer than 24 hours ago and sums the counts.  If the count is lower than your daily limit, take the difference and pull that many messages off the Inbox SQS and send them to the Outbox SQS for processing.  Update the current hour item count with however many more you've sent to Outbox making sure to set the TTL to 24 hours in the future for cleanup.  If you've reached your limit, just exit and try again next rate(5 minutes) or whatever.  Since each hour is an item and they are TTL set to auto-expire after 24 hours, DynamoDB will handle the cleanup of your old counts. -Expirations can take a while, even if they are usually quick, so don't rely on them for the scan() to only pull the latest 24...but the count is low enough a scan() + filter is very cheap here.\n\nLambda B subscribes to Outbox SQS: This is the setup you have now, just processes as fast as it can and doesn't care about limits because the first half of this already manages the rate limit into this SQS.\n\nThis Inbox/Outbox model gives you complete control on your processing rate for 3rd party calls while still allowing clients to burst without worry.  Add DLQ to everything of course, monitoring, etc, but that's the basic framework.\n\nThere's other ways to do this you might consider, such as manually handling exponential backoff + jitter and retry patterns using a 3 queue model and metadata in the message attributes, but if we're strictly talking about honoring 3rd party rate limits the above inbox/outbox pattern has worked well for me and doesn't waste costly CPU/memory cycles on sleep() calls.",
          "score": 5,
          "created_utc": "2026-02-05 01:03:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mphmj",
          "author": "JustinSK1",
          "text": "Set Maximum Concurrency / Event Source Mapping [https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-scaling.html](https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-scaling.html)",
          "score": 14,
          "created_utc": "2026-02-05 00:25:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3mt3kx",
              "author": "Zenin",
              "text": "How does this help?  It's entirely possible to saturate the 3rd party API limits with a single Lambda process, so reducing concurrency doesn't reliably throttle.",
              "score": -1,
              "created_utc": "2026-02-05 00:45:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mu3v9",
                  "author": "kilobrew",
                  "text": "That’s the exact way to do it though…..",
                  "score": 10,
                  "created_utc": "2026-02-05 00:50:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3mxqd3",
                  "author": "Donzulu",
                  "text": "Agree if you are limited to 5000/day, depending on how many API calls a single event can do, and even if it is only 1 api call, you can still do 5000 lambda invocation with max concurrency of 2 batch size of 1 in way less than a day. Let’s say each lambda takes 1 second to do the API calls, in 42 minutes you but your 5k cap.\n\nIf you are limited to 5/sec this solution is more feasible.",
                  "score": 3,
                  "created_utc": "2026-02-05 01:11:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mu6z4",
              "author": "kilobrew",
              "text": "This is the way. It’s a well known pattern. Just ask chatGPT.",
              "score": -5,
              "created_utc": "2026-02-05 00:51:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3morzy",
          "author": "aromaticfoxsquirrel",
          "text": "I would look at using a single Fargate container to process the whole queue.  You can have a lot more control over the rate that way.\n\nThe downside, of course, is that it's a bit more to set up and it's more expensive.",
          "score": 11,
          "created_utc": "2026-02-05 00:21:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3mpmyy",
              "author": "PuppyLand95",
              "text": "Yeah I was considering using fargate as well. I guess I can just make the fargate task sleep for however long it needs to until the next minute-window, instead of constantly receiving messages from the queue and having to call changeMessageVisibility()",
              "score": 1,
              "created_utc": "2026-02-05 00:26:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3mq6pi",
                  "author": "aromaticfoxsquirrel",
                  "text": "I think you might be able to do something with Step Functions as well, but I have no idea how that would work with pricing.  And setting that up would be a real pain.",
                  "score": 3,
                  "created_utc": "2026-02-05 00:29:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3mrgpv",
                  "author": "Dangle76",
                  "text": "You can set a poll rate with an algo instead of using sleeps. It’s a better control pattern",
                  "score": 2,
                  "created_utc": "2026-02-05 00:36:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3mspo5",
                  "author": "x86brandon",
                  "text": "Overthinking it with message visibility.  SQS is durable.  \n\nWhen you're not doing Lambda triggers you have to make 2 calls.  Receive and Delete.  \n\nSo you Receive a message, hit the API, if throttled, retry and back off.  Once you get confirmation from the API, \\*then\\* Delete the message.  It's very ok for things to live in SQS for days.\n\nYou shouldn't have to throttle your calls to them as they should return a 429 to you.",
                  "score": 2,
                  "created_utc": "2026-02-05 00:43:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3mq8dc",
          "author": "Forward_Context1013",
          "text": "check this out:\n\n[https://github.com/aws-samples/fine-grained-rate-limit-demo](https://github.com/aws-samples/fine-grained-rate-limit-demo)\n\nyou can set up something like this and reuse it for different apis with different rate limits ",
          "score": 3,
          "created_utc": "2026-02-05 00:29:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3mti5p",
              "author": "Zenin",
              "text": "That's for API Gateway, which is great for adding rate limits to your *own* API providers, but does little to address the OP's ask of accommodating the rate limits of *third party APIs*.",
              "score": 2,
              "created_utc": "2026-02-05 00:47:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mu6no",
                  "author": "Forward_Context1013",
                  "text": "I think it is relevant still. OP can use the leaky bucket/token bucket concepts outside of api gateway implementation . But, fair point to clarify this",
                  "score": 2,
                  "created_utc": "2026-02-05 00:51:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3mrumn",
          "author": "Dull_Caterpillar_642",
          "text": "You get coarse grained control over it with these settings on the SQS trigger config:\n\n\n- Batch Size\n- Max Concurrency\n- Max Batching Window \n\n\nBut the unfortunate fact is AWS still doesn’t have a way to precisely dial in the requests per second.  You have to roll your own with something like a fargate task which serves as the consumer which triggers your lambdas, step functions, whatever.  But if you can get by with the coarse level of control you get by combining those things above, you can go purely lambda which is nice if you can swing it.\n\nI’ve done some experimenting with and you can actually get pretty slow and extremely fast with a purely lambda solution.  But you won’t get to just say “I want to go exactly 17 messages per second”",
          "score": 3,
          "created_utc": "2026-02-05 00:38:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n66nt",
          "author": "ifyoudothingsright1",
          "text": "Eventbridge api destinations work for something measured in requests per second.\n\nStep functions also has some rate limiting functionality similar to above.\n\nFor something that low measured per day, it might be good to have an initial queue feed a lambda that keeps a atomically updated/read counter in dynamodb, and based on that counter schedules future events in aws scheduler.\n\nYou could also use step function wait steps instead of aws scheduler, using a dynamodb based counter like above.",
          "score": 2,
          "created_utc": "2026-02-05 02:00:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n57ra",
          "author": "HKChad",
          "text": "If you get consumed/available counts in the results run it though a step function, use up your allocated amount then pause the step function, fill it up with queue.",
          "score": 1,
          "created_utc": "2026-02-05 01:54:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mqi5i",
          "author": "x86brandon",
          "text": "Using Lambda to do that is an anti-pattern to Lambda.  You're better running a container worker.  And also, throttling is asking to increase your Lambda costs for no reason.  You want that to process the item as fast as possible to reduce # of seconds of CPU/RAM time.   If you throttle, you're slowing down something you pay per second, pay per invocation.\n\nIf you must use a Lambda and you have potentially a backlog of async processing with a throttled API, I would split the workers to have 1 Lambda handle the incoming, make state changes and then drop it into a queue that makes the external calls.  That queue you can set a Max Concurrency of 2.   It also gives you a place to do batching.   Especially if you're using single threaded stuff like Python, this gives you the ability to take a bunch of randomized incoming requests, enqueue them in a 2 worker batch queue.  The batching will let you pull like 100 messages and then throttle your calls in batches.",
          "score": -1,
          "created_utc": "2026-02-05 00:31:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3njjs6",
              "author": "ReporterNervous6822",
              "text": "Yeah idk why you are getting downvoted here using lambda just isn’t the right call. You need something in the middle triggering lambdas or to do it statefully or like use a database to orchestrate against",
              "score": 3,
              "created_utc": "2026-02-05 03:16:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3n6i8z",
          "author": "BoringTone2932",
          "text": "This would be hacky, but couldn’t you have the lambda disable the event source mapping, effectively to stop it from getting messages? \n\nYou’d have to solve for re-enabling it though, which would require a secondary lambda.\n\nBut you could theoretically have the processing lambda disable its own event source mapping and tag itself with a “re-enable” timestamp. Have another lambda check every X minutes to re-enable the mapping.\n\nLike I said, hacky.",
          "score": 0,
          "created_utc": "2026-02-05 02:02:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu8h4m",
      "title": "College student wondering if getting the AWS SAA is worth it for my goals",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qu8h4m/college_student_wondering_if_getting_the_aws_saa/",
      "author": "Kaisaroll",
      "created_utc": "2026-02-02 21:34:55",
      "score": 10,
      "num_comments": 9,
      "upvote_ratio": 0.82,
      "text": "Recently started my first year of college, studying ITS, with the goals of getting an AWS CSA/CSE internship. Just for some background, I currently hold the CompTIA Security+ certification and have been working with Linux for quite some time. I have a security-related project under my belt and will be working on more in the future. Just wanted to ask if it's worth studying for and taking the AWS SAA to get me closer to and improve my chances of getting that internship, or other internships in general.",
      "is_original_content": false,
      "link_flair_text": "training/certification",
      "permalink": "https://reddit.com/r/aws/comments/1qu8h4m/college_student_wondering_if_getting_the_aws_saa/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o38bkve",
          "author": "Sirwired",
          "text": "It can’t hurt, and you have the IT basics down enough that it won’t be a waste of your time.",
          "score": 6,
          "created_utc": "2026-02-02 21:39:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38h537",
          "author": "sandy_coyote",
          "text": "It depends, but overall yes, and it's not gonna be waste of time for any enterprise IT-related internship.",
          "score": 6,
          "created_utc": "2026-02-02 22:05:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38ircw",
          "author": "lifelong1250",
          "text": "Many positions for infra or devops will ask for this certification.",
          "score": 4,
          "created_utc": "2026-02-02 22:13:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39b18v",
          "author": "TophatDevilsSon",
          "text": "It's a good introduction to AWS concepts.  AWS isn't going anywhere anytime soon.  If you've got interest and bandwidth I'd say it's a worthwhile investment of effort.\n\nThat said, I doubt the certification by itself is going to land you any jobs.  Not to be discouraging, just don't want to plant any false hopes.\n\nGood on you, though.  Go forth and kick ass.",
          "score": 2,
          "created_utc": "2026-02-03 00:44:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o39jnxu",
              "author": "ManyRevolutionary170",
              "text": "For the knowledge only. I’d say do it.",
              "score": 2,
              "created_utc": "2026-02-03 01:33:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3b9xw0",
          "author": "Thinguist",
          "text": "Getting your first job is the hardest, anything you can do to get that job is worth it.",
          "score": 1,
          "created_utc": "2026-02-03 09:06:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3d8t5d",
          "author": "TechDebtSommelier",
          "text": "Yes, it’s worth it. A lot of consultancies actively look for candidates with AWS certs because they need certified staff to maintain partner tiers, so the SAA is a real signal, not just resume fluff. Even if an AWS-specific internship doesn’t pan out, it still shows you’re investing time in cloud as a specialty, which helps with other internships too.\n\nFor your first role, small to mid-sized companies are often the easiest entry point (that’s how I started), and that experience makes it much easier to land bigger name opportunities later.",
          "score": 1,
          "created_utc": "2026-02-03 16:40:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qts6xs",
      "title": "Is it possible to fix the sorting of dashboards in Quicksight?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qts6xs/is_it_possible_to_fix_the_sorting_of_dashboards/",
      "author": "Dresi91",
      "created_utc": "2026-02-02 11:08:45",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "We use multiple dashboards at work for different use cases in our AWS Quicksight environment. These are currently sorted by last reload timestamp which messes up the sorting every day due to different reload times of each dashboard.\n\nIs it possible to give the dashboards a fixed sorting? I do not mean any data sorting INSIDE the dashboards but the dashboards itself before opening them.",
      "is_original_content": false,
      "link_flair_text": "discussion",
      "permalink": "https://reddit.com/r/aws/comments/1qts6xs/is_it_possible_to_fix_the_sorting_of_dashboards/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o35oo9v",
          "author": "safeinitdotcom",
          "text": "No, quicksight doesn’t support a custom dashboard sort order in the console. The list is mainly driven by recent activity/last updated, so refresh schedules will keep reshuffling it.\n\nBest workarounds today:\n\n* Use **Folders** to group dashboards by use case (but no manual ordering inside folders) [https://docs.aws.amazon.com/quicksuite/latest/userguide/folders.html](https://docs.aws.amazon.com/quicksuite/latest/userguide/folders.html)\n* Use **naming prefixes** (`01 - Sales`, `02 - Ops`) so alphabetical sort stays consistent",
          "score": 2,
          "created_utc": "2026-02-02 14:11:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b4002",
              "author": "Dresi91",
              "text": "Good idea, thank you!",
              "score": 1,
              "created_utc": "2026-02-03 08:08:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o373m93",
          "author": "SpecialistMode3131",
          "text": "I strongly prefer having a dashboard landing that is platform agnostic (like a wiki page) - with links to whatever.  That way you can embed any context you need to completely without respect to platform limitations.    \n\n\nGoing to the actual platform's list of dashes as your landing page puts you at the mercy of whatever they chose, which is jarring and hard to process quickly in an incident. To me, dashboards must always serve the incident management role first, and everything else second. So invest hard into making it easy to find what you need when time is short.",
          "score": 1,
          "created_utc": "2026-02-02 18:14:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b40u6",
              "author": "Dresi91",
              "text": "Good idea, thank you!",
              "score": 1,
              "created_utc": "2026-02-03 08:08:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qsp22w",
      "title": "Query performance issue with high CPU usage",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qsp22w/query_performance_issue_with_high_cpu_usage/",
      "author": "Upper-Lifeguard-8478",
      "created_utc": "2026-02-01 04:55:58",
      "score": 7,
      "num_comments": 22,
      "upvote_ratio": 0.89,
      "text": "Hello,\n\nIts aurora postgres DB R6g.Large machine , version 17. \n\nWe have a \"Select\" query which is using three to five main transaction tables (txn\\_tbl, txn\\_status, txn\\_decision, txn\\_sale, ath) holding \\~2million rows in each of them(which is going to increase to have \\~50-100million in future) and others(6-7) tables out of which some are master and some other small tables. \n\nWhen we are running this query , and its taking \\~2-3seconds , however when we hit this query from 10-15 sesion at same time its causing CPU spike up to \\~50-60% for the DB instance and this is incraesing and touching 90% when we are increasing the hits further to 40-50 times concurrently. \n\nThis query is going to be called in the first page of an UI screen and is supposed to show first latest 1000 rows. This query is supposed to be thousands of users can hit this same query at the first landing page at the same time. The instance has 2-VCPU and 16GB RAM.\n\nMy questions are as below.\n\n1)Why this query is causing high cpu spike ,if any way to understand what part/line of the query is contributing to the high cpu time?\n\n2)How we can tune this query to further reduce response time and mainly CPU consumption ? Is any additional index or anything will make this plan better further?\n\n3) Also is there any expert guidance to create queries or designs for such UI scenarios where performance or response time is important?\n\n4)And based on the instance CPU core and memory , is there any calulation which by using which , we can say that this machine can support maximum N number of such concurrent queries of such type beyond which we need larger machines?\n\nBelow is the query having the query and its current plan:- \n\n[https://gist.github.com/databasetech0073/6688701431dc4bf4eaab8d345c1dc65f](https://gist.github.com/databasetech0073/6688701431dc4bf4eaab8d345c1dc65f)",
      "is_original_content": false,
      "link_flair_text": "database",
      "permalink": "https://reddit.com/r/aws/comments/1qsp22w/query_performance_issue_with_high_cpu_usage/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o2x44p2",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'database'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+database).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-01 04:55:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2x5f15",
          "author": "Durakan",
          "text": "Nested loop is likely the beginning of the trailhead to trace this.\n\nBut you're going to probably need to pay someone to optimize this query.",
          "score": 6,
          "created_utc": "2026-02-01 05:04:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2x99qf",
          "author": "rudigern",
          "text": "That query looks pretty bad, thing that jumps out is joining on something that is based on varchar (line 67) probably. As the other person said you’ll need to pay someone to fix it, this isn’t really an aws issue even if its exacerbated by a distributed environment (storage isn’t directly attached to the compute)",
          "score": 6,
          "created_utc": "2026-02-01 05:33:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xfoz8",
          "author": "ElectricSpice",
          "text": "All the time is spent matching up filtered entities with filtered transactions. First off, simplify by removing limited_txn and ordering+limited within filtered_txn. Then two things to try: a) index txn_tbl by date and b) materialize the filtered_entity CTE. \n\nBut a query like this is always going to be hairy. Can you cache the results at all to prevent too much load? Alternatively, read replicas are great for this kind of thing, it’s very freeing to not have to worry how a query like this might affect the primary.\n\nEdit: on second look, I don’t think materialize will change anything. Easy enough to try though.",
          "score": 5,
          "created_utc": "2026-02-01 06:23:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xhzzj",
              "author": "Upper-Lifeguard-8478",
              "text": "Thank you so much u/ElectricSpice  , will try these modifications and additional indexes as you suggested and update. \n\nAs you mentioned to cache the results , is there any hints or anything in postgres aurora which can cache teh query results fully unless the base data is not modified on the transaction tables ?\n\nalso another question i had was, as these queries were are planning to make it run from 100's of concurrent sessions by different users , so is there a calculation which we can do and say , to whatever extent we tune the query , only N number of maximum concurrent session can be executed on this machine?\n\nIs any indicator in the query plan , which suggest the main section of the of the query which is CPU consuming?",
              "score": 1,
              "created_utc": "2026-02-01 06:43:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2xj1bq",
                  "author": "ElectricSpice",
                  "text": "Postgres does not have anything built in to cache results or manage concurrency, you’ll need to put those in the application. \n\nYes, the query plan is quite clear that the sequential scan of txn_tbl and cross-referencing against filtered_entities is where most of the work is happening.\n\nIf you’re going to be dealing with queries like this you need to invest time into becoming very comfortable with query plans.",
                  "score": 5,
                  "created_utc": "2026-02-01 06:52:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o34zuzg",
              "author": "Upper-Lifeguard-8478",
              "text": "You are spot on. I modified just created the query with a new index on the \"txn\\_date\" column of txn\\_tbl and that itself dropped the time to \\~150-200ms. \n\nNow i am just trying to execute the portion which is taking time i.e. the two initial CTE's  and i see , its now the \"nested loop\" which takes the time , can it be made any better/faster?\n\nAnd another question i had was , i am still unable to understand , with this new index the tablescan is now fully eliminated by the \"Index Scan Backward\" even i have other columns from that tabkle projected in the query, so how its getting all those column values without visiting table but just that index? \n\n[https://gist.github.com/databasetech0073/344df46c328e02b98961fab0cd221492](https://gist.github.com/databasetech0073/344df46c328e02b98961fab0cd221492)",
              "score": 1,
              "created_utc": "2026-02-02 11:27:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2x9xiu",
          "author": "Old-Astronomer3995",
          "text": "Post it on r/PostgreSQL you need help with query and there are better people for this kind of questions.",
          "score": 2,
          "created_utc": "2026-02-01 05:38:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xi3yq",
              "author": "Upper-Lifeguard-8478",
              "text": "Somehow when i am trying to post this question there, its getting automatically deleted by the moderator , not sure why.",
              "score": 1,
              "created_utc": "2026-02-01 06:44:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2xim28",
          "author": "EconomistAnxious5913",
          "text": "If you need to match and for at runtime, caching might help.\nCaching table data.\nThen run query of filters in your app.\nGive it a try.",
          "score": 2,
          "created_utc": "2026-02-01 06:48:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2x44o7",
          "author": "AutoModerator",
          "text": "Here are a few handy links you can try:\n\n- https://aws.amazon.com/products/databases/\n- https://aws.amazon.com/rds/\n- https://aws.amazon.com/dynamodb/\n- https://aws.amazon.com/aurora/\n- https://aws.amazon.com/redshift/\n- https://aws.amazon.com/documentdb/\n- https://aws.amazon.com/neptune/\n\nTry [this search](https://www.reddit.com/r/aws/search?q=flair%3A'database'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+database).\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-02-01 04:55:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yp21i",
          "author": "jonathantn",
          "text": "u/op Thank you for posting the query with explain plan.  I ran this through Claude Opus 4.5 for you.  Here is the output ",
          "score": 1,
          "created_utc": "2026-02-01 13:01:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yp4de",
              "author": "jonathantn",
              "text": "# Performance Analysis (~1056ms)\n\n**1. The Big One: Full Sequential Scan of** `txn_tbl` **(1.4M rows)**\n\nThe plan does a Parallel Seq Scan on `txn_tbl`, reading \\~1.4M rows total. The `txn_type` filter removes almost nothing (only \\~1700 rows out of 1.4M). Then it hash-joins with just **12 entity IDs**. This is completely backwards — with only 12 entities, Postgres should be doing index lookups, not scanning millions of rows.\n\n**Fix:** Create a composite index:\n\n    CREATE INDEX idx_txn_tbl_ent_tran ON APP_schema.txn_tbl(ent_id, tran_date);\n\nThis lets Postgres scan only rows for those 12 entities, already in `tran_date` order — eliminating the seq scan AND the sort that's currently spilling to disk. This alone could take the query from \\~1s to single-digit milliseconds.",
              "score": 2,
              "created_utc": "2026-02-01 13:01:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2yp5qw",
              "author": "jonathantn",
              "text": "**2. 57,848 Probes into** `txn_decision` **to Find 1000 Rows**\n\nBecause of the `LIMIT 1000`, the plan walks sorted transactions and probes `txn_decision` for each one to check `is_prnt = 'Y'` and status filters. It checked **57,848 rows** to find 1000 qualifying ones (\\~1.7% hit rate). That's 231K buffer hits just in this loop.\n\n**Fix:** A partial index:\n\n    CREATE INDEX idx_txn_decision_filter \n    ON APP_schema.txn_decision(txn_id) \n    WHERE is_prnt = 'Y' AND txn_status NOT IN ('STATUS_A','STATUS_B');\n\nThis skips non-qualifying decisions at the index level.",
              "score": 2,
              "created_utc": "2026-02-01 13:02:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2yp6zf",
              "author": "jonathantn",
              "text": "**3.** `txn_tbl` **and** `txn_decision` **Are Joined TWICE**\n\nYour CTEs do this:\n\n* `filtered_txns`: joins `txn_tbl` ↔ `txn_decision` with filters\n* Final SELECT: joins `limited_txns` back to `txn_tbl` and `txn_decision` **again** with the same conditions\n\nThe `limited_txns` CTE only carries `(txn_id, ent_id, tran_date)`, so the final SELECT has to re-fetch everything from scratch.\n\n**Fix:** Carry all needed columns through the CTEs so you don't re-join:\n\n    filtered_txns AS (\n        SELECT c.*, cd.decision_desc, cd.txn_status, cd.assigned_user, ...\n        FROM APP_schema.txn_tbl c\n        JOIN APP_schema.txn_decision cd ON ...\n        WHERE ...\n    )\n\nThen reference `limited_txns` columns directly in the final SELECT instead of re-joining those two tables.",
              "score": 2,
              "created_utc": "2026-02-01 13:02:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2yp83t",
              "author": "jonathantn",
              "text": "**4.** `work_mem` **Too Low — Sorts Spilling to Disk**\n\nThe parallel sort workers are spilling 3.4–5.7 MB each to disk. Easy fix:\n\n    SET LOCAL work_mem = '16MB';\n\n**5.** `ckpt_flg` **Aggregates the Entire Checkpoint Table**\n\nFull seq scan + aggregation of all \\~15K checkpoint rows, but only \\~933 are used. Add a filter:\n\n    ckpt_flg AS (\n        SELECT ck.txn_id,\n            MAX(CASE WHEN ck.ckpt_name = 'FLAG_A' THEN 'Y' END) AS RCL_flg,\n            MAX(CASE WHEN ck.ckpt_name = 'FLAG_B' THEN 'Y' END) AS d_rcv_flg\n        FROM APP_schema.txn_checkpoint ck\n        WHERE ck.txn_id IN (SELECT txn_id FROM limited_txns)\n        GROUP BY ck.txn_id\n    )\n\nOnly \\~20ms currently, but good practice and matters as the table grows.",
              "score": 2,
              "created_utc": "2026-02-01 13:02:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2yp94p",
              "author": "jonathantn",
              "text": "**6. 🐛 Data Correctness Bug: INNER JOIN on** `txn_ath` **Currency Drops Rows**\n\nYou correctly use `LEFT JOIN` on `txn_ath`, but then do `JOIN APP_schema.crnc cur3 ON cur3.code = a.txn_crnc` — that's an **INNER JOIN**. When `txn_ath` has no match (LEFT JOIN returns NULL), the inner join on `cur3` silently drops the row.\n\nResult: 1000 limited rows → only **933** output rows. You're losing 67 transactions.\n\n**Fix:** Change to:\n\n    LEFT JOIN APP_schema.crnc cur3 ON cur3.code = a.txn_crnc",
              "score": 2,
              "created_utc": "2026-02-01 13:02:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ypa7k",
              "author": "jonathantn",
              "text": "# TL;DR — Priority Order\n\n1. **Add index** `(ent_id, tran_date)` **on** `txn_tbl` — biggest win by far, eliminates full table scan + disk sort\n2. **Stop joining** `txn_tbl`**/**`txn_decision` **twice** — carry columns through CTEs\n3. **Partial index on** `txn_decision` for the `is_prnt`/status filter\n4. **Bump** `work_mem` to avoid disk spills\n5. **Fix** `cur3` **to LEFT JOIN** — this is a **data bug**, you're silently losing rows\n6. **Filter** `ckpt_flg` CTE to relevant txn\\_ids\n7. **Fix** `CASE WHEN` **syntax** in `ckpt_flg`\n\nFixes #1 and #2 alone should take this from \\~1s to well under 100ms. Good luck!",
              "score": 2,
              "created_utc": "2026-02-01 13:03:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35018a",
                  "author": "Upper-Lifeguard-8478",
                  "text": " I modified just created the query with a new index just on the \"txn\\_date\" column of txn\\_tbl and that itself dropped the time to \\~150-200ms.\n\nNow,  i am executing the portion of the query, which is taking time i.e. the two initial CTE's and i see , its now the \"nested loop\" which takes the time (\\~150ms alone) , can it be made any better/faster?\n\nAnd another question i had was , i am still unable to understand , with this new index the tablescan is now fully eliminated by the \"Index Scan Backward\" even i have other columns from that tabkle projected in the query, so how its getting all those column values without visiting table but just that index?\n\n[https://gist.github.com/databasetech0073/344df46c328e02b98961fab0cd221492](https://gist.github.com/databasetech0073/344df46c328e02b98961fab0cd221492)",
                  "score": 1,
                  "created_utc": "2026-02-02 11:29:25",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3160ou",
          "author": "joelrwilliams1",
          "text": "Lots of tables, lots of joins...it's going to take a lot of CPU to pull that off.  Sounds like you may need a larger RDS instance.\n\nQuery tuning may help a little, but something this massive is going to chew up plenty of I/O and CPU.",
          "score": 1,
          "created_utc": "2026-02-01 20:19:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu0b38",
      "title": "I've had a Quota Request take almost 3 weeks. Is there a SLA on these?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qu0b38/ive_had_a_quota_request_take_almost_3_weeks_is/",
      "author": "Wrexis",
      "created_utc": "2026-02-02 16:47:43",
      "score": 7,
      "num_comments": 10,
      "upvote_ratio": 0.89,
      "text": "We've never had a Quota Increase Request take longer than 3 days, and this one is now in its third week. I'm actually shocked by how long it's taking. They are responding to the ticket and apologizing for the delay, but jeez.\n\nThis is on a paid support account as well.",
      "is_original_content": false,
      "link_flair_text": "general aws",
      "permalink": "https://reddit.com/r/aws/comments/1qu0b38/ive_had_a_quota_request_take_almost_3_weeks_is/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o36leom",
          "author": "CircularCircumstance",
          "text": "AWS support has really completely fallen apart over the past year.  Do you have a TAM?",
          "score": 9,
          "created_utc": "2026-02-02 16:51:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36ls9l",
              "author": "Wrexis",
              "text": "Yeah, and while we're getting a discount because of the delay, but it doesn't solve the problem.",
              "score": 2,
              "created_utc": "2026-02-02 16:52:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o36mop6",
          "author": "oneplane",
          "text": "Depends on the specific quota and your spend I suppose. Most of our changes are near real-time (to the point where they are approved before Terraform Apply has had time to write its state back to S3).",
          "score": 1,
          "created_utc": "2026-02-02 16:57:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36n98r",
              "author": "Wrexis",
              "text": "Yearly spend is seven figures. I wonder if they have an escalation flag for that depending on the spend.",
              "score": 1,
              "created_utc": "2026-02-02 16:59:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o36nnls",
                  "author": "oneplane",
                  "text": "Are you trying to get more GPUs? Or is it something much simpler like getting more SG Rules or something like that.",
                  "score": 3,
                  "created_utc": "2026-02-02 17:01:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36o0a0",
          "author": "AWSSupport",
          "text": "Hello,\n\nI apologize for the inconvenience you're facing due to this delay. If you're willing to provide your case ID through direct message, we'd be glad to look into it.\n\n \\- Rick N.",
          "score": -1,
          "created_utc": "2026-02-02 17:03:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36q55m",
              "author": "Wrexis",
              "text": "It's being actioned, but appreciate the response.",
              "score": 1,
              "created_utc": "2026-02-02 17:13:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqo0xt",
      "title": "AWS Bedrock KB S3 ingestion - Reduce amount of metadata.json files?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qqo0xt/aws_bedrock_kb_s3_ingestion_reduce_amount_of/",
      "author": "Nahxify",
      "created_utc": "2026-01-29 22:59:44",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I'm working on implementing a RAG system with the Retrieve and Generate API and S3/S3 Vectors. Currently, we have thousands of documents and it seems overall messy and tedious to have a .metadata.json file associated with each one. Is there any way around this? I want to try and improve the retrieval with implicit metadata filtering.\n\nIn the docs, Bedrock seems to support one centralized metadata.json file for a single CSV with multiple content rows, but I don't see any references to how/if this can be applied to documents that are not CSV.\n\nIs there no way to handle this nicely? Do I need to generate a .metadata.json for each of my thousands of documents?\n\nEdit: I should mention, I'm aware there are other options to handle this, I was just looking for something native to Bedrock to reduce extra ingestion pre-processing steps",
      "is_original_content": false,
      "link_flair_text": "ai/ml",
      "permalink": "https://reddit.com/r/aws/comments/1qqo0xt/aws_bedrock_kb_s3_ingestion_reduce_amount_of/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o2ih2rj",
          "author": "Fatel28",
          "text": "It's one metadata file per content file, unless like you said you're using csv",
          "score": 2,
          "created_utc": "2026-01-30 00:27:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvpj6t",
      "title": "AWS Organizations",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qvpj6t/aws_organizations/",
      "author": "Razzleberry_Fondue",
      "created_utc": "2026-02-04 14:09:30",
      "score": 6,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "We have three seperate AWS accounts, we are looking to create an org. One account is gov which holds web apps, one account holds DNS and one account has AWS bedrock and does billing. I havent done too much with AWS, so i just wanted a little advice. If i create an organization to have all accounts under the org, will it cause any impact to our services? Reading through the domcumentation it seems like no, but wanted to double check",
      "is_original_content": false,
      "link_flair_text": "technical resource",
      "permalink": "https://reddit.com/r/aws/comments/1qvpj6t/aws_organizations/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o3j7wjz",
          "author": "CyberViking949",
          "text": "It won't impact your services. However, if you have a gov account, im assuming you have a fedramp env? \n\nAdding your fedramp environment to an organization introduces a significant change. It also brings the org master into scope. Which means any change in the org you need to do for the other accounts, is subject to SI and fedramp controls, oversight, and reporting. \n\nTLDR, do NOT mix your fedramp account in with the others. I would create 2 orgs. 1 fedramp, 1 normal.",
          "score": 13,
          "created_utc": "2026-02-04 14:18:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3j8fwg",
              "author": "Razzleberry_Fondue",
              "text": "Great point",
              "score": 2,
              "created_utc": "2026-02-04 14:21:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3j8c3f",
          "author": "menge101",
          "text": "I am not sure you can bring your gov account into the same Org, in fact fairly certain you can't.\n\nGov cloud isn't a different region, its a different partition, its a higher level division in AWS.",
          "score": 7,
          "created_utc": "2026-02-04 14:20:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k0355",
          "author": "omerhaim",
          "text": "You can’t add gov account to regular accounts. \nThose are different partitions \n\nLike china account. \n\nNot sharing the same IAM",
          "score": 2,
          "created_utc": "2026-02-04 16:34:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j84f6",
          "author": "AWSSupport",
          "text": "Hi there.\n\nFor additional guidance on creating an AWS Organization for your accounts, you're welcome to reach out to our Support team by creating a case via our Support Center: http://go.aws/support-center.\n\n\\- Roman Z.",
          "score": 1,
          "created_utc": "2026-02-04 14:19:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq4w3p",
      "title": "Find logs in cloudwatch bigger or smaller than 750 bytes",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qq4w3p/find_logs_in_cloudwatch_bigger_or_smaller_than/",
      "author": "Viskerz",
      "created_utc": "2026-01-29 10:19:45",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "According to [https://aws.amazon.com/cloudwatch/pricing/](https://aws.amazon.com/cloudwatch/pricing/) example 4 it states that cloudfront request logs below 750 are not billed. But i cant seem to find a way to query that. Any help is appreciated.",
      "is_original_content": false,
      "link_flair_text": "billing",
      "permalink": "https://reddit.com/r/aws/comments/1qq4w3p/find_logs_in_cloudwatch_bigger_or_smaller_than/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o2dxlxh",
          "author": "AutoModerator",
          "text": "Try [this search](https://www.reddit.com/r/aws/search?q=flair%3A'billing'&sort=new&restrict_sr=on) for more information on this topic.\n\n^Comments, ^questions ^or ^suggestions ^regarding ^this ^autoresponse? ^Please ^send ^them ^[here](https://www.reddit.com/message/compose/?to=%2Fr%2Faws&subject=autoresponse+tweaks+-+billing).\n\nLooking for more information regarding billing, securing your account or anything related? [Check it out here!](https://www.reddit.com/r/aws/comments/vn4ebe/check_it_first_operating_within_amazon_web/)\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/aws) if you have any questions or concerns.*",
          "score": 1,
          "created_utc": "2026-01-29 10:19:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e23hd",
          "author": "alex_aws_solutions",
          "text": "You can query the log group by size as a standard metric. Or you can use Clouwatch Insights and query the size of individual log events.",
          "score": 3,
          "created_utc": "2026-01-29 10:59:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu5lhd",
      "title": "VPC Peering Connections: What happens when traffic arrives at a VPC with multiple route tables for the same destination?",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qu5lhd/vpc_peering_connections_what_happens_when_traffic/",
      "author": "CamiloDFM",
      "created_utc": "2026-02-02 19:51:30",
      "score": 5,
      "num_comments": 19,
      "upvote_ratio": 0.86,
      "text": "I couldn't find this with a quick Google, and I'm hesitant to trust any LLMs on this:\n\nSuppose I have two peered VPCs, vpc-A (10.0.1.0/24) and vpc-B (10.0.2.0/24). vpc-A is the source for traffic, and vpc-B will work as a bridge. B has two subnets, let's call them subnet-B1 and subnet-B2, and each has its own route table rtb-B1 and rtb-B2.\n\nIn the route table for vpc-A's traffic, I point an IP range I want to route though vpc-B (let's say 10.0.3.0/24 as an example) towards the peering connection pcx-AB. Then, in rtb-B1 I set 10.0.3.0/24 to a correctly configured service (living in another VPC, the Internet, doesn't matter) that dumps incoming traffic to a log, but in rtb-B2 I set 10.0.3.0/24 to a NAT gateway living within subnet-B1.\n\nWhat is going to happen? Am I going to see packets from 10.0.1.0/24 in the log, along with connection errors because the destination doesn't know where vpc-A is? Or are they going to come from 10.0.2.0/24, network translated through the NAT in subnet-B1? Or am I going to see a mix of both?\n\nEssentially: when traffic arrives to a VPC with multiple route tables through a peering connection, which table's routes does it prioritise?\n\nHere's a shitty drawing of the situation:\n\nhttps://preview.redd.it/o4ozyrjiy4hg1.png?width=1086&format=png&auto=webp&s=3d568bd211d5403c140da0a682a491ea82238aad\n\n",
      "is_original_content": false,
      "link_flair_text": "networking",
      "permalink": "https://reddit.com/r/aws/comments/1qu5lhd/vpc_peering_connections_what_happens_when_traffic/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o37q6oo",
          "author": "Contrandy_",
          "text": "This model is essentially trying to use \"transitive peering\" which is not supported by AWS for VPC Peering (see: [Transitive peering](https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-basics.html#vpc-peering-limitations:~:text=IPv6%20CIDR%20blocks.-,Transitive%20peering,-VPC%20peering%20does)**)**. In this model you would need to use Transit Gateways. \n\nHere's an example of what you're trying to do with a centralized egress configuration: [https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/using-nat-gateway-for-centralized-egress.html](https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/using-nat-gateway-for-centralized-egress.html)\n\nI've built this before with great success to reduce the cost of NAT Gateways for our workloads in a centralized egress account. The great thing about this model is that it makes it really easy to connect management resources to your TGW network (such as a SIEM, etc.)",
          "score": 17,
          "created_utc": "2026-02-02 19:58:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37rnof",
              "author": "CamiloDFM",
              "text": "Nice. The answer is \"nothing happens because your example is fundamentally broken\", then. Thanks for the quick reply.\n\nI'll give Transit Gateways a shot. Never needed them before this.",
              "score": 4,
              "created_utc": "2026-02-02 20:05:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o392uaf",
                  "author": "b3542",
                  "text": "Bear in mind that TGW's cost money to build and to use.",
                  "score": 3,
                  "created_utc": "2026-02-02 23:59:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3940ac",
                  "author": "Contrandy_",
                  "text": "You're welcome! As u/b3542 mentioned, be mindful of cost. Depending on the number of attachments you will have and the amount of bandwidth being pushed through, the increases could be small or very large. \n\nTGWs are very powerful; big fan of them :)",
                  "score": 2,
                  "created_utc": "2026-02-03 00:06:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o38m55c",
                  "author": "x86brandon",
                  "text": "Yes, if you have a direct connect you need to share with multiple VPC's and multiple accounts, shared subnet within multiple VPC's and/or TGW's is the way.",
                  "score": 1,
                  "created_utc": "2026-02-02 22:30:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o37rwok",
              "author": "b3542",
              "text": "This is correct - VPC peering is only effective if you need to communicate from within one VPC directly to another VPC's resources - nothing external/beyond it.",
              "score": 2,
              "created_utc": "2026-02-02 20:06:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o38ly9w",
                  "author": "x86brandon",
                  "text": "You can use routing appliances and such though.  Before TGW's and GLB's, we used to run Linux routers with BGP and such to go across VPC's to use a VPC as a connector of VPC's.\n\nBut I would never do that again.",
                  "score": 1,
                  "created_utc": "2026-02-02 22:29:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37qg9k",
          "author": "champtar",
          "text": "https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-basics.html \"VPC peering does not support transitive peering relationships\", so packets for VPC A will only go to subnet B1 or B2 ",
          "score": 5,
          "created_utc": "2026-02-02 19:59:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38gc6e",
          "author": "aqyno",
          "text": "pcx doesn’t support transitive. There’s no way you can make packets coming from A into NAT, you need a TGW to do so. \nIn this scenario you have packets “entering” a vpc where they don't belong, so those are automatically dropped.",
          "score": 2,
          "created_utc": "2026-02-02 22:01:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38knyp",
              "author": "CamiloDFM",
              "text": "I observed that transit gateway route tables can only point to attachments, not to network elements within the attached networks. Do transit gateways perform some sort of NAT on their own when transferring traffic between VPCs?\n\nI solved my problem without transit gateways in the end, so I didn't get to test this.",
              "score": 1,
              "created_utc": "2026-02-02 22:23:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o38l7o7",
                  "author": "aqyno",
                  "text": "You need to use both, you direct the traffic between VPC A using TGW route tables (you route traffic from VPC A into the attachmenent of VPC B, and the other way around)\n\nThe TGW Attachment is located on a subnet on creation, then you use the VPC Route Tables to route from that subnet to NAT Gateway.",
                  "score": 1,
                  "created_utc": "2026-02-02 22:26:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3932li",
                  "author": "b3542",
                  "text": "Transit gateways are just routers. Think of attachments as interfaces on a router. This analogy mostly holds, but it has some quirks. It almost behaves as if each TGW Attachment lives in its own VRF.",
                  "score": 1,
                  "created_utc": "2026-02-03 00:00:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3aml2f",
                  "author": "nevaNevan",
                  "text": "TGW attachments, as mentioned, deploy into subnets. A subnet has a route table.\n\nIf you need traffic to go from VPC-A, to TGW, to VPC-B and then to a network appliance in VPC-B (or whatever), you would place a route to that appliance in the subnet route table.\n\nNetworking in native AWS isn’t quite like networking in the enterprise or service provider world.\n\nYou can place 10 instances/servers/vms/nodes(don’t care what the term is anymore) in the same subnet, and have a single default route to one interface in that same subnet, and that’s how they’ll route. No broadcast and can’t communicate with each other at all. Traffic is forced to that one interface.",
                  "score": 1,
                  "created_utc": "2026-02-03 05:37:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qun5r7",
      "title": "Guys where do y'all study about networking and AWS to practice and complete the lab works and all that",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qun5r7/guys_where_do_yall_study_about_networking_and_aws/",
      "author": "FactsNotFate",
      "created_utc": "2026-02-03 09:06:26",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 0.86,
      "text": "Suggest me how's it gonna be, I've done it before but am not able to find the exact link so can y'all help with that \nI had done the lab works from aws academy online ",
      "is_original_content": false,
      "link_flair_text": "general aws",
      "permalink": "https://reddit.com/r/aws/comments/1qun5r7/guys_where_do_yall_study_about_networking_and_aws/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o3cay7n",
          "author": "Sirwired",
          "text": "How much networking do you know so far?  Anyone working in IT at all should certainly have the basics of: IP addressing (especially subnet masking), DNS, HTTPS, understanding OSI layers 1-4, and 7, the different types of firewalls and how they work, NAT, private addressing, and at least a cursory understanding of IPv6.",
          "score": 2,
          "created_utc": "2026-02-03 13:52:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bbtip",
          "author": "AWSSupport",
          "text": "Hi there!\n\nTo access your AWS Academy labs, log in at https://go.aws/3Mh8l3z. You'll see your Dashboard with all active courses and can access the Learner Labs from there.\n\n\\- Kay B.",
          "score": 1,
          "created_utc": "2026-02-03 09:24:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bsthi",
          "author": "KayeYess",
          "text": "I completed Network Specialty certification last year. Have been using AWS for a while. So, I only used AWS SkillBuilder for prepping. While I didn't pay for anything, if you need more hands-on, SkillBuilder has some subscription based sessions too\nhttps://skillbuilder.aws/category/exam-prep/advanced-networking-specialty-ANS-C01\n\n\nAWS Academy is another option.\n\n\nThere are many 3rd party trainings too, like TutorialsDojo. I haven't used them myself but some people said they were helpful.",
          "score": 1,
          "created_utc": "2026-02-03 11:57:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvi031",
      "title": "AWS EKS networking question",
      "subreddit": "aws",
      "url": "https://www.reddit.com/r/aws/comments/1qvi031/aws_eks_networking_question/",
      "author": "DopeyMcDouble",
      "created_utc": "2026-02-04 07:20:08",
      "score": 4,
      "num_comments": 5,
      "upvote_ratio": 0.84,
      "text": "Hello all, I have a question on this process. Currently we have 4 VPCs:\n\n* dev\n* stage\n* production\n* internal\n\nWe have dev, stage, production and not internal yet.\n\nMy plan is to host our Gitlab server, Grafana stack, and VPN server all on internal VPC CIDR. Now, we will be hosting the Grafana stack and Gitlab runners on the EKS cluster; however, I do have a question though.\n\nWould it be correct to set the EKS cluster's \"Cluster Endpoint access\" to \"Private\" and use Transit Gateway to have the internal VPC CIDR communicate to all other VPC CIDRs (i.e. dev, stage, production)? I have seen companies setup a \"Public and Private\" setup where Security Groups were paramount in the setup for access.  \n  \nWould appreciate any help or documentation on this.",
      "is_original_content": false,
      "link_flair_text": "technical resource",
      "permalink": "https://reddit.com/r/aws/comments/1qvi031/aws_eks_networking_question/",
      "domain": "self.aws",
      "is_self": true,
      "comments": [
        {
          "id": "o3kd657",
          "author": "sirstan",
          "text": "\\> We have dev, stage, production and not internal yet.\n\nSetup AWS Organization before you move forward.  Setup a billing account.  An org account.  And four accounts (dev, stage, prod, internal).",
          "score": 5,
          "created_utc": "2026-02-04 17:34:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ocnix",
              "author": "GuyWithLag",
              "text": "This. It will make billing much cleaner than whatever tagging system you come up with.",
              "score": 1,
              "created_utc": "2026-02-05 06:44:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jgytw",
          "author": "Living_off_coffee",
          "text": "I'm not too sure about the endpoint setup, but I'd suggest you go a bit further with your isolation and have separate aws accounts for dev/prod/infrastructure. You can link them together with an organisation so you still only pay one bill.\n\nThis way, you have a greater separation of concerns - you have less risk of something in dev breaking prod and you can set tighter restrictions on prod.",
          "score": 2,
          "created_utc": "2026-02-04 15:04:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3im9yl",
          "author": "alex_aws_solutions",
          "text": "I would do it as you mentioned as well. Cluster Endpoint Private and than use the TGW. Be aware of the proper  config of DNS and Routing.",
          "score": 1,
          "created_utc": "2026-02-04 12:09:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}