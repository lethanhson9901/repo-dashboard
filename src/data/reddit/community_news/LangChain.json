{
  "metadata": {
    "last_updated": "2026-02-16 17:16:25",
    "time_filter": "week",
    "subreddit": "LangChain",
    "total_items": 20,
    "total_comments": 53,
    "file_size_bytes": 86773
  },
  "items": [
    {
      "id": "1r1o9hf",
      "title": "EpsteinFiles-RAG: Building a RAG Pipeline on 2M+ Pages",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r1o9hf/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-11 05:01:03",
      "score": 106,
      "num_comments": 18,
      "upvote_ratio": 0.96,
      "text": "I love playing around with RAG and AI, optimizing every layer to squeeze out better performance. Last night I thought: why not tackle something massive?\n\nTook the Epstein Files dataset from Hugging Face (teyler/epstein-files-20k) ‚Äì 2 million+ pages of trending news and documents. The cleaning, chunking, and optimization challenges are exactly what excites me.\n\nWhat I built:\n\n\\- Full RAG pipeline with optimized data processing\n\n\\- Processed 2M+ pages (cleaning, chunking, vectorization)\n\n\\- Semantic search & Q&A over massive dataset\n\n\\- Constantly tweaking for better retrieval & performance\n\n\\- Python, MIT Licensed, open source\n\nWhy I built this:\n\nIt‚Äôs trending, real-world data at scale, the perfect playground.\n\nWhen you operate at scale, every optimization matters. This project lets me experiment with RAG architectures, data pipelines, and AI performance tuning on real-world workloads.\n\nRepo: [https://github.com/AnkitNayak-eth/EpsteinFiles-RAG](https://github.com/AnkitNayak-eth/EpsteinFiles-RAG)\n\nOpen to ideas, optimizations, and technical discussions!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1r1o9hf/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o4w6ivw",
          "author": "BeerBatteredHemroids",
          "text": "This is the best post I've seen on this thread in a long time üòÇ",
          "score": 4,
          "created_utc": "2026-02-11 23:53:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xjyvo",
              "author": "Cod3Conjurer",
              "text": "Haha appreciate that üòÑ just experimenting and sharing what I‚Äôm building.",
              "score": 2,
              "created_utc": "2026-02-12 05:03:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sitcm",
          "author": "Tengoles",
          "text": "How much space is needed to store the huggingface dataset + JSONS + vector DB?",
          "score": 3,
          "created_utc": "2026-02-11 13:00:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ti4vu",
              "author": "Cod3Conjurer",
              "text": "huggingface dataset- 250mb  \nvector DB - 1.5gb  \n",
              "score": 3,
              "created_utc": "2026-02-11 16:05:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4xx64k",
                  "author": "Cobra_venom12",
                  "text": "From where did you learn hugging face and other stuff.",
                  "score": 2,
                  "created_utc": "2026-02-12 06:55:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4u31mi",
          "author": "generate-addict",
          "text": "Is this the same post from /r/epstein?\n\nYou‚Äôre missing the Jan 30 dataset so the content in your DB likely isn‚Äôt as interesting .",
          "score": 2,
          "created_utc": "2026-02-11 17:43:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xl5b4",
              "author": "Cod3Conjurer",
              "text": "Yeah, I‚Äôm aware the Jan 30 dataset isn‚Äôt included. I tried finding updated versions, but most of the newer mirrors seem to have been taken down.",
              "score": 1,
              "created_utc": "2026-02-12 05:12:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4xzgdq",
                  "author": "generate-addict",
                  "text": "Really? All the torrent links are failing? They should be working. I think dataset 11 is still slow and seeding out there. \n\nhttps://github.com/yung-megafone/Epstein-Files",
                  "score": 1,
                  "created_utc": "2026-02-12 07:16:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xwpua",
          "author": "SharpRule4025",
          "text": "2M+ pages is where the ingestion pipeline design really matters. At that scale, the bottleneck shifts from retrieval quality to processing throughput and storage costs.\n\nCurious about your chunking approach for scanned documents. OCR quality on these types of files is usually inconsistent, lots of headers, page numbers, redaction markers mixed in with actual content. Are you doing any pre-filtering to strip out low-information chunks before they hit the vector store?\n\nThe storage question above is worth answering too. At 2M pages, your vector DB size can get significant depending on your embedding dimensions and whether you're storing the raw text alongside the vectors.",
          "score": 2,
          "created_utc": "2026-02-12 06:51:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zjo9v",
              "author": "Cod3Conjurer",
              "text": "shifted from pure semantic search to **MMR**, which reduced redundant chunks and improved retrieval quality.",
              "score": 1,
              "created_utc": "2026-02-12 14:38:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rnfwn",
          "author": "Kooky-Breadfruit-837",
          "text": "Extract out everything about the terrornation israel",
          "score": 6,
          "created_utc": "2026-02-11 08:38:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s4c1t",
              "author": "Cod3Conjurer",
              "text": "Ha haa ü§£",
              "score": 1,
              "created_utc": "2026-02-11 11:14:08",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4x4maw",
              "author": "Sungog1",
              "text": "That‚Äôs a pretty intense focus! If you‚Äôre looking for specific data about Israel, maybe try filtering by keywords or key events in your processing steps. What kind of insights are you hoping to extract?",
              "score": 1,
              "created_utc": "2026-02-12 03:17:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4uifs8",
          "author": "Cobra_venom12",
          "text": "I'm looking to start from the absolute basics of RAG. Beyond just 'using' a tool, what are the fundamental concepts (like embeddings or vector math) I should grasp first so I actually understand what's happening under the hood? I'd love a recommendation on a 'Step 1' resource for someone starting at zero.",
          "score": 1,
          "created_utc": "2026-02-11 18:54:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xvagu",
              "author": "fishishuntingsharks",
              "text": "For embeddings in particular, I really like https://jalammar.github.io/illustrated-word2vec/\n\nFor how LLMs work in general, there is also a great video by 3blue1brown https://youtu.be/LPZh9BOjkQs?si=9QJWSXTTZJM5G7oe",
              "score": 1,
              "created_utc": "2026-02-12 06:38:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4t2pb2",
          "author": "Exciting_Passage5443",
          "text": "Indian product is just a mess¬†",
          "score": 1,
          "created_utc": "2026-02-11 14:51:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r38uf7",
      "title": "I built a Recursive Language Model (RLM) with LangGraph that spawns child agents to beat context rot",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r38uf7/i_built_a_recursive_language_model_rlm_with/",
      "author": "DolphinSyndrome",
      "created_utc": "2026-02-12 23:18:56",
      "score": 27,
      "num_comments": 2,
      "upvote_ratio": 0.97,
      "text": "Hey r/LangChain üëã\n\nI built¬†**Fractal Context**¬†‚Äî a LangGraph implementation of Recursive Language Models that solves the \"context rot\" problem by letting an LLM¬†**recursively spawn child agents**¬†to process large text.\n\n**The problem:**¬†When you stuff a massive document into an LLM, attention degrades ‚Äî details in the middle get \"forgotten\" and the model starts hallucinating. This is context rot.\n\n**The solution:**¬†Instead of cramming everything into one prompt, the parent agent:\n\n1. Evaluates if the context is too large\n2. Uses a Python REPL to slice the text into chunks\n3. Calls¬†`delegate_subtask` ¬†to spawn a¬†**child agent**¬†at¬†`depth + 1`\n4. Each child processes its chunk and reports back\n5. The parent synthesizes all answers\n\nThe recursion is depth-limited to prevent runaway chains.\n\n**The \"Glass Box\" UI:**¬†Built with Chainlit, the UI shows nested steps in real-time so you can actually¬†*see*¬†the recursion happening:\n\n* üß†¬†**Thinking‚Ä¶**¬†‚Äî LLM reasoning (token by token)\n* üíª¬†**Coding‚Ä¶**¬†‚Äî when the agent writes Python to slice text\n* üîÄ¬†**Sub-Agent (Depth N)**¬†‚Äî child agents spawning and reporting\n\n**Tech stack:**\n\n* LangGraph (StateGraph with conditional edges)\n* LangChain + Groq API (Llama 3.3 70B)\n* Chainlit for the UI\n* Python 3.11+\n\n**Repo:**¬†[github.com/Dolphin-Syndrom/fractal-context](https://github.com/Dolphin-Syndrom/fractal-context)\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r38uf7/i_built_a_recursive_language_model_rlm_with/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o54lfa3",
          "author": "Don_Ozwald",
          "text": "I just wish people would use the word recursion appropriately. \n\nBut I like what you describe with the UI, well done there!\n\nEdit: I see now your implementation is much closer to actual recursion than what it usually is when the term ‚ÄúRecursive language model‚Äù is thrown around. Bravo!",
          "score": 2,
          "created_utc": "2026-02-13 07:35:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55sy2o",
              "author": "DolphinSyndrome",
              "text": "Thanks, i appreciate you taking a second look. Ik the term gets thrown around lossely so i tried my best to make the architecture authentically recursive as per the published paper",
              "score": 2,
              "created_utc": "2026-02-13 13:31:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3mp8b",
      "title": "Semantic chunking + metadata filtering actually fixes RAG hallucinations",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r3mp8b/semantic_chunking_metadata_filtering_actually/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-13 11:25:21",
      "score": 26,
      "num_comments": 10,
      "upvote_ratio": 0.96,
      "text": "I noticed that most people don't realize their chunking and retrieval strategy might be causing their RAG hallucinations.\n\nFixed-size chunking (split every 512 tokens regardless of content) fragments semantic units. Single explanation gets split across two chunks. Tables lose their structure. Headers separate from data. The chunks going into your vector DB are semantically incoherent.\n\nI've been testing semantic boundary detection instead where I use a model to find where topics actually change. Generate embeddings for each sentence, calculate similarity between consecutive ones, split when it sees sharp drops. The results are variable chunks but each represents a complete clear idea.\n\nThis alone gets 2-3 percentage points better recall but the bigger win for me was adding metadata. I pass each chunk through an LLM to extract time periods, doc types, entities, whatever structured info matters and store that alongside the embedding.\n\nThis metadata filters narrow the search space first, then vector similarity runs on that subset. Searching 47 relevant chunks instead of 20,000 random ones.\n\nFor complex documents with inherent structure this seems obviously better than fixed chunking. Anyway thought I should share. :)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r3mp8b/semantic_chunking_metadata_filtering_actually/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o55zr5a",
          "author": "timmy166",
          "text": "We haven‚Äôt used naive fixed-size chunking since the first pass at RAG last year.\n\nA simple summarization pass does wonders before getting into more advanced collection techniques.",
          "score": 3,
          "created_utc": "2026-02-13 14:08:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5g6by6",
              "author": "Sungog1",
              "text": "Summarization really is a game changer. It helps maintain context and coherence, especially with complex documents. What techniques do you find most effective for summarizing before chunking?",
              "score": 1,
              "created_utc": "2026-02-15 03:14:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5iuk6v",
                  "author": "timmy166",
                  "text": "It entirely depends on the downstream agent‚Äôs task and the dimensionality of the data. I work with a lot of source code so I might prefer to summarize the import statements as a first pass to get the skeleton of a repo. Then if I were to start using the codebase, I might want to get function signatures before extracting the actual logic. \n\nMost frontier models now can ingest a whole file (depending on lines of code and if it‚Äôs a generated file). I‚Äôm even working with more deterministic summarization passes since source code is so well structured if you know the syntax of the language.",
                  "score": 1,
                  "created_utc": "2026-02-15 15:47:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o55af1j",
          "author": "Independent-Cost-971",
          "text": "Wrote up a more detailed explanation if anyone's interested:¬†[https://kudra.ai/metadata-enriched-retrieval-the-next-evolution-of-rag/](https://kudra.ai/metadata-enriched-retrieval-the-next-evolution-of-rag/)\n\nGoes into the different semantic chunking approaches (embedding similarity detection, LLM-driven structural analysis, proposition extraction) and the full metadata enrichment pipeline. Probably more detail than necessary but figured it might help someone else debugging the same issues.",
          "score": 5,
          "created_utc": "2026-02-13 11:25:47",
          "is_submitter": true,
          "replies": [
            {
              "id": "o5p5era",
              "author": "inguz",
              "text": "ok, good writeup - how are the results from \"proposition extraction\"?  Does that only apply to certain types of document, or with a certain granularity of structure that you're targeting?",
              "score": 1,
              "created_utc": "2026-02-16 15:38:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o56mc4z",
          "author": "pbalIII",
          "text": "You're right that fixed-size chunking shreds structure, especially when headers and tables get split away from the values. Calling it a hallucination fix is a bit too generous, you can tighten retrieval and still get confident wrong answers during synthesis.\n\nWhat's helped me is an answer contract: extract the exact supporting spans first, then write, and refuse if nothing supports it. And on metadata, treat it as a scoring hint not a hard filter if the tags come from a model, otherwise one bad tag can hide the best chunk.",
          "score": 2,
          "created_utc": "2026-02-13 16:01:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fv9ai",
              "author": "aimless_rider",
              "text": "I like the scoring hint idea for metadata! my first thought was that this steers the behavior towards mimicking keyword search...",
              "score": 1,
              "created_utc": "2026-02-15 01:58:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5glbvf",
                  "author": "pbalIII",
                  "text": "nah it's structured attrs not content terms",
                  "score": 2,
                  "created_utc": "2026-02-15 05:06:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5gqtc5",
                  "author": "pbalIII",
                  "text": "Regression sets start getting noisy around 200-300 cases... past that you spend more time maintaining stale examples than catching real regressions. For LLM-as-judge, 30-50 gold-labeled examples gets you surprisingly far on calibration, but you need to re-run alignment checks whenever you swap the judge model or change rubric criteria. On rollback triggers, pure metrics miss the weird qualitative stuff (model confidently generating plausible but wrong answers), so we do metric gates plus a human spot-check on a random sample from each deployment.",
                  "score": 1,
                  "created_utc": "2026-02-15 05:52:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o56ayq3",
          "author": "Savings_Divide_9164",
          "text": "What model are you using for the boundary detection?",
          "score": 1,
          "created_utc": "2026-02-13 15:06:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4hhvp",
      "title": "Good UI / UX solution for langchain deployments",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r4hhvp/good_ui_ux_solution_for_langchain_deployments/",
      "author": "ddewaele",
      "created_utc": "2026-02-14 10:37:26",
      "score": 19,
      "num_comments": 14,
      "upvote_ratio": 1.0,
      "text": "We really like LangChain as an AI orchestration engine but we're seeing a shift that a lot of our customers come to expect more autonomy in defining agents / configuring models / managing data and knowledge bases themselves.\n\n  \nFor that a good UI / UX experience is required and that is something that Langchain is currently not able to provide. It lacks an off the shelf UI / UX solutions.\n\n  \nWe've tried using [https://github.com/langchain-ai/agent-chat-ui](https://github.com/langchain-ai/agent-chat-ui), customizing it a little bit (adding OIDC connectors and stuff), but it is not something we necessarily want to spend time on. You would have to build a lot of features on top of it to make it useful (sharing chats / multi-user chats / agent config / prompt mgmt / memory system). Langchain offers that on the backend via langsmith, but this is not really user-friendly.\n\nSolutions like LibreChat already offer a really nice UI/UX experience.\n\nWhat do you think the strategic vision of LangChain is with regards to this. \n\nDo they keep focussing on the engine and believe people should build their UI / UX solutions themselves ? \n\nShould all customizations be done on the LangGraph platform side of things (Langsmith)\n\nIs LangChain ChatUI the way to go ? (extending it yourself).\n\nHooking up LangChain to Librehcat (via Agent-as-a-tool) seems very limiting and also forces you to use 2 systems, 2 types of message threading , configuration , ....)\n\n  \nWonder what the communities thoughts are on this.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r4hhvp/good_ui_ux_solution_for_langchain_deployments/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o5bmbc7",
          "author": "Otherwise_Wave9374",
          "text": "I have hit the same gap: LangChain/LangGraph is great as an orchestration layer, but most teams eventually need a real \"agent console\" for non-devs (prompt/version mgmt, tool permissions, KB/data connectors, chat sharing, audit logs, evals).\n\nOne path I have seen work is pairing the backend with a generic chat UI, then incrementally adding an admin surface just for agent config and observability. This overview might help frame the pieces: https://www.agentixlabs.com/blog/",
          "score": 4,
          "created_utc": "2026-02-14 11:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bmap5",
          "author": "Diao_nasing",
          "text": "Both assistant ui and copilotkit provide more complete ui, but assistant ui is not friendly to local deployment, and copilotkit has more bugs.",
          "score": 2,
          "created_utc": "2026-02-14 11:02:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cvlbz",
              "author": "ddewaele",
              "text": "Haven't tried either of them but will take a look.   \n  \nWas hoping the langchain team was going to give [https://github.com/langchain-ai/agent-chat-ui](https://github.com/langchain-ai/agent-chat-ui) some love but i have the impression they have a habit of launching stuff and then quickly abandoning it, leaving it in a pre-alpha state.",
              "score": 1,
              "created_utc": "2026-02-14 16:03:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5c0nxv",
          "author": "AccountantGlad9947",
          "text": "I‚Äôm building on top of chainlit for the UI. Was great and feature rich. Unsure what the future holds for the project but I like it.",
          "score": 1,
          "created_utc": "2026-02-14 13:03:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cvxu2",
              "author": "ddewaele",
              "text": "Do you use LangGraph platform and deploy your graphs to langgraph ? Or just embedding langchain in your own systems / backends",
              "score": 1,
              "created_utc": "2026-02-14 16:05:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5cx6m6",
                  "author": "AccountantGlad9947",
                  "text": "I use langgraph on my own servers",
                  "score": 1,
                  "created_utc": "2026-02-14 16:11:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5c7y2l",
          "author": "JasperTesla",
          "text": "What about a few boilerplate applications? The user may start off with one idea, and then find out they really prefer something else.",
          "score": 1,
          "created_utc": "2026-02-14 13:51:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cwr3h",
              "author": "ddewaele",
              "text": "Customers these day expect the following out of the box (without custom development) : \n\n\\- the ability to create their own agents (with prompts / tools / knowledge)  \n\\- want to link agents together (multi-agency via handoffs or tools)  \n\\- want to be able to share their agents  \n\\- have easy integration with their identity provider (azure / google / ...)  \n\\- have easy integration with their knowledge base (sharepoint / drive / ....)\n\nNot something you'll easily vibe-code into existence\n\nAt the end of the day I think 90% of the people are happy with a generic chat interface type application (like chatgpt). these things are multi-modal and very flexible.\n\nIn some cases you might want some agentic flows embedded in custom UI / UX, but I would say today that this is a minority of the cases\n\n\n\n",
              "score": 2,
              "created_utc": "2026-02-14 16:09:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5cz60v",
                  "author": "JasperTesla",
                  "text": "Hmm, I see. So basically they want an AI that makes AIs. Very amusing, but a fun challenge!\n\nWho are your customers, by the way? Are they other developers or people who use AI without knowing? Would they be okay with a low-code platform where they can drag-and-drop pre-built components?\n\nThe workflow you're describing sounds a bit like an n8n or LangDock workflow. n8n specifically would solve all of your issues, and I do know LangDock has a mechanism where you can explain to an AI what you want, and it builds the workflow itself. That might be worth trying out, or at least using as an inspiration.\n\nI think the best would be a system akin to Oracle VBCS, but with AI stuff, and focus on microservices with customisability.",
                  "score": 1,
                  "created_utc": "2026-02-14 16:21:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5cr03b",
          "author": "International_Quail8",
          "text": "I built a custom UI using the AG-UI protocol. I stopped using CoPilotKit and went to the protocol layer instead. It‚Äôs easy to work with and customize and bonus: you‚Äôll actually understand the protocol!",
          "score": 1,
          "created_utc": "2026-02-14 15:39:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5csuzc",
              "author": "ddewaele",
              "text": "Does the custom UI also allow customers to create their own agents / prompts / knowledge ? \n\nThat's the main drawback we see as it requires a lot of custom dev to get all of that in place. Our clients aren't always willing to fund this type of development.\n\nNot to mention security, chat sharing , multi-user chats, ....  This would almost need to be a like a strategic thing within a company to put the time and effort in. (some context : we're a software development company delivering AI solutions to many different clients).  We don't think our added value should be in delivering a UI/UX experience for that. People nowadays see lots of platforms where you can create an agent , add some prompts and some documents and you have an agentic system. They also want this level of autonomy.\n\nWith an app like Librechat you get a lot of that stuff for free. But there is no clean way to integrate langchain into it, and Librechat's approach to multi agent systems (using handoffs) is more limited to what langchain has to offer.",
              "score": 1,
              "created_utc": "2026-02-14 15:49:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ct68v",
          "author": "ar_tyom2000",
          "text": "I've been working a lot with LangGraph and ran into this once graphs get large - understanding execution paths, branching, and why an agent behaved a certain way quickly becomes hard. I ended up building [https://github.com/proactive-agent/langgraphics](https://github.com/proactive-agent/langgraphics) to make complex graphs easier to follow and reason about during runs. My takeaway so far is that the ecosystem is still very engine-centric, and the operability/UX layer around real deployments is only starting to take shape.",
          "score": 1,
          "created_utc": "2026-02-14 15:50:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d4ztn",
          "author": "jannemansonh",
          "text": "lol this is literally the problem we kept running into too. ended up just using needle (needle.app) instead of trying to duct-tape a UI together. might be worth a look if you don't want to spend months building agent config screens and knowledge base integrations from scratch",
          "score": 1,
          "created_utc": "2026-02-14 16:50:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0dvgp",
      "title": "How do you handle agent-to-agent discovery as you scale past 20+ agents?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r0dvgp/how_do_you_handle_agenttoagent_discovery_as_you/",
      "author": "Sea-Perception1619",
      "created_utc": "2026-02-09 19:28:42",
      "score": 15,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "We're running about 30 specialized agents (mix of LangGraph and custom) and the coordination is getting painful. Right now everything goes through a central orchestrator that maintains a registry of who can do what. It works but it's fragile ‚Äî orchestrator went down last week and everything stopped.\n\nCurious how other teams are handling this:\n\n* How do your agents find each other's capabilities?\n* What breaks first as you add more agents?\n* Anyone running agents across multiple teams/orgs? How do you handle discovery across boundaries?\n* Is anyone using MCP or A2A for this, and how's that going?\n\nNot looking for a specific tool recommendation ‚Äî more interested in architectural patterns that work at scale.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1r0dvgp/how_do_you_handle_agenttoagent_discovery_as_you/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o4iurm5",
          "author": "ArmOk3290",
          "text": "This feels a lot like service discovery plus workflow orchestration, with LLMs making the edges fuzzier.\n\nWhat has worked for me is separating three things:\n- Capability registry as data, not a single process. Put it in a durable store and replicate it. Treat updates as events.\n- Routing as stateless. The orchestrator can die and come back because it only reads registry plus current task state.\n- Execution as durable jobs. If an agent dies mid task, you can retry or reassign based on idempotent steps.\n\nFor agent to agent calls, I would keep a small, versioned contract for each tool or capability and require each agent to self report health and supported versions. At 30 plus agents, the first thing that breaks is observability, so I would invest early in traces and per agent quotas so one bad loop cannot take the whole system down.",
          "score": 6,
          "created_utc": "2026-02-09 23:40:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4iwmzg",
              "author": "Sea-Perception1619",
              "text": "This is a clean separation. The registry-as-data pattern with event-driven updates is something I keep seeing in mature setups ‚Äî it avoids the SPOF of a registry process while keeping things consistent.\n\nCurious about the routing layer ‚Äî when multiple agents can handle the same capability and you need to pick one, what's the selection logic? Round-robin, random, or something quality-aware? And does the routing improve over time based on past outcomes, or is it static once the registry is populated?\n\nThe observability point resonates. Per-agent traces and quotas before anything else at scale.",
              "score": 2,
              "created_utc": "2026-02-09 23:50:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4klqca",
                  "author": "TheExodu5",
                  "text": "I'm speaking out of my depth here as I don't yet have any experience at this scale, but what about using a reranking model to choose the best agent if multiple fit the criteria? You'd need to store agent metadata as embeddings, but that would be cheap.",
                  "score": 2,
                  "created_utc": "2026-02-10 06:19:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mc12s",
                  "author": "fasti-au",
                  "text": "Shrug first in locks item.  Timing based allocation to groups",
                  "score": 1,
                  "created_utc": "2026-02-10 14:32:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mbryl",
          "author": "fasti-au",
          "text": "Hold control of the strings and use real-time state control and then task based then is queues based.   It‚Äôs just state checking various sources and needing a green or red light for a I‚Äôd in a queue.",
          "score": 1,
          "created_utc": "2026-02-10 14:31:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xx9lv",
          "author": "YUYbox",
          "text": "Hi , if you want better results from your agents you should try InsAIts:\nhttps://github.com/Nomadu27/InsAIts\n\n    InsAIts V2.5 features: Edge/Hybrid Swarm Routing (routes embeddings across local/cloud/edge with privacy modes: STRICT/BALANCED/PERFORMANCE, get_embedding() failover); AI Lineage Oracle (provenance tracking: record_message(), verify_chain_integrity(), export_audit_trail() for GDPR/HIPAA compliance); Decipher enhancements., anchor-aware detection and forensic visualization. \nIf you find this helpfull please star the repo and install free: https://github.com/Nomadu27/InsAIts ‚Äì 100 free keys for early adopters.'",
          "score": 1,
          "created_utc": "2026-02-12 06:56:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r499cg",
      "title": "GuardLLM, hardened tool calls for LLM apps",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r499cg/guardllm_hardened_tool_calls_for_llm_apps/",
      "author": "MapDoodle",
      "created_utc": "2026-02-14 02:58:14",
      "score": 15,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I keep seeing LLM agents wired to tools with basically no app-layer safety. The common failure mode is: the agent ingests untrusted text (web/email/docs), that content steers the model, and the model then calls a tool in a way that leaks secrets or performs a destructive action. Model-side ‚Äúbe careful‚Äù prompting is not a reliable control once tools are involved.\n\nSo I open-sourced GuardLLM, a small Python ‚Äúsecurity middleware‚Äù for tool-calling LLM apps:\n\n* Inbound hardening: isolate and sanitize untrusted text so it is treated as data, not instructions.\n* Tool-call firewall: gate destructive tools behind explicit authorization and fail-closed human confirmation.\n* Request binding: bind tool calls (tool + canonical args + message hash + TTL) to prevent replay and arg substitution.\n* Exfiltration detection: secret-pattern scanning plus overlap checks against recently ingested untrusted content.\n* Provenance tracking: stricter no-copy rules for known-untrusted spans.\n* Canary tokens: generation and detection to catch prompt leakage into outputs.\n* Source gating: reduce memory/KG poisoning by blocking high-risk sources from promotion.\n\nIt is intentionally application-layer: it does not replace least-privilege credentials or sandboxing; it sits above them.\n\nRepo: [https://github.com/mhcoen/guardllm](https://github.com/mhcoen/guardllm)\n\nI‚Äôd like feedback on:\n\n* Threat model gaps I missed\n* Whether the default overlap thresholds work for real summarization and quoting workflows\n* Which framework adapters would be most useful (LangChain, OpenAI tool calling, MCP proxy, etc.)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1r499cg/guardllm_hardened_tool_calls_for_llm_apps/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o5a4dtg",
          "author": "AdditionalWeb107",
          "text": "I like this idea - but you should check out FilterChains from plano: https://github.com/katanemo/plano. This would make for a great integration point",
          "score": 3,
          "created_utc": "2026-02-14 03:13:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5a5eh7",
              "author": "MapDoodle",
              "text": "Thanks for the pointer! Looks like they'd play nicely together",
              "score": 1,
              "created_utc": "2026-02-14 03:20:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5a609e",
                  "author": "AdditionalWeb107",
                  "text": "they do - the whole idea behind a filter chain is to hand off to a pre/post processor so that the request can be mutated or secured ahead of the agent. ",
                  "score": 1,
                  "created_utc": "2026-02-14 03:24:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r30gjb",
      "title": "The MCP thread got me paranoid about community skills and supply chain risks",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r30gjb/the_mcp_thread_got_me_paranoid_about_community/",
      "author": "Independent_Plum_489",
      "created_utc": "2026-02-12 17:59:43",
      "score": 14,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "That discussion about MCPs being outdated and agents moving toward raw CLI access sent me down a rabbit hole I wasn't expecting. Been experimenting with OpenClaw recently (hard to ignore 160K stars) and saw someone in the GitHub issues flag a calendar integration skill that was requesting file system write access and network permissions for something that should just be reading a schedule. They dug into the code and found base64 encoded strings that decoded to external URLs and some sketchy eval statements. Maintainer removed it pretty quick but it had already been up for a few days.\n\nStarted googling after reading that thread and honestly got a bit worried. Apparently a decent chunk of community built skills have been flagged for doing sketchy things like data collection or downloading external payloads. Can't verify the exact numbers myself but it tracks with what I've been seeing in issue trackers. OpenClaw's own docs call the whole setup a \"Faustian bargain\" which... yeah.\n\nFeels like we're repeating the npm left-pad era except now the stakes are higher because these agents have real permissions. Read your emails, browse authenticated pages, execute shell commands. One bad skill and you've basically handed over the keys.\n\nSo now I'm being paranoid about everything. Manual code review when I have patience for it, though I'm slow at spotting obfuscated stuff. Checking GitHub issues and recent commits before installing anything. Running everything in Docker with network monitoring just in case. I've tried throwing Semgrep at some skills with mixed results, poked around with Snyk and Agent Trust Hub too. Even grep for obvious patterns like base64 or eval. The automated scanners all feel like security theater though when you're dealing with prompt injection vectors they weren't really designed for.\n\nStarting to think the only real answer is extreme permission isolation but that defeats half the usefulness of these agents. There's probably no good solution here.\n\nWhat does your vetting process look like before installing community stuff? Or is the general approach to just run everything sandboxed and hope for the best?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r30gjb/the_mcp_thread_got_me_paranoid_about_community/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r2n74g",
      "title": "Open Source Kreuzberg v4.3.0 and benchmarks",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r2n74g/open_source_kreuzberg_v430_and_benchmarks/",
      "author": "Eastern-Surround7763",
      "created_utc": "2026-02-12 07:32:00",
      "score": 13,
      "num_comments": 3,
      "upvote_ratio": 0.88,
      "text": "Hi all,\n\nI have two announcements related to¬†[Kreuzberg](https://github.com/kreuzberg-dev/kreuzberg):\n\n1. We released our new¬†[comparative benchmarks](https://kreuzberg.dev/benchmarks). These have a slick UI and we have been working hard on them for a while now (more on this below), and we'd love to hear your impressions and get some feedback from the community!\n2. We released v4.3.0, which brings in a bunch of improvements including PaddleOCR as an optional backend, document structure extraction, and native Word97 format support. More details below.\n\n# What is Kreuzberg?\n\n[Kreuzberg](https://github.com/kreuzberg-dev/kreuzberg)¬†is an open-source (MIT license) polyglot document intelligence framework written in Rust, with bindings for Python, TypeScript/JavaScript (Node/Bun/WASM), PHP, Ruby, Java, C#, Golang and Elixir. It's also available as a docker image and standalone CLI tool you can install via homebrew.\n\nIf the above is unintelligible to you (understandably so), here is the TL;DR: Kreuzberg allows users to extract text from 75+ formats (and growing), perform OCR, create embeddings and quite a few other things as well. This is necessary for many AI applications, data pipelines, machine learning, and basically any use case where you need to process documents and images as sources for textual outputs.\n\n# Comparative Benchmarks\n\nOur new comparative benchmarks UI is live here:¬†[https://kreuzberg.dev/benchmarks](https://kreuzberg.dev/benchmarks)\n\nThe comparative benchmarks compare Kreuzberg with several of the top open source alternatives - Apache Tika, Docling, Markitdown, [Unstructured.io](http://Unstructured.io), PDFPlumber, Mineru, MuPDF4LLM. In a nutshell - Kreuzberg is 9x faster on average, uses substantially less memory, has much better cold start, and a smaller installation footprint. It also requires less system dependencies to function (only¬†**optional**¬†system dependency for it is onnxruntime, for embeddings/PaddleOCR).\n\nThe benchmarks measure throughput, duration, p99/95/50, memory, installation size and cold start with more than 50 different file formats. They are run in GitHub CI on ubuntu latest machines and the results are published into GitHub releases (here is an¬†[example](https://github.com/kreuzberg-dev/kreuzberg/releases/tag/benchmark-run-21923145045)). The¬†[source code](https://github.com/kreuzberg-dev/kreuzberg/tree/main/tools/benchmark-harness)¬†for the benchmarks and the full data is available in GitHub, and you are invited to check it out.\n\n# V4.3.0 Changes\n\nThe v4.3.0 full release notes can be found here:¬†[https://github.com/kreuzberg-dev/kreuzberg/releases/tag/v4.3.0](https://github.com/kreuzberg-dev/kreuzberg/releases/tag/v4.3.0)\n\nKey highlights:\n\n1. PaddleOCR optional backend - in Rust. Yes, you read this right, Kreuzberg now supports PaddleOCR in Rust and by extension - across all languages and bindings except WASM. This is a big one, especially for Chinese speakers and other east Asian languages, at which these models excel.\n2. Document structure extraction - while we already had page hierarchy extraction, we had requests to give document structure extraction similar to Docling, which has very good extraction. We now have a different but up to par implementation that extracts document structure from a huge variety of text documents - yes, including PDFs.\n3. Native Word97 format extraction - wait, what? Yes, we now support the legacy¬†`.doc`¬†and¬†`.ppt`¬†formats directly in Rust. This means we no longer need LibreOffice as an optional system dependency, which saves a lot of space. Who cares you may ask? Well, usually enterprises and governmental orgs to be honest, but we still live in a world where legacy is a thing.\n\n# How to get involved with Kreuzberg\n\n* Kreuzberg is an open-source project, and as such contributions are welcome. You can check us out on GitHub, open issues or discussions, and of course submit fixes and pull requests. Here is the GitHub:¬†[https://github.com/kreuzberg-dev/kreuzberg](https://github.com/kreuzberg-dev/kreuzberg)\n* We have a¬†[Discord Server](https://discord.gg/rzGzur3kj4)¬†and you are all invited to join (and lurk)!\n\nThat's it for now. As always, if you like it -- star it on GitHub, it helps us get visibility!",
      "is_original_content": false,
      "link_flair_text": "Announcement",
      "permalink": "https://reddit.com/r/LangChain/comments/1r2n74g/open_source_kreuzberg_v430_and_benchmarks/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o4yvj92",
          "author": "___cjg___",
          "text": "Nice, thanx!",
          "score": 1,
          "created_utc": "2026-02-12 12:13:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o523b13",
          "author": "GiveMeAegis",
          "text": "Are you located in xberg?",
          "score": 1,
          "created_utc": "2026-02-12 21:53:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54wbg1",
              "author": "Eastern-Surround7763",
              "text": "yup, best place in berlin",
              "score": 2,
              "created_utc": "2026-02-13 09:16:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0sd5p",
      "title": "Looked into OpenClaw security after the MCP discussion here and the numbers are worse than I expected",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r0sd5p/looked_into_openclaw_security_after_the_mcp/",
      "author": "Drysetcat",
      "created_utc": "2026-02-10 05:32:54",
      "score": 13,
      "num_comments": 2,
      "upvote_ratio": 0.81,
      "text": "Been setting up OpenClaw for a side project (local automation stuff, nothing crazy) and the recent thread about MCPs being outdated got me thinking about the actual security posture of what I'm running. Did some digging and found research from Gen Threat Labs that honestly made me reconsider my setup.\n\nThe big one: over 18,000 OpenClaw instances are currently exposed to the public internet. That's not instances running locally as intended, that's port 18789 sitting open for anyone to poke at. Given that these agents often have filesystem access, shell execution, and credentials to various services, that's a lot of attack surface just sitting there. Made me immediately go check my own firewall rules.\n\nThe other number that stood out: their analysis claims nearly 15% of community skills contain malicious instructions. Now I'm genuinely not sure how they verified that or what threshold they used for \"malicious\" so take it with some salt. But even if the real number is half that it's pretty concerning. Apparently when bad skills get flagged and removed from ClawHub they frequently reappear under different names which tracks with what I've seen in other package ecosystems.\n\nHonestly the OpenClaw FAQ itself is refreshingly blunt about this being a \"Faustian bargain\" with no \"perfectly safe\" setup. The power comes from deep system access which is exactly what creates the exposure. I respect the transparency but it does make me reconsider how casually I've been treating this stuff. I had my instance connected to my actual email for testing which in retrospect was pretty dumb.\n\nThe concept that stuck with me is what the research called \"delegated compromise\" where attackers don't need to target you directly, they just compromise the agent and inherit whatever permissions you gave it. Obvious in hindsight but I hadn't really thought about my agents as high value targets in their own right. That realization is what finally got me to actually change my setup instead of just thinking \"I should probably fix this eventually.\"\n\nI've since moved everything into a Docker container with network set to none except when I explicitly need external access, and stripped permissions down to just filesystem read on a single project directory mounted as a volume. No email, no shell execution, no browser. Basically treating it like I would any random npm package from an unknown author.\n\nWhat security practices are others here using? Curious whether people are actually running these in isolated environments or just going full send on their dev machines. For those who do vet skills before installing, what does your workflow look like? I've seen a few scanner tools floating around (something called Agent Trust Hub and a couple others) but haven't tried any yet and manually reviewing every skill is getting tedious.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r0sd5p/looked_into_openclaw_security_after_the_mcp/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o4lwm15",
          "author": "tabdon",
          "text": "I did a google search just for fun to see what security practices are. Found the following link (no affiliation). Just skimming through it a lot of interesting things pop out.  \n[https://aimaker.substack.com/p/openclaw-security-hardening-guide](https://aimaker.substack.com/p/openclaw-security-hardening-guide)\n\nSo there's clear need for OpenClaw. If someone can crack the \"secure and easy to use\" nut, there be gold.",
          "score": 2,
          "created_utc": "2026-02-10 13:06:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nkgqt",
          "author": "felixthekraut",
          "text": "The only secure way to run OpenClaw is to not run it at all.",
          "score": 1,
          "created_utc": "2026-02-10 18:03:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3i9eq",
      "title": "A lovable like application that utilizes LangChain, deep agents, tools, and MCP servers.",
      "subreddit": "LangChain",
      "url": "https://i.redd.it/8nxd51bkl7jg1.png",
      "author": "ban_rakash",
      "created_utc": "2026-02-13 06:51:11",
      "score": 12,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r3i9eq/a_lovable_like_application_that_utilizes/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o54pgvt",
          "author": "t12e_",
          "text": "For file operations have a look at the opencode codebase. The agent has tools for handling files so you can get inspo from there. For creating apps you might need to fork opencode, make your app start a server in the background, and have it run commands or use the api to manage sessions (or chats)",
          "score": 1,
          "created_utc": "2026-02-13 08:12:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54hz6d",
          "author": "Otherwise_Wave9374",
          "text": "This sounds like a fun build. For file ops with agents, the thing that helped me most was forcing a \"plan then propose diff\" step.\n\nLike, agent reads workspace -> produces an explicit operations list (create/edit/delete) -> you validate -> only then execute tools. And for MCP servers, having a strict schema for tool outputs saves a ton of time.\n\nIf you want a few concrete patterns for tool design and multi-step agents, I have seen good examples here: https://www.agentixlabs.com/blog/",
          "score": 0,
          "created_utc": "2026-02-13 07:03:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2mdz1",
      "title": "Detecting infinite loops in LangGraph multi-agent systems (before tokens explode)",
      "subreddit": "LangChain",
      "url": "https://i.redd.it/5orp23uxe0jg1.png",
      "author": "Responsible-Peach503",
      "created_utc": "2026-02-12 06:42:47",
      "score": 12,
      "num_comments": 4,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r2mdz1/detecting_infinite_loops_in_langgraph_multiagent/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4yt71l",
          "author": "SharpRule4025",
          "text": "The loop pattern you're describing usually starts with bad data coming back from the web_search tool. The research agent gets partial or garbage HTML, passes it to analysis, analysis says insufficient data, and research goes back to search again with slightly different params. Rinse repeat.\n\nFixing it at the detection layer works but the cleaner solution is making the data step reliable. If web_search consistently returns clean structured content instead of raw HTML soup, the analysis agent has enough signal to move forward on the first pass. Most of the loops I've debugged trace back to the agent retrying because the input data was technically there but unusable.",
          "score": 3,
          "created_utc": "2026-02-12 11:55:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yttcz",
              "author": "Responsible-Peach503",
              "text": "You‚Äôre absolutely right , a lot of coordination loops do originate from poor tool output.\n\nIf web\\_search returns messy or partially structured data, analysis often says there‚Äôs not enough signal, and research just tries again with slightly tweaked params. That definitely creates retry loops. In those cases improving the data layer can solve the root issue.\n\nWhat I‚Äôm trying to look at here is a slightly different class of loops. I‚Äôve seen systems loop even when the data is clean, just because there‚Äôs no clear ‚Äúdone‚Äù condition, or agents keep optimizing instead of converging, or the planner state doesn‚Äôt reflect real progress. Sometimes two agents just implicitly disagree on whether the task is complete.\n\nSo I see Watchtower more as a runtime guardrail than a fix for bad tools. It doesn‚Äôt replace improving data quality, but it can surface pathological behavior early, regardless of the cause.\n\nOut of curiosity, in your experience were most loops purely data-quality issues, or did you also see coordination-level ones?",
              "score": 1,
              "created_utc": "2026-02-12 12:00:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4xw3cb",
          "author": "Responsible-Peach503",
          "text": "Repo: [github.com/yairsabag/watchtower](http://github.com/yairsabag/watchtower) ‚Äî clone, run python -m demo.loop\\_demo, takes 30 seconds. Happy to hear feedback.",
          "score": 1,
          "created_utc": "2026-02-12 06:45:48",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5254ic",
          "author": "SystemFlowStudio",
          "text": "In LangGraph specifically, loops usually show up as repeated node traversals with no net state change.\n\nTwo practical techniques:  \n‚Ä¢ hash the serialized graph state at each step and abort on repeats  \n‚Ä¢ enforce monotonic progress (a value that must strictly decrease or converge)\n\nAlso worth separating ‚Äúretry‚Äù edges from normal edges ‚Äî when retries are implicit, loops are invisible until the bill shows up.",
          "score": 1,
          "created_utc": "2026-02-12 22:02:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1zpai",
      "title": "memv ‚Äî open-source memory for AI agents that only stores what it failed to predict",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r1zpai/memv_opensource_memory_for_ai_agents_that_only/",
      "author": "brgsk",
      "created_utc": "2026-02-11 15:02:55",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I built an open-source memory system for AI agents with a different approach to knowledge extraction.\n\nThe problem: Most memory systems extract every fact from conversations and rely on retrieval to sort out what matters. This leads to noisy knowledge bases full of redundant information.\n\nThe approach: memv uses predict-calibrate extraction (based on the [https://arxiv.org/abs/2508.03341](https://arxiv.org/abs/2508.03341)). Before extracting knowledge from a new conversation, it predicts what the episode should contain given existing knowledge. Only facts that were unpredicted ‚Äî the prediction errors ‚Äî get stored. Importance emerges from surprise, not upfront LLM scoring.\n\nOther things worth mentioning:\n\n* Bi-temporal model ‚Äî every fact tracks both when it was true in the world (event time) and when you learned it (transaction time). You can query \"what did we know about this user in January?\"\n* Hybrid retrieval ‚Äî vector similarity (sqlite-vec) + BM25 text search (FTS5), fused via Reciprocal Rank Fusion\n* Contradiction handling ‚Äî new facts automatically invalidate conflicting old ones, but full history is preserved\n* SQLite default ‚Äî zero external dependencies, no Postgres/Redis/Pinecone needed\n* Framework agnostic ‚Äî works with LangGraph, CrewAI, AutoGen, LlamaIndex, or plain Python\n\n\n```python\n    from memv import Memory\n    from memv.embeddings import OpenAIEmbedAdapter\n    from memv.llm import PydanticAIAdapter\n    \n    memory = Memory(\n        db_path=\"memory.db\",\n        embedding_client=OpenAIEmbedAdapter(),\n        llm_client=PydanticAIAdapter(\"openai:gpt-4o-mini\"),\n    )\n    \n    async with memory:\n        await memory.add_exchange(\n            user_id=\"user-123\",\n            user_message=\"I just started at Anthropic as a researcher.\",\n            assistant_message=\"Congrats! What's your focus area?\",\n        )\n        await memory.process(\"user-123\")\n        result = await memory.retrieve(\"What does the user do?\", user_id=\"user-123\")\n```\n\nMIT licensed. Python 3.13+. Async everywhere.  \n\\- GitHub: [https://github.com/vstorm-co/memv](https://github.com/vstorm-co/memv)  \n\\- Docs: [https://vstorm-co.github.io/memv/](https://vstorm-co.github.io/memv/)  \n\\- PyPI: [https://pypi.org/project/memvee/](https://pypi.org/project/memvee/)\n\nEarly stage (v0.1.0). Feedback welcome ‚Äî especially on the extraction approach and what integrations would be useful.",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1r1zpai/memv_opensource_memory_for_ai_agents_that_only/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o4xwov2",
          "author": "arul-ql",
          "text": "Super interesting approach! Any benchmarks yet on noise reduction vs mem0-style ‚Äústore everything‚Äù approaches?\n\nWould love to see precision/recall + db growth curves.",
          "score": 1,
          "created_utc": "2026-02-12 06:51:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5itf52",
              "author": "brgsk",
              "text": "Thanks! No benchmarks as of now, it‚Äôs too early for that (in my opinion). Will do benchmarks once we stabilise the library",
              "score": 2,
              "created_utc": "2026-02-15 15:42:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hmjuh",
          "author": "Landyn_4682",
          "text": "The predict-calibrate idea is interesting. We ran into the opposite problem where agents stored everything and slowly drowned in their own history. Moving toward storing only what actually changes the model‚Äôs expectations makes a lot of sense. We‚Äôve been experimenting with Hindsight for a similar reason, focusing on evolving conclusions instead of accumulating raw facts. Curious how memv handles gradual belief updates when something isn‚Äôt a hard contradiction but a soft shift over time.",
          "score": 1,
          "created_utc": "2026-02-15 10:54:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5iuvmy",
              "author": "brgsk",
              "text": "good question ‚Äî memv currently handles hard contradictions (new fact supersedes old with temporal bounds) but gradual belief drift is an open problem. observation consolidation like Hindsight does could complement predict-calibrate well ‚Äî one decides what's worth storing, the other how beliefs evolve over time. on my radar for future work. what was your experience with Hindsight's consolidation in practice?",
              "score": 1,
              "created_utc": "2026-02-15 15:49:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r2xv1b",
      "title": "Where can I learn to build my own AI agent framework? Any solid video courses?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r2xv1b/where_can_i_learn_to_build_my_own_ai_agent/",
      "author": "Feisty-Promise-78",
      "created_utc": "2026-02-12 16:23:42",
      "score": 10,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm trying to go beyond using existing tools and actually learn how to design and build my own AI agentic framework from scratch.\n\nI‚Äôm especially interested in topics like:\n\n* Tool use and function calling\n* Planning and memory\n* Multi-agent systems\n* Orchestration and evaluation\n* Building real projects instead of just theory\n\nDoes anyone know of **good video tutorials or courses** that cover this well? YouTube series, Udemy courses, paid bootcamps, anything.\n\nI‚Äôd love recommendations that are practical and hands-on rather than purely conceptual.\n\nIf you‚Äôve learned this yourself, what resources helped you the most?",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1r2xv1b/where_can_i_learn_to_build_my_own_ai_agent/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o50ic0h",
          "author": "PretendPop4647",
          "text": "You can follow langchain academy course.  Read their blogs, documentation. It helps me a lot. I watch random video on YouTube. \n\nAnthropic's engineering blog  are highly recommended. \n\nBtw, i was exploring langchain - deepagent. It's give some power what claude code have.\nUsing this, i built an agent,  i used tool call, subagents, file system etc.. \n\nYou can explore this to understand how it works \n\nRepo : https://github.com/Rahat-Kabir/job-search-agent",
          "score": 3,
          "created_utc": "2026-02-12 17:23:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50jzak",
              "author": "Feisty-Promise-78",
              "text": "I m learning for job and interviews. I have done some research and many startup have built their own AI agent framework. I already have used langchain and langgraph. But thanks!",
              "score": 2,
              "created_utc": "2026-02-12 17:31:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o50kd5f",
                  "author": "PretendPop4647",
                  "text": "good to know. you are already good at. best of luck your jobs and interview.  what type of jobs you are looking for? remote?",
                  "score": 1,
                  "created_utc": "2026-02-12 17:33:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o52iw6c",
          "author": "Fanof07",
          "text": "I‚Äôve mostly learned by following LangChain and LlamaIndex tutorials and building small projects Hands on stuff teaches multi agent setups best",
          "score": 3,
          "created_utc": "2026-02-12 23:13:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55ljki",
          "author": "Worth_Rabbit_6262",
          "text": "If you have Udemy you could try with the course: \"LangChain- Develop AI Agents with LangChain & LangGraph\" made by Eden Marco, engineer for Google. I'm studying with this but at the moment I can't find any use cases for the practice but I have to force myself\n\n",
          "score": 2,
          "created_utc": "2026-02-13 12:47:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o561avt",
              "author": "Feisty-Promise-78",
              "text": "In this course, does the instructor use LangChain v1 or earlier versions?",
              "score": 1,
              "created_utc": "2026-02-13 14:17:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o563sle",
                  "author": "Worth_Rabbit_6262",
                  "text": "\"**COURSE¬†WAS¬†RE-RECORDED¬†and supports- LangChain Version 1.0+**\"",
                  "score": 1,
                  "created_utc": "2026-02-13 14:30:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o53umla",
          "author": "Preconf",
          "text": "Building frameworks for anything is not a trivial undertaking. For the likes of python and js/ts it requires being comfortable enough with the language to be able to effectively leverage relatively advanced features.",
          "score": 1,
          "created_utc": "2026-02-13 04:03:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55sk0h",
          "author": "wheres-my-swingline",
          "text": "[here?](https://github.com/humanlayer/12-factor-agents)",
          "score": 1,
          "created_utc": "2026-02-13 13:29:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o560vnw",
              "author": "Feisty-Promise-78",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-13 14:15:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o56l1u9",
          "author": "pbalIII",
          "text": "Built a multi-tool agent loop from raw API calls before touching any framework, and that's honestly what clicked for me. Start with just the tool-calling loop... send a prompt, parse the function call, execute it, feed the result back, repeat until the model stops requesting tools. That's the entire skeleton.\n\nOnce that works, the next unlock is separating planning from execution. Have the model output a plan first, then execute each step. Without that split, anything beyond 2-3 tools falls apart because the model loses track of where it is.\n\nAnthropic's engineering blog and OpenAI's practical guide to building agents PDF both walk through this architecture. For interview prep specifically, being able to explain why you'd add memory, retry logic, or a supervisor pattern on top of that bare loop is more valuable than knowing any specific framework's API.",
          "score": 1,
          "created_utc": "2026-02-13 15:55:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56o6g2",
              "author": "Feisty-Promise-78",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-02-13 16:10:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5920wa",
          "author": "Ok-Priority35",
          "text": "Use an intelligent code editor like antigravity or cursor to manage an openclaw agent",
          "score": 1,
          "created_utc": "2026-02-13 23:15:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51fvft",
          "author": "alimhabidi",
          "text": "Take a look at this live workshop\n\nhttps://www.eventbrite.com/e/build-ai-agents-over-the-weekendcohort-3-tickets-1980455085473",
          "score": 0,
          "created_utc": "2026-02-12 20:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53k72u",
          "author": "Remote-Evening1437",
          "text": "Hey there! It's great you're diving into building your own AI agent frameworks. That's a powerful area! While AmenLink focuses on spiritual AI assistance rather than AI agent development courses, I can offer some general advice from the tech world perspective that might be helpful. For practical, hands-on learning in AI agent frameworks, I'd suggest looking into:\n\n\n\n1.  Official documentation and examples from libraries like LangChain or LlamaIndex. They often have solid tutorials that walk you through building components like tool use, planning, and memory.\n\n2.  YouTube channels that focus on MLOps, AI engineering, or specific framework tutorials. Many independent creators offer deep dives into building custom agents.\n\n3.  Udemy/Coursera often have courses specifically on \"building AI agents\" or \"LLM application development\" which would cover your listed interests like orchestration and evaluation. Look for courses with high ratings and recent updates.\n\n4.  For multi-agent systems, research papers and their accompanying open-source implementations are gold. Platforms like Hugging Face often host these projects.\n\n\n\nMany learned by just jumping in, building small projects, and debugging their way through. Good luck with your learning journey ‚Äì it's a fascinating field!",
          "score": 0,
          "created_utc": "2026-02-13 02:55:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1u6qk",
      "title": "activefence quietly rebranded to alice, anyone notice?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r1u6qk/activefence_quietly_rebranded_to_alice_anyone/",
      "author": "Aggravating_Log9704",
      "created_utc": "2026-02-11 10:47:58",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 0.92,
      "text": "just saw in some t&s newsletter that activefence the moderation company behind content filtering for a bunch of big platforms is now going by alice. happened back on Jan 14, 2026, new site is [alice.io](http://alice.io) and old one redirects.\n\nfrom what i can tell, its mostly a branding update as they shift more toward ai/genai safety stuff like guardrails for models, handling prompt attacks, that kind of thing while keeping the core ugc moderation side. apis and tools seem unchanged so far.\n\nanyone using them run into issues with the name change in tickets orsupport or is it just a logo refresh?\n\n(Their blog post:[ https://alice.io/blog/why-we-became-alice](https://alice.io/blog/why-we-became-alice))\n\nThoughts?",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LangChain/comments/1r1u6qk/activefence_quietly_rebranded_to_alice_anyone/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o4s2ya1",
          "author": "Kitchen_West_3482",
          "text": "This seems like a strategic pivot rather than just a logo change. They are signaling a broader focus on AI and GenAI safety and prompt attack mitigation which aligns with the growing LangChain ecosystem. For anyone using their moderation APIs the main thing to watch is support continuity and naming in tickets. Operationally it is probably fine but the rebrand indicates they want to be perceived as a full spectrum AI safety company not just UGC moderators.",
          "score": 2,
          "created_utc": "2026-02-11 11:02:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r122yn",
      "title": "Dlovable  is an open-source, AI-powered web UI/UX",
      "subreddit": "LangChain",
      "url": "https://i.redd.it/lwt5s1cjfoig1.png",
      "author": "LeadingFun1849",
      "created_utc": "2026-02-10 14:23:34",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r122yn/dlovable_is_an_opensource_aipowered_web_uiux/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r47zi8",
      "title": "What are the best LangChain practical tutorials?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r47zi8/what_are_the_best_langchain_practical_tutorials/",
      "author": "LargeSinkholesInNYC",
      "created_utc": "2026-02-14 01:59:10",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "I want to build leading edge stuffs, so I was wondering if there were practical tutorials that would help me get my foot in the door. Feel free to share.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r47zi8/what_are_the_best_langchain_practical_tutorials/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o5agh58",
          "author": "Hackerjurassicpark",
          "text": "Use Claude code to build exactly what you need without any of these harnesses like langchain",
          "score": 2,
          "created_utc": "2026-02-14 04:40:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a40vx",
          "author": "KalZaxSea",
          "text": "I generaly refer their youutbe channel and docs. Bcs there are lots of thing that we need to consider and their docs and tutorials cover them.\n\n  \nFor example: What happens when you reload history but changed the system prompt in a tool call, will it load as old system prompt or changed version",
          "score": 1,
          "created_utc": "2026-02-14 03:11:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bvbju",
          "author": "shadowcorp",
          "text": "[LangChain Academy](https://academy.langchain.com) is good, as are most of the courses on DeepLearning.ai.",
          "score": 1,
          "created_utc": "2026-02-14 12:23:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ohfoq",
          "author": "Kooky-Elephant8905",
          "text": "I learned it through campusx youtube great teacher.",
          "score": 1,
          "created_utc": "2026-02-16 13:32:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4k671",
      "title": "I built an python AI agent framework that doesn't make me want to mass-delete my venv",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r4k671/i_built_an_python_ai_agent_framework_that_doesnt/",
      "author": "anandesh-sharma",
      "created_utc": "2026-02-14 13:06:32",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hey all. I've been building [https://github.com/definableai/definable.ai](https://github.com/definableai/definable.ai) \\- a Python framework for AI agents. I got frustrated with existing options being either too bloated or too toy-like, so I built what I actually wanted to use in production.\n\n\n\nHere's what it looks like:\n\n    ```from definable.agents import Agent\n    from definable.models.openai import OpenAIChat\n    from definable.tools.decorator import tool\n    from definable.interfaces.telegram import TelegramInterface, TelegramConfig\n    \n    @tool\n    def search_docs(query: str) -> str:\n        \"\"\"Search internal documentation.\"\"\"\n        return db.search(query)\n    \n    agent = Agent(\n        model=OpenAIChat(id=\"gpt-5.2\"),\n        tools=[search_docs],\n        instructions=\"You are a docs assistant.\",\n    )\n    \n    # Use it directly\n    response = agent.run(\"Steps for configuring auth?\")\n    \n    # Or deploy it ‚Äî HTTP API + Telegram bot in one line\n    agent.add_interface(TelegramInterface(\n        config=TelegramConfig(bot_token=os.environ[\"TELEGRAM_BOT_TOKEN\"]),\n    ))\n    agent.serve(port=8000)\n    \n\n\n\n**What My Project Does**\n\nPython framework for AI agents with built-in cognitive memory, run replay, file parsing (14+ formats), streaming, HITL workflows, and one-line deployment to HTTP + Telegram/Discord/Signal. Async-first, fully typed, non-fatal error handling by design.\n\n\n\n**Target Audience**\n\nDevelopers building production AI agents who've outgrown raw API calls but don't want LangChain-level complexity. v0.2.6, running in production.\n\n\n\n**Comparison**\n\n\\- \\*\\*vs LangChain\\*\\* - No chain/runnable abstraction. Normal Python. Memory is multi-tier with distillation, not just a chat buffer. Deployment is built-in, not a separate project.\n\n\\- \\*\\*vs CrewAI/AutoGen\\*\\* - Those focus on multi-agent orchestration. Definable focuses on making a single agent production-ready: memory, replay, file parsing, streaming, HITL.\n\n\\- \\*\\*vs raw OpenAI SDK\\*\\* - Adds tool management, RAG, cognitive memory, tracing, middleware, deployment, and file parsing out of the box.\n\n\n\n\\`*pip install definable*\\`\n\n\n\nWould love feedback. Still early but it's been running in production for a few weeks now.\n\n\n\n[https://github.com/definableai/definable.ai](https://github.com/definableai/definable.ai)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r4k671/i_built_an_python_ai_agent_framework_that_doesnt/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o5cob3h",
          "author": "bsampera",
          "text": "It looks painfully similar to langchain. Can you repeat what are the advantages without using buzzwords?",
          "score": 1,
          "created_utc": "2026-02-14 15:25:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ks7hm",
              "author": "anandesh-sharma",
              "text": "The structure overall is simple, now i have covered most of the hard parts here like interfaces, cognitive memory,etc. And you want customisation you can simply extend functionality and change the behaviour.\n\nAnd thats nearly impossible to manage with langchain. Its more performant wrt langchain.",
              "score": 1,
              "created_utc": "2026-02-15 21:33:01",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5ksdas",
              "author": "anandesh-sharma",
              "text": "rest u can also take a look at, https://docs.definable.ai",
              "score": 1,
              "created_utc": "2026-02-15 21:33:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5oi5an",
          "author": "Feisty-Promise-78",
          "text": "Hi, this is really cool. Recently I have been trying to learn to build an AI framework by myself. Is there any resource that helped you build this framework? Can you share it with me?",
          "score": 1,
          "created_utc": "2026-02-16 13:36:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5lb28",
      "title": "I built an autonomous agent with DeepAgents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r5lb28/i_built_an_autonomous_agent_with_deepagents/",
      "author": "Releow",
      "created_utc": "2026-02-15 18:01:47",
      "score": 7,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[CianaParrot](https://preview.redd.it/rq9dgmdy6pjg1.png?width=1024&format=png&auto=webp&s=57714055a2897397227f67ff326251af488456eb)\n\nHi\n\nI built this project for myself because I wanted full control over what my personal assistant does and the ability to modify it quickly whenever I need to. I decided to share it on GitHub here's the link: [https://github.com/emanueleielo/ciana-parrot](https://github.com/emanueleielo/ciana-parrot)\n\nIf you find it useful, leave a star or some feedback\n\n  \n\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r5lb28/i_built_an_autonomous_agent_with_deepagents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o5jrehc",
          "author": "hwchase17",
          "text": "very very cool!",
          "score": 2,
          "created_utc": "2026-02-15 18:27:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5jtixt",
              "author": "Releow",
              "text": "thank u!",
              "score": 1,
              "created_utc": "2026-02-15 18:37:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r470hr",
      "title": "We Benchmarked 7 Chunking Strategies. Most 'Best Practice' Advice Is Wrong.",
      "subreddit": "LangChain",
      "url": "https://www.runvecta.com/blog/we-benchmarked-7-chunking-strategies-most-advice-was-wrong",
      "author": "Confident-Honeydew66",
      "created_utc": "2026-02-14 01:13:26",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1r470hr/we_benchmarked_7_chunking_strategies_most_best/",
      "domain": "runvecta.com",
      "is_self": false,
      "comments": [
        {
          "id": "o59xnzq",
          "author": "timmy166",
          "text": "The article is touting a tool‚Äôs accuracy improvements but the model selection is several generations back - touting  more of a cost-savings value proposition.\n\nWe‚Äôre missing a key control group:\nFrontier models with full document retrieval.",
          "score": 2,
          "created_utc": "2026-02-14 02:30:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4olni",
      "title": "I built a visual execution tracking for LangGraph workflows",
      "subreddit": "LangChain",
      "url": "https://github.com/proactive-agent/langgraphics",
      "author": "ar_tyom2000",
      "created_utc": "2026-02-14 16:15:02",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r4olni/i_built_a_visual_execution_tracking_for_langgraph/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5iy8yp",
          "author": "ar_tyom2000",
          "text": "https://i.redd.it/bkymbofulojg1.gif\n\nJust one line of code is all it takes to visualize your LangGraph agent's workflow in real-time as it executes. Any feedback?",
          "score": 1,
          "created_utc": "2026-02-15 16:06:00",
          "is_submitter": true,
          "replies": []
        }
      ]
    }
  ]
}