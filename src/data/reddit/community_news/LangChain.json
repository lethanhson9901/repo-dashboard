{
  "metadata": {
    "last_updated": "2026-02-24 17:19:14",
    "time_filter": "week",
    "subreddit": "LangChain",
    "total_items": 20,
    "total_comments": 119,
    "file_size_bytes": 159807
  },
  "items": [
    {
      "id": "1r8k1qu",
      "title": "Building an opensource Living Context Engine",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/wo2lnacmfckg1",
      "author": "DeathShot7777",
      "created_utc": "2026-02-19 00:12:36",
      "score": 158,
      "num_comments": 25,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1r8k1qu/building_an_opensource_living_context_engine/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o666sm6",
          "author": "Reasonable-Froyo3181",
          "text": "Ok",
          "score": 3,
          "created_utc": "2026-02-19 02:22:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66anvg",
              "author": "DeathShot7777",
              "text": "Thanks",
              "score": 2,
              "created_utc": "2026-02-19 02:44:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67jxom",
          "author": "Msense_",
          "text": "Impressive!",
          "score": 3,
          "created_utc": "2026-02-19 08:23:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67k739",
              "author": "DeathShot7777",
              "text": "‚ù§Ô∏è",
              "score": 1,
              "created_utc": "2026-02-19 08:25:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o66irxh",
          "author": "SithLordRising",
          "text": "Very cool. Working on a thinking engine myself",
          "score": 2,
          "created_utc": "2026-02-19 03:33:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o677mug",
              "author": "DeathShot7777",
              "text": "Thanks. What's your approach?",
              "score": 1,
              "created_utc": "2026-02-19 06:32:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o696gin",
                  "author": "ble1901",
                  "text": "I'm focusing on integrating neural networks with a more interactive user interface. The goal is to make it easier for users to experiment and see real-time results. What about your thinking engine?",
                  "score": 2,
                  "created_utc": "2026-02-19 15:25:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6bzo69",
                  "author": "SithLordRising",
                  "text": "Just spent time going through the GitNexus codebase in detail. Really impressive work. A few things stood out:\n\n\\- The phased ingestion pipeline is cleanly separated - structure ‚Üí parse ‚Üí resolve ‚Üí relate ‚Üí cluster ‚Üí trace ‚Üí embed. That's a strong pattern.\n\n\\- Leiden community detection on the relationship graph is a smart choice. The heuristic labelling from folder/naming patterns is pragmatic.\n\n\\- Process tracing via BFS from scored entry points is elegant. The confidence tiers on CALLS edges (import-resolved ‚Üí same-file ‚Üí fuzzy-global) show good engineering judgment.\n\n\\- RRF for combining BM25 + semantic search is the right call over trying to normalize different score scales.\n\n\\- KuzuDB as the embedded graph store is an interesting choice - embedded like SQLite but with native Cypher traversal.\n\nI'm working on a knowledge infrastructure project where we compile documents - academic papers, technical standards, legal texts - into structured knowledge graphs with explicit relationships (prerequisites, dependencies, contradictions between sources). Think of it as applying the same intuition you've had about code to text: raw documents are to knowledge what source files are to architecture. Both need to be parsed into structured units and their relationships made explicit before you can reason about them properly. One of the differences we have is contradictions, something code typically fixes at compile.\n\nYour work has been genuinely useful for validating some architectural directions we've been considering, particularly around graph storage and hybrid search. The parallel between tracing execution flows through code and tracing reasoning chains through knowledge is  closer than I expected.\n\nGreat project. Following with interest.",
                  "score": 1,
                  "created_utc": "2026-02-19 23:46:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67ka5m",
          "author": "DeathShot7777",
          "text": "Thanks for all the github stars idk where they r coming from. But holly shit 496 stars üò≠",
          "score": 2,
          "created_utc": "2026-02-19 08:26:26",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o65ozs6",
          "author": "VanillaOk4593",
          "text": "Obsidian on steroids!",
          "score": 1,
          "created_utc": "2026-02-19 00:39:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65zx0p",
              "author": "DeathShot7777",
              "text": "üòÅ",
              "score": 1,
              "created_utc": "2026-02-19 01:42:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kg1jn",
          "author": "cleverhoods",
          "text": "wow",
          "score": 1,
          "created_utc": "2026-02-21 07:40:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ua7ib",
          "author": "Brave-Photograph9845",
          "text": "oh cool , i actually built something similar ... you can check it on [https://nomik.co/](https://nomik.co/)",
          "score": 1,
          "created_utc": "2026-02-22 21:10:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6udf6v",
              "author": "DeathShot7777",
              "text": "Cool, the website looks great. What DB r u using?",
              "score": 1,
              "created_utc": "2026-02-22 21:26:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6vbuaq",
          "author": "adspendagency",
          "text": "you have my attention",
          "score": 1,
          "created_utc": "2026-02-23 00:33:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6w3cgu",
              "author": "DeathShot7777",
              "text": "üòÅ",
              "score": 1,
              "created_utc": "2026-02-23 03:20:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6vjkng",
          "author": "rafapozzi",
          "text": "I think this is the future of AI. I'll come back a few years later and see.",
          "score": 1,
          "created_utc": "2026-02-23 01:19:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vxb0x",
          "author": "SuperIce07",
          "text": "I'm interested to know how does it work",
          "score": 1,
          "created_utc": "2026-02-23 02:42:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6w3afk",
              "author": "DeathShot7777",
              "text": "U can read the github code. Or DM me ü´†",
              "score": 1,
              "created_utc": "2026-02-23 03:20:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9u4uj",
      "title": "Noob question... is LangChain still relevant?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r9u4uj/noob_question_is_langchain_still_relevant/",
      "author": "Odd-Aside456",
      "created_utc": "2026-02-20 12:31:58",
      "score": 110,
      "num_comments": 76,
      "upvote_ratio": 0.92,
      "text": "I'm planning to build an AI personal assistant. First capabilities it will need include the standard assistant stuff: calendar, contracts, email, tasks, etc. But EVENTUALLY I'd like to build it up to be able to do autonomous work to along the lines of research, building tools, etc, and acting more like an employee than an agent (similarish to the whole OpenClaw hype, but much more on rails and personalized). Doing some research on tech stacks with LLMs, I keep getting pointed to LangChain and / or LangGraph. However, doing some Googling of my own, I keep finding people who say they've moved away from LangChain or that it's generally disliked (which I find hard to fully believe). Given the rapid pace at which new AI technologies are being developed, is LangChain / LangGraph still hyper-relevant today, and applicable for my end goal?",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1r9u4uj/noob_question_is_langchain_still_relevant/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o6f8r13",
          "author": "Friendly-Ask6895",
          "text": "LangChain gets a lot of hate but IMO most of it is from people who used it like a year ago when the API was changing every 2 weeks and the abstractions were pretty leaky. It's matured a lot since then. LangGraph specifically is actually really solid for the kind of agent workflows you're describing, especially when you need things like conditional branching, human-in-the-loop steps, and persistent state across conversations.\n\nThat said, for the basic personal assistant stuff (calendar, email, etc) you could honestly start with just raw API calls + function calling and get pretty far. The place where LangChain/LangGraph really starts paying off is when you want the agent to plan multi-step tasks, recover from errors, and maintain context across tool calls. Which sounds like exactly where you want to end up.\n\nI'd say start simple with direct API calls so you actually understand whats happening under the hood, then bring in LangGraph when you need the orchestration layer. Going straight to a framework before you understand the basics is how people end up confused when something breaks.",
          "score": 62,
          "created_utc": "2026-02-20 14:00:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fsrgi",
              "author": "Odd-Aside456",
              "text": "Thank you!",
              "score": 3,
              "created_utc": "2026-02-20 15:40:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6h69er",
              "author": "mzinz",
              "text": "In each of these scenarios (direct API calls with LLM; LangGraph agents) - what are the common ways for invoking? Dependent on how you want the trigger to occur I assume, but what are the most common setups?\n\nThinking about things like:\n\n* checking email occasionally and sending a notification when an urgent email is identified (probably Cron?)\n* work scenario: a ticket is cut when a document is ready for review/approval. Need a way to monitor for tickets of that type to come in, then invoke the agent/call in some way\n\n",
              "score": 2,
              "created_utc": "2026-02-20 19:27:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6gwqop",
              "author": "Singularity-42",
              "text": "Does LangChain support image models now?\n\nI've tried it for a real production project in early 2023 (and it was the TS version) and I was so disgusted by the experience that I swore I'd never go back to Langchain again. The only value I ever got out of it was vendor consolidation. But just cumbersome as fuck.",
              "score": 4,
              "created_utc": "2026-02-20 18:43:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6idatx",
              "author": "FMWizard",
              "text": "Code base is still spaghetti. Pedantic AI is super clean and reliable and their API is stable ie they are real software engineers.",
              "score": 1,
              "created_utc": "2026-02-20 23:04:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6iza49",
                  "author": "Budget_Bar2294",
                  "text": "the worst part of my amazing job is dealing with this crap ass framework. LangGraph is so much better but no one uses that. and the core abstractions in the Lang\\* ecosystem are awful anyway. the best part of my job is sneakily avoiding using LangChain whenever possible",
                  "score": 1,
                  "created_utc": "2026-02-21 01:12:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6obr05",
              "author": "municorn_ai",
              "text": "Langgraph is great and we made a hybrid version with HATEOAS to generate portal, voice and chat agents from a shared configuration for a consistent deterministic behavior. This is too much complexity unless you are looking to build a SaaS AI application. For most one off customer specific applications, the graph is pretty static usually and hard coded implementations are more efficient than incorporating complexity. LLMs thrive on static instructions and RAG is not always a value addition.",
              "score": 1,
              "created_utc": "2026-02-21 22:20:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6evmc3",
          "author": "Freed4ever",
          "text": "Langchain IMO has a lot of fluff that is not needed. Langgraph OTOH is solid. But you can just build your own if I were you.",
          "score": 26,
          "created_utc": "2026-02-20 12:45:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fsdck",
              "author": "Odd-Aside456",
              "text": "Good to know, thank you!",
              "score": 3,
              "created_utc": "2026-02-20 15:38:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ev3l9",
          "author": "Bubba_deets",
          "text": "if you care about long-term maintainability, keep the core logic modular so you‚Äôre not locked into any one framework",
          "score": 16,
          "created_utc": "2026-02-20 12:41:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fsboi",
              "author": "Odd-Aside456",
              "text": "Good advice, thank you.",
              "score": 1,
              "created_utc": "2026-02-20 15:38:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ffc2a",
          "author": "cleverhoods",
          "text": "I think LangGraph might still be relevant for those who want to work with graph based approach. However the more I dig into claude the more I question this thing. One thing LangGraph does absolutely fantastic - imo - is the visualisation of what is happening. ",
          "score": 4,
          "created_utc": "2026-02-20 14:35:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h7m9c",
              "author": "mzinz",
              "text": "Using LangStudio for visualization?",
              "score": 3,
              "created_utc": "2026-02-20 19:34:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hoqo5",
                  "author": "cleverhoods",
                  "text": "now reading back, yeah, that could be misunderstood. What I meant is that for agentic work I found LangSmith a great tool to \\*see what is going where, how token expensive it is, what goes to system prompt, to agent prompt, etc. Not \\*LangGraph for \\*visualization.",
                  "score": 3,
                  "created_utc": "2026-02-20 20:58:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6fszzg",
              "author": "Odd-Aside456",
              "text": "Thank you!",
              "score": 0,
              "created_utc": "2026-02-20 15:41:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fvq4y",
          "author": "PretendPop4647",
          "text": "As someone mentioned earlier,keep core things modular. \nCome the point, yes langchain especially deepagent is good.  They provide built in agent harness like file system,sub agent etc\n\nUsing it i built a job search agent.  you can check this out how i implement deepagent for get some idea.\n\nhttps://github.com/Rahat-Kabir/job-search-agent\n\nbtw when you finish your project, give us Update.",
          "score": 5,
          "created_utc": "2026-02-20 15:54:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fycs0",
              "author": "Odd-Aside456",
              "text": "Will do! And thank you!",
              "score": 2,
              "created_utc": "2026-02-20 16:07:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6f3ypj",
          "author": "adlx",
          "text": "Have a look at deepagents, by LangChain. Your Ai assistant is already built üòÇ",
          "score": 8,
          "created_utc": "2026-02-20 13:34:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fsees",
              "author": "Odd-Aside456",
              "text": "I'll check it out, thanks!",
              "score": 2,
              "created_utc": "2026-02-20 15:39:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fylnc",
          "author": "DavidtheLawyer",
          "text": "I find that Claude Code can accomplish much of this work, but I‚Äôm big fan of LangGraph.",
          "score": 3,
          "created_utc": "2026-02-20 16:08:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fzw0s",
              "author": "Odd-Aside456",
              "text": "I do love Claude Code. But I can't afford anything beyond the Pro plan right now, and I can't spend those precious tokens on an assistant at the moment.",
              "score": 4,
              "created_utc": "2026-02-20 16:14:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6g9jda",
                  "author": "DavidtheLawyer",
                  "text": "True, it does go hog sometimes",
                  "score": 2,
                  "created_utc": "2026-02-20 16:57:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ibab2",
          "author": "MathematicianSome289",
          "text": "Absolutely. It is the de-facto OSS choice. It‚Äôs why OSS-friendly companies like cloudflare have native support and why closed source companies like AWS built their own agent framework. They know langgraph is too portable given it‚Äôs ubiquity.",
          "score": 3,
          "created_utc": "2026-02-20 22:53:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z89n7",
          "author": "Ok-Ferret-534",
          "text": "Oh, so.\n\nI started to build an AI application for a real business 6 month ago.   \nChosed Langchain. I've looked for Pydantic AI and LlamaIndex too, but Langchain own my trust with Langgraph.  \nActually I use langchain only for core, focused on LangGraph.  \nThe application that I build is in BETA, but users who tested liked very much.  \nI modularized all things like tools, prompts, logic...   \nFor my expirence until now, add a new feature is really easy.",
          "score": 3,
          "created_utc": "2026-02-23 16:48:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fcqtx",
          "author": "Delicious-One-5129",
          "text": "Yes. LangChain is still a practical go to for prototyping and integrations.",
          "score": 5,
          "created_utc": "2026-02-20 14:21:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fsvp2",
              "author": "Odd-Aside456",
              "text": "\"For Prototyping,\" So what would you recommend for production, assuming a product got to that point?",
              "score": 4,
              "created_utc": "2026-02-20 15:41:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6hmb6z",
                  "author": "zhuki",
                  "text": "Its always prototyping, everything is just for prototyping, never production ü•≤",
                  "score": 12,
                  "created_utc": "2026-02-20 20:46:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6fe61z",
          "author": "Odd-Literature-5302",
          "text": "If you need more structured state and graph style workflows try LangGraph as a complement",
          "score": 2,
          "created_utc": "2026-02-20 14:28:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fsxc3",
              "author": "Odd-Aside456",
              "text": "Thank you!",
              "score": 2,
              "created_utc": "2026-02-20 15:41:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gu88x",
          "author": "micupa",
          "text": "Unpopular opinion: Langchain was very early.. we didn‚Äôt even need a framework to build on top of ai APIs.. now we are seeing the need but with a totally different approach: the problem isn‚Äôt the api or the code integration but the context and tools. Frameworks like openclaw are the new way of angetic frameworks, to build good ai assistant think in terms of a new layer of abstraction.",
          "score": 2,
          "created_utc": "2026-02-20 18:32:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6p74lf",
              "author": "vvitali26",
              "text": "Could you elaborate more on the problem with the langchain/langgraph approach in the modern environment comparing to openclaw, please?",
              "score": 1,
              "created_utc": "2026-02-22 01:30:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6r4wbp",
                  "author": "micupa",
                  "text": "I mean, Langchain is a framework for human developers, a layer of abstraction for managing APIs from different providers.. but openclaw is a framework for the agent. The user is the agent not the human.",
                  "score": 1,
                  "created_utc": "2026-02-22 10:57:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6hqmxv",
          "author": "HotMud9713",
          "text": "with agent skill, not anymore",
          "score": 2,
          "created_utc": "2026-02-20 21:07:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g4gn1",
          "author": "AlexRenz",
          "text": "Who are you building this for - your own personal use? Then I'd rather see how far you can get with Claude or n8n first and see if this gets you there. \n\nIf you want to ship something that's an app to use for others, scalable or that you can sell, deploy it somewhere, scale it, trace it etc. that's a whole different thing. \n\nI find LangGraph to be a great option (and I'm not really differentiating between Chain & Graph tbh). Others have mentioned already that a lot of the online reviews are pre-version 1.0 which just came out end of last year and improved a ton of stuff. \n\nYou'll want to use their pre-built agent and customize that one with integrations and middleware. Nobody can really tell you where all the frameworks are going, so keep your stuff modular as much as you can. But honestly, you can optimize for that later - get started and don't overthink this before you must. And often, a lot of value will lie in your context and prompts which are portable. ",
          "score": 2,
          "created_utc": "2026-02-20 16:34:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g5chv",
              "author": "Odd-Aside456",
              "text": "It's gonna start for personal use, but then I'm gonna ship it for some friends and family. If they receive it well, I'll probably make it public. Mainly for that reason I'm trying to stay provider/model agnostic.\n\nThanks so much for all the info!",
              "score": 2,
              "created_utc": "2026-02-20 16:38:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6gwahn",
                  "author": "EveYogaTech",
                  "text": "You could use Nyno (see my profile), unlike n8n it's open-source and it's based on multiple languages, so you can stay as agnostic as possible even though using a framework + GUI builder.\n\nLanguages currently supported: Python, Node, Typescript, PHP and Ruby.",
                  "score": 3,
                  "created_utc": "2026-02-20 18:41:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6kuef5",
                  "author": "AlexRenz",
                  "text": "Another question btw is how comfortable you're coding. \n\nIn what I've seen on many many products, it's best to lean towards least resistance early on - in your case put something together with Claude/ChatGPT or n8n and see what features you actually  need. Once you know that AND you hit a limit, write code. ",
                  "score": 1,
                  "created_utc": "2026-02-21 10:01:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6h739u",
              "author": "mzinz",
              "text": "What do you mean by pre-built agents? Re-usable code they offer, or a diff package or product?",
              "score": 2,
              "created_utc": "2026-02-20 19:31:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ktwv2",
                  "author": "AlexRenz",
                  "text": "They have a ReAct agent package that already gets you a core agent, to which one can add integrations and middleware: [https://docs.langchain.com/oss/python/langchain/agents](https://docs.langchain.com/oss/python/langchain/agents)\n\nThey also offer a \"Deep Agent\" which I feel is the same but with more built-in features: [https://docs.langchain.com/oss/python/deepagents/overview](https://docs.langchain.com/oss/python/deepagents/overview)  \n",
                  "score": 2,
                  "created_utc": "2026-02-21 09:56:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ffxdy",
          "author": "Hackerjurassicpark",
          "text": "Just feed your requirements to Claude code and let it build it from scratch. Langchain is unnecessary bloatware built for an age when the ability reuse code and design patterns were important. That is no longer the case now. Claude code can pretty much be your scaffolding using native components instead of an additional layer like Langchain.",
          "score": 1,
          "created_utc": "2026-02-20 14:38:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ft34s",
              "author": "Odd-Aside456",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-02-20 15:42:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6imf9r",
                  "author": "No_Indication_1238",
                  "text": "Do not listen to people in this sub. They are clueless. ",
                  "score": 4,
                  "created_utc": "2026-02-20 23:56:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6h6h5j",
              "author": "mzinz",
              "text": "If asking Claude to do it - would it make sense to ask it to use LangChain/LangGraph, though? If not, why?",
              "score": 1,
              "created_utc": "2026-02-20 19:28:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hxh3o",
                  "author": "Leo2000Immortal",
                  "text": "It's an extra layer of abstraction which does not really help much. It's easier to debug with less abstractions when things don't work as expected",
                  "score": 1,
                  "created_utc": "2026-02-20 21:41:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gkdyx",
          "author": "damanamathos",
          "text": "If you want to build an effective personal assistant, my recommendation is start with something like OpenCode or Claude Code (depending on which underlying model you want to use), then write custom command line tools that let it access calendar, email, tasks, image generation, whatever you want. Then give it skills so it can learn how to use all those commands when needed.\n\nI think this is the fastest way to get up to speed with building agents, which will help you think about how to build something more customised in code. The nice thing about using OpenCode is that it's open source so you can always see how they implement agents.\n\nI don't have a strong view on LangGraph as haven't seen their latest updates. I do use it in parts of my codebase for handling general LLM calls, and do have an older agent that runs on LangChain, though my more recent agents have all been custom code with direct calls to the provider APIs.",
          "score": 1,
          "created_utc": "2026-02-20 17:48:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hq2hx",
          "author": "sundevil21CS",
          "text": "I have found myself recently ditching langchain and graph and using models SDK structured outputs and manually chaining calls based off structured outputs.",
          "score": 1,
          "created_utc": "2026-02-20 21:04:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ignqe",
          "author": "coreofapples-",
          "text": "Use Temporal and suddenly your life becomes infinitely easier",
          "score": 1,
          "created_utc": "2026-02-20 23:23:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6irwyk",
          "author": "AdWorried6080",
          "text": "tbh if you want to learn with project start with scratch then you started going to know how this chains,agent, memory and tools are working. If you build small stuff and can check langchain repo than you‚Äôll say why langchain need to use or not. Langchain is good starter for early knowledge and quick build stuff. But when it comes to more customisation, integration, scalability and cost you‚Äôll not go for any framework. As per experience big organisation often build their POC with this frameworks but when it comes to make a product then they‚Äôll create from scratch. It‚Äôs more reliable, scalable, cost optimised(highly needed, lacks in this frameworks), customisable and maintainable.",
          "score": 1,
          "created_utc": "2026-02-21 00:28:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jrbxi",
          "author": "autoshag",
          "text": "LangGraph for sure. \nAnd then within the nodes I usually use Claude Agents SDK, or OpenCode SDK or langchain depending on the complexity of the task",
          "score": 1,
          "created_utc": "2026-02-21 04:13:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jvpfd",
          "author": "sergeant113",
          "text": "Use baml-ai for all the core LLM abstraction. Don‚Äôt bother with langChain at all.",
          "score": 1,
          "created_utc": "2026-02-21 04:45:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k2uj4",
          "author": "bornwithmistake",
          "text": "you can vibe code a LangChain equivalent for internal use, focused on your exact workflows, with cleaner abstractions and fewer moving parts than the open source stack",
          "score": 1,
          "created_utc": "2026-02-21 05:41:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ka0ej",
          "author": "themessymiddle",
          "text": "Personal experience - I‚Äôve tested our same agentic workflow with langchain, strands, and Claude agents sdk and the langchain one has the best results. I think it‚Äôs because you have so much more control. It‚Äôs more complex to manage than something like Claude agents sdk, but you have much more granular control",
          "score": 1,
          "created_utc": "2026-02-21 06:43:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kfn8w",
          "author": "AloneSYD",
          "text": "I would honestly try Agno as it's much simpler compared to lang*  frameworks",
          "score": 1,
          "created_utc": "2026-02-21 07:36:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kgyhx",
          "author": "SaltedFesh",
          "text": "Only use LangGraph to build graph based workflow and some LangChain necessary components like chunking text, and directly use OpenAI sdk to work with AI",
          "score": 1,
          "created_utc": "2026-02-21 07:48:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kpqm6",
          "author": "ParticularBasket6187",
          "text": "I used this and in future I‚Äôll definitely continue this. Have there some issue but it‚Äôs not big as such.",
          "score": 1,
          "created_utc": "2026-02-21 09:15:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kvxn7",
          "author": "ws6kid",
          "text": "TBH seems it‚Äôs only relevant in abstracted low code tools like n8n‚Ä¶ openclaw is eating its lunch in terms of autonomous agent capabilities even Claude code..",
          "score": 1,
          "created_utc": "2026-02-21 10:16:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lna0e",
          "author": "Gold_Emphasis1325",
          "text": "My understanding and people with more direct experience please correct/supplement: LanchChain is great abstraction, orchestration for linear flows. More branching and complex flows warrant LangGraph, another orchestrator. All of the other DevSecOps and software engineering are still 95% of the work and these are nifty and for people with the complexity to push them into the extra work -- game changing.",
          "score": 1,
          "created_utc": "2026-02-21 13:58:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lnzz8",
          "author": "Acrobatic_Task_6573",
          "text": "Interesting point. I ran into something similar when building multi-step chains. The config layer is way more important than it looks on the surface.",
          "score": 1,
          "created_utc": "2026-02-21 14:02:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lwsdh",
          "author": "RandomForest42",
          "text": "Just vibe code the whole thing without any dependencies, just like half the Internet is doing already",
          "score": 1,
          "created_utc": "2026-02-21 14:54:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mcbas",
          "author": "Cats4BreakfastPlz",
          "text": "I've been here before. Try to build with langchain/langgraph, end up wondering why its not working, can't get proper obsevability on whats going wrong, try and figure out what's wrong, just end up building my own that claude understands perfectly and doesn't have to guess around\n\nafter trying pretty hard for a while I'm pretty sure langgraph is moronic and pointless and only good for people who want to sound fancy on their resume. you'll notice most devs who are forced to use it will tell you they hate it and prefer to do it themselves but their bosses want them to use it.\n\nits like stringing templates together. none ofthem work exactly the way you want to and you end up having to implement your own custom solution the majority of the time.\n\nthese tools try to simplify things by giving it to you easy but they just end up making everything more difficult than it needs to be. especially these days where opus/sonnet can do almost anything.",
          "score": 1,
          "created_utc": "2026-02-21 16:14:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r13ny",
          "author": "Eugeniusz87",
          "text": "I still see langchain used in a bunch of projects, especially for prototyping and chaining LLMs with tools. It may not be perfect, but i've found it useful when i needed a quick integration and didn't want to build everything from scratch",
          "score": 1,
          "created_utc": "2026-02-22 10:21:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s3et3",
          "author": "coccoinomane",
          "text": "Give a chance to Pydantic AI, too: it feels it has the must-have features (including observability with Logfire) without as many of the technicalities as Langchain/Langgraph.",
          "score": 1,
          "created_utc": "2026-02-22 14:57:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t29rp",
          "author": "Accomplished-Score28",
          "text": "I have a project that I am building out with langchain. Has 6 different agents. User can dictate the model being used. The project is a POC and being submitted to try and win a grant to build it out more",
          "score": 1,
          "created_utc": "2026-02-22 17:38:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fqaxw",
          "author": "Interesting_Ride2443",
          "text": "LangChain is great for prototyping and short workflows, but once you need durable state, retries, or long-running multi-step execution, you run into limits. Tools like Calljmp provide built-in execution management, pause/resume, and observability, which makes scaling autonomous agents more practical without reinventing all that infrastructure.",
          "score": 1,
          "created_utc": "2026-02-20 15:29:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ft59f",
              "author": "Odd-Aside456",
              "text": "Thank you!",
              "score": 2,
              "created_utc": "2026-02-20 15:42:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6fth36",
                  "author": "sandman_br",
                  "text": "It‚Äôs a ad dude. Don‚Äôt fall for it",
                  "score": 6,
                  "created_utc": "2026-02-20 15:44:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gn80d",
          "author": "BeerBatteredHemroids",
          "text": "If you have to ask this, maybe you should just focus on learning the framework first",
          "score": -2,
          "created_utc": "2026-02-20 18:00:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbd4x5",
      "title": "LangGraph-based production-style RAG (Parent-Child retrieval, idempotent ingestion) ‚Äî feedback on recursive loops?",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/91vqmkq1czkg1",
      "author": "Lazy-Kangaroo-573",
      "created_utc": "2026-02-22 05:11:47",
      "score": 92,
      "num_comments": 38,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1rbd4x5/langgraphbased_productionstyle_rag_parentchild/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6r89j6",
          "author": "RubenC35",
          "text": "Great. Out of scope, how did you make that diagram?",
          "score": 17,
          "created_utc": "2026-02-22 11:29:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sc1ir",
              "author": "Oddly_Even_Pi",
              "text": "Also would love to know",
              "score": 3,
              "created_utc": "2026-02-22 15:40:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ses7z",
                  "author": "ar_tyom2000",
                  "text": "u/RubenC35, u/Oddly_Even_Pi, the graph in the picture seems like a Gemini-generated graph based on a description. But if you're interested in real-time graph animation, here is one I know of [https://github.com/proactive-agent/langgraphics](https://github.com/proactive-agent/langgraphics)",
                  "score": 7,
                  "created_utc": "2026-02-22 15:53:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6qreaw",
          "author": "SithLordRising",
          "text": "Looks really nice. I just use cytoscape.min.js for most of my flows",
          "score": 2,
          "created_utc": "2026-02-22 08:48:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ra9h4",
              "author": "Lazy-Kangaroo-573",
              "text": "Glad you liked it.",
              "score": 2,
              "created_utc": "2026-02-22 11:48:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rckd6",
          "author": "International-Mood83",
          "text": "Also curious as to how you made this animation. its pretty cool!",
          "score": 2,
          "created_utc": "2026-02-22 12:07:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6re9zg",
              "author": "Lazy-Kangaroo-573",
              "text": "Built it as an SVG with CSS animations + animateMotion paths. Happy to share the approach if useful for anyone documenting RAG architectures.",
              "score": 8,
              "created_utc": "2026-02-22 12:21:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rgmge",
                  "author": "jay-phi",
                  "text": "Would definitely be interested in your approach!",
                  "score": 3,
                  "created_utc": "2026-02-22 12:40:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rv3oy",
                  "author": "croninsiglos",
                  "text": "Definitely interested. It‚Äôs both visually appealing and could probably be easily integrated into documentation pages.",
                  "score": 3,
                  "created_utc": "2026-02-22 14:13:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6tsrg5",
                  "author": "Nzkx",
                  "text": "The fun fact is people are more interested to the graphics itself than the AI lmao.\n\nGood job anyway. Really like the graphs presentation.",
                  "score": 3,
                  "created_utc": "2026-02-22 19:42:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rm8eu",
          "author": "peregrinefalco9",
          "text": "The parent-child split with Qdrant for children and Postgres for parents is solid. For context growth in recursive retrieval, have you tried summarizing intermediate results before feeding them back into the loop? Keeps the window manageable without losing signal.",
          "score": 2,
          "created_utc": "2026-02-22 13:19:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6scazv",
          "author": "Oddly_Even_Pi",
          "text": "Would you be open to sharing the project? Would love to take a look at it",
          "score": 2,
          "created_utc": "2026-02-22 15:41:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sg5zt",
              "author": "ar_tyom2000",
              "text": "As I can understand from [OP's reply](https://www.reddit.com/r/LangChain/comments/1rbd4x5/comment/o6re9zg/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) above, it is a custom-made SVG. If you're interested in autogenerating animated flows of your existing graphs, take a look at the [LangGraphics](https://github.com/proactive-agent/langgraphics) project.\n\nhttps://i.redd.it/8smvzdjfj2lg1.gif",
              "score": 0,
              "created_utc": "2026-02-22 15:58:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6th2dp",
                  "author": "meridian_dan123",
                  "text": "That LangGraphics project looks interesting! Autogenerating animated flows could really enhance the visual aspect of complex graph structures. Have you had any experience using it with similar setups?",
                  "score": 2,
                  "created_utc": "2026-02-22 18:46:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6sv3g7",
          "author": "Key-Place-273",
          "text": "Hmm‚Ä¶.dont hate me but like‚Ä¶why? What‚Äôs the use case? Genuinely asking. I have a prod agentic app with 400+ enterprise users now and we‚Äôve basically written off vector RAG unless for very specific niche use cases.\n\nThis rag pipeline looks like the ultimate like technical bleeding edge, but, in practice and testing that usually means little production outcome. So defs really interested in seeing how this would go against realistic use cases",
          "score": 2,
          "created_utc": "2026-02-22 17:05:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sypap",
              "author": "Lazy-Kangaroo-573",
              "text": "https://preview.redd.it/c5fquai9y2lg1.jpeg?width=3196&format=pjpg&auto=webp&s=474149d965b0f46f22ef74e67c71f6984c440b22\n\nValid point! I understand the skepticism around RAG at scale. However, the reason I'm sticking with this architecture instead of just a Long-Context Window is the Precision-Risk in Legal Data. ‚Äã\\*Attached are some shots from my Supabase backend. I'm processing heavy compliance docs. One of the files alone has over 3,500 child chunks mapped to hundreds of parents. ‚ÄãFeeding this entire context into a single prompt creates too much noise and 'Lost in the Middle' issues. For 400+ general enterprise users, RAG might be overkill, but for a Legal AI where a single 'mis-retrieved' clause can invalidate an entire reasoning chain, I need this level of granular control that LangGraph provides. ‚ÄãI'd love to hear more about why you wrote off RAG‚Äîwas it the latency, the cost, or just the retrieval hallucinations?",
              "score": 3,
              "created_utc": "2026-02-22 17:21:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6tb5iz",
                  "author": "Key-Place-273",
                  "text": "So like ‚Äòvector embedding‚Äô is the issue for us within RAG realm. Legal docs might be different so for sure good to know your work! For us we use procedural guidance data for enterprise apps like NetSuite. So right now the best we managed with semantic search, BM25, reranking, hierarchy indexing etc, is around 85% match..at best‚Ä¶ and we‚Äôre still at like 90% reliability for the chunks being in the correct order.  \n\nI read this paper from Chroma that they showed the order of chunk retrieval does NOT actually have to be accurate. If anything they showed the LM had better contextual understanding when it wasn‚Äôt in logical order BUT, for us it‚Äôs very much like ‚Äúyou must do Step A and step A then branches into 5 different scenarios each with 10 steps each with 10 sub scenarios. \n\nSo we haven‚Äôt found a semantic space where we can reliably embed out knowledge into, and match it with the same semantic space that queries go to. One is a ‚Äúto do a take you must do x and then y‚Äù the other is ‚Äúdo me x‚Äù, it‚Äôs the Agent‚Äôs job to asses given the context, X really mean scenario 6A, schema C (for example) out of 100 scenarios and 1000s of schemas. \n\nI will say though. We‚Äôre a tiny ass team. The fact that I‚Äôve not managed to do it, doesn‚Äôt mean it‚Äôs 100% not meant to be for us. However, now, 2 years into doing this at Enterprise level and two agentic firms in‚Ä¶I tend to give my own experience and insight a little more weight than industry and research now (even though kinda cocky‚Ä¶ it legit I come up with ideas daily that the industry comes up with names for in like 6 months lol) \n\nSo idk, like I have experiment after experiment where my own creation is stilll tops (I made a specialized knowledge graph that isn‚Äôt vector basically), but could be that I‚Äôm way too engrossed in my own niche now!\n\nBest of luck to you I‚Äôm gonna follow your profile :)",
                  "score": 2,
                  "created_utc": "2026-02-22 18:19:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6tc2p0",
              "author": "ichig0_kurosaki",
              "text": "How would you do it without RAG?",
              "score": 1,
              "created_utc": "2026-02-22 18:23:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6teitv",
                  "author": "Key-Place-273",
                  "text": "Really meticulous context engineering. We‚Äôre patenting so I can‚Äôt share much, but it‚Äôs a none - vector knowledge graph at its core",
                  "score": 1,
                  "created_utc": "2026-02-22 18:34:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6vnteo",
          "author": "Lazy-Kangaroo-573",
          "text": "Seeing a lot of comments hyper-focusing on the diagram aesthetics and debating whether it's generated by Gemini or Claude. üòÇ \nTaking that as a massive compliment!\nHowever, the real AI magic is under the hood. The core focus of this post isn't the pixels, but the actual LangGraph orchestration, the PII sanitization layer, and the Agentic RAG logic. üß†\nIf we can move past the graphic design debate, I‚Äôd genuinely love to hear your technical critiques on the backend architecture, the chunking strategy, or the retrieval logic. The live demo is up (link in bio) ‚Äî feel free to try and break the actual system instead of pixel-peeping the diagram! üöÄ\"",
          "score": 2,
          "created_utc": "2026-02-23 01:45:14",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o6sga6d",
          "author": "NigaTroubles",
          "text": "SVG animated ??",
          "score": 1,
          "created_utc": "2026-02-22 15:59:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sis4u",
              "author": "Lazy-Kangaroo-573",
              "text": "Exactly! It‚Äôs just pure XML. I used <rect> for the systemnodes and <path> for the connections.. The   traveling data packets is just <circle> tags paired with <animateMotion> along those paths, plus some CSS @keyframesfor the glowing effecls.",
              "score": 4,
              "created_utc": "2026-02-22 16:09:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o706epq",
          "author": "Holiday-Case-4524",
          "text": "Here you can find an example of how the agent‚Äôs context is managed without letting it become excessively large, by compressing it and retaining only the information necessary for retrieval. It also shows how to avoid retrieving the same parent entities multiple times, as well as how to set a maximum number of tool calls and iteration calls, with a fallback to response generation when those thresholds are exceeded.\n\n\nhttps://github.com/GiovanniPasq/agentic-rag-for-dummies",
          "score": 1,
          "created_utc": "2026-02-23 19:26:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73ye0p",
          "author": "Friendly-Ask6895",
          "text": "really clean architecture. the circuit breaker + recursive retrieval combo is smart, most people don't think about failure modes in their graph until they're in production and things are hanging.\n\n  \non context compression during recursive loops - have you looked at using a summarization step between retrieval passes? basically after each loop iteration, compress the accumulated context before feeding it back into the next retrieval decision. adds latency per loop but keeps your token budget from exploding. you can set a hard ceiling on context size and let the compression step decide what to keep vs drop.\n\n  \nfor the duplicate embeddings on re-index, content-addressable hashing before the embedding call is probably the cheapest solution. hash the chunk content, check if the hash exists in your index, skip if it does. saves you the embedding API cost and makes your idempotency guarantee actually work at the embedding level too.\n\n  \ncurious what your React SPA is doing during the recursive loops though - are you streaming intermediate states back to the frontend or just waiting for the full response? that's usually where the UX breaks down with these kinds of architectures.",
          "score": 1,
          "created_utc": "2026-02-24 09:40:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o744w0c",
              "author": "Lazy-Kangaroo-573",
              "text": "Thanks ‚Äî great points and really appreciated.\n\n1. Context Compression & Loops:\nI am not explicitly summarizing between loops, nor am I using an LLM-as-a-judge. Instead, my LangGraph orchestration runs a strict state-machine (CLASSIFY ‚Üí RETRIEVE ‚Üí GENERATE ‚Üí POST_PROCESS). To prevent context bloat and token exhaustion, the absolute hard-limit is set at the database level: the MongoDB history injection strictly fetches only the last 6 conversation turns. (get_chat_history default = 6), and use parent-child chunking so retrieval returns compact parent_texts rather than dumping whole docs.\n\n2. Duplicate Embeddings & Idempotency:\nFor the Core Knowledge Base, I built a Sync Engine using a Postgres registry (Supabase). It computes the SHA-256 hash of the file. If the hash hasn't changed, the entire extraction and embedding pipeline is bypassed.\nFor Temporary Uploads, vectors are tagged with metadata (is_temporary=True and uploaded_by). To prevent duplicates, the backend performs a strict delete-before-insert for that filename+user before upserting. When the user logs out, the frontend triggers a backend cleanup that purges only these temporary vectors, keeping the core brain untouched.\n\n3. React UX & Streaming:\nYes, the system uses SSE (Server-Sent Events). During the LangGraph retrieval phase, the frontend just displays a 'Thinking' loading state. Once the final generation node starts, the FastAPI backend streams the response token-by-token to the React SPA.\nThe V2 roadmap definitely includes your idea of deterministic point IDs (using sha256(file_hash + chunk_index)) to handle race conditions during the temp-file upserts. Appreciate the review!\"",
              "score": 1,
              "created_utc": "2026-02-24 10:40:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6q4gzr",
          "author": "Lazy-Kangaroo-573",
          "text": "For context ‚Äî LangGraph is mainly handling the cyclic retrieval loop and retry control.\n\nEach node:\n- Classifies intent\n- Applies PII masking\n- Executes parent-child retrieval\n- Applies circuit breaker logic before LLM call\n\nOne thing I'm still evaluating:\nShould recursive retrieval stop based on confidence threshold, token growth, or graph depth?\n\nCurious how others are handling termination criteria in LangGraph loops.",
          "score": 0,
          "created_utc": "2026-02-22 05:20:39",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbyd8x",
      "title": "Why flat Vector DBs aren't enough for true LLM memory (and why I'm building a database around \"Gaussian Splats\" instead)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rbyd8x/why_flat_vector_dbs_arent_enough_for_true_llm/",
      "author": "TallAdeptness6550",
      "created_utc": "2026-02-22 21:33:26",
      "score": 45,
      "num_comments": 23,
      "upvote_ratio": 0.88,
      "text": "Hey everyone,\n\nLately, I've been thinking about the limitations of standard RAG setups. Right now, we treat LLM memory as a flat bag of vectors (whether via Pinecone, Milvus, or FAISS). You embed a chunk of text, throw it in a database, and do a cosine similarity search.\n\nFlat vectors lack¬†*shape, density, and hierarchical context*.\n\nI‚Äôve been experimenting with storing memory chunks as¬†**Gaussian Splats**¬†(nodes with a mean¬†`¬µ`, precision¬†`Œ±`, and concentration¬†`Œ∫`) mapped to a high-dimensional S\\^639 hypersphere.\n\nBy giving embeddings a \"shape\" rather than just a point, the implications for LLM databases are massive:\n\nüß†¬†**1. Dynamic Forgetting & Consolidation (Self-Organized Criticality)**¬†Instead of deleting old embeddings or keeping everything forever, Splats can naturally decay or merge. If an LLM encounters the same concept multiple times, the \"splat\" increases in concentration (`Œ∫`). If a concept is trivial and never accessed, it degrades. The database curates itself like biological memory.\n\nüîç¬†**2. Hierarchical \"Zoom\" for Context (HRM2)**¬†When querying a flat vector DB, you just get the Top-K closest chunks. With splats, you can query at different resolutions. Need a broad summary of a topic? Retrieve the massive, low-density \"parent\" splat. Need a specific quote? Zoom into the high-density \"child\" splat. It turns O(N) search into O(log N).\n\nüíæ¬†**3. 3-Tier Biological Memory Routing**¬†Because splats have metadata about their importance/density, the DB can automatically route them:\n\n* **VRAM (Hot):**¬†Highly active, dense splats ready for instant LLM attention.\n* **RAM (Warm):**¬†Broad conceptual splats.\n* **SSD (Cold):**¬†Low-density, rarely accessed memory.\n\n**Current Status:**¬†I‚Äôve actually managed to get a functional implementation of this working on CPU. By using a Hierarchical Retrieval Engine (HRM2) and Mini-Batch K-Means, I‚Äôm currently benchmarking a¬†**96x speedup**¬†against linear search on 100K splats (`0.99ms`¬†vs¬†`94.7ms`), proving the O(log N) math works.\n\nI‚Äôm currently heavily refactoring the codebase and building Vulkan GPU acceleration before I officially push the full V1.0 to GitHub. Now here \"https://github.com/schwabauerbriantomas-gif/m2m-vector-search\"\n\nHas anyone else experimented with non-flat, hierarchical, or density-based memory structures for their local LLMs? I‚Äôd love to hear your thoughts on where this architecture might face bottlenecks before I finalize the release.\n\nhttps://preview.redd.it/0yzr6ttu64lg1.jpg?width=640&format=pjpg&auto=webp&s=c9602b890ad39acb2101b6c6b10ee07df9aca39a\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1rbyd8x/why_flat_vector_dbs_arent_enough_for_true_llm/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o6v04sw",
          "author": "Don_Ozwald",
          "text": "Conceptually interesting, but have you actually measured it? Beating linear time is not really a meaningful benchmark here.",
          "score": 6,
          "created_utc": "2026-02-22 23:27:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75n5oi",
              "author": "TallAdeptness6550",
              "text": "I'm testing with different datasets; the results are on GitHub. But I still need help, and the results are mixed. It currently has some bugs but is functional. I'd like to be able to test with CUDA, but my hardware doesn't allow it. I welcome any criticism. The initial results were with a random synthetic dataset.",
              "score": 1,
              "created_utc": "2026-02-24 16:03:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6v1oe4",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 5,
          "created_utc": "2026-02-22 23:36:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6w339i",
              "author": "TallAdeptness6550",
              "text": "To validate the theoretical architecture against production unstructured data, M2M was stress-tested using¬†`VectorDBBench`¬†configurations executing 10,000 queries against high-dimensional (636D) real semantic embeddings from Wikipedia.\n\n**Hardware & Execution Environment**\n\n* **CPU Matrix**: AMD Ryzen 5 3400G (4 Cores, 8 Threads)\n* **GPU Compute**: AMD Radeon RX 6650 XT (8GB VRAM)\n* **Memory**: 16 GB DDR4 RAM\n* **Test Scale**: 10,000 Vectors (Dense, unstructured)\n\n**Methodology & Execution Paradigms**¬†We compared three native retrieval paths to measure raw algorithmic throughput (k=10¬†neighbors):\n\n1. **Exact CPU (Brute Force)**: Exhaustive linear search via optimized CPU¬†`numpy`¬†routines serving as the baseline ground truth.\n2. **Exact GPU (Vulkan KNN)**: Explicit GPU-accelerated parallel brute force utilizing custom SPIR-V compute shaders interacting directly with VRAM memory buffers.\n3. **CPU Index (HRM2 Fallback)**: Two-tier hierarchical clustering index utilizing¬†`MiniBatchKMeans`. Configured intelligently (nprobe=15) to capitalize on the natural semantic topology of the vector space, hitting exactly¬†>99.90¬†recall without penalizing throughput.\n\n**Measured Performance Metrics (QPS & Recall Quality):**\n\n|Paradigm|Architecture Focus|Search QPS|Latency (Avg)|Recall Quality|\n|:-|:-|:-|:-|:-|\n|**GPU Exact**|`PyVulkan`¬†Shaders|**168**|**5.94ms**|**100.00%**¬†(Absolute)|\n|**CPU Exact**|`numpy`¬†Linear Scan|42|23.88ms|**100.00%**¬†(Baseline)|\n|**CPU Index**|`HRM2`¬†Fallback|41|24.12ms|**99.90%**¬†(Tuned Index)|",
              "score": 0,
              "created_utc": "2026-02-23 03:18:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6umnq9",
          "author": "georgeApuiu",
          "text": "![gif](giphy|5xfcseoKgpAcg)",
          "score": 3,
          "created_utc": "2026-02-22 22:13:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ung12",
              "author": "TallAdeptness6550",
              "text": "Same",
              "score": 1,
              "created_utc": "2026-02-22 22:17:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wflqo",
          "author": "cmddata",
          "text": "Why is human memory the benchmark? AGI and ASI will not have the drawbacks of human cognition.",
          "score": 2,
          "created_utc": "2026-02-23 04:44:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wg06e",
              "author": "TallAdeptness6550",
              "text": "Well i'm a solo developer with a ai assistant named Alfred not AGI",
              "score": 1,
              "created_utc": "2026-02-23 04:47:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6whsr3",
                  "author": "cmddata",
                  "text": "I ask because you mention AGI in your post and compare it to human memory.",
                  "score": 1,
                  "created_utc": "2026-02-23 05:00:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6yf070",
          "author": "peregrinefalco9",
          "text": "Flat vector search loses temporal and relational context which is exactly what makes human memory useful. The retrieval problem isn't finding similar content, it's finding relevant content given what the agent is trying to do right now. Graph-based memory is the right direction.",
          "score": 2,
          "created_utc": "2026-02-23 14:26:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yjhkm",
              "author": "TallAdeptness6550",
              "text": "Flat vector databases (Pinecone, Milvus, FAISS) treat all embeddings as static points. While this is efficient for search, it creates a fundamental limitation: they lose temporal and relational context, which is critical for AI agents and complex reasoning tasks.\n\nThe \"Gaussian Splats + SOC\" architecture is biologically inspired and technically superior to flat vector databases for agents requiring persistent, long-term memory with dynamic consolidation. It transforms \"storing vectors\" into \"simulating a living brain\". But it's just code and can be modified. I was testing LLM In DB with this metodology",
              "score": 1,
              "created_utc": "2026-02-23 14:49:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6yjsba",
                  "author": "TallAdeptness6550",
                  "text": "Hard to do with AMD ",
                  "score": 1,
                  "created_utc": "2026-02-23 14:51:27",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ujh8o",
          "author": "4b3c",
          "text": "im mind blown, that sounds sick",
          "score": 1,
          "created_utc": "2026-02-22 21:57:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6uktu8",
              "author": "TallAdeptness6550",
              "text": "Thanks man! Really appreciate it. It‚Äôs been a wild ride trying to wrap my head around projecting semantic data onto a hyper-sphere instead of a flat plane, but seeing the O(log N) speedup actually working on CPU made it worth it.\n\nLet me know if you want to poke around the repo when you have time, always looking for feedback on where the math might break at larger scales!",
              "score": 2,
              "created_utc": "2026-02-22 22:04:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6ul4pj",
              "author": "TallAdeptness6550",
              "text": "feel free to take a look at the repo \"https://github.com/schwabauerbriantomas-gif/m2m-vector-search\"",
              "score": 2,
              "created_utc": "2026-02-22 22:05:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6uutvc",
          "author": "yoyo4581",
          "text": "This is amazing OP.\nI'm really interested if you've dabled with the idea of creating structured databases from this hierarchical splats as you referred to them. \n\nAs you said you can refer to parent splats for summaries of child splats, but what about categorizing data in the parents, and forming something akin to an ontology from derived data.",
          "score": 1,
          "created_utc": "2026-02-22 22:57:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v284n",
          "author": "Affectionate-Leg8133",
          "text": "Would assist with experimenting, do you have a repo?",
          "score": 1,
          "created_utc": "2026-02-22 23:39:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6v2uum",
              "author": "TallAdeptness6550",
              "text": "this is the repo [https://github.com/schwabauerbriantomas-gif/m2m-vector-search](https://github.com/schwabauerbriantomas-gif/m2m-vector-search) I'm sorry, but I rely heavily on artificial intelligence for coding. Feel free to give my feed back ",
              "score": 1,
              "created_utc": "2026-02-22 23:42:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xjnys",
          "author": "HoldZealousideal1966",
          "text": "What database are you using to store the splats?",
          "score": 1,
          "created_utc": "2026-02-23 10:46:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6y2v4c",
              "author": "TallAdeptness6550",
              "text": "I was trying to make a new db just for splats with tiling and MoE. Implementing a Mixture of Experts (MoE) and tiling system for massive Gaussian Splatting datasets is an effective way to scale rendering while maintaining high fidelity. For \"massive\" databases, a traditional JSON approach becomes a bottleneck due to memory overhead and slow parsing.",
              "score": 1,
              "created_utc": "2026-02-23 13:16:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6y6mtu",
                  "author": "TallAdeptness6550",
                  "text": "the fact that you have c1 c2 c3 memory makes speed in the database c3 less relevant in my tests",
                  "score": 1,
                  "created_utc": "2026-02-23 13:38:51",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r8x2mi",
      "title": "Alternative to LangChain memory for agents ‚Äî zero deps, file-based, 1ms search, no API key needed",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r8x2mi/alternative_to_langchain_memory_for_agents_zero/",
      "author": "fourbeersthepirates",
      "created_utc": "2026-02-19 11:43:44",
      "score": 19,
      "num_comments": 2,
      "upvote_ratio": 0.83,
      "text": "I like LangChain for orchestration but always found the memory options limiting ‚Äî ConversationBufferMemory doesn't do real retrieval (just returns recent items), and VectorStoreRetrieverMemory needs an embedding API key and a vector store.\n\nI built antaris-memory as an alternative that sits in the middle: real relevance-ranked retrieval (BM25, not just recency), but with zero external dependencies. No OpenAI key, no Pinecone, no Chroma. Pure Python, file-based, portable.\n\n**Quick comparison:**\n\n||antaris-memory|LangChain Buffer|LangChain VectorStore|\n|:-|:-|:-|:-|\n|Search latency|1.01ms|0.005ms|Depends on provider|\n|Finds relevant (not just recent)|‚úì|‚úó|‚úì|\n|Scales past 1K memories|‚úì (sharding)|‚úó (dumps all to LLM)|‚úì|\n|API key required|None|None|Yes (embeddings)|\n|Persistent storage|‚úì (file-based)|‚úó (in-memory)|Depends on store|\n|WAL + crash recovery|‚úì|‚úó|Depends on store|\n\nIt's part of a larger suite (guard, router, context, pipeline) but antaris-memory works standalone:\n\npython\n\n    pip install antaris-memory\n    \n    from antaris_memory import MemorySystem\n    memory = MemorySystem(workspace=\"./my_agent_memory\")\n    memory.ingest(\"User prefers dark mode and uses Python 3.12\")\n    results = memory.search(\"what does the user prefer?\")\n\n293 tests on antaris-memory,  1,183 tests on the whole suite (0 failures), Apache 2.0. Also ships as an MCP server and an OpenClaw plugin.\n\nAll the modules work together and compliment each other though, and pipeline ties them all together. Take a look at the Git if you want to see the insides.  \n\n\nGitHub: [https://github.com/Antaris-Analytics/antaris-suite](https://github.com/Antaris-Analytics/antaris-suite)\n\nDocs: [https://docs.antarisanalytics.ai](https://docs.antarisanalytics.ai)\n\nSite: [https://antarisanalytics.ai](https://antarisanalytics.ai)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r8x2mi/alternative_to_langchain_memory_for_agents_zero/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o6a73rr",
          "author": "Delicious-One-5129",
          "text": "actually pretty cool. Zero deps and no API key is a big win, especially for local agents.\n\nBM25 for memory feels underrated too. Nice middle ground between dumb buffer and full vector stack. Gonna check the repo",
          "score": 3,
          "created_utc": "2026-02-19 18:21:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6auiky",
              "author": "fourbeersthepirates",
              "text": "Cool let me know what you think! Have a ton of features to add this week on the way to 3.0. Shared agent memory, sub-agent context and semantic search is next in line.",
              "score": 1,
              "created_utc": "2026-02-19 20:12:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rau962",
      "title": "How are you actually evaluating your LangChain agents in production, not just in the notebook?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rau962/how_are_you_actually_evaluating_your_langchain/",
      "author": "Afzaalch00",
      "created_utc": "2026-02-21 15:42:16",
      "score": 18,
      "num_comments": 11,
      "upvote_ratio": 0.96,
      "text": "I have been building a LangChain-based customer support agent for the past few months and kept running into the same issue. Everything looked fine locally, but once it hit production I had no real way to know if quality was holding up or slowly degrading. I was basically eyeballing outputs and hoping for the best.\n\nI started with DeepEval for offline evals since it integrates cleanly with LangChain and the pytest-style setup felt familiar. It was genuinely useful for pre-deployment checks: testing faithfulness, answer relevancy, and hallucination on a fixed dataset before each release. Highly recommend it as a starting point if you haven't tried it.\n\nThe gap I kept hitting though was that my offline dataset didn't reflect what real users were actually sending. I'd pass all my tests and still get weird failures in prod that I never anticipated. That's when I moved to Confident AI, which is built by the same team behind DeepEval. The big difference is it runs those same evals continuously on production traces instead of just a static dataset. When a metric like faithfulness or relevance drops, you get alerted before users complain. The other thing I didn't expect to find useful was the automatic dataset curation from real traces. Bad production outputs get turned into test cases, so over time your eval dataset actually reflects your real traffic instead of synthetic examples you wrote on day one.\n\nThe combo that works for us now is DeepEval for pre-deployment regression testing in CI and Confident AI for live quality monitoring in prod. Took a while to get here but the iteration loop is way tighter now.\n\nAnyone else using a similar setup or found a different approach for keeping LangChain agent quality stable over time?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1rau962/how_are_you_actually_evaluating_your_langchain/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o6m728y",
          "author": "cool_girrl",
          "text": "Solid setup. We do something similar but use LangSmith for tracing and run DeepEval checks separately. The annoying part is keeping the two in sync.",
          "score": 3,
          "created_utc": "2026-02-21 15:48:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6m7pfm",
          "author": "Otherwise_Wave9374",
          "text": "This is exactly the pain point, notebook success means nothing once real traffic hits. The DeepEval + continuous monitoring split makes a lot of sense, offline for regression and then production traces to catch drift.\n\nOne thing thats helped us is defining a small set of \"agent contract\" checks: tool call validity, JSON schema compliance, refusal behavior, and grounding/citation rules. Even if answer quality is subjective, those checks catch a surprising amount of breakage.\n\nIf youre collecting patterns around agent evals, a few notes and links here: https://www.agentixlabs.com/blog/",
          "score": 2,
          "created_utc": "2026-02-21 15:51:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mq1dy",
          "author": "Happy-Fruit-8628",
          "text": "The automatic dataset curation part is underrated. We were manually curating failure cases into test sets which took forever. Anything that makes that automatic is a big deal when you are trying to ship fast.",
          "score": 1,
          "created_utc": "2026-02-21 17:23:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6muivj",
          "author": "Abdullah_3254",
          "text": "+1 on this. DeepEval offline plus Confident AI on live traffic is honestly the combo I wish I had set up from day one.",
          "score": 1,
          "created_utc": "2026-02-21 17:46:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6n5aqc",
          "author": "SpareIntroduction721",
          "text": "I used langfuse since it‚Äôs very similar to langsmith and private",
          "score": 1,
          "created_utc": "2026-02-21 18:39:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pd8wa",
          "author": "Clear-Dimension-6890",
          "text": "Pet peeve about deepeval : it overtakes your system , makes .deepeval directories everyhere",
          "score": 1,
          "created_utc": "2026-02-22 02:09:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pdj9c",
          "author": "Clear-Dimension-6890",
          "text": "If you‚Äôre already in the LangChain ecosystem, LangSmith covers a lot of this ‚Äî tracing, annotation, dataset curation from prod ‚Äî and it‚Äôs a more natural fit. Alternatively, a lightweight custom loop (sample prod traces, run LLM-as-judge scoring, funnel failures back into tests) gets you most of the way without adding a vendor dependency.",
          "score": 1,
          "created_utc": "2026-02-22 02:11:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pvyg4",
          "author": "Disastrous-Royal-829",
          "text": "This matches what we've been seeing researching the agent observability space. The gap between \"passes offline evals\" and \"works in prod\" is where quality silently degrades.\n\nOne pattern that keeps coming up: tracing alone isn't enough. If you can't tie a quality drop back to the exact step in your chain ‚Äî retrieval, prompt, model ‚Äî you're just collecting expensive logs.\n\nCurious ‚Äî are you tracking quality per-chain-step or just at the final output level? Step-level observability seems to catch issues way earlier from what we've seen.",
          "score": 1,
          "created_utc": "2026-02-22 04:16:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6q2m9g",
          "author": "Used-Middle1640",
          "text": "I am actually running almost the exact same stack with LangChain and it is honestly a lifesaver. We were struggling with those random production failures too until we plugged in Confident AI to monitor the live traces. The best part for me was definitely the regression testing because now we actually know if a prompt change is going to mess up the retrieval quality before it even hits the main branch. It is way better than just crossing your fingers and hoping the agent behaves after a deployment.",
          "score": 1,
          "created_utc": "2026-02-22 05:05:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qka7c",
          "author": "Far-Run-3778",
          "text": "Following! Hoping to learn something useful",
          "score": 1,
          "created_utc": "2026-02-22 07:40:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mr65s",
          "author": "darkluna_94",
          "text": "The CI + prod monitoring split is key. We started with just offline evals and kept getting blindsided. Now we use a similar setup evals in CI for regressions prod traces feeding back into the test set.",
          "score": 0,
          "created_utc": "2026-02-21 17:29:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdg2f9",
      "title": "Is Adding a Reranker to My RAG Stack Actually Worth the Extra Latency? (Explained Simply)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rdg2f9/is_adding_a_reranker_to_my_rag_stack_actually/",
      "author": "Silent_Employment966",
      "created_utc": "2026-02-24 13:20:49",
      "score": 15,
      "num_comments": 7,
      "upvote_ratio": 0.8,
      "text": "This comes up constantly and I want to give an honest answer because the reaction (\"rerankers add latency, avoid them\") is wrong but not for the reason most people think.\n\nWe had a good discussion in our office about the same & therefore we dig it deeper & will try to reply to it in a simpler manner. \n\nA typical RAG pipeline looks like this:\n\n    User query\n      ‚Üí Embed query\n      ‚Üí Vector search ‚Üí top 50 chunks\n      ‚Üí Stuff all 50 chunks into LLM prompt\n      ‚Üí Generate answer\n\nThe instinct is: adding a reranker inserts *another* step, so latency goes up. That's true in isolation. But it completely ignores what happens downstream.\n\n**Where the Latency Actually Lives**\n\nLet's be concrete. Here's where time actually gets spent in a RAG call:\n\n|Step|Typical latency|\n|:-|:-|\n|Vector search (top 50)|50‚Äì150ms|\n|Reranker (re-score top 50)|80‚Äì200ms|\n|LLM generation (50 chunks, \\~15k tokens)|4,000‚Äì8,000ms|\n|**Total without reranker**|\\~4,500‚Äì8,500ms|\n|LLM generation (top 5 chunks, \\~1.5k tokens)|600‚Äì1,200ms|\n|**Total with reranker**|\\~1,200‚Äì1,800ms|\n\nThe reranker adds \\~100‚Äì200ms. But it lets you cut your LLM context from 50 chunks to 5. LLM generation time scales roughly linearly with context length ‚Äî so you're trading 200ms of reranker time for 3,000‚Äì7,000ms of LLM savings.\n\n**Net result: total pipeline latency goes** ***down*****, not up.**\n\n**But That's Not the Only Benefit**\n\nEven if latency was neutral, the accuracy argument alone justifies reranking:\n\n**The core problem:** Vector search ranks by embedding similarity, not relevance. These are not the same thing. A chunk that shares vocabulary with your query will score high even if it doesn't actually answer it. Your LLM then hallucinates around bad context.\n\nA reranker does a deep query-document comparison. it reads both the query and the chunk together and scores true relevance. This is fundamentally more accurate than cosine similarity on pre-computed embeddings.\n\nReal-world result: reranking typically gives you 15‚Äì30% improvement in answer quality on standard benchmarks like NDCG@10.\n\n# What Reranker Should You Actually Use?\n\nHere are your main options, honestly compared:\n\n**Open-source / self-hosted**\n\n**BGE-reranker-v2-m3** (BAAI)\n\n* Strong general performance, multilingual\n* Apache 2.0 license, free to self-host\n* Good starting point if you want full control\n* \\~200‚Äì400ms on CPU, \\~50‚Äì100ms on GPU\n\n**ms-marco-MiniLM-L-6-v2** (cross-encoder)\n\n* Lightweight, fast, good for English\n* Great for prototyping\n* Weaker on domain-specific or non-English content\n\n**Managed APIs**\n\n**ZeroEntropy zerank-2**\n\n* Instruction-following (you can pass business context to influence scoring)\n* Calibrated scores (0.8 actually means \\~80% relevance, consistently)\n* Strong multilingual performance across 100+ languages\n* $0.025/1M tokens (\\~50% cheaper than Cohere)\n* Models are open-weight on HuggingFace if you want to self-host\n* Worth evaluating if you're hitting Cohere's limitations or need multilingual support\n\n**Cohere Rerank 3.5**\n\n* Industry standard, solid accuracy\n* \\~$1/1000 queries, \\~100‚Äì150ms latency\n* No instruction-following, scores aren't calibrated (0.7 means different things in different contexts)\n\n**When a Reranker Genuinely Doesn't Help**\n\nTo be fair, there are cases where adding a reranker won't move the needle:\n\n* **Your first-stage retrieval recall is the problem.** If the right chunk isn't in your top 50 at all, no reranker can fix that. \n* **Your chunks are already very short and precise.** If you're chunking at 100 tokens and have a small corpus, the reranker has less room to help.\n* **Your queries are extremely simple and unambiguous.** Basic keyword lookups where BM25 works perfectly don't need reranking.\n\n# Practical Implementation (LangChain)\n\n`from langchain.retrievers import ContextualCompressionRetriever`\n\n`from langchain.retrievers.document_compressors import CrossEncoderReranker`\n\n`from langchain_community.cross_encoders import HuggingFaceCrossEncoder`\n\n\n\n`# Using BGE open-source reranker`\n\n`model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")`\n\n`compressor = CrossEncoderReranker(model=model, top_n=5)`\n\n\n\n`compression_retriever = ContextualCompressionRetriever(`\n\n`base_compressor=compressor,`\n\n`base_retriever=your_vector_retriever  # your existing retriever`\n\n`)`\n\n\n\n`# Now returns top 5 reranked results instead of top 50 raw chunks`\n\n`docs = compression_retriever.invoke(\"your query here\")`\n\nFor a managed API option (ZeroEntropy, Cohere, etc.) the pattern is similar. swap the compressor for an API-based one.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1rdg2f9/is_adding_a_reranker_to_my_rag_stack_actually/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o7539kn",
          "author": "jannemansonh",
          "text": "the manual rag stack config is real... spent way too long optimizing rerankers and chunk sizes for doc workflows. ended up moving those to needle app since hybrid search/reranking is built in... way easier than wiring it yourself",
          "score": 1,
          "created_utc": "2026-02-24 14:28:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74vsg5",
          "author": "Deep_Structure2023",
          "text": "Reranker may add little latency but pulls up accurate information, good explanation btw",
          "score": 1,
          "created_utc": "2026-02-24 13:49:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74we39",
              "author": "Silent_Employment966",
              "text": "glad you find it helpful",
              "score": 0,
              "created_utc": "2026-02-24 13:52:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74wqzw",
                  "author": "Deep_Structure2023",
                  "text": "One more thing I forgot to mention, it reduces hallucinations by a great deal",
                  "score": 1,
                  "created_utc": "2026-02-24 13:54:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74scj2",
          "author": "Raseaae",
          "text": "If the first-stage retrieval is the bottleneck, would you recommend switching to hybrid searchbefore even touching a reranker?",
          "score": 1,
          "created_utc": "2026-02-24 13:30:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74wzg6",
              "author": "InteractionSmall6778",
              "text": "Hybrid search first, always. Reranking bad retrieval results still gives you bad results.",
              "score": 2,
              "created_utc": "2026-02-24 13:55:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o74sx52",
              "author": "Silent_Employment966",
              "text": "Yes, always fix recall before precision a reranker can only reorder what retrieval already found. Hybrid search (BM25 + vector) is the fastest recall boost with no added infrastructure; once recall@50 is solid, *then* layer in the reranker",
              "score": 0,
              "created_utc": "2026-02-24 13:33:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9ur3x",
      "title": "Structure-first RAG with metadata enrichment (stop chunking PDFs into text blocks)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r9ur3x/structurefirst_rag_with_metadata_enrichment_stop/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-20 13:01:41",
      "score": 13,
      "num_comments": 11,
      "upvote_ratio": 0.84,
      "text": "I think most people are still chunking PDFs into flat text and hoping semantic search works. This breaks completely on structured documents like research papers.\n\nTraditional approach extracts PDFs into text strings (tables become garbled, figures disappear), then chunks into 512-token blocks with arbitrary boundaries. Ask \"What methodology did the authors use?\" and you get three disconnected paragraphs from different sections or papers.\n\nThe problem is research papers aren't random text. They're hierarchically organized (Abstract, Introduction, Methodology, Results, Discussion). Each section answers different question types. Destroying this structure makes precise retrieval impossible.\n\nI've been using structure-first extraction where documents get converted to JSON objects (sections, tables, figures) enriched with metadata like section names, content types, and semantic tags. The JSON gets flattened to natural language only for embedding while metadata stays available for filtering.\n\nThe workflow uses Kudra for extraction (OCR ‚Üí vision-based table extraction ‚Üí VLM generates summaries and semantic tags). Then LangChain agents with tools that leverage the metadata. When someone asks about datasets, the agent filters by content\\_type=\"table\" and semantic\\_tags=\"datasets\" before running vector search.\n\nThis enables multi-hop reasoning, precise citations (\"Table 2 from Methods section\" instead of \"Chunk 47\"), and intelligent routing based on query intent. For structured documents where hierarchy matters, metadata enrichment during extraction seems like the right primitive.\n\nAnyway thought I should share since most people are still doing naive chunking by default.",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1r9ur3x/structurefirst_rag_with_metadata_enrichment_stop/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o6eycn4",
          "author": "Independent-Cost-971",
          "text": "I wrote a whole blog about this that goes into the steps with code if anyone's interested:¬†[https://kudra.ai/metadata-enriched-rag-agent-why-document-structure-beats-text-chunking/](https://kudra.ai/metadata-enriched-rag-agent-why-document-structure-beats-text-chunking/)",
          "score": 2,
          "created_utc": "2026-02-20 13:02:09",
          "is_submitter": true,
          "replies": [
            {
              "id": "o6oep6n",
              "author": "patrick9331",
              "text": "I would avoid OCR at all cost, usually it‚Äôs expensive and I would only use it if you have to parse Images. Well that‚Äôs what it‚Äôs therefore I guess. Otherwise just parse it in to markdown and then split it based on the sections.\n\nMicrosoft has an awesome library for that:¬†https://github.com/microsoft/markitdown Have you tried this library? I think that covers most of the use cases and also supports OCR actually. One approach I also like. Often the PDF have stable of content at the beginning. Parse that and use it to split your PDF. And as a very last resort use OCR.",
              "score": 2,
              "created_utc": "2026-02-21 22:37:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rrjsi",
                  "author": "pillieee",
                  "text": "The link doesn‚Äôt work",
                  "score": 1,
                  "created_utc": "2026-02-22 13:52:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ezxh1",
          "author": "PopPsychological4106",
          "text": "Correct idea. Don't like throwing vlm on it to make it work though. Also table structure understanding is a science for itself.\nWhat labels do you use? How well does toc reconstruction work with vlm?",
          "score": 1,
          "created_utc": "2026-02-20 13:11:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f7lf3",
          "author": "AdRepresentative6947",
          "text": "Yeah had this realisation the other day. Really love your blog gonna implement it into a project I‚Äôve been working on.\n\nDoes the Kundra AI bit run separate as you‚Äôd only have to run the documents in that pipeline once to get the data or when new data arrives ?",
          "score": 1,
          "created_utc": "2026-02-20 13:54:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6f7xda",
              "author": "AdRepresentative6947",
              "text": "Dw I just saw in the blog, looks slick",
              "score": 1,
              "created_utc": "2026-02-20 13:56:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ffj3p",
          "author": "Happy-Fruit-8628",
          "text": "Totally agree. Structure first extraction with metadata filtering makes RAG far more precise and lets you cite exact tables and sections.",
          "score": 1,
          "created_utc": "2026-02-20 14:36:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jelfe",
          "author": "BigDry3037",
          "text": "Docling solves this problem",
          "score": 1,
          "created_utc": "2026-02-21 02:48:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6t1iag",
              "author": "ResidentTicket1273",
              "text": "What's the output format that docling generates? Is there a meta-model I can look at to see what the post-docling process generates?",
              "score": 1,
              "created_utc": "2026-02-22 17:34:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6tldeq",
                  "author": "BigDry3037",
                  "text": "It returns a docling document, and maintains relational tabular data representations in different output formats",
                  "score": 1,
                  "created_utc": "2026-02-22 19:06:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r7fsnr",
      "title": "Debugging LangChain agents is painful until you can visualize the full trace",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r7fsnr/debugging_langchain_agents_is_painful_until_you/",
      "author": "ruhila12",
      "created_utc": "2026-02-17 19:19:32",
      "score": 11,
      "num_comments": 13,
      "upvote_ratio": 0.93,
      "text": "I really like working with LangChain, but debugging multi step agents can feel like a black box.\nWhen something breaks, it‚Äôs never obvious where it actually failed.\n\n\nDid retrieval return garbage?\n\n\nDid the reranker strip out the only useful chunk?\n\n\nDid the LLM just hallucinate?\n\n\nOr did the agent get stuck in some weird tool loop?\n\n\nFor the longest time, I was just staring at terminal logs and scrolling through JSON traces trying to piece things together. It technically works‚Ä¶ but once your chain gets even slightly complex, it becomes painful.\n\nRecently, I plugged my chains into a tracing tool (Confident AI) mostly out of frustration. I wasn‚Äôt looking for metrics or anything fancy. I just wanted to see what was happening step by step.\nThe biggest difference for me wasn‚Äôt scoring or dashboards. It was the visual breakdown of each hop in the chain. I could literally see:\n\n\nRetrieval step\n\n\nReranking\n\n\nTool calls\n\n\nLLM responses\n\n\nLatency per step\n\n\nAt one point, I realized my agent wasn‚Äôt ‚Äúfailing‚Äù randomly, it was looping on a specific tool call because my system prompt wasn‚Äôt strict enough about exit conditions. That would‚Äôve taken me way longer to diagnose just from logs.\n\nBeing able to replay a failed interaction and inspect the full flow changed how I debug. It feels less like guessing and more like actual engineering.\n\nCurious how others are handling debugging for multi-step agents. Are you just logging everything, or using something more structured?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r7fsnr/debugging_langchain_agents_is_painful_until_you/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o5x9ab5",
          "author": "Overall_Insurance956",
          "text": "Use langsmith",
          "score": 4,
          "created_utc": "2026-02-17 19:59:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o623tdf",
              "author": "LuckySwimming8564",
              "text": "This.  It is super easy to setup (just a couple vars in your .env) and it is very detailed.  https://docs.langchain.com/langsmith/trace-with-langchain. ",
              "score": 1,
              "created_utc": "2026-02-18 14:28:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6a3bbh",
                  "author": "sunglasses-guy",
                  "text": "to be fair though literally every tool out there now offers 1-2 line integration with langchain and langgraph, langsmith is expensive as hell",
                  "score": 1,
                  "created_utc": "2026-02-19 18:03:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5x2c9v",
          "author": "pvatokahu",
          "text": "Check out open source monocle2ai from Linux foundation - it does the full tracing with agentic attribute capture built on OpenTelemetry and part of pytest.",
          "score": 2,
          "created_utc": "2026-02-17 19:27:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o658ixy",
          "author": "TheExodu5",
          "text": "Please don‚Äôt interact with the fake-engagement advertising bot.",
          "score": 2,
          "created_utc": "2026-02-18 23:08:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o678rvy",
              "author": "NotAHost",
              "text": "Yup just search author:username to see all their spam.",
              "score": 1,
              "created_utc": "2026-02-19 06:42:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o61sh9d",
          "author": "Informal_Tangerine51",
          "text": "Yeah, once a chain has retrieval + reranking + tools, ‚Äúprint the logs‚Äù stops being a debugging strategy and starts being archaeology. A good trace view pays for itself fast, especially when you can replay a run and see exactly where the agent diverged or started looping.\n\nOne thing I‚Äôd add (even if you keep the fancy UI) is a small ‚Äústructured trace contract‚Äù: every hop logs inputs/outputs, tool args, and a reason code for why the agent continued or stopped. Then you can write regression tests off real failures: ‚Äúthis tool loop should terminate‚Äù or ‚Äúthis retrieval query should return at least one relevant chunk,‚Äù instead of hoping prompts stay stable.\n\nWe‚Äôre working on this at Clyra (open source here): [https://github.com/Clyra-AI](https://github.com/Clyra-AI)",
          "score": 1,
          "created_utc": "2026-02-18 13:28:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o624842",
          "author": "93simoon",
          "text": "Use langfuse, it's lang Smith but foss",
          "score": 1,
          "created_utc": "2026-02-18 14:30:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o625evm",
          "author": "Revolutionary-Bet-58",
          "text": "I'm biased but I can recommend you to check out [inkog.io](http://inkog.io) , you can insert your LangChain agent in there and get feedback directly to solve some issues that you will face before debugging like infinite loops, tool calls etc . It will also recommend you how to fix the problems with examples, or you can just use the Inkog MCP and let Claude fix it for you :D\n\nHappy to sit down with you if you have any questions ",
          "score": 1,
          "created_utc": "2026-02-18 14:36:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o628qhb",
          "author": "penguinzb1",
          "text": "trace replay is great for diagnosing what happened, but the tool loop you described, where the agent ignored exit conditions, is also the kind of thing that shows up before users see it if you run it against adversarial or edge case scenarios first. what we've found is that simulating these before deployment catches them earlier than any trace tool can, because you're finding the failure before the first incident.",
          "score": 1,
          "created_utc": "2026-02-18 14:52:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rc7zst",
      "title": "How are you persisting agent work products across sessions? (research docs, reports, decisions)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rc7zst/how_are_you_persisting_agent_work_products_across/",
      "author": "syumpx",
      "created_utc": "2026-02-23 04:40:52",
      "score": 11,
      "num_comments": 17,
      "upvote_ratio": 0.87,
      "text": "I've been building agents with LangGraph for a few months now (research agents that monitor Reddit/TikTok, draft reports, send Slack messages) and the thing that keeps biting me is what happens between sessions.\n\n\n\nLangGraph checkpointers handle in-graph state fine. But the actual artifacts agents produce, a 2-page research report, a campaign brief with competitor analysis, a list of sourced Reddit threads, that stuff just disappears. Next session the agent starts from zero. I end up manually pasting previous outputs into the system prompt which feels completely wrong.\n\n\n\nThe approach I kept coming back to was giving agents a shared file store where they write their work as versioned files (markdown with YAML frontmatter for metadata). One agent writes research/competitor-pricing.md with status: draft, next session another agent picks it up, reads it, updates it. Every write is a new version so nothing gets overwritten.\n\n\n\nI open sourced this as [https://github.com/pixell-global/sayou](https://github.com/pixell-global/sayou) if anyone wants to look at the approach. But I'm more interested in how others are handling this:\n\n\n\nAre you using LangGraph's persistent checkpointers for cross-session artifact storage, or only for in-graph state? Just dumping outputs to JSON/text files and re-loading them?\n\nUsing a vector DB for this? (I tried Pinecone but you can't version or diff anything stored as embeddings, which made it useless for docs that evolve over time.) Or just accepting that agents start fresh every session?\n\nThe more agents I build the more I think the real bottleneck isn't reasoning or tool use. It's that agents have nowhere to put their work.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1rc7zst/how_are_you_persisting_agent_work_products_across/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o6wj3qg",
          "author": "KalZaxSea",
          "text": "I use mongodb memory, it loads chat to mongob and reload when needed for another chat",
          "score": 1,
          "created_utc": "2026-02-23 05:10:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wjws0",
              "author": "syumpx",
              "text": "Interesting, so you're storing full chat histories and reloading them as context? How do you handle it when the conversation history¬†gets too long for the context window? Do you summarize, truncate, or selectively load specific messages?\n\nThe thing that kept bugging me with chat persistence was that conversations are a lossy format for knowledge. The useful info (a decision, a research finding, a config choice) is buried in 50 messages of back and forth. Curious if you've run into that or if raw chat reload works well enough for your use case.\n\n",
              "score": 1,
              "created_utc": "2026-02-23 05:16:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6wk5h8",
                  "author": "KalZaxSea",
                  "text": "I trush gemini flash long context and performance. when I reload I do not load tools calls only ai and human messages. also if there is a long context needed task I use subagents to get rid of long context and subagent retruns the core info to main agent",
                  "score": 1,
                  "created_utc": "2026-02-23 05:18:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6woffu",
          "author": "kikkoman23",
          "text": "Yea sounds like you‚Äôre just persisting your thread or conversation history. \n\nThis is different than langgraph check pointing which is needed for usage with langgraph and for the workflow to know when to continue to next node in the workflow. \n\nI overlooked this at first thinking checkpoint was the same as conversation history. But it is not. \n\nSo you‚Äôll need some type of persistence for the messages. And then just retrieve it when you‚Äôre navigating to different threads.",
          "score": 1,
          "created_utc": "2026-02-23 05:52:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wrcvu",
              "author": "syumpx",
              "text": "Yeah exactly, checkpointers track graph state, not the stuff agents actually produce. That distinction tripped me up too. ¬† ¬† ¬† ¬† ¬† ¬†\n\nBut even with message persistence, the useful output (a research doc, a decision, a config choice) is buried in 50 messages of back and forth. You end up burning context window tokens replaying a whole thread just to recover one artifact.¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n\n\n\nThat's what pushed me toward storing work products separately from conversations. The conversation is how the work happened. The¬†artifact is what matters next session.¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n\n\n\n¬† Are you loading full threads back in or doing some kind of selective filtering?",
              "score": 1,
              "created_utc": "2026-02-23 06:17:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xi05g",
          "author": "Friendly-Ask6895",
          "text": "we landed on something really similar to your versioned markdown approach tbh. the key insight for us was separating \"conversation state\" from \"work products\" completely. conversations are ephemeral, but the reports/analyses/decisions the agent produces should live in a proper file system with git-style versioning.\n\n  \nwhat made it click was giving the agent explicit \"save artifact\" and \"load artifact\" tools instead of trying to shove everything through memory. the agent writes a research doc to disk, tags it with metadata, and any future session can pull it back by topic or date. feels way more natural than trying to reconstruct work from chat history.",
          "score": 1,
          "created_utc": "2026-02-23 10:31:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xiiwa",
              "author": "syumpx",
              "text": "This is exactly what we built Sayou around. Separating conversation state from work products, with agents writing to a filesystem instead of memory. Versioning and metadata retrieval built in.\nSounds like you landed on a really similar approach. Would love to compare notes. Mind if I DM you to set up a quick call? I would love to see what you have built and what you think about sayou in general",
              "score": 1,
              "created_utc": "2026-02-23 10:36:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xx13i",
          "author": "Comfortable_Poem_866",
          "text": "The semantic search approach you dismissed with Pinecone makes more sense when the retrieval layer is local and the artifacts stay as plain files ‚Äî you get fuzzy cross-session recall without losing the ability to read, diff, or version the files directly.\n\nThat‚Äôs the approach I took with CtxVault ‚Äî agents write markdown files to a local vault via an API endpoint, content gets embedded and indexed immediately, and any future session can retrieve it semantically. The files are just files, so versioning is a separate concern you can handle however you want.\n\nDifferent tradeoff than yours ‚Äî no built-in versioning, but semantic retrieval across sessions without managing metadata manually. https://github.com/Filippo-Venturini/ctxvault  if useful.",
          "score": 1,
          "created_utc": "2026-02-23 12:37:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6y7w71",
              "author": "syumpx",
              "text": "Nice, CtxVault looks interesting. We made a different bet though. Instead of building a retrieval system for agents, Sayou just gives them a filesystem and lets them figure out what they need. The goal is to make agents truly autonomous. If an agent needs a retrieval system built by humans to find its own work, it's not really autonomous.\n\nWith context windows getting bigger and reasoning getting better, agents can navigate a simple filesystem on their own. They just need somewhere to put their work and pick it up later. That's it.\n\nSemantic retrieval is powerful but adds a layer of complexity that agents shouldn't need if the filesystem is well structured",
              "score": 1,
              "created_utc": "2026-02-23 13:46:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6yyr1k",
                  "author": "Comfortable_Poem_866",
                  "text": "Fair point on autonomy ‚Äî it‚Äôs a genuine philosophical difference. Filesystem navigation works well when the structure is predictable and the agent knows roughly what it‚Äôs looking for.\n\nThe case for semantic retrieval is when it doesn‚Äôt ‚Äî when the query and the stored content use different vocabulary, or when the agent needs to surface something it didn‚Äôt know it would need later. At that point you‚Äôre spending reasoning tokens on search instead of on the actual task.\n\nThe other angle for me is control ‚Äî a vault gives you a monitorable, isolated knowledge base. The agent is still autonomous in how it queries, but you have visibility and separation by design.\n\nSayou looks interesting though, the versioned artifact approach is a smart way to handle docs that evolve over time. Different bets on where the complexity should live.",
                  "score": 1,
                  "created_utc": "2026-02-23 16:04:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o71zdbb",
          "author": "inguz",
          "text": "The SAMB benchmark looks interesting.",
          "score": 1,
          "created_utc": "2026-02-24 00:58:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71zv05",
              "author": "syumpx",
              "text": "its quite simple benchmark. i would never dream of submitting this to neurips, but sayou had some interesting data point against other agent memories like mem0 and zep \n\nhttps://preview.redd.it/05hxl229dclg1.png?width=2160&format=png&auto=webp&s=e39808eddb396b5464ece79940cbdc9197b332aa\n\n",
              "score": 1,
              "created_utc": "2026-02-24 01:01:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o71zx2t",
                  "author": "syumpx",
                  "text": "https://preview.redd.it/nuvglfdbdclg1.png?width=2160&format=png&auto=webp&s=0d3c9bb23e7261676ecc2716056851e86103cb68\n\n",
                  "score": 1,
                  "created_utc": "2026-02-24 01:02:01",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rcykas",
      "title": "MCP that blocks prompt injection attacks locally",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rcykas/mcp_that_blocks_prompt_injection_attacks_locally/",
      "author": "AssumptionNew9900",
      "created_utc": "2026-02-24 00:03:40",
      "score": 11,
      "num_comments": 7,
      "upvote_ratio": 0.91,
      "text": "Guys guys guys‚Ä¶i really got tired of burning API credits on prompt injections, so I built an open-source local MCP firewall.. because i want my openclaw to be secure. I run 2 instances.. one on vps and one mac mini.. so i wanted something (not gonna pay) thing so all the prompts are validated before it reaches to openclaw.. so i build a small utility tool..\n\nBeen deep in MCP development lately, mostly through Claude Desktop, and kept running into the same frustrating problem: when an injection attack hits your app, you are going to be the the one eating the API costs for the model to process it.\n\nIf you are working with agentic workflows or heavy tool-calling loops, prompt injections stop being theoretical pretty fast.\n\nActually i have seen them trigger unintended tool actions and leak context before you even have a chance to catch it.\n\nThe idea of just trusting cloud providers to handle filtering and paying them per token (meehhh) for the privilege so it really started feeling really backwards to me.\n\nSo I built a local middleware that acts as a firewall. It‚Äôs called Shield-MCP and it‚Äôs up on GitHub. aniketkarne/PromptInjectionShield : [https://github.com/aniketkarne/PromptInjectionShield/](https://github.com/aniketkarne/PromptInjectionShield/)\n\nIt sits directly between your UI or backend etc and the LLM API, inspecting every prompt locally before anything touches the network.\n\nI structured the detection around a ‚ÄúCute Swiss Cheese‚Äù model making it on a layering multiple filters so if something slips past one, the next one catches it.\n\nBecause everything runs locally, two things happen that I actually care about:\n\n1. Sensitive prompts never leave your machine during the inspection step\n2. Malicious requests get blocked before they ever rack up API usage\n\nDecided to open source the whole thing since I figured others are probably dealing with the same headache",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1rcykas/mcp_that_blocks_prompt_injection_attacks_locally/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o71uihx",
          "author": "johndoerayme1",
          "text": "Curious how this fits into MCP architecture. It's a tool? How do tools act as middleware?",
          "score": 1,
          "created_utc": "2026-02-24 00:31:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71vecx",
              "author": "AssumptionNew9900",
              "text": "Well, Technically, yes, this server just exposes a single tool called analyze_prompt.\nIf you plug it straight into Claude Desktop, it acts like a standard tool (meaning the LLM has to decide to call it to check the prompt, which means you already sent the text over the wire).\nBut where it actually acts as \"middleware\" is when your building custom apps, RAG pipelines, or using Langchain. Because MCP is just a standard protocol, you can configure your client app to call the Shield MCP server locally first.\nSo the flow basically looks like this:\nUser Input -> Your App -> (local MCP call) Shield analyze_prompt -> [If safe] -> Cloud LLM API.\nYour orchestration layer just uses the tool output (is_injection: true/false) as a gatekeeper to kill the request before you spend any API tokens. Hope that makes sense!",
              "score": 2,
              "created_utc": "2026-02-24 00:36:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o72shen",
                  "author": "Sungog1",
                  "text": "Got it, so it acts as a gatekeeper for the API calls by checking prompts locally first. That‚Äôs a smart way to ensure you‚Äôre not wasting credits on bad inputs. Makes sense for custom setups, especially in RAG pipelines!",
                  "score": 1,
                  "created_utc": "2026-02-24 03:51:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o71y4h3",
          "author": "Andres10976",
          "text": "That's called a safeguard and Lllama Prompt Guard is already good at it. If you want you can add it to a hook with a custom implementation.",
          "score": 1,
          "created_utc": "2026-02-24 00:51:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71yw1q",
              "author": "AssumptionNew9900",
              "text": "Yeah 100% fair. Prompt Guard is super solid and definitely fits the \"safeguard\" bill perfectly.\nThe main reason I built Shield-MCP wasn't to compete with the underlying ML models (I'm actually just pulling ProtectAI's DeBERTa model for the ML tier anyway). I mostly built it to solve the integration friction.\nWriting custom hooks, building the API, and wrapping an ML model in a microservice takes time. Since a lot of agent frameworks are adopting MCP natively right now, I wanted something that just speaks the protocol out of the box. If your stack already uses MCP, you don't need to write a custom implementation or manage a separate service‚Äîyou just drop it in.\nPlus, I prefer the \"swiss cheese\" tiered approach (fast regex heuristics -> ML model -> entropy checks) rather than throwing every single prompt at a heavy ML model right away.\nBut yeah, if your already running custom hooks and infrastructure, Prompt Guard is a great way to go. Have you implemented it in your own stack yet?",
              "score": 1,
              "created_utc": "2026-02-24 00:56:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o74mo9h",
          "author": "FishIndividual2208",
          "text": "But why? There are tiny modells available to do prompt injection checks..",
          "score": 1,
          "created_utc": "2026-02-24 12:56:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9kx8o",
      "title": "I built an open-source library on top of LangChain for batch-transforming structured data through LLMs",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r9kx8o/i_built_an_opensource_library_on_top_of_langchain/",
      "author": "papipapi419",
      "created_utc": "2026-02-20 03:50:54",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "The problem: every project I worked on needed the same thing ‚Äî take rows of data, run them through an LLM, get back validated Pydantic models. I kept rewriting batching, retries, concurrency, and row-ordering logic.\n\nSo I packaged it up: **smelt-ai**.\n\nWhat it handles under the hood:\n\n* **Batching** ‚Äî splits rows into configurable batch sizes\n* **Concurrency** ‚Äî async semaphore-based, no threads\n* **Retries** ‚Äî exponential backoff for 429s, 5xx, validation errors\n* **Row ordering** ‚Äî injects row\\_id, validates it, reorders results to match input\n* **Structured output** ‚Äî uses `with_structured_output` so you get typed Pydantic models back\n* **Metrics** ‚Äî token counts, timing, retry stats per run\n\nWorks with any LangChain provider (OpenAI, Anthropic, Gemini, etc.) since it uses `init_chat_model` under the hood.\n\n`pip install smelt-ai`\n\nGitHub: [https://github.com/Cydra-Tech/smelt-ai](https://github.com/Cydra-Tech/smelt-ai)  \nDocs: [https://cydra-tech.github.io/smelt-ai/](https://cydra-tech.github.io/smelt-ai/)\n\nWould love feedback from the LangChain community. What would you add?\n\nEdit: Grammer",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r9kx8o/i_built_an_opensource_library_on_top_of_langchain/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rclq3t",
      "title": "Looking to Join Serious LangChain / AI Backend Projects",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rclq3t/looking_to_join_serious_langchain_ai_backend/",
      "author": "arap_bii",
      "created_utc": "2026-02-23 16:15:30",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "Hi everyone, I‚Äôm Kevin, a backend-focused developer with deep experience in Python and production-grade systems. I‚Äôm looking to join serious AI/LLM projects to contribute technically and help build scalable solutions.\n\nI‚Äôm open to small equity or modest pay setups to get the project moving‚Äîmainly looking for impactful work and a strong team.\n\nIf you‚Äôre building something interesting with LangChain or other AI tooling and need someone to handle backend, pipeline, or AI integration work, drop me a message!",
      "is_original_content": false,
      "link_flair_text": "Announcement",
      "permalink": "https://reddit.com/r/LangChain/comments/1rclq3t/looking_to_join_serious_langchain_ai_backend/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rccvho",
      "title": "I built an Agentic OS using LangGraph & MCP (Looking for contributors!)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rccvho/i_built_an_agentic_os_using_langgraph_mcp_looking/",
      "author": "Top_Conversation7452",
      "created_utc": "2026-02-23 09:23:15",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nOver the last few months, I've been building an open-source, Multi-Agent operating system. It is fully local, uses a distributed MCP (Model Context Protocol) architecture, and hooks deeply into Google Workspace.\n\n**The Tech Stack:**\n\n* **Orchestration:** LangGraph (using a strict \"One-Way Turnstile\" routing pattern so the LLM doesn't drown in 50+ tool schemas).\n* **Memory:** Episodic RAG + a KuzuDB Knowledge Graph.\n* **Tools:** Multi-Server MCP handling Gmail, Calendar, Drive, Docs, Sheets, and a Docker code execution sandbox.\n* **UI:** Chainlit for real-time text and continuous voice listening (Whisper STT / Piper TTS).\n\nI built this to solve the context-bloat and tool-hallucination problems I kept seeing in monolithic agent designs.\n\n  \n**Why I'm posting here:** Right now it is very much an basic prototype. The architecture works beautifully, but it needs hardening and testing .\n\nI just made the repo public and created a few `[help wanted]` issues if anyone is interested in collaborating on Agentic AI patterns:\n\n1. **Safety:** Implementing a Human-in-the-Loop (HITL) interrupt in LangGraph before the agent executes dangerous Python code.\n2. **Context Management:** Building payload pointers/pagination for when the Google Sheets tool tries to read a massive CSV and blows up the token limit.\n3. **Testing:** Adding `pytest` coverage for the MCP tool schemas.\n\nRaise any issue u find and contribute\n\n**Repo link:** [https://github.com/Yadeesht/Agentic-AI-EXP](https://github.com/Yadeesht/Agentic-AI-EXP)\n\nWould love any brutal feedback on the system foundation. thanks for spending time in reading this post",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1rccvho/i_built_an_agentic_os_using_langgraph_mcp_looking/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o6xhpu4",
          "author": "Friendly-Ask6895",
          "text": "the one-way turnstile routing pattern is smart, we hit that exact same wall with tool schema overload. once you get past \\~20 tools the model just starts picking wrong ones or hallucinating parameters.\n\n  \nre: the HITL interrupt for code execution, we ended up going with a tiered approach. read-only operations run automatically, anything that writes or deletes goes through approval, and anything touching external APIs gets a full preview of what's about to happen. took some iteration but it cut our false-positive interrupts by like 80%.\n\n  \nhow are you handling the knowledge graph updates? like does the agent decide what goes into KuzuDB or is there a separate indexing pipeline?",
          "score": 3,
          "created_utc": "2026-02-23 10:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xzfyf",
              "author": "Top_Conversation7452",
              "text": "the 20+ tool wall is so real. I was losing my mind trying to pack 57 different content tools into a single agent. The model just starts grabbing the completely wrong tool. Breaking it down into a strict supervisor with a hierarchical routing structure was the only way to get it stable.\n\ni have been testing the oss-20b model across a few free-tier providers and the hosting setup makes a massive difference. openrouters endpoint has been giving me the best results by far. groq version of the oss-20b model seems highly overfitted for tool use it just aggressively spams tool calls even when the prompt doesn't require one.\n\nHuge kudos for that tiered HITL idea, by the way. \n\nknowledge grpah its handled by a dedicated memory update node  in the graph. I just trigger at EOD (mostly because I didn't have a frontend built out yet), but it‚Äôs also exposed as a tool so the supervisor can trigger an update dynamically on request.\n\nThe actual indexing pipeline runs in two layers: \n\n1. the extraction part the LLM scans the recent context and generates the node id, keywords, descriptions, and edge relationships. \n\n2. validation by semantic:  before blindly inserting the system runs a similarity search against existing entities in KuzuDB. If it finds potential matches, it feeds those back the LLM alongside the new data. The LLM then validates whether to merge, update the existing node, or discard the new extraction entirely.\n\nIt adds an extra inference step, but it keeps the graph super clean and stops it from creating duplicate nodes for the same entity.",
              "score": 2,
              "created_utc": "2026-02-23 12:54:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o749wwc",
          "author": "tech2biz",
          "text": "Great architecture direction, like! One thing wedid: explicit per-step runtime budgets (model, retries, tool timeouts), otherwise orchestration-heavy systems drift in cost quickly",
          "score": 2,
          "created_utc": "2026-02-24 11:24:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74ekth",
              "author": "Top_Conversation7452",
              "text": "thanks for the tip, i truly missed that i dont have any budget or timeout setup right now. did you track your token budgets directly inside your graph state or handle it somewhere else?",
              "score": 1,
              "created_utc": "2026-02-24 12:00:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74gecc",
                  "author": "tech2biz",
                  "text": "We dynamically cascade through models based on classifier and quality evals. Open sourced it: cascadeflow on github. So you can manage (and reduce) token costs.",
                  "score": 2,
                  "created_utc": "2026-02-24 12:14:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r7vn7p",
      "title": "I can‚Äôt figure out how to ask LLM to write an up-to-date LangChain script with the latest docs.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r7vn7p/i_cant_figure_out_how_to_ask_llm_to_write_an/",
      "author": "gowtham150",
      "created_utc": "2026-02-18 06:34:18",
      "score": 8,
      "num_comments": 16,
      "upvote_ratio": 0.73,
      "text": "Whenever I ask claude or chatgpt to write me a simple langchain agent - even the very simple ones - it always gives me a script with outdated libraries. I tried using claude with context7mcp and langchain docs mcp - still i get out of date obsolete script with deprecated libraries. Even for a simple use case i have to go to langchain docs and get it. Its frustrating to ask LLM to write a sample code and later on to find that its deprecated. How you are you guys solving this problem.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r7vn7p/i_cant_figure_out_how_to_ask_llm_to_write_an/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o60jchc",
          "author": "mdrxy",
          "text": "Would encourage cloning the repos locally and letting your agent know that it can traverse the source code in your filesystem!",
          "score": 5,
          "created_utc": "2026-02-18 07:22:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63mjh1",
              "author": "orthogonal-ghost",
              "text": "This is a fantastic idea",
              "score": 1,
              "created_utc": "2026-02-18 18:38:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o60tj3n",
          "author": "gowtham150",
          "text": "The general observation is that if i use claude code with context 7 MCP and ask it to write a a simple agent with Langchain it gives me a script most of the time with outdated versions and libraries. Same with chatgpt. So it's becoming extremely difficult to just test out a feature.",
          "score": 3,
          "created_utc": "2026-02-18 08:57:01",
          "is_submitter": true,
          "replies": [
            {
              "id": "o61bc9k",
              "author": "Individual_Day_9508",
              "text": "Use a CLAUDE.md or AGENTS.md file in your workspace to set a strict rule enforcing Langchain 1.x syntax. If you pair that explicit instruction with context 7, it completely fixes the outdated library issue.",
              "score": 1,
              "created_utc": "2026-02-18 11:34:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o61ci16",
                  "author": "gowtham150",
                  "text": "Ok let me check that. Thanks for sharing",
                  "score": 1,
                  "created_utc": "2026-02-18 11:44:01",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o60eznz",
          "author": "gaureshai",
          "text": "Very hard.  Because langchain docs are also outdated.",
          "score": 4,
          "created_utc": "2026-02-18 06:44:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60zbec",
              "author": "NoleMercy05",
              "text": "Not true anymore",
              "score": 2,
              "created_utc": "2026-02-18 09:51:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6179bd",
                  "author": "gaureshai",
                  "text": "Well then it's good. I had really hard time in js docs. Will try it again then.",
                  "score": 1,
                  "created_utc": "2026-02-18 11:01:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o60jaqu",
              "author": "mdrxy",
              "text": "Not sure what you mean -- can you point to specific pages? Will flag with the team",
              "score": 1,
              "created_utc": "2026-02-18 07:22:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o60l0t8",
          "author": "Character_Leg1134",
          "text": "Use chat.langchain.com \nIts their own bot \nWhich can give you the code with updated libraries",
          "score": 2,
          "created_utc": "2026-02-18 07:38:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60tdxv",
              "author": "gowtham150",
              "text": "Will try this",
              "score": 1,
              "created_utc": "2026-02-18 08:55:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o61cjj2",
              "author": "gowtham150",
              "text": "This has been working well so far. Thanks for sharing.",
              "score": 1,
              "created_utc": "2026-02-18 11:44:20",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o63c2w2",
              "author": "rk_11",
              "text": "Second this",
              "score": 1,
              "created_utc": "2026-02-18 17:52:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o62l7az",
          "author": "notAllBits",
          "text": "That is the cost of unstable conventions (API/classes) in coding. If LLMs cannot be confident about their memory, they spoil it for everyone",
          "score": 1,
          "created_utc": "2026-02-18 15:51:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60qx1v",
          "author": "kolmar41k",
          "text": "If you using and IDE, try using an MCP called 'context7', it provides up to date docs including langgraph/langchain to your llm",
          "score": 1,
          "created_utc": "2026-02-18 08:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60tczj",
              "author": "gowtham150",
              "text": "I already did, like i mentioned in my post. I used context 7 and Langchain has its own mc as well",
              "score": 1,
              "created_utc": "2026-02-18 08:55:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rarwc6",
      "title": "My LangChain agent kept ignoring its own rules. Took me three days to figure out why.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rarwc6/my_langchain_agent_kept_ignoring_its_own_rules/",
      "author": "Acrobatic_Task_6573",
      "created_utc": "2026-02-21 14:03:43",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 0.75,
      "text": "Built a personal assistant agent on top of LangChain about two months ago. It worked fine at first. Then it started skipping steps I had explicitly told it to skip, making API calls it was never supposed to make. Once it tried to respond to a message as a completely different persona.\n\nI spent two days tweaking the system prompt. Different model temperatures. Re-read the LangChain docs twice. Nothing worked consistently.\n\nTurned out the problem wasn't the code or the model at all. It was the config files. I had a rough SOUL.md and a few notes in AGENTS.md but they were inconsistent, half-finished, and contradicting each other in spots I hadn't noticed.\n\nSomeone pointed me to Lattice OpenClaw. You answer questions about what your agent is supposed to do, what it should never do, how it handles memory and communication, and it generates SOUL.md, AGENTS.md, SECURITY.md, MEMORY.md, and HEARTBEAT.md in one shot. Five minutes.\n\nNight and day difference. Same model, same code, stable for three weeks now just from having coherent config files.\n\nAnyone else hit this? Wondering if it's a common blind spot or just me not paying enough attention early on.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1rarwc6/my_langchain_agent_kept_ignoring_its_own_rules/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o6lyb8m",
          "author": "mzinz",
          "text": "Wait, was this OpenClaw running a langchain agent?",
          "score": 5,
          "created_utc": "2026-02-21 15:02:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pp7br",
          "author": "doncheeto12",
          "text": "Honestly there are too many .md files now. Some repos at my company are drowning in Claude.md, agents.md, readme.md, soul.md, a million different docs Md files. It‚Äôs out of hand and like your experience, proves very hard to debug. Crazy how unscientific applying AI is.",
          "score": 2,
          "created_utc": "2026-02-22 03:29:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71ce26",
              "author": "ruach137",
              "text": "And they all degrade performance",
              "score": 1,
              "created_utc": "2026-02-23 22:51:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ra2b0r",
      "title": "expectllm: A lightweight alternative when you just need pattern matching",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1ra2b0r/expectllm_a_lightweight_alternative_when_you_just/",
      "author": "Final_Signature9950",
      "created_utc": "2026-02-20 17:51:55",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "I built a small library called **expectllm**.\n\n\n\nIf you've ever thought \"I just need to extract a number from an LLM response, why am I importing 50 modules?\" - this might be for you.\n\n\n\nIt treats LLM conversations like classic expect scripts:\n\n\n\nsend ‚Üí pattern match ‚Üí branch\n\n\n\nYou explicitly define what response format you expect from the model.\n\nIf it matches, you capture it.\n\nIf it doesn't, it fails fast with an explicit ExpectError.\n\n\n\nExample:\n\n    from expectllm import Conversation\n    \n    c = Conversation()\n    \n    c.send(\"Review this code for security issues. Reply exactly: 'found N issues'\")\n    c.expect(r\"found (\\d+) issues\")\n    \n    issues = int(c.match.group(1))\n    \n    if issues > 0:\n       c.send(\"Fix the top 3 issues\")\n\n\n\nCore features:\n\n\\- expect\\_json(), expect\\_number(), expect\\_yesno()\n\n\\- Regex pattern matching with capture groups\n\n\\- Auto-generates format instructions from patterns\n\n\\- Raises explicit errors on mismatch (no silent failures)\n\n\\- Works with OpenAI and Anthropic (more providers planned)\n\n\\- \\~365 lines of code, fully readable\n\n\\- Full type hints\n\n\n\nRepo: [https://github.com/entropyvector/expectllm](https://github.com/entropyvector/expectllm)\n\nPyPI: [https://pypi.org/project/expectllm/](https://pypi.org/project/expectllm/)\n\n\n\nIt's not designed to replace LangChain or similar frameworks - those are great when you need the full toolbox. This is for when you don't. Minimalism, control, transparent flow.\n\n\n\nWould appreciate feedback:\n\n\\- Is this approach useful in real-world projects?\n\n\\- What edge cases should I handle?\n\n\\- Where would this break down?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1ra2b0r/expectllm_a_lightweight_alternative_when_you_just/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r8ngzq",
      "title": "üöÄ Launch Idea: A Curated Marketplace for AI Agents, Workflows & Automations",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r8ngzq/launch_idea_a_curated_marketplace_for_ai_agents/",
      "author": "NoSwimming4210",
      "created_utc": "2026-02-19 02:46:10",
      "score": 7,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Right now, discovering reliable AI agents and automation systems is messy ‚Äî too many scattered tools, too little trust, and almost no true curation.\n\nThe vision:\nA single marketplace where businesses and creators can find tested, ready-to-deploy AI agents, structured workflows, and powerful automations ‚Äî all organized by real-world use cases.\n\nWhat makes it different:\n‚úîÔ∏è Curated listings ‚Äî quality over quantity\n‚úîÔ∏è No-code + full-code solutions in one place\n‚úîÔ∏è Verified workflows that actually work\n‚úîÔ∏è Builders can monetize their systems\n‚úîÔ∏è Companies adopt AI faster without technical chaos\n\nThis isn‚Äôt another tool directory ‚Äî it‚Äôs an execution layer for applied AI.\n\nLooking for:\n‚Ä¢ Early adopters who want to try curated AI workflows\n‚Ä¢ Builders interested in listing their agents\n‚Ä¢ Feedback on must-have features before MVP\n\nComment or connect if you want to be part of shaping it.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r8ngzq/launch_idea_a_curated_marketplace_for_ai_agents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o6a7lfx",
          "author": "Delicious-One-5129",
          "text": "A curated marketplace could save so much time - finding reliable AI workflows is such a headache right now. Would definitely be interested in testing or giving feedback.",
          "score": 1,
          "created_utc": "2026-02-19 18:23:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lbxby",
          "author": "Traditional-Carry409",
          "text": "AI slop post",
          "score": 1,
          "created_utc": "2026-02-21 12:41:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lceab",
              "author": "NoSwimming4210",
              "text": "Hey its not a Ai Slop post we are actually being constantly improving the idea, behind a team of people with professors are working on it. This is a part to know the peoples response on it.",
              "score": 1,
              "created_utc": "2026-02-21 12:44:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rd7jja",
      "title": "I built a new MCP Server to stop agents from hallucinating medical math (has 54 calculators + 14 clinical guidelines)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rd7jja/i_built_a_new_mcp_server_to_stop_agents_from/",
      "author": "Magodo123",
      "created_utc": "2026-02-24 05:23:34",
      "score": 7,
      "num_comments": 7,
      "upvote_ratio": 0.89,
      "text": "Hey guys,  \n  \n  \nI've been building health agents lately and kept running into a scary problem: LLMs are terrible at medical math and following strict clinical guidelines. If you ask an agent to evaluate a patient's case, it will often boldly hallucinate a MELD score or agree with treatments that actually violate standard care.   \n  \n  \nTo fix this, I put together \\*\\*Open Medicine\\*\\*. It's an open-source Python library and an MCP Server.  \n  \n  \nInstead of letting the agent guess, you just give it these tools:  \n\\- \\`search\\_clinical\\_calculators\\`: Let the agent find the right formula (like Glasgow-Blatchford).  \n\\- \\`execute\\_clinical\\_calculator\\`: Runs the math in pure, tested Python. No LLM logic involved. It takes a JSON payload, validates it via Pydantic, and returns the exact score, interpretation, and the DOI of the original medical paper.  \n\\- \\`retrieve\\_guideline\\`: Lets the agent read version-controlled markdown text of actual clinical guidelines (like the 2023 AHA guidelines) instead of relying on its latent training data or searching PubMed and retrieving tons of irrelevant papers.  \n  \n  \nAs a quick example of why this matters: I gave an agent a clinical note for a GI Bleed where the doctor planned for \"aggressive fluid resuscitation.\" Without the tools, the LLM just agreed. But when connected to the open-medicine-mcp server, the agent pulled the actual NICE guidelines, realized it was a variceal bleed, and corrected the plan to a \"restrictive transfusion strategy\" because aggressive fluids increase portal pressure.   \n  \n  \nSource code is here: [https://github.com/RamosFBC/openmedicine](https://github.com/RamosFBC/openmedicine)  \n  \n  \nIt's all MIT licensed. I'd love to hear from other folks building in this space. Have you been using MCP servers for this kind of deterministic logic yet? What calculators or guidelines should I try to add next?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1rd7jja/i_built_a_new_mcp_server_to_stop_agents_from/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o739yy6",
          "author": "invertednz",
          "text": "Up voting, thanks for doing this, not something I'll use but amazing work.",
          "score": 1,
          "created_utc": "2026-02-24 05:58:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73ayvg",
              "author": "Magodo123",
              "text": "Thanks! Really helpful for healthcare use cases, didn‚Äôt find anything that would do these tasks",
              "score": 1,
              "created_utc": "2026-02-24 06:06:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73f49b",
          "author": "Reasonable_Event1494",
          "text": "Great work. So, it is using RAG and tool calling both to make the outputs reliable. So, are you giving it memory also small context memory?",
          "score": 1,
          "created_utc": "2026-02-24 06:41:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73gcm4",
              "author": "Magodo123",
              "text": "This repo is actually only the tool layer. The tool definitions inside the MCP server instruct the model on how to  call the tool searcher and find the necessary calculator tool or medical guideline. From there, the context retrieved by the tools guides the model through retrieving the most accurate information. For now it only makes regex match to find tools but I plan to implement semantic search in the near future to allow more complex queries. The big advantage here is that these tools allow the model to replicate accurately how doctors actually make real decisions, which are made by reproducing evidences from numerous researches based on these clinical scores",
              "score": 1,
              "created_utc": "2026-02-24 06:51:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73wb4t",
          "author": "jovansstupidaccount",
          "text": "Sounds so cool",
          "score": 1,
          "created_utc": "2026-02-24 09:20:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73wbzx",
          "author": "makinggrace",
          "text": "Super important work. Anyone that works with AI needs to help educate laypeople about math fail. It's a good gateway conversation into how LLM works and limitations. But actual use cases -- and solutions? Better than anything else.",
          "score": 1,
          "created_utc": "2026-02-24 09:20:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75e5yq",
          "author": "penguinzb1",
          "text": "the GI bleed example is the clearest illustration of the problem. you'd never catch that gap from unit tests or benchmarks. the agent agreeing with incorrect treatment is a behavioral failure that only surfaces when you run it through the actual clinical scenarios it'll face. the right answer varies by context (variceal vs non-variceal) in ways that look fine in general testing but fail on the specific input patterns that matter. this is why domain-specific simulation before deployment matters so much more in medical than anywhere else.",
          "score": 1,
          "created_utc": "2026-02-24 15:22:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcemox",
      "title": "Has MCP actually changed how your team handles integrations, or is it still mostly hype?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rcemox/has_mcp_actually_changed_how_your_team_handles/",
      "author": "Friendly-Ask6895",
      "created_utc": "2026-02-23 11:08:42",
      "score": 6,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Genuine question because I keep seeing MCP discussed as this game changer but I want to hear from teams actually using it in production.\n\nour situation: we had like 12 custom API integrations for our agent stack. Each one with its own auth handling, error states, rate limiting, the works. every time an upstream API changed we'd burn a sprint or two patching connectors. Classic N√óM problem where adding a new data source meant building separate connectors for each agent that needed it.\n\nWe've been migrating to MCP servers over the past few months and honestly the \"build once, use everywhere\" promise is mostly holding up. one server exposes capabilities through a standard interface and any agent supporting the protocol can discover and use it. The capability negotiation at runtime is the part that surprised me most, clients just figure out what the server can do without hardcoded schemas.\n\nthe part I'm less sure about is governance at scale. When you have dozens of MCP servers across different tenant environments, how are people handling security review and audit logging? do you review the protocol implementation once and trust it across all servers, or are you doing per-server reviews?\n\nAlso curious about the portability angle. has anyone actually swapped out a model provider and had their MCP servers just work with the new one? That's the promise but I haven't stress tested it yet.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1rcemox/has_mcp_actually_changed_how_your_team_handles/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o6xq25n",
          "author": "Sharp_Branch_1489",
          "text": "The governance question is the real one. Build once use everywhere works until you have 20 MCP servers across different environments and no central view of what each one can access. The protocol standardizes the interface but it doesn't standardize who gets to audit what. That visibility gap gets expensive fast.",
          "score": 2,
          "created_utc": "2026-02-23 11:44:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}