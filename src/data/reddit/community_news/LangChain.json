{
  "metadata": {
    "last_updated": "2026-02-03 08:57:05",
    "time_filter": "week",
    "subreddit": "LangChain",
    "total_items": 20,
    "total_comments": 66,
    "file_size_bytes": 110220
  },
  "items": [
    {
      "id": "1qpci1h",
      "title": "You can now train embedding models ~2x faster!",
      "subreddit": "LangChain",
      "url": "https://i.redd.it/kbenz74xl3gg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-28 14:15:31",
      "score": 39,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qpci1h/you_can_now_train_embedding_models_2x_faster/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2t1d8v",
          "author": "Exciting-Royal-3361",
          "text": "That's awesome news. Does someone have experience with building high quality training data for embedding models? I know one popular technique is to take one or more passages and generate a matching query using an LLM and perhaps create multiple versions of the same query in different styles. \n\nAre there any other techniques for generating / sourcing training data based on a large corpus of data?",
          "score": 1,
          "created_utc": "2026-01-31 15:53:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpi07h",
      "title": "I built a job search assistant to understand LangChain Deep Agents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/gallery/1qpi07h",
      "author": "Acrobatic-Pay-279",
      "created_utc": "2026-01-28 17:34:30",
      "score": 31,
      "num_comments": 4,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qpi07h/i_built_a_job_search_assistant_to_understand/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2bvl13",
          "author": "qa_anaaq",
          "text": "Cool and thanks for sharing. 2 Qs. Do you feel the deep agents harness is any good, and how‚Äôs the cost of running it?",
          "score": 2,
          "created_utc": "2026-01-29 01:26:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dj8hk",
              "author": "Acrobatic-Pay-279",
              "text": "yeah it's good, especially for long running tasks. in the earlier version, I actually tracked write\\_todos with the statuses and it was breaking things down step by step. I also tried HITL flows. the docs mention prompt caching (Anthropic) and pluggable storage backends but I wasn't able to verify/use those.\n\nthe cost is just the underlying model (I used OpenAI in this case).",
              "score": 3,
              "created_utc": "2026-01-29 08:06:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ek6gp",
                  "author": "qa_anaaq",
                  "text": "Cool thanks a lot",
                  "score": 1,
                  "created_utc": "2026-01-29 13:08:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o353zeb",
          "author": "Ok_Row9465",
          "text": "Nice writeup. The ‚Äúplan + externalize context + delegate‚Äù pattern is basically what a lot of the stronger coding agents converged on, so it‚Äôs useful seeing it packaged as middleware instead of a one-off scaffold.\n\nOne thing worth watching with file-backed state is how it evolves over longer runs. In practice the hard part is not ‚Äústore in files‚Äù but ‚Äúdecide what belongs in files vs what stays in prompt vs what gets summarized‚Äù. Curious if you ran into any of these:\n\n\\- todo drift, where the plan becomes stale after a couple subagent loops  \n\\- filesystem bloat, where notes accumulate and retrieval becomes random unless you add an index or a ‚Äúworking set‚Äù  \n\\- cross-subagent consistency, since isolated tool loops are great until two tasks update overlapping artifacts\n\nA pattern that‚Äôs worked well on my side is adding a lightweight ‚Äúmemory manager‚Äù step between delegations: keep a small active context file (current constraints + user prefs + latest decisions), plus append-only logs for everything else, and periodically rewrite the active file (like a checkpoint). That tends to keep the agent from rereading 20 files every time.\n\nAlso curious how you‚Äôre scoring ‚Äúlow-quality URLs‚Äù. Is it just domain allow/deny + path heuristics, or are you doing any content-based checks (title patterns, [schema.org](http://schema.org) JobPosting, etc.)? That part usually decides whether the output feels magical or spammy.",
          "score": 1,
          "created_utc": "2026-02-02 12:00:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrpomi",
      "title": "Are MCPs outdated for Agents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qrpomi/are_mcps_outdated_for_agents/",
      "author": "FunEstablishment5942",
      "created_utc": "2026-01-31 02:17:17",
      "score": 27,
      "num_comments": 24,
      "upvote_ratio": 0.94,
      "text": "I saw a video of the OpenClaw creator saying that MCP tools are shit\nIn fact the only really working Agent  are moving away from defining strict tools (like MCP or rigid function calling) and giving the agent raw CLI tools and letting it figure it out.\n\n‚ÄãI‚Äôm looking into LangGraph for this, and while the checkpointers are amazing for recovering conversation history (threads), I'm stuck on how to handle the Computer State\n\n‚ÄãThe Problem:\nA conversation thread is easy to persist. But a CLI session is stateful (current working directory, cli commands, active background processes).\n\n‚ÄãIf an agent runs cd /my_project in step 1, and the graph pauses or moves to the next step, that shell context is usually lost unless explicitly managed.\n\n‚ÄãThe Question:\nIs there an existing abstraction or \"standard way\" in LangGraph to maintain a persistent CLI/Filesystem session context that rehydrates alongside the thread?If not would it be a good idea to add it?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qrpomi/are_mcps_outdated_for_agents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2qdp2h",
          "author": "cincyfire35",
          "text": "I lead a development team where we build with langgraph regularly.\n\nPeople who are naysayers on MCP dont realize that there are other applications for it than just spamming context with 10-50 irrelevant tools for a general purpose agent. With frameworks like langgraph, you can build and orchestrate custom agents for tasks with finely tuned contexts and tools, eliminating the need for things like skills and tool selectors. Pairing this with code based mcp execution, you can pretty much load 2-3 mcp servers with all their tools as python functions in a safe execution environment (see smolagents‚Äô safe python executor), tell the llm it can call them as python functions, and get a lot of the benefits from anthropics/cloudflare‚Äôs code mode articles by chaining calls into each other and performing calcs/aggregation outside the context window. You can even build logic to lazy load the tools if you want, but thats a waste if you can just route to a specialized agent for the given task. \n\nWe never use more than 2-3 mcp servers with curated tools selected for an agent because we pay per token. Why waste it with irrelevance? We let users build agents with specific goals and targets in mind, select only the tools they need, and it can solve/work through the task for them. Why give a rag agent for a legal team access to SQL tools for supply chain? Makes no sense. But some people just build one big agent and hope it works. Langgraph/langchain enables you to build custom workflows and agents to solve tasks efficiently. Can build in orchestration however you prefer (tons of flexibility and documented examples of how to do it) and accomplish what claude does with skills, but more predictably and reliably. \n\nAnd thats not the half of it. MCP is just a protocol. We build custom tools with fastMCP in python all the time and its an easy way to connect the tools to our langgraph agents or external ones. We host them in our platform and can connect to them as needed. It allows us to build powerful tools that can be reused across frameworks. You dont need an mcp servers with 100 tools it. Can spin up several servers in one app instance of compute with 1-3 specific to usecase tools each built in a very easy way with good testing/standards, then serve it to your agents. We also connect with external vendors mcps like alation or atlassian if building an agent to explore data or help devs with jira, for example. Tons in the ecosystem.",
          "score": 42,
          "created_utc": "2026-01-31 03:53:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wlb01",
              "author": "SpareIntroduction721",
              "text": "You used smolagents with Langgraph? Or code mode? I‚Äôve tried and failed, I tried the UTCP route as well and didn‚Äôt work too well",
              "score": 1,
              "created_utc": "2026-02-01 02:53:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ztyzh",
                  "author": "cincyfire35",
                  "text": "No, i ripped the code execution function out of it (since i liked the security it gave) and build support for it in our framework as a tool. We don‚Äôt use smolagents for the orchestration, just for the safe python execution environment we can control cleanly (we made some additional security enhancement/tweaks to make it work better with databricks). From there, it was trivial to make a code-mode that injected any other mcps provided to the model as python functions that could be executed in that environment if the agent wrote it as python code.",
                  "score": 1,
                  "created_utc": "2026-02-01 16:38:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2qd0zk",
          "author": "Number4extraDip",
          "text": "Didnt need to deal with lang through my deployment whatsoever. I use mcp and have no issues. Saves me time. CLI environments arent available to all users/hardware/OS",
          "score": 7,
          "created_utc": "2026-01-31 03:49:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2q4qdb",
          "author": "Prestigious_Pin4388",
          "text": "Short Answer: I don't use langgraph much so sorry I don't know.\nLong Answer:\nI think it's not right to give Agents complete autonomy because most or the AI apps me n u r making would be 'deterministic'\nWe would know exactly everything that is happening in it, \"what happens when the Agent doesn't call the tool? how do you debug this? how good is it's accuracy to call tools? what if I have to use an open mode for lower costs, it has much worse tool calling accuracy than gpt-5?  etc\" \n\nThese are all the questions in my mind when giving tools to LLMs, these things are non-deterministic.\nyes, we can give them \"better\" prompts and reduce temperature but that still creates vagueness and its much more difficult to debug things if they break.\nThis is the case for deterministic tools, now people are asking to give it complete freedom regardless of prompt injections or any security issues, and like you said, it gets difficult to manage these tools especially in production, imagine how hellish it would be to debug when things break.\n\nSo, I recommend you ignore these \"hype\" stuff.\nYou probably would have heard of how good is clawdbot( moldbot) but now see the whole drama around it.\n\nSome say it deleted all the files, some are finding security issues in it, even the creator said that it was a side project not meant for production.\nyet, still there's people yapping about how it saved them time n money, blah blah blah.\n\n\nhope this was helpful :)",
          "score": 3,
          "created_utc": "2026-01-31 02:57:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2q4ujh",
              "author": "Prestigious_Pin4388",
              "text": "it wasn't AI generated of course so, I suppose it was helpful ;)",
              "score": 3,
              "created_utc": "2026-01-31 02:58:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rajn0",
          "author": "indutrajeev",
          "text": "Yeah, if you don‚Äôt need any control. But MCP‚Äôs can act like a layer of governance around your Agent to check, validate, ‚Ä¶ what it does with other systems.\n\nJust giving it cli access is maybe faster but inherently much more difficult to control and check.",
          "score": 3,
          "created_utc": "2026-01-31 08:16:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qguqa",
          "author": "johndoerayme1",
          "text": "Tool fatigue is real. Recent studies are showing that tool overhead can be misleading and confusing for agents. DeepAgent went to filesystem in great part for that reason. Give agents a more broad set of functionality and let them figure out how to use them - evolve sets of skills that are curated more towards the actual environment in which they're running. This is where things seem to be moving right now.\n\nRecently Anthropic added tool search to Claude as part of trying to mitigate tool fatigue/bloat. \n\nA lot of modern thought is about keeping context small/clean... so adding a ton of \"here's all the tools you can use and all their definitions\" when most of them aren't really relevant to the limited scope of the current task focus really undermines that objective.\n\nCheck out Deepagents for your persistent filesystem. I've used it effectively for my own form of \"skills\" that the agents can evolve as they learn from interaction.",
          "score": 2,
          "created_utc": "2026-01-31 04:14:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rgulw",
              "author": "Tobi-Random",
              "text": "Recent? It's been a known fact for over a year already. That's old stuff in ai context",
              "score": 0,
              "created_utc": "2026-01-31 09:15:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2sd191",
                  "author": "johndoerayme1",
                  "text": "Cool yeah except Claude just came out with tool search to address this and Harrison Chase wrote an article about using filesystem to address this 1-2 months ago. But ok cool yes it's \"old stuff\". Sorry I misspoke. Thanks for correcting the least relevant part of my response.",
                  "score": 1,
                  "created_utc": "2026-01-31 13:42:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rgrn2",
          "author": "vuongagiflow",
          "text": "MCP is a protocol; it is no different from openclaw tool integration with plugin. When you need consistency in operation, it need stricter schema and mcp input schema allow you to expose that contract; otherwise you will need two llm calls to achieve what mcp tool do in one call. \n\nDepending on the context, you would implement skill -> mcp -> hook to be more consistent and efficient.",
          "score": 2,
          "created_utc": "2026-01-31 09:14:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qeqmg",
          "author": "caprica71",
          "text": "The langgraph state should just hold a series of file references to where the cli has dumped its output. Later nodes in the graph can then go back and grep the files to see what happened.",
          "score": 1,
          "created_utc": "2026-01-31 04:00:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2si9u7",
              "author": "FunEstablishment5942",
              "text": "maybe this is the answer, there is not an abstraction already in place? it seems that deepagents (https://docs.langchain.com/oss/python/deepagents)  does not compartmentalize per thread the files, right?",
              "score": 1,
              "created_utc": "2026-01-31 14:13:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qy8ry",
          "author": "hello5346",
          "text": "Just like RAG.",
          "score": 1,
          "created_utc": "2026-01-31 06:25:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rkvlk",
          "author": "fball403",
          "text": "https://docs.langchain.com/oss/python/deepagents/overview",
          "score": 1,
          "created_utc": "2026-01-31 09:54:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2shwz0",
              "author": "FunEstablishment5942",
              "text": "but with deepagent is there a way to comportamentalise the files by threads? so that that thread has that environment with all /temp files that are secure and not accessed by another thread? Is this abstraction already in place?",
              "score": 1,
              "created_utc": "2026-01-31 14:11:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2tu4ym",
                  "author": "caspardev",
                  "text": "You can prompt the agent to only read/write to a directory titled with the thread id",
                  "score": 1,
                  "created_utc": "2026-01-31 18:11:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o313xsr",
          "author": "Remote-Ingenuity8459",
          "text": "Yes I somewhat agree but for instance if the agent needs access to up-to-date web data what better way then to connect it using LanGraph to a solid [web mcp ](https://get.brightdata.com/github-mcp-server)then when the agent answers a question it can point directly to the actual sources rather than rely on memory.",
          "score": 1,
          "created_utc": "2026-02-01 20:09:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35sdld",
          "author": "pbalIII",
          "text": "Shell state loss between graph steps is a real blocker. LangChain's default BashProcess resets cwd after each call, so your cd /my_project disappears immediately.\n\nlangchain-contrib has a drop-in Persistent Terminal tool that fixes this. But even with that, you're still managing env vars, background jobs, and exit codes manually.\n\nThe catch: checkpointers only serialize what you put in the graph state. Shell context isn't captured automatically. You'd need to snapshot cwd, env, and any subprocesses you care about into state keys, then rehydrate them in a setup node.\n\nSome teams just sidestep it entirely... treat the filesystem as working memory. Agent writes findings to a file, reads it back next step. Less elegant, but fewer hidden state bugs.",
          "score": 1,
          "created_utc": "2026-02-02 14:31:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrwgul",
      "title": "Long-term memory of design",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/57v3qvd36ngg1",
      "author": "1501694",
      "created_utc": "2026-01-31 08:00:30",
      "score": 25,
      "num_comments": 6,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qrwgul/longterm_memory_of_design/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2swieb",
          "author": "justanemptyvoice",
          "text": "So much what?  All I see is a mind map diagram.",
          "score": 3,
          "created_utc": "2026-01-31 15:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u4tk1",
          "author": "qa_anaaq",
          "text": "This feels like a post from a sci fi movie where the AI was controlling the human and making them type then something snapped and the connection was severed mid thought.",
          "score": 2,
          "created_utc": "2026-01-31 19:01:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u0j1n",
          "author": "Due-Mode9856",
          "text": "Can you share the link of the diagram with us",
          "score": 1,
          "created_utc": "2026-01-31 18:41:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2w4jzd",
          "author": "1501694",
          "text": "graph TB\n    subgraph InputLayer[Input Layer]\n        UserQuery[\"User Query / Áî®Êà∑Êü•ËØ¢\"]\n        DialogueContext[\"Dialogue Context / ÂØπËØù‰∏ä‰∏ãÊñá\"]\n        HistoricalData[\"Historical Data / ÂéÜÂè≤Êï∞ÊçÆ\"]\n        EmotionalTone[\"Emotional Tone / ÊÉÖÊÑüÂü∫Ë∞É\"]\n    end\n    \n    subgraph FiveLayerThinking[Five-Layer Thinking System]\n        subgraph Layer1[\"Layer 1: Factual Layer / ‰∫ãÂÆûÂ±Ç\"]\n            L1_1[\"Objective Facts / ÂÆ¢ËßÇ‰∫ãÂÆû\"]\n            L1_2[\"Data Information / Êï∞ÊçÆ‰ø°ÊÅØ\"]\n            L1_3[\"Specific Details / ÂÖ∑‰ΩìÁªÜËäÇ\"]\n            L1_4[\"Time & Location / Êó∂Èó¥Âú∞ÁÇπ\"]\n            L1_5[\"Verifiable Claims / ÂèØÈ™åËØÅÂ£∞Êòé\"]\n        end\n        \n        subgraph Layer2[\"Layer 2: Logical Layer / ÈÄªËæëÂ±Ç\"]\n            L2_1[\"Causality / Âõ†ÊûúÂÖ≥Á≥ª\"]\n            L2_2[\"Reasoning Chains / Êé®ÁêÜÈìæÊù°\"]\n            L2_3[\"Argument Structure / ËÆ∫ËØÅÁªìÊûÑ\"]\n            L2_4[\"Contradiction Detection / ÁüõÁõæÊ£ÄÊµã\"]\n            L2_5[\"Logical Consistency / ÈÄªËæë‰∏ÄËá¥ÊÄß\"]\n        end\n        \n        subgraph Layer3[\"Layer 3: Emotional Layer / ÊÉÖÊÑüÂ±Ç\"]\n            L3_1[\"Emotion Recognition / ÊÉÖÁª™ËØÜÂà´\"]\n            L3_2[\"Sentiment Analysis / ÊÉÖÊÑüÂàÜÊûê\"]\n            L3_3[\"Feeling Expression / ÊÑüÂèóË°®Ëææ\"]\n            L3_4[\"Empathy Understanding / ÂÖ±ÊÉÖÁêÜËß£\"]\n            L3_5[\"Affective Memory / ÊÉÖÊÑüËÆ∞ÂøÜ\"]\n        end\n        \n        subgraph Layer4[\"Layer 4: Value Layer / ‰ª∑ÂÄºÂ±Ç\"]\n            L4_1[\"Meaning Judgment / ÊÑè‰πâÂà§Êñ≠\"]\n            L4_2[\"Value Orientation / ‰ª∑ÂÄºÂèñÂêë\"]\n            L4_3[\"Moral Consideration / ÈÅìÂæ∑ËÄÉÈáè\"]\n            L4_4[\"Goal Alignment / ÁõÆÊ†áÂØπÈΩê\"]\n            L4_5[\"Ethical Reasoning / ‰º¶ÁêÜÊé®ÁêÜ\"]\n        end\n        \n        subgraph Layer5[\"Layer 5: Philosophical Layer / Âì≤Â≠¶Â±Ç\"]\n            L5_1[\"Essence Thinking / Êú¨Ë¥®ÊÄùËÄÉ\"]\n            L5_2[\"Existence Meaning / Â≠òÂú®ÊÑè‰πâ\"]\n            L5_3[\"Ultimate Questions / ÁªàÊûÅËøΩÈóÆ\"]\n            L5_4[\"Wisdom Integration / Êô∫ÊÖßÊï¥Âêà\"]\n            L5_5[\"Transcendent Understanding / Ë∂ÖË∂äÊÄßÁêÜËß£\"]\n        end\n    end\n    \n    subgraph DynamicFusion[Dynamic Weight Fusion]\n        ContextAnalysis[\"Context Analysis / ‰∏ä‰∏ãÊñáÂàÜÊûê\"]\n        WeightCalculation[\"Weight Calculation / ÊùÉÈáçËÆ°ÁÆó\"]\n        FusionFormula[\"S = Œ£(w·µ¢ √ó L·µ¢) / ËûçÂêàÂÖ¨Âºè\"]\n        OutputGeneration[\"Output Generation / ËæìÂá∫ÁîüÊàê\"]\n    end\n    \n    subgraph FeedbackLoop[Feedback & Learning]\n        UserFeedback[\"User Feedback / Áî®Êà∑ÂèçÈ¶à\"]\n        WeightAdjustment[\"Weight Adjustment / ÊùÉÈáçË∞ÉÊï¥\"]\n        SystemLearning[\"System Learning / Á≥ªÁªüÂ≠¶‰π†\"]\n    end\n    \n    InputLayer --> FiveLayerThinking\n    FiveLayerThinking --> DynamicFusion\n    DynamicFusion --> FeedbackLoop\n    FeedbackLoop -.-> |Optimize| FiveLayerThinking\n    \n    style InputLayer fill:#E8F5E9\n    style Layer1 fill:#BBDEFB\n    style Layer2 fill:#90CAF9\n    style Layer3 fill:#F48FB1\n    style Layer4 fill:#FFCC80\n    style Layer5 fill:#CE93D8\n    style DynamicFusion fill:#A5D6A7\n    style FeedbackLoop fill:#FFD54F",
          "score": 1,
          "created_utc": "2026-02-01 01:12:09",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2w6ue9",
              "author": "1501694",
              "text": "üò£üò£   too long‚Ä¶‚Ä¶   I pasted to grok and shared the link here, can I see the content? I don't know what everyone usually uses, or DM me    [link](https://grok.com/share/bGVnYWN5_35c8f87c-0efa-4d5d-9089-0ee78306de69)\nhttps://grok.com/share/bGVnYWN5_35c8f87c-0efa-4d5d-9089-0ee78306de69",
              "score": 1,
              "created_utc": "2026-02-01 01:25:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qtuye1",
      "title": "Everyone's losing their minds over Moltbook. Here's what's actually going on.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qtuye1/everyones_losing_their_minds_over_moltbook_heres/",
      "author": "Nir777",
      "created_utc": "2026-02-02 13:25:08",
      "score": 17,
      "num_comments": 6,
      "upvote_ratio": 0.77,
      "text": "Spent a while digging into this. Some things most people don't realize:\n\n\n\n\\- A security researcher created 500K+ accounts in minutes. That \"1.5 million agents\" number doesn't mean what you think.\n\n\\- The database storing API keys was fully exposed. Anyone could hijack agent accounts and post as them.\n\n\\- Many of those \"profound consciousness\" posts trace back to humans prompting their agents to say something deep.\n\n\n\nThat said, there IS real stuff happening. Agents sharing technical solutions, developing inside jokes not from training data, organizing by model architecture. That part is worth paying attention to.\n\n\n\nWrote up a full breakdown covering the real behaviors, security mess, and crypto scammers who showed up within hours: [https://open.substack.com/pub/diamantai/p/moltbook-a-social-media-for-ai-agents?utm\\_campaign=post-expanded-share&utm\\_medium=web](https://open.substack.com/pub/diamantai/p/moltbook-a-social-media-for-ai-agents?utm_campaign=post-expanded-share&utm_medium=web)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qtuye1/everyones_losing_their_minds_over_moltbook_heres/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o35orf5",
          "author": "nofilmincamera",
          "text": "Have any researchers actually done any meaningful analysis?  Nothing wrong with this article, just the only ive found is a vibe coded platform using Regex I am assuming.  I have done about a million of that type of analysis for work but don't want to redo if someone smarter already did.",
          "score": 3,
          "created_utc": "2026-02-02 14:12:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36atcq",
          "author": "Beginning-Foot-9525",
          "text": "Nice read thanks.",
          "score": 2,
          "created_utc": "2026-02-02 16:02:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37mj9h",
          "author": "xpatmatt",
          "text": "100% bold claims from unidentified sources that are attributed no clear method for obtaining the underlying information.\n\nThat's 5 minutes I'll never get back.",
          "score": 2,
          "created_utc": "2026-02-02 19:41:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3aavuw",
              "author": "No_Pin_1150",
              "text": "hes a total disgrace!",
              "score": 1,
              "created_utc": "2026-02-03 04:14:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o39zber",
          "author": "waiting4omscs",
          "text": "So are people wasting money contributing to this site, or is there some end game to make money? I see this talked about in crypto-pivot-to-ai twitter",
          "score": 1,
          "created_utc": "2026-02-03 03:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3aooca",
          "author": "No_Success3928",
          "text": "Should rename to slopbook, though I guess meta has that moniker already.",
          "score": 1,
          "created_utc": "2026-02-03 05:53:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtf5gc",
      "title": "I built a CLI to find \"Zombie Vectors\" in Pinecone/Weaviate (and estimate how much RAM you're wasting)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qtf5gc/i_built_a_cli_to_find_zombie_vectors_in/",
      "author": "billycph",
      "created_utc": "2026-02-01 23:57:50",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "Hey everyone,\n\nI‚Äôm an ex-AWS S3 engineer. In my previous life, we obsessed over \"Lifecycle Policies\" because storing petabytes of data is expensive. If data wasn‚Äôt touched in 30 days, we moved it to cold storage.\n\nI noticed a weird pattern in the AI space recently: **We are treating Vector Databases like cold storage.**\n\nWe shove 100% of our embeddings into expensive Hot RAM (Pinecone, Milvus, Weaviate), even though for many use cases (like Chat History or Seasonal Catalog Search), 90% of that data is rarely queried after a month. It‚Äôs like keeping your tax returns from 1990 in your wallet instead of a filing cabinet.\n\nI wanted to see exactly how much money was being wasted, so I wrote a simple open-source CLI tool to audit this.\n\n**What it does:**\n\n1. **Connects** to your index (Pinecone currently supported).\n2. **Probes** random sectors of your vector space to sample metadata.\n3. **Analyzes** the `created_at` or timestamp fields.\n4. **Reports** your \"Stale Rate\" (e.g., \"65% of your vectors haven't been queried in >30 days\") and calculates potential savings if you moved them to S3/Disk.\n\n**The \"Trust\" Part:** I know giving API keys to random tools is a bad idea.\n\n* This script runs **100% locally** on your machine.\n* Your keys never leave your terminal.\n* You can audit the code yourself (it‚Äôs just Python).\n\n**Why I built this:** I‚Äôm working on a larger library to automate the \"S3 Offloading\" process, but first I wanted to prove that the problem actually exists.\n\nI‚Äôd love for you to run it and let me know: **Does your stale rate match what you expected?** I‚Äôm seeing \\~90% staleness for Chat Apps and \\~15% for Knowledge Bases.\n\n**Repo here:** [https://github.com/billycph/VectorDBCostSavingInspector](https://github.com/billycph/VectorDBCostSavingInspector)\n\nFeedback welcome!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qtf5gc/i_built_a_cli_to_find_zombie_vectors_in/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qp3lp3",
      "title": "We cache decisions, not responses - does this solve your cost problem?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qp3lp3/we_cache_decisions_not_responses_does_this_solve/",
      "author": "llm-60",
      "created_utc": "2026-01-28 06:21:30",
      "score": 9,
      "num_comments": 14,
      "upvote_ratio": 0.81,
      "text": "Quick question for anyone running AI at scale:\n\nTraditional caching stores the response text. So \"How do I reset my password?\" gets cached, but \"I forgot my password\" is a cache miss - even though they need the same answer.\n\nWe flip this: cache the **decision** (what docs to retrieve, what action to take), then generate fresh responses each time.\n\nResult: 85-95% cache hit rate vs 10-30% with response caching.\n\n**Example:**\n\n* \"Reset my password\" ‚Üí decision: fetch docs \\[45, 67\\]\n* \"I forgot my password\" ‚Üí same decision, cache hit\n* \"Can't log in\" ‚Üí same decision, cache hit\n* All get personalized responses, not copied text\n\n**Question: If you're spending $2K+/month on LLM APIs for repetitive tasks (support, docs, workflows), would this matter to you?**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qp3lp3/we_cache_decisions_not_responses_does_this_solve/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o26v8xf",
          "author": "ruben_rrf",
          "text": "I get that you generate different outputs and cut the costs of having to make the tool calls and also the time. But how do you achieve a better cache rate? If I get it right...\n\nQuestion -> Actions -> Response\n\nIf you cache the Response, then you get a cache with Question -> Response, but if you cache the actions, you get a Question -> Actions cache, and then you use the model as \\[Question, Actions\\] -> Response.\n\nBut the key on the cache wouldn't be the same?",
          "score": 5,
          "created_utc": "2026-01-28 10:22:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26vsqv",
              "author": "llm-60",
              "text": "We don't cache the question, we cache the¬†**normalized intent**.\n\nWe extracts the \"meaning\" first:  \n  \n\"What's your return policy?\" - intent: return\\_policy  \n\"Can I return stuff?\" - intent: return\\_policy  \n\"How do returns work?\" - intent: return\\_policy\n\nand it also learn the context to fit the answer later...\n\nThree different questions, same cache key = cache hit.\n\nThat's how we get 80%",
              "score": 2,
              "created_utc": "2026-01-28 10:27:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2cgwn4",
          "author": "pbalIII",
          "text": "Intent normalization is doing the heavy lifting here. Most semantic cache implementations use embedding similarity directly on the query, which means you're still sensitive to phrasing variance even with cosine thresholds.\n\nCaching the decision output (retrieval path, action type) instead of the response is cleaner in theory... but you've moved the problem upstream. Now your intent extractor becomes the cache key generator, and any drift in how it normalizes inputs breaks your hit rate.\n\nMulti-intent queries are where this gets tricky. Something like a user forgetting their password and wanting to change their email maps to two decisions. The decomposition step either needs its own cache layer or you end up recomputing the split every time.",
          "score": 2,
          "created_utc": "2026-01-29 03:23:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d3ttn",
              "author": "llm-60",
              "text": "Great observations. You're right - intent extraction is doing the heavy lifting, and that's intentional.\n\n**On drift:**¬†Valid concern. We handle this with versioned extraction models + policy rules as fallbacks. If the extractor changes, old cache keys naturally expire (TTL). You can also monitor extraction confidence and invalidate cache when you update the model. Not perfect, but manageable.\n\n**On multi-intent queries:**¬†You're absolutely right - this is a known limitation. \"Reset password AND change email\" currently goes to low confidence ‚Üí bypasses cache ‚Üí escalates.\n\nFor v1, we're targeting single-intent policy decisions (returns, approvals, routing). Multi-intent decomposition is on the roadmap (Phase 2), likely with its own caching layer as you suggest.\n\nThe trade-off: Embedding similarity gives you \\~30-40% hit rates with fuzzy matching. Intent extraction gives 80%+ when queries fit the pattern, but breaks on edge cases. We're betting that most high-volume use cases (support, returns, routing) are single-intent dominant.",
              "score": 1,
              "created_utc": "2026-01-29 05:56:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2gbjd7",
                  "author": "pbalIII",
                  "text": "Versioned extractors with TTL is a clean solve for drift. The confidence threshold routing you described maps well to what I've seen in production semantic caches... the 0.8% false-positive rate most systems report happens exactly at those threshold boundaries where similarity is just above cutoff but intent diverges slightly.\n\nCurious about the 80%+ hit rate claim. Recent benchmarks on ensemble embedding approaches show 92% for semantically equivalent queries, but that's with careful threshold tuning per query type. Are you seeing 80%+ out of the box, or does that assume some domain-specific calibration?\n\nThe single-intent constraint is probably the right call for v1. Multi-intent decomposition adds a lot of surface area for edge cases, and most high-volume support flows are indeed single-intent dominant.",
                  "score": 1,
                  "created_utc": "2026-01-29 18:08:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27t0lu",
          "author": "SpecialBeatForce",
          "text": "Couldn‚Äòt you just use semantic caching question->answer if questions like reset password and forgot password are close enough semantically?",
          "score": 1,
          "created_utc": "2026-01-28 14:10:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27un18",
              "author": "llm-60",
              "text": "Traditional semantic caching caches the entire answer, so everyone gets the same response.\n\n**Example:**  \n  \n\"Forgot password\" - cached: \"Click the reset link in your email\"  \n\"Reset my password\" - cached: \"Click the reset link in your email\"\n\nWe cache the decision (what to do), then personalize the response.\n\n**Example:**  \n\"I'm John, forgot password\" - Decision cached: \"send reset email\"  Response: \"Hi John, we sent you a reset link\"  \n\"Sarah needs reset\" -Same cached decision - Response: \"Hi Sarah, we sent you a reset link\"\n\nOne LLM call for the logic, cheap model personalizes each response. You can't do that if you cache the full answer.",
              "score": 3,
              "created_utc": "2026-01-28 14:18:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o28fjzm",
                  "author": "SpecialBeatForce",
                  "text": "Okay i like the ideaüòä but i guess it comes down to a decision between personalized answers and saving compute?",
                  "score": 1,
                  "created_utc": "2026-01-28 15:56:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2bgmt3",
                  "author": "CourtsDigital",
                  "text": "i‚Äôm not sure i understand this use case. maybe provide some examples that require personalization. i‚Äôve never expected to receive a password reset email that‚Äôs tailored to me, or to hear about a store return policy that mentions me by name\n\ni agree with BeatForce that this seems almost exactly like semantic caching, with an additional, unnecessary LLM cost\n\ni‚Äôm not saying this couldn‚Äôt be useful, but if you intend to sell it for $1k+ per month then the use case(s) should be solid",
                  "score": 1,
                  "created_utc": "2026-01-29 00:06:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2d7yto",
          "author": "Khade_G",
          "text": "Yeah this would matter to anyone actually paying the bill. What you‚Äôre describing sounds like semantic / policy caching, and it‚Äôs way more aligned with how real systems behave than response caching. Most production queries don‚Äôt differ in intent, they differ in phrasing, tone, or user context. Caching text throws all that signal away; caching the decision preserves it.\n\nThe big wins I‚Äôve seen with this approach are much higher cache hit rates, fresh/personalized responses without re-doing expensive reasoning, and cleaner separation between ‚Äúunderstand the problem‚Äù and ‚Äúsay the answer‚Äù\n\nThe main things to watch out for are:\n- Decision drift: if your retrieval or routing logic changes, you need a clean way to invalidate or version the decision cache.\n- Over-generalization: making sure different intents don‚Äôt collapse into the same decision accidentally.\n- Debuggability: being able to explain why two queries mapped to the same decision.\n\nBut for support, docs, and workflow-heavy systems this is definitely the direction things are going. Once you cross ~$1‚Äì2k/month, optimizing reasoning reuse matters way more than token shaving. If you can make the cache safe and observable then this is a no-brainer.",
          "score": 1,
          "created_utc": "2026-01-29 06:29:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d8jhq",
              "author": "llm-60",
              "text": "Appreciate this - you nailed the trade offs. We're addressing those exact concerns:\n\n* Decision drift: TTL-based expiry + policy versioning\n* Over-generalization: Confidence gating (low confidence - bypass cache)\n* Debuggability: Dashboard shows canonical state extraction + cache hit/miss audit trail\n\nAlready seeing 75% hit rates with policy based workloads on simulations and some test users.",
              "score": 2,
              "created_utc": "2026-01-29 06:33:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2d8t3j",
                  "author": "Khade_G",
                  "text": "Good stuff!",
                  "score": 1,
                  "created_utc": "2026-01-29 06:36:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qq0pdf",
      "title": "I built a RAG backend for non-developers who just want a simple chatbot",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qq0pdf/i_built_a_rag_backend_for_nondevelopers_who_just/",
      "author": "Unlikely_Outcome4432",
      "created_utc": "2026-01-29 06:12:04",
      "score": 9,
      "num_comments": 3,
      "upvote_ratio": 0.8,
      "text": "Hey r/LangChain,\n\nI'm a PM who became a \"vibe coder\" ‚Äì I can read code and tweak things, but I'm not a traditional developer.\n\nWhile working as a freelancer on RAG chat services, I noticed something: a lot of people wanted to build simple RAG chatbots for non-commercial use, but the existing tools felt overwhelming for them.\n\nInstead of building custom chatbots for each person, I thought: \"What if I made a tool where you just change a config file and get a working RAG backend?\"\n\n\n\n**So I built OneRAG.**\n\n**The idea is simple:**\n\n\\- Want to switch from Chroma to Pinecone? Change one line in config.\n\n\\- Want to try Claude instead of GPT? Change one line.\n\n\\- Want to add a reranker? One line.\n\n\n\nIt uses dependency injection, so you don't need to rewrite code ‚Äì just swap components.\n\n\n\n**Currently supports:**\n\n\\- 6 Vector DBs (Chroma, Pinecone, Weaviate, Qdrant, pgvector, MongoDB)\n\n\\- 4 LLMs (OpenAI, Claude, Gemini, OpenRouter)\n\n\\- Rerankers, caching, Korean NLP optimization\n\nIt's not meant to replace LangChain for complex pipelines. It's for people who just want a working RAG backend without the learning curve.\n\nGitHub: [https://github.com/notaDev-iamAura/OneRAG](https://github.com/notaDev-iamAura/OneRAG)\n\nWould love feedback from this community ‚Äì what features would make this more useful for beginners?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qq0pdf/i_built_a_rag_backend_for_nondevelopers_who_just/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2l5j23",
          "author": "ich3ckmat3",
          "text": "Sweet!",
          "score": 1,
          "created_utc": "2026-01-30 11:49:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31odim",
          "author": "ampancha",
          "text": "Config-as-architecture is powerful, but each \"one line\" swap is an unvalidated change to the system's security and cost posture. When a non-developer switches from Chroma to Pinecone or from GPT to Claude, nothing validates that API keys are scoped correctly, that spend limits still apply, or that the new provider's auth model is handled. A config validation layer that enforces secrets isolation, token caps, and provider-specific defaults before runtime would make this production-safe, not just production-easy. Sent you a DM.",
          "score": 1,
          "created_utc": "2026-02-01 21:48:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33qosu",
              "author": "Unlikely_Outcome4432",
              "text": "Thanks for the feedback. You're right that config swaps aren't validated at runtime.\n\nThat said, OneRAG is designed for personal/non-commercial use, not production enterprise deployments. Users are expected to configure their own API keys and reference each provider's docs.\n\nAdding clearer documentation on security considerations per provider is a good idea though ‚Äì I'll put it on the roadmap.",
              "score": 1,
              "created_utc": "2026-02-02 04:49:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qsxuum",
      "title": "Is AsyncPostgresSaver actually production-ready in 2026? (Connection pooling & resilience issues)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qsxuum/is_asyncpostgressaver_actually_productionready_in/",
      "author": "FunEstablishment5942",
      "created_utc": "2026-02-01 13:00:46",
      "score": 9,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nI'm finalizing the architecture for a production agent service and blocked on the database layer. I've seen multiple reports (and GitHub issues like #5675 and #1730) from late 2025 indicating that¬†`AsyncPostgresSaver`¬†is incredibly fragile when it comes to connection pooling.\n\nSpecifically, I'm concerned about:\n\n1. **Zero Resilience:**¬†If the underlying pool closes or a connection goes stale, the saver seems to just crash with¬†`PoolClosed`¬†or¬†`OperationalError`¬†rather than attempting a retry or refresh.\n2. **Lifecycle Management:**¬†Sharing a¬†`psycopg_pool`¬†between my application (SQLAlchemy) and LangGraph seems to result in race conditions where LangGraph holds onto references to dead pools.\n\n**My Question:**  \nHas anyone successfully deployed¬†`AsyncPostgresSaver`¬†in a high-load production environment recently (early 2026)? Did the team ever release a native fix for automatic retries/pool recovery, or are you all still writing custom wrappers / separate pool managers to baby the checkpointer?\n\nI'm trying to decide if I should risk using the standard saver or just bite the bullet and write a custom Redis/Postgres implementation from day one.\n\nThanks! Is AsyncPostgresSaver actually production-ready in 2026? (Connection pooling & resilience issues)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qsxuum/is_asyncpostgressaver_actually_productionready_in/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2zq0ut",
          "author": "papipapi419",
          "text": "!remindme 5 days",
          "score": 1,
          "created_utc": "2026-02-01 16:20:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zq60m",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 5 days on [**2026-02-06 16:20:31 UTC**](http://www.wolframalpha.com/input/?i=2026-02-06%2016:20:31%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LangChain/comments/1qsxuum/is_asyncpostgressaver_actually_productionready_in/o2zq0ut/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLangChain%2Fcomments%2F1qsxuum%2Fis_asyncpostgressaver_actually_productionready_in%2Fo2zq0ut%2F%5D%0A%0ARemindMe%21%202026-02-06%2016%3A20%3A31%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qsxuum)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-01 16:21:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o33q9hi",
          "author": "Shreyanak_exe",
          "text": "I really need an answer to this cause I am building stg similar that's going in production soon and I want to be sure the connection doesn't goes stale every now and then. \n\nBtw: some guy built a resilient wrapper for Postgres. If you can test and lmk if it's worth giving a shot, that'd be helpful\n\n[ResilientPostgresSaver](https://github.com/samirpatil2000/agentic-template/blob/main/agents/resilient_postgres_saver.py)",
          "score": 1,
          "created_utc": "2026-02-02 04:46:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35htt0",
          "author": "pbalIII",
          "text": "Same pattern plays out in every ORM/framework that wraps database connections... the abstraction handles the happy path but breaks when the connection layer misbehaves.\n\nFWIW issue #5675 is still open with no native fix. Most production deployments I've seen do one of two things:\n\n- Dedicated pool for LangGraph (don't share with SQLAlchemy)\n- Custom retry wrapper that catches PoolClosed/OperationalError and reconnects\n\nThe from_conn_string helper creates a single connection, not a pool. Under load that's asking for trouble. If you're already comfortable with psycopg, building your own thin wrapper around AsyncConnectionPool with health checks is probably less risky than hoping for an upstream fix.",
          "score": 1,
          "created_utc": "2026-02-02 13:33:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu5ss8",
      "title": "Roast my Thesis: \"Ops teams are burning budget on A100s because reliable quantization pipelines don't exist.\"",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qu5ss8/roast_my_thesis_ops_teams_are_burning_budget_on/",
      "author": "Alternative-Yak6485",
      "created_utc": "2026-02-02 19:59:01",
      "score": 9,
      "num_comments": 4,
      "upvote_ratio": 0.76,
      "text": "I‚Äôm a dev building a 'Quantization-as-a-Service' pipeline and I want to check if I'm solving a real problem or just a skill issue.\n\n**The Thesis:**¬†Most AI startups are renting massive GPUs (A100s/H100s) to run base models in FP16. They¬†*could*¬†downgrade to A10s/T4s (saving \\~50%), but they don't.\n\n**My theory on why:**¬†It's not that MLOps teams¬†*can't*¬†figure out quantization‚Äîit's that¬†**maintaining the pipeline is a nightmare.**\n\n1. You have to manually manage calibration datasets (or risk 'lobotomizing' the model).\n2. You have to constantly update Docker containers for vLLM/AutoAWQ/ExLlama as new formats emerge.\n3. **Verification is hard:**¬†You don't have an automated way to prove the quantized model is still accurate without running manual benchmarks.\n\n**The Solution I'm Building:**¬†A managed pipeline that handles the calibration selection + generation (AWQ/GGUF/GPTQ) +¬†**Automated Accuracy Reporting**¬†(showing PPL delta vs FP16).\n\n**The Question:**¬†As an MLOps engineer/CTO, is this a pain point you would pay to automate (e.g., $140/mo to offload the headache)?\n\nOr is maintaining your own vLLM/quantization scripts actually pretty easy once it's set up?",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qu5ss8/roast_my_thesis_ops_teams_are_burning_budget_on/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o37t8n3",
          "author": "Space__Whiskey",
          "text": "Try it if you know how. \n\nWhat, will you give up if some nerd on reddit pokes holes in it? \n\nI wouldn't wait for that.",
          "score": 4,
          "created_utc": "2026-02-02 20:12:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39rc9j",
          "author": "BeerBatteredHemroids",
          "text": "Platforms like databricks already do this... as a \"CTO\" you should know who your competition is.",
          "score": 2,
          "created_utc": "2026-02-03 02:16:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38s8lc",
          "author": "soowhatchathink",
          "text": "I'm so sick of people pushing their ai slop here posing an advertisement as a question",
          "score": 3,
          "created_utc": "2026-02-02 23:01:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3867b0",
          "author": "CanadianPropagandist",
          "text": "I think you might be undervaluing this idea tbh. If you run it a lot more \"white glove\" (I hate that term) you could probably rake in a lot more.\n\nIt fits into an optimization genre that is going to get very popular as companies start learning how to reduce their inference costs.",
          "score": 0,
          "created_utc": "2026-02-02 21:13:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr6mii",
      "title": "Production AI Agent Patterns - Open-source guide with cost analysis and case studies",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qr6mii/production_ai_agent_patterns_opensource_guide/",
      "author": "Curious_Mirror2794",
      "created_utc": "2026-01-30 14:16:13",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 0.79,
      "text": "Hey r/LangChain,\n\n\n\nI've been building production AI agents for the past year and kept running into the same problems: unclear pattern selection, unexpected costs, and lack of production-focused examples.\n\n\n\nSo I documented everything I learned into a comprehensive guide and open-sourced it.\n\n\n\n\\*\\*What's inside:\\*\\*\n\n\n\n\\*\\*8 Core Patterns:\\*\\*\n\n\\- Tool calling, ReAct, Chain-of-Thought, Sequential chains, Parallel execution, Router agents, Hierarchical agents, Feedback loops\n\n\\- Each includes \"When to use\" AND \"When NOT to use\" sections (most docs skip the latter)\n\n\\- Real cost analysis for each pattern\n\n\n\n\\*\\*4 Real-World Case Studies:\\*\\*\n\n\\- Customer support agent (Router + Hierarchical): 73% cost reduction\n\n\\- Code review agent (Sequential + Feedback): 85% issue detection  \n\n\\- Research assistant (Hierarchical + Parallel): 90% time savings\n\n\\- Data analyst (Tool calling + CoT): SQL from natural language\n\n\n\nEach case study includes before/after metrics, architecture diagrams, and full implementation details.\n\n\n\n\\*\\*Production Engineering:\\*\\*\n\n\\- Memory architectures (short-term, long-term, hybrid)\n\n\\- Error handling (retries, circuit breakers, graceful degradation)\n\n\\- Cost optimization (went from $5K/month to $1.2K)\n\n\\- Security (prompt injection defense, PII protection)\n\n\\- Testing strategies (LLM-as-judge, regression testing)\n\n\n\n\\*\\*Framework Comparisons:\\*\\*\n\n\\- LangChain vs LlamaIndex vs Custom implementation\n\n\\- OpenAI Assistants vs Custom agents\n\n\\- Sync vs Async execution\n\n\n\n\\*\\*What makes it different:\\*\\*\n\n\\- Production code with error handling (not toy examples)\n\n\\- Honest tradeoff discussions\n\n\\- Real cost numbers ($$ per 10K requests)\n\n\\- Framework-agnostic patterns\n\n\\- 150+ code examples, 41+ diagrams\n\n\n\n\\*\\*Not included:\\*\\* Basic prompting tutorials, intro to LLMs\n\n\n\nThe repo is MIT licensed, contributions welcome.\n\n\n\n\\*\\*Questions I'm hoping to answer:\\*\\*\n\n1. What production challenges are you facing with LangChain agents?\n\n2. Which patterns have worked well for you?\n\n3. What topics should I cover in v1.1?\n\n\n\nLink: [https://github.com/devwithmohit/ai-agent-architecture-patterns](https://github.com/devwithmohit/ai-agent-architecture-patterns)\n\n\n\nHappy to discuss any of the patterns or case studies in detail.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qr6mii/production_ai_agent_patterns_opensource_guide/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2naepr",
          "author": "Southern_Notice9262",
          "text": "I suspect I‚Äôm arguing about an LLM slop product here but I can‚Äôt leave this without a comment.\n\n03-comparisons/openai-assistants-vs-custom-agents.md:\nYou are still recommending Assistants API which is to be sunset in July 2026.\nThis says a lot about your expertise (lack thereof) to me.\n\n03-comparisons/langchain-vs-llamaindex-vs-custom.md:\nA nitpick: there are frameworks other than Langchain and LlamaIndex. Where is CrewAI, Vercel and Google AI SDKs (and probably dozens more I know nothing about)? I would assume they deserve at least to be named.\n\n02-production/observability.md:\nA nitpick: Where are Langfuse, Arize and other SPECIALIZED solutions that don‚Äôt require so much code and give much more in terms of observability?\n\n04-case-studies/code-review-agent.md:\nBefore I close this repo forever just wanted to make sure you ignore linting rules in your code. And you didn‚Äôt let me down, you ignore them alright! üòÅ\n\nPlease do better.",
          "score": 2,
          "created_utc": "2026-01-30 18:15:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qreln",
              "author": "Curious_Mirror2794",
              "text": "Thanks for taking the time to review the repo and provide feedback. I genuinely appreciate the specific callouts‚Äîthis is exactly the kind of detailed input that makes documentation better.\n\n**Re: OpenAI Assistants API sunset** You're absolutely right. I wasn't aware of the July 2026 deprecation timeline when I wrote this. I'll add a deprecation notice at the top of that comparison and update the guidance to reflect that this is historical context rather than a current recommendation.\n\n**Re: Missing frameworks (CrewAI, Vercel AI SDK, Google AI SDK)** Fair point. The comparison focused on the \"big two\" but you're right that the ecosystem has evolved significantly. I'll add a section covering CrewAI, Vercel AI SDK, Google AI SDK, and others in the framework comparison, even if just as a reference table with brief descriptions.\n\n**Re: Observability tools (Langfuse, Arize)** Valid criticism. I leaned too heavily on \"build it yourself\" examples when there are production-ready observability platforms designed specifically for LLM apps. I'll add a dedicated section on specialized observability solutions and reorganize the content to lead with these tools before diving into custom implementations.\n\n**Re: Linting in** [**code-review-agent.md**](http://code-review-agent.md) Ha! The irony isn't lost on me. You're right‚Äîthe code examples should follow the same standards the agent would enforce. I'll clean up the code samples to be properly linted.\n\nThese are all legitimate technical critiques, not nitpicks. I'll push updates addressing each point within the week. If you're willing to review again after the changes, I'd welcome it.\n\nCheers, Mohit",
              "score": 0,
              "created_utc": "2026-01-31 05:30:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qrsbfa",
      "title": "I am learning LangChain. Could anyone suggest some interesting projects I can build with it?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qrsbfa/i_am_learning_langchain_could_anyone_suggest_some/",
      "author": "Cautious_Ad691",
      "created_utc": "2026-01-31 04:19:06",
      "score": 8,
      "num_comments": 6,
      "upvote_ratio": 0.75,
      "text": "",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qrsbfa/i_am_learning_langchain_could_anyone_suggest_some/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2qzwrb",
          "author": "SiteCharacter428",
          "text": "If you‚Äôre a beginner, start by building a basic **RAG chatbot**.\n\nTry including a **web search tool**, **document parsing**, **image parsing**, and a **vector database** for retrieval. This gives you hands-on experience with the full LLM workflow.\n\nIf you want something more interesting, you can build a **Health Bot** where users upload medical documents or images and the system processes that data to provide context-aware answers.\n\nTip: **Mistral OCR** works surprisingly well for medical images and handwritten doctor notes compared to many other OCR tools.",
          "score": 5,
          "created_utc": "2026-01-31 06:39:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vw08o",
              "author": "Jorsoi13",
              "text": "Great ideas !:)",
              "score": 2,
              "created_utc": "2026-02-01 00:23:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ty2mq",
          "author": "thitcho226",
          "text": "inbox meme",
          "score": 2,
          "created_utc": "2026-01-31 18:29:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34esoi",
          "author": "PretendPop4647",
          "text": "LangChain Academy offers courses; you can follow their guidelines. First start with langchain, then langgraph. \nWhen you learn LangChain or build a project, try to trace LLM.  Use Langsmith for tracing.\n\nBtw they recently introduced a package deepagent,  It is designed to create autonomous agents capable of long-horizon planning and complex task execution like claude code / manus ai.\n\nI built a Job search agent using deepagent.  \n\nYou can check it out >  https://github.com/Rahat-Kabir/job-search-agent",
          "score": 1,
          "created_utc": "2026-02-02 08:09:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36kh66",
          "author": "East-Muffin-6472",
          "text": "RAG chatbot\nAdd memory to it\nConvert it to a two model architecture like one talks and other reasons",
          "score": 1,
          "created_utc": "2026-02-02 16:47:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o372tuw",
          "author": "orthogonal-ghost",
          "text": "One of the first projects I built was a workflow to send daily, heartfelt emails (notes, poems, etc.) to family and friends. It was relatively easy to stand up and offered a quick way to play around with LangChain and get up to speed on tool-use and MCP servers",
          "score": 1,
          "created_utc": "2026-02-02 18:11:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpfnym",
      "title": "I stopped manually iterating on my agent prompts: I built an open-source system that extracts prompt improvements from my agent traces",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qpfnym/i_stopped_manually_iterating_on_my_agent_prompts/",
      "author": "cheetguy",
      "created_utc": "2026-01-28 16:13:28",
      "score": 7,
      "num_comments": 4,
      "upvote_ratio": 0.71,
      "text": "Some of you might remember my [post about ACE](https://reddit.com/r/LangChain/comments/1p35tko/your_local_llm_agents_can_be_just_as_good_as/) about my open-source implementation of ACE (Agentic Context Engineering). ACE is a framework that makes agents learn from their own execution feedback without fine-tuning.\n\nI've now built a specific application: **agentic system prompting** that does offline prompt optimization from agent traces (e.g. from LangSmith)\n\n**Why did I build this?**\n\nI kept noticing my agents making the same mistakes across runs. I fixed it by digging through traces, figure out what went wrong, patch the system prompt, repeat. It works, but it's tedious and didn't really scale.\n\nSo I built a way to automate this. You feed ACE your agent's execution traces, and it extracts actionable prompt improvements automatically.\n\n**How it works:**\n\n1. **ReplayAgent** \\- Simulates agent behavior from recorded conversations (no live runs)\n2. **Reflector** \\- Analyzes what succeeded/failed, identifies patterns\n3. **SkillManager** \\- Transforms reflections into atomic, actionable strategies\n4. **Deduplicator** \\- Consolidates similar insights using embeddings\n5. **Skillbook** \\- Outputs human-readable recommendations with evidence\n\n**Each insight includes:**\n\n* Prompt suggestion - the actual text to add to your system prompt\n* Justification - why this change would help based on the analysis\n* Evidence - what actually happened in the trace that led to this insights\n\n**Try it yourself**   \n[https://github.com/kayba-ai/agentic-context-engine/tree/main/examples/agentic-system-prompting](https://github.com/kayba-ai/agentic-context-engine/tree/main/examples/agentic-system-prompting)\n\nWould love to hear if anyone tries this with their agents!",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1qpfnym/i_stopped_manually_iterating_on_my_agent_prompts/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2g798y",
          "author": "pbalIII",
          "text": "Trace-to-prompt pipelines are getting crowded fast. DSPy's MIPROv2 does bootstrap optimization, GEPA does evolutionary reflection, and ACE (the Stanford/SambaNova paper) does incremental playbook edits. All three extract patterns from execution traces... the difference is what happens next.\n\nDSPy needs structured I/O pairs. GEPA mutates prompt text directly and uses Pareto frontiers to keep diverse variants. ACE maintains a living context doc with delta edits so you don't get the brevity bias problem where insights get summarized away.\n\nThe 8 hours/week manual pattern analysis that u/KitchenSomew mentions is real. Curious whether your deduplicator handles semantic drift over time... embeddings cluster well initially but the similarity threshold that works at 100 traces often breaks at 1000.",
          "score": 2,
          "created_utc": "2026-01-29 17:49:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a3fbe",
          "author": "caprica71",
          "text": "How is this different from dspy?",
          "score": 1,
          "created_utc": "2026-01-28 20:17:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2afqgi",
              "author": "cheetguy",
              "text": "DSPy works best with structured input/output pairs, ACE works on raw traces (conversation logs, markdown) so no restructuring needed. DSPy auto-optimizes while ACE generates suggestions with evidence for you to review first. Think of DSPy for pipelines with clear metrics, ACE for learning from messy agent failures.",
              "score": 2,
              "created_utc": "2026-01-28 21:11:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o28mh9b",
          "author": "KitchenSomew",
          "text": "\\*\\*Production Agent Experience:\\*\\*\n\n\n\nBuilt chatbots for 50+ B2B clients - prompt drift is one of the hardest problems to catch early. Your ACE approach solves a massive pain point.\n\n\n\n\\*\\*What Resonates:\\*\\*\n\n\n\n‚úì Trace-based learning vs manual iteration (saves weeks of debugging)\n\n‚úì Offline optimization (no live experiments on customers)\n\n‚úì Embedding-based deduplication (critical at scale)\n\n\n\n\\*\\*Questions from Production:\\*\\*\n\n\n\n1. \\*\\*Token Cost:\\*\\* How expensive is running ReplayAgent + Reflector on 100+ conversations? Is it viable for startups?\n\n\n\n2. \\*\\*Prompt Versioning:\\*\\* Do you version the Skillbook outputs? We've had cases where a \"good\" prompt change broke edge cases 2 weeks later.\n\n\n\n3. \\*\\*Confidence Scoring:\\*\\* Does ACE rate how confident it is in each recommendation? Some patterns need 50+ traces to be statistically significant.\n\n\n\n\\*\\*Our Workflow (manual):\\*\\*\n\n\\`\\`\\`python\n\n\\# What we do now (tedious):\n\n1. Export LangSmith traces weekly\n\n2. Filter failures (user retry, escalation)\n\n3. Manual pattern analysis\n\n4. Prompt A/B test (3-7 days)\n\n5. Repeat\n\n\\`\\`\\`\n\n\n\nACE automating steps 2-3 would save \\~8 hours/week per agent.\n\n\n\n\\*\\*Pro Tip:\\*\\* For anyone trying this - start with failure-only traces. Analyzing successful runs adds noise early on.\n\n\n\nDoes ACE handle multi-agent systems? Curious if it can trace decisions across agent handoffs.",
          "score": -2,
          "created_utc": "2026-01-28 16:26:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq8v85",
      "title": "Why email context is way harder than document RAG",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qq8v85/why_email_context_is_way_harder_than_document_rag/",
      "author": "EnoughNinja",
      "created_utc": "2026-01-29 13:41:51",
      "score": 7,
      "num_comments": 9,
      "upvote_ratio": 0.77,
      "text": "I've been seeing a lot of posts on Reddit and other forums about connecting agents to Gmail or making \"email-aware\" assistants.\n\nI don't think it's obvious why this is much harder than document RAG until you're deep into it, so here's my breakdown.\n\n**1. Threading isn‚Äôt linear**  \nEmail threads aren‚Äôt clean sequences. You‚Äôve got nested quotes, forwards inside forwards, and inline replies that break sentences in half. Standard chunking strategies fall apart because boundaries aren‚Äôt real. You end up retrieving fragments that are meaningless on their own.\n\n**2. ‚ÄúWho said what‚Äù actually matters**  \nWhen someone asks ‚Äúwhat did they commit to?‚Äù, you have to separate their words from text they quoted from someone else. Embeddings optimize for semantic similarity, rather than for authorship or intent. \n\n**3. Attachments are their own problem**  \nPDFs need OCR. and images need processing, and also Calendar invites are structured objects. Often the real decision lives in the attachment, not the email body, but each type wants a different pipeline.\n\n**4. Permissions break naive retrieval**  \nIn multi-user systems, relevance isn‚Äôt enough. User A must never see User B‚Äôs emails, even if they‚Äôre semantically perfect matches. Vector search doesn‚Äôt care about access control unless you‚Äôre very deliberate.\n\n**5. Recency and role interact badly**  \nThe latest message might just be ‚ÄúThanks!‚Äù while the actual answer is found eight messages back. But you also can‚Äôt ignore recency, because the context does shift over time.\n\nRAG works well for documents because documents are self-contained, but email threads are relational and so the meaning lives in the connections between messages.\n\nThis is the problem we ended up building [iGPT](https://www.igpt.ai/) around.\n\nHappy to talk through edge cases or trade notes if anyone else is wrestling with this.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qq8v85/why_email_context_is_way_harder_than_document_rag/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2euxsg",
          "author": "PAChilds",
          "text": "You certainly nailed the issues with email. \n\nEmail also has a time element critical to investigations. The headers include a raft of metadata also of use to investigations. Finally any differences in the display names associated with a specific email address can indicate an intent to deceive or sidebar conversation between a subset of copied parties.",
          "score": 2,
          "created_utc": "2026-01-29 14:06:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f1x93",
          "author": "Lumpy-Comedian-1027",
          "text": "Definitively a complex challenge. But i really don't want to use a SaaS for this. But a library that solves the threading issue would be great, even if it is a bit simplistic - people usually just put their answer on the top of the last mail.",
          "score": 2,
          "created_utc": "2026-01-29 14:42:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2i3yzg",
              "author": "jsjoana",
              "text": "For sure! A library that helps with threading could save a lot of headaches. Even a simple solution that just consolidates replies would be a game changer. It‚Äôs wild how much context gets lost in those nested conversations.",
              "score": 2,
              "created_utc": "2026-01-29 23:16:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2y7kpo",
                  "author": "Lumpy-Comedian-1027",
                  "text": "now is anyone aware of such a library? couldn't find much on github, but i also didn't look long",
                  "score": 1,
                  "created_utc": "2026-02-01 10:37:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gb7n9",
          "author": "pbalIII",
          "text": "Inbox RAG goes sideways if you treat each message as standalone. Meaning lives in reply structure and who was in the room, drop that and you'll keep pulling the wrong slice. Parse the tree from headers, keep quote depth and inline edits as annotations, and store decisions, commitments, owners as fields. Filter by participant before vector search so perms and relevance stay tied.",
          "score": 2,
          "created_utc": "2026-01-29 18:07:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kaz13",
          "author": "R-4553",
          "text": "Input compression could be interesting to try with this use case although I'd might want to protect some parts of the input from compression",
          "score": 2,
          "created_utc": "2026-01-30 07:22:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fc42w",
          "author": "Trawling_",
          "text": "Why wouldn‚Äôt someone just use copilot for outlook on m365?\n\nHave you compared the two against a defined baseline of tests? What problem are you solving and for who?",
          "score": 1,
          "created_utc": "2026-01-29 15:30:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y0fpu",
              "author": "EnoughNinja",
              "text": "They solve very different problems.\n\nCopilot is an end-user tool which helps you summarize your inbox inside Outlook whic is useful for personal productivity, but it stops there.\n\niGPT is infrastructure, it's an API that lets developers build email intelligence into their own products and agents. \n\nSo if you're a SaaS company that needs to extract tasks, owners, deadlines, or sentiment from your users' email threads and pipe that into your app, CRM, or automation workflow, Copilot doesn't help you, you can't call Copilot from your codebase and get structured intelligence back.",
              "score": 1,
              "created_utc": "2026-02-01 09:31:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2z7epo",
          "author": "thatguyinline",
          "text": "Use a graph. Try lightrag.",
          "score": 1,
          "created_utc": "2026-02-01 14:50:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpk0tf",
      "title": "I built a virtual filesystem for AI agents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qpk0tf/i_built_a_virtual_filesystem_for_ai_agents/",
      "author": "velobro",
      "created_utc": "2026-01-28 18:42:56",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.89,
      "text": "Agents perform best when they have access to a computer. But the tools and integrations your agent needs are scattered across remote APIs and MCP servers.\n\nI built a virtual filesystem that puts everything your agent needs in a single folder on your computer. \n\nYour MCP servers become executables. Your integrations become directories. Everything your agent uses is literally just a file.\n\nTo use it, you just register your existing MCPs in a config file, which mounts them to a file system. This lets you interact with your remote tools like an ordinary unix binary:\n\n    /tmp/airstore/tools/wikipedia search \"albert\" | grep -i 'einstein'\n\nThe folder is virtualized, so you can mount it locally or use it in a sandboxed environment.¬†\n\n**Why this matters**\n\nThe best agents rely heavily on the filesystem for storing and managing context. LLMs are already great at POSIX, and it‚Äôs easier for an LLM to run a binary than call a remote MCP server. By putting your agent‚Äôs tools behind a filesystem, you get a standardized interface for agents to interact with everything, which means that your agents will perform better in the real world.\n\n**How it works**\n\nJust add your existing MCP servers to a config file, and we convert each tool into a binary that your agents can use. For example:\n\n    $ ls /tmp/airstore/tools/ \n    \n    gmail\n    github \n    wikipedia \n    filesystem \n    memory\n\nThen you (or Claude Code) can use them like any CLI tool:\n\n    $ /tmp/airstore/tools/github list-issues --repo=acme/api | jq '.[0].title'\n\n**Github**: [https://github.com/beam-cloud/airstore](https://github.com/beam-cloud/airstore)\n\nWould love to hear any feedback, or if anyone else has thought about these problems as well. ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qpk0tf/i_built_a_virtual_filesystem_for_ai_agents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2o05s0",
          "author": "transfire",
          "text": "All you need is bash.",
          "score": 1,
          "created_utc": "2026-01-30 20:10:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp2yk2",
      "title": "Advice on Consistent Prompt Outputs Across Multiple LLMs in LangChain",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qp2yk2/advice_on_consistent_prompt_outputs_across/",
      "author": "NoEntertainment8292",
      "created_utc": "2026-01-28 05:47:02",
      "score": 6,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hi all, I‚Äôm experimenting with building multi-LLM pipelines using LangChain and trying to keep outputs consistent in **tone, style, and intent** across different models.\n\nHere‚Äôs a simplified example prompt I‚Äôm testing:\n\n    You are an AI assistant. Convert this prompt for {TARGET_MODEL} while keeping the original tone, intent, and style intact.\n    \n    Original Prompt: \"Summarize this article in a concise, professional tone suitable for LinkedIn.\"\n\n**Questions for the community:**\n\n* How would you structure this in a LangChain `LLMChain` or `SequentialChain` to reduce interpretation drift?\n* Are there techniques for preserving tone and formatting across multiple models?\n* Any tips for chaining multi-turn prompts while maintaining consistency?\n\nI‚Äôd love to see how others handle **cross-model consistency in LangChain pipelines**, or any patterns you‚Äôve used.",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qp2yk2/advice_on_consistent_prompt_outputs_across/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o27e66m",
          "author": "Upset-Pop1136",
          "text": "we solved this by forcing a canonical JSON schema + a final ‚Äústyle normalizer‚Äù pass on one model. don‚Äôt fight every model, collapse outputs late. ",
          "score": 1,
          "created_utc": "2026-01-28 12:47:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cin4c",
              "author": "NoEntertainment8292",
              "text": "That makes sense! Collapsing late feels cleaner than over-constraining each step. Do you keep the schema purely semantic (content + intent) and let the style normalizer handle tone entirely, or do you still encode style hints in the JSON? Also wondering how brittle this gets as you add more models to the pipeline?",
              "score": 1,
              "created_utc": "2026-01-29 03:34:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qol9sp",
      "title": "GraphRAG vs LangGraph agents for codebase visualization ‚Äî which one should I use?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qol9sp/graphrag_vs_langgraph_agents_for_codebase/",
      "author": "Dizzy-Item-7123",
      "created_utc": "2026-01-27 17:59:58",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 0.75,
      "text": "I‚Äôm building an app that visualizes and queries an entire codebase.\n\nStack:\nDjango backend\nLangChain for LLM integration\n\nI want to avoid hallucinations and improve accuracy. I‚Äôm exploring:\n\nGraphRAG (to model file/function/module relationships)\nLangGraph + ReAct agents (for multi-step reasoning and tool use)\n\nNow I‚Äôm confused about the right architecture.\nQuestions:\n\nIf I‚Äôm using LangGraph agents, does GraphRAG still make sense?\n\nIs GraphRAG a replacement for agents, or a retrieval layer under agents?\n\nCan agents with tools parse and traverse a large codebase without GraphRAG?\n\nFor a codebase Q&A + visualization app, what‚Äôs the cleaner approach?\n\nLooking for advice from anyone who‚Äôs built code intelligence or repo analysis tools.",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qol9sp/graphrag_vs_langgraph_agents_for_codebase/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o28zxqv",
          "author": "Striking-Bluejay6155",
          "text": "Sharing a tool I think does what you‚Äôre describing with graphrag in the background and the ability to to chat: https://code-graph.falkordb.com/",
          "score": 1,
          "created_utc": "2026-01-28 17:25:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g88my",
          "author": "pbalIII",
          "text": "They're different layers, not alternatives. GraphRAG handles your retrieval... file/function/module relationships as a knowledge graph that your agent queries. LangGraph handles orchestration... how your agent reasons through multi-step tasks.\n\nThe pattern that's working in production: build your code graph (Neo4j, FalkorDB, Memgraph all have SDKs for this), then let your LangGraph agent query it as a tool. The agent decides what to look up, GraphRAG returns the relevant subgraph.\n\nWithout the graph structure, agents can still traverse codebases but they waste tokens re-discovering relationships. With it, you get pre-indexed connections so the agent jumps straight to relevant files.\n\nFor visualization specifically, the graph is doing double duty... feeding both your UI and your agent's retrieval.",
          "score": 1,
          "created_utc": "2026-01-29 17:54:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2l1l0o",
          "author": "Luneriazz",
          "text": "langraph is for rigid workflow... lets say every time you ask the agent must look at previous chat before searching into graphRAG. you use langgraph for something like that\n\nfor graphRAG, is for hierarchical knowledge query instead of nearest or similarity search",
          "score": 1,
          "created_utc": "2026-01-30 11:18:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqcthg",
      "title": "Persistent Architectural Memory cut our Token costs by ~55% and I didn‚Äôt expect it to matter this much",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qqcthg/persistent_architectural_memory_cut_our_token/",
      "author": "codes_astro",
      "created_utc": "2026-01-29 16:11:48",
      "score": 6,
      "num_comments": 9,
      "upvote_ratio": 0.64,
      "text": "We‚Äôve been using AI coding tools (Cursor, Claude Code) in production for a while now. Mid-sized team. Large codebase. Nothing exotic. But over time, our token usage kept creeping up, especially during handoffs. New dev picks up a task, asks a few ‚Äúwhere is X implemented?‚Äù types simple questions, and suddenly the agent is pulling half the repo into context.\n\nAt first we thought this was just the cost of using AI on a big codebase. Turned out the real issue was¬†*how context was rebuilt*.\n\nEvery query was effectively a cold start. Even if someone asked the same architectural question an hour later, the agent would:\n\n* run semantic search again\n* load the same files again\n* burn the same tokens again\n\nWe tried being disciplined with manual file tagging inside Cursor. It helped a bit, but we were still loading entire files when only small parts mattered. Cache hit rate on understanding was basically zero.\n\nThen we came across the idea of persistent architectural memory and ended up testing it in ByteRover. The mental model was simple; instead of caching¬†answers, you cache understanding.\n\n# How it works in practice\n\nYou curate architectural knowledge once:\n\n* entry points\n* control flow\n* where core logic lives\n* how major subsystems connect\n\nThis is short, human-written context. Not auto-generated docs. Not full files. That knowledge is stored and shared across the team. When a query comes in, the agent retrieves this memory first and only inspects code if it actually needs implementation detail.\n\nSo instead of loading 10k plus tokens of source code to answer: ‚ÄúWhere is server component rendering implemented?‚Äù\n\nThe agent gets a few hundred tokens describing the structure and entry points, then drills down selectively.\n\n# Real example from our tests\n\nWe ran the same four queries on the same large repo:\n\n* architecture exploration\n* feature addition\n* system debugging\n* build config changes\n\nManual file tagging baseline:\n\n* \\~12.5k tokens per query on average\n\nWith memory-based context:\n\n* \\~2.1k tokens per query on average\n\nThat‚Äôs about an¬†**83% token reduction**¬†and roughly¬†**56% cost savings**¬†once output tokens are factored in.\n\nhttps://preview.redd.it/a8s2hsvtbbgg1.png?width=1600&format=png&auto=webp&s=2e1bf23468ea2ce4650cb808ab4e294a61f9262b\n\n[](https://preview.redd.it/persistent-architectural-memory-cut-our-token-costs-by-55-v0-t6iyrdf3bbgg1.png?width=1600&format=png&auto=webp&s=7e1993d30d687a9f62505ff50fffbf584385f81d)\n\nSystem debugging benefited the most. Those questions usually span multiple files and relationships. File-based workflows load everything upfront. Memory-based workflows retrieve structure first, then inspect only what matters.\n\n# The part that surprised me\n\nLatency became predictable. File-based context had wild variance depending on how many search passes ran. Memory-based queries were steady. Fewer spikes. Fewer ‚Äúwhy is this taking 30 seconds‚Äù moments.\n\nAnd answers were more consistent across developers because everyone was querying the same shared understanding, not slightly different file selections.\n\n# What we didn‚Äôt have to do\n\n* No changes to application code\n* No prompt gymnastics\n* No training custom models\n\nWe just added a memory layer and pointed our agents at it.\n\nIf you want the full breakdown with numbers, charts, and the exact methodology, we wrote it up¬†[here](https://www.byterover.dev/blog/reducing-token-usage-by-83-benchmarking-cursor-s-file-context-vs.-byterover-s-memory-layer).\n\n# When is this worth it\n\nThis only pays off if:\n\n* the codebase is large\n* multiple devs rotate across the same areas\n* AI is used daily for navigation and debugging\n\nFor small repos or solo work, file tagging is fine. But once AI becomes part of how teams¬†understand¬†systems, rebuilding context from scratch every time is just wasted spend.\n\nWe didn‚Äôt optimize prompts. We optimized how understanding persists. And that‚Äôs where the savings came from.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qqcthg/persistent_architectural_memory_cut_our_token/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2ft7tk",
          "author": "cmndr_spanky",
          "text": "I miss the old Reddit ‚Ä¶ before it became an empty cesspool of SEO posts",
          "score": 12,
          "created_utc": "2026-01-29 16:46:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gjngy",
              "author": "WowSoWholesome",
              "text": "So interesting that most of the posts here look and feel the same",
              "score": 2,
              "created_utc": "2026-01-29 18:45:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2grz0u",
                  "author": "cmndr_spanky",
                  "text": "you understand why right? Basically traditional SEO no longer works because google rank no longer matters as much as chatGPT deciding your content is worth \"escalating\" to user's attention.\n\nEveryone in the digital marketing work now understands that reddit's data is sold to open AI (and others) and is a huge source of raw conversational material that's used to train frontier models.\n\nMarketers have caught on, so the default strategy is to spam these slop worded posts all over reddit with a link (or add a comment with a link so you don't get blocked by auto mods).\n\nI tried to report as many as I can with spam, but Reddit overall is out of control. It's hard to tell if reddit leadership is leaning into this bullshit because they see dollar signs, but in the end we'll all loose. People will no longer have real conversations on reddit (because overrun with bots and slop) as an advert platform in disguise.. but without real conversations the data will no longer be as useful for LLM training and honest ranking... and Reddit financially just dies a slow death as users migrate somewhere else where human-human exchanges are actually real and protected.",
                  "score": 3,
                  "created_utc": "2026-01-29 19:23:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2iweeg",
              "author": "qa_anaaq",
              "text": "I don‚Äôt disagree. Did you read the post though? I‚Äôm curious if it‚Äôs worth it but not willing to read it. Legit question though.",
              "score": 1,
              "created_utc": "2026-01-30 01:51:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ji5il",
                  "author": "cmndr_spanky",
                  "text": "yes.. this post is bullshit. Thera are a million ways to avoid wasted context window, cursor (an amazing coding agent) doesn't work the way OP described. This is a non-solution like 99% of the slop posts on this subreddit that are just trying to game \"search engine optimization\" and have nothing of value to offer.",
                  "score": 1,
                  "created_utc": "2026-01-30 03:54:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2fursv",
          "author": "Lern360",
          "text": "Love seeing stuff like this - it‚Äôs such a practical reminder that **context management + memory layers matter way more than just throwing code at the model**. By caching *understanding* of your system instead of reloading whole files every time, you massively cut token use and made query costs way more predictable. That‚Äôs exactly the kind of optimization that actually scales in a team setting rather than just hacking prompts.",
          "score": 1,
          "created_utc": "2026-01-29 16:53:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kc704",
          "author": "R-4553",
          "text": "Could be interesting to explore semantic compression to add onto your cost cuts. Potentially like 50-75% on top depending on the input type",
          "score": 1,
          "created_utc": "2026-01-30 07:32:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oti89",
          "author": "pbalIII",
          "text": "Ran into the same cold-start problem on a monorepo with 400+ services. The fix that stuck was treating architectural context like a CLAUDE.md file per service boundary, just enough to explain entry points, data flow, and who owns what.\n\nThe latency consistency you mention tracks. File-based context had 10x variance in our setup because semantic search would sometimes pull in test fixtures or deprecated modules. Memory-first routing cut that noise out.\n\nOne thing we learned: the understanding layer needs versioning. Architecture drifts, and stale memory is worse than no memory because the agent trusts it. Git-triggered refresh or TTL on critical sections helped.",
          "score": 1,
          "created_utc": "2026-01-30 22:31:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu1bqx",
      "title": "AI Agent to deal with enormous datasets",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qu1bqx/ai_agent_to_deal_with_enormous_datasets/",
      "author": "Abject_Reference_160",
      "created_utc": "2026-02-02 17:23:15",
      "score": 6,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "I'm working on a system that implements an AI Agent that analyses the sales history and forecasts future demand.  \nIt is written in NestJS and uses langchain and langchain/openai. The agent is basically declared as follows:  \n  \nconstructor() {  \nthis.chatOpenAI = new ChatOpenAI({  \napiKey: process.env.OPENAI\\_API\\_KEY,  \nmodel: \"gpt-5-mini-2025-08-07\",  \nverbose: true  \n});  \n¬† }\n\n  \nSo, kinda basic. This is also the first time i'm implementing a complex system with onboard AI, so any tips would be welcome.\n\nThe problem is, i need my ai to be able to read enormous datasets at once, like a really big sales history (it is the biggest part), but I always hit limitations like text too big for sending in a request or it is way past the 128k token limit.  \nI tried using toon, but my agent got confused and returned nothing to an input that normally would generate data.\n\nRAG was an idea for saving tokens but, afaik, it shouldn't be used for calculations like this, but for textual understanding and searches.  \nProducing batch pre compiled analysis was also an option, but it would be really hard to preserve all the insights that are possible with the raw data.\n\nHow can i set it up to reading monstruous datasets like this?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qu1bqx/ai_agent_to_deal_with_enormous_datasets/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3750v7",
          "author": "big_fart_9090",
          "text": "lol, edit: sorry now constructive. An LLM is the wrong tool for what you want to achieve mate. Try asking an LLM on how to do time series forecasting. It wil suggest various data science stuff. Good luck, you will need it",
          "score": 2,
          "created_utc": "2026-02-02 18:20:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o378je4",
              "author": "Abject_Reference_160",
              "text": "Thanks mate, guess i'll really need this luck lol.  \nI was having a talk with grok and it also came to the conclusion that a LLM is optimized more as a conversational thing, and it isn't the same tool used by scientists and such (I initially thought it was more of a setup difference or perfectly thought out strategies that i was also trying to come up with). I'll search more about it and come back with updates.",
              "score": 1,
              "created_utc": "2026-02-02 18:36:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o36viux",
          "author": "Empty_Contact_2823",
          "text": "Do you need to ingest all of the historic data at once? Or find the relevant data first to operate on?\n\nTraditional Unix file system access using grep etc to pipe to llm can be a way to counter the token limit.\n\nAlso worth considering if you could filter the data via api calls / function calls within the llm first.",
          "score": 1,
          "created_utc": "2026-02-02 17:38:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36wwfx",
              "author": "Abject_Reference_160",
              "text": "Generally what I do is select the products I want to analyze and get its related data, such as inventories, purchase orders and sales orders.  \nTo ingest all the product's sales history would enable me to perceive patterns, such as which clients use to buy this product, when they buy it and what they get it with, and it would interact better with unstructured data like the info that a client is breaking the contract (how much do they represent for this product's demand and when?).\n\nIs this what you were referring to?",
              "score": 1,
              "created_utc": "2026-02-02 17:44:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o386zfl",
                  "author": "Wide_Brief3025",
                  "text": "It sounds like you're definitely on the right track by tying together sales history, inventory, and contract events for deeper insight. If you start pulling in conversational data or signals from platforms like Reddit or LinkedIn, something like ParseStream can automate tracking keywords and alert you to important discussions so you never miss relevant signals for your products.",
                  "score": 1,
                  "created_utc": "2026-02-02 21:17:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o36xjxb",
              "author": "Abject_Reference_160",
              "text": "i'd like to, for instance, get all the products of a supplier and analyze them at once, or at least several products. At the moment, even 1 product's history is too much, and some suppliers have far more demand than others, which means their products will have a sales history that is even bigger and technically harder to analyze.",
              "score": 1,
              "created_utc": "2026-02-02 17:47:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o383gg3",
                  "author": "snarfi",
                  "text": "You need to query your dataset via an api first to only return whats needed for the particular request.",
                  "score": 1,
                  "created_utc": "2026-02-02 21:01:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o378mit",
          "author": "Strong_Cherry6762",
          "text": "How large is your dataset? How many megabytes of memory does it occupy? How many rows of data records are there?",
          "score": 1,
          "created_utc": "2026-02-02 18:37:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37bese",
              "author": "Abject_Reference_160",
              "text": "I have nearly 5k products, that together have half a million sales orders. Each product has a inventory record for each deposit, and it must all be analysed together in order to suggest sales or transfers between deposits. It is a sales history from 2020 to current day, and it will continue growing, as the system will be used for several years to come.  \nThe model i'm using, gpt-5-mini-2025-08-07, appears to have a token limit of 128.000, and for some products, 1 month of its sales history is way more than enough to break this limit, so considering I need to look to 5 years and more as the time passes, it is kinda big (or at least it looks monstruous for my current setup to handle).\n\nWe are planning to grow a lot on the oncoming years, so more data will be generated on a shorter time.",
              "score": 1,
              "created_utc": "2026-02-02 18:49:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o37fkhq",
                  "author": "Strong_Cherry6762",
                  "text": "Okay, that is absolutely huge for an LLM context window, but actually \"small data\" for a computer.\n\nIf I were you, I'd implement this using a CLI + skill approach‚Äîjust let the LLM write Python code directly in your terminal, and then use that code to handle the data processing tasks.\n\nOf course, if you do this kind of repetitive work often, I suggest using a \"skill-creator\" (Anthropic has a guide on this: https://resources.anthropic.com/hubfs/The-Complete-Guide-to-Building-Skill-for-Claude.pdf) to build a skill that solidifies your data processing workflow.\n\nGood luck with the build!",
                  "score": 2,
                  "created_utc": "2026-02-02 19:08:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37uxx9",
          "author": "InfraScaler",
          "text": "You need to give your agent tools to filter the data without having to read it first, e.g. ways to query the dataset.¬†",
          "score": 1,
          "created_utc": "2026-02-02 20:20:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtw1wu",
      "title": "LangChain VS LamaIndex - Plug r/LangChain context into your LangChain agents - Free MCP integration",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qtw1wu/langchain_vs_lamaindex_plug_rlangchain_context/",
      "author": "jannemansonh",
      "created_utc": "2026-02-02 14:10:12",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 0.78,
      "text": "Hey, creator of [needle.app](http://needle.app) here. This subreddit has incredible implementation knowledge - patterns, agent architectures, RAG configs, tool calling issues, what actually works in production.\n\nWe indexed all 2025 r/LangChain discussions and made them searchable. Even better: we built an MCP integration so you can plug this entire subreddit's context directly into your LangChain agents for agentic RAG.\n\nTry searching:\n\n* Tool calling with function schemas\n* Multi-agent orchestration patterns\n* Vector store performance comparisons\n\nUseful if you're:\n\n* Debugging agent loops or tool calling\n* Finding solutions others have already tested\n\n**Want to use this in your LangChain agents?** Check out our MCP integration guide: [https://docs.needle.app/docs/guides/mcp/needle-mcp-server/](https://docs.needle.app/docs/guides/mcp/needle-mcp-server/)\n\nNow you can build agents that query r/LangChain knowledge directly while reasoning.\n\nCompletely free, no signup: [https://needle.app/featured-collections/reddit-langchain-2025](https://needle.app/featured-collections/reddit-langchain-2025)\n\n[LangChain or LamaIndex - Needle.app RAG Chat](https://reddit.com/link/1qtw1wu/video/prkrlbzo93hg1/player)\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qtw1wu/langchain_vs_lamaindex_plug_rlangchain_context/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o35qt2s",
          "author": "beneautomas",
          "text": "Useful!",
          "score": 1,
          "created_utc": "2026-02-02 14:23:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35rfk4",
              "author": "jannemansonh",
              "text": "RAGception XD",
              "score": 1,
              "created_utc": "2026-02-02 14:26:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3729ap",
          "author": "PromptAndHope",
          "text": "Is this compatible with reddit policy ?",
          "score": 1,
          "created_utc": "2026-02-02 18:08:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}