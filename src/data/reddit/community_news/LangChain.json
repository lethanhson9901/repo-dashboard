{
  "metadata": {
    "last_updated": "2026-02-02 09:09:57",
    "time_filter": "week",
    "subreddit": "LangChain",
    "total_items": 20,
    "total_comments": 59,
    "file_size_bytes": 105460
  },
  "items": [
    {
      "id": "1qpci1h",
      "title": "You can now train embedding models ~2x faster!",
      "subreddit": "LangChain",
      "url": "https://i.redd.it/kbenz74xl3gg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-28 14:15:31",
      "score": 39,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qpci1h/you_can_now_train_embedding_models_2x_faster/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2t1d8v",
          "author": "Exciting-Royal-3361",
          "text": "That's awesome news. Does someone have experience with building high quality training data for embedding models? I know one popular technique is to take one or more passages and generate a matching query using an LLM and perhaps create multiple versions of the same query in different styles. \n\nAre there any other techniques for generating / sourcing training data based on a large corpus of data?",
          "score": 1,
          "created_utc": "2026-01-31 15:53:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpi07h",
      "title": "I built a job search assistant to understand LangChain Deep Agents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/gallery/1qpi07h",
      "author": "Acrobatic-Pay-279",
      "created_utc": "2026-01-28 17:34:30",
      "score": 31,
      "num_comments": 3,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qpi07h/i_built_a_job_search_assistant_to_understand/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2bvl13",
          "author": "qa_anaaq",
          "text": "Cool and thanks for sharing. 2 Qs. Do you feel the deep agents harness is any good, and how‚Äôs the cost of running it?",
          "score": 2,
          "created_utc": "2026-01-29 01:26:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dj8hk",
              "author": "Acrobatic-Pay-279",
              "text": "yeah it's good, especially for long running tasks. in the earlier version, I actually tracked write\\_todos with the statuses and it was breaking things down step by step. I also tried HITL flows. the docs mention prompt caching (Anthropic) and pluggable storage backends but I wasn't able to verify/use those.\n\nthe cost is just the underlying model (I used OpenAI in this case).",
              "score": 3,
              "created_utc": "2026-01-29 08:06:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ek6gp",
                  "author": "qa_anaaq",
                  "text": "Cool thanks a lot",
                  "score": 1,
                  "created_utc": "2026-01-29 13:08:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qrpomi",
      "title": "Are MCPs outdated for Agents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qrpomi/are_mcps_outdated_for_agents/",
      "author": "FunEstablishment5942",
      "created_utc": "2026-01-31 02:17:17",
      "score": 25,
      "num_comments": 23,
      "upvote_ratio": 0.94,
      "text": "I saw a video of the OpenClaw creator saying that MCP tools are shit\nIn fact the only really working Agent  are moving away from defining strict tools (like MCP or rigid function calling) and giving the agent raw CLI tools and letting it figure it out.\n\n‚ÄãI‚Äôm looking into LangGraph for this, and while the checkpointers are amazing for recovering conversation history (threads), I'm stuck on how to handle the Computer State\n\n‚ÄãThe Problem:\nA conversation thread is easy to persist. But a CLI session is stateful (current working directory, cli commands, active background processes).\n\n‚ÄãIf an agent runs cd /my_project in step 1, and the graph pauses or moves to the next step, that shell context is usually lost unless explicitly managed.\n\n‚ÄãThe Question:\nIs there an existing abstraction or \"standard way\" in LangGraph to maintain a persistent CLI/Filesystem session context that rehydrates alongside the thread?If not would it be a good idea to add it?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qrpomi/are_mcps_outdated_for_agents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2qdp2h",
          "author": "cincyfire35",
          "text": "I lead a development team where we build with langgraph regularly.\n\nPeople who are naysayers on MCP dont realize that there are other applications for it than just spamming context with 10-50 irrelevant tools for a general purpose agent. With frameworks like langgraph, you can build and orchestrate custom agents for tasks with finely tuned contexts and tools, eliminating the need for things like skills and tool selectors. Pairing this with code based mcp execution, you can pretty much load 2-3 mcp servers with all their tools as python functions in a safe execution environment (see smolagents‚Äô safe python executor), tell the llm it can call them as python functions, and get a lot of the benefits from anthropics/cloudflare‚Äôs code mode articles by chaining calls into each other and performing calcs/aggregation outside the context window. You can even build logic to lazy load the tools if you want, but thats a waste if you can just route to a specialized agent for the given task. \n\nWe never use more than 2-3 mcp servers with curated tools selected for an agent because we pay per token. Why waste it with irrelevance? We let users build agents with specific goals and targets in mind, select only the tools they need, and it can solve/work through the task for them. Why give a rag agent for a legal team access to SQL tools for supply chain? Makes no sense. But some people just build one big agent and hope it works. Langgraph/langchain enables you to build custom workflows and agents to solve tasks efficiently. Can build in orchestration however you prefer (tons of flexibility and documented examples of how to do it) and accomplish what claude does with skills, but more predictably and reliably. \n\nAnd thats not the half of it. MCP is just a protocol. We build custom tools with fastMCP in python all the time and its an easy way to connect the tools to our langgraph agents or external ones. We host them in our platform and can connect to them as needed. It allows us to build powerful tools that can be reused across frameworks. You dont need an mcp servers with 100 tools it. Can spin up several servers in one app instance of compute with 1-3 specific to usecase tools each built in a very easy way with good testing/standards, then serve it to your agents. We also connect with external vendors mcps like alation or atlassian if building an agent to explore data or help devs with jira, for example. Tons in the ecosystem.",
          "score": 43,
          "created_utc": "2026-01-31 03:53:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wlb01",
              "author": "SpareIntroduction721",
              "text": "You used smolagents with Langgraph? Or code mode? I‚Äôve tried and failed, I tried the UTCP route as well and didn‚Äôt work too well",
              "score": 1,
              "created_utc": "2026-02-01 02:53:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ztyzh",
                  "author": "cincyfire35",
                  "text": "No, i ripped the code execution function out of it (since i liked the security it gave) and build support for it in our framework as a tool. We don‚Äôt use smolagents for the orchestration, just for the safe python execution environment we can control cleanly (we made some additional security enhancement/tweaks to make it work better with databricks). From there, it was trivial to make a code-mode that injected any other mcps provided to the model as python functions that could be executed in that environment if the agent wrote it as python code.",
                  "score": 1,
                  "created_utc": "2026-02-01 16:38:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2qd0zk",
          "author": "Number4extraDip",
          "text": "Didnt need to deal with lang through my deployment whatsoever. I use mcp and have no issues. Saves me time. CLI environments arent available to all users/hardware/OS",
          "score": 6,
          "created_utc": "2026-01-31 03:49:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2q4qdb",
          "author": "Prestigious_Pin4388",
          "text": "Short Answer: I don't use langgraph much so sorry I don't know.\nLong Answer:\nI think it's not right to give Agents complete autonomy because most or the AI apps me n u r making would be 'deterministic'\nWe would know exactly everything that is happening in it, \"what happens when the Agent doesn't call the tool? how do you debug this? how good is it's accuracy to call tools? what if I have to use an open mode for lower costs, it has much worse tool calling accuracy than gpt-5?  etc\" \n\nThese are all the questions in my mind when giving tools to LLMs, these things are non-deterministic.\nyes, we can give them \"better\" prompts and reduce temperature but that still creates vagueness and its much more difficult to debug things if they break.\nThis is the case for deterministic tools, now people are asking to give it complete freedom regardless of prompt injections or any security issues, and like you said, it gets difficult to manage these tools especially in production, imagine how hellish it would be to debug when things break.\n\nSo, I recommend you ignore these \"hype\" stuff.\nYou probably would have heard of how good is clawdbot( moldbot) but now see the whole drama around it.\n\nSome say it deleted all the files, some are finding security issues in it, even the creator said that it was a side project not meant for production.\nyet, still there's people yapping about how it saved them time n money, blah blah blah.\n\n\nhope this was helpful :)",
          "score": 3,
          "created_utc": "2026-01-31 02:57:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2q4ujh",
              "author": "Prestigious_Pin4388",
              "text": "it wasn't AI generated of course so, I suppose it was helpful ;)",
              "score": 3,
              "created_utc": "2026-01-31 02:58:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rajn0",
          "author": "indutrajeev",
          "text": "Yeah, if you don‚Äôt need any control. But MCP‚Äôs can act like a layer of governance around your Agent to check, validate, ‚Ä¶ what it does with other systems.\n\nJust giving it cli access is maybe faster but inherently much more difficult to control and check.",
          "score": 3,
          "created_utc": "2026-01-31 08:16:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qguqa",
          "author": "johndoerayme1",
          "text": "Tool fatigue is real. Recent studies are showing that tool overhead can be misleading and confusing for agents. DeepAgent went to filesystem in great part for that reason. Give agents a more broad set of functionality and let them figure out how to use them - evolve sets of skills that are curated more towards the actual environment in which they're running. This is where things seem to be moving right now.\n\nRecently Anthropic added tool search to Claude as part of trying to mitigate tool fatigue/bloat. \n\nA lot of modern thought is about keeping context small/clean... so adding a ton of \"here's all the tools you can use and all their definitions\" when most of them aren't really relevant to the limited scope of the current task focus really undermines that objective.\n\nCheck out Deepagents for your persistent filesystem. I've used it effectively for my own form of \"skills\" that the agents can evolve as they learn from interaction.",
          "score": 2,
          "created_utc": "2026-01-31 04:14:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rgulw",
              "author": "Tobi-Random",
              "text": "Recent? It's been a known fact for over a year already. That's old stuff in ai context",
              "score": 0,
              "created_utc": "2026-01-31 09:15:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2sd191",
                  "author": "johndoerayme1",
                  "text": "Cool yeah except Claude just came out with tool search to address this and Harrison Chase wrote an article about using filesystem to address this 1-2 months ago. But ok cool yes it's \"old stuff\". Sorry I misspoke. Thanks for correcting the least relevant part of my response.",
                  "score": 1,
                  "created_utc": "2026-01-31 13:42:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rgrn2",
          "author": "vuongagiflow",
          "text": "MCP is a protocol; it is no different from openclaw tool integration with plugin. When you need consistency in operation, it need stricter schema and mcp input schema allow you to expose that contract; otherwise you will need two llm calls to achieve what mcp tool do in one call. \n\nDepending on the context, you would implement skill -> mcp -> hook to be more consistent and efficient.",
          "score": 2,
          "created_utc": "2026-01-31 09:14:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qeqmg",
          "author": "caprica71",
          "text": "The langgraph state should just hold a series of file references to where the cli has dumped its output. Later nodes in the graph can then go back and grep the files to see what happened.",
          "score": 1,
          "created_utc": "2026-01-31 04:00:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2si9u7",
              "author": "FunEstablishment5942",
              "text": "maybe this is the answer, there is not an abstraction already in place? it seems that deepagents (https://docs.langchain.com/oss/python/deepagents)  does not compartmentalize per thread the files, right?",
              "score": 1,
              "created_utc": "2026-01-31 14:13:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qy8ry",
          "author": "hello5346",
          "text": "Just like RAG.",
          "score": 1,
          "created_utc": "2026-01-31 06:25:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rkvlk",
          "author": "fball403",
          "text": "https://docs.langchain.com/oss/python/deepagents/overview",
          "score": 1,
          "created_utc": "2026-01-31 09:54:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2shwz0",
              "author": "FunEstablishment5942",
              "text": "but with deepagent is there a way to comportamentalise the files by threads? so that that thread has that environment with all /temp files that are secure and not accessed by another thread? Is this abstraction already in place?",
              "score": 1,
              "created_utc": "2026-01-31 14:11:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2tu4ym",
                  "author": "caspardev",
                  "text": "You can prompt the agent to only read/write to a directory titled with the thread id",
                  "score": 1,
                  "created_utc": "2026-01-31 18:11:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o313xsr",
          "author": "Remote-Ingenuity8459",
          "text": "Yes I somewhat agree but for instance if the agent needs access to up-to-date web data what better way then to connect it using LanGraph to a solid [web mcp ](https://get.brightdata.com/github-mcp-server)then when the agent answers a question it can point directly to the actual sources rather than rely on memory.",
          "score": 1,
          "created_utc": "2026-02-01 20:09:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrwgul",
      "title": "Long-term memory of design",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/57v3qvd36ngg1",
      "author": "1501694",
      "created_utc": "2026-01-31 08:00:30",
      "score": 21,
      "num_comments": 6,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qrwgul/longterm_memory_of_design/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2swieb",
          "author": "justanemptyvoice",
          "text": "So much what?  All I see is a mind map diagram.",
          "score": 3,
          "created_utc": "2026-01-31 15:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u4tk1",
          "author": "qa_anaaq",
          "text": "This feels like a post from a sci fi movie where the AI was controlling the human and making them type then something snapped and the connection was severed mid thought.",
          "score": 2,
          "created_utc": "2026-01-31 19:01:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u0j1n",
          "author": "Due-Mode9856",
          "text": "Can you share the link of the diagram with us",
          "score": 1,
          "created_utc": "2026-01-31 18:41:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2w4jzd",
          "author": "1501694",
          "text": "graph TB\n    subgraph InputLayer[Input Layer]\n        UserQuery[\"User Query / Áî®Êà∑Êü•ËØ¢\"]\n        DialogueContext[\"Dialogue Context / ÂØπËØù‰∏ä‰∏ãÊñá\"]\n        HistoricalData[\"Historical Data / ÂéÜÂè≤Êï∞ÊçÆ\"]\n        EmotionalTone[\"Emotional Tone / ÊÉÖÊÑüÂü∫Ë∞É\"]\n    end\n    \n    subgraph FiveLayerThinking[Five-Layer Thinking System]\n        subgraph Layer1[\"Layer 1: Factual Layer / ‰∫ãÂÆûÂ±Ç\"]\n            L1_1[\"Objective Facts / ÂÆ¢ËßÇ‰∫ãÂÆû\"]\n            L1_2[\"Data Information / Êï∞ÊçÆ‰ø°ÊÅØ\"]\n            L1_3[\"Specific Details / ÂÖ∑‰ΩìÁªÜËäÇ\"]\n            L1_4[\"Time & Location / Êó∂Èó¥Âú∞ÁÇπ\"]\n            L1_5[\"Verifiable Claims / ÂèØÈ™åËØÅÂ£∞Êòé\"]\n        end\n        \n        subgraph Layer2[\"Layer 2: Logical Layer / ÈÄªËæëÂ±Ç\"]\n            L2_1[\"Causality / Âõ†ÊûúÂÖ≥Á≥ª\"]\n            L2_2[\"Reasoning Chains / Êé®ÁêÜÈìæÊù°\"]\n            L2_3[\"Argument Structure / ËÆ∫ËØÅÁªìÊûÑ\"]\n            L2_4[\"Contradiction Detection / ÁüõÁõæÊ£ÄÊµã\"]\n            L2_5[\"Logical Consistency / ÈÄªËæë‰∏ÄËá¥ÊÄß\"]\n        end\n        \n        subgraph Layer3[\"Layer 3: Emotional Layer / ÊÉÖÊÑüÂ±Ç\"]\n            L3_1[\"Emotion Recognition / ÊÉÖÁª™ËØÜÂà´\"]\n            L3_2[\"Sentiment Analysis / ÊÉÖÊÑüÂàÜÊûê\"]\n            L3_3[\"Feeling Expression / ÊÑüÂèóË°®Ëææ\"]\n            L3_4[\"Empathy Understanding / ÂÖ±ÊÉÖÁêÜËß£\"]\n            L3_5[\"Affective Memory / ÊÉÖÊÑüËÆ∞ÂøÜ\"]\n        end\n        \n        subgraph Layer4[\"Layer 4: Value Layer / ‰ª∑ÂÄºÂ±Ç\"]\n            L4_1[\"Meaning Judgment / ÊÑè‰πâÂà§Êñ≠\"]\n            L4_2[\"Value Orientation / ‰ª∑ÂÄºÂèñÂêë\"]\n            L4_3[\"Moral Consideration / ÈÅìÂæ∑ËÄÉÈáè\"]\n            L4_4[\"Goal Alignment / ÁõÆÊ†áÂØπÈΩê\"]\n            L4_5[\"Ethical Reasoning / ‰º¶ÁêÜÊé®ÁêÜ\"]\n        end\n        \n        subgraph Layer5[\"Layer 5: Philosophical Layer / Âì≤Â≠¶Â±Ç\"]\n            L5_1[\"Essence Thinking / Êú¨Ë¥®ÊÄùËÄÉ\"]\n            L5_2[\"Existence Meaning / Â≠òÂú®ÊÑè‰πâ\"]\n            L5_3[\"Ultimate Questions / ÁªàÊûÅËøΩÈóÆ\"]\n            L5_4[\"Wisdom Integration / Êô∫ÊÖßÊï¥Âêà\"]\n            L5_5[\"Transcendent Understanding / Ë∂ÖË∂äÊÄßÁêÜËß£\"]\n        end\n    end\n    \n    subgraph DynamicFusion[Dynamic Weight Fusion]\n        ContextAnalysis[\"Context Analysis / ‰∏ä‰∏ãÊñáÂàÜÊûê\"]\n        WeightCalculation[\"Weight Calculation / ÊùÉÈáçËÆ°ÁÆó\"]\n        FusionFormula[\"S = Œ£(w·µ¢ √ó L·µ¢) / ËûçÂêàÂÖ¨Âºè\"]\n        OutputGeneration[\"Output Generation / ËæìÂá∫ÁîüÊàê\"]\n    end\n    \n    subgraph FeedbackLoop[Feedback & Learning]\n        UserFeedback[\"User Feedback / Áî®Êà∑ÂèçÈ¶à\"]\n        WeightAdjustment[\"Weight Adjustment / ÊùÉÈáçË∞ÉÊï¥\"]\n        SystemLearning[\"System Learning / Á≥ªÁªüÂ≠¶‰π†\"]\n    end\n    \n    InputLayer --> FiveLayerThinking\n    FiveLayerThinking --> DynamicFusion\n    DynamicFusion --> FeedbackLoop\n    FeedbackLoop -.-> |Optimize| FiveLayerThinking\n    \n    style InputLayer fill:#E8F5E9\n    style Layer1 fill:#BBDEFB\n    style Layer2 fill:#90CAF9\n    style Layer3 fill:#F48FB1\n    style Layer4 fill:#FFCC80\n    style Layer5 fill:#CE93D8\n    style DynamicFusion fill:#A5D6A7\n    style FeedbackLoop fill:#FFD54F",
          "score": 1,
          "created_utc": "2026-02-01 01:12:09",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2w6ue9",
              "author": "1501694",
              "text": "üò£üò£   too long‚Ä¶‚Ä¶   I pasted to grok and shared the link here, can I see the content? I don't know what everyone usually uses, or DM me    [link](https://grok.com/share/bGVnYWN5_35c8f87c-0efa-4d5d-9089-0ee78306de69)\nhttps://grok.com/share/bGVnYWN5_35c8f87c-0efa-4d5d-9089-0ee78306de69",
              "score": 1,
              "created_utc": "2026-02-01 01:25:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnqln9",
      "title": "A practical open-source repo for learning AI agents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qnqln9/a_practical_opensource_repo_for_learning_ai_agents/",
      "author": "Creepy-Row970",
      "created_utc": "2026-01-26 19:43:49",
      "score": 15,
      "num_comments": 3,
      "upvote_ratio": 0.9,
      "text": "A practical open-source repo for learning AI agents. I‚Äôve contributed 10+ examples\n\nI‚Äôve contributed 10+ agent examples to an open-source repo that‚Äôs grown into a solid reference for building AI agents.\n\nRepo:[ https://github.com/Arindam200/awesome-ai-apps](https://github.com/Arindam200/awesome-ai-apps)\n\nWhat makes it useful:\n\n* 70+ runnable agent projects, not toy demos\n* Same ideas built across different frameworks\n* Covers starter agents, MCP, memory, RAG, and multi-stage workflows\n\nFrameworks include LangChain, LangGraph, LlamaIndex, CrewAI, Agno, Google ADK, OpenAI Agents SDK, AWS Strands, and PydanticAI.\n\nSharing in case others here prefer learning agents by reading real code instead of theory.\n\n",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1qnqln9/a_practical_opensource_repo_for_learning_ai_agents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o1vok23",
          "author": "Pristine_Shelter_28",
          "text": "are these live apps?",
          "score": 1,
          "created_utc": "2026-01-26 19:48:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o246fka",
          "author": "YUYbox",
          "text": "Hi, \n  I built a tool for anyone running multi-agent AI systems.\n When LLMs talk to each other, they develop patterns that are hard to audit - invented acronyms, lost context, meaning drift.\r\n\r\n   The solution: InsAIts monitors these communications and flags anomalies.\r\n\r\n```python\r\nfrom insa_its import insAItsMonitor\r\n\r\nmonitor = insAItsMonitor()  # Free tier, no key needed\r\nmonitor.register_agent(\"agent_1\", \"gpt-4\")\r\n\r\nresult = monitor.send_message(\r\n    text=\"The QFC needs recalibration on sector 7G\",\r\n    sender_id=\"agent_1\"\r\n)\r\n\r\nif result[\"anomalies\"]:\r\n    print(\"Warning:\", result[\"anomalies\"])\r\n```\r\n\r\n  Features:\r\n- Local processing (sentence-transformers)\r\n- LangChain & CrewAI integrations\r\n- Adaptive jargon dictionary\r\n- Zero cloud dependency for detection\r\n\r\nGitHub: https://github.com/Nomadu27/InsAIts\r\nPyPI: pip install insa-its",
          "score": 1,
          "created_utc": "2026-01-27 23:42:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo6uax",
      "title": "What It Actually Takes to Build a Context-Aware Multi-Agent AI System",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qo6uax/what_it_actually_takes_to_build_a_contextaware/",
      "author": "pretty_prit",
      "created_utc": "2026-01-27 07:05:33",
      "score": 15,
      "num_comments": 7,
      "upvote_ratio": 0.94,
      "text": "Designing a multi-agent system with memory raises a different set of problems than most demos show.  \n  \nThe diagram below shows a simple multi-agent architecture I built to explore that gap.  \n  \nInstead of agents talking to each other directly, everything goes through an orchestration layer that handles:  \n\\-intent routing  \n\\-shared user context  \n\\-memory retrieval and compaction  \n  \nWhile designing this, a set of product questions surfaced that you don‚Äôt see in most demos  \n\\-What belongs in long-term memory vs. short-term history?  \n\\-When do you summarize context, and what do you risk losing?  \n\\-How do you keep multiple agents consistent as context evolves?  \n  \nI wrote a detailed breakdown of this architecture, including routing strategy, memory design, and the trade-offs this approach introduces.  \n  \n[https://medium.com/towards-artificial-intelligence/how-i-built-a-context-aware-multi-agent-wellness-system-a3eacbc33fe4?sk=c37c88e2f74aa9e5c2b2d681292d26c2](https://medium.com/towards-artificial-intelligence/how-i-built-a-context-aware-multi-agent-wellness-system-a3eacbc33fe4?sk=c37c88e2f74aa9e5c2b2d681292d26c2)  \n  \nIf you‚Äôre a PM, founder, or student trying to move beyond one-off agent demos, this might be useful.\n\nhttps://preview.redd.it/mr1w53kmcufg1.png?width=1838&format=png&auto=webp&s=e36245c419d44c006fdd8e3ff006c060eb320489\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qo6uax/what_it_actually_takes_to_build_a_contextaware/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o1zgezp",
          "author": "DaRandomStoner",
          "text": "This looks pretty well put together... Is there a github repo we could check out?",
          "score": 1,
          "created_utc": "2026-01-27 08:51:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zusom",
              "author": "pretty_prit",
              "text": "Thank you. the github link is there at the end of the article but posting here again - [https://github.com/pritha21/llm\\_projects/tree/main/wellness\\_langchain\\_app](https://github.com/pritha21/llm_projects/tree/main/wellness_langchain_app)",
              "score": 1,
              "created_utc": "2026-01-27 11:01:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zmbdv",
          "author": "sleepnow",
          "text": "Good effort, but there is nothing at all new or particularly unique about this approach. ",
          "score": 1,
          "created_utc": "2026-01-27 09:46:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20aayx",
              "author": "pretty_prit",
              "text": "Maybe. But I was just exploring this topic, so its new for me.",
              "score": 3,
              "created_utc": "2026-01-27 12:55:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o244gr7",
          "author": "YUYbox",
          "text": "Hi there, I think InsAIts could be very helpful in this matter. \nhttps://github.com/Nomadu27/InsAIts",
          "score": 1,
          "created_utc": "2026-01-27 23:32:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o260i4c",
              "author": "pretty_prit",
              "text": "Will check it out",
              "score": 2,
              "created_utc": "2026-01-28 05:55:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o24l5lq",
          "author": "pbalIII",
          "text": "Context compaction is where most of these designs break down quietly. You summarize to save tokens, but summaries drop the specifics that matter six turns later... and you only find out when an agent makes a decision based on stale assumptions.\n\nOne pattern that's helped: treating memory writes as versioned facts rather than mutable state. Agents can reference which version they're working from, and conflicts surface explicitly instead of silently diverging.\n\nThe orchestration-layer-as-bottleneck tradeoff you're hitting is real. Centralizing routing keeps agents consistent, but it also means every context update round-trips through one chokepoint. Some teams split into private vs shared memory tiers to let agents work locally until they need to sync. Adds complexity, but scales better when you're past 3-4 agents.",
          "score": 0,
          "created_utc": "2026-01-28 00:57:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp3lp3",
      "title": "We cache decisions, not responses - does this solve your cost problem?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qp3lp3/we_cache_decisions_not_responses_does_this_solve/",
      "author": "llm-60",
      "created_utc": "2026-01-28 06:21:30",
      "score": 10,
      "num_comments": 14,
      "upvote_ratio": 0.86,
      "text": "Quick question for anyone running AI at scale:\n\nTraditional caching stores the response text. So \"How do I reset my password?\" gets cached, but \"I forgot my password\" is a cache miss - even though they need the same answer.\n\nWe flip this: cache the **decision** (what docs to retrieve, what action to take), then generate fresh responses each time.\n\nResult: 85-95% cache hit rate vs 10-30% with response caching.\n\n**Example:**\n\n* \"Reset my password\" ‚Üí decision: fetch docs \\[45, 67\\]\n* \"I forgot my password\" ‚Üí same decision, cache hit\n* \"Can't log in\" ‚Üí same decision, cache hit\n* All get personalized responses, not copied text\n\n**Question: If you're spending $2K+/month on LLM APIs for repetitive tasks (support, docs, workflows), would this matter to you?**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qp3lp3/we_cache_decisions_not_responses_does_this_solve/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o26v8xf",
          "author": "ruben_rrf",
          "text": "I get that you generate different outputs and cut the costs of having to make the tool calls and also the time. But how do you achieve a better cache rate? If I get it right...\n\nQuestion -> Actions -> Response\n\nIf you cache the Response, then you get a cache with Question -> Response, but if you cache the actions, you get a Question -> Actions cache, and then you use the model as \\[Question, Actions\\] -> Response.\n\nBut the key on the cache wouldn't be the same?",
          "score": 4,
          "created_utc": "2026-01-28 10:22:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26vsqv",
              "author": "llm-60",
              "text": "We don't cache the question, we cache the¬†**normalized intent**.\n\nWe extracts the \"meaning\" first:  \n  \n\"What's your return policy?\" - intent: return\\_policy  \n\"Can I return stuff?\" - intent: return\\_policy  \n\"How do returns work?\" - intent: return\\_policy\n\nand it also learn the context to fit the answer later...\n\nThree different questions, same cache key = cache hit.\n\nThat's how we get 80%",
              "score": 2,
              "created_utc": "2026-01-28 10:27:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2cgwn4",
          "author": "pbalIII",
          "text": "Intent normalization is doing the heavy lifting here. Most semantic cache implementations use embedding similarity directly on the query, which means you're still sensitive to phrasing variance even with cosine thresholds.\n\nCaching the decision output (retrieval path, action type) instead of the response is cleaner in theory... but you've moved the problem upstream. Now your intent extractor becomes the cache key generator, and any drift in how it normalizes inputs breaks your hit rate.\n\nMulti-intent queries are where this gets tricky. Something like a user forgetting their password and wanting to change their email maps to two decisions. The decomposition step either needs its own cache layer or you end up recomputing the split every time.",
          "score": 2,
          "created_utc": "2026-01-29 03:23:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d3ttn",
              "author": "llm-60",
              "text": "Great observations. You're right - intent extraction is doing the heavy lifting, and that's intentional.\n\n**On drift:**¬†Valid concern. We handle this with versioned extraction models + policy rules as fallbacks. If the extractor changes, old cache keys naturally expire (TTL). You can also monitor extraction confidence and invalidate cache when you update the model. Not perfect, but manageable.\n\n**On multi-intent queries:**¬†You're absolutely right - this is a known limitation. \"Reset password AND change email\" currently goes to low confidence ‚Üí bypasses cache ‚Üí escalates.\n\nFor v1, we're targeting single-intent policy decisions (returns, approvals, routing). Multi-intent decomposition is on the roadmap (Phase 2), likely with its own caching layer as you suggest.\n\nThe trade-off: Embedding similarity gives you \\~30-40% hit rates with fuzzy matching. Intent extraction gives 80%+ when queries fit the pattern, but breaks on edge cases. We're betting that most high-volume use cases (support, returns, routing) are single-intent dominant.",
              "score": 1,
              "created_utc": "2026-01-29 05:56:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2gbjd7",
                  "author": "pbalIII",
                  "text": "Versioned extractors with TTL is a clean solve for drift. The confidence threshold routing you described maps well to what I've seen in production semantic caches... the 0.8% false-positive rate most systems report happens exactly at those threshold boundaries where similarity is just above cutoff but intent diverges slightly.\n\nCurious about the 80%+ hit rate claim. Recent benchmarks on ensemble embedding approaches show 92% for semantically equivalent queries, but that's with careful threshold tuning per query type. Are you seeing 80%+ out of the box, or does that assume some domain-specific calibration?\n\nThe single-intent constraint is probably the right call for v1. Multi-intent decomposition adds a lot of surface area for edge cases, and most high-volume support flows are indeed single-intent dominant.",
                  "score": 1,
                  "created_utc": "2026-01-29 18:08:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27t0lu",
          "author": "SpecialBeatForce",
          "text": "Couldn‚Äòt you just use semantic caching question->answer if questions like reset password and forgot password are close enough semantically?",
          "score": 1,
          "created_utc": "2026-01-28 14:10:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27un18",
              "author": "llm-60",
              "text": "Traditional semantic caching caches the entire answer, so everyone gets the same response.\n\n**Example:**  \n  \n\"Forgot password\" - cached: \"Click the reset link in your email\"  \n\"Reset my password\" - cached: \"Click the reset link in your email\"\n\nWe cache the decision (what to do), then personalize the response.\n\n**Example:**  \n\"I'm John, forgot password\" - Decision cached: \"send reset email\"  Response: \"Hi John, we sent you a reset link\"  \n\"Sarah needs reset\" -Same cached decision - Response: \"Hi Sarah, we sent you a reset link\"\n\nOne LLM call for the logic, cheap model personalizes each response. You can't do that if you cache the full answer.",
              "score": 3,
              "created_utc": "2026-01-28 14:18:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o28fjzm",
                  "author": "SpecialBeatForce",
                  "text": "Okay i like the ideaüòä but i guess it comes down to a decision between personalized answers and saving compute?",
                  "score": 1,
                  "created_utc": "2026-01-28 15:56:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2bgmt3",
                  "author": "CourtsDigital",
                  "text": "i‚Äôm not sure i understand this use case. maybe provide some examples that require personalization. i‚Äôve never expected to receive a password reset email that‚Äôs tailored to me, or to hear about a store return policy that mentions me by name\n\ni agree with BeatForce that this seems almost exactly like semantic caching, with an additional, unnecessary LLM cost\n\ni‚Äôm not saying this couldn‚Äôt be useful, but if you intend to sell it for $1k+ per month then the use case(s) should be solid",
                  "score": 1,
                  "created_utc": "2026-01-29 00:06:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2d7yto",
          "author": "Khade_G",
          "text": "Yeah this would matter to anyone actually paying the bill. What you‚Äôre describing sounds like semantic / policy caching, and it‚Äôs way more aligned with how real systems behave than response caching. Most production queries don‚Äôt differ in intent, they differ in phrasing, tone, or user context. Caching text throws all that signal away; caching the decision preserves it.\n\nThe big wins I‚Äôve seen with this approach are much higher cache hit rates, fresh/personalized responses without re-doing expensive reasoning, and cleaner separation between ‚Äúunderstand the problem‚Äù and ‚Äúsay the answer‚Äù\n\nThe main things to watch out for are:\n- Decision drift: if your retrieval or routing logic changes, you need a clean way to invalidate or version the decision cache.\n- Over-generalization: making sure different intents don‚Äôt collapse into the same decision accidentally.\n- Debuggability: being able to explain why two queries mapped to the same decision.\n\nBut for support, docs, and workflow-heavy systems this is definitely the direction things are going. Once you cross ~$1‚Äì2k/month, optimizing reasoning reuse matters way more than token shaving. If you can make the cache safe and observable then this is a no-brainer.",
          "score": 1,
          "created_utc": "2026-01-29 06:29:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d8jhq",
              "author": "llm-60",
              "text": "Appreciate this - you nailed the trade offs. We're addressing those exact concerns:\n\n* Decision drift: TTL-based expiry + policy versioning\n* Over-generalization: Confidence gating (low confidence - bypass cache)\n* Debuggability: Dashboard shows canonical state extraction + cache hit/miss audit trail\n\nAlready seeing 75% hit rates with policy based workloads on simulations and some test users.",
              "score": 2,
              "created_utc": "2026-01-29 06:33:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2d8t3j",
                  "author": "Khade_G",
                  "text": "Good stuff!",
                  "score": 1,
                  "created_utc": "2026-01-29 06:36:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qpfnym",
      "title": "I stopped manually iterating on my agent prompts: I built an open-source system that extracts prompt improvements from my agent traces",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qpfnym/i_stopped_manually_iterating_on_my_agent_prompts/",
      "author": "cheetguy",
      "created_utc": "2026-01-28 16:13:28",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 0.75,
      "text": "Some of you might remember my [post about ACE](https://reddit.com/r/LangChain/comments/1p35tko/your_local_llm_agents_can_be_just_as_good_as/) about my open-source implementation of ACE (Agentic Context Engineering). ACE is a framework that makes agents learn from their own execution feedback without fine-tuning.\n\nI've now built a specific application: **agentic system prompting** that does offline prompt optimization from agent traces (e.g. from LangSmith)\n\n**Why did I build this?**\n\nI kept noticing my agents making the same mistakes across runs. I fixed it by digging through traces, figure out what went wrong, patch the system prompt, repeat. It works, but it's tedious and didn't really scale.\n\nSo I built a way to automate this. You feed ACE your agent's execution traces, and it extracts actionable prompt improvements automatically.\n\n**How it works:**\n\n1. **ReplayAgent** \\- Simulates agent behavior from recorded conversations (no live runs)\n2. **Reflector** \\- Analyzes what succeeded/failed, identifies patterns\n3. **SkillManager** \\- Transforms reflections into atomic, actionable strategies\n4. **Deduplicator** \\- Consolidates similar insights using embeddings\n5. **Skillbook** \\- Outputs human-readable recommendations with evidence\n\n**Each insight includes:**\n\n* Prompt suggestion - the actual text to add to your system prompt\n* Justification - why this change would help based on the analysis\n* Evidence - what actually happened in the trace that led to this insights\n\n**Try it yourself**   \n[https://github.com/kayba-ai/agentic-context-engine/tree/main/examples/agentic-system-prompting](https://github.com/kayba-ai/agentic-context-engine/tree/main/examples/agentic-system-prompting)\n\nWould love to hear if anyone tries this with their agents!",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1qpfnym/i_stopped_manually_iterating_on_my_agent_prompts/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2g798y",
          "author": "pbalIII",
          "text": "Trace-to-prompt pipelines are getting crowded fast. DSPy's MIPROv2 does bootstrap optimization, GEPA does evolutionary reflection, and ACE (the Stanford/SambaNova paper) does incremental playbook edits. All three extract patterns from execution traces... the difference is what happens next.\n\nDSPy needs structured I/O pairs. GEPA mutates prompt text directly and uses Pareto frontiers to keep diverse variants. ACE maintains a living context doc with delta edits so you don't get the brevity bias problem where insights get summarized away.\n\nThe 8 hours/week manual pattern analysis that u/KitchenSomew mentions is real. Curious whether your deduplicator handles semantic drift over time... embeddings cluster well initially but the similarity threshold that works at 100 traces often breaks at 1000.",
          "score": 2,
          "created_utc": "2026-01-29 17:49:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a3fbe",
          "author": "caprica71",
          "text": "How is this different from dspy?",
          "score": 1,
          "created_utc": "2026-01-28 20:17:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2afqgi",
              "author": "cheetguy",
              "text": "DSPy works best with structured input/output pairs, ACE works on raw traces (conversation logs, markdown) so no restructuring needed. DSPy auto-optimizes while ACE generates suggestions with evidence for you to review first. Think of DSPy for pipelines with clear metrics, ACE for learning from messy agent failures.",
              "score": 2,
              "created_utc": "2026-01-28 21:11:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o28mh9b",
          "author": "KitchenSomew",
          "text": "\\*\\*Production Agent Experience:\\*\\*\n\n\n\nBuilt chatbots for 50+ B2B clients - prompt drift is one of the hardest problems to catch early. Your ACE approach solves a massive pain point.\n\n\n\n\\*\\*What Resonates:\\*\\*\n\n\n\n‚úì Trace-based learning vs manual iteration (saves weeks of debugging)\n\n‚úì Offline optimization (no live experiments on customers)\n\n‚úì Embedding-based deduplication (critical at scale)\n\n\n\n\\*\\*Questions from Production:\\*\\*\n\n\n\n1. \\*\\*Token Cost:\\*\\* How expensive is running ReplayAgent + Reflector on 100+ conversations? Is it viable for startups?\n\n\n\n2. \\*\\*Prompt Versioning:\\*\\* Do you version the Skillbook outputs? We've had cases where a \"good\" prompt change broke edge cases 2 weeks later.\n\n\n\n3. \\*\\*Confidence Scoring:\\*\\* Does ACE rate how confident it is in each recommendation? Some patterns need 50+ traces to be statistically significant.\n\n\n\n\\*\\*Our Workflow (manual):\\*\\*\n\n\\`\\`\\`python\n\n\\# What we do now (tedious):\n\n1. Export LangSmith traces weekly\n\n2. Filter failures (user retry, escalation)\n\n3. Manual pattern analysis\n\n4. Prompt A/B test (3-7 days)\n\n5. Repeat\n\n\\`\\`\\`\n\n\n\nACE automating steps 2-3 would save \\~8 hours/week per agent.\n\n\n\n\\*\\*Pro Tip:\\*\\* For anyone trying this - start with failure-only traces. Analyzing successful runs adds noise early on.\n\n\n\nDoes ACE handle multi-agent systems? Curious if it can trace decisions across agent handoffs.",
          "score": -2,
          "created_utc": "2026-01-28 16:26:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq0pdf",
      "title": "I built a RAG backend for non-developers who just want a simple chatbot",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qq0pdf/i_built_a_rag_backend_for_nondevelopers_who_just/",
      "author": "Unlikely_Outcome4432",
      "created_utc": "2026-01-29 06:12:04",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 0.79,
      "text": "Hey r/LangChain,\n\nI'm a PM who became a \"vibe coder\" ‚Äì I can read code and tweak things, but I'm not a traditional developer.\n\nWhile working as a freelancer on RAG chat services, I noticed something: a lot of people wanted to build simple RAG chatbots for non-commercial use, but the existing tools felt overwhelming for them.\n\nInstead of building custom chatbots for each person, I thought: \"What if I made a tool where you just change a config file and get a working RAG backend?\"\n\n\n\n**So I built OneRAG.**\n\n**The idea is simple:**\n\n\\- Want to switch from Chroma to Pinecone? Change one line in config.\n\n\\- Want to try Claude instead of GPT? Change one line.\n\n\\- Want to add a reranker? One line.\n\n\n\nIt uses dependency injection, so you don't need to rewrite code ‚Äì just swap components.\n\n\n\n**Currently supports:**\n\n\\- 6 Vector DBs (Chroma, Pinecone, Weaviate, Qdrant, pgvector, MongoDB)\n\n\\- 4 LLMs (OpenAI, Claude, Gemini, OpenRouter)\n\n\\- Rerankers, caching, Korean NLP optimization\n\nIt's not meant to replace LangChain for complex pipelines. It's for people who just want a working RAG backend without the learning curve.\n\nGitHub: [https://github.com/notaDev-iamAura/OneRAG](https://github.com/notaDev-iamAura/OneRAG)\n\nWould love feedback from this community ‚Äì what features would make this more useful for beginners?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qq0pdf/i_built_a_rag_backend_for_nondevelopers_who_just/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2l5j23",
          "author": "ich3ckmat3",
          "text": "Sweet!",
          "score": 1,
          "created_utc": "2026-01-30 11:49:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31odim",
          "author": "ampancha",
          "text": "Config-as-architecture is powerful, but each \"one line\" swap is an unvalidated change to the system's security and cost posture. When a non-developer switches from Chroma to Pinecone or from GPT to Claude, nothing validates that API keys are scoped correctly, that spend limits still apply, or that the new provider's auth model is handled. A config validation layer that enforces secrets isolation, token caps, and provider-specific defaults before runtime would make this production-safe, not just production-easy. Sent you a DM.",
          "score": 1,
          "created_utc": "2026-02-01 21:48:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33qosu",
              "author": "Unlikely_Outcome4432",
              "text": "Thanks for the feedback. You're right that config swaps aren't validated at runtime.\n\nThat said, OneRAG is designed for personal/non-commercial use, not production enterprise deployments. Users are expected to configure their own API keys and reference each provider's docs.\n\nAdding clearer documentation on security considerations per provider is a good idea though ‚Äì I'll put it on the roadmap.",
              "score": 1,
              "created_utc": "2026-02-02 04:49:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qsxuum",
      "title": "Is AsyncPostgresSaver actually production-ready in 2026? (Connection pooling & resilience issues)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qsxuum/is_asyncpostgressaver_actually_productionready_in/",
      "author": "FunEstablishment5942",
      "created_utc": "2026-02-01 13:00:46",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nI'm finalizing the architecture for a production agent service and blocked on the database layer. I've seen multiple reports (and GitHub issues like #5675 and #1730) from late 2025 indicating that¬†`AsyncPostgresSaver`¬†is incredibly fragile when it comes to connection pooling.\n\nSpecifically, I'm concerned about:\n\n1. **Zero Resilience:**¬†If the underlying pool closes or a connection goes stale, the saver seems to just crash with¬†`PoolClosed`¬†or¬†`OperationalError`¬†rather than attempting a retry or refresh.\n2. **Lifecycle Management:**¬†Sharing a¬†`psycopg_pool`¬†between my application (SQLAlchemy) and LangGraph seems to result in race conditions where LangGraph holds onto references to dead pools.\n\n**My Question:**  \nHas anyone successfully deployed¬†`AsyncPostgresSaver`¬†in a high-load production environment recently (early 2026)? Did the team ever release a native fix for automatic retries/pool recovery, or are you all still writing custom wrappers / separate pool managers to baby the checkpointer?\n\nI'm trying to decide if I should risk using the standard saver or just bite the bullet and write a custom Redis/Postgres implementation from day one.\n\nThanks! Is AsyncPostgresSaver actually production-ready in 2026? (Connection pooling & resilience issues)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qsxuum/is_asyncpostgressaver_actually_productionready_in/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2zq0ut",
          "author": "papipapi419",
          "text": "!remindme 5 days",
          "score": 1,
          "created_utc": "2026-02-01 16:20:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zq60m",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 5 days on [**2026-02-06 16:20:31 UTC**](http://www.wolframalpha.com/input/?i=2026-02-06%2016:20:31%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LangChain/comments/1qsxuum/is_asyncpostgressaver_actually_productionready_in/o2zq0ut/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLangChain%2Fcomments%2F1qsxuum%2Fis_asyncpostgressaver_actually_productionready_in%2Fo2zq0ut%2F%5D%0A%0ARemindMe%21%202026-02-06%2016%3A20%3A31%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qsxuum)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-01 16:21:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o33q9hi",
          "author": "Shreyanak_exe",
          "text": "I really need an answer to this cause I am building stg similar that's going in production soon and I want to be sure the connection doesn't goes stale every now and then. \n\nBtw: some guy built a resilient wrapper for Postgres. If you can test and lmk if it's worth giving a shot, that'd be helpful\n\n[ResilientPostgresSaver](https://github.com/samirpatil2000/agentic-template/blob/main/agents/resilient_postgres_saver.py)",
          "score": 1,
          "created_utc": "2026-02-02 04:46:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr6mii",
      "title": "Production AI Agent Patterns - Open-source guide with cost analysis and case studies",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qr6mii/production_ai_agent_patterns_opensource_guide/",
      "author": "Curious_Mirror2794",
      "created_utc": "2026-01-30 14:16:13",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 0.79,
      "text": "Hey r/LangChain,\n\n\n\nI've been building production AI agents for the past year and kept running into the same problems: unclear pattern selection, unexpected costs, and lack of production-focused examples.\n\n\n\nSo I documented everything I learned into a comprehensive guide and open-sourced it.\n\n\n\n\\*\\*What's inside:\\*\\*\n\n\n\n\\*\\*8 Core Patterns:\\*\\*\n\n\\- Tool calling, ReAct, Chain-of-Thought, Sequential chains, Parallel execution, Router agents, Hierarchical agents, Feedback loops\n\n\\- Each includes \"When to use\" AND \"When NOT to use\" sections (most docs skip the latter)\n\n\\- Real cost analysis for each pattern\n\n\n\n\\*\\*4 Real-World Case Studies:\\*\\*\n\n\\- Customer support agent (Router + Hierarchical): 73% cost reduction\n\n\\- Code review agent (Sequential + Feedback): 85% issue detection  \n\n\\- Research assistant (Hierarchical + Parallel): 90% time savings\n\n\\- Data analyst (Tool calling + CoT): SQL from natural language\n\n\n\nEach case study includes before/after metrics, architecture diagrams, and full implementation details.\n\n\n\n\\*\\*Production Engineering:\\*\\*\n\n\\- Memory architectures (short-term, long-term, hybrid)\n\n\\- Error handling (retries, circuit breakers, graceful degradation)\n\n\\- Cost optimization (went from $5K/month to $1.2K)\n\n\\- Security (prompt injection defense, PII protection)\n\n\\- Testing strategies (LLM-as-judge, regression testing)\n\n\n\n\\*\\*Framework Comparisons:\\*\\*\n\n\\- LangChain vs LlamaIndex vs Custom implementation\n\n\\- OpenAI Assistants vs Custom agents\n\n\\- Sync vs Async execution\n\n\n\n\\*\\*What makes it different:\\*\\*\n\n\\- Production code with error handling (not toy examples)\n\n\\- Honest tradeoff discussions\n\n\\- Real cost numbers ($$ per 10K requests)\n\n\\- Framework-agnostic patterns\n\n\\- 150+ code examples, 41+ diagrams\n\n\n\n\\*\\*Not included:\\*\\* Basic prompting tutorials, intro to LLMs\n\n\n\nThe repo is MIT licensed, contributions welcome.\n\n\n\n\\*\\*Questions I'm hoping to answer:\\*\\*\n\n1. What production challenges are you facing with LangChain agents?\n\n2. Which patterns have worked well for you?\n\n3. What topics should I cover in v1.1?\n\n\n\nLink: [https://github.com/devwithmohit/ai-agent-architecture-patterns](https://github.com/devwithmohit/ai-agent-architecture-patterns)\n\n\n\nHappy to discuss any of the patterns or case studies in detail.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qr6mii/production_ai_agent_patterns_opensource_guide/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2naepr",
          "author": "Southern_Notice9262",
          "text": "I suspect I‚Äôm arguing about an LLM slop product here but I can‚Äôt leave this without a comment.\n\n03-comparisons/openai-assistants-vs-custom-agents.md:\nYou are still recommending Assistants API which is to be sunset in July 2026.\nThis says a lot about your expertise (lack thereof) to me.\n\n03-comparisons/langchain-vs-llamaindex-vs-custom.md:\nA nitpick: there are frameworks other than Langchain and LlamaIndex. Where is CrewAI, Vercel and Google AI SDKs (and probably dozens more I know nothing about)? I would assume they deserve at least to be named.\n\n02-production/observability.md:\nA nitpick: Where are Langfuse, Arize and other SPECIALIZED solutions that don‚Äôt require so much code and give much more in terms of observability?\n\n04-case-studies/code-review-agent.md:\nBefore I close this repo forever just wanted to make sure you ignore linting rules in your code. And you didn‚Äôt let me down, you ignore them alright! üòÅ\n\nPlease do better.",
          "score": 2,
          "created_utc": "2026-01-30 18:15:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qreln",
              "author": "Curious_Mirror2794",
              "text": "Thanks for taking the time to review the repo and provide feedback. I genuinely appreciate the specific callouts‚Äîthis is exactly the kind of detailed input that makes documentation better.\n\n**Re: OpenAI Assistants API sunset** You're absolutely right. I wasn't aware of the July 2026 deprecation timeline when I wrote this. I'll add a deprecation notice at the top of that comparison and update the guidance to reflect that this is historical context rather than a current recommendation.\n\n**Re: Missing frameworks (CrewAI, Vercel AI SDK, Google AI SDK)** Fair point. The comparison focused on the \"big two\" but you're right that the ecosystem has evolved significantly. I'll add a section covering CrewAI, Vercel AI SDK, Google AI SDK, and others in the framework comparison, even if just as a reference table with brief descriptions.\n\n**Re: Observability tools (Langfuse, Arize)** Valid criticism. I leaned too heavily on \"build it yourself\" examples when there are production-ready observability platforms designed specifically for LLM apps. I'll add a dedicated section on specialized observability solutions and reorganize the content to lead with these tools before diving into custom implementations.\n\n**Re: Linting in** [**code-review-agent.md**](http://code-review-agent.md) Ha! The irony isn't lost on me. You're right‚Äîthe code examples should follow the same standards the agent would enforce. I'll clean up the code samples to be properly linted.\n\nThese are all legitimate technical critiques, not nitpicks. I'll push updates addressing each point within the week. If you're willing to review again after the changes, I'd welcome it.\n\nCheers, Mohit",
              "score": 0,
              "created_utc": "2026-01-31 05:30:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qq8v85",
      "title": "Why email context is way harder than document RAG",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qq8v85/why_email_context_is_way_harder_than_document_rag/",
      "author": "EnoughNinja",
      "created_utc": "2026-01-29 13:41:51",
      "score": 7,
      "num_comments": 9,
      "upvote_ratio": 0.77,
      "text": "I've been seeing a lot of posts on Reddit and other forums about connecting agents to Gmail or making \"email-aware\" assistants.\n\nI don't think it's obvious why this is much harder than document RAG until you're deep into it, so here's my breakdown.\n\n**1. Threading isn‚Äôt linear**  \nEmail threads aren‚Äôt clean sequences. You‚Äôve got nested quotes, forwards inside forwards, and inline replies that break sentences in half. Standard chunking strategies fall apart because boundaries aren‚Äôt real. You end up retrieving fragments that are meaningless on their own.\n\n**2. ‚ÄúWho said what‚Äù actually matters**  \nWhen someone asks ‚Äúwhat did they commit to?‚Äù, you have to separate their words from text they quoted from someone else. Embeddings optimize for semantic similarity, rather than for authorship or intent. \n\n**3. Attachments are their own problem**  \nPDFs need OCR. and images need processing, and also Calendar invites are structured objects. Often the real decision lives in the attachment, not the email body, but each type wants a different pipeline.\n\n**4. Permissions break naive retrieval**  \nIn multi-user systems, relevance isn‚Äôt enough. User A must never see User B‚Äôs emails, even if they‚Äôre semantically perfect matches. Vector search doesn‚Äôt care about access control unless you‚Äôre very deliberate.\n\n**5. Recency and role interact badly**  \nThe latest message might just be ‚ÄúThanks!‚Äù while the actual answer is found eight messages back. But you also can‚Äôt ignore recency, because the context does shift over time.\n\nRAG works well for documents because documents are self-contained, but email threads are relational and so the meaning lives in the connections between messages.\n\nThis is the problem we ended up building [iGPT](https://www.igpt.ai/) around.\n\nHappy to talk through edge cases or trade notes if anyone else is wrestling with this.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qq8v85/why_email_context_is_way_harder_than_document_rag/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2euxsg",
          "author": "PAChilds",
          "text": "You certainly nailed the issues with email. \n\nEmail also has a time element critical to investigations. The headers include a raft of metadata also of use to investigations. Finally any differences in the display names associated with a specific email address can indicate an intent to deceive or sidebar conversation between a subset of copied parties.",
          "score": 2,
          "created_utc": "2026-01-29 14:06:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f1x93",
          "author": "Lumpy-Comedian-1027",
          "text": "Definitively a complex challenge. But i really don't want to use a SaaS for this. But a library that solves the threading issue would be great, even if it is a bit simplistic - people usually just put their answer on the top of the last mail.",
          "score": 2,
          "created_utc": "2026-01-29 14:42:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2i3yzg",
              "author": "jsjoana",
              "text": "For sure! A library that helps with threading could save a lot of headaches. Even a simple solution that just consolidates replies would be a game changer. It‚Äôs wild how much context gets lost in those nested conversations.",
              "score": 2,
              "created_utc": "2026-01-29 23:16:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2y7kpo",
                  "author": "Lumpy-Comedian-1027",
                  "text": "now is anyone aware of such a library? couldn't find much on github, but i also didn't look long",
                  "score": 1,
                  "created_utc": "2026-02-01 10:37:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gb7n9",
          "author": "pbalIII",
          "text": "Inbox RAG goes sideways if you treat each message as standalone. Meaning lives in reply structure and who was in the room, drop that and you'll keep pulling the wrong slice. Parse the tree from headers, keep quote depth and inline edits as annotations, and store decisions, commitments, owners as fields. Filter by participant before vector search so perms and relevance stay tied.",
          "score": 2,
          "created_utc": "2026-01-29 18:07:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kaz13",
          "author": "R-4553",
          "text": "Input compression could be interesting to try with this use case although I'd might want to protect some parts of the input from compression",
          "score": 2,
          "created_utc": "2026-01-30 07:22:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fc42w",
          "author": "Trawling_",
          "text": "Why wouldn‚Äôt someone just use copilot for outlook on m365?\n\nHave you compared the two against a defined baseline of tests? What problem are you solving and for who?",
          "score": 1,
          "created_utc": "2026-01-29 15:30:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y0fpu",
              "author": "EnoughNinja",
              "text": "They solve very different problems.\n\nCopilot is an end-user tool which helps you summarize your inbox inside Outlook whic is useful for personal productivity, but it stops there.\n\niGPT is infrastructure, it's an API that lets developers build email intelligence into their own products and agents. \n\nSo if you're a SaaS company that needs to extract tasks, owners, deadlines, or sentiment from your users' email threads and pipe that into your app, CRM, or automation workflow, Copilot doesn't help you, you can't call Copilot from your codebase and get structured intelligence back.",
              "score": 1,
              "created_utc": "2026-02-01 09:31:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2z7epo",
          "author": "thatguyinline",
          "text": "Use a graph. Try lightrag.",
          "score": 1,
          "created_utc": "2026-02-01 14:50:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtf5gc",
      "title": "I built a CLI to find \"Zombie Vectors\" in Pinecone/Weaviate (and estimate how much RAM you're wasting)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qtf5gc/i_built_a_cli_to_find_zombie_vectors_in/",
      "author": "billycph",
      "created_utc": "2026-02-01 23:57:50",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "Hey everyone,\n\nI‚Äôm an ex-AWS S3 engineer. In my previous life, we obsessed over \"Lifecycle Policies\" because storing petabytes of data is expensive. If data wasn‚Äôt touched in 30 days, we moved it to cold storage.\n\nI noticed a weird pattern in the AI space recently: **We are treating Vector Databases like cold storage.**\n\nWe shove 100% of our embeddings into expensive Hot RAM (Pinecone, Milvus, Weaviate), even though for many use cases (like Chat History or Seasonal Catalog Search), 90% of that data is rarely queried after a month. It‚Äôs like keeping your tax returns from 1990 in your wallet instead of a filing cabinet.\n\nI wanted to see exactly how much money was being wasted, so I wrote a simple open-source CLI tool to audit this.\n\n**What it does:**\n\n1. **Connects** to your index (Pinecone currently supported).\n2. **Probes** random sectors of your vector space to sample metadata.\n3. **Analyzes** the `created_at` or timestamp fields.\n4. **Reports** your \"Stale Rate\" (e.g., \"65% of your vectors haven't been queried in >30 days\") and calculates potential savings if you moved them to S3/Disk.\n\n**The \"Trust\" Part:** I know giving API keys to random tools is a bad idea.\n\n* This script runs **100% locally** on your machine.\n* Your keys never leave your terminal.\n* You can audit the code yourself (it‚Äôs just Python).\n\n**Why I built this:** I‚Äôm working on a larger library to automate the \"S3 Offloading\" process, but first I wanted to prove that the problem actually exists.\n\nI‚Äôd love for you to run it and let me know: **Does your stale rate match what you expected?** I‚Äôm seeing \\~90% staleness for Chat Apps and \\~15% for Knowledge Bases.\n\n**Repo here:** [https://github.com/billycph/VectorDBCostSavingInspector](https://github.com/billycph/VectorDBCostSavingInspector)\n\nFeedback welcome!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qtf5gc/i_built_a_cli_to_find_zombie_vectors_in/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qrsbfa",
      "title": "I am learning LangChain. Could anyone suggest some interesting projects I can build with it?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qrsbfa/i_am_learning_langchain_could_anyone_suggest_some/",
      "author": "Cautious_Ad691",
      "created_utc": "2026-01-31 04:19:06",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 0.72,
      "text": "",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qrsbfa/i_am_learning_langchain_could_anyone_suggest_some/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2qzwrb",
          "author": "SiteCharacter428",
          "text": "If you‚Äôre a beginner, start by building a basic **RAG chatbot**.\n\nTry including a **web search tool**, **document parsing**, **image parsing**, and a **vector database** for retrieval. This gives you hands-on experience with the full LLM workflow.\n\nIf you want something more interesting, you can build a **Health Bot** where users upload medical documents or images and the system processes that data to provide context-aware answers.\n\nTip: **Mistral OCR** works surprisingly well for medical images and handwritten doctor notes compared to many other OCR tools.",
          "score": 3,
          "created_utc": "2026-01-31 06:39:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vw08o",
              "author": "Jorsoi13",
              "text": "Great ideas !:)",
              "score": 2,
              "created_utc": "2026-02-01 00:23:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ty2mq",
          "author": "thitcho226",
          "text": "inbox meme",
          "score": 2,
          "created_utc": "2026-01-31 18:29:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34esoi",
          "author": "PretendPop4647",
          "text": "LangChain Academy offers courses; you can follow their guidelines. First start with langchain, then langgraph. \nWhen you learn LangChain or build a project, try to trace LLM.  Use Langsmith for tracing.\n\nBtw they recently introduced a package deepagent,  It is designed to create autonomous agents capable of long-horizon planning and complex task execution like claude code / manus ai.\n\nI built a Job search agent using deepagent.  \n\nYou can check it out >  https://github.com/Rahat-Kabir/job-search-agent",
          "score": 1,
          "created_utc": "2026-02-02 08:09:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp2yk2",
      "title": "Advice on Consistent Prompt Outputs Across Multiple LLMs in LangChain",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qp2yk2/advice_on_consistent_prompt_outputs_across/",
      "author": "NoEntertainment8292",
      "created_utc": "2026-01-28 05:47:02",
      "score": 6,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hi all, I‚Äôm experimenting with building multi-LLM pipelines using LangChain and trying to keep outputs consistent in **tone, style, and intent** across different models.\n\nHere‚Äôs a simplified example prompt I‚Äôm testing:\n\n    You are an AI assistant. Convert this prompt for {TARGET_MODEL} while keeping the original tone, intent, and style intact.\n    \n    Original Prompt: \"Summarize this article in a concise, professional tone suitable for LinkedIn.\"\n\n**Questions for the community:**\n\n* How would you structure this in a LangChain `LLMChain` or `SequentialChain` to reduce interpretation drift?\n* Are there techniques for preserving tone and formatting across multiple models?\n* Any tips for chaining multi-turn prompts while maintaining consistency?\n\nI‚Äôd love to see how others handle **cross-model consistency in LangChain pipelines**, or any patterns you‚Äôve used.",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qp2yk2/advice_on_consistent_prompt_outputs_across/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o27e66m",
          "author": "Upset-Pop1136",
          "text": "we solved this by forcing a canonical JSON schema + a final ‚Äústyle normalizer‚Äù pass on one model. don‚Äôt fight every model, collapse outputs late. ",
          "score": 1,
          "created_utc": "2026-01-28 12:47:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cin4c",
              "author": "NoEntertainment8292",
              "text": "That makes sense! Collapsing late feels cleaner than over-constraining each step. Do you keep the schema purely semantic (content + intent) and let the style normalizer handle tone entirely, or do you still encode style hints in the JSON? Also wondering how brittle this gets as you add more models to the pipeline?",
              "score": 1,
              "created_utc": "2026-01-29 03:34:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qo70un",
      "title": "I built an SEO Content Agent Team that optimizes articles for Google AI Search",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qo70un/i_built_an_seo_content_agent_team_that_optimizes/",
      "author": "Arindam_200",
      "created_utc": "2026-01-27 07:16:05",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I‚Äôve been working with multi-agent workflows and wanted to build something useful for real SEO work, so I put together an SEO Content Agent Team that helps optimize existing articles or generate SEO-ready content briefs before writing.\n\nThe system focuses on Google AI Search, including AI Mode and AI Overviews, instead of generic keyword stuffing.\n\nThe flow has a few clear stages:\n\n\\- Research Agent: Uses SerpAPI to analyze Google AI Mode, AI Overviews, keywords, questions, and competitors  \n\\- Strategy Agent: Clusters keywords, identifies search intent, and plans structure and gaps  \n\\- Editor Agent: Audits existing content or rewrites sections with natural keyword integration  \n\\- Coordinator: Agno orchestrates the agents into a single workflow\n\nYou can use it in two ways:\n\n1. Optimize an existing article from a URL or pasted content  \n2. Generate a full SEO content brief before writing, just from a topic\n\nEverything runs through a Streamlit UI with real-time progress and clean, document-style outputs. Here‚Äôs the stack I used to build it:\n\n\\- Agno for multi-agent orchestration  \n\\- Nebius for LLM inference  \n\\- SerpAPI for Google AI Mode and AI Overview data  \n\\- Streamlit for the UI\n\nAll reports are saved locally so teams can reuse them.\n\nThe project is intentionally focused and not a full SEO suite, but it‚Äôs been useful for content refreshes and planning articles that actually align with how Google AI surfaces results now.\n\nI‚Äôve shared a full walkthrough here: [Demo](https://www.youtube.com/watch?v=BZwgey_YeF0)  \nAnd the code is here if you want to explore or extend it: [GitHub Repo](https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/content_team_agent)\n\nWould love feedback on missing features or ideas to push this further.",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qo70un/i_built_an_seo_content_agent_team_that_optimizes/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o1z7y4f",
          "author": "Creepy-Row970",
          "text": "great project",
          "score": 1,
          "created_utc": "2026-01-27 07:34:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20bj68",
              "author": "Arindam_200",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-01-27 13:03:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zukyl",
          "author": "Easy_Cable6224",
          "text": "looking cool",
          "score": 1,
          "created_utc": "2026-01-27 11:00:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20birt",
              "author": "Arindam_200",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-01-27 13:03:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpk0tf",
      "title": "I built a virtual filesystem for AI agents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qpk0tf/i_built_a_virtual_filesystem_for_ai_agents/",
      "author": "velobro",
      "created_utc": "2026-01-28 18:42:56",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.78,
      "text": "Agents perform best when they have access to a computer. But the tools and integrations your agent needs are scattered across remote APIs and MCP servers.\n\nI built a virtual filesystem that puts everything your agent needs in a single folder on your computer. \n\nYour MCP servers become executables. Your integrations become directories. Everything your agent uses is literally just a file.\n\nTo use it, you just register your existing MCPs in a config file, which mounts them to a file system. This lets you interact with your remote tools like an ordinary unix binary:\n\n    /tmp/airstore/tools/wikipedia search \"albert\" | grep -i 'einstein'\n\nThe folder is virtualized, so you can mount it locally or use it in a sandboxed environment.¬†\n\n**Why this matters**\n\nThe best agents rely heavily on the filesystem for storing and managing context. LLMs are already great at POSIX, and it‚Äôs easier for an LLM to run a binary than call a remote MCP server. By putting your agent‚Äôs tools behind a filesystem, you get a standardized interface for agents to interact with everything, which means that your agents will perform better in the real world.\n\n**How it works**\n\nJust add your existing MCP servers to a config file, and we convert each tool into a binary that your agents can use. For example:\n\n    $ ls /tmp/airstore/tools/ \n    \n    gmail\n    github \n    wikipedia \n    filesystem \n    memory\n\nThen you (or Claude Code) can use them like any CLI tool:\n\n    $ /tmp/airstore/tools/github list-issues --repo=acme/api | jq '.[0].title'\n\n**Github**: [https://github.com/beam-cloud/airstore](https://github.com/beam-cloud/airstore)\n\nWould love to hear any feedback, or if anyone else has thought about these problems as well. ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qpk0tf/i_built_a_virtual_filesystem_for_ai_agents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2o05s0",
          "author": "transfire",
          "text": "All you need is bash.",
          "score": 1,
          "created_utc": "2026-01-30 20:10:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qol9sp",
      "title": "GraphRAG vs LangGraph agents for codebase visualization ‚Äî which one should I use?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qol9sp/graphrag_vs_langgraph_agents_for_codebase/",
      "author": "Dizzy-Item-7123",
      "created_utc": "2026-01-27 17:59:58",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 0.73,
      "text": "I‚Äôm building an app that visualizes and queries an entire codebase.\n\nStack:\nDjango backend\nLangChain for LLM integration\n\nI want to avoid hallucinations and improve accuracy. I‚Äôm exploring:\n\nGraphRAG (to model file/function/module relationships)\nLangGraph + ReAct agents (for multi-step reasoning and tool use)\n\nNow I‚Äôm confused about the right architecture.\nQuestions:\n\nIf I‚Äôm using LangGraph agents, does GraphRAG still make sense?\n\nIs GraphRAG a replacement for agents, or a retrieval layer under agents?\n\nCan agents with tools parse and traverse a large codebase without GraphRAG?\n\nFor a codebase Q&A + visualization app, what‚Äôs the cleaner approach?\n\nLooking for advice from anyone who‚Äôs built code intelligence or repo analysis tools.",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qol9sp/graphrag_vs_langgraph_agents_for_codebase/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o28zxqv",
          "author": "Striking-Bluejay6155",
          "text": "Sharing a tool I think does what you‚Äôre describing with graphrag in the background and the ability to to chat: https://code-graph.falkordb.com/",
          "score": 1,
          "created_utc": "2026-01-28 17:25:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g88my",
          "author": "pbalIII",
          "text": "They're different layers, not alternatives. GraphRAG handles your retrieval... file/function/module relationships as a knowledge graph that your agent queries. LangGraph handles orchestration... how your agent reasons through multi-step tasks.\n\nThe pattern that's working in production: build your code graph (Neo4j, FalkorDB, Memgraph all have SDKs for this), then let your LangGraph agent query it as a tool. The agent decides what to look up, GraphRAG returns the relevant subgraph.\n\nWithout the graph structure, agents can still traverse codebases but they waste tokens re-discovering relationships. With it, you get pre-indexed connections so the agent jumps straight to relevant files.\n\nFor visualization specifically, the graph is doing double duty... feeding both your UI and your agent's retrieval.",
          "score": 1,
          "created_utc": "2026-01-29 17:54:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2l1l0o",
          "author": "Luneriazz",
          "text": "langraph is for rigid workflow... lets say every time you ask the agent must look at previous chat before searching into graphRAG. you use langgraph for something like that\n\nfor graphRAG, is for hierarchical knowledge query instead of nearest or similarity search",
          "score": 1,
          "created_utc": "2026-01-30 11:18:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnckty",
      "title": "Multi Agent system losing state + breaking routing. Stuck after days of debugging.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qnckty/multi_agent_system_losing_state_breaking_routing/",
      "author": "goodevibes",
      "created_utc": "2026-01-26 10:34:03",
      "score": 5,
      "num_comments": 5,
      "upvote_ratio": 0.86,
      "text": "Hey team üëãüèº, I‚Äôm building a multi-agent system that switches between different personas and connects to a legacy API using custom tools. I‚Äôve spent a few days deep in code and Ive run into some architectural issues and I‚Äôm hoping to get advice from anyone who‚Äôs dealt with similar problems.\n\nCouple of the main issues I‚Äôm trying to solve;\n\nThe system forgets what it‚Äôs doing when asking for confirmation\n\n\\- I‚Äôm trying to set up a flow where the agent proposes an action, asks for confirmation, then executes it. But the graph loses track of what action was pending between turns, so when I say ‚Äúyes,‚Äù it just treats it like normal conversation instead of confirming the action I was asked about.\n\nPersonas keep switching unexpectedly\n\n\\- I have different roles (like admin vs. field user) that the system switches between. But the router and state initialization seem to clash sometimes, causing the persona to flip back to the wrong one unexpectedly. It feels like there‚Äôs some circular state issue or the defaults are fighting each other, but I can‚Äôt for the life of me find them.\n\nTrouble passing context into tools\n\n\\- I need to inject things like auth tokens and user context when tools actually run. But this causes type errors because the tools aren‚Äôt expecting those extra arguments. I‚Äôm not sure what the clean pattern is for handling stateful context when the tools themselves are supposed to be stateless. This is relatively new for the projects I have been working on.\n\nThe legacy API is misleading\n\n\\- The API returns a 200 success code even when things actually fail (bad parameters, malformed XML, etc). Agents think everything worked when it didn‚Äôt, which makes debugging inside the graph really frustrating.\n\nWhat I‚Äôm hoping to find some solid advice on is;\n\n\\- Best way to debug why state gets wiped between nodes/turns\n\n\\- The standard pattern for propose ‚Üí confirm ‚Üí execute flows\n\n\\- How to make personas ‚Äústick‚Äù without conflicting with graph initialization\n\n\\- How others cleanly pass execution context into tools\n\nIf you‚Äôve built something similar, I‚Äôd really appreciate any pointers or heads-up about gotchas. I feel like I‚Äôm missing a few fundamental patterns and just going in circles at this point.‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã I‚Äôve watched a heap of YouTube guides etc, studied Dev docs but I feel like I‚Äôve hit a point where I‚Äôm going in circles üòÆ‚Äçüí®\n\nCheers :)",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qnckty/multi_agent_system_losing_state_breaking_routing/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o1tc1td",
          "author": "bzImage",
          "text": "Langgraph",
          "score": 1,
          "created_utc": "2026-01-26 13:29:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tnxd1",
          "author": "Tough-Permission-804",
          "text": "just use github agent via vs code.  it will help you get sorted",
          "score": 1,
          "created_utc": "2026-01-26 14:31:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21t3qv",
          "author": "kacxdak",
          "text": "I think it‚Äôs just a fundamental approach of how to think about agents. \n\nhttps://youtu.be/wD3zieaV0Yc?si=SVu-nJhiUmZ8nJ-S (Starting at 4:37)\n\nOnce you model agents and tool calling into traditional software (as opposed to new paradigms), controlling an agent becomes a lot easier.",
          "score": 1,
          "created_utc": "2026-01-27 17:17:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22nv9e",
          "author": "YUYbox",
          "text": "\nSharing a tool I built for anyone running multi-agent AI systems.\n\nThe problem: When LLMs talk to each other, they develop patterns that are hard to audit - invented acronyms, lost context, meaning drift.\n\nThe solution: InsAIts monitors these communications and flags anomalies.\n\nfrom insa_its import insAItsMonitor\n\nmonitor = insAItsMonitor()  # Free tier, no key needed\nmonitor.register_agent(\"agent_1\", \"gpt-4\")\n\nresult = monitor.send_message(\n    text=\"The QFC needs recalibration on sector 7G\",\n    sender_id=\"agent_1\"\n)\n\nif result[\"anomalies\"]:\n    print(\"Warning:\", result[\"anomalies\"])\n\nFeatures:\n- Local processing (sentence-transformers)\n- LangChain & CrewAI integrations\n- Adaptive jargon dictionary\n- Zero cloud dependency for detection\n\nGitHub: https://github.com/Nomadu27/InsAIts\nPyPI: pip install insa-its",
          "score": 1,
          "created_utc": "2026-01-27 19:29:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tz4l0",
          "author": "saurabhjain1592",
          "text": "You‚Äôre not missing a random trick, you‚Äôve hit a real architectural boundary that most agent frameworks don‚Äôt make explicit.\n\nAll four issues you describe stem from the same root problem: execution-critical state is implicit and conversational, not explicit and owned.\n\nIn propose ‚Üí confirm ‚Üí execute flows, the ‚Äúpending action‚Äù cannot live only in the LLM context. It needs to be a first-class execution object that survives turns, otherwise a simple ‚Äúyes‚Äù has no stable referent.\n\nPersona flipping is usually the same issue in disguise. Routing logic and initialization are both mutating shared state, so whichever runs last wins.\n\nTool context injection breaks because tools are treated as stateless functions, while the system actually needs scoped execution context (auth, role, intent) that is managed outside the tool signature.\n\nAnd legacy APIs returning 200 on failure is the worst case for agents, because success needs to be derived from semantic validation, not HTTP status.\n\nThe common pattern that helps is to separate:\n\n* conversational reasoning (LLM context)\n* execution state (what is pending, allowed, approved, failed)\n\nOnce those are decoupled, confirmation flows, personas, retries, and debugging become tractable again.\n\nYou‚Äôre not going in circles because you‚Äôre bad at this. You‚Äôre there because the abstractions stop short right where things become stateful and irreversible.",
          "score": 0,
          "created_utc": "2026-01-26 15:25:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqcthg",
      "title": "Persistent Architectural Memory cut our Token costs by ~55% and I didn‚Äôt expect it to matter this much",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qqcthg/persistent_architectural_memory_cut_our_token/",
      "author": "codes_astro",
      "created_utc": "2026-01-29 16:11:48",
      "score": 5,
      "num_comments": 9,
      "upvote_ratio": 0.62,
      "text": "We‚Äôve been using AI coding tools (Cursor, Claude Code) in production for a while now. Mid-sized team. Large codebase. Nothing exotic. But over time, our token usage kept creeping up, especially during handoffs. New dev picks up a task, asks a few ‚Äúwhere is X implemented?‚Äù types simple questions, and suddenly the agent is pulling half the repo into context.\n\nAt first we thought this was just the cost of using AI on a big codebase. Turned out the real issue was¬†*how context was rebuilt*.\n\nEvery query was effectively a cold start. Even if someone asked the same architectural question an hour later, the agent would:\n\n* run semantic search again\n* load the same files again\n* burn the same tokens again\n\nWe tried being disciplined with manual file tagging inside Cursor. It helped a bit, but we were still loading entire files when only small parts mattered. Cache hit rate on understanding was basically zero.\n\nThen we came across the idea of persistent architectural memory and ended up testing it in ByteRover. The mental model was simple; instead of caching¬†answers, you cache understanding.\n\n# How it works in practice\n\nYou curate architectural knowledge once:\n\n* entry points\n* control flow\n* where core logic lives\n* how major subsystems connect\n\nThis is short, human-written context. Not auto-generated docs. Not full files. That knowledge is stored and shared across the team. When a query comes in, the agent retrieves this memory first and only inspects code if it actually needs implementation detail.\n\nSo instead of loading 10k plus tokens of source code to answer: ‚ÄúWhere is server component rendering implemented?‚Äù\n\nThe agent gets a few hundred tokens describing the structure and entry points, then drills down selectively.\n\n# Real example from our tests\n\nWe ran the same four queries on the same large repo:\n\n* architecture exploration\n* feature addition\n* system debugging\n* build config changes\n\nManual file tagging baseline:\n\n* \\~12.5k tokens per query on average\n\nWith memory-based context:\n\n* \\~2.1k tokens per query on average\n\nThat‚Äôs about an¬†**83% token reduction**¬†and roughly¬†**56% cost savings**¬†once output tokens are factored in.\n\nhttps://preview.redd.it/a8s2hsvtbbgg1.png?width=1600&format=png&auto=webp&s=2e1bf23468ea2ce4650cb808ab4e294a61f9262b\n\n[](https://preview.redd.it/persistent-architectural-memory-cut-our-token-costs-by-55-v0-t6iyrdf3bbgg1.png?width=1600&format=png&auto=webp&s=7e1993d30d687a9f62505ff50fffbf584385f81d)\n\nSystem debugging benefited the most. Those questions usually span multiple files and relationships. File-based workflows load everything upfront. Memory-based workflows retrieve structure first, then inspect only what matters.\n\n# The part that surprised me\n\nLatency became predictable. File-based context had wild variance depending on how many search passes ran. Memory-based queries were steady. Fewer spikes. Fewer ‚Äúwhy is this taking 30 seconds‚Äù moments.\n\nAnd answers were more consistent across developers because everyone was querying the same shared understanding, not slightly different file selections.\n\n# What we didn‚Äôt have to do\n\n* No changes to application code\n* No prompt gymnastics\n* No training custom models\n\nWe just added a memory layer and pointed our agents at it.\n\nIf you want the full breakdown with numbers, charts, and the exact methodology, we wrote it up¬†[here](https://www.byterover.dev/blog/reducing-token-usage-by-83-benchmarking-cursor-s-file-context-vs.-byterover-s-memory-layer).\n\n# When is this worth it\n\nThis only pays off if:\n\n* the codebase is large\n* multiple devs rotate across the same areas\n* AI is used daily for navigation and debugging\n\nFor small repos or solo work, file tagging is fine. But once AI becomes part of how teams¬†understand¬†systems, rebuilding context from scratch every time is just wasted spend.\n\nWe didn‚Äôt optimize prompts. We optimized how understanding persists. And that‚Äôs where the savings came from.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qqcthg/persistent_architectural_memory_cut_our_token/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2ft7tk",
          "author": "cmndr_spanky",
          "text": "I miss the old Reddit ‚Ä¶ before it became an empty cesspool of SEO posts",
          "score": 11,
          "created_utc": "2026-01-29 16:46:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gjngy",
              "author": "WowSoWholesome",
              "text": "So interesting that most of the posts here look and feel the same",
              "score": 2,
              "created_utc": "2026-01-29 18:45:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2grz0u",
                  "author": "cmndr_spanky",
                  "text": "you understand why right? Basically traditional SEO no longer works because google rank no longer matters as much as chatGPT deciding your content is worth \"escalating\" to user's attention.\n\nEveryone in the digital marketing work now understands that reddit's data is sold to open AI (and others) and is a huge source of raw conversational material that's used to train frontier models.\n\nMarketers have caught on, so the default strategy is to spam these slop worded posts all over reddit with a link (or add a comment with a link so you don't get blocked by auto mods).\n\nI tried to report as many as I can with spam, but Reddit overall is out of control. It's hard to tell if reddit leadership is leaning into this bullshit because they see dollar signs, but in the end we'll all loose. People will no longer have real conversations on reddit (because overrun with bots and slop) as an advert platform in disguise.. but without real conversations the data will no longer be as useful for LLM training and honest ranking... and Reddit financially just dies a slow death as users migrate somewhere else where human-human exchanges are actually real and protected.",
                  "score": 2,
                  "created_utc": "2026-01-29 19:23:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2iweeg",
              "author": "qa_anaaq",
              "text": "I don‚Äôt disagree. Did you read the post though? I‚Äôm curious if it‚Äôs worth it but not willing to read it. Legit question though.",
              "score": 1,
              "created_utc": "2026-01-30 01:51:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ji5il",
                  "author": "cmndr_spanky",
                  "text": "yes.. this post is bullshit. Thera are a million ways to avoid wasted context window, cursor (an amazing coding agent) doesn't work the way OP described. This is a non-solution like 99% of the slop posts on this subreddit that are just trying to game \"search engine optimization\" and have nothing of value to offer.",
                  "score": 1,
                  "created_utc": "2026-01-30 03:54:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2fursv",
          "author": "Lern360",
          "text": "Love seeing stuff like this - it‚Äôs such a practical reminder that **context management + memory layers matter way more than just throwing code at the model**. By caching *understanding* of your system instead of reloading whole files every time, you massively cut token use and made query costs way more predictable. That‚Äôs exactly the kind of optimization that actually scales in a team setting rather than just hacking prompts.",
          "score": 1,
          "created_utc": "2026-01-29 16:53:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kc704",
          "author": "R-4553",
          "text": "Could be interesting to explore semantic compression to add onto your cost cuts. Potentially like 50-75% on top depending on the input type",
          "score": 1,
          "created_utc": "2026-01-30 07:32:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oti89",
          "author": "pbalIII",
          "text": "Ran into the same cold-start problem on a monorepo with 400+ services. The fix that stuck was treating architectural context like a CLAUDE.md file per service boundary, just enough to explain entry points, data flow, and who owns what.\n\nThe latency consistency you mention tracks. File-based context had 10x variance in our setup because semantic search would sometimes pull in test fixtures or deprecated modules. Memory-first routing cut that noise out.\n\nOne thing we learned: the understanding layer needs versioning. Architecture drifts, and stale memory is worse than no memory because the agent trusts it. Git-triggered refresh or TTL on critical sections helped.",
          "score": 1,
          "created_utc": "2026-01-30 22:31:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}