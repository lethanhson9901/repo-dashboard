{
  "metadata": {
    "last_updated": "2026-02-01 08:57:56",
    "time_filter": "week",
    "subreddit": "LangChain",
    "total_items": 20,
    "total_comments": 65,
    "file_size_bytes": 100135
  },
  "items": [
    {
      "id": "1qmpjef",
      "title": "Do actual AI practitioners find the Clawdbot hype realistic?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qmpjef/do_actual_ai_practitioners_find_the_clawdbot_hype/",
      "author": "julsezerus",
      "created_utc": "2026-01-25 17:34:28",
      "score": 90,
      "num_comments": 38,
      "upvote_ratio": 0.95,
      "text": "Iâ€™m curious what people who actually work with AI think about the Clawdbot hype. \n\nHereâ€™s my take:\n\nThe capabilities Clawdbot demonstrates arenâ€™t particularly difficult to achieve technically - we can already make LLMs do most of what itâ€™s doing. The real challenge has always been implementing proper security procedures and guardrails, not the core functionality itself.\n\nFrom what I can tell, Clawdbot is essentially burning through massive amounts of LLM tokens to accomplish certain tasks without much concern for security protocols. \n\nThatâ€™sâ€¦ not exactly groundbreaking? Itâ€™s more like â€œlook what happens when you remove the safety rails and throw credits at it.â€\n\nMaybe Iâ€™m missing something, but this doesnâ€™t feel like the revolution people are making it out to be. It feels more like a demo of â€œwhat if we just didnâ€™t worry about the hard parts?â€\n\nWhat do people actually working in this space think? Am I being too cynical here, or is this hype as overblown as it seems?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qmpjef/do_actual_ai_practitioners_find_the_clawdbot_hype/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o1nm307",
          "author": "ImaginaryRea1ity",
          "text": "Saw the YT video of the guy hyping it up but I think all the marketing is paid.",
          "score": 14,
          "created_utc": "2026-01-25 17:43:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o94n7",
              "author": "gastro_psychic",
              "text": "Who is making money?",
              "score": 2,
              "created_utc": "2026-01-25 19:19:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1p175w",
                  "author": "LiveBeyondNow",
                  "text": "The creator and the suppliers, Anthropic, YT etc",
                  "score": 2,
                  "created_utc": "2026-01-25 21:22:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1pl3v6",
          "author": "Hackerjurassicpark",
          "text": "More hype. Most likely paid marketing.",
          "score": 5,
          "created_utc": "2026-01-25 22:50:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qv0do",
              "author": "mediaempire45",
              "text": "Mranwhile the creator of the clawdbot is saying on X that nobody is checking out his \"sponsor\" link",
              "score": 2,
              "created_utc": "2026-01-26 02:35:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1rci5q",
                  "author": "Hackerjurassicpark",
                  "text": "Theyâ€™re playing the long gameâ€¦ hyping up and creating massive visibility to add more users, the use those impressive user growth to justify a 1B valuation in their next seed round",
                  "score": 1,
                  "created_utc": "2026-01-26 04:13:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1r9f36",
          "author": "Proof-Sand-7157",
          "text": "What Clawdbot does is essentially the same as **Copilot/Cowork-style agents connected to channels like WhatsApp or Slack**.\n\nThereâ€™s no particularly complex capability involved.  \nIt mainly feels impressive because **itâ€™s open source**, so everything is visible and reproducible.",
          "score": 3,
          "created_utc": "2026-01-26 03:54:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wfsgw",
              "author": "laslog",
              "text": "And, unlike Copilot, it works.",
              "score": 3,
              "created_utc": "2026-01-26 21:48:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1r9wka",
          "author": "Zatkoma",
          "text": "Hype, let them some time to prove that is make sense... :)",
          "score": 2,
          "created_utc": "2026-01-26 03:57:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tacxy",
          "author": "Educational_Bag_4003",
          "text": "Don't think the founder is out to make money. Look at all of his recent dev work and look at his background [https://github.com/steipete](https://github.com/steipete) Really don't think it is paid marketing pushing this...",
          "score": 2,
          "created_utc": "2026-01-26 13:19:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vleje",
              "author": "auskadi",
              "text": "It's a great little tool. Lots of people here seem to be taking through their ...",
              "score": 2,
              "created_utc": "2026-01-26 19:34:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bosay",
              "author": "Street_Profile_8998",
              "text": "It is literally paid marketing. It says as much on the sponsored posts that endlessly run on my linked in feed",
              "score": 1,
              "created_utc": "2026-01-29 00:48:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vxvzq",
          "author": "Significant-Sweet-53",
          "text": "https://preview.redd.it/tl5mnc7u6rfg1.jpeg?width=945&format=pjpg&auto=webp&s=eda45bcd2bf6a97b9844cee99223ccd2d2c9ce4b\n\nI just made a trimmed down version of this bot that uses Deepseek or Ollama, less noise and you can add your own skills easily, Ive added gog cli to test, controlling Google workspace from WhatsApp",
          "score": 2,
          "created_utc": "2026-01-26 20:29:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1w8mu9",
              "author": "Kubuli",
              "text": "Can you share your methods, results, limitations and suggestions?",
              "score": 1,
              "created_utc": "2026-01-26 21:17:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1w1j1w",
          "author": "Lonely-Elephant2130",
          "text": "Honestly spent an afternoon trying to set up ClawdBot and gave up - I'm not technical and the Mac Mini + local setup was way over my head. But I do love the idea of bringing AI into messaging apps. Been using something similar called Super Intern ï¼ˆhttps://www.superintern.ai/ï¼‰ lately - same concept of AI in your chat, but way simpler to get started. Just works in browser/Slack, no setup needed. Maybe I'm just not the target user for ClawdBot, but feels like there's a gap between \"impressive tech\" and \"actually usable for non-tech people.",
          "score": 1,
          "created_utc": "2026-01-26 20:45:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z6ksy",
          "author": "damanamathos",
          "text": "I created an AI Personal Assistant over Christmas and it's amazing â€” one of the best things I've created. I haven't used Clawdbot but have seen a bit, and did download the repo and get my AI PA to assess it vs what I already do, and it seemed pretty similar but probably a downgrade for me. So if you're not using any kind of AI assistant, it's probably worth checking out.",
          "score": 1,
          "created_utc": "2026-01-27 07:22:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1z6vji",
              "author": "damanamathos",
              "text": "I should add that my \"assistant\" isn't even that complicated. It's just Claude Code or OpenCode will a decent AGENTS file and specialised skills combined with custom-made CLI commands that let it access all my services so it can search and read email, create drafts, create images, etc from command line tools.",
              "score": 1,
              "created_utc": "2026-01-27 07:24:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o20t0q9",
          "author": "AccountEffective369",
          "text": "I haven't checked it out but that's why people are not looking to hire AI driven employees they still looking for individuals who knows processes comprehensively. Good Knowledge before trying the application itself.",
          "score": 1,
          "created_utc": "2026-01-27 14:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21p92q",
          "author": "Classic-Log-162",
          "text": "Completely useless and unsafe.",
          "score": 1,
          "created_utc": "2026-01-27 17:01:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1o4z7a",
          "author": "Not_a_doxxtor",
          "text": "We've had these Jarvis things for a while now\n\nSomeone paid for advertising",
          "score": 1,
          "created_utc": "2026-01-25 19:01:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1pb1md",
          "author": "cqzero",
          "text": "Does it even work on Windows? I saw just MacOS/iOS/Android. Not interested unless it's Windows (or even Linux)",
          "score": 1,
          "created_utc": "2026-01-25 22:05:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sfm4t",
              "author": "4rtdud3",
              "text": "Just got it up and running under WSL (Ubuntu) on Win 11.",
              "score": 1,
              "created_utc": "2026-01-26 09:16:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o25mk7x",
              "author": "dmees",
              "text": "Yes but you should join the hype and buy a Mac Mini!",
              "score": 1,
              "created_utc": "2026-01-28 04:21:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1qezr6",
              "author": "almeidamarcell",
              "text": "are you 12?!",
              "score": -1,
              "created_utc": "2026-01-26 01:13:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sfkan",
                  "author": "Sore6",
                  "text": "are you?",
                  "score": 0,
                  "created_utc": "2026-01-26 09:16:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1qnodi",
              "author": "PositiveShallot7191",
              "text": "lol its self hosted its not running on phones",
              "score": 0,
              "created_utc": "2026-01-26 01:58:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uhk9w",
          "author": "Significant-Sweet-53",
          "text": "Paid-first assumptions",
          "score": 1,
          "created_utc": "2026-01-26 16:44:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nn0th",
          "author": "AykutSek",
          "text": "Youâ€™re not being cynical; youâ€™re being realistic. Clawdbot feels like a high-speed car with no brakes. Itâ€™s exciting to watch until it hits a wall of security protocols or a massive API bill. The real breakthrough won't be 'doing things,' it will be doing things safely and efficiently within the chaos of real-world data.",
          "score": 1,
          "created_utc": "2026-01-25 17:47:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nndhy",
          "author": "Resident_Green8814",
          "text": "Spot on. From a GTM and product perspective, the 'revolution' here isn't technical brilliance, but the removal of friction by ignoring guardrails. Scaling an AI agent in an enterprise environment requires solving the 'hard parts' you mentioned: security, token efficiency, and reliability. Clawdbot is a great demo of potential, but a nightmare for risk management. We need sustainable innovation, not just credit-burning experiments.",
          "score": 0,
          "created_utc": "2026-01-25 17:49:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpci1h",
      "title": "You can now train embedding models ~2x faster!",
      "subreddit": "LangChain",
      "url": "https://i.redd.it/kbenz74xl3gg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-28 14:15:31",
      "score": 40,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qpci1h/you_can_now_train_embedding_models_2x_faster/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2t1d8v",
          "author": "Exciting-Royal-3361",
          "text": "That's awesome news. Does someone have experience with building high quality training data for embedding models? I know one popular technique is to take one or more passages and generate a matching query using an LLM and perhaps create multiple versions of the same query in different styles. \n\nAre there any other techniques for generating / sourcing training data based on a large corpus of data?",
          "score": 1,
          "created_utc": "2026-01-31 15:53:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpi07h",
      "title": "I built a job search assistant to understand LangChain Deep Agents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/gallery/1qpi07h",
      "author": "Acrobatic-Pay-279",
      "created_utc": "2026-01-28 17:34:30",
      "score": 31,
      "num_comments": 3,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qpi07h/i_built_a_job_search_assistant_to_understand/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2bvl13",
          "author": "qa_anaaq",
          "text": "Cool and thanks for sharing. 2 Qs. Do you feel the deep agents harness is any good, and howâ€™s the cost of running it?",
          "score": 2,
          "created_utc": "2026-01-29 01:26:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dj8hk",
              "author": "Acrobatic-Pay-279",
              "text": "yeah it's good, especially for long running tasks. in the earlier version, I actually tracked write\\_todos with the statuses and it was breaking things down step by step. I also tried HITL flows. the docs mention prompt caching (Anthropic) and pluggable storage backends but I wasn't able to verify/use those.\n\nthe cost is just the underlying model (I used OpenAI in this case).",
              "score": 3,
              "created_utc": "2026-01-29 08:06:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ek6gp",
                  "author": "qa_anaaq",
                  "text": "Cool thanks a lot",
                  "score": 1,
                  "created_utc": "2026-01-29 13:08:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qrpomi",
      "title": "Are MCPs outdated for Agents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qrpomi/are_mcps_outdated_for_agents/",
      "author": "FunEstablishment5942",
      "created_utc": "2026-01-31 02:17:17",
      "score": 25,
      "num_comments": 21,
      "upvote_ratio": 0.96,
      "text": "I saw a video of the OpenClaw creator saying that MCP tools are shit\nIn fact the only really working Agent  are moving away from defining strict tools (like MCP or rigid function calling) and giving the agent raw CLI tools and letting it figure it out.\n\nâ€‹Iâ€™m looking into LangGraph for this, and while the checkpointers are amazing for recovering conversation history (threads), I'm stuck on how to handle the Computer State\n\nâ€‹The Problem:\nA conversation thread is easy to persist. But a CLI session is stateful (current working directory, cli commands, active background processes).\n\nâ€‹If an agent runs cd /my_project in step 1, and the graph pauses or moves to the next step, that shell context is usually lost unless explicitly managed.\n\nâ€‹The Question:\nIs there an existing abstraction or \"standard way\" in LangGraph to maintain a persistent CLI/Filesystem session context that rehydrates alongside the thread?If not would it be a good idea to add it?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qrpomi/are_mcps_outdated_for_agents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2qdp2h",
          "author": "cincyfire35",
          "text": "I lead a development team where we build with langgraph regularly.\n\nPeople who are naysayers on MCP dont realize that there are other applications for it than just spamming context with 10-50 irrelevant tools for a general purpose agent. With frameworks like langgraph, you can build and orchestrate custom agents for tasks with finely tuned contexts and tools, eliminating the need for things like skills and tool selectors. Pairing this with code based mcp execution, you can pretty much load 2-3 mcp servers with all their tools as python functions in a safe execution environment (see smolagentsâ€™ safe python executor), tell the llm it can call them as python functions, and get a lot of the benefits from anthropics/cloudflareâ€™s code mode articles by chaining calls into each other and performing calcs/aggregation outside the context window. You can even build logic to lazy load the tools if you want, but thats a waste if you can just route to a specialized agent for the given task. \n\nWe never use more than 2-3 mcp servers with curated tools selected for an agent because we pay per token. Why waste it with irrelevance? We let users build agents with specific goals and targets in mind, select only the tools they need, and it can solve/work through the task for them. Why give a rag agent for a legal team access to SQL tools for supply chain? Makes no sense. But some people just build one big agent and hope it works. Langgraph/langchain enables you to build custom workflows and agents to solve tasks efficiently. Can build in orchestration however you prefer (tons of flexibility and documented examples of how to do it) and accomplish what claude does with skills, but more predictably and reliably. \n\nAnd thats not the half of it. MCP is just a protocol. We build custom tools with fastMCP in python all the time and its an easy way to connect the tools to our langgraph agents or external ones. We host them in our platform and can connect to them as needed. It allows us to build powerful tools that can be reused across frameworks. You dont need an mcp servers with 100 tools it. Can spin up several servers in one app instance of compute with 1-3 specific to usecase tools each built in a very easy way with good testing/standards, then serve it to your agents. We also connect with external vendors mcps like alation or atlassian if building an agent to explore data or help devs with jira, for example. Tons in the ecosystem.",
          "score": 38,
          "created_utc": "2026-01-31 03:53:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wlb01",
              "author": "SpareIntroduction721",
              "text": "You used smolagents with Langgraph? Or code mode? Iâ€™ve tried and failed, I tried the UTCP route as well and didnâ€™t work too well",
              "score": 1,
              "created_utc": "2026-02-01 02:53:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qd0zk",
          "author": "Number4extraDip",
          "text": "Didnt need to deal with lang through my deployment whatsoever. I use mcp and have no issues. Saves me time. CLI environments arent available to all users/hardware/OS",
          "score": 7,
          "created_utc": "2026-01-31 03:49:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2q4qdb",
          "author": "Prestigious_Pin4388",
          "text": "Short Answer: I don't use langgraph much so sorry I don't know.\nLong Answer:\nI think it's not right to give Agents complete autonomy because most or the AI apps me n u r making would be 'deterministic'\nWe would know exactly everything that is happening in it, \"what happens when the Agent doesn't call the tool? how do you debug this? how good is it's accuracy to call tools? what if I have to use an open mode for lower costs, it has much worse tool calling accuracy than gpt-5?  etc\" \n\nThese are all the questions in my mind when giving tools to LLMs, these things are non-deterministic.\nyes, we can give them \"better\" prompts and reduce temperature but that still creates vagueness and its much more difficult to debug things if they break.\nThis is the case for deterministic tools, now people are asking to give it complete freedom regardless of prompt injections or any security issues, and like you said, it gets difficult to manage these tools especially in production, imagine how hellish it would be to debug when things break.\n\nSo, I recommend you ignore these \"hype\" stuff.\nYou probably would have heard of how good is clawdbot( moldbot) but now see the whole drama around it.\n\nSome say it deleted all the files, some are finding security issues in it, even the creator said that it was a side project not meant for production.\nyet, still there's people yapping about how it saved them time n money, blah blah blah.\n\n\nhope this was helpful :)",
          "score": 3,
          "created_utc": "2026-01-31 02:57:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2q4ujh",
              "author": "Prestigious_Pin4388",
              "text": "it wasn't AI generated of course so, I suppose it was helpful ;)",
              "score": 2,
              "created_utc": "2026-01-31 02:58:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rajn0",
          "author": "indutrajeev",
          "text": "Yeah, if you donâ€™t need any control. But MCPâ€™s can act like a layer of governance around your Agent to check, validate, â€¦ what it does with other systems.\n\nJust giving it cli access is maybe faster but inherently much more difficult to control and check.",
          "score": 3,
          "created_utc": "2026-01-31 08:16:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rgrn2",
          "author": "vuongagiflow",
          "text": "MCP is a protocol; it is no different from openclaw tool integration with plugin. When you need consistency in operation, it need stricter schema and mcp input schema allow you to expose that contract; otherwise you will need two llm calls to achieve what mcp tool do in one call. \n\nDepending on the context, you would implement skill -> mcp -> hook to be more consistent and efficient.",
          "score": 2,
          "created_utc": "2026-01-31 09:14:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qeqmg",
          "author": "caprica71",
          "text": "The langgraph state should just hold a series of file references to where the cli has dumped its output. Later nodes in the graph can then go back and grep the files to see what happened.",
          "score": 1,
          "created_utc": "2026-01-31 04:00:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2si9u7",
              "author": "FunEstablishment5942",
              "text": "maybe this is the answer, there is not an abstraction already in place? it seems that deepagents (https://docs.langchain.com/oss/python/deepagents)  does not compartmentalize per thread the files, right?",
              "score": 1,
              "created_utc": "2026-01-31 14:13:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qguqa",
          "author": "johndoerayme1",
          "text": "Tool fatigue is real. Recent studies are showing that tool overhead can be misleading and confusing for agents. DeepAgent went to filesystem in great part for that reason. Give agents a more broad set of functionality and let them figure out how to use them - evolve sets of skills that are curated more towards the actual environment in which they're running. This is where things seem to be moving right now.\n\nRecently Anthropic added tool search to Claude as part of trying to mitigate tool fatigue/bloat. \n\nA lot of modern thought is about keeping context small/clean... so adding a ton of \"here's all the tools you can use and all their definitions\" when most of them aren't really relevant to the limited scope of the current task focus really undermines that objective.\n\nCheck out Deepagents for your persistent filesystem. I've used it effectively for my own form of \"skills\" that the agents can evolve as they learn from interaction.",
          "score": 1,
          "created_utc": "2026-01-31 04:14:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rgulw",
              "author": "Tobi-Random",
              "text": "Recent? It's been a known fact for over a year already. That's old stuff in ai context",
              "score": 0,
              "created_utc": "2026-01-31 09:15:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2sd191",
                  "author": "johndoerayme1",
                  "text": "Cool yeah except Claude just came out with tool search to address this and Harrison Chase wrote an article about using filesystem to address this 1-2 months ago. But ok cool yes it's \"old stuff\". Sorry I misspoke. Thanks for correcting the least relevant part of my response.",
                  "score": 1,
                  "created_utc": "2026-01-31 13:42:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2qy8ry",
          "author": "hello5346",
          "text": "Just like RAG.",
          "score": 1,
          "created_utc": "2026-01-31 06:25:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rkvlk",
          "author": "fball403",
          "text": "https://docs.langchain.com/oss/python/deepagents/overview",
          "score": 1,
          "created_utc": "2026-01-31 09:54:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2shwz0",
              "author": "FunEstablishment5942",
              "text": "but with deepagent is there a way to comportamentalise the files by threads? so that that thread has that environment with all /temp files that are secure and not accessed by another thread? Is this abstraction already in place?",
              "score": 1,
              "created_utc": "2026-01-31 14:11:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2tu4ym",
                  "author": "caspardev",
                  "text": "You can prompt the agent to only read/write to a directory titled with the thread id",
                  "score": 1,
                  "created_utc": "2026-01-31 18:11:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qmhxxi",
      "title": "Quantifying Hallucinations: By calculating a multi-dimensional 'Trust Score' for LLM outputs.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/gallery/1qmhxxi",
      "author": "Charming_Group_2950",
      "created_utc": "2026-01-25 12:33:20",
      "score": 20,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Announcement",
      "permalink": "https://reddit.com/r/LangChain/comments/1qmhxxi/quantifying_hallucinations_by_calculating_a/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qnqln9",
      "title": "A practical open-source repo for learning AI agents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qnqln9/a_practical_opensource_repo_for_learning_ai_agents/",
      "author": "Creepy-Row970",
      "created_utc": "2026-01-26 19:43:49",
      "score": 16,
      "num_comments": 3,
      "upvote_ratio": 0.91,
      "text": "A practical open-source repo for learning AI agents. Iâ€™ve contributed 10+ examples\n\nIâ€™ve contributed 10+ agent examples to an open-source repo thatâ€™s grown into a solid reference for building AI agents.\n\nRepo:[ https://github.com/Arindam200/awesome-ai-apps](https://github.com/Arindam200/awesome-ai-apps)\n\nWhat makes it useful:\n\n* 70+ runnable agent projects, not toy demos\n* Same ideas built across different frameworks\n* Covers starter agents, MCP, memory, RAG, and multi-stage workflows\n\nFrameworks include LangChain, LangGraph, LlamaIndex, CrewAI, Agno, Google ADK, OpenAI Agents SDK, AWS Strands, and PydanticAI.\n\nSharing in case others here prefer learning agents by reading real code instead of theory.\n\n",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1qnqln9/a_practical_opensource_repo_for_learning_ai_agents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o1vok23",
          "author": "Pristine_Shelter_28",
          "text": "are these live apps?",
          "score": 1,
          "created_utc": "2026-01-26 19:48:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o246fka",
          "author": "YUYbox",
          "text": "Hi, \n  I built a tool for anyone running multi-agent AI systems.\n When LLMs talk to each other, they develop patterns that are hard to audit - invented acronyms, lost context, meaning drift.\r\n\r\n   The solution: InsAIts monitors these communications and flags anomalies.\r\n\r\n```python\r\nfrom insa_its import insAItsMonitor\r\n\r\nmonitor = insAItsMonitor()  # Free tier, no key needed\r\nmonitor.register_agent(\"agent_1\", \"gpt-4\")\r\n\r\nresult = monitor.send_message(\r\n    text=\"The QFC needs recalibration on sector 7G\",\r\n    sender_id=\"agent_1\"\r\n)\r\n\r\nif result[\"anomalies\"]:\r\n    print(\"Warning:\", result[\"anomalies\"])\r\n```\r\n\r\n  Features:\r\n- Local processing (sentence-transformers)\r\n- LangChain & CrewAI integrations\r\n- Adaptive jargon dictionary\r\n- Zero cloud dependency for detection\r\n\r\nGitHub: https://github.com/Nomadu27/InsAIts\r\nPyPI: pip install insa-its",
          "score": 1,
          "created_utc": "2026-01-27 23:42:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo6uax",
      "title": "What It Actually Takes to Build a Context-Aware Multi-Agent AI System",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qo6uax/what_it_actually_takes_to_build_a_contextaware/",
      "author": "pretty_prit",
      "created_utc": "2026-01-27 07:05:33",
      "score": 14,
      "num_comments": 7,
      "upvote_ratio": 0.94,
      "text": "Designing a multi-agent system with memory raises a different set of problems than most demos show.  \n  \nThe diagram below shows a simple multi-agent architecture I built to explore that gap.  \n  \nInstead of agents talking to each other directly, everything goes through an orchestration layer that handles:  \n\\-intent routing  \n\\-shared user context  \n\\-memory retrieval and compaction  \n  \nWhile designing this, a set of product questions surfaced that you donâ€™t see in most demos  \n\\-What belongs in long-term memory vs. short-term history?  \n\\-When do you summarize context, and what do you risk losing?  \n\\-How do you keep multiple agents consistent as context evolves?  \n  \nI wrote a detailed breakdown of this architecture, including routing strategy, memory design, and the trade-offs this approach introduces.  \n  \n[https://medium.com/towards-artificial-intelligence/how-i-built-a-context-aware-multi-agent-wellness-system-a3eacbc33fe4?sk=c37c88e2f74aa9e5c2b2d681292d26c2](https://medium.com/towards-artificial-intelligence/how-i-built-a-context-aware-multi-agent-wellness-system-a3eacbc33fe4?sk=c37c88e2f74aa9e5c2b2d681292d26c2)  \n  \nIf youâ€™re a PM, founder, or student trying to move beyond one-off agent demos, this might be useful.\n\nhttps://preview.redd.it/mr1w53kmcufg1.png?width=1838&format=png&auto=webp&s=e36245c419d44c006fdd8e3ff006c060eb320489\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qo6uax/what_it_actually_takes_to_build_a_contextaware/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o1zgezp",
          "author": "DaRandomStoner",
          "text": "This looks pretty well put together... Is there a github repo we could check out?",
          "score": 1,
          "created_utc": "2026-01-27 08:51:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zusom",
              "author": "pretty_prit",
              "text": "Thank you. the github link is there at the end of the article but posting here again - [https://github.com/pritha21/llm\\_projects/tree/main/wellness\\_langchain\\_app](https://github.com/pritha21/llm_projects/tree/main/wellness_langchain_app)",
              "score": 1,
              "created_utc": "2026-01-27 11:01:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zmbdv",
          "author": "sleepnow",
          "text": "Good effort, but there is nothing at all new or particularly unique about this approach. ",
          "score": 1,
          "created_utc": "2026-01-27 09:46:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20aayx",
              "author": "pretty_prit",
              "text": "Maybe. But I was just exploring this topic, so its new for me.",
              "score": 3,
              "created_utc": "2026-01-27 12:55:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o244gr7",
          "author": "YUYbox",
          "text": "Hi there, I think InsAIts could be very helpful in this matter. \nhttps://github.com/Nomadu27/InsAIts",
          "score": 1,
          "created_utc": "2026-01-27 23:32:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o260i4c",
              "author": "pretty_prit",
              "text": "Will check it out",
              "score": 2,
              "created_utc": "2026-01-28 05:55:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o24l5lq",
          "author": "pbalIII",
          "text": "Context compaction is where most of these designs break down quietly. You summarize to save tokens, but summaries drop the specifics that matter six turns later... and you only find out when an agent makes a decision based on stale assumptions.\n\nOne pattern that's helped: treating memory writes as versioned facts rather than mutable state. Agents can reference which version they're working from, and conflicts surface explicitly instead of silently diverging.\n\nThe orchestration-layer-as-bottleneck tradeoff you're hitting is real. Centralizing routing keeps agents consistent, but it also means every context update round-trips through one chokepoint. Some teams split into private vs shared memory tiers to let agents work locally until they need to sync. Adds complexity, but scales better when you're past 3-4 agents.",
          "score": 0,
          "created_utc": "2026-01-28 00:57:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrwgul",
      "title": "Long-term memory of design",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/57v3qvd36ngg1",
      "author": "1501694",
      "created_utc": "2026-01-31 08:00:30",
      "score": 13,
      "num_comments": 6,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qrwgul/longterm_memory_of_design/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2swieb",
          "author": "justanemptyvoice",
          "text": "So much what?  All I see is a mind map diagram.",
          "score": 3,
          "created_utc": "2026-01-31 15:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u0j1n",
          "author": "Due-Mode9856",
          "text": "Can you share the link of the diagram with us",
          "score": 1,
          "created_utc": "2026-01-31 18:41:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u4tk1",
          "author": "qa_anaaq",
          "text": "This feels like a post from a sci fi movie where the AI was controlling the human and making them type then something snapped and the connection was severed mid thought.",
          "score": 1,
          "created_utc": "2026-01-31 19:01:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2w4jzd",
          "author": "1501694",
          "text": "graph TB\n    subgraph InputLayer[Input Layer]\n        UserQuery[\"User Query / ç”¨æˆ·æŸ¥è¯¢\"]\n        DialogueContext[\"Dialogue Context / å¯¹è¯ä¸Šä¸‹æ–‡\"]\n        HistoricalData[\"Historical Data / åŽ†å²æ•°æ®\"]\n        EmotionalTone[\"Emotional Tone / æƒ…æ„ŸåŸºè°ƒ\"]\n    end\n    \n    subgraph FiveLayerThinking[Five-Layer Thinking System]\n        subgraph Layer1[\"Layer 1: Factual Layer / äº‹å®žå±‚\"]\n            L1_1[\"Objective Facts / å®¢è§‚äº‹å®ž\"]\n            L1_2[\"Data Information / æ•°æ®ä¿¡æ¯\"]\n            L1_3[\"Specific Details / å…·ä½“ç»†èŠ‚\"]\n            L1_4[\"Time & Location / æ—¶é—´åœ°ç‚¹\"]\n            L1_5[\"Verifiable Claims / å¯éªŒè¯å£°æ˜Ž\"]\n        end\n        \n        subgraph Layer2[\"Layer 2: Logical Layer / é€»è¾‘å±‚\"]\n            L2_1[\"Causality / å› æžœå…³ç³»\"]\n            L2_2[\"Reasoning Chains / æŽ¨ç†é“¾æ¡\"]\n            L2_3[\"Argument Structure / è®ºè¯ç»“æž„\"]\n            L2_4[\"Contradiction Detection / çŸ›ç›¾æ£€æµ‹\"]\n            L2_5[\"Logical Consistency / é€»è¾‘ä¸€è‡´æ€§\"]\n        end\n        \n        subgraph Layer3[\"Layer 3: Emotional Layer / æƒ…æ„Ÿå±‚\"]\n            L3_1[\"Emotion Recognition / æƒ…ç»ªè¯†åˆ«\"]\n            L3_2[\"Sentiment Analysis / æƒ…æ„Ÿåˆ†æž\"]\n            L3_3[\"Feeling Expression / æ„Ÿå—è¡¨è¾¾\"]\n            L3_4[\"Empathy Understanding / å…±æƒ…ç†è§£\"]\n            L3_5[\"Affective Memory / æƒ…æ„Ÿè®°å¿†\"]\n        end\n        \n        subgraph Layer4[\"Layer 4: Value Layer / ä»·å€¼å±‚\"]\n            L4_1[\"Meaning Judgment / æ„ä¹‰åˆ¤æ–­\"]\n            L4_2[\"Value Orientation / ä»·å€¼å–å‘\"]\n            L4_3[\"Moral Consideration / é“å¾·è€ƒé‡\"]\n            L4_4[\"Goal Alignment / ç›®æ ‡å¯¹é½\"]\n            L4_5[\"Ethical Reasoning / ä¼¦ç†æŽ¨ç†\"]\n        end\n        \n        subgraph Layer5[\"Layer 5: Philosophical Layer / å“²å­¦å±‚\"]\n            L5_1[\"Essence Thinking / æœ¬è´¨æ€è€ƒ\"]\n            L5_2[\"Existence Meaning / å­˜åœ¨æ„ä¹‰\"]\n            L5_3[\"Ultimate Questions / ç»ˆæžè¿½é—®\"]\n            L5_4[\"Wisdom Integration / æ™ºæ…§æ•´åˆ\"]\n            L5_5[\"Transcendent Understanding / è¶…è¶Šæ€§ç†è§£\"]\n        end\n    end\n    \n    subgraph DynamicFusion[Dynamic Weight Fusion]\n        ContextAnalysis[\"Context Analysis / ä¸Šä¸‹æ–‡åˆ†æž\"]\n        WeightCalculation[\"Weight Calculation / æƒé‡è®¡ç®—\"]\n        FusionFormula[\"S = Î£(wáµ¢ Ã— Láµ¢) / èžåˆå…¬å¼\"]\n        OutputGeneration[\"Output Generation / è¾“å‡ºç”Ÿæˆ\"]\n    end\n    \n    subgraph FeedbackLoop[Feedback & Learning]\n        UserFeedback[\"User Feedback / ç”¨æˆ·åé¦ˆ\"]\n        WeightAdjustment[\"Weight Adjustment / æƒé‡è°ƒæ•´\"]\n        SystemLearning[\"System Learning / ç³»ç»Ÿå­¦ä¹ \"]\n    end\n    \n    InputLayer --> FiveLayerThinking\n    FiveLayerThinking --> DynamicFusion\n    DynamicFusion --> FeedbackLoop\n    FeedbackLoop -.-> |Optimize| FiveLayerThinking\n    \n    style InputLayer fill:#E8F5E9\n    style Layer1 fill:#BBDEFB\n    style Layer2 fill:#90CAF9\n    style Layer3 fill:#F48FB1\n    style Layer4 fill:#FFCC80\n    style Layer5 fill:#CE93D8\n    style DynamicFusion fill:#A5D6A7\n    style FeedbackLoop fill:#FFD54F",
          "score": 1,
          "created_utc": "2026-02-01 01:12:09",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2w6ue9",
              "author": "1501694",
              "text": "ðŸ˜£ðŸ˜£   too longâ€¦â€¦   I pasted to grok and shared the link here, can I see the content? I don't know what everyone usually uses, or DM me    [link](https://grok.com/share/bGVnYWN5_35c8f87c-0efa-4d5d-9089-0ee78306de69)\nhttps://grok.com/share/bGVnYWN5_35c8f87c-0efa-4d5d-9089-0ee78306de69",
              "score": 1,
              "created_utc": "2026-02-01 01:25:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qna46j",
      "title": "Best practice for managing LangGraph Postgres checkpoints for short-term memory in production?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qna46j/best_practice_for_managing_langgraph_postgres/",
      "author": "Major_Ad7865",
      "created_utc": "2026-01-26 08:05:35",
      "score": 11,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "â€™m building a memory system for a chatbot using **LangGraph**.  \nRight now Iâ€™m focusing on **short-term memory**, backed by **PostgresSaver**.\n\nEvery state transition is stored in the `checkpoints` table. As expected, each user interaction (graph invocation / LLM call) creates multiple checkpoints, so the checkpoint data in checkpoints table grows **linearly with usage**.\n\nIn a production setup, whatâ€™s the recommended strategy for managing this growth?\n\nSpecifically:\n\n* Is it best practice to **keep only the last N checkpoints per** thread\\_id  and delete older ones?\n* How do people balance **resume/recovery safety** vs **database growth** at scale?\n\nFor context:\n\n* I already use conversation summarization, so older messages arenâ€™t required for context\n* Checkpoints are mainly needed for short-term recovery and state continuity, not long-term memory\n* LangGraph can **resume from the last checkpoint**\n\nCurious how others handle this in real production systems.\n\nAdditionally in postgres langgraph creates 4 tables regarding checkpoints : checkpoints,checkpoint\\_writes,checkpoint\\_migrations,checkpoint\\_blobs",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qna46j/best_practice_for_managing_langgraph_postgres/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o1v6nu7",
          "author": "TextHour2838",
          "text": "Youâ€™re already thinking about this the right way: treat checkpoints as operational logs, not permanent memory, and prune aggressively.\n\n\n\nMain point: keep only a small, rolling window per thread (last N or last T minutes/hours) and purge the rest with a background job.\n\n\n\nWhatâ€™s worked for us:\n\n\\- Per-thread policy: e.g., keep last 10â€“20 checkpoints or last 24h, whichever is smaller.\n\n\\- Time-based GC: daily job that deletes old checkpoints/checkpoint\\_writes/checkpoint\\_blobs by thread\\_id + created\\_at, in batches to avoid locks.\n\n\\- Promotion: anything you might need long-term (audit, analytics, durable memory) gets promoted into a separate, slimmer schema / vector store before you delete.\n\n\\- Safety: pair this with idempotent tools and a compensating-action log so you can replay from business events if a resume fails, not from ancient checkpoints.\n\n\n\nOn the tooling side, Iâ€™ve mixed Supabase and RDS for this, and for chatbots in ecom Iâ€™ve tried Gorgias and Intercom; Zipchat sits in that space too but handles the short-term vs long-term memory split for you so you donâ€™t babysit raw checkpoint tables.\n\n\n\nSo: rolling window + periodic GC + promote anything important out of the checkpoint tables before pruning.",
          "score": 1,
          "created_utc": "2026-01-26 18:32:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uya4l",
          "author": "AdditionalWeb107",
          "text": "This should be native to some substrate via durable APIs. Doing this by hand feels like a great way to mess it  up and also distract you from building your agent.",
          "score": 0,
          "created_utc": "2026-01-26 17:57:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp3lp3",
      "title": "We cache decisions, not responses - does this solve your cost problem?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qp3lp3/we_cache_decisions_not_responses_does_this_solve/",
      "author": "llm-60",
      "created_utc": "2026-01-28 06:21:30",
      "score": 10,
      "num_comments": 14,
      "upvote_ratio": 0.86,
      "text": "Quick question for anyone running AI at scale:\n\nTraditional caching stores the response text. So \"How do I reset my password?\" gets cached, but \"I forgot my password\" is a cache miss - even though they need the same answer.\n\nWe flip this: cache the **decision** (what docs to retrieve, what action to take), then generate fresh responses each time.\n\nResult: 85-95% cache hit rate vs 10-30% with response caching.\n\n**Example:**\n\n* \"Reset my password\" â†’ decision: fetch docs \\[45, 67\\]\n* \"I forgot my password\" â†’ same decision, cache hit\n* \"Can't log in\" â†’ same decision, cache hit\n* All get personalized responses, not copied text\n\n**Question: If you're spending $2K+/month on LLM APIs for repetitive tasks (support, docs, workflows), would this matter to you?**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qp3lp3/we_cache_decisions_not_responses_does_this_solve/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o26v8xf",
          "author": "ruben_rrf",
          "text": "I get that you generate different outputs and cut the costs of having to make the tool calls and also the time. But how do you achieve a better cache rate? If I get it right...\n\nQuestion -> Actions -> Response\n\nIf you cache the Response, then you get a cache with Question -> Response, but if you cache the actions, you get a Question -> Actions cache, and then you use the model as \\[Question, Actions\\] -> Response.\n\nBut the key on the cache wouldn't be the same?",
          "score": 3,
          "created_utc": "2026-01-28 10:22:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26vsqv",
              "author": "llm-60",
              "text": "We don't cache the question, we cache theÂ **normalized intent**.\n\nWe extracts the \"meaning\" first:  \n  \n\"What's your return policy?\" - intent: return\\_policy  \n\"Can I return stuff?\" - intent: return\\_policy  \n\"How do returns work?\" - intent: return\\_policy\n\nand it also learn the context to fit the answer later...\n\nThree different questions, same cache key = cache hit.\n\nThat's how we get 80%",
              "score": 2,
              "created_utc": "2026-01-28 10:27:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2cgwn4",
          "author": "pbalIII",
          "text": "Intent normalization is doing the heavy lifting here. Most semantic cache implementations use embedding similarity directly on the query, which means you're still sensitive to phrasing variance even with cosine thresholds.\n\nCaching the decision output (retrieval path, action type) instead of the response is cleaner in theory... but you've moved the problem upstream. Now your intent extractor becomes the cache key generator, and any drift in how it normalizes inputs breaks your hit rate.\n\nMulti-intent queries are where this gets tricky. Something like a user forgetting their password and wanting to change their email maps to two decisions. The decomposition step either needs its own cache layer or you end up recomputing the split every time.",
          "score": 2,
          "created_utc": "2026-01-29 03:23:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d3ttn",
              "author": "llm-60",
              "text": "Great observations. You're right - intent extraction is doing the heavy lifting, and that's intentional.\n\n**On drift:**Â Valid concern. We handle this with versioned extraction models + policy rules as fallbacks. If the extractor changes, old cache keys naturally expire (TTL). You can also monitor extraction confidence and invalidate cache when you update the model. Not perfect, but manageable.\n\n**On multi-intent queries:**Â You're absolutely right - this is a known limitation. \"Reset password AND change email\" currently goes to low confidence â†’ bypasses cache â†’ escalates.\n\nFor v1, we're targeting single-intent policy decisions (returns, approvals, routing). Multi-intent decomposition is on the roadmap (Phase 2), likely with its own caching layer as you suggest.\n\nThe trade-off: Embedding similarity gives you \\~30-40% hit rates with fuzzy matching. Intent extraction gives 80%+ when queries fit the pattern, but breaks on edge cases. We're betting that most high-volume use cases (support, returns, routing) are single-intent dominant.",
              "score": 1,
              "created_utc": "2026-01-29 05:56:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2gbjd7",
                  "author": "pbalIII",
                  "text": "Versioned extractors with TTL is a clean solve for drift. The confidence threshold routing you described maps well to what I've seen in production semantic caches... the 0.8% false-positive rate most systems report happens exactly at those threshold boundaries where similarity is just above cutoff but intent diverges slightly.\n\nCurious about the 80%+ hit rate claim. Recent benchmarks on ensemble embedding approaches show 92% for semantically equivalent queries, but that's with careful threshold tuning per query type. Are you seeing 80%+ out of the box, or does that assume some domain-specific calibration?\n\nThe single-intent constraint is probably the right call for v1. Multi-intent decomposition adds a lot of surface area for edge cases, and most high-volume support flows are indeed single-intent dominant.",
                  "score": 1,
                  "created_utc": "2026-01-29 18:08:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27t0lu",
          "author": "SpecialBeatForce",
          "text": "Couldnâ€˜t you just use semantic caching question->answer if questions like reset password and forgot password are close enough semantically?",
          "score": 1,
          "created_utc": "2026-01-28 14:10:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27un18",
              "author": "llm-60",
              "text": "Traditional semantic caching caches the entire answer, so everyone gets the same response.\n\n**Example:**  \n  \n\"Forgot password\" - cached: \"Click the reset link in your email\"  \n\"Reset my password\" - cached: \"Click the reset link in your email\"\n\nWe cache the decision (what to do), then personalize the response.\n\n**Example:**  \n\"I'm John, forgot password\" - Decision cached: \"send reset email\"  Response: \"Hi John, we sent you a reset link\"  \n\"Sarah needs reset\" -Same cached decision - Response: \"Hi Sarah, we sent you a reset link\"\n\nOne LLM call for the logic, cheap model personalizes each response. You can't do that if you cache the full answer.",
              "score": 3,
              "created_utc": "2026-01-28 14:18:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o28fjzm",
                  "author": "SpecialBeatForce",
                  "text": "Okay i like the ideaðŸ˜Š but i guess it comes down to a decision between personalized answers and saving compute?",
                  "score": 1,
                  "created_utc": "2026-01-28 15:56:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2bgmt3",
                  "author": "CourtsDigital",
                  "text": "iâ€™m not sure i understand this use case. maybe provide some examples that require personalization. iâ€™ve never expected to receive a password reset email thatâ€™s tailored to me, or to hear about a store return policy that mentions me by name\n\ni agree with BeatForce that this seems almost exactly like semantic caching, with an additional, unnecessary LLM cost\n\niâ€™m not saying this couldnâ€™t be useful, but if you intend to sell it for $1k+ per month then the use case(s) should be solid",
                  "score": 1,
                  "created_utc": "2026-01-29 00:06:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2d7yto",
          "author": "Khade_G",
          "text": "Yeah this would matter to anyone actually paying the bill. What youâ€™re describing sounds like semantic / policy caching, and itâ€™s way more aligned with how real systems behave than response caching. Most production queries donâ€™t differ in intent, they differ in phrasing, tone, or user context. Caching text throws all that signal away; caching the decision preserves it.\n\nThe big wins Iâ€™ve seen with this approach are much higher cache hit rates, fresh/personalized responses without re-doing expensive reasoning, and cleaner separation between â€œunderstand the problemâ€ and â€œsay the answerâ€\n\nThe main things to watch out for are:\n- Decision drift: if your retrieval or routing logic changes, you need a clean way to invalidate or version the decision cache.\n- Over-generalization: making sure different intents donâ€™t collapse into the same decision accidentally.\n- Debuggability: being able to explain why two queries mapped to the same decision.\n\nBut for support, docs, and workflow-heavy systems this is definitely the direction things are going. Once you cross ~$1â€“2k/month, optimizing reasoning reuse matters way more than token shaving. If you can make the cache safe and observable then this is a no-brainer.",
          "score": 1,
          "created_utc": "2026-01-29 06:29:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d8jhq",
              "author": "llm-60",
              "text": "Appreciate this - you nailed the trade offs. We're addressing those exact concerns:\n\n* Decision drift: TTL-based expiry + policy versioning\n* Over-generalization: Confidence gating (low confidence - bypass cache)\n* Debuggability: Dashboard shows canonical state extraction + cache hit/miss audit trail\n\nAlready seeing 75% hit rates with policy based workloads on simulations and some test users.",
              "score": 2,
              "created_utc": "2026-01-29 06:33:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2d8t3j",
                  "author": "Khade_G",
                  "text": "Good stuff!",
                  "score": 1,
                  "created_utc": "2026-01-29 06:36:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qq8v85",
      "title": "Why email context is way harder than document RAG",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qq8v85/why_email_context_is_way_harder_than_document_rag/",
      "author": "EnoughNinja",
      "created_utc": "2026-01-29 13:41:51",
      "score": 8,
      "num_comments": 6,
      "upvote_ratio": 0.83,
      "text": "I've been seeing a lot of posts on Reddit and other forums about connecting agents to Gmail or making \"email-aware\" assistants.\n\nI don't think it's obvious why this is much harder than document RAG until you're deep into it, so here's my breakdown.\n\n**1. Threading isnâ€™t linear**  \nEmail threads arenâ€™t clean sequences. Youâ€™ve got nested quotes, forwards inside forwards, and inline replies that break sentences in half. Standard chunking strategies fall apart because boundaries arenâ€™t real. You end up retrieving fragments that are meaningless on their own.\n\n**2. â€œWho said whatâ€ actually matters**  \nWhen someone asks â€œwhat did they commit to?â€, you have to separate their words from text they quoted from someone else. Embeddings optimize for semantic similarity, rather than for authorship or intent. \n\n**3. Attachments are their own problem**  \nPDFs need OCR. and images need processing, and also Calendar invites are structured objects. Often the real decision lives in the attachment, not the email body, but each type wants a different pipeline.\n\n**4. Permissions break naive retrieval**  \nIn multi-user systems, relevance isnâ€™t enough. User A must never see User Bâ€™s emails, even if theyâ€™re semantically perfect matches. Vector search doesnâ€™t care about access control unless youâ€™re very deliberate.\n\n**5. Recency and role interact badly**  \nThe latest message might just be â€œThanks!â€ while the actual answer is found eight messages back. But you also canâ€™t ignore recency, because the context does shift over time.\n\nRAG works well for documents because documents are self-contained, but email threads are relational and so the meaning lives in the connections between messages.\n\nThis is the problem we ended up building [iGPT](https://www.igpt.ai/) around.\n\nHappy to talk through edge cases or trade notes if anyone else is wrestling with this.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qq8v85/why_email_context_is_way_harder_than_document_rag/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2euxsg",
          "author": "PAChilds",
          "text": "You certainly nailed the issues with email. \n\nEmail also has a time element critical to investigations. The headers include a raft of metadata also of use to investigations. Finally any differences in the display names associated with a specific email address can indicate an intent to deceive or sidebar conversation between a subset of copied parties.",
          "score": 2,
          "created_utc": "2026-01-29 14:06:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f1x93",
          "author": "Lumpy-Comedian-1027",
          "text": "Definitively a complex challenge. But i really don't want to use a SaaS for this. But a library that solves the threading issue would be great, even if it is a bit simplistic - people usually just put their answer on the top of the last mail.",
          "score": 2,
          "created_utc": "2026-01-29 14:42:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2i3yzg",
              "author": "jsjoana",
              "text": "For sure! A library that helps with threading could save a lot of headaches. Even a simple solution that just consolidates replies would be a game changer. Itâ€™s wild how much context gets lost in those nested conversations.",
              "score": 2,
              "created_utc": "2026-01-29 23:16:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2gb7n9",
          "author": "pbalIII",
          "text": "Inbox RAG goes sideways if you treat each message as standalone. Meaning lives in reply structure and who was in the room, drop that and you'll keep pulling the wrong slice. Parse the tree from headers, keep quote depth and inline edits as annotations, and store decisions, commitments, owners as fields. Filter by participant before vector search so perms and relevance stay tied.",
          "score": 2,
          "created_utc": "2026-01-29 18:07:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kaz13",
          "author": "R-4553",
          "text": "Input compression could be interesting to try with this use case although I'd might want to protect some parts of the input from compression",
          "score": 2,
          "created_utc": "2026-01-30 07:22:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fc42w",
          "author": "Trawling_",
          "text": "Why wouldnâ€™t someone just use copilot for outlook on m365?\n\nHave you compared the two against a defined baseline of tests? What problem are you solving and for who?",
          "score": 1,
          "created_utc": "2026-01-29 15:30:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpfnym",
      "title": "I stopped manually iterating on my agent prompts: I built an open-source system that extracts prompt improvements from my agent traces",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qpfnym/i_stopped_manually_iterating_on_my_agent_prompts/",
      "author": "cheetguy",
      "created_utc": "2026-01-28 16:13:28",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 0.75,
      "text": "Some of you might remember my [post about ACE](https://reddit.com/r/LangChain/comments/1p35tko/your_local_llm_agents_can_be_just_as_good_as/) about my open-source implementation of ACE (Agentic Context Engineering). ACE is a framework that makes agents learn from their own execution feedback without fine-tuning.\n\nI've now built a specific application: **agentic system prompting** that does offline prompt optimization from agent traces (e.g. from LangSmith)\n\n**Why did I build this?**\n\nI kept noticing my agents making the same mistakes across runs. I fixed it by digging through traces, figure out what went wrong, patch the system prompt, repeat. It works, but it's tedious and didn't really scale.\n\nSo I built a way to automate this. You feed ACE your agent's execution traces, and it extracts actionable prompt improvements automatically.\n\n**How it works:**\n\n1. **ReplayAgent** \\- Simulates agent behavior from recorded conversations (no live runs)\n2. **Reflector** \\- Analyzes what succeeded/failed, identifies patterns\n3. **SkillManager** \\- Transforms reflections into atomic, actionable strategies\n4. **Deduplicator** \\- Consolidates similar insights using embeddings\n5. **Skillbook** \\- Outputs human-readable recommendations with evidence\n\n**Each insight includes:**\n\n* Prompt suggestion - the actual text to add to your system prompt\n* Justification - why this change would help based on the analysis\n* Evidence - what actually happened in the trace that led to this insights\n\n**Try it yourself**   \n[https://github.com/kayba-ai/agentic-context-engine/tree/main/examples/agentic-system-prompting](https://github.com/kayba-ai/agentic-context-engine/tree/main/examples/agentic-system-prompting)\n\nWould love to hear if anyone tries this with their agents!",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1qpfnym/i_stopped_manually_iterating_on_my_agent_prompts/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2g798y",
          "author": "pbalIII",
          "text": "Trace-to-prompt pipelines are getting crowded fast. DSPy's MIPROv2 does bootstrap optimization, GEPA does evolutionary reflection, and ACE (the Stanford/SambaNova paper) does incremental playbook edits. All three extract patterns from execution traces... the difference is what happens next.\n\nDSPy needs structured I/O pairs. GEPA mutates prompt text directly and uses Pareto frontiers to keep diverse variants. ACE maintains a living context doc with delta edits so you don't get the brevity bias problem where insights get summarized away.\n\nThe 8 hours/week manual pattern analysis that u/KitchenSomew mentions is real. Curious whether your deduplicator handles semantic drift over time... embeddings cluster well initially but the similarity threshold that works at 100 traces often breaks at 1000.",
          "score": 2,
          "created_utc": "2026-01-29 17:49:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a3fbe",
          "author": "caprica71",
          "text": "How is this different from dspy?",
          "score": 1,
          "created_utc": "2026-01-28 20:17:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2afqgi",
              "author": "cheetguy",
              "text": "DSPy works best with structured input/output pairs, ACE works on raw traces (conversation logs, markdown) so no restructuring needed. DSPy auto-optimizes while ACE generates suggestions with evidence for you to review first. Think of DSPy for pipelines with clear metrics, ACE for learning from messy agent failures.",
              "score": 2,
              "created_utc": "2026-01-28 21:11:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o28mh9b",
          "author": "KitchenSomew",
          "text": "\\*\\*Production Agent Experience:\\*\\*\n\n\n\nBuilt chatbots for 50+ B2B clients - prompt drift is one of the hardest problems to catch early. Your ACE approach solves a massive pain point.\n\n\n\n\\*\\*What Resonates:\\*\\*\n\n\n\nâœ“ Trace-based learning vs manual iteration (saves weeks of debugging)\n\nâœ“ Offline optimization (no live experiments on customers)\n\nâœ“ Embedding-based deduplication (critical at scale)\n\n\n\n\\*\\*Questions from Production:\\*\\*\n\n\n\n1. \\*\\*Token Cost:\\*\\* How expensive is running ReplayAgent + Reflector on 100+ conversations? Is it viable for startups?\n\n\n\n2. \\*\\*Prompt Versioning:\\*\\* Do you version the Skillbook outputs? We've had cases where a \"good\" prompt change broke edge cases 2 weeks later.\n\n\n\n3. \\*\\*Confidence Scoring:\\*\\* Does ACE rate how confident it is in each recommendation? Some patterns need 50+ traces to be statistically significant.\n\n\n\n\\*\\*Our Workflow (manual):\\*\\*\n\n\\`\\`\\`python\n\n\\# What we do now (tedious):\n\n1. Export LangSmith traces weekly\n\n2. Filter failures (user retry, escalation)\n\n3. Manual pattern analysis\n\n4. Prompt A/B test (3-7 days)\n\n5. Repeat\n\n\\`\\`\\`\n\n\n\nACE automating steps 2-3 would save \\~8 hours/week per agent.\n\n\n\n\\*\\*Pro Tip:\\*\\* For anyone trying this - start with failure-only traces. Analyzing successful runs adds noise early on.\n\n\n\nDoes ACE handle multi-agent systems? Curious if it can trace decisions across agent handoffs.",
          "score": -2,
          "created_utc": "2026-01-28 16:26:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq0pdf",
      "title": "I built a RAG backend for non-developers who just want a simple chatbot",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qq0pdf/i_built_a_rag_backend_for_nondevelopers_who_just/",
      "author": "Unlikely_Outcome4432",
      "created_utc": "2026-01-29 06:12:04",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 0.79,
      "text": "Hey r/LangChain,\n\nI'm a PM who became a \"vibe coder\" â€“ I can read code and tweak things, but I'm not a traditional developer.\n\nWhile working as a freelancer on RAG chat services, I noticed something: a lot of people wanted to build simple RAG chatbots for non-commercial use, but the existing tools felt overwhelming for them.\n\nInstead of building custom chatbots for each person, I thought: \"What if I made a tool where you just change a config file and get a working RAG backend?\"\n\n\n\n**So I built OneRAG.**\n\n**The idea is simple:**\n\n\\- Want to switch from Chroma to Pinecone? Change one line in config.\n\n\\- Want to try Claude instead of GPT? Change one line.\n\n\\- Want to add a reranker? One line.\n\n\n\nIt uses dependency injection, so you don't need to rewrite code â€“ just swap components.\n\n\n\n**Currently supports:**\n\n\\- 6 Vector DBs (Chroma, Pinecone, Weaviate, Qdrant, pgvector, MongoDB)\n\n\\- 4 LLMs (OpenAI, Claude, Gemini, OpenRouter)\n\n\\- Rerankers, caching, Korean NLP optimization\n\nIt's not meant to replace LangChain for complex pipelines. It's for people who just want a working RAG backend without the learning curve.\n\nGitHub: [https://github.com/notaDev-iamAura/OneRAG](https://github.com/notaDev-iamAura/OneRAG)\n\nWould love feedback from this community â€“ what features would make this more useful for beginners?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qq0pdf/i_built_a_rag_backend_for_nondevelopers_who_just/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2l5j23",
          "author": "ich3ckmat3",
          "text": "Sweet!",
          "score": 1,
          "created_utc": "2026-01-30 11:49:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qn6ydn",
      "title": "I built langgraph2slack - connect any LangGraph agent to Slack in 3 lines of code",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qn6ydn/i_built_langgraph2slack_connect_any_langgraph/",
      "author": "syasini",
      "created_utc": "2026-01-26 05:12:19",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.89,
      "text": "https://reddit.com/link/1qn6ydn/video/tbbyo2uz1jgg1/player\n\nHey everyone! I've been working on an open-source package calledÂ `langgraph2slack`Â that makes it superÂ easy to deploy your **LangGraph** agentsÂ to **Slack**.\n\nHere's how you can set it up:\n\n    fromÂ langgraph2slackÂ importÂ SlackBot\n    botÂ =Â SlackBot()\n    appÂ =Â bot.app\n\nThen add it to yourÂ langgraph.json:\n\n    {\n    Â Â \"dependencies\":Â [\"langgraph2slack\",Â \".\"],\n    Â Â \"graphs\":Â {\n    Â Â Â Â \"my-assistant\":Â \"./agent.py:app\"\n    Â Â },\n    Â Â \"http\":Â {\n    Â Â Â Â \"/events/slack\":Â \"slack/server:app\"\n    Â Â }\n    }\n\nThat's it!\n\nThen runÂ `langgraphÂ dev`, point your Slack app'sÂ event URL to it (ngrok worksÂ great for local testing), and you'reÂ done.\n\n**The library currently handles**:\n\n* Real-time streaming responses (uses Slack's streaming API so users see tokens as they come in)\n* Thread management (conversationÂ history is preserved)\n* Works with DMs and mentions in channels/threads\n* Optional feedback buttons that integrate directly with LangSmith\n* Input/outputÂ transformers if you need to customize messagesÂ before/after they hit your agent\n* Markdown to Slack formatting conversion\n* Image extraction from markdown responses\n\n**Why I built this:**\n\nNowÂ that we're all building chatbots and agentic applications, one of the biggest challenges is getting them in front of users in a way thatÂ actually gets adopted. Most enterprise teams alreadyÂ live in Slack. So instead of asking people toÂ context-switch to a separate web app, it makes sense to bring your agentÂ to where they already are.\n\nThis was inspired by theÂ `langgraph-messaging-integrations`Â repoÂ which was a great reference, but I wanted something I could justÂ pip installÂ and have running in minutes without a ton of setup.\n\nLinks:\n\n* GitHub: [https://github.com/syasini/langgraph2slack](https://github.com/syasini/langgraph2slack)\n* PyPI:Â `pip install langgraph2slack`\n\nIt's MIT licensed, and I'd love for folks to try it out. If you end up using it or haveÂ ideas for improvements, let me know!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qn6ydn/i_built_langgraph2slack_connect_any_langgraph/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qol9sp",
      "title": "GraphRAG vs LangGraph agents for codebase visualization â€” which one should I use?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qol9sp/graphrag_vs_langgraph_agents_for_codebase/",
      "author": "Dizzy-Item-7123",
      "created_utc": "2026-01-27 17:59:58",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 0.75,
      "text": "Iâ€™m building an app that visualizes and queries an entire codebase.\n\nStack:\nDjango backend\nLangChain for LLM integration\n\nI want to avoid hallucinations and improve accuracy. Iâ€™m exploring:\n\nGraphRAG (to model file/function/module relationships)\nLangGraph + ReAct agents (for multi-step reasoning and tool use)\n\nNow Iâ€™m confused about the right architecture.\nQuestions:\n\nIf Iâ€™m using LangGraph agents, does GraphRAG still make sense?\n\nIs GraphRAG a replacement for agents, or a retrieval layer under agents?\n\nCan agents with tools parse and traverse a large codebase without GraphRAG?\n\nFor a codebase Q&A + visualization app, whatâ€™s the cleaner approach?\n\nLooking for advice from anyone whoâ€™s built code intelligence or repo analysis tools.",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qol9sp/graphrag_vs_langgraph_agents_for_codebase/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o28zxqv",
          "author": "Striking-Bluejay6155",
          "text": "Sharing a tool I think does what youâ€™re describing with graphrag in the background and the ability to to chat: https://code-graph.falkordb.com/",
          "score": 1,
          "created_utc": "2026-01-28 17:25:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g88my",
          "author": "pbalIII",
          "text": "They're different layers, not alternatives. GraphRAG handles your retrieval... file/function/module relationships as a knowledge graph that your agent queries. LangGraph handles orchestration... how your agent reasons through multi-step tasks.\n\nThe pattern that's working in production: build your code graph (Neo4j, FalkorDB, Memgraph all have SDKs for this), then let your LangGraph agent query it as a tool. The agent decides what to look up, GraphRAG returns the relevant subgraph.\n\nWithout the graph structure, agents can still traverse codebases but they waste tokens re-discovering relationships. With it, you get pre-indexed connections so the agent jumps straight to relevant files.\n\nFor visualization specifically, the graph is doing double duty... feeding both your UI and your agent's retrieval.",
          "score": 1,
          "created_utc": "2026-01-29 17:54:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2l1l0o",
          "author": "Luneriazz",
          "text": "langraph is for rigid workflow... lets say every time you ask the agent must look at previous chat before searching into graphRAG. you use langgraph for something like that\n\nfor graphRAG, is for hierarchical knowledge query instead of nearest or similarity search",
          "score": 1,
          "created_utc": "2026-01-30 11:18:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp2yk2",
      "title": "Advice on Consistent Prompt Outputs Across Multiple LLMs in LangChain",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qp2yk2/advice_on_consistent_prompt_outputs_across/",
      "author": "NoEntertainment8292",
      "created_utc": "2026-01-28 05:47:02",
      "score": 6,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hi all, Iâ€™m experimenting with building multi-LLM pipelines using LangChain and trying to keep outputs consistent in **tone, style, and intent** across different models.\n\nHereâ€™s a simplified example prompt Iâ€™m testing:\n\n    You are an AI assistant. Convert this prompt for {TARGET_MODEL} while keeping the original tone, intent, and style intact.\n    \n    Original Prompt: \"Summarize this article in a concise, professional tone suitable for LinkedIn.\"\n\n**Questions for the community:**\n\n* How would you structure this in a LangChain `LLMChain` or `SequentialChain` to reduce interpretation drift?\n* Are there techniques for preserving tone and formatting across multiple models?\n* Any tips for chaining multi-turn prompts while maintaining consistency?\n\nIâ€™d love to see how others handle **cross-model consistency in LangChain pipelines**, or any patterns youâ€™ve used.",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qp2yk2/advice_on_consistent_prompt_outputs_across/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o27e66m",
          "author": "Upset-Pop1136",
          "text": "we solved this by forcing a canonical JSON schema + a final â€œstyle normalizerâ€ pass on one model. donâ€™t fight every model, collapse outputs late. ",
          "score": 1,
          "created_utc": "2026-01-28 12:47:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cin4c",
              "author": "NoEntertainment8292",
              "text": "That makes sense! Collapsing late feels cleaner than over-constraining each step. Do you keep the schema purely semantic (content + intent) and let the style normalizer handle tone entirely, or do you still encode style hints in the JSON? Also wondering how brittle this gets as you add more models to the pipeline?",
              "score": 1,
              "created_utc": "2026-01-29 03:34:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qmkkkp",
      "title": "MAIRA",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qmkkkp/maira/",
      "author": "SiteCharacter428",
      "created_utc": "2026-01-25 14:30:26",
      "score": 6,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "ðŸš€ **MAIRA: Multi-Agent Intelligent Research Assistant for Automated Report Generation**\n\nIâ€™m currently building **MAIRA**, a **research-oriented multi-agent AI system** designed to automate and streamline academic research workflows.\n\nFrameworks : Langchain , Deep Agents\n\nMAIRA focuses on problems students and researchers commonly face, such as:\n\n* conducting structured literature surveys\n* synthesizing information from academic papers and web sources\n* generating well-organized research drafts and reports\n\nThe system follows a **multi-agent architecture**, where specialized agents collaborate for:\n\n* academic and web-based information retrieval\n* deep reasoning across multiple sources\n* draft creation and validation\n* final report generation in reusable formats\n\nThe goal is not just answering questions, but producing **research-ready artifacts** that can be directly used for assignments, documentation, and academic submissions.\n\nIâ€™m currently at the MVP stage and would love to get insights from the community:\n\n* What are the biggest pain points youâ€™ve faced while doing literature surveys or research documentation?\n* Are there workflows you feel could be better automated?\n* Any thoughts on multi-agent systems in academic research?\n\nI also attached planned architecture\n\nOpen to feedback, ideas, and discussions.  \nAlways excited to learn from fellow researchers and engineers ðŸ™Œ\n\nhttps://preview.redd.it/3i2e0ccp9ifg1.png?width=8192&format=png&auto=webp&s=97b09bbe0889a430aeab99023319f8872a9c1a0d\n\n\\#ResearchAI #MultiAgentSystems #AcademicResearch #AIEngineering #EdTech #LLM #Automation #StudentResearch",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qmkkkp/maira/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o1nnh0f",
          "author": "radicalSymmetry",
          "text": "My two cents: use duckdb and files if this is just for you. Donâ€™t bring a db online unless you intend to build a product. And then that ocean is pretty red. \n\nFocus on getting the research right and for that the eng side stuff is likely overkill. \n\nGl;hf",
          "score": 3,
          "created_utc": "2026-01-25 17:49:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qz57z",
              "author": "SiteCharacter428",
              "text": "Appreciate the feedback thatâ€™s a fair point.\n\nFor pure local experimentation, I agree that DuckDB + files would be sufficient and much lighter. In my case though, the goal isnâ€™t just to get outputs, but to study and document system level design choices in a research-oriented multi-agent architecture (persistence vs cache, traceability, extensibility, etc.).\n\nThe storage layer is mainly there to support reproducibility, agent trace analysis, and future evaluation, rather than productization. That said, I do see value in lightweight setups and may even mention DuckDB/file-based storage as an alternative configuration in the write-up.",
              "score": 1,
              "created_utc": "2026-01-26 02:56:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2l90jo",
          "author": "dephraiiim",
          "text": "For streamlining your report generation workflow, you might want to check out [writer.so](http://writer.so); it handles research and drafting in one place instead of juggling multiple tools.\n\nCould save you serious time on the writing side of MAIRA!",
          "score": 2,
          "created_utc": "2026-01-30 12:15:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lspo5",
              "author": "SiteCharacter428",
              "text": "Thanks for the heads up, will check it out.",
              "score": 1,
              "created_utc": "2026-01-30 14:09:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpk0tf",
      "title": "I built a virtual filesystem for AI agents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qpk0tf/i_built_a_virtual_filesystem_for_ai_agents/",
      "author": "velobro",
      "created_utc": "2026-01-28 18:42:56",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 0.99,
      "text": "Agents perform best when they have access to a computer. But the tools and integrations your agent needs are scattered across remote APIs and MCP servers.\n\nI built a virtual filesystem that puts everything your agent needs in a single folder on your computer. \n\nYour MCP servers become executables. Your integrations become directories. Everything your agent uses is literally just a file.\n\nTo use it, you just register your existing MCPs in a config file, which mounts them to a file system. This lets you interact with your remote tools like an ordinary unix binary:\n\n    /tmp/airstore/tools/wikipedia search \"albert\" | grep -i 'einstein'\n\nThe folder is virtualized, so you can mount it locally or use it in a sandboxed environment.Â \n\n**Why this matters**\n\nThe best agents rely heavily on the filesystem for storing and managing context. LLMs are already great at POSIX, and itâ€™s easier for an LLM to run a binary than call a remote MCP server. By putting your agentâ€™s tools behind a filesystem, you get a standardized interface for agents to interact with everything, which means that your agents will perform better in the real world.\n\n**How it works**\n\nJust add your existing MCP servers to a config file, and we convert each tool into a binary that your agents can use. For example:\n\n    $ ls /tmp/airstore/tools/ \n    \n    gmail\n    github \n    wikipedia \n    filesystem \n    memory\n\nThen you (or Claude Code) can use them like any CLI tool:\n\n    $ /tmp/airstore/tools/github list-issues --repo=acme/api | jq '.[0].title'\n\n**Github**: [https://github.com/beam-cloud/airstore](https://github.com/beam-cloud/airstore)\n\nWould love to hear any feedback, or if anyone else has thought about these problems as well. ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qpk0tf/i_built_a_virtual_filesystem_for_ai_agents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2o05s0",
          "author": "transfire",
          "text": "All you need is bash.",
          "score": 1,
          "created_utc": "2026-01-30 20:10:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnckty",
      "title": "Multi Agent system losing state + breaking routing. Stuck after days of debugging.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qnckty/multi_agent_system_losing_state_breaking_routing/",
      "author": "goodevibes",
      "created_utc": "2026-01-26 10:34:03",
      "score": 5,
      "num_comments": 5,
      "upvote_ratio": 0.86,
      "text": "Hey team ðŸ‘‹ðŸ¼, Iâ€™m building a multi-agent system that switches between different personas and connects to a legacy API using custom tools. Iâ€™ve spent a few days deep in code and Ive run into some architectural issues and Iâ€™m hoping to get advice from anyone whoâ€™s dealt with similar problems.\n\nCouple of the main issues Iâ€™m trying to solve;\n\nThe system forgets what itâ€™s doing when asking for confirmation\n\n\\- Iâ€™m trying to set up a flow where the agent proposes an action, asks for confirmation, then executes it. But the graph loses track of what action was pending between turns, so when I say â€œyes,â€ it just treats it like normal conversation instead of confirming the action I was asked about.\n\nPersonas keep switching unexpectedly\n\n\\- I have different roles (like admin vs. field user) that the system switches between. But the router and state initialization seem to clash sometimes, causing the persona to flip back to the wrong one unexpectedly. It feels like thereâ€™s some circular state issue or the defaults are fighting each other, but I canâ€™t for the life of me find them.\n\nTrouble passing context into tools\n\n\\- I need to inject things like auth tokens and user context when tools actually run. But this causes type errors because the tools arenâ€™t expecting those extra arguments. Iâ€™m not sure what the clean pattern is for handling stateful context when the tools themselves are supposed to be stateless. This is relatively new for the projects I have been working on.\n\nThe legacy API is misleading\n\n\\- The API returns a 200 success code even when things actually fail (bad parameters, malformed XML, etc). Agents think everything worked when it didnâ€™t, which makes debugging inside the graph really frustrating.\n\nWhat Iâ€™m hoping to find some solid advice on is;\n\n\\- Best way to debug why state gets wiped between nodes/turns\n\n\\- The standard pattern for propose â†’ confirm â†’ execute flows\n\n\\- How to make personas â€œstickâ€ without conflicting with graph initialization\n\n\\- How others cleanly pass execution context into tools\n\nIf youâ€™ve built something similar, Iâ€™d really appreciate any pointers or heads-up about gotchas. I feel like Iâ€™m missing a few fundamental patterns and just going in circles at this point.â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹ Iâ€™ve watched a heap of YouTube guides etc, studied Dev docs but I feel like Iâ€™ve hit a point where Iâ€™m going in circles ðŸ˜®â€ðŸ’¨\n\nCheers :)",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qnckty/multi_agent_system_losing_state_breaking_routing/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o1tc1td",
          "author": "bzImage",
          "text": "Langgraph",
          "score": 1,
          "created_utc": "2026-01-26 13:29:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tnxd1",
          "author": "Tough-Permission-804",
          "text": "just use github agent via vs code.  it will help you get sorted",
          "score": 1,
          "created_utc": "2026-01-26 14:31:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21t3qv",
          "author": "kacxdak",
          "text": "I think itâ€™s just a fundamental approach of how to think about agents. \n\nhttps://youtu.be/wD3zieaV0Yc?si=SVu-nJhiUmZ8nJ-S (Starting at 4:37)\n\nOnce you model agents and tool calling into traditional software (as opposed to new paradigms), controlling an agent becomes a lot easier.",
          "score": 1,
          "created_utc": "2026-01-27 17:17:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22nv9e",
          "author": "YUYbox",
          "text": "\nSharing a tool I built for anyone running multi-agent AI systems.\n\nThe problem: When LLMs talk to each other, they develop patterns that are hard to audit - invented acronyms, lost context, meaning drift.\n\nThe solution: InsAIts monitors these communications and flags anomalies.\n\nfrom insa_its import insAItsMonitor\n\nmonitor = insAItsMonitor()  # Free tier, no key needed\nmonitor.register_agent(\"agent_1\", \"gpt-4\")\n\nresult = monitor.send_message(\n    text=\"The QFC needs recalibration on sector 7G\",\n    sender_id=\"agent_1\"\n)\n\nif result[\"anomalies\"]:\n    print(\"Warning:\", result[\"anomalies\"])\n\nFeatures:\n- Local processing (sentence-transformers)\n- LangChain & CrewAI integrations\n- Adaptive jargon dictionary\n- Zero cloud dependency for detection\n\nGitHub: https://github.com/Nomadu27/InsAIts\nPyPI: pip install insa-its",
          "score": 1,
          "created_utc": "2026-01-27 19:29:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tz4l0",
          "author": "saurabhjain1592",
          "text": "Youâ€™re not missing a random trick, youâ€™ve hit a real architectural boundary that most agent frameworks donâ€™t make explicit.\n\nAll four issues you describe stem from the same root problem: execution-critical state is implicit and conversational, not explicit and owned.\n\nIn propose â†’ confirm â†’ execute flows, the â€œpending actionâ€ cannot live only in the LLM context. It needs to be a first-class execution object that survives turns, otherwise a simple â€œyesâ€ has no stable referent.\n\nPersona flipping is usually the same issue in disguise. Routing logic and initialization are both mutating shared state, so whichever runs last wins.\n\nTool context injection breaks because tools are treated as stateless functions, while the system actually needs scoped execution context (auth, role, intent) that is managed outside the tool signature.\n\nAnd legacy APIs returning 200 on failure is the worst case for agents, because success needs to be derived from semantic validation, not HTTP status.\n\nThe common pattern that helps is to separate:\n\n* conversational reasoning (LLM context)\n* execution state (what is pending, allowed, approved, failed)\n\nOnce those are decoupled, confirmation flows, personas, retries, and debugging become tractable again.\n\nYouâ€™re not going in circles because youâ€™re bad at this. Youâ€™re there because the abstractions stop short right where things become stateful and irreversible.",
          "score": 0,
          "created_utc": "2026-01-26 15:25:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo70un",
      "title": "I built an SEO Content Agent Team that optimizes articles for Google AI Search",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qo70un/i_built_an_seo_content_agent_team_that_optimizes/",
      "author": "Arindam_200",
      "created_utc": "2026-01-27 07:16:05",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 0.86,
      "text": "Iâ€™ve been working with multi-agent workflows and wanted to build something useful for real SEO work, so I put together an SEO Content Agent Team that helps optimize existing articles or generate SEO-ready content briefs before writing.\n\nThe system focuses on Google AI Search, including AI Mode and AI Overviews, instead of generic keyword stuffing.\n\nThe flow has a few clear stages:\n\n\\- Research Agent: Uses SerpAPI to analyze Google AI Mode, AI Overviews, keywords, questions, and competitors  \n\\- Strategy Agent: Clusters keywords, identifies search intent, and plans structure and gaps  \n\\- Editor Agent: Audits existing content or rewrites sections with natural keyword integration  \n\\- Coordinator: Agno orchestrates the agents into a single workflow\n\nYou can use it in two ways:\n\n1. Optimize an existing article from a URL or pasted content  \n2. Generate a full SEO content brief before writing, just from a topic\n\nEverything runs through a Streamlit UI with real-time progress and clean, document-style outputs. Hereâ€™s the stack I used to build it:\n\n\\- Agno for multi-agent orchestration  \n\\- Nebius for LLM inference  \n\\- SerpAPI for Google AI Mode and AI Overview data  \n\\- Streamlit for the UI\n\nAll reports are saved locally so teams can reuse them.\n\nThe project is intentionally focused and not a full SEO suite, but itâ€™s been useful for content refreshes and planning articles that actually align with how Google AI surfaces results now.\n\nIâ€™ve shared a full walkthrough here: [Demo](https://www.youtube.com/watch?v=BZwgey_YeF0)  \nAnd the code is here if you want to explore or extend it: [GitHub Repo](https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/content_team_agent)\n\nWould love feedback on missing features or ideas to push this further.",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qo70un/i_built_an_seo_content_agent_team_that_optimizes/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o1z7y4f",
          "author": "Creepy-Row970",
          "text": "great project",
          "score": 1,
          "created_utc": "2026-01-27 07:34:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20bj68",
              "author": "Arindam_200",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-01-27 13:03:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zukyl",
          "author": "Easy_Cable6224",
          "text": "looking cool",
          "score": 1,
          "created_utc": "2026-01-27 11:00:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20birt",
              "author": "Arindam_200",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-01-27 13:03:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}