{
  "metadata": {
    "last_updated": "2026-01-05 02:46:09",
    "time_filter": "week",
    "subreddit": "LangChain",
    "total_items": 22,
    "total_comments": 74,
    "file_size_bytes": 117919
  },
  "items": [
    {
      "id": "1pzno6m",
      "title": "Semantic caching cut our LLM costs by almost 50% and I feel stupid for not doing it sooner",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1pzno6m/semantic_caching_cut_our_llm_costs_by_almost_50/",
      "author": "Otherwise_Flan7339",
      "created_utc": "2025-12-30 17:12:23",
      "score": 129,
      "num_comments": 27,
      "upvote_ratio": 0.86,
      "text": "So we've been running this AI app in production for about 6 months now. Nothing crazy, maybe a few hundred daily users, but our OpenAI bill hit $4K last month and I was losing my mind. Boss asked me to figure out why we're burning through so much money.\n\nTurns out we were caching responses, but only with exact string matching. Which sounds smart until you realize users never type the exact same thing twice. \"What's the weather in SF?\" gets cached. \"What's the weather in San Francisco?\" hits the API again. Cache hit rate was like 12%. Basically useless.\n\nThen I learned about semantic caching and honestly it's one of those things that feels obvious in hindsight but I had no idea it existed. We ended up using Bifrost (it's an open source LLM gateway) because it has semantic caching built in and I didn't want to build this myself.\n\nThe way it works is pretty simple. Instead of matching exact strings, it matches the meaning of queries using embeddings. You generate an embedding for every query, store it with the response in a vector database, and when a new query comes in you check if something semantically similar already exists. If the similarity score is high enough, return the cached response instead of hitting the API.\n\nReal example from our logs - these four queries all had similarity scores above 0.90:\n\n* \"How do I reset my password?\"\n* \"Can't remember my password, help\"\n* \"Forgot password what do I do\"\n* \"Password reset instructions\"\n\nWith traditional caching that's 4 API calls. With semantic caching it's 1 API call and 3 instant cache hits.\n\nBifrost uses Weaviate for the vector store by default but you can configure it to use Qdrant or other options. The embedding cost is negligible - like $8/month for us even with decent traffic. GitHub: [https://github.com/maximhq/bifrost](https://github.com/maximhq/bifrost)\n\nAfter running this for 30 days our bill dropped from $4K to $2.1K. Cache hit rate went from 12% to 47%. And as a bonus, cached responses are way faster - like 180ms vs 2+ seconds for actual API calls.\n\nThe tricky part was picking the similarity threshold. We tried 0.70 at first and got some weird responses where the cache would return something that wasn't quite right. Bumped it to 0.95 and the cache barely hit anything. Settled on 0.85 and it's been working great.\n\nAlso had to think about cache invalidation - we expire responses after 24 hours for time-sensitive stuff and 7 days for general queries.\n\nThe best part is we didn't have to change any of our application code. Just pointed our OpenAI client at Bifrost's gateway instead of OpenAI directly and semantic caching just works. It also handles failover to Claude if OpenAI goes down, which has saved us twice already.\n\nIf you're running LLM stuff in production and not doing semantic caching you're probably leaving money on the table. We're saving almost $2K/month now.",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1pzno6m/semantic_caching_cut_our_llm_costs_by_almost_50/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nwt10pe",
          "author": "hyma",
          "text": "Advertisement?",
          "score": 21,
          "created_utc": "2025-12-30 21:48:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuefsp",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 12,
              "created_utc": "2025-12-31 02:16:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww5ka6",
                  "author": "nihal_ar",
                  "text": "7 months lol, read before commenting duh..",
                  "score": 2,
                  "created_utc": "2025-12-31 10:20:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwvvn72",
                  "author": "Masotsheni",
                  "text": "Not sure how you got that impression, but this sounds like a legit experience. Semantic caching is a real game changer for reducing costs. Maybe check out the GitHub link if you're curious about the tech!",
                  "score": 1,
                  "created_utc": "2025-12-31 08:46:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwuj4r4",
          "author": "Whyme-__-",
          "text": "Just pipe the entire codebase of Roo code into Gemini and ask it to pull the algorithm of semantic caching and distill into simple technical spec sheet.  Then add it to your code. Concepts like these are easier to implement if you already have someone who opensourced the tech.",
          "score": 10,
          "created_utc": "2025-12-31 02:43:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1c1rj",
              "author": "UnionCounty22",
              "text": "I bet it would give you 25% of it and shoo you out the door",
              "score": 1,
              "created_utc": "2026-01-01 05:09:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx31c2r",
                  "author": "Whyme-__-",
                  "text": "Nope if you work it right it gives the whole thing. Plus you can read the readme which has all the features so ask it to double and triple check it",
                  "score": 1,
                  "created_utc": "2026-01-01 14:37:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nws6hgz",
          "author": "Conscious_Nobody9571",
          "text": "Repost",
          "score": 5,
          "created_utc": "2025-12-30 19:22:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrnnl6",
          "author": "Far_Buyer_7281",
          "text": "Seems like something I would warn my users about at least?  \nisn't a query more then its semantic meaning?",
          "score": 3,
          "created_utc": "2025-12-30 17:55:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nws74sx",
              "author": "adiznats",
              "text": "I'm also wondering what is the cosine similarity between \"what's the weather in SF\" and \"whats's the weather in NY\". Also probably longer sentences like those above but with a very small detail changed will be even worse with this kind of caching.",
              "score": 8,
              "created_utc": "2025-12-30 19:26:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwzbml6",
          "author": "tomomcat",
          "text": "Lame advert. This is just pollution.",
          "score": 3,
          "created_utc": "2025-12-31 21:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx215r5",
          "author": "getarbiter",
          "text": "The threshold tuning problem you're describing is fundamental to similarity-based caching. You're essentially guessing where \"same meaning\" ends and \"different meaning\" begins.\n\nWe took a different approach‚Äîcoherence scoring instead of similarity scoring. Rather than asking \"how close are these vectors?\", we ask \"does this cached response actually resolve the query under its constraint field?\"\n\n\"What's the weather in SF\" and \"What's the weather in NY\" have high cosine similarity (~0.95+) but zero coherence as cache matches‚Äîdifferent constraint fields.\n\n\"How do I reset my password\" and \"Forgot my password, help\" have moderate similarity but high coherence‚Äîsame constraint resolution.\n\nThe result: no arbitrary thresholds, deterministic scoring, and the cache knows why something matches, not just how close the vectors are.\n\n26MB engine, runs locally, no API calls for the coherence check itself. Happy to share more if useful.",
          "score": 2,
          "created_utc": "2026-01-01 09:11:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx29si4",
              "author": "louis8799",
              "text": "I'm interested.",
              "score": 2,
              "created_utc": "2026-01-01 10:43:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx495se",
                  "author": "getarbiter",
                  "text": "One concrete example based on what you described:\nIn your password-reset case, similarity caching works until two queries are close in embedding space but differ in constraint (account type, region, auth method, etc.). That‚Äôs where threshold tuning starts leaking correctness.\n\nWe handle that by checking whether the cached response resolves the same constraint set, not just a similar query.\nSo:\n‚ÄúHow do I reset my password?‚Äù\n‚ÄúForgot my password, help‚Äù\n‚Üí same cache hit\n\nBut:\n‚ÄúHow do I reset my password for SSO?‚Äù\n‚ÄúReset password for API key‚Äù\n‚Üí no hit, even if embeddings are close.\n\nSeparately, on cost: we don‚Äôt store 768‚Äì3kD embeddings for cache keys. We compress meaning to a 72-D deterministic representation, which cuts vector storage and ANN overhead by ~10√ó. That‚Äôs where a lot of the hidden infra cost actually is.\n\nThis sits in front of an existing semantic cache ‚Äî not a rip-and-replace.",
                  "score": 1,
                  "created_utc": "2026-01-01 18:34:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwv15or",
          "author": "Practical-Rope-7461",
          "text": "Build that gateway requires 30 minutes vibe coding, with some very basic embedding. Do it yourself. \n\nBtw, this is not a good business idea for offering semantic caching.",
          "score": 1,
          "created_utc": "2025-12-31 04:35:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvd8p1",
          "author": "AftyOfTheUK",
          "text": "How did you measure/quantify the impact on the quality of the responses from your app?",
          "score": 1,
          "created_utc": "2025-12-31 06:02:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwve49n",
          "author": "Dramatic_Strain7370",
          "text": "Great point. I will try out bifrost. Was your application a customer service or IT service agent? where caching was paying dividends?",
          "score": 1,
          "created_utc": "2025-12-31 06:09:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxt3zu",
          "author": "Either_War7733",
          "text": "I keep seeing people saying this is a spam but how can you actually implement it without using the tools being promoted here?",
          "score": 1,
          "created_utc": "2025-12-31 16:44:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1xju1",
          "author": "nf_x",
          "text": "Isn‚Äôt getting embeddings another API call? üòâ",
          "score": 1,
          "created_utc": "2026-01-01 08:32:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4jxdj",
          "author": "elrosegod",
          "text": "Great story man, I'll need to keep this in mind when we start have unstructured querying in our apps.",
          "score": 1,
          "created_utc": "2026-01-01 19:27:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxc431q",
          "author": "baadir_",
          "text": "actually i try to jina ai rerank model . ƒ± pull 10 chunks but second layer jina rerank more relavan 5 chunks. \n\nƒ± think its good idea for relavancy",
          "score": 1,
          "created_utc": "2026-01-02 22:40:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxfyla6",
          "author": "Deep_Spice",
          "text": "This worked well for us too, but we eventually ran into cases where similarity wasn‚Äôt the failure mode,  reuse itself was invalid. Things like tenant boundaries, freshness requirements, or state-dependent tools made some ‚Äúhigh similarity‚Äù hits incorrect. We ended up treating semantic cache hits as conditional, similarity is necessary but not sufficient.",
          "score": 1,
          "created_utc": "2026-01-03 14:28:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxltf9t",
          "author": "adlx",
          "text": "It might be interesting in some cases. But in my case, the exact same question might have a different answer depending on the user, even depending on the day... \n\nFor example, \"when does my password expire\", Or \"when does my team take holidays this month\", \"whats is the state of my incidents\" (real questions our chatbot can answer).\n\nSame questions, same tools, obviously different response per user...",
          "score": 1,
          "created_utc": "2026-01-04 10:46:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt6oc5",
          "author": "qa_anaaq",
          "text": "How does the gateway work? You hit your API which hits your gateway which hits the provider‚Äôs API? Or the gateway becomes your API effectively and it‚Äôs just gateway -> provider?",
          "score": 1,
          "created_utc": "2025-12-30 22:15:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxndjy",
      "title": "Advanced RAG: Token Optimization and Cost Reduction in Production. We Cut Query Costs by 60%",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1pxndjy/advanced_rag_token_optimization_and_cost/",
      "author": "Electrical-Signal858",
      "created_utc": "2025-12-28 09:33:31",
      "score": 47,
      "num_comments": 13,
      "upvote_ratio": 0.9,
      "text": "Following up on my previous RAG post: we've optimized production RAG systems further and discovered cost optimizations that nobody talks about. This is specifically about reducing token spend without sacrificing quality.\n\n# The Problem We Solved\n\nOur RAG system was working well (retrieval was solid, generation was accurate), but the token spend kept climbing:\n\n* Hybrid retrieval (BM25 + vector): \\~2,000 tokens/query\n* Retrieved documents: \\~3,000 tokens\n* LLM processing: \\~500 tokens\n* **Total: \\~5,500 tokens/query**¬†√ó 100k queries/day = expensive\n\nAt $0.03 per 1K input tokens, that's¬†**$16.50/day just for input tokens**.¬†**$495/month**.\n\nWe asked: \"Can we get similar quality with fewer tokens?\"\n\nSpoiler: Yes. We reduced it to¬†**2,200 tokens/query average**¬†(60% reduction) while maintaining 92% accuracy (same as before).\n\n# The Optimizations\n\n# 1. Smart Document Chunking Reduces Retrieved Token Count\n\n**Before:**¬†Fixed 1,000-token chunks\n\n* Simple but wasteful\n* Lots of redundant context\n* Padding with irrelevant info\n\n**After:**¬†Semantic chunks with metadata filtering\n\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    from sentence_transformers import SentenceTransformer\n    import numpy as np\n    \n    class SemanticChunker:\n        def __init__(self, min_chunk_size=200, max_chunk_size=800):\n            self.min_chunk_size = min_chunk_size\n            self.max_chunk_size = max_chunk_size\n            self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        \n        def chunk_semantically(self, text, title=\"\"):\n            \"\"\"Break text into semantic chunks\"\"\"\n            sentences = text.split('. ')\n            \n            embeddings = self.model.encode(sentences)\n            chunks = []\n            current_chunk = []\n            current_embedding = None\n            \n            for i, sentence in enumerate(sentences):\n                current_chunk.append(sentence)\n                \n                if len(' '.join(current_chunk)) >= self.min_chunk_size:\n                    # Check semantic coherence\n                    chunk_embedding = self.model.encode(' '.join(current_chunk))\n                    \n                    if current_embedding is not None:\n                        # Cosine similarity with previous chunk\n                        similarity = np.dot(chunk_embedding, current_embedding) / (\n                            np.linalg.norm(chunk_embedding) * np.linalg.norm(current_embedding)\n                        )\n                        \n                        # If semantic break detected or max size reached\n                        if similarity < 0.6 or len(' '.join(current_chunk)) >= self.max_chunk_size:\n                            chunks.append({\n                                'content': ' '.join(current_chunk),\n                                'title': title,\n                                'tokens': len(' '.join(current_chunk).split())\n                            })\n                            current_chunk = []\n                            current_embedding = None\n                            continue\n                    \n                    current_embedding = chunk_embedding\n            \n            if current_chunk:\n                chunks.append({\n                    'content': ' '.join(current_chunk),\n                    'title': title,\n                    'tokens': len(' '.join(current_chunk).split())\n                })\n            \n            return chunks\n    \n\n**Result:**¬†Average chunk size went from 1,000 tokens ‚Üí 400 tokens (but more relevant). Retrieved fewer chunks but with less padding.\n\n# 2. Retrieval Pre-filtering Reduces What Gets Retrieved\n\n**Before:**¬†\"Get top-5 by relevance, send all to LLM\"\n\n**After:**¬†Multi-stage retrieval pre-filtering\n\n    def filtered_retrieval(query: str, documents: List[str], top_k=5):\n        \"\"\"Retrieve with automatic filtering\"\"\"\n        \n        # Stage 1: Broad retrieval (get more candidates)\n        candidates = vector_store.search(query, top_k=20)\n        \n        # Stage 2: Filter by relevance threshold\n        scored = [(doc, score) for doc, score in candidates]\n        high_confidence = [\n            (doc, score) for doc, score in scored \n            if score > 0.7  # Only confident matches\n        ]\n        \n        if not high_confidence:\n            high_confidence = scored[:5]  # Fallback to top-5\n        \n        # Stage 3: Deduplicate similar content\n        unique = []\n        seen_hashes = set()\n        \n        for doc, score in high_confidence:\n            doc_hash = hash(doc[:200])  # Hash of first 200 chars\n            \n            if doc_hash not in seen_hashes:\n                unique.append((doc, score))\n                seen_hashes.add(doc_hash)\n        \n        # Stage 4: Sort by relevance and return top-k\n        final = sorted(unique, key=lambda x: x[1], reverse=True)[:top_k]\n        \n        return [doc for doc, _ in final]\n    \n\n**Result:**¬†Retrieved fewer documents, but only high-confidence ones. Reduced retrieved token count by 40%.\n\n# 3. Query Simplification Before Retrieval\n\n**Before:**¬†Send raw user query to retriever\n\n    User: \"What are the refund policies for digital products if the customer received \n           a defective item and wants to know about international shipping costs?\"\n    (Complex, confusing retriever)\n    \n\n**After:**¬†Pre-process query to find key concepts\n\n    from langchain.chains import LLMChain\n    from langchain.prompts import PromptTemplate\n    \n    def simplify_query(query: str, llm) -> str:\n        \"\"\"Simplify query for better retrieval\"\"\"\n        \n        prompt = PromptTemplate(\n            input_variables=[\"query\"],\n            template=\"\"\"Extract the main topic from this query. \n            Remove adjectives, clarifications, and side questions.\n            \n            User query: {query}\n            \n            Simplified: \"\"\"\n        )\n        \n        chain = LLMChain(llm=llm, prompt=prompt)\n        \n        # Use cheaper model for this (gpt-3.5-turbo)\n        simplified = chain.run(query=query).strip()\n        \n        return simplified\n    \n    # Usage:\n    simplified = simplify_query(\n        \"What are the refund policies for digital products if the customer received \"\n        \"a defective item and wants to know about international shipping costs?\",\n        llm\n    )\n    # Result: \"refund policy digital products\"\n    \n\n**Result:**¬†Better retrieval queries ‚Üí fewer iterations ‚Üí fewer tokens.\n\n# 4. Response Compression Before Sending to LLM\n\n**Before:**¬†Send all retrieved documents as-is\n\n    Retrieved documents (all 3,000 tokens):\n    [Document 1: 1000 tokens]\n    [Document 2: 1000 tokens]\n    [Document 3: 1000 tokens]\n    \n\n**After:**¬†Compress while preserving information\n\n    def compress_context(documents: List[str], query: str, llm) -> str:\n        \"\"\"Compress documents while preserving relevant info\"\"\"\n        \n        compression_prompt = PromptTemplate(\n            input_variables=[\"documents\", \"query\"],\n            template=\"\"\"Summarize the following documents in as few words as possible \n            while preserving information relevant to the question.\n            \n            Question: {query}\n            \n            Documents:\n            {documents}\n            \n            Compressed summary:\"\"\"\n        )\n        \n        chain = LLMChain(llm=llm, prompt=compression_prompt)\n        \n        documents_text = \"\\n---\\n\".join(documents)\n        \n        compressed = chain.run(\n            documents=documents_text,\n            query=query\n        )\n        \n        return compressed\n    \n    # Usage:\n    context = compress_context(retrieved_docs, user_query, llm)\n    # 3000 tokens ‚Üí 800 tokens (still has all relevant info)\n    \n\n**Result:**¬†60-70% context reduction with minimal quality loss.\n\n# 5. Caching at the Context Level (Not Just Response Level)\n\n**Before:**¬†Cache full responses only\n\n    cache_key = hash(f\"{query}_{user_id}\")\n    cached_response = cache.get(cache_key)  # Only hits if identical query\n    \n\n**After:**¬†Cache compressed context\n\n    def cached_context_retrieval(query: str, user_context: str) -> str:\n        \"\"\"Retrieve and cache at context level\"\"\"\n        \n        # Hash just the query (not user context)\n        context_key = f\"context:{hash(query)}\"\n        \n        # Check if we've retrieved this query before\n        cached_context = cache.get(context_key)\n        \n        if cached_context:\n            return cached_context  # Reuse compressed context\n        \n        # If not cached, retrieve and compress\n        documents = retriever.get_relevant_documents(query)\n        compressed = compress_context(documents, query, llm)\n        \n        # Cache the compressed context\n        cache.set(context_key, compressed, ttl=86400)  # 24 hours\n        \n        return compressed\n    \n    # Usage:\n    context = cached_context_retrieval(query, user_context)\n    \n    # For identical queries from different users:\n    # User A: Retrieves, compresses (3000 tokens), caches\n    # User B: Uses cached context (0 tokens)\n    \n\n**Result:**¬†Context-level caching hits on 35% of queries (many users asking similar things).\n\n# 6. Token Counting Before Sending to LLM\n\n**Before:**¬†Blindly send context to LLM, hope it fits\n\n    response = llm.generate(system_prompt + context + user_query)\n    # Sometimes exceeds context window, sometimes wastes tokens\n    \n\n**After:**¬†Count tokens, optimize if needed\n\n    import tiktoken\n    \n    def smart_context_sending(context: str, query: str, llm, max_tokens=6000):\n        \"\"\"Send context to LLM, optimizing token usage\"\"\"\n        \n        enc = tiktoken.encoding_for_model(\"gpt-4\")\n        \n        # Count tokens in different parts\n        system_tokens = len(enc.encode(SYSTEM_PROMPT))\n        query_tokens = len(enc.encode(query))\n        context_tokens = len(enc.encode(context))\n        \n        total_input = system_tokens + query_tokens + context_tokens\n        \n        # If over budget, compress context further\n        if total_input > max_tokens:\n            compression_ratio = (total_input - max_tokens) / context_tokens\n            \n            # Aggressive compression if needed\n            compressed = aggressive_compress(context, compression_ratio)\n            context_tokens = len(enc.encode(compressed))\n            context = compressed\n        \n        # Now send to LLM\n        response = llm.generate(\n            system_prompt=SYSTEM_PROMPT,\n            context=context,\n            query=query\n        )\n        \n        return response\n    \n\n**Result:**¬†Stayed under token limits, never wasted tokens on too-large contexts.\n\n# The Results\n\n|Optimization|Before|After|Savings|\n|:-|:-|:-|:-|\n|Chunk size|1,000 tokens|400 tokens|Smaller chunks|\n|Retrieved docs|5 docs|3 docs|40% fewer|\n|Context compression|None|60% reduction|2x tokens|\n|Query simplification|None|Applied|Better retrieval|\n|Context caching|0% hit rate|35% hit rate|35% queries free|\n|Token counting|None|Applied|No waste|\n|**Total per query**|**5,500 tokens**|**2,200 tokens**|**60% reduction**|\n\n**Cost Impact:**\n\n* Before: 100k queries √ó 5,500 tokens √ó $0.03/1K =¬†**$16.50/day**¬†($495/month)\n* After: 100k queries √ó 2,200 tokens √ó $0.03/1K =¬†**$6.60/day**¬†($198/month)\n* **Savings: $297/month (60% reduction)**\n\n**Accuracy Impact:**\n\n* Before: 92% accuracy\n* After: 92% accuracy (unchanged)\n\n# Important Caveat\n\nThese optimizations come with tradeoffs:\n\n1. **Query simplification**¬†adds latency (extra LLM call, even if cheap)\n2. **Context compression**¬†could lose edge-case information\n3. **Caching**¬†reduces freshness (stale context for 24 hours)\n4. **Aggressive filtering**¬†might miss relevant documents\n\nWe accepted these tradeoffs. Your situation might differ.\n\n# Implementation Difficulty\n\n* **Easy:**¬†Token counting (1 hour)\n* **Easy:**¬†Retrieval filtering (2 hours)\n* **Medium:**¬†Query simplification (3 hours)\n* **Medium:**¬†Context compression (4 hours)\n* **Medium:**¬†Semantic chunking (4 hours)\n* **Hard:**¬†Context-level caching (5 hours)\n\n**Total:**¬†\\~19 hours of engineering work to save $297/month.\n\nPayback period: \\~1 month.\n\n# Code: Complete Pipeline\n\n    class OptimizedRAGPipeline:\n        def __init__(self, llm, retriever, cache):\n            self.llm = llm\n            self.retriever = retriever\n            self.cache = cache\n            self.encoder = tiktoken.encoding_for_model(\"gpt-4\")\n        \n        def process_query(self, user_query: str) -> str:\n            \"\"\"Complete optimized pipeline\"\"\"\n            \n            # Step 1: Simplify query\n            simplified_query = self.simplify_query(user_query)\n            \n            # Step 2: Retrieve with caching\n            context = self.cached_context_retrieval(simplified_query)\n            \n            # Step 3: Smart token handling\n            response = self.smart_context_sending(\n                context=context,\n                query=user_query\n            )\n            \n            return response\n        \n        def simplify_query(self, query: str) -> str:\n            \"\"\"Extract main topic from query\"\"\"\n            # Implementation from above\n            pass\n        \n        def cached_context_retrieval(self, query: str) -> str:\n            \"\"\"Retrieve and cache at context level\"\"\"\n            # Implementation from above\n            pass\n        \n        def smart_context_sending(self, context: str, query: str) -> str:\n            \"\"\"Send context with token optimization\"\"\"\n            # Implementation from above\n            pass\n    \n\n# Questions for the Community\n\n1. **Are you doing context-level caching?**¬†We found 35% hit rate. What's your experience?\n2. **How much quality loss do you see from compression?**¬†We measured \\~1-2% accuracy drop.\n3. **Query simplification latency trade:**¬†Is it worth the extra LLM call?\n4. **Semantic chunking:**¬†Are you doing it? How much better are results?\n5. **Token optimization:**¬†What's the best bang-for-buck optimization you've found?\n\n# Edit: Responses\n\n**On query simplification latency:**¬†\\~200-300ms added. With caching, only happens once per unique query. Worth it for most systems.\n\n**On context compression quality:**¬†We tested with GPT-3.5-turbo for compression (cheaper). Slightly more loss than GPT-4, but acceptable trade. Saves another $150/month.\n\n**On whether these are general:**¬†Yes, we tested on 3 different domains (legal, technical docs, customer support). Results were similar.\n\n**On LangChain compatibility:**¬†All of this integrates cleanly with LangChain's abstractions. No fighting the framework.\n\nWould love to hear if others have found different optimizations. Token cost is becoming the bottleneck.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1pxndjy/advanced_rag_token_optimization_and_cost/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nwcccn3",
          "author": "KegOfAppleJuice",
          "text": "Even though it's written up by AI, nice overview of things one can do for token optimisation",
          "score": 6,
          "created_utc": "2025-12-28 10:13:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg4nze",
          "author": "OnyxProyectoUno",
          "text": "The semantic chunking approach is solid but you're still doing fixed-size boundaries. The real gains come from content-aware splitting that preserves logical units.\n\nYour compression step is interesting but risky. Compressing 3000 tokens to 800 means you're losing 70% of the information and trusting an LLM to keep what matters. That works until it doesn't. The accuracy might look the same in aggregate but you're probably missing edge cases where the compressed context drops critical details.\n\nThe context-level caching is clever. Most people only cache final responses which misses the opportunity to reuse expensive retrieval work. 35% hit rate is good, though 24-hour TTL might be too aggressive depending on how often your docs change.\n\nOne thing that jumps out is you're optimizing downstream when the real waste might be upstream. If your chunks are poorly formed to begin with, you're just optimizing garbage. Bad chunks mean more retrieval attempts, more irrelevant context, more compression losses. Have you looked at what your semantic chunker is actually producing? Sometimes the chunking strategy matters more than all the downstream optimization combined.\n\nThe token counting is table stakes at this point. Surprised more people aren't doing that by default.\n\nWhat's your chunk quality like after the semantic splitting? Are you seeing clean breaks at logical boundaries or is it still cutting mid-thought sometimes?",
          "score": 4,
          "created_utc": "2025-12-28 23:10:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdi6kh",
          "author": "MathematicianSome289",
          "text": "This is great just don‚Äôt need the code",
          "score": 1,
          "created_utc": "2025-12-28 15:24:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwek7jz",
          "author": "saintskytower",
          "text": "Moderator Comment\n\nA reminder to everyone participating:\n\n‚Ä¢ Personal attacks, profanity, and calls to mass-report users or content violate Rule 2 (‚ÄúBe nice‚Äù) and will be removed.\n‚Ä¢ If you believe a post is spam or low quality, use the report function. Do not harass other users or moderators publicly.\n‚Ä¢ Moderation decisions are made by the moderation team based on the posted rules, not by hostile comments.\n\nThe post itself remains open for technical discussion. Keep replies focused on the substance of the content and within subreddit rules.",
          "score": 1,
          "created_utc": "2025-12-28 18:33:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfzxty",
          "author": "cmndr_spanky",
          "text": "Your solution isn‚Äôt good",
          "score": 1,
          "created_utc": "2025-12-28 22:44:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhlcvc",
              "author": "BothContribution7282",
              "text": "Why isn't the above solution good",
              "score": 1,
              "created_utc": "2025-12-29 04:07:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwhw75d",
                  "author": "cmndr_spanky",
                  "text": "‚ÄúSemantic chunking‚Äù in the implementation is effectively no different than picking a small min sized chunk and relying on the VDB default way of using a similarity score to bring back the right articles‚Ä¶ in fact the implementation is worse because you might bring back an 800 token sized chunk during usage when you only needed 200 tokens out of that chunk.\n\nIt‚Äôs almost 100% the above post is AI generated and the person who generated it didn‚Äôt even read it, nor are they commenting here. I wouldn‚Äôt trust any of it.",
                  "score": 1,
                  "created_utc": "2025-12-29 05:17:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q1qjkz",
      "title": "fastapi-fullstack v0.1.11 released ‚Äì now with LangGraph ReAct agent support + multi-framework AI options!",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q1qjkz/fastapifullstack_v0111_released_now_with/",
      "author": "VanillaOk4593",
      "created_utc": "2026-01-02 05:44:43",
      "score": 37,
      "num_comments": 5,
      "upvote_ratio": 0.96,
      "text": "Hey r/LangChain,\n\nFor those new or catching up: fastapi-fullstack is an open-source CLI generator (pip install fastapi-fullstack) that creates production-ready full-stack AI/LLM apps with FastAPI backend + optional Next.js 15 frontend. It's designed to skip boilerplate, with features like real-time WebSocket streaming, conversation persistence, custom tools, multi-provider support (OpenAI/Anthropic/OpenRouter), and observability via LangSmith.\n\nFull changelog: [https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template/blob/main/docs/CHANGELOG.md](https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template/blob/main/docs/CHANGELOG.md?referrer=grok.com)  \nRepo: [https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template](https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template?referrer=grok.com)\n\n**Full feature set:**\n\n* Backend: Async FastAPI with layered architecture, auth (JWT/OAuth/API keys), databases (PostgreSQL/MongoDB/SQLite with SQLModel/SQLAlchemy options), background tasks (Celery/Taskiq/ARQ), rate limiting, admin panels, webhooks\n* Frontend: React 19, Tailwind, dark mode, i18n, real-time chat UI\n* AI: Now supports **LangChain**, **PydanticAI**, and the new **LangGraph** (more below)\n* 20+ configurable integrations: Redis, Sentry, Prometheus, Docker, CI/CD, Kubernetes\n* Django-style CLI + production Docker with Traefik/Nginx reverse proxy options\n\n**Big news in v0.1.11 (just released):**  \nAdded **LangGraph as a third AI framework option** alongside LangChain and PydanticAI!\n\n* New --ai-framework langgraph CLI flag (or interactive prompt)\n* Implements **ReAct (Reasoning + Acting) agent pattern** with graph-based flow: agent node for LLM decisions, tools node for execution, conditional edges for loops\n* Full memory checkpointing for conversation continuity\n* WebSocket streaming via astream() with modes for token deltas and node updates (tool calls/results)\n* Proper tool result correlation via tool\\_call\\_id\n* Dependencies auto-added: langgraph, langgraph-checkpoint, langchain-core/openai/anthropic\n\nThis makes it even easier to build advanced, stateful agents in your full-stack apps ‚Äì LangGraph's graph architecture shines for complex workflows.\n\nLangChain community ‚Äì how does LangGraph integration fit your projects? Any features to expand (e.g., more graph nodes)? Contributions welcome! üöÄ",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LangChain/comments/1q1qjkz/fastapifullstack_v0111_released_now_with/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nx7ub4r",
          "author": "hopes_alive123",
          "text": "any sample projects built using this?",
          "score": 3,
          "created_utc": "2026-01-02 07:23:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7w4ea",
              "author": "VanillaOk4593",
              "text": "It's our internal tool and we use it in the projects we build for our clients so I cant share real examples but it's really easy to generate a new projects with just only 2 commands [https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template?tab=readme-ov-file#installation](https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template?tab=readme-ov-file#installation)",
              "score": 1,
              "created_utc": "2026-01-02 07:40:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7jxoa",
          "author": "mamaBiskothu",
          "text": "How do you ensure exactly once processing with celery?",
          "score": 1,
          "created_utc": "2026-01-02 05:55:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx85ip6",
          "author": "Preconf",
          "text": "Can I use phoenix for observability instead?",
          "score": 1,
          "created_utc": "2026-01-02 09:09:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx85t8b",
              "author": "VanillaOk4593",
              "text": "It's not implemented, so you would need to do it yourself. However, you can do anything with it since it only generates a project template, there are no limitations",
              "score": 2,
              "created_utc": "2026-01-02 09:12:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0t2qd",
      "title": "GraphQLite - Embedded graph database for building GraphRAG with SQLite",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q0t2qd/graphqlite_embedded_graph_database_for_building/",
      "author": "Fit-Presentation-591",
      "created_utc": "2026-01-01 01:59:35",
      "score": 29,
      "num_comments": 15,
      "upvote_ratio": 0.97,
      "text": "For anyone building GraphRAG systems who doesn't want to run Neo4j just to store a knowledge graph, I've been working on something that might help.\n\n\n\nGraphQLite is an SQLite extension that adds Cypher query support. The idea is that you can store your extracted entities and relationships in a graph structure, then use Cypher to traverse and expand context during retrieval. Combined with sqlite-vec for the vector search component, you get a fully embedded RAG stack in a single database file.\n\n\n\nIt includes graph algorithms like PageRank and community detection, which are useful for identifying important entities or clustering related concepts. There's an example in the repo using the HotpotQA multi-hop reasoning dataset if you want to see how the pieces fit together.\n\n\n\n\\`pip install graphqlite\\`\n\n\n\nGitHub: [https://github.com/colliery-io/graphqlite](https://github.com/colliery-io/graphqlite)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q0t2qd/graphqlite_embedded_graph_database_for_building/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nx0rjch",
          "author": "International_Quail8",
          "text": "Very cool!",
          "score": 2,
          "created_utc": "2026-01-01 02:41:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0qvkb",
          "author": "Don_Ozwald",
          "text": "Can someone explain to me, what‚Äôs useful about GraphRAG?",
          "score": 1,
          "created_utc": "2026-01-01 02:37:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0ut5y",
              "author": "Fit-Presentation-591",
              "text": "There‚Äôs an example in the project at [https://github.com/colliery-io/graphqlite/tree/main/examples/llm-graphrag](https://github.com/colliery-io/graphqlite/tree/main/examples/llm-graphrag)  \n\n\nTraditional RAG uses only vector similarity to find relevant documents. But multi-hop questions require connecting information across documents. By pre processing the documents and building up a set of relationships (COOCCURS in this case is set when two people are in the same sentence), we can use traversals to find information that may be related to the inital query and add it to the context for evaluation. \n\nThe concrete example from the demo is \n\n**Example:**¬†\"Were Scott Derrickson and Ed Wood of the same nationality?\"\n\n* Vector search finds \"Ed Wood\" but may miss \"Scott Derrickson\"\n* Graph traversal via COOCCURS edges discovers the connection\n* Community detection finds topically related articles",
              "score": 4,
              "created_utc": "2026-01-01 03:04:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx101ol",
                  "author": "Don_Ozwald",
                  "text": "I understand that. My point is more regarding, does the LLM understand that? I mean. Isn't GraphRAG just adding an unnecessary layer of abstraction for the LLM to struggle with, that are better dealt with just hybrid search. With the example in mind, it certainly seems so, as hybrid search would only fail on that one with k=1.",
                  "score": 1,
                  "created_utc": "2026-01-01 03:39:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx0uiez",
          "author": "BetaCuck80085",
          "text": "Wow, *exactly* what I‚Äôve been looking for. Use Neo4j on the job, wanted to use something more lightweight for a few personal endeavors. Really appreciate you sharing this.",
          "score": 1,
          "created_utc": "2026-01-01 03:02:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0uwh0",
              "author": "Fit-Presentation-591",
              "text": "Music to my ears !",
              "score": 1,
              "created_utc": "2026-01-01 03:05:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2p68j",
          "author": "eternviking",
          "text": "It's SQLite and you missed the chance to name it GRAPHite.",
          "score": 1,
          "created_utc": "2026-01-01 13:11:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2pqds",
              "author": "Fit-Presentation-591",
              "text": "I mean I can edit text its not like that's immutable.",
              "score": 2,
              "created_utc": "2026-01-01 13:15:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2y1vn",
                  "author": "eternviking",
                  "text": "Do it. Will be worth it.",
                  "score": 1,
                  "created_utc": "2026-01-01 14:15:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxn5ia3",
          "author": "Bjalal",
          "text": "Wÿ§ÿ´ÿ§ÿåÿå¬§„Ää Ÿ©ÿ±",
          "score": 1,
          "created_utc": "2026-01-04 15:58:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pykyuj",
      "title": "Are agent evals the new unit tests?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1pykyuj/are_agent_evals_the_new_unit_tests/",
      "author": "Hot-Guide-4464",
      "created_utc": "2025-12-29 12:23:16",
      "score": 21,
      "num_comments": 14,
      "upvote_ratio": 0.8,
      "text": "I‚Äôve been thinking about this a lot as agent workflows get more complex. Because in software, we‚Äôd never ship anything without unit tests. But right now most people just ‚Äútry a few prompts‚Äù and call it good. That clearly doesn‚Äôt scale once you have agents doing workflow automation or anything that has a real failure cost.\n\nSo I‚Äôm wondering if we‚Äôre moving to a future where CI-style evals become a standard part of building and deploying agents? Or am I overthinking it and we‚Äôre still too early for something this structured? I‚Äôd appreciate any insights on how folks in this community are running evals without drowning in infra.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1pykyuj/are_agent_evals_the_new_unit_tests/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nwjsa3j",
          "author": "Kortopi-98",
          "text": "I think evals have to become the new unit tests because once an agent interacts with real data or systems, \"vibes-based QA\" becomes a liability. So we‚Äôve been moving towards lightweight CI-like evals for our internal agents. Nothing super formal, just a set of representative tasks and expected behaviors. Just so you know, setting up the infra for this sucks unless you build your own harness. We switched to Moyai because they make this a lot less painful. Their eval workflow is basically: define agent, run them across diverse tasks, get diffs or outliers, done.",
          "score": 11,
          "created_utc": "2025-12-29 14:28:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjyb12",
              "author": "Hot-Guide-4464",
              "text": "Are you testing reasoning chains, final outputs or both?",
              "score": 2,
              "created_utc": "2025-12-29 15:00:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwk4dqi",
                  "author": "Kortopi-98",
                  "text": "Both. You can check the final result (e.g. reasoning steps, metrics pulled) but also the intermediate reasoning if you want consistency across steps. We treat it almost like snapshot tests for LLMs.",
                  "score": 2,
                  "created_utc": "2025-12-29 15:31:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwjhors",
          "author": "imnotafanofit",
          "text": "We started doing mini regression suite for agents. They're fast and lightweight but the biggest challenge is infra though. Spinning up evals can get expensive if you run them often.",
          "score": 6,
          "created_utc": "2025-12-29 13:25:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjnm9b",
              "author": "Hot-Guide-4464",
              "text": "Yep, infra costs are kind of an underrated part of this conversation. How are you managing overhead?",
              "score": 3,
              "created_utc": "2025-12-29 14:01:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwj9wzh",
          "author": "charlyAtWork2",
          "text": "Most of the time is only ETL.\n\nA boring step by step transformation with LLM in the middle.",
          "score": 5,
          "created_utc": "2025-12-29 12:31:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwld9em",
          "author": "MathematicianSome289",
          "text": "See em as more integration tests than unit tests. Evals test how the pieces work together. Units test the individual pieces.",
          "score": 3,
          "created_utc": "2025-12-29 19:01:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlo4uu",
          "author": "hidai25",
          "text": "I‚Äôm mostly with you, but I don‚Äôt think it maps 1:1 to unit tests. For agents it feels more like integration/regression tests, because the ‚Äúoutput string‚Äù is the least stable thing in the system. What *is* stable is behavior: did it call the right tools, avoid the wrong ones, return valid structure, and stay within time/$ budgets.\n\nThe only way I‚Äôve seen this not turn into eval-infra hell is keeping a small ‚Äúthis can‚Äôt break‚Äù suite in CI, running the bigger flaky stuff nightly, and turning every real failure into a test case. That‚Äôs when it starts compounding like real testing.\n\nFull disclosure: I‚Äôm building an OSS harness around exactly this idea (EvalView). If it‚Äôs useful, it‚Äôs here: [https://github.com/hidai25/eval-view]()",
          "score": 2,
          "created_utc": "2025-12-29 19:54:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa9f75",
          "author": "No-Common1466",
          "text": "Absolutely spot on ‚Äì agent evals are becoming the new unit tests (or more accurately, the new integration/regression suite) for anything that's going to touch production.\n\nTraditional software has deterministic functions, so unit tests with exact assertions work great. Agents? They're non-deterministic, multi-turn, tool-using beasts that can go off the rails in infinite creative ways. \"Try a few prompts\" catches the obvious hallucinations, but it won't save you when a real user phrashes something weirdly, injects noise, or triggers an edge case in a 20-step workflow.\n\nFrom LangChain's own State of Agent Engineering report (late 2025 survey): observability is basically table stakes now (\\~89% adoption), but offline evals on test sets are only at \\~52%. That gap is closing fast though ‚Äì teams shipping real-stakes agents (workflow automation, customer-facing, etc.) are treating evals as non-negotiable regression gates in CI/CD, just like we do with code.\n\nLangSmith/LangGraph is pushing hard here with multi-turn evals, trajectory evaluators, open evals catalog, and even running evals directly in Studio. Other tools (Braintrust, Promptfoo, etc.) are making it easy to fail builds on dropping robustness scores.\n\nThe missing piece a lot of folks run into: most evals focus on correctness (did it get the right answer?), but in production the bigger killer is robustness (does it still work when the input is sloppy, paraphrased, noisy, or adversarial?). That's where adversarial stress-testing comes in ‚Äì mutate prompts automatically and enforce invariants to quantify how \"flaky\" your agent really is.\n\nWe're still early-ish, but the direction is clear: no serious agent ships without automated evals in the pipeline. Curious ‚Äì what tools/workflows are you all using today to avoid drowning in manual testing?\n\n(Full disclosure: I'm building an open-source tool called Flakestorm exactly for the robustness side ‚Äì local-first adversarial mutations + reports. Early days, would love feedback if anyone wants to kick the tires. LInk here: https://github.com/flakestorm/flakestorm)",
          "score": 2,
          "created_utc": "2026-01-02 17:20:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhh1zj",
              "author": "Born_Owl7750",
              "text": "Interesting idea. Do you plan to support hosted model APIs from Azure or Open AI?",
              "score": 1,
              "created_utc": "2026-01-03 18:49:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxjo8vs",
                  "author": "No-Common1466",
                  "text": "Hi, yes but that would be on the cloud version. Still on the roadmap, to see if there's demand. There's a waitlist page on the website for those who are interested. I'll build it there's enough traction",
                  "score": 1,
                  "created_utc": "2026-01-04 01:28:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwm58wp",
          "author": "piyaviraj",
          "text": "We use evals as agent dev testing tool as a part of what we call agent development life cycle. Since it is part of the dev testing, every changes like prompt changes, tool changes, memory schema changes, etc are covered during the dev testing for the agents and their orchestration. This will ensure changes will not break the logic(reasoning) assumptions and as well as integration assumptions. However, we do not configure eval evaluation in a regular build CI or local build because for large project the token economics will be hard to justify at scale.",
          "score": 1,
          "created_utc": "2025-12-29 21:17:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx06uuq",
          "author": "Severe_Insurance_861",
          "text": "Within my team we call them regression eval. 400 examples covering a variety of scenarios.",
          "score": 1,
          "created_utc": "2026-01-01 00:28:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0znnd",
          "author": "Fit-Presentation-591",
          "text": "I‚Äôd say they‚Äôre more analogous to a mix of CI/CD and production testing TBH. They‚Äôre a bit more complex than your average unit test IME.",
          "score": 1,
          "created_utc": "2026-01-01 03:37:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q38zoo",
      "title": "Built a Lovable with Deepagents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q38zoo/built_a_lovable_with_deepagents/",
      "author": "Releow",
      "created_utc": "2026-01-03 23:01:18",
      "score": 19,
      "num_comments": 12,
      "upvote_ratio": 0.86,
      "text": "Hi guys, just wanted to share my project done used to deep dive into the deepagents architecture.\n\nIt is a little coding agent to build react app inspired by lovable.\n\n[https://github.com/emanueleielo/deepagents-open-lovable](https://github.com/emanueleielo/deepagents-open-lovable)\n\nAsking for feedback!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q38zoo/built_a_lovable_with_deepagents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nxjh686",
          "author": "hwchase17",
          "text": "This is great!",
          "score": 5,
          "created_utc": "2026-01-04 00:49:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlhfrh",
              "author": "Releow",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-01-04 08:57:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxk046x",
          "author": "vtrivedy-lc",
          "text": "Hey this is awesome!!  Also nice agent eng design with the subagent delegation.  Would love to hear how it went using deepagents and how we can make it better to help you build more cool stuff :). Will DM you!",
          "score": 3,
          "created_utc": "2026-01-04 02:33:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxk6oi0",
          "author": "hot4botz",
          "text": "nice work!",
          "score": 2,
          "created_utc": "2026-01-04 03:10:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkddnm",
          "author": "Feisty-Promise-78",
          "text": "Great work! Did you come up with the prompt as by yourself or ask AI to generate it or you copy pasted from some else?",
          "score": 2,
          "created_utc": "2026-01-04 03:49:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlhbrz",
              "author": "Releow",
              "text": "I‚Äôve been inspired from claude code frontend design skill",
              "score": 1,
              "created_utc": "2026-01-04 08:56:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxll5qg",
                  "author": "Feisty-Promise-78",
                  "text": "Can you share the link of it? I am now aware of it",
                  "score": 1,
                  "created_utc": "2026-01-04 09:31:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxkz6xb",
          "author": "monkeybrain_",
          "text": "Seems pretty interesting, will come back later to review in detail. Did you build the skills middleware yourself?",
          "score": 2,
          "created_utc": "2026-01-04 06:18:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlhxte",
              "author": "Releow",
              "text": "Yes, inspired by deepagents cli",
              "score": 2,
              "created_utc": "2026-01-04 09:02:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxl59pv",
          "author": "lundrog",
          "text": "Ill check this out",
          "score": 2,
          "created_utc": "2026-01-04 07:08:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlfd3h",
          "author": "ryan1257",
          "text": "I‚Äôm curious. Won‚Äôt your GitHub saved app simply be ingested by AI?",
          "score": 1,
          "created_utc": "2026-01-04 08:38:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q23mny",
      "title": "I mutation-tested my LangChain agent and it failed in ways evals didn‚Äôt catch",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q23mny/i_mutationtested_my_langchain_agent_and_it_failed/",
      "author": "No-Common1466",
      "created_utc": "2026-01-02 16:48:05",
      "score": 16,
      "num_comments": 4,
      "upvote_ratio": 0.91,
      "text": "I‚Äôve been working on an agent that passed all its evals and manual tests.\n\nOut of curiosity, I ran it through mutation testing  small changes like:\n\n\\- typos\n\n\\- formatting changes\n\n\\- tone shifts\n\n\\- mild prompt injection attempts\n\n\n\nIt broke. Repeatedly.\n\n\n\nSome examples:\n\n\\- Agent ignored tool constraints under minor wording changes\n\n\\- Safety logic failed when context order changed\n\n\\- Agent hallucinated actions it never took before\n\n\n\nI built a small open-source tool to automate this kind of testing (Flakestorm).\n\nIt generates adversarial mutations and runs them against your agent.\n\n\n\nI put together a minimal reproducible example here:\n\n GitHub repo:  [https://github.com/flakestorm/flakestorm](https://github.com/flakestorm/flakestorm)\n\nExample: [https://github.com/flakestorm/flakestorm/tree/main/examples/langchain\\_agent](https://github.com/flakestorm/flakestorm/tree/main/examples/langchain_agent)\n\n\n\nYou can reproduce the failure locally in \\~10 minutes:\n\n\\- pip install\n\n\\- run one command\n\n\\- see the report\n\n\n\nThis is very early and rough - I‚Äôm mostly looking for:\n\n\\- feedback on whether this is useful\n\n\\- what kinds of failures you‚Äôve seen but couldn‚Äôt test for\n\n\\- whether mutation testing belongs in agent workflows at all\n\n\n\nNot selling anything. Genuinely curious if others hit the same issues.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q23mny/i_mutationtested_my_langchain_agent_and_it_failed/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nxat6a4",
          "author": "Reasonable-Life7326",
          "text": "Of, that's rough. Glad you found it though!",
          "score": 1,
          "created_utc": "2026-01-02 18:51:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxcs9hd",
              "author": "No-Common1466",
              "text": "Yeah",
              "score": 1,
              "created_utc": "2026-01-03 00:51:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxd5g1d",
          "author": "erikg1337",
          "text": "Flakestorm looks very interesting..",
          "score": 1,
          "created_utc": "2026-01-03 02:07:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxd5tbp",
              "author": "No-Common1466",
              "text": "Thanks. it started as a way to catch failures in my own agents that evals missed.\nStill early, but I‚Äôm curious what others are using (or missing) today",
              "score": 1,
              "created_utc": "2026-01-03 02:09:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0dpty",
      "title": "mem0, Zep, Letta, Supermemory etc: why do memory layers keep remembering the wrong things?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q0dpty/mem0_zep_letta_supermemory_etc_why_do_memory/",
      "author": "nicolo_memorymodel",
      "created_utc": "2025-12-31 14:02:45",
      "score": 11,
      "num_comments": 7,
      "upvote_ratio": 0.87,
      "text": "Hi everyone,\nthis question is for people building AI agents that go a bit beyond basic demos.\nI keep running into the same limitation: many memory layers (mem0, Zep, Letta, Supermemory, etc.) decide for you what should be remembered.\n\nConcrete example: contracts that evolve over time\n‚Äì initial agreement\n‚Äì addenda / amendments\n‚Äì clauses that get modified or replaced\n\nWhat I see in practice:\nRAG: good at retrieving text, but it doesn‚Äôt understand versions, temporal priority, or clause replacement.\nVector DBs: they flatten everything, mixing old and new clauses together.\n\nMemory layers: they store generic or conversational ‚Äúmemories‚Äù, but not the information that actually matters, such as:\n\n-clause IDs or fingerprints\n-effective dates\n-active vs superseded clauses\n-relationships between different versions of the same contract\n\nThe problem isn‚Äôt how much is remembered, but what gets chosen as memory.\n\nSo my questions are:\nhow do you handle cases where you need structured, deterministic, temporal memory?\n\ndo you build custom schemas, graphs, or event logs on top of the LLM?\n\nor do these use cases inevitably require a fully custom memory layer?",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1q0dpty/mem0_zep_letta_supermemory_etc_why_do_memory/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nwxs397",
          "author": "southern_gio",
          "text": "Have you tried EverMemOS?",
          "score": 1,
          "created_utc": "2025-12-31 16:39:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2gvr1",
              "author": "nicolo_memorymodel",
              "text": "Honestly no, do you think it could be right for me? \nI am very interested in deciding what kind of memories to save mainly",
              "score": 1,
              "created_utc": "2026-01-01 11:57:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwz0z2s",
          "author": "rkpandey20",
          "text": "All of these memory layers just solve the problem of compaction of context. Compaction can be done on context from many different ways and depending on the use case, some may work better than others.¬†\nI am not sure if general purpose memory layer can solve all use cases. You may have to plug-in your code to extract important bits from the context and preserve it.¬†",
          "score": 1,
          "created_utc": "2025-12-31 20:27:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0qbi4",
          "author": "BeerBatteredHemroids",
          "text": "Idk wtf you're talking about. You can store anything you want in your database as memory. Why are you pretending like you have no choice in the matter? Im not even sure you know what you're talking about.",
          "score": 0,
          "created_utc": "2026-01-01 02:33:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2h7db",
              "author": "nicolo_memorymodel",
              "text": "Wow, quiet haha\n\nSaving them in my personal db means managing vdb, building ingestion and retrieval middleware, avoiding duplicates, and structuring the vdb so that it scales over time without creating hallucinations in memory retrieval. \n\nThere are great (cloud-managed) systems that do this, but I struggle to find one that fits very vertical use cases, they are mainly made for personal assistants",
              "score": 1,
              "created_utc": "2026-01-01 12:00:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzierz",
      "title": "I built a lightweight, durable full stack AI orchestration framework",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1pzierz/i_built_a_lightweight_durable_full_stack_ai/",
      "author": "Worried_Market4466",
      "created_utc": "2025-12-30 13:41:19",
      "score": 9,
      "num_comments": 5,
      "upvote_ratio": 0.91,
      "text": "Hello everyone,\n\nI've been building agentic webapps for around a year and a half now. Started with loops, then moved onto langgraph + Assistant UI. I've been using the lang ecosystem since their launch and have seen their evolution.\n\nIt's great and easy to build agents, but things got really frustrating once I needed more fine grained control, especially has a hard time building interesting user experiences. I loved the idea of building agents as DAGs, but I really wanted to model UIs in my flow as nodes too. \n\nDeployment was another nightmare. I am kinda cheap and the per node executed tax seemed ... Well, not great. But hey, the devs gotta eat.\n\n\nAround six months back, I snapped and started working on an idea i had been throwing around for a while. It's called Cascaide.\n\nCascaide is a lightweight low level AI orchestration framework written in typescript designed to run anywhere JS/TS can. It is primarily built for web applications. However, you can create headless AI agents and workflows with it in Node.js.\n\nHere are the reasons why you should try it out. We are in the process of opensourcing it(probably Jan first week).\n\nDeveloper Experience and UX\n\nüç± Learn Fast ‚Äì Simple, powerful abstractions you can learn over lunch\n\nüé® Build UI First ‚Äì UI and human-in-the-loop support is natural, not an add-on\n\nüèéÔ∏è Build Fast ‚Äì Single codebase (if you choose), no context switching\n\n‚è≥ Debug Easily ‚Äì Debugging and time-travel out of the box\n\nüåç Deploy Anywhere ‚Äì Deploy like any other application, no caveats\n\nü™∂ Stay Light ‚Äì Tiny bundle size, small enough to actually understand\n\nüîÆ UX Possibilities ‚Äì Enables novel UX patterns beyond chatbots: smart components, AI workflow visualization, and dynamic portalling\n\nüîå Extensibility ‚Äì Easily extend for custom capabilities via middleware patterns\n\nüßë‚ÄçüíªStack Agnostic ‚Äì Use with your favorite stack\n\nCosts\n\nZero orchestration costs in production \n\nLow TCO - far less moving parts to maintain\n\nTalent pool: enable any web dev to easily transition to AI engineering.\n\nObservability and reliability \n\n\nDurability: enterprise grade durability with no new overhead. Resume workflows post server/client crashes easily, or pick up weeks or months later.\n\nObservability and control: full observability out of the box with easy timetravel rollback and forking\n\n\nI have two production apps running on it and it's working great for us. It's very easy to use with serverless as well.\n\nI would love to talk to devs and get some feedback. We can do an early sneek peek!\n\nCheers!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1pzierz/i_built_a_lightweight_durable_full_stack_ai/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nwqjlwz",
          "author": "hyma",
          "text": "is this a competitor to langgraph?",
          "score": 2,
          "created_utc": "2025-12-30 14:43:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqla0m",
              "author": "Worried_Market4466",
              "text": "Yes, it's an alternative. Especially if you've been struggling with UI integrations, deployment/pricing and prefer to work with TS. A great use case would be AI SaaS.",
              "score": 1,
              "created_utc": "2025-12-30 14:52:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwt1dzz",
                  "author": "Plaszz",
                  "text": "Interesting!",
                  "score": 1,
                  "created_utc": "2025-12-30 21:50:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwu75h4",
          "author": "Live-Guitar-8661",
          "text": "Sounds interesting, we are building something similar. Would love to chat if you are up for it.",
          "score": 1,
          "created_utc": "2025-12-31 01:34:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww9ud3",
          "author": "Worried_Market4466",
          "text": "Opensourcing on 3rd January!",
          "score": 1,
          "created_utc": "2025-12-31 11:00:18",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1y63d",
      "title": "I wrote a beginner-friendly explanation of how Large Language Models work",
      "subreddit": "LangChain",
      "url": "https://blog.lokes.dev/how-large-language-models-work",
      "author": "Feisty-Promise-78",
      "created_utc": "2026-01-02 13:07:59",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1q1y63d/i_wrote_a_beginnerfriendly_explanation_of_how/",
      "domain": "blog.lokes.dev",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pzpltc",
      "title": "Building AI agents that actually learn from you, instead of just reacting",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1pzpltc/building_ai_agents_that_actually_learn_from_you/",
      "author": "Nir777",
      "created_utc": "2025-12-30 18:25:02",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.91,
      "text": "Just added a brand new tutorial about Mem0 to my \"Agents Towards Production\" repo. It addresses the \"amnesia\" problem in AI, which is the limitation where agents lose valuable context the moment a session ends.\n\nWhile many developers use standard chat history or basic RAG, Mem0 offers a specific approach by creating a self-improving memory layer. It extracts insights, resolves conflicting information, and evolves as you interact with it.\n\nThe tutorial walks through building a Personal AI Research Assistant with a two-phase architecture:\n\n* Vector Memory Foundation: Focusing on storing semantic facts. It covers how the system handles knowledge extraction and conflict resolution, such as updating your preferences when they change.\n* Graph Enhancement: Mapping explicit relationships. This allows the agent to understand lineage, like how one research paper influenced another, rather than just finding similar text.\n\nA significant benefit of this approach is efficiency. Instead of stuffing the entire chat history into a context window, the system retrieves only the specific memories relevant to the current query. This helps maintain accuracy and manages token usage effectively.\n\nThis foundation helps transform a generic chatbot into a personalized assistant that remembers your interests, research notes, and specific domain connections over time.\n\nPart of the collection of practical guides for building production-ready AI systems.\n\nCheck out the full repo with 30+ tutorials and give it a ‚≠ê if you find it useful:[https://github.com/NirDiamant/agents-towards-production](https://github.com/NirDiamant/agents-towards-production)\n\nDirect link to the tutorial:[https://github.com/NirDiamant/agents-towards-production/blob/main/tutorials/agent-memory-with-mem0/mem0\\_tutorial.ipynb](https://github.com/NirDiamant/agents-towards-production/blob/main/tutorials/agent-memory-with-mem0/mem0_tutorial.ipynb)\n\nHow are you handling long-term context? Are you relying on raw history, or are you implementing structured memory layers?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1pzpltc/building_ai_agents_that_actually_learn_from_you/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pys6wv",
      "title": "Implementing Production-Grade Human-in-the-Loop (HITL) with LangGraph for Sensitive Workflows",
      "subreddit": "LangChain",
      "url": "https://rampakanayev.com/blog/langgraph-human-in-the-loop",
      "author": "No-Conversation-8984",
      "created_utc": "2025-12-29 17:23:45",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1pys6wv/implementing_productiongrade_humanintheloop_hitl/",
      "domain": "rampakanayev.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1py0skm",
      "title": "Relay: a proposal for framework-agnostic agent orchestration",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1py0skm/relay_a_proposal_for_frameworkagnostic_agent/",
      "author": "bumswagger",
      "created_utc": "2025-12-28 19:57:35",
      "score": 6,
      "num_comments": 7,
      "upvote_ratio": 0.88,
      "text": "You have LangGraph agents, teammate has CrewAI, another team uses custom agents. Getting them to work together sucks.\n\nProposal: agents coordinate through \"relay repos\"\n\n* Shared versioned state store\n* Agents commit outputs, read inputs from previous commits\n* Branch for parallel experimentation\n* Policies define triggers (when agent A commits, run agent B)\n* MCP for agent interface - framework agnostic\n\nIt's like git for agent collaboration instead of code collaboration.\n\nWould this actually help? What's wrong with this model?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1py0skm/relay_a_proposal_for_frameworkagnostic_agent/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nwftvl5",
          "author": "rkpandey20",
          "text": "Just wondering if you can use A2A protocol to communicate.¬†",
          "score": 3,
          "created_utc": "2025-12-28 22:13:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwh9936",
              "author": "AdditionalWeb107",
              "text": "You have to implement all the low-level logic yourself in that case.",
              "score": 1,
              "created_utc": "2025-12-29 02:54:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwhlexy",
                  "author": "rkpandey20",
                  "text": "You are right. But it is not that much. It is just Agent card besides the agent endpoint. ¬†",
                  "score": 1,
                  "created_utc": "2025-12-29 04:07:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwg7ztz",
          "author": "Khade_G",
          "text": "Interesting idea. I think the ‚Äúgit for agents‚Äù framing is directionally right, but the pain usually isn‚Äôt state storage‚Ä¶ it‚Äôs semantics + contracts.\n\nWhat would help:\n- Framework-agnostic interfaces (MCP-style)\n- Versioned artifacts so runs are reproducible\n- Branching for experimentation\n\nPotential pitfalls id see:\n- ‚ÄúShared state‚Äù becomes a junk drawer unless outputs are strongly typed\n- Triggers quickly turn into a hidden workflow engine (Airflow/Temporal vibes)\n- Merge conflicts aren‚Äôt like code‚Ä¶ agents need domain-specific conflict rules\n- Latency + debugging get ugly if everything is commit/poll/trigger\n\nSo this works only if it‚Äôs really a typed artifact registry + eventing + policies, with git-like versioning as UX‚Ä¶ not literally ‚Äúgit as the runtime.‚Äù\n\nOtherwise it risks being a clever abstraction that mostly re-implements orchestration, but harder to debug.",
          "score": 2,
          "created_utc": "2025-12-28 23:28:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwh96o3",
          "author": "AdditionalWeb107",
          "text": "Checkout: [https://github.com/katanemo/plano](https://github.com/katanemo/plano) \\- Plano is delivery infrastructure for agents, and promises to be framework-agnostic and offers support for \"agents as tools\" via MCP. It doesn't have policy-defined orchestration, but has user-trigger orchestration worfklows built in.",
          "score": 1,
          "created_utc": "2025-12-29 02:54:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwikr0w",
          "author": "Ok-Priority35",
          "text": " were actively working on this at [slashmcp.com](http://slashmcp.com)",
          "score": 1,
          "created_utc": "2025-12-29 08:48:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q32p4h",
      "title": "Do you prefer to make Human-in-the-loop approvals on your phone or PC",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q32p4h/do_you_prefer_to_make_humanintheloop_approvals_on/",
      "author": "Antique_Try7765",
      "created_utc": "2026-01-03 18:52:36",
      "score": 5,
      "num_comments": 15,
      "upvote_ratio": 0.73,
      "text": "I am currently building an HITL system that initiates with systems but I want to understand how people prefer to make human input into their workflows or agents. >> [hitl.sh](http://hitl.sh)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q32p4h/do_you_prefer_to_make_humanintheloop_approvals_on/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nxiq316",
          "author": "Signal_Question9074",
          "text": "ok, let me save you from some hair pulling.\n\nGreat question furst if all becuase I've been building HITL into my own agent system, so I have some thoughts on this.\n\n**Short answer: PC for complex decisions, phone for quick approvals.**\n\nBut the real insight I learned: **the UX design matters more than the device.**\n\n**What I built:**\n\nMy system uses a request/response pattern (similar to how Claude Code handles it). When the agent needs human input, it doesn't truly \"pause\" mid-execution it completes its turn, returns a question marker, and the backend emits a socket event to the frontend:\n\n    // Agent returns this when it needs input\n    {\n      __askUser: true,\n      question: \"Which database should we use?\",\n      options: [\n        { label: \"PostgreSQL\", description: \"Relational, ACID compliant\" },\n        { label: \"MongoDB\", description: \"Document store, flexible schema\" }\n      ],\n      multiSelect: false\n    }",
          "score": 4,
          "created_utc": "2026-01-03 22:28:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxiq3ye",
              "author": "Signal_Question9074",
              "text": "Frontend catches this and renders clickable options. User picks one, answer goes back as a new message, agent continues with full context preserved.\n\n**Why this matters for phone vs PC:**\n\n1. **Single-select options** ‚Äî Work great on phone. Tap and done.\n2. **Multi-select or free-text** ‚Äî Better on PC. Typing on phone sucks.\n3. **Context-heavy decisions** ‚Äî PC. You need to see the conversation history.\n4. **Quick \"approve/reject\"** ‚Äî Phone. Perfect for notifications.\n\n**The two main patterns:**\n\n| Pattern | How it works | Best for |\n\n|---------|--------------|----------|\n\n| **Request/Response** | Agent completes turn, returns question, waits for answer as new message | Simpler to build, works well with existing chat UIs |\n\n| **True Interrupt** (LangGraph style) | Agent pauses mid-execution via `interrupt()`, state checkpointed, resumes with `Command(resume=...)` | More complex, but cleaner for multi-step approvals |\n\nLangGraph's `interrupt()` is cleaner architecturally execution actually pauses and resumes from the exact point. My approach re-executes with full context, which works but burns more tokens on long conversations.\n\n**What I'd recommend for your build:**\n\n1. **Start with simple options UI:** 2-4 clickable buttons covers 80% of cases\n2. **Add \"Other\" free-text fallback:** Users hate being boxed in\n3. **Persist pending questions:** If user refreshes page or switches devices, the question should still be there (I use Redis for this)\n4. **Handle timeouts gracefully:** What happens if user takes 10 minutes? Don't let the agent hang.\n\n**Honest take on phone:**\n\nI rarely approve from phone unless it's truly urgent. Context switching kills focus. But for async workflows where you're not actively watching the agent and phone notifications for \"needs approval\" ‚Üí tap to approve ‚Üí done. that flow is solid.\n\nWhat kind of decisions are you handling? Simple approve/reject, or complex multi-option stuff?",
              "score": 5,
              "created_utc": "2026-01-03 22:28:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxjkpg3",
                  "author": "Antique_Try7765",
                  "text": "Is there any out of the box solution I can use for this ?",
                  "score": 2,
                  "created_utc": "2026-01-04 01:08:32",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nxoyxs6",
                  "author": "vansterdam_city",
                  "text": "Add voice to text support for mobile and your text box issues are mitigated quite a bit.",
                  "score": 1,
                  "created_utc": "2026-01-04 20:54:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxj37vi",
          "author": "amilo111",
          "text": "Who are you building this for?  The answer to your question depends heavily on the target persona and use case.\n\nAlso what hitl interactions are you solving for?  I‚Äôve worked on applications that had hitl and we had to solve for multiple different interaction models depending on what the customer was trying to do and how we fit into their processes.",
          "score": 2,
          "created_utc": "2026-01-03 23:35:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxjkgsh",
              "author": "Antique_Try7765",
              "text": "The client is into content management and want to approve before content is automatically posted to their social media platform.",
              "score": 1,
              "created_utc": "2026-01-04 01:07:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxjrx5k",
                  "author": "pebblebypebble",
                  "text": "I‚Äôd definitely want to show them any related news stories that could shift meaning before pushing live if you want 1 click from mobile",
                  "score": 2,
                  "created_utc": "2026-01-04 01:48:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxk8ium",
          "author": "Reasonable-Life7326",
          "text": "PC all the way",
          "score": 2,
          "created_utc": "2026-01-04 03:21:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjrn6k",
          "author": "pebblebypebble",
          "text": "Phone if it is quick but mostly on my laptop‚Ä¶ either way I prefer to do any of it with audio playback at the same time. Some of it can be complicated and the audio playback as I read really helps.",
          "score": 1,
          "created_utc": "2026-01-04 01:46:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxjs8i8",
              "author": "Antique_Try7765",
              "text": "Are you available to try it and give me an honest opinion ?? >> [hitl.sh](http://hitl.sh)",
              "score": 1,
              "created_utc": "2026-01-04 01:49:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxjulg3",
                  "author": "pebblebypebble",
                  "text": "In the morning if you help me with answering some user research questions I have about professional teams working and communicating via AI. You can even send me a user onboarding test and I‚Äôll do a 30m greenfield voice of customer for that kind of a trade. My 30m for your 30m.",
                  "score": 1,
                  "created_utc": "2026-01-04 02:03:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pyp3x5",
      "title": "Everyone and their mother building AI agents while document extraction is still broken for most companies",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1pyp3x5/everyone_and_their_mother_building_ai_agents/",
      "author": "GloomyEquipment2120",
      "created_utc": "2025-12-29 15:27:31",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 0.67,
      "text": "While everyone's hyped about AI agents, I've been looking at a problem that's way more mundane but costs companies actual money: document processing still sucks.\n\nMost OCR systems plateau around 60% automation despite claiming high accuracy. The gap between test benchmarks and production reality is brutal - invoices with weird table layouts, multi-column forms, handwritten notes on printed docs, or just shitty scans break these pipelines constantly.\n\nSo I tried combining the two: built an agentic chatbot that uses a fine-tuned VLM (Qwen2.5-VL) for document extraction with a reflection-based verification process.\n\n**The basic idea:**\n\nInstead of rigid OCR ‚Üí parse ‚Üí extract, the agent plans extraction strategy based on document type, uses the VLM to pull information, then reflects on its own output before answering. If something looks wrong (numbers don't add up, dates are illogical, missing required fields), it re-runs extraction with refined prompts.\n\nThat self-correction loop is what pushes automation rates from 60% to 90%+. The system catches its own mistakes instead of sending garbage to humans.\n\n**What makes this different from traditional pipelines:**\n\n* VLM processes images directly, no separate OCR/layout analysis steps that create error cascades\n* Agent can adapt extraction strategy per document (financial tables ‚â† contracts)\n* Reflection pattern validates outputs before returning answers\n* Handles the real failure cases: nested tables, multi-column layouts, mixed print/handwriting\n\nBuilt the whole thing as a chatbot interface where you can ask questions about documents and it extracts + verifies answers. Uses langchain for the agent framework, fine-tuned the VLM on DocVQA dataset (real invoices/receipts/forms with all the complexity issues).\n\nWrote up the full implementation with code - dataset prep, fine-tuning workflow, agent setup with reflection patterns, deployment approach: link in comments.\n\nThe accuracy/automation tradeoff part was interesting to figure out. Traditional systems can't escape it (high confidence = low automation, low confidence = high errors), but reflection changes the game since you can be less certain initially because validation catches mistakes.\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1pyp3x5/everyone_and_their_mother_building_ai_agents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nwk3r6m",
          "author": "GloomyEquipment2120",
          "text": "You can read the details of the project here:  [https://ubiai.tools/agentic-document-intelligence-building-a-self-correcting-document-qna-pipeline/](https://ubiai.tools/agentic-document-intelligence-building-a-self-correcting-document-qna-pipeline/)",
          "score": 1,
          "created_utc": "2025-12-29 15:28:18",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwl07dx",
          "author": "Terrible_Attention83",
          "text": "What was the impact on throughout numbers? How much more time your implementation took compared to just the ocr one? How scalable would this approach be?",
          "score": 1,
          "created_utc": "2025-12-29 18:02:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpz4qh",
              "author": "GloomyEquipment2120",
              "text": "Good questions. The reflection approach is definitely slower per document since it makes multiple VLM passes for validation. Roughly 3-4x the processing time compared to standard OCR.\n\nScalability: This works best for moderate volumes where accuracy matters more than raw speed. For high-volume scenarios, you'd need aggressive batching or use it selectively on complex documents where basic OCR fails.\n\nThe key tradeoff: slower per-doc processing but way less human review overall, so total end-to-end time can actually improve despite the slower extraction.",
              "score": 1,
              "created_utc": "2025-12-30 12:40:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pywsxg",
      "title": "[Open Source] LangGraph Threads Export Tool - Backup, migrate, and own your conversation data",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1pywsxg/open_source_langgraph_threads_export_tool_backup/",
      "author": "SignatureHuman8057",
      "created_utc": "2025-12-29 20:13:30",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "Hey everyone! üëã\n\nI built a tool to solve a problem I had with LangGraph Cloud and wanted to share it with the community.\n\n### The Problem\n\nI had two LangGraph Cloud deployments - a production one (expensive) and a dev one (cheaper). I wanted to:\n- Migrate all user conversations from prod to dev\n- Keep the same thread IDs so users don't lose their chat history\n- Preserve multi-tenancy (each user only sees their own threads)\n\nThere's no built-in way to do this in LangGraph Cloud, so I built one.\n\n### What This Tool Does\n\n**Export your LangGraph threads to:**\n- üìÑ **JSON file** - Simple backup you can store anywhere\n- üêò **PostgreSQL database** - Own your data with proper schema and indexes\n- üîÑ **Another deployment** - Migrate between environments\n\n**What gets exported:**\n- Thread IDs (preserved exactly)\n- Metadata (including `owner` for multi-tenancy)\n- Full checkpoint history\n- Conversation values/messages\n\n### Quick Example\n\n```bash\n# Export all threads to JSON\npython migrate_threads.py \\\n  --source-url https://my-deployment.langgraph.app \\\n  --export-json backup.json\n\n# Export to PostgreSQL\npython migrate_threads.py \\\n  --source-url https://my-deployment.langgraph.app \\\n  --export-postgres\n\n# Migrate between deployments\npython migrate_threads.py \\\n  --source-url https://prod.langgraph.app \\\n  --target-url https://dev.langgraph.app \\\n  --full\n```\n\n### Why You Might Need This\n\n- **Cost optimization** - Move from expensive prod to cheaper deployment\n- **Backup before deletion** - Export everything before removing a deployment\n- **Compliance** - Store conversation data in your own database\n- **Analytics** - Query your threads with SQL\n- **Disaster recovery** - Restore from JSON backup\n\n### GitHub\n\nüîó **[github.com/farouk09/langgraph-threads-migration](https://github.com/farouk09/langgraph-threads-migration)**\n\nMIT licensed, PRs welcome!\n\n---\n\n### Note for deployments with custom auth\n\nIf you use Auth0 or custom authentication, you'll need to temporarily disable it during export (the tool uses the LangSmith API key, not user tokens). Just set `\"auth\": null` in your `langgraph.json`, export, then re-enable.\n\n---\n\nHope this helps someone! Let me know if you have questions or feature requests. üôÇ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1pywsxg/open_source_langgraph_threads_export_tool_backup/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q0uf3q",
      "title": "Is it one big agent, or sub-agents?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q0uf3q/is_it_one_big_agent_or_subagents/",
      "author": "AdditionalWeb107",
      "created_utc": "2026-01-01 03:16:38",
      "score": 4,
      "num_comments": 6,
      "upvote_ratio": 0.84,
      "text": "If you are building agents, are you resorting to send traffic to one agent that is responsible for all sub-tasks (via its instructions) and packaging tools intelligently - or are you using a lightweight router to define/test/update sub-agents that can handle user specific tasks.\n\nThe former is a simple architecture, but I feel its a large bloated piece of software that's harder to debug. The latter is cleaner and simpler to build (especially packaging tools) but requires a great/robust orchestration/router.\n\nHow are you all thinking about this? Would love framework-agnostic approaches because these frameworks add very little value and become an operational nightmare as you push agents to production.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1q0uf3q/is_it_one_big_agent_or_subagents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nx1xzxr",
          "author": "Seeking_Adrenaline",
          "text": "Start with s single prompt and tools. Grow and split as needed. Use evals to measure this and detect when to change and if you are making progress. Every additional prompt layer is a slower end result, take this into consideration.",
          "score": 3,
          "created_utc": "2026-01-01 08:37:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx44q1p",
              "author": "AdditionalWeb107",
              "text": "Managing the split feels very painful - as I am now in the business of routing and middleware.",
              "score": 0,
              "created_utc": "2026-01-01 18:12:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx58xdh",
          "author": "attn-transformer",
          "text": "Once you include more than ~8 tools then the llm may not select the right tool. This is the limitation of the llm that should drive the decision between single or multi agent.",
          "score": 2,
          "created_utc": "2026-01-01 21:36:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx593uq",
              "author": "AdditionalWeb107",
              "text": "interesting way to think about - is this practical/empirical testing or a fact that more than 8 tools and the models hallucinate?",
              "score": 0,
              "created_utc": "2026-01-01 21:37:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx5dsu0",
                  "author": "attn-transformer",
                  "text": "It‚Äôs just fact. A tool call is a decision that the llm needs to make. The more options the more confusion. Humans have this exact limitation.\n\nEmpirically after 8-10 tool choices the llm starts to make mistakes.",
                  "score": 1,
                  "created_utc": "2026-01-01 22:00:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx663tu",
          "author": "Zealousideal_Emu7912",
          "text": "Depends on how long a single agent's context gets. I find that agents handle upto 100k context length quite well these days, so I tend to keep to simple and have everything in one prompt. I only use sub-agents when there's an isolated context-intensive task with little to no follow ups for the sub-agent.",
          "score": 1,
          "created_utc": "2026-01-02 00:36:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q15sf0",
      "title": "What is the best embedding and retrieval model both OSS/proprietary for technical texts (e.g manuals, datasheets, and so on)?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q15sf0/what_is_the_best_embedding_and_retrieval_model/",
      "author": "Imaginary-Bee-8770",
      "created_utc": "2026-01-01 14:45:57",
      "score": 4,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1q15sf0/what_is_the_best_embedding_and_retrieval_model/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nx39059",
          "author": "Khade_G",
          "text": "I‚Äôd think in embedding + reranker pairs (since rerankers usually move quality more than swapping embeddings).\n\nIf you just want a solid default:\n- Proprietary (pretty easy): OpenAI text-embedding-3-large + a good reranker.  Ôøº\n- OSS (best all-around starting point): BAAI bge-m3 (it‚Äôs popular for RAG and supports multiple retrieval styles).  Ôøº\n- Another strong proprietary option: Cohere Embed v3/v4 (used a lot in retrieval stacks).  Ôøº\n\nFor tech docs, I think you‚Äôll usually get the biggest impact from clean chunking (sections/headers) + hybrid retrieval (BM25 + embeddings) + reranking, vs trying to find the one perfect embedding model.Ôøº",
          "score": 1,
          "created_utc": "2026-01-01 15:25:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3f1pv",
              "author": "Imaginary-Bee-8770",
              "text": "Thanks a lot! \n\nWe were exploring better parsing options as well. We are currently using LlamaParse with the cost-effective tier option.   \n  \nWe were exploring OSS options such as Paddle, MinerU, Docling, Marker and so on, there are strong proprietary options as well such as Unstructured, Landing AI, Mathpix and many more.\n\nDo you have any insights that you could share with us please? Thanks in advance",
              "score": 1,
              "created_utc": "2026-01-01 15:58:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxb8dk0",
                  "author": "Ok-Introduction354",
                  "text": "How are you evaluating your parsing quality currently? Is it mainly through eyeballing or do you have some kind of an eval set with <documents, ground truth parse> pairs?\n\nIf you don't have an eval set, I'd recommend creating one even if it's small. That way you'll be able to cleanly compare the different parsing alternatives.\n\nApart from the ones you mentioned, I've heard Gemini-3-Flash, DeepSeek-OCR and Mistral are quite good as well. Among the proprietary options, I've heard good things about Reducto and Nanonets.\n\n[https://github.com/opendatalab/OmniDocBench](https://github.com/opendatalab/OmniDocBench) looked quite relevant as well.",
                  "score": 2,
                  "created_utc": "2026-01-02 20:03:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxc207l",
                  "author": "Khade_G",
                  "text": "Totally agree with the @ok-introduction‚Ä¶ if you don‚Äôt have even a small eval set, you‚Äôre mostly guessing.\n\nWhat‚Äôs worked for me is creating a small, realistic benchmark (dozens of docs is enough) that matches your actual inputs: clean PDFs, messy scans, tables, multi-column manuals. Instead of judging parses by how ‚Äúnice‚Äù they look, evaluate them by downstream RAG behavior: does the right chunk get retrieved, and can the model answer questions that depend on a specific section or table?\n\nIn practice, a hybrid approach usually wins. Use a fast, cheap parser for most pages, detect the hard ones (tables, scans, math), and only route those to heavier tools. Also, reliable structure matters more than people expect‚Ä¶ if headings and reading order are right, retrieval quality improves even with the same embeddings.",
                  "score": 1,
                  "created_utc": "2026-01-02 22:29:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q30qw1",
      "title": "How are fintech companies auditing what their AI actually does?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q30qw1/how_are_fintech_companies_auditing_what_their_ai/",
      "author": "Such-Persimmon1622",
      "created_utc": "2026-01-03 17:39:19",
      "score": 4,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "I keep reading about companies adding AI to handle refunds,\n\n  chargebacks, account changes, etc. But I never see anyone\n\n  talk about how they track what the AI decided or why.\n\n\n\n  Is everyone just logging stuff to a database and hoping\n\n  for the best? Genuinely curious what the reality looks like.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q30qw1/how_are_fintech_companies_auditing_what_their_ai/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nxh8gdx",
          "author": "indutrajeev",
          "text": "Same KPI‚Äôs as when you had humans do this. Don‚Äôt see why that would change?",
          "score": 2,
          "created_utc": "2026-01-03 18:10:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxp9hbt",
              "author": "ChanceKale7861",
              "text": "LOTS. But what you mention aren‚Äôt audits. run an end to end audit of your code, access controls, privileges and access controls for agents, etc‚Ä¶ but because these are agents and no people, your threat model is different. So no, these aren‚Äôt the same as for humans.",
              "score": 1,
              "created_utc": "2026-01-04 21:43:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhq8bl",
          "author": "Khade_G",
          "text": "A lot of companies are basically ‚Äúlogging and praying‚Äù early on. In practice, the mature setups look more like ‚Äî> every AI action becomes an auditable record. That usually means you log the user request, the final decision, the key facts the model relied on (inputs/retrieved docs), the tool calls it made, and a reason code (policy section / rule) that a human can understand. Then you add guardrails like confidence thresholds, human review for edge cases, and dashboards for reversal rates and chargeback outcomes.\n\nI think the real shift is treating it like payments/fraud systems‚Ä¶ I.e. you don‚Äôt just want an answer, you want traceability and the ability to replay ‚Äúwhy did this happen?‚Äù months later. If a company can‚Äôt do that, they‚Äôre taking on a lot of risk and usually don‚Äôt realize it until something blows up.",
          "score": 2,
          "created_utc": "2026-01-03 19:31:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpadpf",
              "author": "ChanceKale7861",
              "text": "BOOM! THANK YOU! Someone gets it! :)\n\nThere is a paradigm shift in observability, because agents don‚Äôt stay siloed. The value is when agents don‚Äôt have to regard silos or the arbitrary boundaries of an org, or the dumbass entrenched folks, who always want everyone to stay in their lane. I‚Äôm ready for the many roles that aren‚Äôt necessary and the folks who are joy status quo and stability to be forced out of the workforce in favor of the largest orgs failing and shutting down, and people using tech to become direct competitors with their employers overnight. \n\nIt‚Äôs basically the dicks sporting goods case study at scale. Either put your employer out of business, or let a competitor do it for you.",
              "score": 2,
              "created_utc": "2026-01-04 21:48:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxihq2w",
          "author": "adiberk",
          "text": "Build our own audit on top of existing framework (Agno).\n\nBasically audit and track every run providing fast debugging and easy ways of viewing conversations. We auto run evals to catch things early.\n\nYou can a lot of this from tracing providers as well, but this integration allows us to really customize it to our needs",
          "score": 2,
          "created_utc": "2026-01-03 21:46:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpagpo",
              "author": "ChanceKale7861",
              "text": "whoopp! Let‚Äôs go agno agentOS!!!!!\n\nHell yeah!",
              "score": 1,
              "created_utc": "2026-01-04 21:48:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxpcmcu",
                  "author": "adiberk",
                  "text": "Yeah they are awesome very active. Give it a run. The only downside is now dynamic configuration, though that is in the roadmap.\n\nI have personally had my own entire framework on top of Agno to enable high db based customization lol",
                  "score": 2,
                  "created_utc": "2026-01-04 21:58:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhf0ju",
          "author": "Born_Owl7750",
          "text": "There are many ways to do it.\nThere are logging and tracing stack for almost any framework.\nSemantic kernel - Application insights \nLangchain/langgraph - Langsmith\n\nSome might have custom dashboards done on top of the data. This dashboard itself has the scope to be a whole new project.\n\nWe have even done power bi dashboards with analytics on top of chat history. It will need some form of data processing pipelines that run behind the scene",
          "score": 1,
          "created_utc": "2026-01-03 18:40:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxp9lij",
              "author": "ChanceKale7861",
              "text": "But this doesn‚Äôt touch anything related to security, GRC, business process controls, etc.",
              "score": 1,
              "created_utc": "2026-01-04 21:44:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxp93x6",
          "author": "ChanceKale7861",
          "text": "Career auditor here‚Ä¶ companies don‚Äôt have the capability to do this in a real way,  it they sure can ‚Äúbe compliant‚Äù which doesn‚Äôt mean the audits are effective for fixing anything.",
          "score": 1,
          "created_utc": "2026-01-04 21:42:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q26o5g",
      "title": "How are you handling governance and guardrails in your LangChain agents?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q26o5g/how_are_you_handling_governance_and_guardrails_in/",
      "author": "forevergeeks",
      "created_utc": "2026-01-02 18:37:35",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.84,
      "text": "Hi Everyone,\n\nHow are you handling governance/guardrails in your agents today? Are you building in regulated fields like healthcare, legal, or finance and how are you dealing with compliance requirements?\n\nFor the last year, I've been working on¬†SAFi, an open-source governance engine that wraps your LLM agents in ethical guardrails. It can block responses before they  are delivered to the user, audit every decision, and detect behavioral drift over time.\n\nIt's based on four principles:\n\n* **Value Sovereignty -** You decide the values your AI enforces, not the model provider\n* **Full Traceability -** Every response is logged and auditable\n* **Model Independence -** Switch LLMs without losing your governance layer\n* **Long-Term Consistency -** Detect and correct ethical drift over time\n\nI'd love feedback on how SAFi could¬†complement¬†the work you're doing with LangChain:\n\n* **Live demo:**¬†[safi.selfalignmentframework.com](https://safi.selfalignmentframework.com/)\n* **GitHub:**¬†[github.com/jnamaya/SAFi](https://github.com/jnamaya/SAFi)\n\nTry the pre-built agents:¬†*SAFi Guide*¬†(RAG),¬†*Fiduciary*, or¬†*Health Navigator*.\n\nHappy to answer any questions!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q26o5g/how_are_you_handling_governance_and_guardrails_in/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nxazax3",
          "author": "AdditionalWeb107",
          "text": "Great work OP. But look at Plano: [https://github.com/katanemo/plano](https://github.com/katanemo/plano) agent filter chains: https://docs.planoai.dev/concepts/filter\\_chain.html. Framework-agnostic and easily extensible.",
          "score": 1,
          "created_utc": "2026-01-02 19:19:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2lyx4",
      "title": "Langgraph history summarisation",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q2lyx4/langgraph_history_summarisation/",
      "author": "ankitsi9gh",
      "created_utc": "2026-01-03 05:16:11",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "How do you guys summarise old chats in langgraph with trim_message, without deleting or removing old chats from state. ??\n\nLike for summarizing should I use langmem our build custom node and also for trim_message what would be best token base trimming or message count base trimming ??",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1q2lyx4/langgraph_history_summarisation/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q3a09f",
      "title": "Autonomous Manim Coder with Deepagents",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/1krmmk8t08bg1",
      "author": "Eastwindy123",
      "created_utc": "2026-01-03 23:43:28",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.7,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1q3a09f/autonomous_manim_coder_with_deepagents/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxjz7go",
          "author": "vtrivedy-lc",
          "text": "Hey this is awesome thanks for sharing!!  Would love to feature it and hear about how using DeepAgents went (and how we can make it better).  Will DM you!\n\nWas a math major in undergrad, Manim was always magic :)",
          "score": 1,
          "created_utc": "2026-01-04 02:28:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}