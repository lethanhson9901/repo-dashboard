{
  "metadata": {
    "last_updated": "2026-02-19 09:09:59",
    "time_filter": "week",
    "subreddit": "LangChain",
    "total_items": 20,
    "total_comments": 72,
    "file_size_bytes": 100209
  },
  "items": [
    {
      "id": "1r38uf7",
      "title": "I built a Recursive Language Model (RLM) with LangGraph that spawns child agents to beat context rot",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r38uf7/i_built_a_recursive_language_model_rlm_with/",
      "author": "DolphinSyndrome",
      "created_utc": "2026-02-12 23:18:56",
      "score": 27,
      "num_comments": 3,
      "upvote_ratio": 0.94,
      "text": "Hey r/LangChain ðŸ‘‹\n\nI builtÂ **Fractal Context**Â â€” a LangGraph implementation of Recursive Language Models that solves the \"context rot\" problem by letting an LLMÂ **recursively spawn child agents**Â to process large text.\n\n**The problem:**Â When you stuff a massive document into an LLM, attention degrades â€” details in the middle get \"forgotten\" and the model starts hallucinating. This is context rot.\n\n**The solution:**Â Instead of cramming everything into one prompt, the parent agent:\n\n1. Evaluates if the context is too large\n2. Uses a Python REPL to slice the text into chunks\n3. CallsÂ `delegate_subtask` Â to spawn aÂ **child agent**Â atÂ `depth + 1`\n4. Each child processes its chunk and reports back\n5. The parent synthesizes all answers\n\nThe recursion is depth-limited to prevent runaway chains.\n\n**The \"Glass Box\" UI:**Â Built with Chainlit, the UI shows nested steps in real-time so you can actuallyÂ *see*Â the recursion happening:\n\n* ðŸ§ Â **Thinkingâ€¦**Â â€” LLM reasoning (token by token)\n* ðŸ’»Â **Codingâ€¦**Â â€” when the agent writes Python to slice text\n* ðŸ”€Â **Sub-Agent (Depth N)**Â â€” child agents spawning and reporting\n\n**Tech stack:**\n\n* LangGraph (StateGraph with conditional edges)\n* LangChain + Groq API (Llama 3.3 70B)\n* Chainlit for the UI\n* Python 3.11+\n\n**Repo:**Â [github.com/Dolphin-Syndrom/fractal-context](https://github.com/Dolphin-Syndrom/fractal-context)\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r38uf7/i_built_a_recursive_language_model_rlm_with/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o54lfa3",
          "author": "Don_Ozwald",
          "text": "I just wish people would use the word recursion appropriately. \n\nBut I like what you describe with the UI, well done there!\n\nEdit: I see now your implementation is much closer to actual recursion than what it usually is when the term â€œRecursive language modelâ€ is thrown around. Bravo!",
          "score": 2,
          "created_utc": "2026-02-13 07:35:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55sy2o",
              "author": "DolphinSyndrome",
              "text": "Thanks, i appreciate you taking a second look. Ik the term gets thrown around lossely so i tried my best to make the architecture authentically recursive as per the published paper",
              "score": 2,
              "created_utc": "2026-02-13 13:31:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5r2lga",
          "author": "jovansstupidaccount",
          "text": "[https://github.com/jovanSAPFIONEER/Network-AI](https://github.com/jovanSAPFIONEER/Network-AI) This might help with future issues if you need them to be coordinated",
          "score": 1,
          "created_utc": "2026-02-16 21:03:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8k1qu",
      "title": "Building an opensource Living Context Engine",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/wo2lnacmfckg1",
      "author": "DeathShot7777",
      "created_utc": "2026-02-19 00:12:36",
      "score": 27,
      "num_comments": 9,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1r8k1qu/building_an_opensource_living_context_engine/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o666sm6",
          "author": "Reasonable-Froyo3181",
          "text": "Ok",
          "score": 3,
          "created_utc": "2026-02-19 02:22:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66anvg",
              "author": "DeathShot7777",
              "text": "Thanks",
              "score": 2,
              "created_utc": "2026-02-19 02:44:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o65ozs6",
          "author": "VanillaOk4593",
          "text": "Obsidian on steroids!",
          "score": 1,
          "created_utc": "2026-02-19 00:39:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65zx0p",
              "author": "DeathShot7777",
              "text": "ðŸ˜",
              "score": 1,
              "created_utc": "2026-02-19 01:42:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o66irxh",
          "author": "SithLordRising",
          "text": "Very cool. Working on a thinking engine myself",
          "score": 1,
          "created_utc": "2026-02-19 03:33:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o677mug",
              "author": "DeathShot7777",
              "text": "Thanks. What's your approach?",
              "score": 1,
              "created_utc": "2026-02-19 06:32:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67jxom",
          "author": "Msense_",
          "text": "Impressive!",
          "score": 1,
          "created_utc": "2026-02-19 08:23:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67k739",
              "author": "DeathShot7777",
              "text": "â¤ï¸",
              "score": 1,
              "created_utc": "2026-02-19 08:25:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67ka5m",
          "author": "DeathShot7777",
          "text": "Thanks for all the github stars idk where they r coming from. But holly shit 496 stars ðŸ˜­",
          "score": 1,
          "created_utc": "2026-02-19 08:26:26",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3mp8b",
      "title": "Semantic chunking + metadata filtering actually fixes RAG hallucinations",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r3mp8b/semantic_chunking_metadata_filtering_actually/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-13 11:25:21",
      "score": 26,
      "num_comments": 10,
      "upvote_ratio": 0.96,
      "text": "I noticed that most people don't realize their chunking and retrieval strategy might be causing their RAG hallucinations.\n\nFixed-size chunking (split every 512 tokens regardless of content) fragments semantic units. Single explanation gets split across two chunks. Tables lose their structure. Headers separate from data. The chunks going into your vector DB are semantically incoherent.\n\nI've been testing semantic boundary detection instead where I use a model to find where topics actually change. Generate embeddings for each sentence, calculate similarity between consecutive ones, split when it sees sharp drops. The results are variable chunks but each represents a complete clear idea.\n\nThis alone gets 2-3 percentage points better recall but the bigger win for me was adding metadata. I pass each chunk through an LLM to extract time periods, doc types, entities, whatever structured info matters and store that alongside the embedding.\n\nThis metadata filters narrow the search space first, then vector similarity runs on that subset. Searching 47 relevant chunks instead of 20,000 random ones.\n\nFor complex documents with inherent structure this seems obviously better than fixed chunking. Anyway thought I should share. :)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r3mp8b/semantic_chunking_metadata_filtering_actually/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o55zr5a",
          "author": "timmy166",
          "text": "We havenâ€™t used naive fixed-size chunking since the first pass at RAG last year.\n\nA simple summarization pass does wonders before getting into more advanced collection techniques.",
          "score": 3,
          "created_utc": "2026-02-13 14:08:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5g6by6",
              "author": "Sungog1",
              "text": "Summarization really is a game changer. It helps maintain context and coherence, especially with complex documents. What techniques do you find most effective for summarizing before chunking?",
              "score": 1,
              "created_utc": "2026-02-15 03:14:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5iuk6v",
                  "author": "timmy166",
                  "text": "It entirely depends on the downstream agentâ€™s task and the dimensionality of the data. I work with a lot of source code so I might prefer to summarize the import statements as a first pass to get the skeleton of a repo. Then if I were to start using the codebase, I might want to get function signatures before extracting the actual logic. \n\nMost frontier models now can ingest a whole file (depending on lines of code and if itâ€™s a generated file). Iâ€™m even working with more deterministic summarization passes since source code is so well structured if you know the syntax of the language.",
                  "score": 1,
                  "created_utc": "2026-02-15 15:47:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o55af1j",
          "author": "Independent-Cost-971",
          "text": "Wrote up a more detailed explanation if anyone's interested:Â [https://kudra.ai/metadata-enriched-retrieval-the-next-evolution-of-rag/](https://kudra.ai/metadata-enriched-retrieval-the-next-evolution-of-rag/)\n\nGoes into the different semantic chunking approaches (embedding similarity detection, LLM-driven structural analysis, proposition extraction) and the full metadata enrichment pipeline. Probably more detail than necessary but figured it might help someone else debugging the same issues.",
          "score": 4,
          "created_utc": "2026-02-13 11:25:47",
          "is_submitter": true,
          "replies": [
            {
              "id": "o5p5era",
              "author": "inguz",
              "text": "ok, good writeup - how are the results from \"proposition extraction\"?  Does that only apply to certain types of document, or with a certain granularity of structure that you're targeting?",
              "score": 1,
              "created_utc": "2026-02-16 15:38:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o56mc4z",
          "author": "pbalIII",
          "text": "You're right that fixed-size chunking shreds structure, especially when headers and tables get split away from the values. Calling it a hallucination fix is a bit too generous, you can tighten retrieval and still get confident wrong answers during synthesis.\n\nWhat's helped me is an answer contract: extract the exact supporting spans first, then write, and refuse if nothing supports it. And on metadata, treat it as a scoring hint not a hard filter if the tags come from a model, otherwise one bad tag can hide the best chunk.",
          "score": 2,
          "created_utc": "2026-02-13 16:01:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fv9ai",
              "author": "aimless_rider",
              "text": "I like the scoring hint idea for metadata! my first thought was that this steers the behavior towards mimicking keyword search...",
              "score": 1,
              "created_utc": "2026-02-15 01:58:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5glbvf",
                  "author": "pbalIII",
                  "text": "nah it's structured attrs not content terms",
                  "score": 2,
                  "created_utc": "2026-02-15 05:06:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5gqtc5",
                  "author": "pbalIII",
                  "text": "Regression sets start getting noisy around 200-300 cases... past that you spend more time maintaining stale examples than catching real regressions. For LLM-as-judge, 30-50 gold-labeled examples gets you surprisingly far on calibration, but you need to re-run alignment checks whenever you swap the judge model or change rubric criteria. On rollback triggers, pure metrics miss the weird qualitative stuff (model confidently generating plausible but wrong answers), so we do metric gates plus a human spot-check on a random sample from each deployment.",
                  "score": 1,
                  "created_utc": "2026-02-15 05:52:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o56ayq3",
          "author": "Savings_Divide_9164",
          "text": "What model are you using for the boundary detection?",
          "score": 1,
          "created_utc": "2026-02-13 15:06:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4hhvp",
      "title": "Good UI / UX solution for langchain deployments",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r4hhvp/good_ui_ux_solution_for_langchain_deployments/",
      "author": "ddewaele",
      "created_utc": "2026-02-14 10:37:26",
      "score": 18,
      "num_comments": 14,
      "upvote_ratio": 0.96,
      "text": "We really like LangChain as an AI orchestration engine but we're seeing a shift that a lot of our customers come to expect more autonomy in defining agents / configuring models / managing data and knowledge bases themselves.\n\n  \nFor that a good UI / UX experience is required and that is something that Langchain is currently not able to provide. It lacks an off the shelf UI / UX solutions.\n\n  \nWe've tried using [https://github.com/langchain-ai/agent-chat-ui](https://github.com/langchain-ai/agent-chat-ui), customizing it a little bit (adding OIDC connectors and stuff), but it is not something we necessarily want to spend time on. You would have to build a lot of features on top of it to make it useful (sharing chats / multi-user chats / agent config / prompt mgmt / memory system). Langchain offers that on the backend via langsmith, but this is not really user-friendly.\n\nSolutions like LibreChat already offer a really nice UI/UX experience.\n\nWhat do you think the strategic vision of LangChain is with regards to this. \n\nDo they keep focussing on the engine and believe people should build their UI / UX solutions themselves ? \n\nShould all customizations be done on the LangGraph platform side of things (Langsmith)\n\nIs LangChain ChatUI the way to go ? (extending it yourself).\n\nHooking up LangChain to Librehcat (via Agent-as-a-tool) seems very limiting and also forces you to use 2 systems, 2 types of message threading , configuration , ....)\n\n  \nWonder what the communities thoughts are on this.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r4hhvp/good_ui_ux_solution_for_langchain_deployments/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o5bmbc7",
          "author": "Otherwise_Wave9374",
          "text": "I have hit the same gap: LangChain/LangGraph is great as an orchestration layer, but most teams eventually need a real \"agent console\" for non-devs (prompt/version mgmt, tool permissions, KB/data connectors, chat sharing, audit logs, evals).\n\nOne path I have seen work is pairing the backend with a generic chat UI, then incrementally adding an admin surface just for agent config and observability. This overview might help frame the pieces: https://www.agentixlabs.com/blog/",
          "score": 6,
          "created_utc": "2026-02-14 11:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bmap5",
          "author": "Diao_nasing",
          "text": "Both assistant ui and copilotkit provide more complete ui, but assistant ui is not friendly to local deployment, and copilotkit has more bugs.",
          "score": 2,
          "created_utc": "2026-02-14 11:02:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cvlbz",
              "author": "ddewaele",
              "text": "Haven't tried either of them but will take a look.   \n  \nWas hoping the langchain team was going to give [https://github.com/langchain-ai/agent-chat-ui](https://github.com/langchain-ai/agent-chat-ui) some love but i have the impression they have a habit of launching stuff and then quickly abandoning it, leaving it in a pre-alpha state.",
              "score": 1,
              "created_utc": "2026-02-14 16:03:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5c0nxv",
          "author": "AccountantGlad9947",
          "text": "Iâ€™m building on top of chainlit for the UI. Was great and feature rich. Unsure what the future holds for the project but I like it.",
          "score": 1,
          "created_utc": "2026-02-14 13:03:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cvxu2",
              "author": "ddewaele",
              "text": "Do you use LangGraph platform and deploy your graphs to langgraph ? Or just embedding langchain in your own systems / backends",
              "score": 1,
              "created_utc": "2026-02-14 16:05:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5cx6m6",
                  "author": "AccountantGlad9947",
                  "text": "I use langgraph on my own servers",
                  "score": 1,
                  "created_utc": "2026-02-14 16:11:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5c7y2l",
          "author": "JasperTesla",
          "text": "What about a few boilerplate applications? The user may start off with one idea, and then find out they really prefer something else.",
          "score": 1,
          "created_utc": "2026-02-14 13:51:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cwr3h",
              "author": "ddewaele",
              "text": "Customers these day expect the following out of the box (without custom development) : \n\n\\- the ability to create their own agents (with prompts / tools / knowledge)  \n\\- want to link agents together (multi-agency via handoffs or tools)  \n\\- want to be able to share their agents  \n\\- have easy integration with their identity provider (azure / google / ...)  \n\\- have easy integration with their knowledge base (sharepoint / drive / ....)\n\nNot something you'll easily vibe-code into existence\n\nAt the end of the day I think 90% of the people are happy with a generic chat interface type application (like chatgpt). these things are multi-modal and very flexible.\n\nIn some cases you might want some agentic flows embedded in custom UI / UX, but I would say today that this is a minority of the cases\n\n\n\n",
              "score": 2,
              "created_utc": "2026-02-14 16:09:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5cz60v",
                  "author": "JasperTesla",
                  "text": "Hmm, I see. So basically they want an AI that makes AIs. Very amusing, but a fun challenge!\n\nWho are your customers, by the way? Are they other developers or people who use AI without knowing? Would they be okay with a low-code platform where they can drag-and-drop pre-built components?\n\nThe workflow you're describing sounds a bit like an n8n or LangDock workflow. n8n specifically would solve all of your issues, and I do know LangDock has a mechanism where you can explain to an AI what you want, and it builds the workflow itself. That might be worth trying out, or at least using as an inspiration.\n\nI think the best would be a system akin to Oracle VBCS, but with AI stuff, and focus on microservices with customisability.",
                  "score": 1,
                  "created_utc": "2026-02-14 16:21:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5cr03b",
          "author": "International_Quail8",
          "text": "I built a custom UI using the AG-UI protocol. I stopped using CoPilotKit and went to the protocol layer instead. Itâ€™s easy to work with and customize and bonus: youâ€™ll actually understand the protocol!",
          "score": 1,
          "created_utc": "2026-02-14 15:39:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5csuzc",
              "author": "ddewaele",
              "text": "Does the custom UI also allow customers to create their own agents / prompts / knowledge ? \n\nThat's the main drawback we see as it requires a lot of custom dev to get all of that in place. Our clients aren't always willing to fund this type of development.\n\nNot to mention security, chat sharing , multi-user chats, ....  This would almost need to be a like a strategic thing within a company to put the time and effort in. (some context : we're a software development company delivering AI solutions to many different clients).  We don't think our added value should be in delivering a UI/UX experience for that. People nowadays see lots of platforms where you can create an agent , add some prompts and some documents and you have an agentic system. They also want this level of autonomy.\n\nWith an app like Librechat you get a lot of that stuff for free. But there is no clean way to integrate langchain into it, and Librechat's approach to multi agent systems (using handoffs) is more limited to what langchain has to offer.",
              "score": 1,
              "created_utc": "2026-02-14 15:49:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ct68v",
          "author": "ar_tyom2000",
          "text": "I've been working a lot with LangGraph and ran into this once graphs get large - understanding execution paths, branching, and why an agent behaved a certain way quickly becomes hard. I ended up building [https://github.com/proactive-agent/langgraphics](https://github.com/proactive-agent/langgraphics) to make complex graphs easier to follow and reason about during runs. My takeaway so far is that the ecosystem is still very engine-centric, and the operability/UX layer around real deployments is only starting to take shape.",
          "score": 1,
          "created_utc": "2026-02-14 15:50:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5d4ztn",
          "author": "jannemansonh",
          "text": "lol this is literally the problem we kept running into too. ended up just using needle (needle.app) instead of trying to duct-tape a UI together. might be worth a look if you don't want to spend months building agent config screens and knowledge base integrations from scratch",
          "score": 1,
          "created_utc": "2026-02-14 16:50:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r499cg",
      "title": "GuardLLM, hardened tool calls for LLM apps",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r499cg/guardllm_hardened_tool_calls_for_llm_apps/",
      "author": "MapDoodle",
      "created_utc": "2026-02-14 02:58:14",
      "score": 15,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "I keep seeing LLM agents wired to tools with basically no app-layer safety. The common failure mode is: the agent ingests untrusted text (web/email/docs), that content steers the model, and the model then calls a tool in a way that leaks secrets or performs a destructive action. Model-side â€œbe carefulâ€ prompting is not a reliable control once tools are involved.\n\nSo I open-sourced GuardLLM, a small Python â€œsecurity middlewareâ€ for tool-calling LLM apps:\n\n* Inbound hardening: isolate and sanitize untrusted text so it is treated as data, not instructions.\n* Tool-call firewall: gate destructive tools behind explicit authorization and fail-closed human confirmation.\n* Request binding: bind tool calls (tool + canonical args + message hash + TTL) to prevent replay and arg substitution.\n* Exfiltration detection: secret-pattern scanning plus overlap checks against recently ingested untrusted content.\n* Provenance tracking: stricter no-copy rules for known-untrusted spans.\n* Canary tokens: generation and detection to catch prompt leakage into outputs.\n* Source gating: reduce memory/KG poisoning by blocking high-risk sources from promotion.\n\nIt is intentionally application-layer: it does not replace least-privilege credentials or sandboxing; it sits above them.\n\nRepo: [https://github.com/mhcoen/guardllm](https://github.com/mhcoen/guardllm)\n\nIâ€™d like feedback on:\n\n* Threat model gaps I missed\n* Whether the default overlap thresholds work for real summarization and quoting workflows\n* Which framework adapters would be most useful (LangChain, OpenAI tool calling, MCP proxy, etc.)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1r499cg/guardllm_hardened_tool_calls_for_llm_apps/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o5a4dtg",
          "author": "AdditionalWeb107",
          "text": "I like this idea - but you should check out FilterChains from plano: https://github.com/katanemo/plano. This would make for a great integration point",
          "score": 3,
          "created_utc": "2026-02-14 03:13:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5a5eh7",
              "author": "MapDoodle",
              "text": "Thanks for the pointer! Looks like they'd play nicely together",
              "score": 1,
              "created_utc": "2026-02-14 03:20:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5a609e",
                  "author": "AdditionalWeb107",
                  "text": "they do - the whole idea behind a filter chain is to hand off to a pre/post processor so that the request can be mutated or secured ahead of the agent. ",
                  "score": 1,
                  "created_utc": "2026-02-14 03:24:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5shs5y",
          "author": "Illustrious_Slip331",
          "text": "This addresses a massive gap; relying solely on system prompts for security is usually a losing battle in production. Regarding threat model gaps, have you considered how this handles indirect injection specifically within RAG chunks where the \"untrusted text\" is retrieved piecemeal from a vector store? For adapters, a LangGraph integration would be incredibly high-value right now given the community shift away from linear chains. From a compliance angle, ensuring granular logging of exactly *why* a tool call was blocked (e.g., which rule triggered) is essential for enterprise audit trails.",
          "score": 2,
          "created_utc": "2026-02-17 01:41:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sjz91",
              "author": "MapDoodle",
              "text": "Thanks! On the RAG question, GuardLLM handles that fine as long as the retriever (or its wrapper) passes source metadata per chunk at ingress. The pipeline labels each chunk with trust and sensitivity independently, so piecemeal retrieval isn't a problem. The vector store just needs to carry provenance alongside the embedding. Logging is already granular per control surface, so you get the full audit trail of which rule fired and why. And I think that prompt engineering for risk mitigation is wishful thinking.",
              "score": 1,
              "created_utc": "2026-02-17 01:54:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o61fsci",
          "author": "Outrageous_Hat_9852",
          "text": "Tool call validation is tricky because you need to test both the selection logic (did it pick the right tool?) and the parameter construction (are the arguments safe and correct?). We've found that separating these concerns helps. Test tool selection with conversation simulation to see if the agent maintains context about available tools, then run adversarial tests specifically targeting parameter injection and boundary violations. The hardest part is usually catching edge cases where valid-looking parameters cause unexpected behavior downstream.",
          "score": 2,
          "created_utc": "2026-02-18 12:08:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61m1en",
              "author": "MapDoodle",
              "text": "Yeah, I like this decomposition. One thing I kept running into is separating selection logic from parameter construction still assumes the context driving both is clean. If the context is compromised, the agent can pick the \"right\" tool with \"safe\" parameters and still be doing what an attacker wanted. That pushed me toward context-level detection, with metalabels carrying provenance and authority as a fallback when detection alone isn't enough. I'm writing a paper now about this, with an example of catching an attack even when the context \"seems\" clean.",
              "score": 1,
              "created_utc": "2026-02-18 12:49:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3i9eq",
      "title": "A lovable like application that utilizes LangChain, deep agents, tools, and MCP servers.",
      "subreddit": "LangChain",
      "url": "https://i.redd.it/8nxd51bkl7jg1.png",
      "author": "ban_rakash",
      "created_utc": "2026-02-13 06:51:11",
      "score": 15,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r3i9eq/a_lovable_like_application_that_utilizes/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o54pgvt",
          "author": "t12e_",
          "text": "For file operations have a look at the opencode codebase. The agent has tools for handling files so you can get inspo from there. For creating apps you might need to fork opencode, make your app start a server in the background, and have it run commands or use the api to manage sessions (or chats)",
          "score": 1,
          "created_utc": "2026-02-13 08:12:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54hz6d",
          "author": "Otherwise_Wave9374",
          "text": "This sounds like a fun build. For file ops with agents, the thing that helped me most was forcing a \"plan then propose diff\" step.\n\nLike, agent reads workspace -> produces an explicit operations list (create/edit/delete) -> you validate -> only then execute tools. And for MCP servers, having a strict schema for tool outputs saves a ton of time.\n\nIf you want a few concrete patterns for tool design and multi-step agents, I have seen good examples here: https://www.agentixlabs.com/blog/",
          "score": 0,
          "created_utc": "2026-02-13 07:03:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r30gjb",
      "title": "The MCP thread got me paranoid about community skills and supply chain risks",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r30gjb/the_mcp_thread_got_me_paranoid_about_community/",
      "author": "Independent_Plum_489",
      "created_utc": "2026-02-12 17:59:43",
      "score": 14,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "That discussion about MCPs being outdated and agents moving toward raw CLI access sent me down a rabbit hole I wasn't expecting. Been experimenting with OpenClaw recently (hard to ignore 160K stars) and saw someone in the GitHub issues flag a calendar integration skill that was requesting file system write access and network permissions for something that should just be reading a schedule. They dug into the code and found base64 encoded strings that decoded to external URLs and some sketchy eval statements. Maintainer removed it pretty quick but it had already been up for a few days.\n\nStarted googling after reading that thread and honestly got a bit worried. Apparently a decent chunk of community built skills have been flagged for doing sketchy things like data collection or downloading external payloads. Can't verify the exact numbers myself but it tracks with what I've been seeing in issue trackers. OpenClaw's own docs call the whole setup a \"Faustian bargain\" which... yeah.\n\nFeels like we're repeating the npm left-pad era except now the stakes are higher because these agents have real permissions. Read your emails, browse authenticated pages, execute shell commands. One bad skill and you've basically handed over the keys.\n\nSo now I'm being paranoid about everything. Manual code review when I have patience for it, though I'm slow at spotting obfuscated stuff. Checking GitHub issues and recent commits before installing anything. Running everything in Docker with network monitoring just in case. I've tried throwing Semgrep at some skills with mixed results, poked around with Snyk and Agent Trust Hub too. Even grep for obvious patterns like base64 or eval. The automated scanners all feel like security theater though when you're dealing with prompt injection vectors they weren't really designed for.\n\nStarting to think the only real answer is extreme permission isolation but that defeats half the usefulness of these agents. There's probably no good solution here.\n\nWhat does your vetting process look like before installing community stuff? Or is the general approach to just run everything sandboxed and hope for the best?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r30gjb/the_mcp_thread_got_me_paranoid_about_community/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r5lb28",
      "title": "I built an autonomous agent with DeepAgents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r5lb28/i_built_an_autonomous_agent_with_deepagents/",
      "author": "Releow",
      "created_utc": "2026-02-15 18:01:47",
      "score": 11,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "[CianaParrot](https://preview.redd.it/rq9dgmdy6pjg1.png?width=1024&format=png&auto=webp&s=57714055a2897397227f67ff326251af488456eb)\n\nHi\n\nI built this project for myself because I wanted full control over what my personal assistant does and the ability to modify it quickly whenever I need to. I decided to share it on GitHub here's the link: [https://github.com/emanueleielo/ciana-parrot](https://github.com/emanueleielo/ciana-parrot)\n\nIf you find it useful, leave a star or some feedback\n\n  \n\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r5lb28/i_built_an_autonomous_agent_with_deepagents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o5jrehc",
          "author": "hwchase17",
          "text": "very very cool!",
          "score": 2,
          "created_utc": "2026-02-15 18:27:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5jtixt",
              "author": "Releow",
              "text": "thank u!",
              "score": 1,
              "created_utc": "2026-02-15 18:37:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5txwrs",
                  "author": "nm-ranga",
                  "text": "Well.. a silly question from a novice (in AI). How to test this? Any instructions?",
                  "score": 1,
                  "created_utc": "2026-02-17 07:58:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5txrkv",
              "author": "nm-ranga",
              "text": "Iâ€™m amazed to see that you find time to reply even at this stage where you busy doing so many things. Keep it up!! \n\nI came to know about LangChain through coursera. Just now started with the course. Iâ€™ll come up with some (possibly) weird questions. \n\nBtw, Iâ€™m a mechanical engineer by profession with CAD programming skills.",
              "score": 1,
              "created_utc": "2026-02-17 07:57:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4k671",
      "title": "I built an python AI agent framework that doesn't make me want to mass-delete my venv",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r4k671/i_built_an_python_ai_agent_framework_that_doesnt/",
      "author": "anandesh-sharma",
      "created_utc": "2026-02-14 13:06:32",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hey all. I've been building [https://github.com/definableai/definable.ai](https://github.com/definableai/definable.ai) \\- a Python framework for AI agents. I got frustrated with existing options being either too bloated or too toy-like, so I built what I actually wanted to use in production.\n\n\n\nHere's what it looks like:\n\n    ```from definable.agents import Agent\n    from definable.models.openai import OpenAIChat\n    from definable.tools.decorator import tool\n    from definable.interfaces.telegram import TelegramInterface, TelegramConfig\n    \n    @tool\n    def search_docs(query: str) -> str:\n        \"\"\"Search internal documentation.\"\"\"\n        return db.search(query)\n    \n    agent = Agent(\n        model=OpenAIChat(id=\"gpt-5.2\"),\n        tools=[search_docs],\n        instructions=\"You are a docs assistant.\",\n    )\n    \n    # Use it directly\n    response = agent.run(\"Steps for configuring auth?\")\n    \n    # Or deploy it â€” HTTP API + Telegram bot in one line\n    agent.add_interface(TelegramInterface(\n        config=TelegramConfig(bot_token=os.environ[\"TELEGRAM_BOT_TOKEN\"]),\n    ))\n    agent.serve(port=8000)\n    \n\n\n\n**What My Project Does**\n\nPython framework for AI agents with built-in cognitive memory, run replay, file parsing (14+ formats), streaming, HITL workflows, and one-line deployment to HTTP + Telegram/Discord/Signal. Async-first, fully typed, non-fatal error handling by design.\n\n\n\n**Target Audience**\n\nDevelopers building production AI agents who've outgrown raw API calls but don't want LangChain-level complexity. v0.2.6, running in production.\n\n\n\n**Comparison**\n\n\\- \\*\\*vs LangChain\\*\\* - No chain/runnable abstraction. Normal Python. Memory is multi-tier with distillation, not just a chat buffer. Deployment is built-in, not a separate project.\n\n\\- \\*\\*vs CrewAI/AutoGen\\*\\* - Those focus on multi-agent orchestration. Definable focuses on making a single agent production-ready: memory, replay, file parsing, streaming, HITL.\n\n\\- \\*\\*vs raw OpenAI SDK\\*\\* - Adds tool management, RAG, cognitive memory, tracing, middleware, deployment, and file parsing out of the box.\n\n\n\n\\`*pip install definable*\\`\n\n\n\nWould love feedback. Still early but it's been running in production for a few weeks now.\n\n\n\n[https://github.com/definableai/definable.ai](https://github.com/definableai/definable.ai)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r4k671/i_built_an_python_ai_agent_framework_that_doesnt/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o5cob3h",
          "author": "bsampera",
          "text": "It looks painfully similar to langchain. Can you repeat what are the advantages without using buzzwords?",
          "score": 1,
          "created_utc": "2026-02-14 15:25:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ks7hm",
              "author": "anandesh-sharma",
              "text": "The structure overall is simple, now i have covered most of the hard parts here like interfaces, cognitive memory,etc. And you want customisation you can simply extend functionality and change the behaviour.\n\nAnd thats nearly impossible to manage with langchain. Its more performant wrt langchain.",
              "score": 1,
              "created_utc": "2026-02-15 21:33:01",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5ksdas",
              "author": "anandesh-sharma",
              "text": "rest u can also take a look at, https://docs.definable.ai",
              "score": 1,
              "created_utc": "2026-02-15 21:33:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5oi5an",
          "author": "Feisty-Promise-78",
          "text": "Hi, this is really cool. Recently I have been trying to learn to build an AI framework by myself. Is there any resource that helped you build this framework? Can you share it with me?",
          "score": 1,
          "created_utc": "2026-02-16 13:36:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2xv1b",
      "title": "Where can I learn to build my own AI agent framework? Any solid video courses?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r2xv1b/where_can_i_learn_to_build_my_own_ai_agent/",
      "author": "Feisty-Promise-78",
      "created_utc": "2026-02-12 16:23:42",
      "score": 10,
      "num_comments": 19,
      "upvote_ratio": 1.0,
      "text": "Iâ€™m trying to go beyond using existing tools and actually learn how to design and build my own AI agentic framework from scratch.\n\nIâ€™m especially interested in topics like:\n\n* Tool use and function calling\n* Planning and memory\n* Multi-agent systems\n* Orchestration and evaluation\n* Building real projects instead of just theory\n\nDoes anyone know of **good video tutorials or courses** that cover this well? YouTube series, Udemy courses, paid bootcamps, anything.\n\nIâ€™d love recommendations that are practical and hands-on rather than purely conceptual.\n\nIf youâ€™ve learned this yourself, what resources helped you the most?",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1r2xv1b/where_can_i_learn_to_build_my_own_ai_agent/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o50ic0h",
          "author": "PretendPop4647",
          "text": "You can follow langchain academy course.  Read their blogs, documentation. It helps me a lot. I watch random video on YouTube. \n\nAnthropic's engineering blog  are highly recommended. \n\nBtw, i was exploring langchain - deepagent. It's give some power what claude code have.\nUsing this, i built an agent,  i used tool call, subagents, file system etc.. \n\nYou can explore this to understand how it works \n\nRepo : https://github.com/Rahat-Kabir/job-search-agent",
          "score": 4,
          "created_utc": "2026-02-12 17:23:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50jzak",
              "author": "Feisty-Promise-78",
              "text": "I m learning for job and interviews. I have done some research and many startup have built their own AI agent framework. I already have used langchain and langgraph. But thanks!",
              "score": 2,
              "created_utc": "2026-02-12 17:31:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o50kd5f",
                  "author": "PretendPop4647",
                  "text": "good to know. you are already good at. best of luck your jobs and interview.  what type of jobs you are looking for? remote?",
                  "score": 1,
                  "created_utc": "2026-02-12 17:33:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o52iw6c",
          "author": "Fanof07",
          "text": "Iâ€™ve mostly learned by following LangChain and LlamaIndex tutorials and building small projects Hands on stuff teaches multi agent setups best",
          "score": 3,
          "created_utc": "2026-02-12 23:13:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55ljki",
          "author": "Worth_Rabbit_6262",
          "text": "If you have Udemy you could try with the course: \"LangChain- Develop AI Agents with LangChain & LangGraph\" made by Eden Marco, engineer for Google. I'm studying with this but at the moment I can't find any use cases for the practice but I have to force myself\n\n",
          "score": 2,
          "created_utc": "2026-02-13 12:47:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o561avt",
              "author": "Feisty-Promise-78",
              "text": "In this course, does the instructor use LangChain v1 or earlier versions?",
              "score": 1,
              "created_utc": "2026-02-13 14:17:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o563sle",
                  "author": "Worth_Rabbit_6262",
                  "text": "\"**COURSEÂ WASÂ RE-RECORDEDÂ and supports- LangChain Version 1.0+**\"",
                  "score": 1,
                  "created_utc": "2026-02-13 14:30:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o53umla",
          "author": "Preconf",
          "text": "Building frameworks for anything is not a trivial undertaking. For the likes of python and js/ts it requires being comfortable enough with the language to be able to effectively leverage relatively advanced features.",
          "score": 1,
          "created_utc": "2026-02-13 04:03:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55sk0h",
          "author": "wheres-my-swingline",
          "text": "[here?](https://github.com/humanlayer/12-factor-agents)",
          "score": 1,
          "created_utc": "2026-02-13 13:29:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o560vnw",
              "author": "Feisty-Promise-78",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-13 14:15:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o56l1u9",
          "author": "pbalIII",
          "text": "Built a multi-tool agent loop from raw API calls before touching any framework, and that's honestly what clicked for me. Start with just the tool-calling loop... send a prompt, parse the function call, execute it, feed the result back, repeat until the model stops requesting tools. That's the entire skeleton.\n\nOnce that works, the next unlock is separating planning from execution. Have the model output a plan first, then execute each step. Without that split, anything beyond 2-3 tools falls apart because the model loses track of where it is.\n\nAnthropic's engineering blog and OpenAI's practical guide to building agents PDF both walk through this architecture. For interview prep specifically, being able to explain why you'd add memory, retry logic, or a supervisor pattern on top of that bare loop is more valuable than knowing any specific framework's API.",
          "score": 1,
          "created_utc": "2026-02-13 15:55:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56o6g2",
              "author": "Feisty-Promise-78",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-02-13 16:10:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5920wa",
          "author": "Ok-Priority35",
          "text": "Use an intelligent code editor like antigravity or cursor to manage an openclaw agent",
          "score": 1,
          "created_utc": "2026-02-13 23:15:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v9srv",
          "author": "Alive_Challenge1287",
          "text": "Yt: krish naiak",
          "score": 1,
          "created_utc": "2026-02-17 14:12:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51fvft",
          "author": "alimhabidi",
          "text": "Take a look at this live workshop\n\nhttps://www.eventbrite.com/e/build-ai-agents-over-the-weekendcohort-3-tickets-1980455085473",
          "score": 0,
          "created_utc": "2026-02-12 20:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53k72u",
          "author": "Remote-Evening1437",
          "text": "Hey there! It's great you're diving into building your own AI agent frameworks. That's a powerful area! While AmenLink focuses on spiritual AI assistance rather than AI agent development courses, I can offer some general advice from the tech world perspective that might be helpful. For practical, hands-on learning in AI agent frameworks, I'd suggest looking into:\n\n\n\n1.  Official documentation and examples from libraries like LangChain or LlamaIndex. They often have solid tutorials that walk you through building components like tool use, planning, and memory.\n\n2.  YouTube channels that focus on MLOps, AI engineering, or specific framework tutorials. Many independent creators offer deep dives into building custom agents.\n\n3.  Udemy/Coursera often have courses specifically on \"building AI agents\" or \"LLM application development\" which would cover your listed interests like orchestration and evaluation. Look for courses with high ratings and recent updates.\n\n4.  For multi-agent systems, research papers and their accompanying open-source implementations are gold. Platforms like Hugging Face often host these projects.\n\n\n\nMany learned by just jumping in, building small projects, and debugging their way through. Good luck with your learning journey â€“ it's a fascinating field!",
          "score": 0,
          "created_utc": "2026-02-13 02:55:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7fsnr",
      "title": "Debugging LangChain agents is painful until you can visualize the full trace",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r7fsnr/debugging_langchain_agents_is_painful_until_you/",
      "author": "ruhila12",
      "created_utc": "2026-02-17 19:19:32",
      "score": 10,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "I really like working with LangChain, but debugging multi step agents can feel like a black box.\nWhen something breaks, itâ€™s never obvious where it actually failed.\n\n\nDid retrieval return garbage?\n\n\nDid the reranker strip out the only useful chunk?\n\n\nDid the LLM just hallucinate?\n\n\nOr did the agent get stuck in some weird tool loop?\n\n\nFor the longest time, I was just staring at terminal logs and scrolling through JSON traces trying to piece things together. It technically worksâ€¦ but once your chain gets even slightly complex, it becomes painful.\n\nRecently, I plugged my chains into a tracing tool (Confident AI) mostly out of frustration. I wasnâ€™t looking for metrics or anything fancy. I just wanted to see what was happening step by step.\nThe biggest difference for me wasnâ€™t scoring or dashboards. It was the visual breakdown of each hop in the chain. I could literally see:\n\n\nRetrieval step\n\n\nReranking\n\n\nTool calls\n\n\nLLM responses\n\n\nLatency per step\n\n\nAt one point, I realized my agent wasnâ€™t â€œfailingâ€ randomly, it was looping on a specific tool call because my system prompt wasnâ€™t strict enough about exit conditions. That wouldâ€™ve taken me way longer to diagnose just from logs.\n\nBeing able to replay a failed interaction and inspect the full flow changed how I debug. It feels less like guessing and more like actual engineering.\n\nCurious how others are handling debugging for multi-step agents. Are you just logging everything, or using something more structured?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r7fsnr/debugging_langchain_agents_is_painful_until_you/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o5x9ab5",
          "author": "Overall_Insurance956",
          "text": "Use langsmith",
          "score": 4,
          "created_utc": "2026-02-17 19:59:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o623tdf",
              "author": "LuckySwimming8564",
              "text": "This.  It is super easy to setup (just a couple vars in your .env) and it is very detailed.  https://docs.langchain.com/langsmith/trace-with-langchain. ",
              "score": 1,
              "created_utc": "2026-02-18 14:28:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5x2c9v",
          "author": "pvatokahu",
          "text": "Check out open source monocle2ai from Linux foundation - it does the full tracing with agentic attribute capture built on OpenTelemetry and part of pytest.",
          "score": 2,
          "created_utc": "2026-02-17 19:27:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o658ixy",
          "author": "TheExodu5",
          "text": "Please donâ€™t interact with the fake-engagement advertising bot.",
          "score": 2,
          "created_utc": "2026-02-18 23:08:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o678rvy",
              "author": "NotAHost",
              "text": "Yup just search author:username to see all their spam.",
              "score": 1,
              "created_utc": "2026-02-19 06:42:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o61sh9d",
          "author": "Informal_Tangerine51",
          "text": "Yeah, once a chain has retrieval + reranking + tools, â€œprint the logsâ€ stops being a debugging strategy and starts being archaeology. A good trace view pays for itself fast, especially when you can replay a run and see exactly where the agent diverged or started looping.\n\nOne thing Iâ€™d add (even if you keep the fancy UI) is a small â€œstructured trace contractâ€: every hop logs inputs/outputs, tool args, and a reason code for why the agent continued or stopped. Then you can write regression tests off real failures: â€œthis tool loop should terminateâ€ or â€œthis retrieval query should return at least one relevant chunk,â€ instead of hoping prompts stay stable.\n\nWeâ€™re working on this at Clyra (open source here): [https://github.com/Clyra-AI](https://github.com/Clyra-AI)",
          "score": 1,
          "created_utc": "2026-02-18 13:28:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o624842",
          "author": "93simoon",
          "text": "Use langfuse, it's lang Smith but foss",
          "score": 1,
          "created_utc": "2026-02-18 14:30:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o625evm",
          "author": "Revolutionary-Bet-58",
          "text": "I'm biased but I can recommend you to check out [inkog.io](http://inkog.io) , you can insert your LangChain agent in there and get feedback directly to solve some issues that you will face before debugging like infinite loops, tool calls etc . It will also recommend you how to fix the problems with examples, or you can just use the Inkog MCP and let Claude fix it for you :D\n\nHappy to sit down with you if you have any questions ",
          "score": 1,
          "created_utc": "2026-02-18 14:36:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o628qhb",
          "author": "penguinzb1",
          "text": "trace replay is great for diagnosing what happened, but the tool loop you described, where the agent ignored exit conditions, is also the kind of thing that shows up before users see it if you run it against adversarial or edge case scenarios first. what we've found is that simulating these before deployment catches them earlier than any trace tool can, because you're finding the failure before the first incident.",
          "score": 1,
          "created_utc": "2026-02-18 14:52:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r78a13",
      "title": "Run untrusted code locally in LangChain using WASM sandboxes",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r78a13/run_untrusted_code_locally_in_langchain_using/",
      "author": "Tall_Insect7119",
      "created_utc": "2026-02-17 15:00:43",
      "score": 9,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Lately I've seen a lot of cloud-based solutions for running untrusted code. But in reality, you can do it safely on your local machine without sending anything to the cloud.\n\n**Quick context**: When an AI generates code to perform a task, executing it directly could be dangerous for your host system. Sandboxing helps protect your host from any issues that untrusted code might cause.\n\nI built an open-source runtime that isolates code using WebAssembly sandboxes. You can plug it into an existing project in just a few lines:\n\n    from capsule import run\n    \n    result = await run(\n        file=\"./capsule.py\",\n        args=[\"code to execute\"]\n    ]\n\nThen you define your sandboxed logic like this:\n\n    from capsule import task\n    \n    @task(name=\"main\", compute=\"MEDIUM\", ram=\"512mb\")\n    def main(code: str) -> str:\n        \"\"\"Execute untrusted code in an isolated sandbox\"\"\"\n        return exec(code)\n\nThe code (task) runs in its own isolated WASM sandbox. You can define multiple tasks with different limits and even run it standalone.\n\nI put together an example integrated with LangChain here: [https://github.com/mavdol/capsule/tree/main/examples/python/langchain-agent](https://github.com/mavdol/capsule/tree/main/examples/python/langchain-agent)\n\nAnd hereâ€™s the main repo: [https://github.com/mavdol/capsule](https://github.com/mavdol/capsule)\n\nWould love to hear your feedback or thoughts !",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r78a13/run_untrusted_code_locally_in_langchain_using/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o5vtc83",
          "author": "vansterdam_city",
          "text": "Iâ€™m absolutely in favor of local sandboxes for agentic coding. There is no way Iâ€™m turning on the super unsafe mode on my local userspace with all my personal creds, but without doing so itâ€™s super annoying. I like codex web for that reason, but it has limitations.\n\nIâ€™m curious, why not dev containers? Containers are already a mature platform for creating isolation.",
          "score": 2,
          "created_utc": "2026-02-17 15:51:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w3paq",
              "author": "Tall_Insect7119",
              "text": "That's a valid question. Currently, containers are great for safe application isolation, but they share the host kernel, which could be a risk for untrusted code, even if it's hard to exploit in practice. The real difference is overhead. WASM is lighter than a container, and after the cold start, it's about 100x faster than Docker, for example.\n\nThe only limitation for now is that C extensions (like numpy) aren't supported yet, so it really depends on the use case.",
              "score": 0,
              "created_utc": "2026-02-17 16:43:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5z08jl",
          "author": "ChanceKale7861",
          "text": "This. Those who have done security work, would do the same I think.",
          "score": 2,
          "created_utc": "2026-02-18 01:20:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61vywp",
          "author": "peregrinefalco9",
          "text": "un arbitrary code with the agent's full permissions\" which is terrifying in production.\n\n  \nThe key things to validate with any sandbox approach: can the sandboxed code make network requests? Can it read the filesystem outside its sandbox? What happens when the LLM generates code that tries to escape the sandbox (because it will â€” not maliciously, just because the model doesn't understand sandbox boundaries)?\n\n  \nWASM's capability-based security model is actually well-suited for this. You can explicitly grant only the capabilities the code needs â€” file access to specific paths, network access to specific hosts, memory limits. The attack surface is much smaller than a container and the startup overhead is negligible.\n\n  \nCurious how this handles cases where the agent needs to install dependencies at runtime. That's usually where sandboxed execution falls apart in practice.",
          "score": 2,
          "created_utc": "2026-02-18 13:47:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61y5ge",
          "author": "Informal_Tangerine51",
          "text": "This is a great direction. â€œLocal, sandboxed executionâ€ is exactly what you want for agent-generated code, and WASM gives you a cleaner isolation boundary than â€œjust run it in a venv and hope.â€ The ergonomic API matters too, because if itâ€™s annoying people will bypass it.\n\nThe big questions Iâ€™d want answered are around escape hatches: whatâ€™s the default filesystem/network surface, how do you handle timeouts and memory limits deterministically, and can you produce an audit trail of what ran (hash of code, args, resource limits, stdout/stderr) for later debugging. In practice, a sandbox without good observability becomes a new kind of black box.\n\nWeâ€™re working on this at Clyra (open source here): [https://github.com/Clyra-AI](https://github.com/Clyra-AI)",
          "score": 2,
          "created_utc": "2026-02-18 13:58:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r470hr",
      "title": "We Benchmarked 7 Chunking Strategies. Most 'Best Practice' Advice Is Wrong.",
      "subreddit": "LangChain",
      "url": "https://www.runvecta.com/blog/we-benchmarked-7-chunking-strategies-most-advice-was-wrong",
      "author": "Confident-Honeydew66",
      "created_utc": "2026-02-14 01:13:26",
      "score": 9,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1r470hr/we_benchmarked_7_chunking_strategies_most_best/",
      "domain": "runvecta.com",
      "is_self": false,
      "comments": [
        {
          "id": "o59xnzq",
          "author": "timmy166",
          "text": "The article is touting a toolâ€™s accuracy improvements but the model selection is several generations back - touting  more of a cost-savings value proposition.\n\nWeâ€™re missing a key control group:\nFrontier models with full document retrieval.",
          "score": 2,
          "created_utc": "2026-02-14 02:30:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r47zi8",
      "title": "What are the best LangChain practical tutorials?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r47zi8/what_are_the_best_langchain_practical_tutorials/",
      "author": "LargeSinkholesInNYC",
      "created_utc": "2026-02-14 01:59:10",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "I want to build leading edge stuffs, so I was wondering if there were practical tutorials that would help me get my foot in the door. Feel free to share.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r47zi8/what_are_the_best_langchain_practical_tutorials/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o5agh58",
          "author": "Hackerjurassicpark",
          "text": "Use Claude code to build exactly what you need without any of these harnesses like langchain",
          "score": 2,
          "created_utc": "2026-02-14 04:40:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a40vx",
          "author": "KalZaxSea",
          "text": "I generaly refer their youutbe channel and docs. Bcs there are lots of thing that we need to consider and their docs and tutorials cover them.\n\n  \nFor example: What happens when you reload history but changed the system prompt in a tool call, will it load as old system prompt or changed version",
          "score": 1,
          "created_utc": "2026-02-14 03:11:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bvbju",
          "author": "shadowcorp",
          "text": "[LangChain Academy](https://academy.langchain.com) is good, as are most of the courses on DeepLearning.ai.",
          "score": 1,
          "created_utc": "2026-02-14 12:23:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ohfoq",
          "author": "Kooky-Elephant8905",
          "text": "I learned it through campusx youtube great teacher.",
          "score": 1,
          "created_utc": "2026-02-16 13:32:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4olni",
      "title": "I built a visual execution tracking for LangGraph workflows",
      "subreddit": "LangChain",
      "url": "https://github.com/proactive-agent/langgraphics",
      "author": "ar_tyom2000",
      "created_utc": "2026-02-14 16:15:02",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r4olni/i_built_a_visual_execution_tracking_for_langgraph/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5iy8yp",
          "author": "ar_tyom2000",
          "text": "https://i.redd.it/bkymbofulojg1.gif\n\nJust one line of code is all it takes to visualize your LangGraph agent's workflow in real-time as it executes. Any feedback?",
          "score": 1,
          "created_utc": "2026-02-15 16:06:00",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5u45nw",
          "author": "bsampera",
          "text": "So the same that langgraph studio?",
          "score": 1,
          "created_utc": "2026-02-17 08:58:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u4pok",
              "author": "ar_tyom2000",
              "text": "Not quite. Currently adding more tracing features and it will be a light, single line usage (no setup needed), and free (for studio you should have langsmith API key).",
              "score": 1,
              "created_utc": "2026-02-17 09:03:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4nv7j",
      "title": "I built an open-source â€œflight recorderâ€ for AI agents (records every step, works offline, cryptographically verifiable)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r4nv7j/i_built_an_opensource_flight_recorder_for_ai/",
      "author": "ALWAYSHONEST69",
      "created_utc": "2026-02-14 15:45:26",
      "score": 7,
      "num_comments": 9,
      "upvote_ratio": 0.73,
      "text": "Iâ€™ve been working on an open-source project called epi-recorder.\nThe problem I kept running into while building agents was simple:\nwhen something breaks, logs are not enough. You often canâ€™t reconstruct what actually happened step by step, and in many cases you canâ€™t prove what the system did.\nSo I built a recorder that captures: â€¢ prompts, responses, tool calls, and state transitions\nâ€¢ timestamps, token usage, and environment snapshot\nâ€¢ replayable execution history\nâ€¢ optional cryptographic signatures for tamper-evident records\nâ€¢ offline viewer â€” no cloud required\nAn .epi file is basically a flight recorder for AI agents.\nIt works with: â€¢ OpenAI / Anthropic / local LLMs\nâ€¢ LangGraph and async workflows\nâ€¢ any Python agent via wrappers or explicit logging\nInstall: pip install epi-recorder\nIâ€™m a solo founder building this and would really value:\nFeedback from people running agents\nIdeas on real-world use cases\nStars on the repo if you find the project useful or interesting â€” it helps visibility a lot\nGitHub: https://github.com/mohdibrahimaiml/epi-recorder\nIf youâ€™ve ever had an agent fail and wished you could replay exactly what happened, Iâ€™d especially like to hear how youâ€™re debugging today.",
      "is_original_content": false,
      "link_flair_text": "Announcement",
      "permalink": "https://reddit.com/r/LangChain/comments/1r4nv7j/i_built_an_opensource_flight_recorder_for_ai/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o61f97u",
          "author": "Outrageous_Hat_9852",
          "text": "This is really cool - having cryptographically verifiable execution traces is huge for debugging agent behavior and building trust in production systems. One thing we've seen teams struggle with is connecting these detailed execution logs back to whether the agent actually met its requirements or behaved correctly. The flight recorder captures *what* happened, but you still need structured ways to evaluate *whether* it was the right thing, especially for catching subtle issues like context drift or goal deviation that aren't obvious from logs alone.",
          "score": 2,
          "created_utc": "2026-02-18 12:04:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61p8hm",
              "author": "ALWAYSHONEST69",
              "text": "You're exactly right, EPI Recorder captures ground truth, not correctness.\nThe goal is to make debugging and trust possible by preserving the exact execution path and full context in a tamper-evident way. Once you have a canonical trace, structured evaluation becomes much more reliable.\nWithout a verifiable execution record, evaluation systems are often judging reconstructed or partial state. With a cryptographically signed trace, you can attach evaluators directly to specific turns and systematically detect things like context drift, goal deviation, or tool misuse.\n\nI see it as two complementary layers:\n1. Execution capture â†’ What actually happened (verifiable, replayable).\n2. Evaluation layer â†’ Was it correct relative to requirements?\n\nRight now EPI focuses on making the first layer solid. That foundation makes the second layer far more trustworthy.\n\nIf you're experimenting with agent evaluation pipelines, Iâ€™d love for you to try it and see if the trace structure fits your workflow. And if you find it useful, a star on the repo genuinely helps signal that this problem space matters. â­ðŸ™ðŸ»",
              "score": 1,
              "created_utc": "2026-02-18 13:09:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ct5py",
          "author": "baneeishaquek",
          "text": "Any guidance on \"How to use this on Antigravity Chat Window?\".",
          "score": 1,
          "created_utc": "2026-02-14 15:50:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5cudwa",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-14 15:57:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5cuhht",
                  "author": "ALWAYSHONEST69",
                  "text": "PLEASE STAR IF YOU LIKED THE EPI",
                  "score": 1,
                  "created_utc": "2026-02-14 15:57:32",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5u5288",
          "author": "Icy-Cartographer23",
          "text": "\"Logs are not enough\" is exactly the right framing. I've been running into this constantly.\n\nOne specific question: when epi-recorder captures 'prompts and responses' in a multi-turn conversation, does it store the full context window at each step â€” meaning the entire accumulated prompt (system prompt + conversation history + all prior tool results that were included at that point)? Or does it capture the delta (what was new in that turn)?\n\nThe reason I ask: in a 40+ turn agent run, the bugs I struggle to debug are usually not in the last step. They're in a decision made at turn 23 where the model had accumulated a bunch of context from prior web searches and tool calls. To figure out why it made a weird choice at turn 23, I need to see what the FULL context window looked like at that exact moment â€” not just what new input was added.\n\nIf epi-recorder stores the full context snapshot at each step, it would actually solve this for me. The GitHub link is going in my queue either way â€” the cryptographic tamper-evidence angle is clever for any compliance use case.",
          "score": 1,
          "created_utc": "2026-02-17 09:07:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u6k8l",
              "author": "ALWAYSHONEST69",
              "text": "Great question - this is exactly the class of problem EPI Recorder is designed for.\n\nIt captures the full context window at every step, not just the delta.\n\nBecause LLM APIs are stateless, the agent has to send the entire accumulated context (system prompt, conversation history, and any prior tool outputs) with each request. EPI Recorder intercepts the raw request payload just before it is sent, so each step shows exactly what the model saw at that moment.\n\nSo in your example: if something odd happens at turn 23, you can open step 23 and see the full prompt as it was sent, system prompt + turns 1â€“22 + turn 23 input - without reconstructing anything manually.\n\nA couple of implementation notes:\n\n- For OpenAI and Anthropic calls, the full message history is captured without truncation.\n- Gemini currently has a safety truncation in very large payloads (mainly to prevent massive image/video contexts from exploding log size), but thatâ€™s being refined.\n- For generic raw HTTP traffic, only metadata is stored, but LLM calls are recorded in full specifically to support debugging cases like the one you described.\n\nIf you end up trying it, Iâ€™d appreciate a star on the repo, it helps visibility and signals that this kind of tooling is actually needed.\n\nðŸ™ðŸ»",
              "score": 1,
              "created_utc": "2026-02-17 09:21:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r58t34",
      "title": "Using LangGraph for long-term memory (RAG + Obsidian) â€” does this design make sense?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r58t34/using_langgraph_for_longterm_memory_rag_obsidian/",
      "author": "Glittering_Aerie54",
      "created_utc": "2026-02-15 07:47:17",
      "score": 7,
      "num_comments": 9,
      "upvote_ratio": 0.82,
      "text": "Hi everyone,\n\nI'm fairly new to building autonomous agents and recently started experimenting with LangGraph.\n\nI'm trying to solve a simple question:\n\n**How would you design long-term memory for a trading agent?**\n\nInstead of keeping memory only inside a vector DB, I experimented with connecting the agent to my Obsidian notes â€” almost like giving it a \"second brain\".\n\n# Current approach\n\nThe workflow is roughly:\n\n* When analyzing a stock, the agent retrieves related notes from an Obsidian vault (RAG)\n* Bull / Bear analyst agents debate using both live data and retrieved context\n* The final analysis is summarized and saved back into the vault\n\nSo the memory grows over time.\n\n# Tech I'm experimenting with\n\n* LangGraph / LangChain\n* Streamlit\n* ChromaDB\n* Obsidian as long-term memory\n\nSince this is my first serious attempt with LangGraph, I'm not sure if my graph structure or memory recall logic is the right approach.\n\n# What Iâ€™d really like feedback on\n\n* How do you usually structure long-term memory in LangGraph?\n* Should memory retrieval happen once at the start, or at multiple nodes?\n* Any patterns to avoid when using RAG as persistent memory?\n\nIf anyone is curious I can share the repo in comments â€” mainly looking for design feedback first.\n\nThanks ðŸ™",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r58t34/using_langgraph_for_longterm_memory_rag_obsidian/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o5iejo1",
          "author": "WowSoWholesome",
          "text": "LangGraph supports a bunch of stores, and you can use this to implement check pointing and long term memory in langgraph.Â https://docs.langchain.com/oss/python/langgraph/add-memory",
          "score": 2,
          "created_utc": "2026-02-15 14:23:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o5fw8",
          "author": "No-Fail-7644",
          "text": "Why not postgress? Lg4j already has in built checkpointing support for postgres.\nYour biggest challenge would be designing tiers of memory. You wouldnâ€™t want to mix up semantic memory with low level financial information. Youâ€™ll need atleast two tiers. You can wire these with AgentState in graph.. something similar to Plan, Tasks pattern. Lg4j also has supervisor agent pattern.\n\nClone the lg4j repo, there is a directory named â€˜how-tosâ€™, feed it to your coding agent and ask it more detailed questions!",
          "score": 2,
          "created_utc": "2026-02-16 12:11:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5o6evg",
              "author": "No-Fail-7644",
              "text": "If you are new to dev, better start with DBMS a bit. You can code out the app in couple of days but if you intent to use it long term reliably. You would need proper DB procedures so that your DB doesnt get poisoned over time, given your AI would be making up its content.",
              "score": 1,
              "created_utc": "2026-02-16 12:18:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5o6oxz",
                  "author": "No-Fail-7644",
                  "text": "You donâ€™t need to query DB full shot once on startup, you can query per node by simply writing a wrapper over it.",
                  "score": 1,
                  "created_utc": "2026-02-16 12:21:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5hortm",
          "author": "adlx",
          "text": "Take your post and ask ChatGPT, Claude or Gemini...\nIf you're really into what you say, building autonomous agent, you should already be into vibe coding and definitely using AIs first. Being here asking that sounds a contradiction to me. Sorry",
          "score": 2,
          "created_utc": "2026-02-15 11:15:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5hwpnh",
              "author": "Glittering_Aerie54",
              "text": "Fair point.\n\nI did use AI Tool for brainstorming, but I wanted feedback from people actually building with LangGraph in real projects.\n\nThis is the project for context:\n\n[https://github.com/jiwoomap/TradingAgents-Dashboard](https://github.com/jiwoomap/TradingAgents-Dashboard)",
              "score": 3,
              "created_utc": "2026-02-15 12:24:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5idncv",
                  "author": "WowSoWholesome",
                  "text": "I think what youâ€™re doing drives conversation and improves the community. Thank you for not just blindly vibe coding without an understanding of the solution you want first.Â ",
                  "score": 3,
                  "created_utc": "2026-02-15 14:18:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o61xs8j",
          "author": "Informal_Tangerine51",
          "text": "Your design makes sense, and Obsidian can be a great â€œhuman-readable memoryâ€ layer, but Iâ€™d be careful about treating RAG memory as truth in a trading context. Markets change, so a super relevant note from 6 months ago can be actively harmful unless you carry strong timestamps, regime tags, and â€œwhat data did this rely onâ€ alongside it.\n\nIn LangGraph Iâ€™d usually split memory into two lanes: (1) structured state you can trust (positions, constraints, risk limits, last decision, feature values), and (2) narrative notes (theses, learnings, postmortems) that are advisory. Retrieval shouldnâ€™t be only at the start; pull it at key nodes (hypothesis generation, counter-argument, decision), but keep the retrieved set small and require each claim to cite a note or a current datapoint.\n\nBig pattern to avoid: writing back everything the model says. Only persist summaries that pass a simple checklist (dated, sources linked, what changed since last time, explicit confidence), otherwise you end up with a compounding â€œmemory hallucinationâ€ loop. What are you using as the ground-truth price/fundamentals feed, and do you want memory to influence actual trades or just generate research notes?",
          "score": 1,
          "created_utc": "2026-02-18 13:56:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7vn7p",
      "title": "I canâ€™t figure out how to ask LLM to write an up-to-date LangChain script with the latest docs.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r7vn7p/i_cant_figure_out_how_to_ask_llm_to_write_an/",
      "author": "gowtham150",
      "created_utc": "2026-02-18 06:34:18",
      "score": 6,
      "num_comments": 16,
      "upvote_ratio": 0.69,
      "text": "Whenever I ask claude or chatgpt to write me a simple langchain agent - even the very simple ones - it always gives me a script with outdated libraries. I tried using claude with context7mcp and langchain docs mcp - still i get out of date obsolete script with deprecated libraries. Even for a simple use case i have to go to langchain docs and get it. Its frustrating to ask LLM to write a sample code and later on to find that its deprecated. How you are you guys solving this problem.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r7vn7p/i_cant_figure_out_how_to_ask_llm_to_write_an/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o60jchc",
          "author": "mdrxy",
          "text": "Would encourage cloning the repos locally and letting your agent know that it can traverse the source code in your filesystem!",
          "score": 7,
          "created_utc": "2026-02-18 07:22:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63mjh1",
              "author": "orthogonal-ghost",
              "text": "This is a fantastic idea",
              "score": 1,
              "created_utc": "2026-02-18 18:38:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o60tj3n",
          "author": "gowtham150",
          "text": "The general observation is that if i use claude code with context 7 MCP and ask it to write a a simple agent with Langchain it gives me a script most of the time with outdated versions and libraries. Same with chatgpt. So it's becoming extremely difficult to just test out a feature.",
          "score": 2,
          "created_utc": "2026-02-18 08:57:01",
          "is_submitter": true,
          "replies": [
            {
              "id": "o61bc9k",
              "author": "Individual_Day_9508",
              "text": "Use a CLAUDE.md or AGENTS.md file in your workspace to set a strict rule enforcing Langchain 1.x syntax. If you pair that explicit instruction with context 7, it completely fixes the outdated library issue.",
              "score": 1,
              "created_utc": "2026-02-18 11:34:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o61ci16",
                  "author": "gowtham150",
                  "text": "Ok let me check that. Thanks for sharing",
                  "score": 1,
                  "created_utc": "2026-02-18 11:44:01",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o60eznz",
          "author": "gaureshai",
          "text": "Very hard.  Because langchain docs are also outdated.",
          "score": 3,
          "created_utc": "2026-02-18 06:44:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60zbec",
              "author": "NoleMercy05",
              "text": "Not true anymore",
              "score": 2,
              "created_utc": "2026-02-18 09:51:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6179bd",
                  "author": "gaureshai",
                  "text": "Well then it's good. I had really hard time in js docs. Will try it again then.",
                  "score": 1,
                  "created_utc": "2026-02-18 11:01:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o60jaqu",
              "author": "mdrxy",
              "text": "Not sure what you mean -- can you point to specific pages? Will flag with the team",
              "score": 1,
              "created_utc": "2026-02-18 07:22:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o60l0t8",
          "author": "Character_Leg1134",
          "text": "Use chat.langchain.com \nIts their own bot \nWhich can give you the code with updated libraries",
          "score": 2,
          "created_utc": "2026-02-18 07:38:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60tdxv",
              "author": "gowtham150",
              "text": "Will try this",
              "score": 1,
              "created_utc": "2026-02-18 08:55:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o61cjj2",
              "author": "gowtham150",
              "text": "This has been working well so far. Thanks for sharing.",
              "score": 1,
              "created_utc": "2026-02-18 11:44:20",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o63c2w2",
              "author": "rk_11",
              "text": "Second this",
              "score": 1,
              "created_utc": "2026-02-18 17:52:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o62l7az",
          "author": "notAllBits",
          "text": "That is the cost of unstable conventions (API/classes) in coding. If LLMs cannot be confident about their memory, they spoil it for everyone",
          "score": 1,
          "created_utc": "2026-02-18 15:51:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60qx1v",
          "author": "kolmar41k",
          "text": "If you using and IDE, try using an MCP called 'context7', it provides up to date docs including langgraph/langchain to your llm",
          "score": 1,
          "created_utc": "2026-02-18 08:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60tczj",
              "author": "gowtham150",
              "text": "I already did, like i mentioned in my post. I used context 7 and Langchain has its own mc as well",
              "score": 1,
              "created_utc": "2026-02-18 08:55:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r79j1x",
      "title": "Don't Prompt Your Agent for Reliability â€” Engineer It",
      "subreddit": "LangChain",
      "url": "https://www.aiyan.io/blog/engineer-agent-reliability/",
      "author": "NetworkFlux",
      "created_utc": "2026-02-17 15:47:11",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1r79j1x/dont_prompt_your_agent_for_reliability_engineer_it/",
      "domain": "aiyan.io",
      "is_self": false,
      "comments": [
        {
          "id": "o5vspec",
          "author": "NetworkFlux",
          "text": "I've spent the past year at my company building a data engineering agent for non-technical users. I rearchitected it three times, from a rigid state machine, to a multi-agent orchestrator, to a single general-purpose agent with lightweight tools. Each time, the system actually got simpler and more reliable. Wrote up the full evolution and the two biggest lessons I took away!",
          "score": 1,
          "created_utc": "2026-02-17 15:48:21",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o629lt8",
          "author": "penguinzb1",
          "text": "how do you quantitatively verify that the agent improves when the complexity is changed? when I'm building agents sometimes the more simple agents seem more reliable but it turns out they just have a reduced action space / problem-solving area, and refuse to solve many things. we use simulations to gauge the agent behaviour and then grade it, which is a bit of a newer thing ",
          "score": 1,
          "created_utc": "2026-02-18 14:57:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62mjs3",
              "author": "NetworkFlux",
              "text": "That's a good point and is probably worth for me to write about. But in short, we're actually doing something very similar.\n\nWe have \"simulated user\" LLMs with different personas defined in config files. They talk to our agent until the simulated user decides to end the conversation (goal reached), or if certain deterministic criteria are met, like max # of turns or state reached.\n\nFor judging, we have a suite of heuristic judges (checking for tool calls, checking text for data leakage, etc.) and an LLM judge which is given a precise pass/fail criteria.\n\nFinally we compute a weighted average based on the passes and fails and assign a score to the simulation after running enough for a statistically significant result.\n\nAre you doing something similar?",
              "score": 1,
              "created_utc": "2026-02-18 15:57:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o64awwg",
          "author": "ScArL3T",
          "text": "Do your tools do any heavy lifting in regards to semantic understanding and if yes how do you achieve it?  \nLater in the article you mention about having a simple general agent which in turn calls some well defined tools. In that diagram you showcased the possibility of having a sub-agent -- so they are not completely gone?\n\nI'm kind of interested in the technicalities a bit and diving a bit more in-depth into your architecture.  \nI'm also interested how your initial user query (2 edit requests and 1 question in the same message) gets handled by the generic agent now that it is specifically NOT instructed to deconstruct the user query. Do you just rely on the model's intelligence? And if yes, what model are you using.\n\nThank you!",
          "score": 1,
          "created_utc": "2026-02-18 20:30:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6b1wa",
      "title": "What Are DeepAgents in LangChain?",
      "subreddit": "LangChain",
      "url": "https://www.blog.qualitypointtech.com/2026/02/what-are-deepagents-in-langchain.html",
      "author": "qptbook",
      "created_utc": "2026-02-16 14:28:22",
      "score": 5,
      "num_comments": 11,
      "upvote_ratio": 0.73,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r6b1wa/what_are_deepagents_in_langchain/",
      "domain": "blog.qualitypointtech.com",
      "is_self": false,
      "comments": [
        {
          "id": "o5pfckh",
          "author": "justanemptyvoice",
          "text": "A buzzword\n\nSaved you a click",
          "score": 3,
          "created_utc": "2026-02-16 16:24:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5r0wqa",
              "author": "93simoon",
              "text": "Opened the post to comment the same thing ðŸ˜‚",
              "score": 1,
              "created_utc": "2026-02-16 20:55:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5pmlf6",
              "author": "Niightstalker",
              "text": "No a specific concepts of agents that is worth reading but ok",
              "score": 0,
              "created_utc": "2026-02-16 16:57:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5rzw0k",
                  "author": "mamaBiskothu",
                  "text": "A concept introduced a year too late into the most popular framework in the field. My team Interview questuon is to ask what their thought is on langchain and to reject anyone saying positive things.",
                  "score": 1,
                  "created_utc": "2026-02-16 23:57:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o61sng4",
          "author": "Puzzled_Celery_6190",
          "text": "TLDR: compare to normal ReAct agent, deep agent contains a detailed system prompt, plan tool, sub agent plus an external file system (so that you donâ€™t put everything in context/prompt). Take writing paper for example, it could write paper page by page or sentence by sentence, write as long as you want.",
          "score": 1,
          "created_utc": "2026-02-18 13:28:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}