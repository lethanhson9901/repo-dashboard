{
  "metadata": {
    "last_updated": "2026-03-02 02:56:22",
    "time_filter": "week",
    "subreddit": "LangChain",
    "total_items": 20,
    "total_comments": 73,
    "file_size_bytes": 139927
  },
  "items": [
    {
      "id": "1rejcxa",
      "title": "Things I wish LangChain tutorials told you before you ship to real users",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rejcxa/things_i_wish_langchain_tutorials_told_you_before/",
      "author": "cryptoviksant",
      "created_utc": "2026-02-25 17:17:24",
      "score": 40,
      "num_comments": 16,
      "upvote_ratio": 0.95,
      "text": "I've been building a chatbot product where users upload docs and the bot answers questions from them. Started with LangChain like everyone else, followed the tutorials, got a demo working in an afternoon. Then real users showed up and everything broke in ways I didn't expect. Here's what I learned.\n\nThe standard tutorial flow of load docs, split, embed, vector store, RetrievalQA gets you a working demo fast. But the default text splitters destroy document structure in ways that don't show up until someone asks a question that requires context from two diferent sections. RecursiveCharacterTextSplitter with default chunk size is fine for blog posts but terrible for technical documentation with tables and cross references.\n\nEveryone focuses on which embedding model to use and honestly that's the wrong thing to obsess over. I swapped between OpenAI embedding models and the difference was minimal. What actually matters is what happens after retrieval. Are you pulling the right chunks? Are you pulling enough of them? Are chunks that reference each other actually ending up in the same context window? I spent weeks tweaking embeddings when the real problem was my retrieval grabbing 4 chunks where 2 of them were completely irrelevant.\n\nThe stuff that actually moved the needle for us was all boring unglamorous work. Document preprocessing before anything touches the splitter, like actually cleaning your docs, handling tables properly, preserving headers and structure. Then building a proper evaluation loop where I could see exactly which chunks got retrieved for each question, because without that you're just tuning blind. We also added a system where human answers from moderators get fed back into the knowledge base over time, because static docs alone weren't enough for real world questions. And maybe the biggest win was teaching the bot to say \"I don't know\" instead of the default behavior of always generating something, which just leads to confident hallucinations.\n\nHonestly LangChain was great for prototyping but as complexity grew I found myself fighting the abstractions more than they were helping me. The chains are nice until you need to do something slightly outside the standard flow, then you're digging through source code trying to figure out why your custom retriever isn't being called correctly. I ended up replacing a lot of LangChain components with custom code that does exactly what I need with less magic happening underneath.\n\nNot saying LangChain is bad, it's genuinley great for getting started and understanding the patterns. But if you're shipping to real users I think the sooner you understand what's happening under the abstractions the better off you'll be. The framework isn't the product, the retrieval quality is.\n\nCurious where other people landed on this. Are you still running full LangChain in production or did you end up pulling pieces out over time?",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1rejcxa/things_i_wish_langchain_tutorials_told_you_before/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o7d6i1f",
          "author": "penguinzb1",
          "text": "the evaluation loop point is the real one. you can't know what's wrong with retrieval until you've run it through the actual questions your users bring.",
          "score": 4,
          "created_utc": "2026-02-25 17:58:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7dz0rj",
              "author": "cryptoviksant",
              "text": "Yeah but some question's are answered and others aren't... even if they change just a lil bit.\n\nI'm trying to somehow bring my \"bias\" to the bot, so it has that \"extra\" knowledge to respond even more questions..",
              "score": 3,
              "created_utc": "2026-02-25 20:08:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7d0be2",
          "author": "Any_Animator4546",
          "text": "langchain works fine for me, but that is more because my use case is a lot easier, the role of rag here is to map simple human language to sql filters. Like if a use asks \"share of chatgpt\" the rag needs to map \"COMPANY=OPENAI\" ",
          "score": 1,
          "created_utc": "2026-02-25 17:31:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7d0gr8",
              "author": "cryptoviksant",
              "text": "I see",
              "score": 1,
              "created_utc": "2026-02-25 17:31:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7nl2h9",
                  "author": "CBax777",
                  "text": "Mapping human language to SQL filters sounds like a solid approach. Simpler use cases definitely have their own challenges, but it seems like you've found a good way to leverage LangChain. Have you run into any surprises with your setup?",
                  "score": 1,
                  "created_utc": "2026-02-27 05:51:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7evosj",
          "author": "Gold_Emphasis1325",
          "text": "Thanks for taking the time and sharing that detailed accounting with the internet, especially how busy I'm guessing you are trying to get from demo to production. I wish there were more stories like this out there, because all of the sales and promises are made on the first understanding (LangChain, n8n, CrewAI marketing pitches and all of the chatter about Agentic) and then implementation teams, product leaders and especially customers get burned by framework sprawl, security risks, production engineering not covered by frameworks/quickstarts.... hope you get it all figured out and successful.",
          "score": 1,
          "created_utc": "2026-02-25 22:43:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ewc0w",
              "author": "cryptoviksant",
              "text": "thanks bro",
              "score": 1,
              "created_utc": "2026-02-25 22:46:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7i6o77",
          "author": "ar_tyom2000",
          "text": "This is exactly why I built [LangGraphics](https://github.com/proactive-agent/langgraphics). In production, you need visibility into what your agent is actually doing. Real-time tracing of splits, embeddings, and retrieval steps saves weeks of debugging when things break with real users.",
          "score": 1,
          "created_utc": "2026-02-26 12:42:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mb66m",
          "author": "theferalmonkey",
          "text": "üòÇ",
          "score": 1,
          "created_utc": "2026-02-27 01:02:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mzahg",
          "author": "code_vlogger2003",
          "text": "Have you faced any situation that your retrieval is bringing back enough sufficient relevant information sometimes they are in the sequential other times they are in a jumbled pattern. When passing the user question along with retrieved information, sometimes llm is not giving the completeness in the answer even though it has enough sufficient information that is being supplied by retrieval. Also another case is that are you judging that for a given question what is the percentage of probability of page numbers that it gets back when compared to the ground truth page list for the question instead of chunks. I mean sometimes i thought that instead of having the chunk ida as ground truth etc if we store or construct the ground truth in a way where it has relevant page numbers. Such that in our retrievial it is easy to compare do we cover all the ground truth listed page numbers for the first check of evaluation then next checking will be did the retrieval step brings out any other garbage page numbers. \n\n\nLooking for your thoughts",
          "score": 1,
          "created_utc": "2026-02-27 03:22:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7e0ck7",
          "author": "Tall-Appearance-5835",
          "text": "harrison chase & co has already moved on to ‚Äòdeep agents‚Äô",
          "score": 0,
          "created_utc": "2026-02-25 20:15:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7e9avm",
              "author": "cryptoviksant",
              "text": "What's harrison chase & co?",
              "score": 1,
              "created_utc": "2026-02-25 20:57:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7eappw",
                  "author": "Tall-Appearance-5835",
                  "text": "langchain‚Äôs ceo",
                  "score": 1,
                  "created_utc": "2026-02-25 21:03:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ho2qi",
          "author": "FishIndividual2208",
          "text": "Sounds more like an embedder issue/RAG pipeline issue.  \nYou should be able to find both chunks and combine them.\n\nIn one of my applications I used to pass along 10-15 chunks from the vector search, after I finetuned my embedder, i reduced the context needed by almost 50%., so now i only pass along 5-7 chunks. because the quality of the retrieved chunks are higher.",
          "score": 0,
          "created_utc": "2026-02-26 10:11:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdg2f9",
      "title": "Is Adding a Reranker to My RAG Stack Actually Worth the Extra Latency? (Explained Simply)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rdg2f9/is_adding_a_reranker_to_my_rag_stack_actually/",
      "author": "Silent_Employment966",
      "created_utc": "2026-02-24 13:20:49",
      "score": 38,
      "num_comments": 15,
      "upvote_ratio": 0.86,
      "text": "This comes up constantly and I want to give an honest answer because the reaction (\"rerankers add latency, avoid them\") is wrong but not for the reason most people think.\n\nWe had a good discussion in our office about the same & therefore we dig it deeper & will try to reply to it in a simpler manner. \n\nA typical RAG pipeline looks like this:\n\n    User query\n      ‚Üí Embed query\n      ‚Üí Vector search ‚Üí top 50 chunks\n      ‚Üí Stuff all 50 chunks into LLM prompt\n      ‚Üí Generate answer\n\nThe instinct is: adding a reranker inserts *another* step, so latency goes up. That's true in isolation. But it completely ignores what happens downstream.\n\n**Where the Latency Actually Lives**\n\nLet's be concrete. Here's where time actually gets spent in a RAG call:\n\n|Step|Typical latency|\n|:-|:-|\n|Vector search (top 50)|50‚Äì150ms|\n|Reranker (re-score top 50)|80‚Äì200ms|\n|LLM generation (50 chunks, \\~15k tokens)|4,000‚Äì8,000ms|\n|**Total without reranker**|\\~4,500‚Äì8,500ms|\n|LLM generation (top 5 chunks, \\~1.5k tokens)|600‚Äì1,200ms|\n|**Total with reranker**|\\~1,200‚Äì1,800ms|\n\nThe reranker adds \\~100‚Äì200ms. But it lets you cut your LLM context from 50 chunks to 5. LLM generation time scales roughly linearly with context length ‚Äî so you're trading 200ms of reranker time for 3,000‚Äì7,000ms of LLM savings.\n\n**Net result: total pipeline latency goes** ***down*****, not up.**\n\n**But That's Not the Only Benefit**\n\nEven if latency was neutral, the accuracy argument alone justifies reranking:\n\n**The core problem:** Vector search ranks by embedding similarity, not relevance. These are not the same thing. A chunk that shares vocabulary with your query will score high even if it doesn't actually answer it. Your LLM then hallucinates around bad context.\n\nA reranker does a deep query-document comparison. it reads both the query and the chunk together and scores true relevance. This is fundamentally more accurate than cosine similarity on pre-computed embeddings.\n\nReal-world result: reranking typically gives you 15‚Äì30% improvement in answer quality on standard benchmarks like NDCG@10.\n\n# What Reranker Should You Actually Use?\n\nHere are your main options, honestly compared:\n\n**Open-source / self-hosted**\n\n**BGE-reranker-v2-m3** (BAAI)\n\n* Strong general performance, multilingual\n* Apache 2.0 license, free to self-host\n* Good starting point if you want full control\n* \\~200‚Äì400ms on CPU, \\~50‚Äì100ms on GPU\n\n**ms-marco-MiniLM-L-6-v2** (cross-encoder)\n\n* Lightweight, fast, good for English\n* Great for prototyping\n* Weaker on domain-specific or non-English content\n\n**Managed APIs**\n\n**ZeroEntropy zerank-2**\n\n* Instruction-following (you can pass business context to influence scoring)\n* Calibrated scores (0.8 actually means \\~80% relevance, consistently)\n* Strong multilingual performance across 100+ languages\n* $0.025/1M tokens (\\~50% cheaper than Cohere)\n* Models are open-weight on HuggingFace if you want to self-host\n* Worth evaluating if you're hitting Cohere's limitations or need multilingual support\n\n**Cohere Rerank 3.5**\n\n* Industry standard, solid accuracy\n* \\~$1/1000 queries, \\~100‚Äì150ms latency\n* No instruction-following, scores aren't calibrated (0.7 means different things in different contexts)\n\n**When a Reranker Genuinely Doesn't Help**\n\nTo be fair, there are cases where adding a reranker won't move the needle:\n\n* **Your first-stage retrieval recall is the problem.** If the right chunk isn't in your top 50 at all, no reranker can fix that. \n* **Your chunks are already very short and precise.** If you're chunking at 100 tokens and have a small corpus, the reranker has less room to help.\n* **Your queries are extremely simple and unambiguous.** Basic keyword lookups where BM25 works perfectly don't need reranking.\n\n# Practical Implementation (LangChain)\n\n`from langchain.retrievers import ContextualCompressionRetriever`\n\n`from langchain.retrievers.document_compressors import CrossEncoderReranker`\n\n`from langchain_community.cross_encoders import HuggingFaceCrossEncoder`\n\n\n\n`# Using BGE open-source reranker`\n\n`model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")`\n\n`compressor = CrossEncoderReranker(model=model, top_n=5)`\n\n\n\n`compression_retriever = ContextualCompressionRetriever(`\n\n`base_compressor=compressor,`\n\n`base_retriever=your_vector_retriever  # your existing retriever`\n\n`)`\n\n\n\n`# Now returns top 5 reranked results instead of top 50 raw chunks`\n\n`docs = compression_retriever.invoke(\"your query here\")`\n\nFor a managed API option (ZeroEntropy, Cohere, etc.) the pattern is similar. swap the compressor for an API-based one.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1rdg2f9/is_adding_a_reranker_to_my_rag_stack_actually/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o74vsg5",
          "author": "Deep_Structure2023",
          "text": "Reranker may add little latency but pulls up accurate information, good explanation btw",
          "score": 11,
          "created_utc": "2026-02-24 13:49:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74we39",
              "author": "Silent_Employment966",
              "text": "glad you find it helpful",
              "score": 1,
              "created_utc": "2026-02-24 13:52:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74wqzw",
                  "author": "Deep_Structure2023",
                  "text": "One more thing I forgot to mention, it reduces hallucinations by a great deal",
                  "score": 2,
                  "created_utc": "2026-02-24 13:54:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o74scj2",
          "author": "Raseaae",
          "text": "If the first-stage retrieval is the bottleneck, would you recommend switching to hybrid searchbefore even touching a reranker?",
          "score": 2,
          "created_utc": "2026-02-24 13:30:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74wzg6",
              "author": "InteractionSmall6778",
              "text": "Hybrid search first, always. Reranking bad retrieval results still gives you bad results.",
              "score": 2,
              "created_utc": "2026-02-24 13:55:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o74sx52",
              "author": "Silent_Employment966",
              "text": "Yes, always fix recall before precision a reranker can only reorder what retrieval already found. Hybrid search (BM25 + vector) is the fastest recall boost with no added infrastructure; once recall@50 is solid, *then* layer in the reranker",
              "score": 0,
              "created_utc": "2026-02-24 13:33:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7a6ipo",
                  "author": "ShiHouzi",
                  "text": "There a good way to test this?\n\nI got my BM25 and Vector up with ATS and treeshaking and all that and it‚Äôs fast and accurate. \n\nNow I add the reranker in?",
                  "score": 1,
                  "created_utc": "2026-02-25 06:20:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7a5xtf",
          "author": "midaslibrary",
          "text": "Good looks, how does a reranker work?",
          "score": 1,
          "created_utc": "2026-02-25 06:15:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7a9fit",
          "author": "arrty",
          "text": "How does ANN fit in",
          "score": 1,
          "created_utc": "2026-02-25 06:44:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b0t81",
          "author": "Royal-Environment-18",
          "text": "Thanks for sharing ü´∂",
          "score": 1,
          "created_utc": "2026-02-25 10:55:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hpvje",
          "author": "llamacoded",
          "text": "This is spot on. We've seen the exact same results building our sales agent platform.  \n  \nLatency reduction from rerankers is huge. We initially skipped it to save 'complexity' but our LLM calls were 6-8 seconds.  \nAdding BGE reranker, we cut context from 30 chunks to 7. Average LLM response went from 7s to 1.5s. It's a no-brainer.  \n  \nAccuracy also improved for our specific sales data. Less hallucination means agents are more reliable.  \n  \nGood point about when it \\*doesn't\\* help. We've had cases where the first-stage retrieval just misses the mark entirely. Do you find specific embedding models help more with initial recall, or is it mostly about chunking strategy?",
          "score": 1,
          "created_utc": "2026-02-26 10:28:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pz4r7",
          "author": "Separate_Ad3443",
          "text": "Fkin good, thanks for ts twin",
          "score": 1,
          "created_utc": "2026-02-27 16:07:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7539kn",
          "author": "jannemansonh",
          "text": "the manual rag stack config is real... spent way too long optimizing rerankers and chunk sizes for doc workflows. ended up moving those to needle app since hybrid search/reranking is built in... way easier than wiring it yourself",
          "score": 0,
          "created_utc": "2026-02-24 14:28:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78mhr0",
              "author": "Fun-Job-2554",
              "text": "I kept seeing the same problem ‚Äî agents get stuck calling the same\ntool 50 times, wander off-task, or burn through token budgets before\nanyone notices. The big observability platforms exist but they're\nheavy for solo devs and small teams.\n\nSo I built DriftShield Mini ‚Äî a lightweight Python library that wraps\nyour existing LangChain/CrewAI agent, learns what \"normal\" looks like,\nand fires Slack/Discord alerts when something drifts.\n\n3 detectors:\n- Action loops (repeated tool calls, A‚ÜíB‚ÜíA‚ÜíB cycles)\n- Goal drift (agent wandering from its objective, using local embeddings)\n- Resource spikes (abnormal token/time usage vs baseline)\n\n4 lines to integrate:\n\n    from driftshield import DriftMonitor\n    monitor = DriftMonitor(agent\\_id=\"my-agent\", alert\\_webhook=\"https://hooks.slack.com/...\")\n    agent = monitor.wrap(existing\\_agent)\n    result = agent.invoke({\"input\": \"your task\"})\n\n100% local ‚Äî SQLite + CPU embeddings. Nothing leaves your machine\nexcept the alerts you configure.\n\npip install driftshield-mini\nGitHub: https://github.com/ThirumaranAsokan/Driftshield-mini\n\nv0.1 ‚Äî built this solo. Would genuinely love feedback on what\nagent reliability problems you're hitting. What should I build next?",
              "score": 1,
              "created_utc": "2026-02-25 00:31:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rg60aj",
      "title": "8 AI Agent Concepts I Wish I Knew as a Beginner",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rg60aj/8_ai_agent_concepts_i_wish_i_knew_as_a_beginner/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-27 13:00:02",
      "score": 34,
      "num_comments": 4,
      "upvote_ratio": 0.93,
      "text": "Building an AI agent is easy. Building one that actually works reliably in production is where most people hit a wall.\n\nYou can spin up an agent in a weekend. Connect an LLM, add some tools, include conversation history and it seems intelligent. But when you give it real workloads it starts overthinking simple tasks, spiraling into recursive reasoning loops, and quietly multiplying API calls until costs explode.\n\nBeen building agents for a while and figured I'd share the architectural concepts that actually matter when you're trying to move past prototypes.\n\nMCP is the universal plugin layer: Model Context Protocol lets you implement tool integrations once and any MCP-compatible agent can use them automatically. Think API standardization but for agent tooling. Instead of custom integrations for every framework you write it once.\n\nTool calling vs function calling seem identical but aren't: Function calling is deterministic where the LLM generates parameters and your code executes the function immediately. Tool calling is iterative where the agent decides when and how to invoke tools, can chain multiple calls together, and adapts based on intermediate results. Start with function calling for simple workflows, upgrade to tool calling when you need iterative reasoning.\n\nAgentic loops and termination conditions are where most production agents fail catastrophically:The decision loop continues until task complete but without proper termination you get infinite loops, premature exits, resource exhaustion, or stuck states where agents repeat failed actions indefinitely. Use resource budgets as hard limits for safety, goal achievement as primary termination for quality, and loop detection to prevent stuck states for reliability.\n\nMemory architecture isn't just dump everything in a vector database: Production systems need layered memory. Short-term is your context window. Medium-term is session cache with recent preferences, entities mentioned, ongoing task state, and recent failures to avoid repeating. Long-term is vector DB. Research shows lost-in-the-middle phenomenon where information in the middle 50 percent of context has 30 to 40 percent lower retrieval accuracy than beginning or end.\n\nContext window management matters even with 200k tokens: Large context doesn't solve problems it delays them. Information placement affects retreval. First 10 percent of context gets 87 percent retrieval accuracy. Middle 50 percent gets 52 percent. Last 10 percent gets 81 percent. Use hierarchical structure first, add compression when costs matter, reserve multi-pass for complex analytical tasks.\n\nRAG with agents requires knowing when to retrieve: Before embedding extract structured information for better precision, metadata filtering, and proper context. Auto-retrieve always has high latency and low precision. Agent-directed retrieval has variable latency but high precision. Iterative has very high latency but very high precision. Match strategy to use case.\n\nMulti-agent orchestration has three main patterns: Sequential pipeline moves tasks through fixed chain of specialized agents, works for linear workflows but iteration is expensive. Hierarchical manager-worker has coordinator that breaks down tasks and assigns to workers, good for parallelizable problems but manager needs domain expertise. Peer-to-peer has agents communicating directly, flexible but can fall into endless clarification loops without boundaries.\n\nProduction readiness is about architecture not just models: Standards like MCP are emerging, models getting cheaper and faster, but the fundamental challenges around memory management, cost control, and error handling remain architectural problems that frameworks alone won't solve.\n\nAnyway figured this might save someone else the painful learning curve. These concepts separate prototypes that work in demos from systems you can actually trust in production.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1rg60aj/8_ai_agent_concepts_i_wish_i_knew_as_a_beginner/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o7ozk47",
          "author": "Independent-Cost-971",
          "text": "I explained these visually in a blog if anyone's interested:¬†[https://kudra.ai/8-ai-agent-concepts-every-ai-developer-needs-in-2026-visually-explained/](https://kudra.ai/8-ai-agent-concepts-every-ai-developer-needs-in-2026-visually-explained/)",
          "score": 5,
          "created_utc": "2026-02-27 13:00:18",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o7pgxkp",
          "author": "jlebensold",
          "text": "I've been writing about my experience building [jetty.io](http://jetty.io) through a substack blog: [https://lebensold.substack.com/p/ai-optimization-is-a-game-of-whack](https://lebensold.substack.com/p/ai-optimization-is-a-game-of-whack) . I think a lot of the big surprises come from stitching models together, having loops with LLM calls in the middle or using a frontier model for everything. I've also seen a lot of cases where the ratio of input:output tokens is bananas due to a lot of additional context (e.g. imaging using 400k tokens to update 2 lines of a readme).",
          "score": 1,
          "created_utc": "2026-02-27 14:38:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rcqgg",
              "author": "dreadedangelsdesigns",
              "text": "Stitching models can definitely lead to some unexpected token bloat. Balancing context while keeping efficiency in check is a real juggling act. Have you found any effective strategies to optimize that input/output ratio?",
              "score": 1,
              "created_utc": "2026-02-27 20:04:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7rgkpb",
                  "author": "jlebensold",
                  "text": "first step is just logging your traces / telemetry. Langfuse does a good job here and it's not difficult to setup in your application. Then you can do a full analysis of the types of traces after a period of time to uncover the ratios and where the spend might be unreasonable. The agent we built with Jetty does this automatically and sends you a PR. We're still early on in our product journey but feel free to sign up and see if it works for you.",
                  "score": 1,
                  "created_utc": "2026-02-27 20:23:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1re3mp7",
      "title": "I built a Graph-RAG travel engine in 24h Hackathon. The judges said \"ChatGPT can do this.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1re3mp7/i_built_a_graphrag_travel_engine_in_24h_hackathon/",
      "author": "XstonedBonobo",
      "created_utc": "2026-02-25 04:42:04",
      "score": 31,
      "num_comments": 3,
      "upvote_ratio": 0.89,
      "text": "Hey everyone,\n\nI just finished a 24-hour hackathon in Chennai. My team and I built Xplorer a travel web app.\n\nInstead of just being a wrapper for a prompt, we actually built a pipeline:\n\nGraph + Vector RAG: Used graph relations to map user interests to locations.\n\nIntelligent Sequencing: It doesn't just list places; it orders them based on the \"best time to visit\" for that specific spot.\n\nAgentic Workflow: We used Gemini to power agents that handle hotel and cab booking logic.\n\nPersonally, I think there‚Äôs a massive gap between an LLM hallucinating a itinerary and a structured system that handles RAG retrieval and booking logic. But maybe I'm biased.\n\n**I‚Äôd love for some actual devs to look at the demo and settle the debate:**\n\n1. **Watch the demo:**¬†[https://www.youtube.com/watch?v=23-vhrRhCP0](https://www.youtube.com/watch?v=23-vhrRhCP0)\n2. **Feedback:**¬†[https://forms.gle/TRZjWoMiiW4P3kUt7](https://forms.gle/TRZjWoMiiW4P3kUt7)\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1re3mp7/i_built_a_graphrag_travel_engine_in_24h_hackathon/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o7as9m6",
          "author": "Friendly-Ask6895",
          "text": "the \"ChatGPT can do this\" response reveals the core problem: if non-technical evaluators can't immediately see the difference between your structured pipeline and a vanilla LLM prompt, you have a presentation problem not a technology problem.\n\n  \nyour architecture is clearly more robust - graph relations for interest mapping, temporal sequencing for visit ordering, actual booking agents instead of hallucinated recommendations. but if the demo just shows a text output that looks similar to what ChatGPT would produce, judges literally cannot tell the difference.\n\n  \nwhat i'd suggest for next time: make the pipeline visible in the demo. show the graph being traversed, show the sequencing logic working, let people see the agents coordinating in real time. the moment someone can watch a booking agent actually check availability vs a chatbot just saying \"you could book a hotel here,\" the gap becomes obvious.\n\n  \nthis is honestly one of the hardest problems in the agent space right now - the technical sophistication is outpacing the interfaces we use to demonstrate and deliver it. your underlying system is solid, the gap is in how users experience it.",
          "score": 22,
          "created_utc": "2026-02-25 09:37:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7a23va",
          "author": "Armbahr",
          "text": "‚ÄúIs it better‚Äù is always an empirical question: Define what your ‚Äúgap‚Äù actually _is_, measure your competitor, measure yourself, show delta, iterate üôÇ\n\nBoth conceptually and potentially, there _is_ a gap between an LLM generating an itinerary from training data (where hallucination is a legitimate concern) and an LLM generating that itinerary with RAG. Your intuition that there‚Äôs a gap between e.g., cosine similarity and graph retrieval, is also real: Semantics do matter, and incorporating them into retrieval can be an unlock. You‚Äôre onto something. \n\nBut _convincing others_ of that value is a challenge when your solutions is conceptual like this‚Ä¶Most judges and ‚Äúfounders‚Äù aren‚Äôt particularly attuned to high-signal _concepts_.\n\nThey need a single plot and five bullet points, or they just won‚Äôt bite.\n\nMy suggested checklist:\n- Do you have an example of what ‚Äúthe other folks‚Äù are shipping? I.e., who are your competitors, and what is their product? \n- Do you have a comparison example of _your_ output,? You‚Äôre advertising here, so the answer ought to be yes üôÉ\n- Find a compelling way to compare this to the above. \n- Do your results show that your solution is comparable (at least)? Ideally, _clear-cut_ better? \n\n\nPeople will buy if you can ship _any_ of these stories:\n- Clear cost-savings on the agent‚Äôs token consumption, _or_\n- Patently better itineraries, _or_\n- Quality: if your output are reproducibly, measurably better, you can win if you nail GTM.\n\n\nA negative result shouldn‚Äôt be discouraging, by the way. If you can measure the gap, you‚Äôre already leagues ahead. Knowing what you need to fix is a whole battle that most people skip. \n\n\nWhat‚Äôs your actual goal here ‚Äî validate the idea, tune its performance, etc.?\n\nEDIT: A typo I‚Ä¶Already forgot, and a rephrasing I also already forgot ü§¶üèæ\n\nEDIT 2: Straggling typo + phrasing. Last one.",
          "score": 5,
          "created_utc": "2026-02-25 05:44:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ac7hk",
          "author": "External-Stretch7315",
          "text": "its not novel at all, but a good toy project",
          "score": 0,
          "created_utc": "2026-02-25 07:08:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rffpt5",
      "title": "Agentic RAG for Dummies v2.0",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rffpt5/agentic_rag_for_dummies_v20/",
      "author": "CapitalShake3085",
      "created_utc": "2026-02-26 17:02:19",
      "score": 22,
      "num_comments": 2,
      "upvote_ratio": 0.96,
      "text": "Hey everyone! I've been working on **Agentic RAG for Dummies**, an open-source project that shows how to build a modular Agentic RAG system with LangGraph ‚Äî and today I'm releasing v2.0.\n\nThe goal of the project is to bridge the gap between basic RAG tutorials and real, extensible agent-driven systems. It supports any LLM provider (Ollama, OpenAI, Anthropic, Google) and includes a step-by-step notebook for learning + a modular Python project for building.\n\n## What's new in v2.0\n\nüß† **Context Compression** ‚Äî The agent now compresses its working memory when the context exceeds a configurable token threshold, keeping retrieval loops lean and preventing redundant tool calls. Both the threshold and the growth factor are fully tunable.\n\nüõë **Agent Limits & Fallback Response** ‚Äî Hard caps on tool invocations and reasoning iterations ensure the agent never loops indefinitely. When a limit is hit, instead of failing silently, the agent falls back to a dedicated response node and generates the best possible answer from everything retrieved so far.\n\n## Core features\n\n- Hierarchical indexing (parent/child chunks) with hybrid search via Qdrant\n- Conversation memory across questions\n- Human-in-the-loop query clarification\n- Multi-agent map-reduce for parallel sub-query execution\n- Self-correction when retrieval results are insufficient\n- Works fully local with Ollama\n\nThere's also a Google Colab notebook if you want to try it without setting anything up locally.\n\nGitHub: https://github.com/GiovanniPasq/agentic-rag-for-dummies",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1rffpt5/agentic_rag_for_dummies_v20/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o7oho3w",
          "author": "ar_tyom2000",
          "text": "This modular RAG design is exactly where visibility matters most. I built [LangGraphics](https://github.com/proactive-agent/langgraphics) specifically for debugging agents like this - real-time visualization of tool calls, context decisions, fallback routing. Single-line integration shows you the exact decision tree without refactoring.",
          "score": 2,
          "created_utc": "2026-02-27 10:44:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfye1z",
      "title": "I love the OpenClaw idea, but I didn't want to ditch Langchain. So I built a bridge.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rfye1z/i_love_the_openclaw_idea_but_i_didnt_want_to/",
      "author": "tisu1902",
      "created_utc": "2026-02-27 05:47:07",
      "score": 17,
      "num_comments": 3,
      "upvote_ratio": 0.9,
      "text": "Yo, like a lot of you, I've been watching **openclaw** explode. Its core idea is brilliant (but simple): decouple the agent from the UI and give it \"proactive\" powers (crons/heartbeats) so it can reach out to you first on Telegram or Discord.\n\nHowever, as someone who have spent months building things in **langchain** and **langgraph**, switching to **openclaw** complete ecosystem feels like a massive effort and risk.\n\nI wanted those production-ready features without losing the maturity of the langchain ecosystem. So I built [Langclaw](https://github.com/tisu19021997/langclaw).\n\nBasically, it‚Äôs a production gateway for your existing LangGraph agents.\n\n* **Proactive Agents:** It uses APScheduler v4 to let your agents run crons or \"heartbeats\" through your same message pipeline.\n* **Guardrails:** Built-in middleware for PII redaction and RBAC.\n* **Composable:**  Support different message transport, state persistence, channels, langchain middleware, and LLMs, all swappable via config or a single subclass.\n* **Multi-channel:** One bus for Telegram, Discord, and WebSockets.\n\nIf you know LangGraph, you already know how to use this. You just register your `CompiledStateGraph` as a sub-agent and it handles the rest.\n\nIt‚Äôs still early (v0.1 vibes), so I‚Äôm looking for some dev feedback on the architecture. If you like it and would like to support, leave a star ‚≠êÔ∏è. Thanks!\n\n**GitHub:** [https://github.com/tisu19021997/langclaw](https://github.com/tisu19021997/langclaw)  \n**Deepwiki Index:** [https://deepwiki.com/openclaw/openclaw](https://deepwiki.com/openclaw/openclaw)\n\n**Installation:** `pip install langclaw[all]` (or `uv add langclaw[all]`)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1rfye1z/i_love_the_openclaw_idea_but_i_didnt_want_to/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o7pdry3",
          "author": "ItuPhi",
          "text": "Awesome I felt exactly the same and started checking out the openClaw architecture to implement my own. I have a few feature already built in. Will check this out in detail would love to contribute",
          "score": 1,
          "created_utc": "2026-02-27 14:21:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7qwvwu",
              "author": "tisu1902",
              "text": "Yes please, feel free to contribue anything. May I also know what features are you working on?",
              "score": 1,
              "created_utc": "2026-02-27 18:46:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7v2gws",
          "author": "Outrageous_Hyena6143",
          "text": "Pretty cool project, I've built something similar for PydanticAI :) [https://www.initrunner.ai/](https://www.initrunner.ai/)",
          "score": 1,
          "created_utc": "2026-02-28 11:16:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdc0lt",
      "title": "Why every AI memory system only implements 1 of 3 memory types ‚Äî and how to fix it",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rdc0lt/why_every_ai_memory_system_only_implements_1_of_3/",
      "author": "No_Advertising2536",
      "created_utc": "2026-02-24 09:49:18",
      "score": 15,
      "num_comments": 1,
      "upvote_ratio": 0.94,
      "text": "Every memory tool I've seen ‚Äî Mem0, MemGPT, RAG-based approaches ‚Äî does the same thing: extract facts, embed them, retrieve by cosine similarity. \"User likes Python.\" \"User lives in Berlin.\" Done.\n\nBut cognitive science has known since the 1970s (Tulving's work) that human memory has at least 3 distinct types that serve fundamentally different retrieval patterns:\n\n* **Semantic** ‚Äî general facts and knowledge (\"What do I know about X?\")\n* **Episodic** ‚Äî personal experiences tied to time/place (\"What happened last time?\")\n* **Procedural** ‚Äî knowing how to do things, with success/failure tracking (\"What's the best way to do X?\")\n\nI built an open-source memory API that implements all three. Here's what I learned.\n\n**How it actually works**\n\nWhen you send a conversation to `/v1/add`, the LLM doesn't just pull facts. It classifies each piece into: entities+facts (semantic), time-anchored episodes (episodic), and multi-step workflows with success/failure tracking (procedural). One conversation often produces all three types.\n\n`/v1/search` queries all three stores in parallel and merges results. But `/v1/search/all` returns them separated ‚Äî so your agent can reason differently: \"I know X\" (semantic) vs \"last time we tried X, it broke Y\" (episodic) vs \"the reliable way to do X is steps 1‚Üí2‚Üí3, worked 4/5 times\" (procedural).\n\n**The key insight:** retrieval quality improves not because the embeddings are better, but because you're searching a smaller, more coherent space. Searching 500 facts is harder than searching 200 facts + 150 episodes + 50 procedures separately ‚Äî less noise per query.\n\n**What surprised me building this**\n\n* **Episodic memory needs temporal grounding badly.** \"Last Tuesday\" means nothing 3 months later. We embed actual dates into the event text before vectorizing.\n* **Procedural memory is the most underrated type.** Agents that remember \"this deploy process failed when we skipped step 3\" make dramatically fewer repeated mistakes. Procedures also evolve ‚Äî each execution with feedback updates the confidence score.\n* **Deduplication across types is a hard problem.** \"User moved to Berlin\" (fact) and \"User told me they moved to Berlin last week\" (episode) are related but shouldn't be merged.\n\n**What's in it now**\n\n* **MCP server** ‚Äî works with Claude Desktop, Cursor, Windsurf. Your AI remembers everything across sessions.\n* **3 AI agents** ‚Äî curator (finds contradictions), connector (discovers hidden links between entities), digest (generates briefings)\n* **Knowledge graph** ‚Äî D3.js visualization of entities and relationships\n* **Smart triggers** ‚Äî proactive memory that fires when context matches\n* **Cognitive profile** ‚Äî AI builds a user profile from accumulated memory\n* **LangChain & CrewAI integrations** ‚Äî drop-in memory for existing agent frameworks\n* **Team sharing** ‚Äî multiple users/agents sharing one memory space\n* **Sub-users** ‚Äî one API key, isolated memory per end-user (for building SaaS on top)\n* **Hosted version** at [mengram.io](https://mengram.io) if you don't want to self-host\n\nPython SDK, JS/TS SDK, REST API. Apache 2.0.\n\n**GitHub:** [github.com/alibaizhanov/mengram](https://github.com/alibaizhanov/mengram)\n\nHappy to answer any architecture questions.",
      "is_original_content": false,
      "link_flair_text": "Announcement",
      "permalink": "https://reddit.com/r/LangChain/comments/1rdc0lt/why_every_ai_memory_system_only_implements_1_of_3/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o78mqrj",
          "author": "Fun-Job-2554",
          "text": "I kept seeing the same problem ‚Äî agents get stuck calling the same\n\ntool 50 times, wander off-task, or burn through token budgets before\n\nanyone notices. The big observability platforms exist but they're\n\nheavy for solo devs and small teams.\n\n\n\nSo I built DriftShield Mini ‚Äî a lightweight Python library that wraps\n\nyour existing LangChain/CrewAI agent, learns what \"normal\" looks like,\n\nand fires Slack/Discord alerts when something drifts.\n\n\n\n3 detectors:\n\n\\- Action loops (repeated tool calls, A‚ÜíB‚ÜíA‚ÜíB cycles)\n\n\\- Goal drift (agent wandering from its objective, using local embeddings)\n\n\\- Resource spikes (abnormal token/time usage vs baseline)\n\n\n\n4 lines to integrate:\n\n\n\nfrom driftshield import DriftMonitor\n\nmonitor = DriftMonitor(agent\\_id=\"my-agent\", alert\\_webhook=\"https://hooks.slack.com/...\")\n\nagent = monitor.wrap(existing\\_agent)\n\nresult = agent.invoke({\"input\": \"your task\"})\n\n\n\n100% local ‚Äî SQLite + CPU embeddings. Nothing leaves your machine\n\nexcept the alerts you configure.\n\n\n\npip install driftshield-mini\n\nGitHub: [https://github.com/ThirumaranAsokan/Driftshield-mini](https://github.com/ThirumaranAsokan/Driftshield-mini)\n\n\n\nv0.1 ‚Äî built this solo. Would genuinely love feedback on what\n\nagent reliability problems you're hitting. ",
          "score": 0,
          "created_utc": "2026-02-25 00:32:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhpqqx",
      "title": "Best practices for testing LangChain pipelines? Unit testing feels useless for LLM outputs",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rhpqqx/best_practices_for_testing_langchain_pipelines/",
      "author": "DARK_114",
      "created_utc": "2026-03-01 06:12:09",
      "score": 14,
      "num_comments": 9,
      "upvote_ratio": 0.93,
      "text": "I'm building a fairly complex LangChain pipeline, multi step retrieval, tool use, final summarization, and I'm struggling to figure out how to test it properly.\n \nTraditional unit tests feel kind of pointless here. I can assert that a function returns a string, but that tells me nothing about whether the output is actually correct or useful.\n \nMy current approach is a messy mix of: logging outputs to a spreadsheet, manually reviewing a sample every week, and just hoping nothing breaks. Obviously this is not sustainable.\n \nHow are people properly testing their LangChain applications? Looking for both pre deployment testing approaches and runtime monitoring ideas. Any tools or frameworks you'd recommend?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1rhpqqx/best_practices_for_testing_langchain_pipelines/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o80jjp2",
          "author": "Zomunieo",
          "text": "Unit tests are useful for testing tools. You can write a fake agent that uses the tools in a canned manner to confirm they work as intended. \n\nIf you‚Äôre not using structured outputs use that. It goes a long way. \n\nUse an eval framework like pydantic evals. Maybe langchain has it too. This combines static checks. Use structured outputs so agent can signal pass/fail. If the task is to say, marking the approximate location of the cat in an image, you need to give it a way to say success=False reason=‚ÄúThis is a goat, not a cat.‚Äù Then you can build up a collection of test cases and score your pipeline to see how it matches your ground truth cases, and detect regressions. Oops, maybe that edit to the system prompt wasn‚Äôt so well thought out. \n\nYou can also use a lighter weight semantic library like sentence transformers to test if LLM output semantically resembles the correct answer. It can test for equivalent statements. ‚ÄúYes, I can see a cat clearly in the photo.‚Äù ~= ‚ÄúA house cat is present in the center.‚Äù\n\nFor grading truly LLM things like summaries you can use another LLM as a judge (usually a more powerful model). Use sparing; this gets expensive and less reliable.",
          "score": 2,
          "created_utc": "2026-03-01 06:55:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80jtjy",
          "author": "ranp34",
          "text": "Evals",
          "score": 1,
          "created_utc": "2026-03-01 06:57:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o814lca",
          "author": "ar_tyom2000",
          "text": "Try [LangGraphics](https://github.com/proactive-agent/langgraphics) \\- it gives you real-time visibility into how LangChain and LangGraph agents execute. You can follow how data moves through each step, inspect intermediate outputs, and verify that your pipeline logic behaves as expected.",
          "score": 1,
          "created_utc": "2026-03-01 10:13:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o83inpb",
              "author": "motorsportlife",
              "text": "Does it work with langchains create_agent? Or do you explicitly need LangGraph",
              "score": 2,
              "created_utc": "2026-03-01 18:46:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o83je19",
                  "author": "ar_tyom2000",
                  "text": "It works with all kinds of LangChains' agents, such as langchains, langgraph, and deepagents.",
                  "score": 1,
                  "created_utc": "2026-03-01 18:49:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o827y0i",
          "author": "FragrantBox4293",
          "text": "two approaches that are worth trying: LLM-as-judge for subjective quality (use a stronger model to score your pipeline's outputs against your criteria), and DeepEval if you want something more structured it's basically pytest but for LLM outputs, integrates with LangChain and runs in CI.  \n  \nfor runtime monitoring, LangSmith is the path of least resistance if you're already on LangChain you get full traces of every step, which makes debugging way less of a guessing game.",
          "score": 1,
          "created_utc": "2026-03-01 15:02:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o82a10t",
          "author": "ITSamurai",
          "text": "As everyone mentioned tools like OpenEval, DeepEval are way to go compared with LangSmith and Langfuse. From my personal experience LangSmith got quite expensive switched to LangFuse. There you can write your custom evaluators and use LLM as a judge concept. ",
          "score": 1,
          "created_utc": "2026-03-01 15:13:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o82kffa",
          "author": "NoSpeed6264",
          "text": "For LangChain specifically, Confident AI integrates really cleanly. You instrument your chain with their tracing and then you can run automated evals on each step independently. So you can evaluate your retrieval quality separate from your generation quality. They have prebuilt metrics for RAG eval that work out of the box. Confident-ai  has a good getting started guide.",
          "score": 1,
          "created_utc": "2026-03-01 16:04:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o82l3vn",
          "author": "Parking-Concern9575",
          "text": "The key insight I had was to stop thinking about testing LLM apps like software and start thinking about them like ML models  you need a test dataset and metrics, not assertions. Confident AI lets you build those datasets and run evals systematically. You can even have it auto generate test cases from your docs.",
          "score": 1,
          "created_utc": "2026-03-01 16:07:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rekv8r",
      "title": "Stop using LLMs to categorize your prompts (it's too slow)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rekv8r/stop_using_llms_to_categorize_your_prompts_its/",
      "author": "PreviousBear8208",
      "created_utc": "2026-02-25 18:08:35",
      "score": 13,
      "num_comments": 11,
      "upvote_ratio": 0.78,
      "text": "I was burning through API credits just having GPT-5 decide if a user's prompt was simple or complex before routing it. Adding almost a full second of latency just for classification felt completely backwards, so I wrote a tiny TS utility to locally score and route prompts using heuristics instead. It runs in <1ms with zero API cost, completely cutting out the \"router LLM\" middleman. I just open-sourced it as¬†`llm-switchboard`¬†on NPM,  hope it helps someone else stop wasting tokens!",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1rekv8r/stop_using_llms_to_categorize_your_prompts_its/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o7dg7jc",
          "author": "DangerWizzle",
          "text": "Why the fuck are you using GPT-5 for basic stuff like that lol, you bringing a bazooka to a knife fight / are you made of tokens?¬†",
          "score": 10,
          "created_utc": "2026-02-25 18:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7dvd7c",
              "author": "PreviousBear8208",
              "text": "Yeah, fair üòÖ  \nGPT-5 was overkill, it just happened to be the default model in that pipeline.\n\nThe point wasn‚Äôt ‚ÄúGPT-5 is required,‚Äù it was realizing *any* LLM call for basic routing is unnecessary overhead when deterministic logic works.",
              "score": 1,
              "created_utc": "2026-02-25 19:51:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7f3zo9",
                  "author": "Tall-Appearance-5835",
                  "text": "use this instead https://github.com/aurelio-labs/semantic-router",
                  "score": 4,
                  "created_utc": "2026-02-25 23:27:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7emgfx",
                  "author": "Thick-Protection-458",
                  "text": "And if not deterministic logic than you may let testers / user do something, record that data, do train / test splits, augment train data with LLMs - and train some BERT-based classifier with such a dataset.",
                  "score": 1,
                  "created_utc": "2026-02-25 21:58:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7fgboo",
              "author": "naobebocafe",
              "text": "\\+1",
              "score": 1,
              "created_utc": "2026-02-26 00:35:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7f1721",
          "author": "Tough-Permission-804",
          "text": "i download a free router llm for this.  so i have a router LLM a local medium sized llm and its hooked to gpt 5.2",
          "score": 2,
          "created_utc": "2026-02-25 23:12:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7g17md",
              "author": "Comfortable-Power-71",
              "text": "This is the way. Local LLM (and free) can do basic reasoning for you before burning credits. I‚Äôm shouting this hoping anyone is listening. Broad applications.",
              "score": 3,
              "created_utc": "2026-02-26 02:33:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7gicmr",
                  "author": "Tough-Permission-804",
                  "text": "oh!  i been working with my local instance to try to put a cognitive and VAM layer.  local short and long term memory and to agentize and build in curiosity to that it can spend the day researching.  My go is to simulate continuity and hopefully cognition and intelligence someday.  im also building it an avatar in can inhabit.  and if you get this program called lively wallpaper. instead of a web browser, you can out her right on your desktop below the looking glass.  i have it set up so i can click anywhere on the desktop and a chat window shows up.\n\nI feel like i just puked all over. sorry.. üòÜ\n\nhttps://preview.redd.it/ehabfphjlrlg1.jpeg?width=4032&format=pjpg&auto=webp&s=8852b68349a5c02637a08801afb88aad3c4b2757",
                  "score": 1,
                  "created_utc": "2026-02-26 04:16:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7gploo",
          "author": "thecandiedkeynes",
          "text": "depending on the degree of classification there is still some utility to an LLM call, but I just use nano for my use case. ",
          "score": 1,
          "created_utc": "2026-02-26 05:07:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hprzs",
          "author": "iridescent_herb",
          "text": "would my router be of help in this case? [mysteriousHerb/lazyrouter: Lazyrouter - fully self-hosted router for openclaw for cost saving](https://github.com/mysteriousHerb/lazyrouter)\n\ni find gpt oss 120b is really fast and good,",
          "score": 1,
          "created_utc": "2026-02-26 10:27:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rg49yx",
      "title": "The Career Deadlock Nobody Talks About: Not a Fresher, Not Experienced Enough.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rg49yx/the_career_deadlock_nobody_talks_about_not_a/",
      "author": "Royal-Environment-18",
      "created_utc": "2026-02-27 11:32:08",
      "score": 13,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nI‚Äôm sharing this because I feel stuck in a career loop, and I‚Äôm hoping for honest advice ‚Äî and if possible, opportunities.\n\n‚∏ª\n\n2021 ‚Äì BE Graduate\n\nGraduated in 2021.\n\n‚∏ª\n\nMay 2022 ‚Äì Joined Wipro\n\nI joined Wipro and was trained in Oracle OBIEE under the Oracle BI Readiness team.\n\nI received KT and internal training, but never got real production project exposure. Over time, I was moved off allocations and ended up on bench for months.\n\n‚∏ª\n\nJan 2023 ‚Äì Jan 2024 ‚Äì PG Diploma in Data Science (Datatrained Academy)\n\nTo improve my career prospects, I enrolled in a PG Diploma in Data Science from Datatrained Academy (they advertised 100% placement support).\n\nDuring the program:\n\n\t‚Ä¢\tCompleted an internship\n\n\t‚Ä¢\tBuilt \\~20 Data Science projects\n\n\t‚Ä¢\tUploaded all projects on GitHub\n\n\t‚Ä¢\tWorked on EDA, ML models, preprocessing pipelines, etc.\n\nI genuinely believed this would help me transition into a Data Science role.\n\n‚∏ª\n\nMid 2024 ‚Äì Long Bench + Loss of Pay\n\nBench continued at Wipro.\n\nThen loss of pay started.\n\n‚∏ª\n\nNovember 2024 ‚Äì Resigned\n\nHR asked me to resign due to extended bench duration. I resigned.\n\nThat period was mentally and financially difficult.\n\n‚∏ª\n\nLast 6‚Äì8 Months ‚Äì Applying for Data Science Roles\n\nI applied consistently for 6‚Äì8 months.\n\nResults:\n\n\t‚Ä¢\t5‚Äì6 interviews total\n\n\t‚Ä¢\tOnly 2 interviewers seriously evaluated me\n\n\t‚Ä¢\tOthers were short or non-technical\n\nAnd here‚Äôs the trap:\n\n\t‚Ä¢\tBE 2021 graduate\n\n\t‚Ä¢\t\\~2.5 years experience on paper\n\n\t‚Ä¢\tAlmost no real production deployment experience\n\n\t‚Ä¢\tNot treated as fresher\n\n\t‚Ä¢\tNot treated as experienced\n\nI feel stuck in between.\n\n‚∏ª\n\nWhat I‚Äôm Doing Now (Last 4‚Äì5 Months)\n\nInstead of quitting tech, I decided to pivot seriously.\n\nI‚Äôve been focusing deeply on:\n\n\t‚Ä¢\tAI Agents\n\n\t‚Ä¢\tRAG pipelines\n\n\t‚Ä¢\tNLP-to-SQL systems\n\n\t‚Ä¢\tLLM-based application architecture\n\n\t‚Ä¢\tPrompt engineering\n\n\t‚Ä¢\tEvaluation & validation layers\n\n\t‚Ä¢\tDesigning systems with production thinking\n\nNot just tutorials ‚Äî building structured, versioned projects.\n\n‚∏ª\n\nWhat I‚Äôm Looking For\n\nI‚Äôm actively looking for:\n\n\t‚Ä¢\tFull-time roles (AI / ML / Data / LLM-based systems)\n\n\t‚Ä¢\tInternships\n\n\t‚Ä¢\tPart-time roles\n\n\t‚Ä¢\tStartup collaborations\n\n\t‚Ä¢\tOpen-source contribution opportunities\n\nI‚Äôm ready to work hard, contribute, and grow. I‚Äôm not looking for shortcuts ‚Äî just real exposure and real responsibility.\n\n‚∏ª\n\nI‚Äôd Appreciate Honest Advice\n\nIf you‚Äôve been in a similar ‚Äúin-between experience‚Äù situation:\n\n\t‚Ä¢\tHow did you break out of it?\n\n\t‚Ä¢\tShould I double down on AI agents?\n\n\t‚Ä¢\tOr go back and target core Data Science roles?\n\nAny clarity, guidance, or opportunity would genuinely help.\n\nThank you for reading.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1rg49yx/the_career_deadlock_nobody_talks_about_not_a/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o7u3oyf",
          "author": "Ron-Caster",
          "text": "Hii. Myself 2023 BTech ECE Passout. Still got 2 backlogs. Currently working as an AI Engineer for the past 3 months. 1 year experience as a Software Engineer at a <10 employee company where I actually did nothing. Before that namesake Python Developer Intern at a sister concern NGO at the same building where I worked as SWE for the first one year. After that stupid internship, I tried to get into Customer Support and call centers, but didn't join. Tech interviews I failed miserably. Only thing I was doing was making stupid programs with AI on RAG, LlamaIndex, LangChain, LangGraph etc.. I couldn't answer a single technical question. Can't code a python DSA problem. I didn't even know what DSA was. I had around 80+ projects by 2025 December. I stopped applying for jobs for 1.3 years! As I got converted into SWE for namesake at the first company from the NGO on 2024 November. 2025 October I resigned, took my bike and went for Uber Bike taxi. And stopped due to backpain. Luckily with my Naukri profile and GitHub. I got 3-4 calls. 2 interviews and passed both. Chose the product based company. Yeah. Life will change. Work had. Whenever I was unemployed all I did was vibecoding projects and posting it to GitHub. I didn't use any social media. Only GitHub and Naukri. Try my path",
          "score": 2,
          "created_utc": "2026-02-28 05:56:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7u4lh0",
              "author": "Royal-Environment-18",
              "text": "Thanks buddy, kudos to you!\nHow did you manage  career gap on resume and in interview. Have you mentioned your experience or tried as a fresher? If experience was mentioned then how you handled it as you said you did nothing as SWE?",
              "score": 1,
              "created_utc": "2026-02-28 06:03:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rcykas",
      "title": "MCP that blocks prompt injection attacks locally",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rcykas/mcp_that_blocks_prompt_injection_attacks_locally/",
      "author": "AssumptionNew9900",
      "created_utc": "2026-02-24 00:03:40",
      "score": 12,
      "num_comments": 8,
      "upvote_ratio": 0.92,
      "text": "Guys guys guys‚Ä¶i really got tired of burning API credits on prompt injections, so I built an open-source local MCP firewall.. because i want my openclaw to be secure. I run 2 instances.. one on vps and one mac mini.. so i wanted something (not gonna pay) thing so all the prompts are validated before it reaches to openclaw.. so i build a small utility tool..\n\nBeen deep in MCP development lately, mostly through Claude Desktop, and kept running into the same frustrating problem: when an injection attack hits your app, you are going to be the the one eating the API costs for the model to process it.\n\nIf you are working with agentic workflows or heavy tool-calling loops, prompt injections stop being theoretical pretty fast.\n\nActually i have seen them trigger unintended tool actions and leak context before you even have a chance to catch it.\n\nThe idea of just trusting cloud providers to handle filtering and paying them per token (meehhh) for the privilege so it really started feeling really backwards to me.\n\nSo I built a local middleware that acts as a firewall. It‚Äôs called Shield-MCP and it‚Äôs up on GitHub. aniketkarne/PromptInjectionShield : [https://github.com/aniketkarne/PromptInjectionShield/](https://github.com/aniketkarne/PromptInjectionShield/)\n\nIt sits directly between your UI or backend etc and the LLM API, inspecting every prompt locally before anything touches the network.\n\nI structured the detection around a ‚ÄúCute Swiss Cheese‚Äù model making it on a layering multiple filters so if something slips past one, the next one catches it.\n\nBecause everything runs locally, two things happen that I actually care about:\n\n1. Sensitive prompts never leave your machine during the inspection step\n2. Malicious requests get blocked before they ever rack up API usage\n\nDecided to open source the whole thing since I figured others are probably dealing with the same headache",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1rcykas/mcp_that_blocks_prompt_injection_attacks_locally/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o71uihx",
          "author": "johndoerayme1",
          "text": "Curious how this fits into MCP architecture. It's a tool? How do tools act as middleware?",
          "score": 2,
          "created_utc": "2026-02-24 00:31:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71vecx",
              "author": "AssumptionNew9900",
              "text": "Well, Technically, yes, this server just exposes a single tool called analyze_prompt.\nIf you plug it straight into Claude Desktop, it acts like a standard tool (meaning the LLM has to decide to call it to check the prompt, which means you already sent the text over the wire).\nBut where it actually acts as \"middleware\" is when your building custom apps, RAG pipelines, or using Langchain. Because MCP is just a standard protocol, you can configure your client app to call the Shield MCP server locally first.\nSo the flow basically looks like this:\nUser Input -> Your App -> (local MCP call) Shield analyze_prompt -> [If safe] -> Cloud LLM API.\nYour orchestration layer just uses the tool output (is_injection: true/false) as a gatekeeper to kill the request before you spend any API tokens. Hope that makes sense!",
              "score": 2,
              "created_utc": "2026-02-24 00:36:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o72shen",
                  "author": "Sungog1",
                  "text": "Got it, so it acts as a gatekeeper for the API calls by checking prompts locally first. That‚Äôs a smart way to ensure you‚Äôre not wasting credits on bad inputs. Makes sense for custom setups, especially in RAG pipelines!",
                  "score": 1,
                  "created_utc": "2026-02-24 03:51:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o71y4h3",
          "author": "Andres10976",
          "text": "That's called a safeguard and Lllama Prompt Guard is already good at it. If you want you can add it to a hook with a custom implementation.",
          "score": 1,
          "created_utc": "2026-02-24 00:51:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71yw1q",
              "author": "AssumptionNew9900",
              "text": "Yeah 100% fair. Prompt Guard is super solid and definitely fits the \"safeguard\" bill perfectly.\nThe main reason I built Shield-MCP wasn't to compete with the underlying ML models (I'm actually just pulling ProtectAI's DeBERTa model for the ML tier anyway). I mostly built it to solve the integration friction.\nWriting custom hooks, building the API, and wrapping an ML model in a microservice takes time. Since a lot of agent frameworks are adopting MCP natively right now, I wanted something that just speaks the protocol out of the box. If your stack already uses MCP, you don't need to write a custom implementation or manage a separate service‚Äîyou just drop it in.\nPlus, I prefer the \"swiss cheese\" tiered approach (fast regex heuristics -> ML model -> entropy checks) rather than throwing every single prompt at a heavy ML model right away.\nBut yeah, if your already running custom hooks and infrastructure, Prompt Guard is a great way to go. Have you implemented it in your own stack yet?",
              "score": 1,
              "created_utc": "2026-02-24 00:56:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o74mo9h",
          "author": "FishIndividual2208",
          "text": "But why? There are tiny modells available to do prompt injection checks..",
          "score": 1,
          "created_utc": "2026-02-24 12:56:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78zfag",
          "author": "Illustrious_Slip331",
          "text": "Local input filtering is solid for cost control, but for agents with write-access (especially financial tools), I wouldn't trust it as the only line of defense. I've seen models hallucinate valid-looking tool calls without any malicious injection, simply because the context got messy or the temperature was slightly off. The \"Swiss Cheese\" model needs a final hard slice at the execution layer ‚Äî deterministic checks like idempotency keys or velocity limits that ignore the LLM's reasoning entirely. Does your middleware allow for defining post-generation checks on the tool arguments themselves, or is it purely prompt-side?",
          "score": 1,
          "created_utc": "2026-02-25 01:43:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rccvho",
      "title": "I built an Agentic OS using LangGraph & MCP (Looking for contributors!)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rccvho/i_built_an_agentic_os_using_langgraph_mcp_looking/",
      "author": "Top_Conversation7452",
      "created_utc": "2026-02-23 09:23:15",
      "score": 11,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nOver the last few months, I've been building an open-source, Multi-Agent operating system. It is fully local, uses a distributed MCP (Model Context Protocol) architecture, and hooks deeply into Google Workspace.\n\n**The Tech Stack:**\n\n* **Orchestration:** LangGraph (using a strict \"One-Way Turnstile\" routing pattern so the LLM doesn't drown in 50+ tool schemas).\n* **Memory:** Episodic RAG + a KuzuDB Knowledge Graph.\n* **Tools:** Multi-Server MCP handling Gmail, Calendar, Drive, Docs, Sheets, and a Docker code execution sandbox.\n* **UI:** Chainlit for real-time text and continuous voice listening (Whisper STT / Piper TTS).\n\nI built this to solve the context-bloat and tool-hallucination problems I kept seeing in monolithic agent designs.\n\n  \n**Why I'm posting here:** Right now it is very much an basic prototype. The architecture works beautifully, but it needs hardening and testing .\n\nI just made the repo public and created a few `[help wanted]` issues if anyone is interested in collaborating on Agentic AI patterns:\n\n1. **Safety:** Implementing a Human-in-the-Loop (HITL) interrupt in LangGraph before the agent executes dangerous Python code.\n2. **Context Management:** Building payload pointers/pagination for when the Google Sheets tool tries to read a massive CSV and blows up the token limit.\n3. **Testing:** Adding `pytest` coverage for the MCP tool schemas.\n\nRaise any issue u find and contribute\n\n**Repo link:** [https://github.com/Yadeesht/Agentic-AI-EXP](https://github.com/Yadeesht/Agentic-AI-EXP)\n\nWould love any brutal feedback on the system foundation. thanks for spending time in reading this post",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1rccvho/i_built_an_agentic_os_using_langgraph_mcp_looking/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o6xhpu4",
          "author": "Friendly-Ask6895",
          "text": "the one-way turnstile routing pattern is smart, we hit that exact same wall with tool schema overload. once you get past \\~20 tools the model just starts picking wrong ones or hallucinating parameters.\n\n  \nre: the HITL interrupt for code execution, we ended up going with a tiered approach. read-only operations run automatically, anything that writes or deletes goes through approval, and anything touching external APIs gets a full preview of what's about to happen. took some iteration but it cut our false-positive interrupts by like 80%.\n\n  \nhow are you handling the knowledge graph updates? like does the agent decide what goes into KuzuDB or is there a separate indexing pipeline?",
          "score": 3,
          "created_utc": "2026-02-23 10:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xzfyf",
              "author": "Top_Conversation7452",
              "text": "the 20+ tool wall is so real. I was losing my mind trying to pack 57 different content tools into a single agent. The model just starts grabbing the completely wrong tool. Breaking it down into a strict supervisor with a hierarchical routing structure was the only way to get it stable.\n\ni have been testing the oss-20b model across a few free-tier providers and the hosting setup makes a massive difference. openrouters endpoint has been giving me the best results by far. groq version of the oss-20b model seems highly overfitted for tool use it just aggressively spams tool calls even when the prompt doesn't require one.\n\nHuge kudos for that tiered HITL idea, by the way. \n\nknowledge grpah its handled by a dedicated memory update node  in the graph. I just trigger at EOD (mostly because I didn't have a frontend built out yet), but it‚Äôs also exposed as a tool so the supervisor can trigger an update dynamically on request.\n\nThe actual indexing pipeline runs in two layers: \n\n1. the extraction part the LLM scans the recent context and generates the node id, keywords, descriptions, and edge relationships. \n\n2. validation by semantic:  before blindly inserting the system runs a similarity search against existing entities in KuzuDB. If it finds potential matches, it feeds those back the LLM alongside the new data. The LLM then validates whether to merge, update the existing node, or discard the new extraction entirely.\n\nIt adds an extra inference step, but it keeps the graph super clean and stops it from creating duplicate nodes for the same entity.",
              "score": 2,
              "created_utc": "2026-02-23 12:54:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o749wwc",
          "author": "tech2biz",
          "text": "Great architecture direction, like! One thing wedid: explicit per-step runtime budgets (model, retries, tool timeouts), otherwise orchestration-heavy systems drift in cost quickly",
          "score": 2,
          "created_utc": "2026-02-24 11:24:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74ekth",
              "author": "Top_Conversation7452",
              "text": "thanks for the tip, i truly missed that i dont have any budget or timeout setup right now. did you track your token budgets directly inside your graph state or handle it somewhere else?",
              "score": 1,
              "created_utc": "2026-02-24 12:00:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o74gecc",
                  "author": "tech2biz",
                  "text": "We dynamically cascade through models based on classifier and quality evals. Open sourced it: cascadeflow on github. So you can manage (and reduce) token costs.",
                  "score": 2,
                  "created_utc": "2026-02-24 12:14:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rc7zst",
      "title": "How are you persisting agent work products across sessions? (research docs, reports, decisions)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rc7zst/how_are_you_persisting_agent_work_products_across/",
      "author": "syumpx",
      "created_utc": "2026-02-23 04:40:52",
      "score": 11,
      "num_comments": 19,
      "upvote_ratio": 0.92,
      "text": "I've been building agents with LangGraph for a few months now (research agents that monitor Reddit/TikTok, draft reports, send Slack messages) and the thing that keeps biting me is what happens between sessions.\n\n\n\nLangGraph checkpointers handle in-graph state fine. But the actual artifacts agents produce, a 2-page research report, a campaign brief with competitor analysis, a list of sourced Reddit threads, that stuff just disappears. Next session the agent starts from zero. I end up manually pasting previous outputs into the system prompt which feels completely wrong.\n\n\n\nThe approach I kept coming back to was giving agents a shared file store where they write their work as versioned files (markdown with YAML frontmatter for metadata). One agent writes research/competitor-pricing.md with status: draft, next session another agent picks it up, reads it, updates it. Every write is a new version so nothing gets overwritten.\n\n\n\nI open sourced this as [https://github.com/pixell-global/sayou](https://github.com/pixell-global/sayou) if anyone wants to look at the approach. But I'm more interested in how others are handling this:\n\n\n\nAre you using LangGraph's persistent checkpointers for cross-session artifact storage, or only for in-graph state? Just dumping outputs to JSON/text files and re-loading them?\n\nUsing a vector DB for this? (I tried Pinecone but you can't version or diff anything stored as embeddings, which made it useless for docs that evolve over time.) Or just accepting that agents start fresh every session?\n\nThe more agents I build the more I think the real bottleneck isn't reasoning or tool use. It's that agents have nowhere to put their work.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1rc7zst/how_are_you_persisting_agent_work_products_across/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o6wj3qg",
          "author": "KalZaxSea",
          "text": "I use mongodb memory, it loads chat to mongob and reload when needed for another chat",
          "score": 1,
          "created_utc": "2026-02-23 05:10:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wjws0",
              "author": "syumpx",
              "text": "Interesting, so you're storing full chat histories and reloading them as context? How do you handle it when the conversation history¬†gets too long for the context window? Do you summarize, truncate, or selectively load specific messages?\n\nThe thing that kept bugging me with chat persistence was that conversations are a lossy format for knowledge. The useful info (a decision, a research finding, a config choice) is buried in 50 messages of back and forth. Curious if you've run into that or if raw chat reload works well enough for your use case.\n\n",
              "score": 1,
              "created_utc": "2026-02-23 05:16:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6wk5h8",
                  "author": "KalZaxSea",
                  "text": "I trush gemini flash long context and performance. when I reload I do not load tools calls only ai and human messages. also if there is a long context needed task I use subagents to get rid of long context and subagent retruns the core info to main agent",
                  "score": 1,
                  "created_utc": "2026-02-23 05:18:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6woffu",
          "author": "kikkoman23",
          "text": "Yea sounds like you‚Äôre just persisting your thread or conversation history. \n\nThis is different than langgraph check pointing which is needed for usage with langgraph and for the workflow to know when to continue to next node in the workflow. \n\nI overlooked this at first thinking checkpoint was the same as conversation history. But it is not. \n\nSo you‚Äôll need some type of persistence for the messages. And then just retrieve it when you‚Äôre navigating to different threads.",
          "score": 1,
          "created_utc": "2026-02-23 05:52:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wrcvu",
              "author": "syumpx",
              "text": "Yeah exactly, checkpointers track graph state, not the stuff agents actually produce. That distinction tripped me up too. ¬† ¬† ¬† ¬† ¬† ¬†\n\nBut even with message persistence, the useful output (a research doc, a decision, a config choice) is buried in 50 messages of back and forth. You end up burning context window tokens replaying a whole thread just to recover one artifact.¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n\n\n\nThat's what pushed me toward storing work products separately from conversations. The conversation is how the work happened. The¬†artifact is what matters next session.¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n\n\n\n¬† Are you loading full threads back in or doing some kind of selective filtering?",
              "score": 1,
              "created_utc": "2026-02-23 06:17:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xi05g",
          "author": "Friendly-Ask6895",
          "text": "we landed on something really similar to your versioned markdown approach tbh. the key insight for us was separating \"conversation state\" from \"work products\" completely. conversations are ephemeral, but the reports/analyses/decisions the agent produces should live in a proper file system with git-style versioning.\n\n  \nwhat made it click was giving the agent explicit \"save artifact\" and \"load artifact\" tools instead of trying to shove everything through memory. the agent writes a research doc to disk, tags it with metadata, and any future session can pull it back by topic or date. feels way more natural than trying to reconstruct work from chat history.",
          "score": 1,
          "created_utc": "2026-02-23 10:31:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xiiwa",
              "author": "syumpx",
              "text": "This is exactly what we built Sayou around. Separating conversation state from work products, with agents writing to a filesystem instead of memory. Versioning and metadata retrieval built in.\nSounds like you landed on a really similar approach. Would love to compare notes. Mind if I DM you to set up a quick call? I would love to see what you have built and what you think about sayou in general",
              "score": 1,
              "created_utc": "2026-02-23 10:36:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xx13i",
          "author": "Comfortable_Poem_866",
          "text": "The semantic search approach you dismissed with Pinecone makes more sense when the retrieval layer is local and the artifacts stay as plain files ‚Äî you get fuzzy cross-session recall without losing the ability to read, diff, or version the files directly.\n\nThat‚Äôs the approach I took with CtxVault ‚Äî agents write markdown files to a local vault via an API endpoint, content gets embedded and indexed immediately, and any future session can retrieve it semantically. The files are just files, so versioning is a separate concern you can handle however you want.\n\nDifferent tradeoff than yours ‚Äî no built-in versioning, but semantic retrieval across sessions without managing metadata manually. https://github.com/Filippo-Venturini/ctxvault  if useful.",
          "score": 1,
          "created_utc": "2026-02-23 12:37:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6y7w71",
              "author": "syumpx",
              "text": "Nice, CtxVault looks interesting. We made a different bet though. Instead of building a retrieval system for agents, Sayou just gives them a filesystem and lets them figure out what they need. The goal is to make agents truly autonomous. If an agent needs a retrieval system built by humans to find its own work, it's not really autonomous.\n\nWith context windows getting bigger and reasoning getting better, agents can navigate a simple filesystem on their own. They just need somewhere to put their work and pick it up later. That's it.\n\nSemantic retrieval is powerful but adds a layer of complexity that agents shouldn't need if the filesystem is well structured",
              "score": 1,
              "created_utc": "2026-02-23 13:46:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6yyr1k",
                  "author": "Comfortable_Poem_866",
                  "text": "Fair point on autonomy ‚Äî it‚Äôs a genuine philosophical difference. Filesystem navigation works well when the structure is predictable and the agent knows roughly what it‚Äôs looking for.\n\nThe case for semantic retrieval is when it doesn‚Äôt ‚Äî when the query and the stored content use different vocabulary, or when the agent needs to surface something it didn‚Äôt know it would need later. At that point you‚Äôre spending reasoning tokens on search instead of on the actual task.\n\nThe other angle for me is control ‚Äî a vault gives you a monitorable, isolated knowledge base. The agent is still autonomous in how it queries, but you have visibility and separation by design.\n\nSayou looks interesting though, the versioned artifact approach is a smart way to handle docs that evolve over time. Different bets on where the complexity should live.",
                  "score": 1,
                  "created_utc": "2026-02-23 16:04:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o71zdbb",
          "author": "inguz",
          "text": "The SAMB benchmark looks interesting.",
          "score": 1,
          "created_utc": "2026-02-24 00:58:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71zv05",
              "author": "syumpx",
              "text": "its quite simple benchmark. i would never dream of submitting this to neurips, but sayou had some interesting data point against other agent memories like mem0 and zep \n\nhttps://preview.redd.it/05hxl229dclg1.png?width=2160&format=png&auto=webp&s=e39808eddb396b5464ece79940cbdc9197b332aa\n\n",
              "score": 1,
              "created_utc": "2026-02-24 01:01:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o71zx2t",
                  "author": "syumpx",
                  "text": "https://preview.redd.it/nuvglfdbdclg1.png?width=2160&format=png&auto=webp&s=0d3c9bb23e7261676ecc2716056851e86103cb68\n\n",
                  "score": 1,
                  "created_utc": "2026-02-24 01:02:01",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7hqdld",
          "author": "Otherwise_Flan7339",
          "text": "Yeah, this is exactly the bottleneck we hit with our sales agent platform. Glad someone else is seeing it.  \n  \nWe started by just dumping outputs to Supabase (PostgreSQL JSONB columns) and reloading on each run. It became a mess fast. No versioning. No real way for one agent to pick up another's work reliably.  \n  \nTried Pinecone for some structured data related to sales leads, but for evolving reports or campaign briefs, it just doesn't work. Like you said, no diffs, no versioning.  \n  \nRight now, we're building an internal content store for agent artifacts. It's basically a Git-like system. Markdown files with YAML frontmatter. Stores it in S3. Every write is a new commit. It's not pretty, but it means our agents can finally build on each other's work.  \n  \nHow are you handling conflicts when multiple agents try to update the same document? That's our next big problem.",
          "score": 1,
          "created_utc": "2026-02-26 10:32:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hqx8m",
              "author": "syumpx",
              "text": "This is almost exactly the same path we went through. Supabase first, then realized structured DB doesn‚Äôt work for evolving docs. Your Git-like approach with markdown + YAML frontmatter on S3 is really close to what Sayou does under the hood.\nFor the conflict question, honestly still figuring out the best approach. Right now Sayou does last-write-wins which is fine for single agent workflows but breaks down with multiple agents hitting the same file. Been thinking about either file-level locking or branching per agent and merging after. Curious what patterns you‚Äôve tried since you‚Äôre clearly further along on the multi-agent side.\nWould love to compare setups. Mind if I DM you?‚Äù",
              "score": 1,
              "created_utc": "2026-02-26 10:37:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rclq3t",
      "title": "Looking to Join Serious LangChain / AI Backend Projects",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rclq3t/looking_to_join_serious_langchain_ai_backend/",
      "author": "arap_bii",
      "created_utc": "2026-02-23 16:15:30",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.82,
      "text": "Hi everyone, I‚Äôm Kevin, a backend-focused developer with deep experience in Python and production-grade systems. I‚Äôm looking to join serious AI/LLM projects to contribute technically and help build scalable solutions.\n\nI‚Äôm open to small equity or modest pay setups to get the project moving‚Äîmainly looking for impactful work and a strong team.\n\nIf you‚Äôre building something interesting with LangChain or other AI tooling and need someone to handle backend, pipeline, or AI integration work, drop me a message!",
      "is_original_content": false,
      "link_flair_text": "Announcement",
      "permalink": "https://reddit.com/r/LangChain/comments/1rclq3t/looking_to_join_serious_langchain_ai_backend/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rebe70",
      "title": "Built a four-layer RAG memory system for my AI agents (solving the context dilution problem)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rebe70/built_a_fourlayer_rag_memory_system_for_my_ai/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-25 12:04:01",
      "score": 10,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "We all know AI agents suffer from memory problems. Not the kind where they forget between sessions but something like context dilution. I kept running into this with my agents (it's very annoying tbh). Early in the conversation everything's sharp but after enough back and forth the model just stops paying attention to early context. It's buried so deep it might as well not exist.\n\nSo I started building a four-layer memory system that treats conversations as structured knowledge instead of just raw text. The idea is you extract what actually matters from a convo, store it in different layers depending on what it is, then retrieve selectively based on what the user is asking (when needed).\n\nDifferent questions need different layers. If someone asks for an exact quote you pull from verbatim. If they ask about preferences you grab facts and summaries. If they're asking about people or places you filter by entity metadata.\n\nI used workflows to handle the extraction automatically instead of writing a ton of custom parsing code. You just configure components for summarization, fact extraction, and entity recognition. It processes conversation chunks and spits out all four layers. Then I store them in separate ChromaDB collections.\n\nBuilt some tools so the agent can decide which layer to query based on the question. The whole point is retrieval becomes selective instead of just dumping the entire conversation history into every single prompt.\n\nTested it with a few conversations and it actually maintains continuity properly. Remembers stuff from early on, updates when you tell it something new that contradicts old info, doesn't make up facts you never mentioned.\n\nAnyway figured I'd share since context dilution seems like one of those problems everyone deals with but nobody really talks about.",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1rebe70/built_a_fourlayer_rag_memory_system_for_my_ai/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o7b9bwk",
          "author": "Independent-Cost-971",
          "text": "I wrote a whole blog about this that goes way deeper if anyone's interested:¬†[https://kudra.ai/building-production-grade-agent-memory-from-scratch-llm-context-cost-fix/](https://kudra.ai/building-production-grade-agent-memory-from-scratch-llm-context-cost-fix/)",
          "score": 2,
          "created_utc": "2026-02-25 12:04:29",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o7bcewi",
          "author": "Don_Ozwald",
          "text": "‚ÄúProduction grade‚Äù gets thrown around a lot. What concrete evidence do you have beyond a few test conversations? Any benchmarks on long-horizon coherence, contradiction handling, or retrieval precision under load?",
          "score": 2,
          "created_utc": "2026-02-25 12:26:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7blq03",
              "author": "Independent-Cost-971",
              "text": "That‚Äôs a fair question. The blog wasn‚Äôt claiming formal benchmarks or academic validation. It was showing an architectural approach to solving context dilution in long-running conversations.\n\nBy separating memory into structured layers and retrieving selectively instead of stuffing the entire history into every prompt, the system is designed to be measurable, testable, and scalable. You can evaluate retrieval precision, replay long conversations to test coherence, and load test the vector layer.\n\nSo ‚Äúproduction-grade‚Äù in this context means the architecture is built to be hardened and evaluated properly  not that it‚Äôs already backed by a published benchmark suite. Sorry for the confusion",
              "score": 1,
              "created_utc": "2026-02-25 13:24:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bnaep",
                  "author": "Don_Ozwald",
                  "text": "That makes sense, thanks for clarifying.\n\nI‚Äôd suggest avoiding ‚Äúproduction-grade‚Äù then, since most people read that as empirically validated under real workloads rather than architecturally ready.",
                  "score": 3,
                  "created_utc": "2026-02-25 13:33:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bgeee",
          "author": "SkyPL",
          "text": "I have a similar system for AI-driven transcripts manager.\n\nIt creates an index of transcripts with: Summary, Key facts, Risks&Debt, Action items, Open threads + Which objects (from a pre-defined list) were mentioned. And of course I store the full content of each transcript. Though for me it's only in the .md files, so more primitive.\n\nIt has all that LLM needs to find the specific document, or even prep output without even pulling the entire context.",
          "score": 1,
          "created_utc": "2026-02-25 12:52:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7blvuw",
              "author": "Independent-Cost-971",
              "text": "That‚Äôs a really solid approach.\n\nYou‚Äôre basically doing the same core thing: turning raw transcripts into structured, queryable memory instead of treating them as flat text blobs. By separating summary, facts, risks, action items, open threads, and tagged objects, you‚Äôre dramatically increasing retrieval precision while reducing how much context you actually need to load.\n\nThat‚Äôs exactly the shift agents need now\n\n",
              "score": 1,
              "created_utc": "2026-02-25 13:25:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7bkgho",
          "author": "Yashu_Sensei",
          "text": "Splitting memory into verbatim, facts, summaries, and entities makes way more sense. you‚Äôre basically reducing noise before retrieval even happens.   \ncurious how you‚Äôre handling contradictions though. If a user updates a preference, are you overwriting old facts or tracking recency somehow?  \nLoved the approach.  nobody talks about context dilution enough tbh.",
          "score": 1,
          "created_utc": "2026-02-25 13:17:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bm0x2",
              "author": "Independent-Cost-971",
              "text": "Really appreciate that üôè\n\nFor contradictions, I don‚Äôt just blindly append new facts. Facts are stored as structured entries with timestamps and lightweight metadata. When a new statement conflicts with an existing one (like a preference change), the system marks the old fact as superseded rather than deleting it outright. The latest version becomes the ‚Äúactive‚Äù fact for retrieval, but the history is still there for traceability if needed.\n\nSo it‚Äôs not simple overwriting, and it‚Äôs not uncontrolled accumulation either",
              "score": 3,
              "created_utc": "2026-02-25 13:26:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bpf85",
                  "author": "Yashu_Sensei",
                  "text": "Dyammm that‚Äôs actually interesting\n\nMarking stuff as superseded instead of deleting it is actually clean for keeping history without messing up retrieval.",
                  "score": 1,
                  "created_utc": "2026-02-25 13:45:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7hdodn",
                  "author": "cHekiBoy",
                  "text": "Yep, and you could also make a profile based on changes and see the progress of a ‚Äúprofile‚Äù connected to AI, an employee, or even yourself. Cool stuff. I‚Äôve thought about the problem and the solution too, but I never really started implementing it üôÇ",
                  "score": 1,
                  "created_utc": "2026-02-26 08:30:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rd7jja",
      "title": "I built a new MCP Server to stop agents from hallucinating medical math (has 54 calculators + 14 clinical guidelines)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rd7jja/i_built_a_new_mcp_server_to_stop_agents_from/",
      "author": "Magodo123",
      "created_utc": "2026-02-24 05:23:34",
      "score": 10,
      "num_comments": 7,
      "upvote_ratio": 0.92,
      "text": "Hey guys,  \n  \n  \nI've been building health agents lately and kept running into a scary problem: LLMs are terrible at medical math and following strict clinical guidelines. If you ask an agent to evaluate a patient's case, it will often boldly hallucinate a MELD score or agree with treatments that actually violate standard care.   \n  \n  \nTo fix this, I put together \\*\\*Open Medicine\\*\\*. It's an open-source Python library and an MCP Server.  \n  \n  \nInstead of letting the agent guess, you just give it these tools:  \n\\- \\`search\\_clinical\\_calculators\\`: Let the agent find the right formula (like Glasgow-Blatchford).  \n\\- \\`execute\\_clinical\\_calculator\\`: Runs the math in pure, tested Python. No LLM logic involved. It takes a JSON payload, validates it via Pydantic, and returns the exact score, interpretation, and the DOI of the original medical paper.  \n\\- \\`retrieve\\_guideline\\`: Lets the agent read version-controlled markdown text of actual clinical guidelines (like the 2023 AHA guidelines) instead of relying on its latent training data or searching PubMed and retrieving tons of irrelevant papers.  \n  \n  \nAs a quick example of why this matters: I gave an agent a clinical note for a GI Bleed where the doctor planned for \"aggressive fluid resuscitation.\" Without the tools, the LLM just agreed. But when connected to the open-medicine-mcp server, the agent pulled the actual NICE guidelines, realized it was a variceal bleed, and corrected the plan to a \"restrictive transfusion strategy\" because aggressive fluids increase portal pressure.   \n  \n  \nSource code is here: [https://github.com/RamosFBC/openmedicine](https://github.com/RamosFBC/openmedicine)  \n  \n  \nIt's all MIT licensed. I'd love to hear from other folks building in this space. Have you been using MCP servers for this kind of deterministic logic yet? What calculators or guidelines should I try to add next?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1rd7jja/i_built_a_new_mcp_server_to_stop_agents_from/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o739yy6",
          "author": "invertednz",
          "text": "Up voting, thanks for doing this, not something I'll use but amazing work.",
          "score": 1,
          "created_utc": "2026-02-24 05:58:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73ayvg",
              "author": "Magodo123",
              "text": "Thanks! Really helpful for healthcare use cases, didn‚Äôt find anything that would do these tasks",
              "score": 1,
              "created_utc": "2026-02-24 06:06:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73f49b",
          "author": "Reasonable_Event1494",
          "text": "Great work. So, it is using RAG and tool calling both to make the outputs reliable. So, are you giving it memory also small context memory?",
          "score": 1,
          "created_utc": "2026-02-24 06:41:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o73gcm4",
              "author": "Magodo123",
              "text": "This repo is actually only the tool layer. The tool definitions inside the MCP server instruct the model on how to  call the tool searcher and find the necessary calculator tool or medical guideline. From there, the context retrieved by the tools guides the model through retrieving the most accurate information. For now it only makes regex match to find tools but I plan to implement semantic search in the near future to allow more complex queries. The big advantage here is that these tools allow the model to replicate accurately how doctors actually make real decisions, which are made by reproducing evidences from numerous researches based on these clinical scores",
              "score": 1,
              "created_utc": "2026-02-24 06:51:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73wb4t",
          "author": "jovansstupidaccount",
          "text": "Sounds so cool",
          "score": 1,
          "created_utc": "2026-02-24 09:20:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73wbzx",
          "author": "makinggrace",
          "text": "Super important work. Anyone that works with AI needs to help educate laypeople about math fail. It's a good gateway conversation into how LLM works and limitations. But actual use cases -- and solutions? Better than anything else.",
          "score": 1,
          "created_utc": "2026-02-24 09:20:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75e5yq",
          "author": "penguinzb1",
          "text": "the GI bleed example is the clearest illustration of the problem. you'd never catch that gap from unit tests or benchmarks. the agent agreeing with incorrect treatment is a behavioral failure that only surfaces when you run it through the actual clinical scenarios it'll face. the right answer varies by context (variceal vs non-variceal) in ways that look fine in general testing but fail on the specific input patterns that matter. this is why domain-specific simulation before deployment matters so much more in medical than anywhere else.",
          "score": 1,
          "created_utc": "2026-02-24 15:22:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1reitqc",
      "title": "Shannon entropy catches credential leaks between agents better than pattern matching. Here's why.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1reitqc/shannon_entropy_catches_credential_leaks_between/",
      "author": "Sharp_Branch_1489",
      "created_utc": "2026-02-25 16:59:19",
      "score": 9,
      "num_comments": 1,
      "upvote_ratio": 0.91,
      "text": "Pattern matching for credentials works until it doesn't. You write a regex for `AKIA[0-9A-Z]{16}` and catch AWS keys. Then you miss the credential that doesn't fit your pattern.\n\nShannon entropy doesn't care what the credential looks like.\n\nNormal English prose sits between 3.2‚Äì3.8 bits per character. An AWS secret key, a JWT, a private token all sit above 4.5. The statistical signature is different regardless of format.\n\nSo instead of asking \"does this match a known credential pattern\" you ask \"does this string have the entropy profile of a secret.\" Catches things you never wrote a pattern for.\n\nThe catch you tune the threshold carefully or you'll flag base64 encoded content as credentials. Set it too low and everything fires. Set it too high and real leaks slip through.\n\nI ran both approaches against real inter-agent messages. Entropy caught 3 leaks pattern matching missed entirely.\n\nFull breakdown of what I tested and how I tuned it:  \n[https://open.substack.com/pub/mohithkarthikeya/p/i-planted-secret-traps-inside-my?utm\\_campaign=post-expanded-share&utm\\_medium=post%20viewer](https://open.substack.com/pub/mohithkarthikeya/p/i-planted-secret-traps-inside-my?utm_campaign=post-expanded-share&utm_medium=post%20viewer)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1reitqc/shannon_entropy_catches_credential_leaks_between/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rg7wie",
      "title": "Assembly for tool calls orchestration with Langchain",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rg7wie/assembly_for_tool_calls_orchestration_with/",
      "author": "oleg_ivye",
      "created_utc": "2026-02-27 14:18:24",
      "score": 9,
      "num_comments": 3,
      "upvote_ratio": 0.92,
      "text": "Hi everyone,\n\n  \nI'm working on LLAssembly [https://github.com/electronick1/LLAssembly](https://github.com/electronick1/LLAssembly) and would appreciate some feedback.\n\nLLAssembly is a tool-orchestration library for LLM agents that replaces the usual ‚ÄúLLM picks the next tool every step‚Äù loop with a single up-front execution plan written in assembly-like language (with jumps, loops, conditionals, and state). \n\nThe model produces execution plan once, then emulator runs it converting each assembly instruction to LangGraph nodes, calling tools, and handling branching based on the tool results ‚Äî so you can handle complex control flow without dozens of LLM round trips. It currently supports LangChain and LangGraph, and it shines in fast-changing environments like game NPC control, robotics/sensors, code assistants, and workflow automation.¬†\n\n  \n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1rg7wie/assembly_for_tool_calls_orchestration_with/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o7pe4wv",
          "author": "Otherwise_Wave9374",
          "text": "This is a really interesting direction. The \"LLM decides next tool every step\" loop is super flexible, but the latency and cost add up fast, and debugging is painful. Having the model emit a single up-front plan (with jumps/conditionals) feels closer to how youd want agents to run in production, like deterministic control flow with LLM only where it adds value.\n\nHow do you handle plan validity when tool outputs differ from what the model assumed, do you allow partial re-planning or just branch within the assembly?\n\nIf youre into agent orchestration patterns, Ive been collecting examples and notes here too: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-02-27 14:23:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pfswi",
              "author": "oleg_ivye",
              "text": "Thanks, so far re-planning is not implemented, and when assembly plan is generated LLAssembly buits LangGraph with all possible conditional jumps based on the assembly jumps between instructions. In the Example section [https://github.com/electronick1/LLAssembly?tab=readme-ov-file#examples](https://github.com/electronick1/LLAssembly?tab=readme-ov-file#examples) you can find diagram of the LangGraph graph schema generated from the assembly instructions to demostrate that.",
              "score": 1,
              "created_utc": "2026-02-27 14:32:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7pycv5",
          "author": "ar_tyom2000",
          "text": "Great to see discussions around orchestrating tool calls in LangChain! It's a crucial aspect for efficient agent workflows. I built [LangGraphics](https://github.com/proactive-agent/langgraphics) to help visualize these processes in real-time, showing exactly what your agent is doing at each step. It integrates seamlessly without requiring refactoring, which can streamline your debugging efforts.",
          "score": 1,
          "created_utc": "2026-02-27 16:03:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhlb4g",
      "title": "Preventing SQL agents from hallucinating columns and destructive queries",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rhlb4g/preventing_sql_agents_from_hallucinating_columns/",
      "author": "K3shxx",
      "created_utc": "2026-03-01 02:24:45",
      "score": 9,
      "num_comments": 13,
      "upvote_ratio": 0.91,
      "text": "While trying to build a ‚Äúchat with your database‚Äù LangChain agent, I realized the hard part wasn‚Äôt generating SQL ‚Äî it was trusting it.\n\nThe model could write queries, but I kept hitting issues:  \n‚Ä¢ hallucinated column names  \n‚Ä¢ incorrect joins  \n‚Ä¢ answers based on non-existent data  \n‚Ä¢ and once it even produced a DELETE statement\n\nThe scary part wasn‚Äôt wrong answers ‚Äî it was the idea of letting an LLM execute queries on a real DB.\n\nSo I ended up putting a guarded layer between the LLM and Postgres:\n\n* automatically reads the schema\n* constrains generation to real tables/columns\n* checks queries before execution\n* blocks destructive statements\n* executes read-only and answers only from returned rows\n\nAfter that the agent became much more predictable and I could finally run it against a real database without worrying about it nuking tables.\n\nI eventually cleaned up the setup into a small starter kit for those who want to experiment with AI-DB use cases, but I‚Äôm more curious about others‚Äô experiences here.\n\nFor those who‚Äôve built SQL agents ‚Äî what part has been the most painful for you?\n\nSchema grounding? Query correctness? Or execution safety?\n\nIf you want, give me a natural-language database question and I‚Äôll run it and show the SQL it generates.\n\nhttps://preview.redd.it/tr4ohnllgcmg1.png?width=1913&format=png&auto=webp&s=aa5494cbe8e5acf697abfc5412dff959efb6bc5a\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1rhlb4g/preventing_sql_agents_from_hallucinating_columns/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o7zkxj6",
          "author": "Otherwise_Wave9374",
          "text": "Yep, SQL generation is the easy part, execution safety is the real boss fight.\n\nWhat you described (schema grounding + validation + read-only enforcement) is basically the minimum viable safety layer for DB agents. Have you tried adding an intermediate step that forces the model to output a query plan in plain English first, then generate SQL, then verify columns against the schema?\n\nAlso, Ive seen a couple good agent patterns around tool constraints and validators here https://www.agentixlabs.com/blog/ that map pretty closely to what youre building.",
          "score": 5,
          "created_utc": "2026-03-01 02:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7zxvuo",
          "author": "RepulsiveCry8412",
          "text": "Which model are you using?\nDo you really need agent for sql generation?\nI am currently working on Athena sql generation, \nI use gemini 2.5 flash lite to generate sql, pass schema and business rules in prompt ( plan to make this a rag retrieval). \n\nGenerated sql is sent to a python code to run on db.",
          "score": 2,
          "created_utc": "2026-03-01 04:06:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o81drs7",
              "author": "K3shxx",
              "text": "That‚Äôs actually exactly how I started !  ‚Äî an LLM ‚Üí SQL ‚Üí execute pipeline. I too am using Gemini Models (2.5 flash). This is a very solid way to build LLM-SQL pipelines.\n\nIt works decent for generating SQL queries. But I feel the SQL generation is not the problem here , it is the reliability. \n\nI wonder if a single pass would suffice for the model to provide appropriate queries, especially when the size of the DB is huge.  A traditional LLM -> SQL  pipeline assumes the query the model writes is correct , safe to run and result actually answers the user‚Äôs intent.\n\nThe ‚Äúagent‚Äù part for me is adding a decision step between generation and execution. The model proposes an action (a query), but the system verifies it, can reject it, and force a regeneration before anything touches the database. So the LLM becomes an untrusted query writer rather than an executor. This is not a typical \"React Style Agent\" that Langchain offers. But the concept remains the same. Multiple workflows like Evaluator Optimizers (as in Corrective RAG) can be setup which would boost the reliability .\n\nWould love to hear your thoughts on this!",
              "score": 2,
              "created_utc": "2026-03-01 11:40:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o81htei",
                  "author": "RepulsiveCry8412",
                  "text": "Yes i also want to build an llm as a judge step but not sure if i should use flash as judge too, adding similar model will increase cost. Flash also needs decent amout of thought tokens to generate good sql, for me its 500. Size of db should be irrelevant, we can prompt model to write optimized sql.\nI am building a chat application where user can ask to refine queries if not satisfied and also add human in the loop step if user is not satisfied say after 10 turns or 10k tokens whichever is higher. I am newbie so not sure if any better technique are present. You can also try adding skills for your agent.",
                  "score": 1,
                  "created_utc": "2026-03-01 12:15:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o84ku9e",
              "author": "pls_fix_me51",
              "text": "Hey if your using it solely for Athena you can check out AWS data processing MCP server it takes care most of the things.",
              "score": 1,
              "created_utc": "2026-03-01 21:59:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o85r1zs",
                  "author": "RepulsiveCry8412",
                  "text": "Thanks , it looks good ona paper but I'm concerned about costs, amazon q , claude desktop and for specific cases need to use bedrock agent core, will check though.",
                  "score": 1,
                  "created_utc": "2026-03-02 02:05:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o80fe0r",
          "author": "rock_db_saanu",
          "text": "Use vanna ai it does complete job for you",
          "score": 4,
          "created_utc": "2026-03-01 06:18:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o82ho3e",
              "author": "_y0y0_",
              "text": "+1",
              "score": 1,
              "created_utc": "2026-03-01 15:51:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o81qi7b",
          "author": "TheRealIsaacNewton",
          "text": "It's too dangerous",
          "score": 1,
          "created_utc": "2026-03-01 13:19:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o841byq",
          "author": "xCheetaZx",
          "text": "I‚Äôm working on this now actually! I have a Postgres database and I‚Äôm thinking of not having an LLM generate any of my SQL queries. To be specific, I‚Äôm building an app that recommends PC parts for builds based on user specifications, and I need hard guarantees on comparability between things like the CPU and motherboard. My plan is to use LangGraph and have a node for each part.",
          "score": 1,
          "created_utc": "2026-03-01 20:19:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o85wimm",
              "author": "K3shxx",
              "text": "Awesome ! And yeah it makes total sense , having a langgraph flow would be much better since you would be dealling with deterministic output. LLM hallucination would lead to improper part recommendations and might cause problems.\n\n  \nHow are u handling the queries tho? Are you mapping each part to a seperate node and give the output to an LLM to get NL output? \n\nCurious how you‚Äôre passing state between nodes and deciding which tool to call.\n\n",
              "score": 1,
              "created_utc": "2026-03-02 02:39:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o8542sx",
          "author": "deputystaggz",
          "text": "Yes have seen all of the problems you described here and have built a tool to do the complete job. \n\nIt‚Äôs called Inconvo and it‚Äôs open source if you want to check out how we built it.",
          "score": 1,
          "created_utc": "2026-03-01 23:47:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o85v8gu",
              "author": "K3shxx",
              "text": "Interesting! Did you end up solving the reliability side mostly through validation before execution or by improving the prompt/retrieval side? Curious how you handled that part.",
              "score": 1,
              "created_utc": "2026-03-02 02:31:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rh2cvq",
      "title": "Evaluating LangChain agents beyond final output",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1rh2cvq/evaluating_langchain_agents_beyond_final_output/",
      "author": "Fluffy_Salary_5984",
      "created_utc": "2026-02-28 13:14:33",
      "score": 8,
      "num_comments": 17,
      "upvote_ratio": 0.84,
      "text": "I‚Äôve been running a lot of experiments with agents built on LangChain recently. Getting them to *work* wasn‚Äôt the hardest part. Getting them to behave consistently is.\n\n\n\nOnce you combine:\n\n* tool calling\n* retries\n* multi-step reasoning\n* branching logic\n* memory/state\n\nthe system becomes less ‚Äúa prompt‚Äù and more ‚Äúa distributed workflow‚Äù.\n\nAnd evaluating that workflow is surprisingly tricky.\n\n\n\nTwo runs with the same input can:\n\n* take different tool paths\n* retry at different steps\n* recover from errors differently\n* reach the same final answer via completely different trajectories\n\nIf the final answer is correct, is that enough? Or should we care about *how* it got there?\n\nWhat I‚Äôve noticed is that many failures aren‚Äôt LLM failures.\n\nThey‚Äôre orchestration failures.\n\n* retry policies that amplify small errors\n* tool outputs that slightly mismatch expected schemas\n* state drifting over multiple steps\n* subtle branching differences that compound\n\nFrom the outside, the agent ‚Äúworks‚Äù. Internally, it‚Äôs unstable.\n\n\n\nI‚Äôve started treating agent evaluation more like system observability:\n\n* snapshotting full execution traces\n* comparing repeated runs\n* looking at divergence points\n* tracking stability across multiple executions\n\nNot just ‚Äúdid it answer correctly?‚Äù But ‚Äúdoes it behave consistently under repetition?‚Äù\n\n\n\nFor those building with LangChain (or LangGraph):\n\n* Are you evaluating trajectories, or just outputs?\n* Do you test multi-run stability?\n* How do you detect silent orchestration failures?\n* Are you using built-in tracing only, or something beyond that?\n\nCurious how others here are thinking about reliability at the workflow level.",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1rh2cvq/evaluating_langchain_agents_beyond_final_output/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o7vjugc",
          "author": "BeerBatteredHemroids",
          "text": "You're trying to achieve deterministic behavior from inherently non-deterministic models. If you're trying to force repeatable, deterministic behavior that never deviates, you are trying in vain. These are LLMs, they are inherently random. No matter how much you prompt, adjust temp, adjust top-k, orchestrate. There will always be deviation from run to run. If you need rigid adherence to a workflow, LLMs ain't it.",
          "score": 6,
          "created_utc": "2026-02-28 13:29:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vtgl1",
              "author": "Fluffy_Salary_5984",
              "text": "Fair point!! ‚Äî I‚Äôm not trying to make runs¬†perfectly repeatable. That‚Äôs¬†not the goal. What I‚Äôm¬†after is¬†seeing where¬†they start to diverge: same prompt, multiple runs, snapshot the full trajectory¬†and compare.¬†Then you at least¬†know¬†when¬†something went off the rails (e.g. step¬†4 vs step 8), even if the final¬†output looks fine. So less ‚Äúforce determinism‚Äù and more¬†‚Äúobserve stability.‚Äù  \nWe ended up building a small internal¬†thing for this¬†‚Äî snapshots, replay¬†comparison, highlighting where behavior splits across runs. Nothing fancy,¬†but it changed how we think¬†about agent quality.  \nAnd yeah, if you¬†truly need rigid¬†adherence to a workflow, LLMs might not be the¬†right fit. For us¬†it‚Äôs more about ‚Äúhow unstable¬†is it?‚Äù rather than ‚Äúis¬†it deterministic?",
              "score": 1,
              "created_utc": "2026-02-28 14:26:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7wo7io",
                  "author": "BeerBatteredHemroids",
                  "text": "If you're just trying to observe, mlflow and langgraph implemented with a postgres store and checkpoints is what you want. Each node will register a checkpoint into the store where you can observe state and see how it changes from node to node. With Mlflow providing traceability during development.",
                  "score": 3,
                  "created_utc": "2026-02-28 17:05:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7vk3fw",
          "author": "noip1979",
          "text": "RemindMe! 7 days",
          "score": 2,
          "created_utc": "2026-02-28 13:31:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vk8yk",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 7 days on [**2026-03-07 13:31:13 UTC**](http://www.wolframalpha.com/input/?i=2026-03-07%2013:31:13%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LangChain/comments/1rh2cvq/evaluating_langchain_agents_beyond_final_output/o7vk3fw/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLangChain%2Fcomments%2F1rh2cvq%2Fevaluating_langchain_agents_beyond_final_output%2Fo7vk3fw%2F%5D%0A%0ARemindMe%21%202026-03-07%2013%3A31%3A13%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201rh2cvq)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-28 13:32:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vvmnf",
          "author": "ar_tyom2000",
          "text": "[LangGraphics](https://github.com/proactive-agent/langgraphics) addresses this by providing real-time visualization of agent workflows, helping to clarify not just what the final output is, but how the agent arrived at that decision. It integrates with your existing setup seamlessly - just a single line to wrap your graph.",
          "score": 2,
          "created_utc": "2026-02-28 14:39:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vwxon",
              "author": "Fluffy_Salary_5984",
              "text": "Real-time visualization is definitely great for debugging a single run. We approached it differently: we capture runs as snapshots (from live traffic) and then replay them offline for candidate configurations‚Äîrunning multiple times with the same input (e.g., 3‚Äì5 times)‚Äîso that for each case we get PASS/FAIL/FLAKY and pre-deployment gate judgments (failure rate, flakiness). Therefore, we focus more on 'Did this change break something, and is it stable across multiple runs?' rather than 'seeing it in real-time.' LangGraphics seems useful from a real-time perspective, so we are focusing more on pre-deployment checks rather than on visualization. Thanks for good suggestion!!",
              "score": 2,
              "created_utc": "2026-02-28 14:46:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vzjef",
          "author": "Toucanz17",
          "text": "A few options for more determinism in your workflow could be some caching options. You‚Äôll never achieve determinism with just the LLM, they aren‚Äôt made for that. But you could have a semantic caching layer that gets hit first before your agent takes any action.\n\nThis would essentially check to see if that query has been done before and pull the output from the last time it was run stored in a db instead of through the model. You‚Äôd have to have a pretty high threshold for returning a hit but it would help be more deterministic and reduce costs. The real tough part is ensuring only ‚Äúgood‚Äù items persist in your database and tweaking the logic so you aren‚Äôt pulling stale outputs as time progresses.",
          "score": 2,
          "created_utc": "2026-02-28 15:00:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7w0pyc",
              "author": "Fluffy_Salary_5984",
              "text": "That method could work too. Caching is certainly excellent in terms of runtime determinism and cost. For now, we take a different approach: before deploying changes, we rerun snapshots on the candidates, execute the inputs multiple times, and obtain pass/fail/unstable + gate judgments. Therefore, the cache is reliable at runtime; our question is 'Is it safe to deploy this change?' The method you mentioned might also be useful to refer to later.",
              "score": 1,
              "created_utc": "2026-02-28 15:07:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7w6r31",
          "author": "AsianHodlerGuy",
          "text": "Would creating agent skills for each workflow? I‚Äôm running into this issue too",
          "score": 2,
          "created_utc": "2026-02-28 15:38:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7wamrx",
              "author": "Fluffy_Salary_5984",
              "text": "Same situation!! it's tough. We didn't go with the 'workflow-specific skills' approach; instead, we captured execution as snapshots (worst/golden in production), then replayed them against a candidate (new model, prompt) and ran the same inputs a few times to check pass/fail/flaky. That way we know if a change is stable before deployment. That's the approach we're building right now!",
              "score": 1,
              "created_utc": "2026-02-28 15:57:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7y0s9e",
          "author": "thecanonicalmg",
          "text": "This is exactly the gap I keep running into too. Two runs that both get the right answer but one took a clean path and the other retried three times and called tools in a weird order, and you have no idea which pattern is going to blow up in production. The orchestration failures are the hardest to catch because they look like success until they do not. Moltwire helped me a lot with this since it watches the actual tool call sequences and behavioral patterns at runtime so you can spot those trajectory anomalies without building custom eval harnesses for every workflow.",
          "score": 2,
          "created_utc": "2026-02-28 21:15:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7zjjl8",
              "author": "Fluffy_Salary_5984",
              "text": "Yes!!! ‚Äî \"looks like success until it does not\" is exactly the pain. We ended up doing snapshot + replay + repeat runs (pass/fail/flaky) so we see before deploy which changes are stable; orchestration failures show up as policy violations or flaky runs when we replay. Will check out Moltwire for the runtime side; thanks for the pointer!!",
              "score": 1,
              "created_utc": "2026-03-01 02:33:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7vtuet",
          "author": "motorsportlife",
          "text": "How do you follow along with the langchain agent to evaluate?",
          "score": 1,
          "created_utc": "2026-02-28 14:29:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vvr6f",
              "author": "Fluffy_Salary_5984",
              "text": "We don't follow along in real time we do it offline, one agent at a time. We capture production traffic as snapshots (full request/response per call), and for each agent we keep a small set: worst cases (recent failures) and golden cases (recent passes) from the last 7 days. When we want to evaluate a change new model, new system prompt, etc. we replay that agent's snapshot set against the candidate config. We can run the same input multiple times (e.g. 3 or 5 runs), which gives us PASS / FAIL / FLAKY per case (FLAKY = some runs pass, some fail, so we see stability). Evaluation is policy-based: we check things like tool usage rules and schema, plus replay errors no LLM-as-judge in our MVP. So \"following along\" for us is: one agent, its stored snapshots, replay against the candidate, repeat runs to see consistency, then a gate verdict (fail rate, flaky rate) so we know if we're good to ship.",
              "score": 1,
              "created_utc": "2026-02-28 14:39:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}