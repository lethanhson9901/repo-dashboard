{
  "metadata": {
    "last_updated": "2026-02-07 16:47:15",
    "time_filter": "week",
    "subreddit": "LangChain",
    "total_items": 20,
    "total_comments": 57,
    "file_size_bytes": 97417
  },
  "items": [
    {
      "id": "1qwno6v",
      "title": "I built “Vercel for AI agents” — a single click deployment platform for any framework",
      "subreddit": "LangChain",
      "url": "/r/aiagents/comments/1qwnnlq/i_built_vercel_for_ai_agents_a_single_click/",
      "author": "Hisham_El-Halabi",
      "created_utc": "2026-02-05 15:05:46",
      "score": 66,
      "num_comments": 3,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qwno6v/i_built_vercel_for_ai_agents_a_single_click/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o3qp3w1",
          "author": "Existing_Way2258",
          "text": "I hope this is better than LangSmith LOL. Pls add LangChain support soon, I'm curious to try it out",
          "score": 0,
          "created_utc": "2026-02-05 16:32:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qpii9",
              "author": "Hisham_El-Halabi",
              "text": "haha noted, we’re working on it. You can join the waitlist on our website so get notified when we launch with langchain support",
              "score": 1,
              "created_utc": "2026-02-05 16:34:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3s03w7",
              "author": "MathematicianTop1654",
              "text": "check out [crewship.dev](http://crewship.dev), it supports LangGraph",
              "score": 1,
              "created_utc": "2026-02-05 20:10:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qw8s1s",
      "title": "I visualized the LLM workflows of the entire LangChain repo",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/bz7sbdzd6lhg1",
      "author": "Cyanosistaken",
      "created_utc": "2026-02-05 02:23:18",
      "score": 45,
      "num_comments": 6,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qw8s1s/i_visualized_the_llm_workflows_of_the_entire/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3nfyhe",
          "author": "Hungry_Age5375",
          "text": "Try visualizing agent workflows instead. Full repo mapping is like charting every neuron - cool but useless for actual work.",
          "score": 2,
          "created_utc": "2026-02-05 02:55:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oefcg",
              "author": "Relevant-Magic-Card",
              "text": "i still find this pretty useful info",
              "score": 1,
              "created_utc": "2026-02-05 06:59:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3p8m6t",
          "author": "Enough-Blacksmith-80",
          "text": "Very cool, man!",
          "score": 1,
          "created_utc": "2026-02-05 11:40:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qe5po",
              "author": "Cyanosistaken",
              "text": "Thanks! ",
              "score": 1,
              "created_utc": "2026-02-05 15:42:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zvsvt",
                  "author": "_harryj",
                  "text": "How did you manage the performance while visualizing such a large repo? Any tips for optimizing these kinds of workflows?",
                  "score": 1,
                  "created_utc": "2026-02-07 00:08:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qul2dx",
      "title": "NotebookLM For Teams",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qul2dx/notebooklm_for_teams/",
      "author": "Uiqueblhats",
      "created_utc": "2026-02-03 06:57:55",
      "score": 31,
      "num_comments": 4,
      "upvote_ratio": 0.98,
      "text": "For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.\n\nIn short, it is NotebookLM for teams, as it connects any LLM to your internal knowledge sources (search engines, Drive, Calendar, Notion, Obsidian, and 15+ other connectors) and lets you chat with it in real time alongside your team.\n\nI'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.\n\nHere's a quick look at what SurfSense offers right now:\n\n**Features**\n\n* Self-Hostable (with docker support)\n* Real Time Collaborative Chats\n* Real Time Commenting\n* Deep Agentic Agent\n* RBAC (Role Based Access for Teams Members)\n* Supports Any LLM (OpenAI spec with LiteLLM)\n* 6000+ Embedding Models\n* 50+ File extensions supported (Added Docling recently)\n* Local TTS/STT support.\n* Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc\n* Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.\n\n**Upcoming Planned Features**\n\n* Slide Creation Support\n* Multilingual Podcast Support\n* Video Creation Agent\n\nGitHub: [https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1qul2dx/notebooklm_for_teams/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3ax6z3",
          "author": "Otherwise_Wave9374",
          "text": "This is a solid idea, \"NotebookLM for teams\" is basically where a lot of agent work is going (shared context + connectors + permissions). The RBAC + self-hostable angle is especially nice if you are dealing with enterprise data.\n\nOne thing I would love to see is a clear story for evals, like regression tests for retrieval quality and agent actions as you add connectors.\n\nI have been writing up some notes on agent evals and tool-use patterns too, in case its useful: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-02-03 07:05:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b5tjf",
          "author": "No-Rutabaga6243",
          "text": "What's the main focus of this project?",
          "score": 1,
          "created_utc": "2026-02-03 08:26:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3euec3",
              "author": "Uiqueblhats",
              "text": "Basically, a model-agnostic LLM chat client optimized for teams who want to collaborate in real time.",
              "score": 1,
              "created_utc": "2026-02-03 21:06:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jn6uk",
          "author": "tsquig",
          "text": "Similar tool here. [NotebookLM...but more](https://implicit.cloud).",
          "score": 1,
          "created_utc": "2026-02-04 15:34:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvorim",
      "title": "Scalable RAG with LangChain: Handling 2GB+ datasets using Lazy Loading (Generators) + ChromaDB persistence",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qvorim/scalable_rag_with_langchain_handling_2gb_datasets/",
      "author": "jokiruiz",
      "created_utc": "2026-02-04 13:37:20",
      "score": 21,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nWe all love how easy `DirectoryLoader` is in LangChain, but let's be honest: running `.load()` on a massive dataset (2GB+ of PDFs/Docs) is a guaranteed way to get an OOM (Out of Memory) error on a standard machine, since it tries to materialize the full list of Document objects in RAM.\n\nI spent some time refactoring a RAG pipeline to move from a POC to a production-ready architecture capable of ingesting gigabytes of data.\n\n**The Architecture:** Instead of the standard list comprehension, I implemented a **Python Generator pattern (**`yield`**)** wrapping the LangChain loaders.\n\n* **Ingestion:** Custom loop using `DirectoryLoader` but processing files lazily (one by one).\n* **Splitting:** `RecursiveCharacterTextSplitter` with a 200 char overlap (crucial for maintaining context across chunk boundaries).\n* **Embeddings:** Batch processing (groups of 100 chunks) to avoid API timeouts/rate limits with `GoogleGenerativeAIEmbeddings` (though `OpenAIEmbeddings` works the same way).\n* **Storage:** `Chroma` with `persist_directory` (writing to disk, not memory).\n\nI recorded a deep dive video explaining the code structure and the specific LangChain classes used: [**https://youtu.be/QR-jTaHik8k?si=l9jibVhdQmh04Eaz**](https://youtu.be/QR-jTaHik8k?si=l9jibVhdQmh04Eaz)\n\nI found that for this volume of data, Chroma works well locally. Has anyone pushed Chroma to 10GB+ or do you usually switch to Pinecone/Weaviate managed services at that point?",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qvorim/scalable_rag_with_langchain_handling_2gb_datasets/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3jeuxl",
          "author": "pbalIII",
          "text": "Generator pattern is solid for ingestion, but the real question at 10GB+ isn't Chroma vs managed services... it's whether you actually need all that data indexed.\\n\\nChroma has a documented RAM ceiling (can't exceed system memory without LRU cache tuning) and some users hit stability issues around 300k chunks. But before jumping to Pinecone or Weaviate, worth asking: how much of that 2GB+ corpus actually gets retrieved? In most RAG pipelines I've seen, 80% of queries hit maybe 5% of the index.\\n\\nTwo paths:\\n\\n1. Tiered indexing: keep hot docs in Chroma, cold docs in cheaper blob storage with on-demand embedding\\n2. Aggressive deduplication + summarization upstream to shrink the actual index size\\n\\nManaged services solve scale, but they don't solve the retrieval quality problem of having too much noise in the index. Sometimes the better move is pruning before scaling.",
          "score": 3,
          "created_utc": "2026-02-04 14:54:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwpf3u",
      "title": "AG-UI: the protocol layer for LangGraph/LangChain UIs",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qwpf3u/agui_the_protocol_layer_for_langgraphlangchain_uis/",
      "author": "Acrobatic-Pay-279",
      "created_utc": "2026-02-05 16:10:42",
      "score": 20,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Agent UIs in LangChain / LangGraph usually start simple: stream final text, maybe echo some logs. But as soon as the goal is real interactivity: step‑level progress, visible tool calls, shared state, retries - the frontend ends up with a custom event schema tightly coupled to the backend.\n\nI have been digging into the AG‑UI (Agent-User Interaction Protocol) which is trying to standardize that layer. It defines a typed event stream that any agent backend can emit and any UI can consume. Instead of “whatever JSON is on the WebSocket” -- there is a small set of event kinds with clear semantics.\n\nAG-UI is not a UI framework and not a model API -- it’s basically the contract between an agent runtime and the UI layer. It groups all the events into core high-level categories:\n\n* Lifecycle: `RunStarted`, `RunFinished`, `RunError`, plus optional `StepStarted` / `StepFinished` that map nicely onto LangGraph nodes or LangChain tool/chain steps.\n* Text streaming: `TextMessageStart`, `TextMessageContent`, `TextMessageEnd` (and a chunk variant) for incremental LLM output.\n* Tool calls: `ToolCallStart`, `ToolCallArgs`, `ToolCallEnd`, `ToolCallResult` so UIs can render tools as first‑class elements instead of log lines.\n* State management: `StateSnapshot` and `StateDelta` (JSON Patch) for synchronizing shared graph/application state, with `MessagesSnapshot` available to resync after reconnects.\n* Special events: custom events in case an interaction doesn’t fit any of the categories above\n\nEach event has a `type` (such as `TextMessageContent`) plus a payload. There are other properties (like `runId`, `threadId`) that are specific to the event type.   \n  \nBecause the stream is standard and ordered, the frontend can reliably interpret what the backend is doing\n\nThe protocol is **transport‑agnostic**: SSE, WebSockets, or HTTP chunked responses can all carry the same event envelope. If a backend emits an AG‑UI‑compatible event stream (or you add a thin adapter), the frontend wiring can stay largely the same across different agent runtimes.\n\nFor people building agents: curious whether this maps cleanly onto the events you are already logging or streaming today, or if there are gaps.  \n  \n[Events docs](https://docs.ag-ui.com/concepts/events)  \nrepo: [https://github.com/ag-ui-protocol/ag-ui](https://github.com/ag-ui-protocol/ag-ui)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qwpf3u/agui_the_protocol_layer_for_langgraphlangchain_uis/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3r8zc4",
          "author": "Informal_Tangerine51",
          "text": "Protocol standardization is useful but doesn't solve the production debugging problem.\n\nStandard event stream helps UI consistency, but when your agent makes a wrong decision, can you reconstruct what it saw? `ToolCallStart` \\+ `ToolCallResult` show what happened, but not what data the tool returned, how fresh it was, or why the agent chose this tool over others.\n\nThe real production gap: evidence capture, not event standardization. When tool call 47 at 3am returns wrong data, you need content lineage (what was retrieved), policy decisions (was this action authorized), and regression fixtures (how to prevent this after model update).\n\nAG-UI events are ephemeral UI updates. Incident debugging needs durable, verifiable records. `StateSnapshot` helps resume execution but doesn't prove what data informed decisions or help debug why behavior changed between runs.\n\nFor production agents: are you capturing decision context (retrieval results, policy evaluations, input data) separately from UI events? Or assuming event logs are enough for post-incident analysis?\n\nStandard protocols are good for interop. Evidence infrastructure is what makes agents debuggable at scale.",
          "score": 1,
          "created_utc": "2026-02-05 18:05:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rpw94",
              "author": "Acrobatic-Pay-279",
              "text": "fair point and I agree with the distinction you are making, though interop and debuggability are two different concerns.\n\nI don't think AG-UI is trying (or claiming) to solve production debugging, evidence capture or auditability. it's intentionally scoped to the agent-UI boundary, making agent execution observable to the user in a consistent way.\n\nwhere it might help indirectly is by providing consistent run/step IDs and typed lifecycle/text/tool/state events (plus Raw/Custom escape hatches) that other observability systems can hang off. But on its own, that's clearly not sufficient for incident debugging\n\nwe would still need durable traces like LangSmith-style runs, retrieval snapshots, policy decisions. AG-UI feels complementary to that layer rather than a replacement.",
              "score": 2,
              "created_utc": "2026-02-05 19:22:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3rk7ki",
              "author": "Niightstalker",
              "text": "This protocol does not target production debugging, logging though as far as I understand. \n\nRegarding production debugging/monitoring I would turn to tools like LangSmith or LangFuse.  Those target exactly the points you mentioned",
              "score": 1,
              "created_utc": "2026-02-05 18:56:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3r6ouk",
          "author": "Number4extraDip",
          "text": "Lookup a2ui",
          "score": 0,
          "created_utc": "2026-02-05 17:54:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rpwnj",
              "author": "Acrobatic-Pay-279",
              "text": "A2UI is more of a declarative generative UI spec/payload and you can definitely translate A2UI messages into AG‑UI and then stream them + handles sync  \n  \nstill this post is about a different layer..",
              "score": 1,
              "created_utc": "2026-02-05 19:22:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3skzoe",
                  "author": "Number4extraDip",
                  "text": "Sure. Just thought it might be a useful bridge",
                  "score": 1,
                  "created_utc": "2026-02-05 21:51:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qtuye1",
      "title": "Everyone's losing their minds over Moltbook. Here's what's actually going on.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qtuye1/everyones_losing_their_minds_over_moltbook_heres/",
      "author": "Nir777",
      "created_utc": "2026-02-02 13:25:08",
      "score": 18,
      "num_comments": 6,
      "upvote_ratio": 0.76,
      "text": "Spent a while digging into this. Some things most people don't realize:\n\n\n\n\\- A security researcher created 500K+ accounts in minutes. That \"1.5 million agents\" number doesn't mean what you think.\n\n\\- The database storing API keys was fully exposed. Anyone could hijack agent accounts and post as them.\n\n\\- Many of those \"profound consciousness\" posts trace back to humans prompting their agents to say something deep.\n\n\n\nThat said, there IS real stuff happening. Agents sharing technical solutions, developing inside jokes not from training data, organizing by model architecture. That part is worth paying attention to.\n\n\n\nWrote up a full breakdown covering the real behaviors, security mess, and crypto scammers who showed up within hours: [https://open.substack.com/pub/diamantai/p/moltbook-a-social-media-for-ai-agents?utm\\_campaign=post-expanded-share&utm\\_medium=web](https://open.substack.com/pub/diamantai/p/moltbook-a-social-media-for-ai-agents?utm_campaign=post-expanded-share&utm_medium=web)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qtuye1/everyones_losing_their_minds_over_moltbook_heres/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o35orf5",
          "author": "nofilmincamera",
          "text": "Have any researchers actually done any meaningful analysis?  Nothing wrong with this article, just the only ive found is a vibe coded platform using Regex I am assuming.  I have done about a million of that type of analysis for work but don't want to redo if someone smarter already did.",
          "score": 3,
          "created_utc": "2026-02-02 14:12:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37mj9h",
          "author": "xpatmatt",
          "text": "100% bold claims from unidentified sources that are attributed no clear method for obtaining the underlying information.\n\nThat's 5 minutes I'll never get back.",
          "score": 3,
          "created_utc": "2026-02-02 19:41:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3aavuw",
              "author": "No_Pin_1150",
              "text": "hes a total disgrace!",
              "score": 1,
              "created_utc": "2026-02-03 04:14:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36atcq",
          "author": "Beginning-Foot-9525",
          "text": "Nice read thanks.",
          "score": 2,
          "created_utc": "2026-02-02 16:02:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39zber",
          "author": "waiting4omscs",
          "text": "So are people wasting money contributing to this site, or is there some end game to make money? I see this talked about in crypto-pivot-to-ai twitter",
          "score": 2,
          "created_utc": "2026-02-03 03:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3aooca",
          "author": "No_Success3928",
          "text": "Should rename to slopbook, though I guess meta has that moniker already.",
          "score": 2,
          "created_utc": "2026-02-03 05:53:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxg9dd",
      "title": "3D-Agent multi agent system with LangChain for Blender AI",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/kprnhlhgavhg1",
      "author": "Large-Explorer-8532",
      "created_utc": "2026-02-06 12:27:15",
      "score": 15,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qxg9dd/3dagent_multi_agent_system_with_langchain_for/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3ym0vy",
          "author": "Jorsoi13",
          "text": "Cool! Thanks for so much detail! When you mean „verify the frame“ you are talking about sending a screenshot to the agent or how does verification prevent drift? \n\nI‘m also currently building an agent, however I just started out literally a week ago. I‘m really trying to grasp how to orchestrate everything together. I‘m still lacking best practices and Langchain Docs are also a b*tch when it comes to documentation and integrating it with the frontend like yours (nextjs, etc.) \n\nDid you deploy using LangSmith or did you set up a self hosted version yourself? We‘re currently debating what the best approach is since we don’t want to pay 40€ just for their deployment. I was more hoping for an „n8n self-host approach“ on Digitalocean for like 5$ a month :)",
          "score": 3,
          "created_utc": "2026-02-06 20:07:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ynskn",
              "author": "Large-Explorer-8532",
              "text": "We do not use langsmith, we have a mix of custom frameworks and langchain.\n\nI would recomend self-hosted all the way. No reason to pay $40/month for LangSmith when you're starting out. Your n8n on DigitalOcean instinct is solid. Keep it simple, it is easy to get lost buying endless things.",
              "score": 1,
              "created_utc": "2026-02-06 20:16:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zbhks",
                  "author": "mdrxy",
                  "text": "starting out, LangSmith has a free plan for developers ;)",
                  "score": 2,
                  "created_utc": "2026-02-06 22:15:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3zbc8k",
          "author": "mdrxy",
          "text": "cool can you talk more about the langchain architecture you used?",
          "score": 2,
          "created_utc": "2026-02-06 22:14:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o422hvp",
              "author": "Large-Explorer-8532",
              "text": "[](https://docs.langchain.com/oss/python/deepagents/skills)it gives us ready-to-use primitives for subagent orchestration, isolation, memory, and context management out of the box. Each agent (Gemini, GPT, Claude) runs in its own isolated node with its own context window, and LangGraph handles the routing, state persistence, and handoffs between them. It's what lets us keep each agent focused on what it's best at without them stepping on each other. Happy to go deeper on any specific part if you're curious![](https://docs.langchain.com/oss/python/deepagents/skills)",
              "score": 1,
              "created_utc": "2026-02-07 09:58:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3zd2up",
          "author": "Upstairs-Spell7521",
          "text": "why do you use langchain tho? what advantages it gives to you?",
          "score": 2,
          "created_utc": "2026-02-06 22:23:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o421v7z",
              "author": "Large-Explorer-8532",
              "text": "LangChain/LangGraph has some really useful built-in features for subagent orchestration, memory management, human-in-the-loop workflows, and other ways to coordinate a multi-agent system. We've also experimented with skills from DeepAgent... mixed feelings on that so far  \nThe main benefit is not having to build these primitives from scratch, having them ready to use saves us hours of work",
              "score": 1,
              "created_utc": "2026-02-07 09:52:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xiado",
          "author": "Jorsoi13",
          "text": "Wow! Thats amazing. I mean the visuals of that Eiffel Tower are debateable but the fact that it works is great. Now the next step is probably reducing friction. It seems like there are a lot of steps to actually start using the product at least thats what your dashboard suggests. \n\nOther than that: \n\n\\- The agent runs really long. I thought \"Oh shit that thing must swallow some serious credits\". Am I right? How much money did you spend on creating the eiffel tower? Or how many tokens does an operation like that generate? \n\n\\- What is your agent orchestration like? I would really love to see your graph structure for the sake of learning. Would you mind sharing it ?",
          "score": 1,
          "created_utc": "2026-02-06 16:57:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yiph7",
              "author": "Large-Explorer-8532",
              "text": "Thanks man, really appreciate the thoughtful feedback!\n\nyeah the Eiffel Tower is definitely rough blocking, not a portfolio piece. But the point is the agent built it autonomously from a single prompt, which is the hard part. Quality will keep improving as the models get better.\n\nyou're 100% right and we're actively working on this. The onboarding has too many steps right now. Goal is: install the plugin → connect → start chatting. We're cutting it down.\n\nhonestly it's less than you'd think. The agent is smart about batching operations and only calls the reasoning model when it actually needs to make a decision. Most of the \"thinking\" steps are lightweight. The long runtime is mostly Blender executing the code, not the AI burning tokens. The output are short mostly short.\n\nThe high level is: we use a planning agent that breaks the task into stages, another ones comes to reason and think in \"3D/Spatial Math\" then an execution agent handles each stage in a loop (perceive scene → decide next action → execute code → verify via viewport). The key insight was adding the verification step... without it the agent drifts and you get garbage. There's a router in between that decides when to escalate to the reasoning model vs handle it with a faster/cheaper one. Would love to do a deeper write-up at some point once we've solidified the architecture more.\n\nWhat's your background? You sound like you're building agents yourself ",
              "score": 2,
              "created_utc": "2026-02-06 19:51:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qukgay",
      "title": "Preloading MCP tools cost me ~50k tokens per run",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qukgay/preloading_mcp_tools_cost_me_50k_tokens_per_run/",
      "author": "Basic_Tea9680",
      "created_utc": "2026-02-03 06:23:32",
      "score": 14,
      "num_comments": 12,
      "upvote_ratio": 0.89,
      "text": "I ran into something unintuitive while building MCP-based agents using langchain and thought it might be useful to share.\n\nIn my setup, the agent had access to a few common MCP tools like fs, linear, GitHub, figma.\n\nI just added them to the agent and forgot and agent used them sparingly.\nEven with AugmentCode (AI agent I use) I dont want to switch tools on and off. That actually messes up with prompt catching as well .\n\nWhen I actually measured token usage, here’s what it looked like:\n\nSystem instructions: ~7k tokens\nMCP tool defs: ~45–50k tokens\nFirst user message: a few hundred tokens\n\nOn a 200k-context model, that meant ~25% of the context window was gone. Eventually history builds up but this 25% remains consistent. \n\nAs I mentioned earlier, in most runs, the agent only ended up using one or two tools, usually the filesystem. Linear, GitHub and Figma were rarely touched.\n\nSo tens of thousands of tokens were effectively dead weight. The minimum you must do is context caching but on long running agents even that gets expensive. Also the history summarization is triggered more often with this setup.\n\nI tried a different approach, don’t inject all MCP tools upfront. Only surface tools after the model signals it needs them.\n\nThe results were pretty consisten, ~25% fewer total agent tokens for every llm call, lower latency, more context for reasoning, and lessed chat history compaction.\n\nI wrapped this pattern into a small project called mcplexor so I wouldn’t keep re-implementing it. It dynamically discovers MCP tools instead of front-loading them. Feel free to DM if you want to give it a try. Would love feedback to improve it.  ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qukgay/preloading_mcp_tools_cost_me_50k_tokens_per_run/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3atgm5",
          "author": "Ok-Regret3392",
          "text": "Yups. Totally normal! Esp if you have very big mcp’s (I’m looking at you Stripe and PostHog) Highly recommend you turn on/off the mcps that you know you won’t use in your currently dev stint. Alternatively.. some mcp calls can be resolved by having the LLM run a curl command instead of burning expensive tokens.",
          "score": 4,
          "created_utc": "2026-02-03 06:33:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3aw5nh",
              "author": "Basic_Tea9680",
              "text": "Turing mcp on off actually causes more harm because prompt cache is invalidated. \n\nI would not recommend it in tools like Claude code and augment code during the session.",
              "score": 1,
              "created_utc": "2026-02-03 06:56:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ffh1y",
                  "author": "santiagolarrain",
                  "text": "Disabling the mcps you have installed and are not using in Claude Code is harmful? I mean disable, exit and run CC again.",
                  "score": 1,
                  "created_utc": "2026-02-03 22:46:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ci986",
          "author": "pbalIII",
          "text": "Dynamic discovery adds a round trip. Your agent now has to signal intent, wait for schema injection, then actually call the tool. For single-shot tasks that's fine, but in tight agentic loops where the model chains 4-5 tool calls, you're stacking latency.\n\nClaude Code shipped lazy loading last month and the feedback I've seen is mixed... faster cold starts but noticeable pauses mid-conversation when a tool gets pulled in for the first time. The semantic search step to match intent to tool also isn't free.\n\nHonest question: have you measured the latency delta on multi-step runs? Curious if the token savings outweigh the added round trips in practice.",
          "score": 2,
          "created_utc": "2026-02-03 14:32:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hjpd9",
              "author": "Basic_Tea9680",
              "text": "So the find tool call is around 2s delay + then the execute tool call , which is local so at the same time. So basically every tool call before discovery adds 2-3s latency. \n\nThe pattern I used was if there is a tool I use very actively, I integrated directly with coding agent rest which are always available but sparingly used are in tool discovery tool. Would love to hear your feedback. You can try on mcplexor.com . I build a nice shell app as well it shows token bloat each tool is adding as well.",
              "score": 1,
              "created_utc": "2026-02-04 06:27:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3isfwm",
                  "author": "pbalIII",
                  "text": "That 2-3s per discovery call adds up fast when you're chaining multiple tools. The tiered approach you're describing (frequently used tools direct, the rest behind discovery) is basically what Claude's tool search does under the hood... they reported 85% token reduction by loading only 3-5 relevant tools instead of the full catalog. Curious how your shell app visualizes the bloat... per-call breakdown or cumulative across the run?",
                  "score": 1,
                  "created_utc": "2026-02-04 12:51:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3bzq0w",
          "author": "InfraScaler",
          "text": "What I do in my custom agent is have \"templates\" stored like skills and a sqlite with the embeddings to do semantic search, so the agent only pulls relevant templates then learns about the relevant MCPs.",
          "score": 1,
          "created_utc": "2026-02-03 12:46:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hk091",
              "author": "Basic_Tea9680",
              "text": "That's interesting, I wanted to improve the recall precision and rank as well so went with a specific route. Thought of many PMs and marketing sales folks who use so many mcp tools. They can benefit from this. Would love for you to try the tool and compare with your solution. Feel free to dm",
              "score": 1,
              "created_utc": "2026-02-04 06:30:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3hla6u",
                  "author": "Wide_Brief3025",
                  "text": "Tracking token usage is a huge pain when you are running heavy MCP tools and looking for efficiency gains, especially if you want to scale lead discovery for PMs and sales. You might find it easier to refine your targeting with something that offers real time keyword alerts like ParseStream, since it helps cut out a lot of wasted runs and only flags genuinely high potential conversations.",
                  "score": 1,
                  "created_utc": "2026-02-04 06:40:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dr8lr",
          "author": "SearchTricky7875",
          "text": "what about using bigtool agent to load the mcp tools dynamically - \n\n    bigtool_agent = bigtool_create_agent(model, {k: v[\"tool\"] for k, v in tool_registry.items()})",
          "score": 1,
          "created_utc": "2026-02-03 18:05:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv0mmr",
      "title": "We monitor 4 metrics in production that catch most LLM quality issues early",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qv0mmr/we_monitor_4_metrics_in_production_that_catch/",
      "author": "dinkinflika0",
      "created_utc": "2026-02-03 18:52:58",
      "score": 13,
      "num_comments": 3,
      "upvote_ratio": 0.89,
      "text": "After running LLMs in production for a while, we've narrowed down monitoring to what actually predicts failures before users complain.\n\nLatency p99: Not average latency - p99 catches when specific prompts trigger pathological token generation. We set alerts at 2x baseline.\n\nQuality sampling at configurable rates: Running evaluators on every request burns budget. We sample a percentage of traffic with automated judges checking hallucination, instruction adherence, and factual accuracy. Catches drift without breaking the bank.\n\nCost per request by feature: Token costs vary significantly between features. We track this to identify runaway context windows or inefficient prompt patterns. Found one feature burning 40% of inference budget while serving 8% of traffic.\n\nError rate by model provider: API failures happen. We monitor provider-specific error rates so when one has issues, we can route to alternatives.\n\nWe log everything with distributed tracing. When something breaks, we see the exact execution path - which docs were retrieved, which tools were called, what the LLM actually received.\n\nSetup details: [https://www.getmaxim.ai/docs/introduction/overview](https://www.getmaxim.ai/docs/introduction/overview)\n\nWhat production metrics are you tracking?",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qv0mmr/we_monitor_4_metrics_in_production_that_catch/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3eivw9",
          "author": "Ecto-1A",
          "text": "It really comes down to what you are doing and if you are doing any RAG. What you outlined all seems pretty standard. We monitor latency, tokens, relevance of response, proper tool calling, turns to resolution, confidence, and error handling on every run. Any that fall below our threshold as well as a 20% sample of all runs get sent to an annotation queue and kick off a full suite of G-Eval evaluators and we are working to build out a new testing suite based on the CheckEval paper published a couple months ago.\n\nAre you running any evaluators at build time? That has definitely helped catch some things that could have otherwise flooded our evaluator queues.",
          "score": 2,
          "created_utc": "2026-02-03 20:12:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f1fyh",
          "author": "Informal_Tangerine51",
          "text": "You're monitoring outputs but not capturing inputs. When quality sampling flags hallucination, can you replay what was retrieved to cause it?\n\nWe track similar metrics. The debugging gap: p99 latency spike happens, we know which prompt triggered it, but not what documents were retrieved or whether context was stale. Error rate shows provider failure, doesn't show if retry used different data.\n\nYour distributed tracing logs execution path. Does it capture the actual retrieved content with timestamps, or just that retrieval happened? When evaluator flags factual error, can you verify the source chunks were current?\n\nMetrics catch problems. Evidence proves why they happened. Cost per request is useful, but when that 40% budget feature produces wrong output, can you prevent recurrence or just know it's expensive?",
          "score": 1,
          "created_utc": "2026-02-03 21:39:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu5ss8",
      "title": "Roast my Thesis: \"Ops teams are burning budget on A100s because reliable quantization pipelines don't exist.\"",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qu5ss8/roast_my_thesis_ops_teams_are_burning_budget_on/",
      "author": "Alternative-Yak6485",
      "created_utc": "2026-02-02 19:59:01",
      "score": 10,
      "num_comments": 10,
      "upvote_ratio": 0.73,
      "text": "I’m a dev building a 'Quantization-as-a-Service' pipeline and I want to check if I'm solving a real problem or just a skill issue.\n\n**The Thesis:** Most AI startups are renting massive GPUs (A100s/H100s) to run base models in FP16. They *could* downgrade to A10s/T4s (saving \\~50%), but they don't.\n\n**My theory on why:** It's not that MLOps teams *can't* figure out quantization—it's that **maintaining the pipeline is a nightmare.**\n\n1. You have to manually manage calibration datasets (or risk 'lobotomizing' the model).\n2. You have to constantly update Docker containers for vLLM/AutoAWQ/ExLlama as new formats emerge.\n3. **Verification is hard:** You don't have an automated way to prove the quantized model is still accurate without running manual benchmarks.\n\n**The Solution I'm Building:** A managed pipeline that handles the calibration selection + generation (AWQ/GGUF/GPTQ) + **Automated Accuracy Reporting** (showing PPL delta vs FP16).\n\n**The Question:** As an MLOps engineer/CTO, is this a pain point you would pay to automate (e.g., $140/mo to offload the headache)?\n\nOr is maintaining your own vLLM/quantization scripts actually pretty easy once it's set up?",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qu5ss8/roast_my_thesis_ops_teams_are_burning_budget_on/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o39rc9j",
          "author": "BeerBatteredHemroids",
          "text": "Platforms like databricks already do this... as a \"CTO\" you should know who your competition is.",
          "score": 2,
          "created_utc": "2026-02-03 02:16:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lfw8s",
              "author": "Purple-Programmer-7",
              "text": "Trying to use data bricks is like sitting in a Cessna cockpit with zero training. You’ll figure it out eventually, but it’s going to take you a few hours.\n\nIf someone had a LEAN solution that was easy to use and didn’t break the bank, why not?\n\nThough I would say Oxen is already closer to doing what OP suggests.",
              "score": 0,
              "created_utc": "2026-02-04 20:33:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mdbew",
                  "author": "BeerBatteredHemroids",
                  "text": "If you're sitting in a cessna cockpit with no training you shouldn't be in the cockpit babe.",
                  "score": 1,
                  "created_utc": "2026-02-04 23:18:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o38s8lc",
          "author": "soowhatchathink",
          "text": "I'm so sick of people pushing their ai slop here posing an advertisement as a question",
          "score": 3,
          "created_utc": "2026-02-02 23:01:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37t8n3",
          "author": "Space__Whiskey",
          "text": "Try it if you know how. \n\nWhat, will you give up if some nerd on reddit pokes holes in it? \n\nI wouldn't wait for that.",
          "score": 2,
          "created_utc": "2026-02-02 20:12:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jrrt9",
          "author": "PaddingCompression",
          "text": "At $140/mo it's probably too cheap to be worth it!  What I mean is that the reason people burn money on expensive inference, and from what they're telling you, is that the time to deal with it isn't worth the cost savings.\n\nThe more you make it your problem as a thing they can reliably outsource to make the problem go away the better - can you charge 20% plus of the cost savings in a way users know they won't have to worry about it?\n\nThe gold standard would be for you to sell more expensive A10 inference where you guarantee the accuracy, and make it your problem.  The users could pay less for inference and not care how you do it as long as accuracy doesn't suffer.  If you can guarantee accuracy rather than selling a tool, that removes the risk (and includes occasionally having to lose money on an A100 if you can't get the accuracy) - risk shifting would make this much more valuable than just a tool, and if your tool does work means more money for you.",
          "score": 1,
          "created_utc": "2026-02-04 15:55:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lul4f",
          "author": "Unstable_Llama",
          "text": "It’s not too much harder than running local inference, and anybody training their own models probably has the resources to manage their own quantization.\n\nMaybe not though. Here is a free version for exllama I’ve been working on, it does multiple quant sizes and measures ppl and kl div and compiles them into a model card with a single command.\n\nhttps://github.com/UnstableLlama/ezexl3",
          "score": 1,
          "created_utc": "2026-02-04 21:43:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3867b0",
          "author": "CanadianPropagandist",
          "text": "I think you might be undervaluing this idea tbh. If you run it a lot more \"white glove\" (I hate that term) you could probably rake in a lot more.\n\nIt fits into an optimization genre that is going to get very popular as companies start learning how to reduce their inference costs.",
          "score": 0,
          "created_utc": "2026-02-02 21:13:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtf5gc",
      "title": "I built a CLI to find \"Zombie Vectors\" in Pinecone/Weaviate (and estimate how much RAM you're wasting)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qtf5gc/i_built_a_cli_to_find_zombie_vectors_in/",
      "author": "billycph",
      "created_utc": "2026-02-01 23:57:50",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "Hey everyone,\n\nI’m an ex-AWS S3 engineer. In my previous life, we obsessed over \"Lifecycle Policies\" because storing petabytes of data is expensive. If data wasn’t touched in 30 days, we moved it to cold storage.\n\nI noticed a weird pattern in the AI space recently: **We are treating Vector Databases like cold storage.**\n\nWe shove 100% of our embeddings into expensive Hot RAM (Pinecone, Milvus, Weaviate), even though for many use cases (like Chat History or Seasonal Catalog Search), 90% of that data is rarely queried after a month. It’s like keeping your tax returns from 1990 in your wallet instead of a filing cabinet.\n\nI wanted to see exactly how much money was being wasted, so I wrote a simple open-source CLI tool to audit this.\n\n**What it does:**\n\n1. **Connects** to your index (Pinecone currently supported).\n2. **Probes** random sectors of your vector space to sample metadata.\n3. **Analyzes** the `created_at` or timestamp fields.\n4. **Reports** your \"Stale Rate\" (e.g., \"65% of your vectors haven't been queried in >30 days\") and calculates potential savings if you moved them to S3/Disk.\n\n**The \"Trust\" Part:** I know giving API keys to random tools is a bad idea.\n\n* This script runs **100% locally** on your machine.\n* Your keys never leave your terminal.\n* You can audit the code yourself (it’s just Python).\n\n**Why I built this:** I’m working on a larger library to automate the \"S3 Offloading\" process, but first I wanted to prove that the problem actually exists.\n\nI’d love for you to run it and let me know: **Does your stale rate match what you expected?** I’m seeing \\~90% staleness for Chat Apps and \\~15% for Knowledge Bases.\n\n**Repo here:** [https://github.com/billycph/VectorDBCostSavingInspector](https://github.com/billycph/VectorDBCostSavingInspector)\n\nFeedback welcome!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qtf5gc/i_built_a_cli_to_find_zombie_vectors_in/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qunx9g",
      "title": "Why doesn't LangChain support agent skills?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qunx9g/why_doesnt_langchain_support_agent_skills/",
      "author": "Suspicious_Fall6860",
      "created_utc": "2026-02-03 09:55:40",
      "score": 10,
      "num_comments": 11,
      "upvote_ratio": 0.81,
      "text": "Why doesn't LangChain support agent skills? It only allows loading a single [skill.md](http://skill.md) file. How can we support references and scripts?\n\nHere are some materials I found.\n\n[Skills - Docs by LangChain](https://docs.langchain.com/oss/python/langchain/multi-agent/skills)  \n  \n[Build a SQL assistant with on-demand skills - Docs by LangChain](https://docs.langchain.com/oss/python/langchain/multi-agent/skills-sql-assistant)  \n\n\n[deepagents/examples/content-builder-agent/skills/blog-post/SKILL.md at master · langchain-ai/deepagents · GitHub](https://github.com/langchain-ai/deepagents/tree/master/examples)  \n  \n[deepagents/examples/content-builder-agent at master · langchain-ai/deepagents](https://github.com/langchain-ai/deepagents/tree/master/examples/content-builder-agent)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qunx9g/why_doesnt_langchain_support_agent_skills/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3bfsbg",
          "author": "Otherwise_Wave9374",
          "text": "Yeah the single SKILL.md thing feels limiting once you want anything beyond toy examples (versioning, shared snippets, scripts, references, etc). I have seen people treat skills as a mini package, folder per skill with an index plus tests, then load/resolve by name and inject into the agent prompt/runtime. Would be nice if LangChain standardized that pattern. I have a couple writeups saved on agent skills/tooling design here too: https://www.agentixlabs.com/blog/",
          "score": 4,
          "created_utc": "2026-02-03 10:03:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bgsic",
          "author": "Suspicious_Fall6860",
          "text": "Actually, I've found that most current support for Agent Skills is basically in CLI-mode systems. Does anyone know of any frameworks that support explicit skill writing and debugging?",
          "score": 3,
          "created_utc": "2026-02-03 10:12:55",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3bpjrz",
              "author": "Tobi-Random",
              "text": "Official spec project provides tools for validation against spec",
              "score": 1,
              "created_utc": "2026-02-03 11:31:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hojmy",
                  "author": "ohansemmanuel",
                  "text": "I think this is still lacking. At best it validates frontmatter and a few best practices. \n\nIn production systems you'll quickly find out that even with the best Skills, they're (almost) useless if NOT triggered by the AI agent. \n\nCurrent \"eval\" systems don't really work with Skills (at least not in a way I think is optimised). So there's still a need for robust systems to build, test, iterate as we do with prompts today. Something along the lines of trigger evals?\n\nA counter argument would be that Skills are just prompts and you can still get by. True, the difference would be we're bundling a lot more in these \"prompts\"",
                  "score": 1,
                  "created_utc": "2026-02-04 07:08:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3d0ga9",
              "author": "Jords13xx",
              "text": "Check out the Rasa framework for building conversational agents with custom skills. It's pretty robust for skill writing and debugging. Also, you might want to explore Botpress or Dialogflow; they offer good support for skill customization.",
              "score": 1,
              "created_utc": "2026-02-03 16:01:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3chz29",
          "author": "pbalIII",
          "text": "Ran into the same friction building a multi-skill agent last month. The single SKILL.md loader is intentional... LangChain wants skills to be self-contained folders with their own files, scripts, and references bundled together.\n\nThe pattern that worked for me: treat each skill as its own directory under ~/.deepagents/agent/skills/, then let the agent discover and load them by name at runtime. The frontmatter gets indexed for discovery, but the full SKILL.md only loads when the agent actually needs it (saves tokens).\n\nFor debugging, deepagents-CLI has a skills list command that shows what's loaded. Not perfect tooling, but better than dumping everything into one monolithic file.",
          "score": 2,
          "created_utc": "2026-02-03 14:30:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3crow1",
          "author": "cordialgerm",
          "text": "References and scripts are supposed to be loaded on demand by the agent after reading the SKILL.md. so all you need to do is include them in your filesystem and reference them in the SKILL.md and it works great.",
          "score": 1,
          "created_utc": "2026-02-03 15:20:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eofke",
          "author": "Niightstalker",
          "text": "Here they wrote a blogpost about supporting skills within their deep agents cli: https://www.blog.langchain.com/using-skills-with-deep-agents/",
          "score": 1,
          "created_utc": "2026-02-03 20:39:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hmmrj",
          "author": "ohansemmanuel",
          "text": "Technically you could probably get by building a system around this yourself. \n\nYou'd need to connect to a VM / sandboxed machine at runtime, that's capable of running scripts, installin dependencies, leveraging bash for reading additional files etc. You'd then expose tools to interact with the said machine. \n\nYou alluded to the bigger issue in your comment - as an industry it seems we're mostly focused on Skills within CLI (coding agents) atm. But in my opinion, the bigger win comes from the use case you're describing i.e., remote agents running determinsitic workflows / SOPs with references and scripts. \n\nIf you're looking for an off the shelve solution, you may like Bluebag AI (handles all the hard stuff so you can integrate in 2-lines of code) \n\nDisclaimer: I built this and already used in production systems. Would happily walk you through it",
          "score": 1,
          "created_utc": "2026-02-04 06:52:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xs2ng",
          "author": "npentrel",
          "text": "Hi there - just updated the docs a bit to make this clearer: You can use more files, as long as they are referenced in [SKILL.md](http://SKILL.md)  \\- thanks for pointing the needed clarification out to us! [https://docs.langchain.com/oss/python/deepagents/skills](https://docs.langchain.com/oss/python/deepagents/skills)",
          "score": 1,
          "created_utc": "2026-02-06 17:43:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3c5nqq",
          "author": "Upset-Pop1136",
          "text": "langchain’s not trying to be a full agent OS. they optimize for demos and DX, not long-lived agents. ",
          "score": 0,
          "created_utc": "2026-02-03 13:23:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1quni12",
      "title": "AI projects with Langchain and Langgraph",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1quni12/ai_projects_with_langchain_and_langgraph/",
      "author": "Affectionate_Bid2797",
      "created_utc": "2026-02-03 09:28:03",
      "score": 10,
      "num_comments": 8,
      "upvote_ratio": 0.82,
      "text": "Hello everyone,\n\nI hope you’re doing well. I’m a software engineer who’s really passionate about machine learning and AI, and I’d love to get some advice from engineers already working in the field.\n\nI’ve studied the fundamentals and understand the theory and common frameworks, but I feel I need to build more concrete, real-world projects to gain confidence and practical experience.\n\nI’ve gone through tutorials and done quite a bit of research, but much of the advice feels repetitive, and many project suggestions are the same everywhere. So I wanted to ask directly: what projects would you recommend building that are actually useful and help someone stand out?\n\nI’m not looking for generic or cliché advice, but rather insights from people with hands-on experience in the industry.\n\nThanks a lot for your time.I really appreciate any suggestions.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1quni12/ai_projects_with_langchain_and_langgraph/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3gspuy",
          "author": "hrishikamath",
          "text": "Honesty just do projects you really care, where you care about the output and it’s challenging but it’s also reasonable. You will come across problems and then use whatever you learn to diagnose, you will learn better. I did that for finance and ended up learning a lot without taking tutorials or watching any courses. It’s open source happy to share the link/blogpost if you want.",
          "score": 2,
          "created_utc": "2026-02-04 03:19:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42kt3j",
              "author": "Affectionate_Bid2797",
              "text": "Hey, thanks for sharing!   \nYes I would love to have a look and talk more about it.",
              "score": 1,
              "created_utc": "2026-02-07 12:43:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3bdrb1",
          "author": "Witty_System7237",
          "text": "What kind of domain are you most interested in-like data analysis, chat assistants, or something else? That could help narrow down useful project ideas.",
          "score": 1,
          "created_utc": "2026-02-03 09:43:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3be0jw",
              "author": "Affectionate_Bid2797",
              "text": "I am interested in building agentic workflows from end-to-end.   \nThank you!",
              "score": 1,
              "created_utc": "2026-02-03 09:46:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3boith",
                  "author": "Apart_Commercial2279",
                  "text": "the main issue if you want to face real world issue and not getting basic advice is to build a usefull agent for you, your friends family or coworker and make them use them (this is the hardest). If they don't use it, or use it the wrong way or if they don't get what they need you will iterate and make it more complex, fix bug, add retry, guardrails ect... And really have an end to end system",
                  "score": 0,
                  "created_utc": "2026-02-03 11:22:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3c3nsh",
          "author": "Upset-Pop1136",
          "text": "Learn from the open source products and great tools. One of them is Dify",
          "score": 1,
          "created_utc": "2026-02-03 13:11:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ifk6p",
          "author": "MathematicianTop1654",
          "text": "this tutorial might be useful: [https://www.crewship.dev/blog/deploy-langgraph-to-production](https://www.crewship.dev/blog/deploy-langgraph-to-production)",
          "score": 1,
          "created_utc": "2026-02-04 11:16:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41roq2",
          "author": "Kosemani2",
          "text": "You can study this project. Uses Langgraph deepAgent to orchestrate agentic workflow. https://github.com/olasunkanmi-SE/codebuddy/tree/main/src/agents",
          "score": 1,
          "created_utc": "2026-02-07 08:12:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvvnzd",
      "title": "Build a self-updating wiki from codebases (open source, Apache 2.0)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qvvnzd/build_a_selfupdating_wiki_from_codebases_open/",
      "author": "Whole-Assignment6240",
      "created_utc": "2026-02-04 17:57:34",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I recently have been working on [a new project](https://github.com/cocoindex-io/cocoindex/tree/v1/examples/multi_codebase_summarization) to build a self-updating wiki from codebases. I wrote a step-by-step tutorial.\n\nYour code is the source of truth, and documentations out of sync is such a common pain especially in larger teams. Someone refactors a module, and the wiki is already wrong. Nobody updates it until a new engineer asks a question about it.\n\nThis open source project scans your codebases, extracts structured information with LLMs, and generates Markdown documentation with Mermaid diagrams — using CocoIndex + Instructor + Pydantic.\n\nWhat's cool about this example:\n\n• 𝐈𝐧𝐜𝐫𝐞𝐦𝐞𝐧𝐭𝐚𝐥 𝐩𝐫𝐨𝐜𝐞𝐬𝐬𝐢𝐧𝐠 — Only changed files get reprocessed. saving 90%+ of LLM cost and compute.\n\n• 𝐒𝐭𝐫𝐮𝐜𝐭𝐮𝐫𝐞𝐝 𝐞𝐱𝐭𝐫𝐚𝐜𝐭𝐢𝐨𝐧 𝐰𝐢𝐭𝐡 𝐋𝐋𝐌𝐬 — LLM returns real typed objects — classes, functions, signatures, relationships.\n\n• 𝐀𝐬𝐲𝐧𝐜 𝐟𝐢𝐥𝐞 𝐩𝐫𝐨𝐜𝐞𝐬𝐬𝐢𝐧𝐠 — All files in a project get extracted concurrently with asyncio.gather().\n\n• 𝐌𝐞𝐫𝐦𝐚𝐢𝐝 𝐝𝐢𝐚𝐠𝐫𝐚𝐦𝐬 — Auto-generated pipeline visualizations showing how your functions connect across the project.\n\nThis pattern hooks naturally into PR flows — run it on every merge and your docs stay current without anyone thinking about it. I think it would be cool next to build a coding agent with Langchain on top of this fresh knowledge. \n\nIf you want to explore the full example (fully open source, with code, APACHE 2.0), it's here:\n\n👉 [https://cocoindex.io/examples-v1/multi-codebase-summarization](https://cocoindex.io/examples-v1/multi-codebase-summarization)\n\nIf you find CocoIndex useful, a star on Github means a lot :)\n\n⭐ [https://github.com/cocoindex-io/cocoindex](https://github.com/cocoindex-io/cocoindex)\n\ni'd love to learn from your feedback, thanks!",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qvvnzd/build_a_selfupdating_wiki_from_codebases_open/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qtw1wu",
      "title": "LangChain VS LamaIndex - Plug r/LangChain context into your LangChain agents - Free MCP integration",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qtw1wu/langchain_vs_lamaindex_plug_rlangchain_context/",
      "author": "jannemansonh",
      "created_utc": "2026-02-02 14:10:12",
      "score": 9,
      "num_comments": 2,
      "upvote_ratio": 0.91,
      "text": "Hey, creator of [needle.app](http://needle.app) here. This subreddit has incredible implementation knowledge - patterns, agent architectures, RAG configs, tool calling issues, what actually works in production.\n\nWe indexed all 2025 r/LangChain discussions and made them searchable. Even better: we built an MCP integration so you can plug this entire subreddit's context directly into your LangChain agents for agentic RAG.\n\nTry searching:\n\n* Tool calling with function schemas\n* Multi-agent orchestration patterns\n* Vector store performance comparisons\n\nUseful if you're:\n\n* Debugging agent loops or tool calling\n* Finding solutions others have already tested\n\n**Want to use this in your LangChain agents?** Check out our MCP integration guide: [https://docs.needle.app/docs/guides/mcp/needle-mcp-server/](https://docs.needle.app/docs/guides/mcp/needle-mcp-server/)\n\nNow you can build agents that query r/LangChain knowledge directly while reasoning.\n\nCompletely free, no signup: [https://needle.app/featured-collections/reddit-langchain-2025](https://needle.app/featured-collections/reddit-langchain-2025)\n\n[LangChain or LamaIndex - Needle.app RAG Chat](https://reddit.com/link/1qtw1wu/video/prkrlbzo93hg1/player)\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qtw1wu/langchain_vs_lamaindex_plug_rlangchain_context/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o35qt2s",
          "author": "beneautomas",
          "text": "Useful!",
          "score": 1,
          "created_utc": "2026-02-02 14:23:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35rfk4",
              "author": "jannemansonh",
              "text": "RAGception XD",
              "score": 1,
              "created_utc": "2026-02-02 14:26:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qsxuum",
      "title": "Is AsyncPostgresSaver actually production-ready in 2026? (Connection pooling & resilience issues)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qsxuum/is_asyncpostgressaver_actually_productionready_in/",
      "author": "FunEstablishment5942",
      "created_utc": "2026-02-01 13:00:46",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nI'm finalizing the architecture for a production agent service and blocked on the database layer. I've seen multiple reports (and GitHub issues like #5675 and #1730) from late 2025 indicating that `AsyncPostgresSaver` is incredibly fragile when it comes to connection pooling.\n\nSpecifically, I'm concerned about:\n\n1. **Zero Resilience:** If the underlying pool closes or a connection goes stale, the saver seems to just crash with `PoolClosed` or `OperationalError` rather than attempting a retry or refresh.\n2. **Lifecycle Management:** Sharing a `psycopg_pool` between my application (SQLAlchemy) and LangGraph seems to result in race conditions where LangGraph holds onto references to dead pools.\n\n**My Question:**  \nHas anyone successfully deployed `AsyncPostgresSaver` in a high-load production environment recently (early 2026)? Did the team ever release a native fix for automatic retries/pool recovery, or are you all still writing custom wrappers / separate pool managers to baby the checkpointer?\n\nI'm trying to decide if I should risk using the standard saver or just bite the bullet and write a custom Redis/Postgres implementation from day one.\n\nThanks! Is AsyncPostgresSaver actually production-ready in 2026? (Connection pooling & resilience issues)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qsxuum/is_asyncpostgressaver_actually_productionready_in/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o33q9hi",
          "author": "Shreyanak_exe",
          "text": "I really need an answer to this cause I am building stg similar that's going in production soon and I want to be sure the connection doesn't goes stale every now and then. \n\nBtw: some guy built a resilient wrapper for Postgres. If you can test and lmk if it's worth giving a shot, that'd be helpful\n\n[ResilientPostgresSaver](https://github.com/samirpatil2000/agentic-template/blob/main/agents/resilient_postgres_saver.py)",
          "score": 2,
          "created_utc": "2026-02-02 04:46:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3uuae1",
          "author": "rookastle",
          "text": "This is a known fragility point. The saver is lean and expects a persistent, valid connection, which isn't always realistic. Many production setups add a layer on top.\n\nAs a practical diagnostic, you could try wrapping your checkpointer's \\`get\\` and \\`put\\` methods with a simple exponential backoff retry decorator (e.g., from \\`tenacity\\`). Targeting \\`psycopg.OperationalError\\` specifically can help isolate whether the failures are due to transient network issues or a more fundamental state management problem. This often confirms the root cause without requiring a full custom implementation upfront.",
          "score": 2,
          "created_utc": "2026-02-06 05:59:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3we7wj",
              "author": "FunEstablishment5942",
              "text": "i ended up copying this class: [https://github.com/samirpatil2000/agentic-template/blob/main/agents/resilient\\_postgres\\_saver.py](https://github.com/samirpatil2000/agentic-template/blob/main/agents/resilient_postgres_saver.py) what do you think? that should work?\n\n",
              "score": 1,
              "created_utc": "2026-02-06 13:38:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zq0ut",
          "author": "papipapi419",
          "text": "!remindme 5 days",
          "score": 1,
          "created_utc": "2026-02-01 16:20:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zq60m",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 5 days on [**2026-02-06 16:20:31 UTC**](http://www.wolframalpha.com/input/?i=2026-02-06%2016:20:31%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LangChain/comments/1qsxuum/is_asyncpostgressaver_actually_productionready_in/o2zq0ut/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLangChain%2Fcomments%2F1qsxuum%2Fis_asyncpostgressaver_actually_productionready_in%2Fo2zq0ut%2F%5D%0A%0ARemindMe%21%202026-02-06%2016%3A20%3A31%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qsxuum)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-01 16:21:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35htt0",
          "author": "pbalIII",
          "text": "Same pattern plays out in every ORM/framework that wraps database connections... the abstraction handles the happy path but breaks when the connection layer misbehaves.\n\nFWIW issue #5675 is still open with no native fix. Most production deployments I've seen do one of two things:\n\n- Dedicated pool for LangGraph (don't share with SQLAlchemy)\n- Custom retry wrapper that catches PoolClosed/OperationalError and reconnects\n\nThe from_conn_string helper creates a single connection, not a pool. Under load that's asking for trouble. If you're already comfortable with psycopg, building your own thin wrapper around AsyncConnectionPool with health checks is probably less risky than hoping for an upstream fix.",
          "score": 1,
          "created_utc": "2026-02-02 13:33:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qx41gl",
      "title": "Open source trust verification for multi-agent systems",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qx41gl/open_source_trust_verification_for_multiagent/",
      "author": "HolidayCharge1511",
      "created_utc": "2026-02-06 01:28:31",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,  \n  \nI've been working on a problem that's been bugging me: as AI agents start talking to each other (Google's A2A protocol, LangChain multi-agent systems, etc.), there's no way to verify if an external agent is trustworthy.  \n  \nSo I built \\*\\*TrustAgents\\*\\* — essentially a firewall for the agentic era.  \n  \n**What it does:**  \n\\- Scans agent interactions for prompt injection, jailbreaks, data exfiltration (65+ threat patterns)  \n\\- Tracks reputation scores per agent over time  \n\\- Lets agents prove legitimacy via email/domain verification  \n\\- Sub-millisecond scan times  \n  \n**Stack:**  \n\\- FastAPI + PostgreSQL (Railway)  \n\\- Next.js landing page (Vercel)  \n\\- Clerk auth + Stripe billing  \n\\- Python SDK on PyPI, TypeScript SDK on npm, LangChain integration  \n  \n  \nWould love feedback from anyone building with AI agents. What security concerns do you run into?  \n  \n[https://trustagents.dev](https://trustagents.dev)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qx41gl/open_source_trust_verification_for_multiagent/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3u0cvs",
          "author": "AdditionalWeb107",
          "text": "you should look at [https://github.com/katanemo/plano](https://github.com/katanemo/plano) \\- similar ideas but designed to be framework-agnostic. Its a substrate to manage and handle all traffic coming in/out of agents in a protocol-native way",
          "score": 3,
          "created_utc": "2026-02-06 02:38:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu1bqx",
      "title": "AI Agent to deal with enormous datasets",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qu1bqx/ai_agent_to_deal_with_enormous_datasets/",
      "author": "Abject_Reference_160",
      "created_utc": "2026-02-02 17:23:15",
      "score": 6,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "I'm working on a system that implements an AI Agent that analyses the sales history and forecasts future demand.  \nIt is written in NestJS and uses langchain and langchain/openai. The agent is basically declared as follows:  \n  \nconstructor() {  \nthis.chatOpenAI = new ChatOpenAI({  \napiKey: process.env.OPENAI\\_API\\_KEY,  \nmodel: \"gpt-5-mini-2025-08-07\",  \nverbose: true  \n});  \n  }\n\n  \nSo, kinda basic. This is also the first time i'm implementing a complex system with onboard AI, so any tips would be welcome.\n\nThe problem is, i need my ai to be able to read enormous datasets at once, like a really big sales history (it is the biggest part), but I always hit limitations like text too big for sending in a request or it is way past the 128k token limit.  \nI tried using toon, but my agent got confused and returned nothing to an input that normally would generate data.\n\nRAG was an idea for saving tokens but, afaik, it shouldn't be used for calculations like this, but for textual understanding and searches.  \nProducing batch pre compiled analysis was also an option, but it would be really hard to preserve all the insights that are possible with the raw data.\n\nHow can i set it up to reading monstruous datasets like this?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qu1bqx/ai_agent_to_deal_with_enormous_datasets/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3750v7",
          "author": "big_fart_9090",
          "text": "lol, edit: sorry now constructive. An LLM is the wrong tool for what you want to achieve mate. Try asking an LLM on how to do time series forecasting. It wil suggest various data science stuff. Good luck, you will need it",
          "score": 2,
          "created_utc": "2026-02-02 18:20:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o378je4",
              "author": "Abject_Reference_160",
              "text": "Thanks mate, guess i'll really need this luck lol.  \nI was having a talk with grok and it also came to the conclusion that a LLM is optimized more as a conversational thing, and it isn't the same tool used by scientists and such (I initially thought it was more of a setup difference or perfectly thought out strategies that i was also trying to come up with). I'll search more about it and come back with updates.",
              "score": 1,
              "created_utc": "2026-02-02 18:36:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dsloi",
          "author": "Otherwise_Flan7339",
          "text": "You can't pass raw sales history directly to an LLM. It's not a database.  \n  \nFocus on \\*agent tooling\\*. Build tools that run SQL queries or Pandas operations on your dataset. The agent calls these tools based on the user's need.  \n  \nIt processes summarized data, not raw rows. We use this pattern for sales data too.",
          "score": 2,
          "created_utc": "2026-02-03 18:11:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36viux",
          "author": "Empty_Contact_2823",
          "text": "Do you need to ingest all of the historic data at once? Or find the relevant data first to operate on?\n\nTraditional Unix file system access using grep etc to pipe to llm can be a way to counter the token limit.\n\nAlso worth considering if you could filter the data via api calls / function calls within the llm first.",
          "score": 1,
          "created_utc": "2026-02-02 17:38:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36wwfx",
              "author": "Abject_Reference_160",
              "text": "Generally what I do is select the products I want to analyze and get its related data, such as inventories, purchase orders and sales orders.  \nTo ingest all the product's sales history would enable me to perceive patterns, such as which clients use to buy this product, when they buy it and what they get it with, and it would interact better with unstructured data like the info that a client is breaking the contract (how much do they represent for this product's demand and when?).\n\nIs this what you were referring to?",
              "score": 1,
              "created_utc": "2026-02-02 17:44:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o386zfl",
                  "author": "Wide_Brief3025",
                  "text": "It sounds like you're definitely on the right track by tying together sales history, inventory, and contract events for deeper insight. If you start pulling in conversational data or signals from platforms like Reddit or LinkedIn, something like ParseStream can automate tracking keywords and alert you to important discussions so you never miss relevant signals for your products.",
                  "score": 1,
                  "created_utc": "2026-02-02 21:17:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o36xjxb",
              "author": "Abject_Reference_160",
              "text": "i'd like to, for instance, get all the products of a supplier and analyze them at once, or at least several products. At the moment, even 1 product's history is too much, and some suppliers have far more demand than others, which means their products will have a sales history that is even bigger and technically harder to analyze.",
              "score": 1,
              "created_utc": "2026-02-02 17:47:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o383gg3",
                  "author": "snarfi",
                  "text": "You need to query your dataset via an api first to only return whats needed for the particular request.",
                  "score": 1,
                  "created_utc": "2026-02-02 21:01:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o378mit",
          "author": "Strong_Cherry6762",
          "text": "How large is your dataset? How many megabytes of memory does it occupy? How many rows of data records are there?",
          "score": 1,
          "created_utc": "2026-02-02 18:37:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37bese",
              "author": "Abject_Reference_160",
              "text": "I have nearly 5k products, that together have half a million sales orders. Each product has a inventory record for each deposit, and it must all be analysed together in order to suggest sales or transfers between deposits. It is a sales history from 2020 to current day, and it will continue growing, as the system will be used for several years to come.  \nThe model i'm using, gpt-5-mini-2025-08-07, appears to have a token limit of 128.000, and for some products, 1 month of its sales history is way more than enough to break this limit, so considering I need to look to 5 years and more as the time passes, it is kinda big (or at least it looks monstruous for my current setup to handle).\n\nWe are planning to grow a lot on the oncoming years, so more data will be generated on a shorter time.",
              "score": 1,
              "created_utc": "2026-02-02 18:49:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o37fkhq",
                  "author": "Strong_Cherry6762",
                  "text": "Okay, that is absolutely huge for an LLM context window, but actually \"small data\" for a computer.\n\nIf I were you, I'd implement this using a CLI + skill approach—just let the LLM write Python code directly in your terminal, and then use that code to handle the data processing tasks.\n\nOf course, if you do this kind of repetitive work often, I suggest using a \"skill-creator\" (Anthropic has a guide on this: https://resources.anthropic.com/hubfs/The-Complete-Guide-to-Building-Skill-for-Claude.pdf) to build a skill that solidifies your data processing workflow.\n\nGood luck with the build!",
                  "score": 2,
                  "created_utc": "2026-02-02 19:08:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37uxx9",
          "author": "InfraScaler",
          "text": "You need to give your agent tools to filter the data without having to read it first, e.g. ways to query the dataset. ",
          "score": 1,
          "created_utc": "2026-02-02 20:20:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3c4enn",
              "author": "Abject_Reference_160",
              "text": "Is it somehow different to use tools and to send the same data in the prompt?",
              "score": 1,
              "created_utc": "2026-02-03 13:15:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3cnsdb",
                  "author": "InfraScaler",
                  "text": "It's about managing context bloat. Instead of letting the agent read all the data then filter, you tell the agent it can use tools (for the sake of argument: put it in a sqlite and query it using sql) to send queries and get subsets of the data already filtered.",
                  "score": 2,
                  "created_utc": "2026-02-03 15:00:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3nbueh",
          "author": "Potential-Analyst571",
          "text": "Don’t feed huge raw datasets to the agent.. Preaggregate and chunk the data first, then let the model reason on summaries instead of rows. Keeping that data flow clearly defined (tools like Traycer help) avoids token limits and confused agents.",
          "score": 1,
          "created_utc": "2026-02-05 02:32:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxa5ip",
      "title": "Built a Website Crawler + RAG (fixed it last night 😅)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qxa5ip/built_a_website_crawler_rag_fixed_it_last_night/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-06 06:27:22",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I’m **new to RAG** and learning by building projects.  \nAlmost **2 months ago** I made a very simple RAG, but the **crawler & ingestion were hallucinating**, so the answers were bad.\n\nYesterday night (after office stuff 💻), I thought:  \nEveryone is feeding PDFs… **why not try something that’s not PDF ingestion?**\n\nSo I focused on fixing the **real problem — crawling quality**.\n\n🔗 GitHub: [https://github.com/AnkitNayak-eth/CrawlAI-RAG](https://github.com/AnkitNayak-eth/CrawlAI-RAG)\n\n**What’s better now:**\n\n* Playwright-based crawler (handles JS websites)\n* Clean content extraction (no navbar/footer noise)\n* Smarter chunking + deduplication\n* RAG over **entire websites**, not just PDFs\n\nBad crawling = bad RAG.\n\nIf you all want, **I can make this live / online** as well 👀  \nFeedback, suggestions, and ⭐s are welcome!",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qxa5ip/built_a_website_crawler_rag_fixed_it_last_night/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3vh34k",
          "author": "Ok_Signature_6030",
          "text": "the \"bad crawling = bad RAG\" insight is spot on and something a lot of people skip over. most tutorials jump straight to chunking strategy or retrieval tuning but if your source data is garbage none of that matters.\n\none thing i noticed looking at the repo... the README mentions BeautifulSoup for scraping but your post says Playwright-based. did you switch between versions? because that distinction actually matters a lot for production use. BS4 is fine for static content but if you're targeting JS-heavy sites (SPAs, dynamic dashboards), Playwright is worth the overhead.\n\nthe ChromaDB + Sentence-Transformers + Groq stack is solid for a learning project. if you do make it live, watch out for near-duplicate pages (like paginated content or URL params) polluting your index... a simple content hash before embedding can save you a lot of headaches there.\n\ncool project for 2 months in.",
          "score": 2,
          "created_utc": "2026-02-06 09:26:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vl9ev",
              "author": "Cod3Conjurer",
              "text": "Good catch - my bad\n\n\nInitially it was BeautifulSoup-only for static pages. When I revisited it last night, I switched to Playwright + BS4 because JS-heavy sites were killing extraction quality. That distinction definitely matters.\nAlso agreed on near-duplicates - I'm doing content normalization + hashing before embedding for now, and improving pagination/ URL param handling next.\n\n\nAnd yeah - not really 2 months of active work More like built a rough version, got lazy, realized the crawling was garbage, fixed it properly last night. \n\n\nAppreciate the detailed feedback 🙏",
              "score": 1,
              "created_utc": "2026-02-06 10:06:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3vooed",
                  "author": "Ok_Signature_6030",
                  "text": "nice, the playwright + bs4 combo is solid for that... content hashing before embedding is smart too, avoids wasting vector space on near-identical chunks. good luck with the pagination stuff, that's usually where the edge cases get annoying",
                  "score": 1,
                  "created_utc": "2026-02-06 10:37:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qurqb7",
      "title": "Code vs. Low-Code for AI Agents: Am I over-engineering my \"Social Listening\" swarm?",
      "subreddit": "LangChain",
      "url": "https://i.redd.it/mqrwi0uq5ahg1.png",
      "author": "Tzipi_builds",
      "created_utc": "2026-02-03 13:20:06",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qurqb7/code_vs_lowcode_for_ai_agents_am_i/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3hqdso",
          "author": "emmy_talks_reddit",
          "text": "What's the issue exactly? Are you worried your solution isn't good enough compared to low-code solutions? \n\nAre you worried about writing code and architecting a solution (even though you're a software architect?) \n\nSorry but the post just doesn't add up. Whats the problem exactly?",
          "score": 1,
          "created_utc": "2026-02-04 07:24:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i3zaj",
              "author": "Tzipi_builds",
              "text": "Fair question! It’s definitely not a doubt about my ability to architect or code the solution.\n\nThe 'issue' is more about Opportunity Cost and Maintenance Debt. As an architect, I’m constantly weighing whether I should spend my time building 'plumbing' (orchestration, memory management, handoffs) or focusing on the core business logic of my agents.\n\nI’m seeing a massive explosion in stable, low-code/specialized agent frameworks and methodologies like ABC TOM (Agents, Brain, Context, Tools, Output, Memory) that promise to handle the infra out-of-the-box.\n\nThe core of my dilemma is:\n\n* If I build it custom in LangGraph/FastAPI now, will I be stuck maintaining a custom-coded infrastructure in 6 months that could have been a 5-minute configuration in a more stable, specialized tool?\n* Is the 'Control' I get from pure code actually a competitive advantage for a Social Listening tool like SidKick, or is it just 'gold-plating' the engine?\n\nI haven't gone 'all-in' on the implementation yet, so I’m trying to gauge if others feel that custom-coded orchestration is becoming a liability rather than an asset as the ecosystem matures.\n\nWould love to hear how you decide where to draw the line between 'Building the Engine' and 'Using the Car'.",
              "score": 1,
              "created_utc": "2026-02-04 09:31:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3iae42",
                  "author": "Wide_Brief3025",
                  "text": "Focusing on business logic over infrastructure is almost always the better use of your time, especially as these specialized frameworks get more robust. Maintenance debt for custom orchestration can spiral fast. For social listening specifically, a tool like ParseStream can handle the real time monitoring and lead discovery side, so you can spend your energy on differentiated features instead of background plumbing.",
                  "score": 2,
                  "created_utc": "2026-02-04 10:31:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}