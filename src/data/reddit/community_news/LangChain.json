{
  "metadata": {
    "last_updated": "2026-01-19 08:49:34",
    "time_filter": "week",
    "subreddit": "LangChain",
    "total_items": 20,
    "total_comments": 90,
    "file_size_bytes": 131456
  },
  "items": [
    {
      "id": "1qfkeuf",
      "title": "We tested Vector RAG on a real production codebase (~1,300 files), and it didn‚Äôt work",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qfkeuf/we_tested_vector_rag_on_a_real_production/",
      "author": "Julianna_Faddy",
      "created_utc": "2026-01-17 18:12:58",
      "score": 52,
      "num_comments": 17,
      "upvote_ratio": 0.86,
      "text": "Vector RAG has become the default pattern for coding agents: embed the code, store it in a vector DB, retrieve top-k chunks that it feels obvious to do so.\n\nWe tested this on a real production codebase (\\~1,300 files) and it mostly‚Ä¶ didn‚Äôt work.\n\nThe issue isn‚Äôt embeddings or models but we realized that **similarity is a bad proxy for relevance in code**.\n\nIn practice, vector RAG kept pulling:\n\n* test files instead of implementations\n* deprecated backups alongside the current code\n* unrelated files that just happened to share keywords\n\nSo the agent‚Äôs context window filled up with noise and reasoning got worse.\n\nhttps://preview.redd.it/39j5yotaaydg1.png?width=1430&format=png&auto=webp&s=7fd32a52a167a6b6f16e565874a2c5baab4ddc93\n\nWe compared this against an **agentic search approach using context tree** (structured, intent-aware navigation instead of similarity search). We won‚Äôt dump all the numbers here, but a few highlights:\n\n* **Orders of magnitude fewer tokens per query**\n* **Much higher precision on ‚Äúwhere is X implemented?‚Äù questions**\n* **More consistent answers for refactors and feature changes**\n\nVector RAG did slightly better on recall in some cases, but that mostly came from dumping more files into context, which turned out to be actively harmful for reasoning.\n\nThe takeaway for me:\n\nCode isn‚Äôt documentation but it's a graph with structure, boundaries, and dependencies. If being treated like a bag of words, it will break down fast once the repo gets large.\n\nI wrote a [detailed breakdown](https://www.byterover.dev/blog/why-vector-rag-fails-for-code-we-tested-it-on-1-300-files) of the experiment, failure modes, and why context trees work better for code (with full setup in this [repo ](https://github.com/RyanNg1403/agentic-search-vs-rag)and metrics) here if you want the full take.\n\nLet me know if you've found better approach",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qfkeuf/we_tested_vector_rag_on_a_real_production/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o05cnsy",
          "author": "Disneyskidney",
          "text": "Interesting! Though I feel like the like the industry standard for RAG on a codebase has now become grep and other terminal commands. Would like to see it benchmarked against that, or see how they can work in tandem.",
          "score": 12,
          "created_utc": "2026-01-17 18:29:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05mce6",
              "author": "Challseus",
              "text": "I think that's the key. Combinations of semantic search/hybrid RAG, along with how agents build up context with grep/cat.",
              "score": 6,
              "created_utc": "2026-01-17 19:14:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o05kd0n",
          "author": "OnyxProyectoUno",
          "text": "Yeah this matches what I've seen. Vector similarity works when semantic proximity actually means relevance, which it does for docs but not for code.\n\nThe test file problem is brutal. Tests and implementations share so much vocabulary that embeddings can't distinguish them. Same with deprecated code sitting next to current versions. Structurally they're different, semantically they're nearly identical.\n\nYour context tree approach makes sense because code has explicit structure that embeddings throw away. Import graphs, call hierarchies, module boundaries. All that gets flattened into a vector.\n\nOne thing I'd add: the preprocessing step matters more than people realize. How you chunk code affects what gets retrieved. Function-level chunks vs file-level vs arbitrary token windows all produce different failure modes. I've been building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_i) to make that visible, being able to see what your chunks actually look like before embedding catches a lot of issues early.\n\nFor hybrid approaches, have you tried combining your tree navigation with vector search for specific cases? Like using structure for \"where is X implemented\" but falling back to similarity for \"find similar patterns to this function\"? Curious if there's a sweet spot.",
          "score": 7,
          "created_utc": "2026-01-17 19:04:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05pwgl",
          "author": "lundrog",
          "text": "What I am currently using and made\n\nhttps://preview.redd.it/5vdslgfjoydg1.jpeg?width=2816&format=pjpg&auto=webp&s=9180d887ade94e5a92554342f734ec0be67a200d",
          "score": 3,
          "created_utc": "2026-01-17 19:31:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0aywxz",
              "author": "ExtentHot9139",
              "text": "Mmmh bloated no ?",
              "score": 1,
              "created_utc": "2026-01-18 15:33:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0b29gt",
                  "author": "Crafty_Disk_7026",
                  "text": "ExtremelyZ. I garantee just running Claude in your codebase will give you better results than this mondtrosity",
                  "score": 2,
                  "created_utc": "2026-01-18 15:49:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o06gven",
          "author": "kpgalligan",
          "text": "> Vector RAG has become the default pattern for coding agents\n\nHas it? I've been building a focused agent and haven't even touched it. Everything I've seen in the past year+ was \"don't use RAG\". The agents tend to find what they need directly. At least that's been my experience. If I wanted to do something like this, I'd probably start a distinct conversation, use RAG to potentially highlight some hits, have a model review the results and pull out what the original model actually wanted. Keeps the noise out of the main context.\n\nIn our case, the agent just uses a combination of file search tools. Not that I'd be opposed to trying something else, but RAG just seemed like it wouldn't do a whole lot better.\n\nWe have a similar solution for searching and grabbing web content. Some agents do a \"web fetch\" that grabs a URL, pushes it through a markdown converter, then returns the content. That can fill up the context with useless info. Instead, we have a \"research\" tool that takes a detailed description of what the LLM wants, does a web search and content download, then extracts what the model is actually looking for in a concise \"report\".",
          "score": 3,
          "created_utc": "2026-01-17 21:47:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06d9wc",
          "author": "CriticalBandicoot27",
          "text": "Firstly very insightful! I also had similar findings. For the code part i feel like the MIT rlm research paper seems promising to tackle the codebase problem. I am yet to implement it to see the actual results, but I feel like using chunks as a variable to test the actual relevance might fix most of these issues.",
          "score": 2,
          "created_utc": "2026-01-17 21:29:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o096xer",
          "author": "ClinchySphincter",
          "text": "is this an ad?",
          "score": 2,
          "created_utc": "2026-01-18 07:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0a0ght",
              "author": "Glad-Champion5767",
              "text": "I've seen this exact post 1:1 a week ago or so. I thought i was getting a dejavu. Its definitely a bot post atleast, with some bot replies.",
              "score": 1,
              "created_utc": "2026-01-18 12:00:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o09kten",
          "author": "WarlaxZ",
          "text": "If you want to solve this problem for code, you need to think like an ide. They didn't build up knowledge base of code, they added 'find references' and auto complete method name to existing ones, and automatic import etc. Refactoring and moving methods with a single click and pointing at a file. Solve these things and it will work better and more efficiently, as we've already been here many moons ago. I made a simple refactor mcp that accepted method name and method above and below to allow methods to be moved around files easily without needing to rewrite 3/4 of the file as a diff to great results, and there's so many things we already solved with ide's that have yet to be implemented for the new ai coding tools",
          "score": 2,
          "created_utc": "2026-01-18 09:38:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o09c93h",
          "author": "-Cubie-",
          "text": "Honestly, I don't really buy it. Your big \"gain\" is 130x less token use. Now tell me: how would a different retrieval approach yield so much less tokens? You could, after all, have both approaches return the same number of documents. That would help make it a fair comparison.\n\nBut you don't do that.\n\nI think you spent much more time on optimizing your (presumably paid) product that you're advertising here and purposefully created a poor baseline so your product looks better.",
          "score": 1,
          "created_utc": "2026-01-18 08:19:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0apoca",
          "author": "_thedeveloper",
          "text": "Interesting findings.\n\nFrom my experience, pure semantic similarity is almost guaranteed to fail at this scale. You really need a hybrid approach that combines metadata (paths, ownership, recency, file type, dependencies) with semantic signals to get anything reliable for code.\n\nPreprocessing also matters a lot here. Blind chunking or fixed-length chunks tend to bloat context and amplify noise, especially in large repos. Without structure-aware chunking, retrieval quality degrades quickly.\n\nAST-based approaches help, but they‚Äôre not sufficient on their own. Code understanding is repo-specific ‚Äî effective chunking and retrieval usually need to align with the project‚Äôs architecture and conventions. That means maintenance and institutional knowledge of the codebase become first-class concerns, not implementation details.\n\nCurious if you experimented with metadata-weighted retrieval or repo-aware chunking alongside the context tree approach.",
          "score": 1,
          "created_utc": "2026-01-18 14:46:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ayszn",
          "author": "ExtentHot9139",
          "text": "Personally, I always craft my context window manually and ended automating it. I built code2prompt check it out.\n\nThe idea is simple: select relevant file, flatten them in a big file that you can just dump in a LLM chat.\n\nIt makes coding with agents stateless, semi auto and focused.",
          "score": 1,
          "created_utc": "2026-01-18 15:32:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0b2ht5",
          "author": "Crafty_Disk_7026",
          "text": "You can not do RAG with code you need specialized format like ast so you can actually reason about the code.  Finding 2 functions with similar names that do completely different things is meaningless and the kind of garbage you'll get doing rag to understand code",
          "score": 1,
          "created_utc": "2026-01-18 15:50:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g1tbi",
          "author": "Creepy-Row970",
          "text": "pretty interesting read",
          "score": 1,
          "created_utc": "2026-01-19 08:39:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06s2rm",
          "author": "BeerBatteredHemroids",
          "text": "I've built a production vector rag application on over twice that amount of documents and it works just fine. You can't just slap a chatbot on a vector store and expect it to work out of the box. \n\nYou need smart prompting, reranking, good chunking strategy and a quality embedding model. Also, you should probably use a hybrid search combining similarity and keyword search.\n\nBasically, you need to be smarter than the tools you're working with boo",
          "score": 1,
          "created_utc": "2026-01-17 22:43:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfcwe0",
      "title": "Really Bad Etiquette from Langchain maintainers",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qfcwe0/really_bad_etiquette_from_langchain_maintainers/",
      "author": "Total-Chef-420",
      "created_utc": "2026-01-17 13:13:33",
      "score": 32,
      "num_comments": 15,
      "upvote_ratio": 0.86,
      "text": "I have tried contributing to langchain's ecosystem multiple times and both times my commits were taken by the maintainers, added a bunch of extra things on it and immediately raised a new PR without any kind of attribution.\n\nIs this how langchain expects people to contribute to their repository ?\n\n  \nThis has happened twice (and even maintainers acknowledged it)\n\n  \n1. [https://github.com/langchain-ai/deepagents/pull/713](https://github.com/langchain-ai/deepagents/pull/713)\n\n2. [https://github.com/langchain-ai/deepagentsjs/pull/84](https://github.com/langchain-ai/deepagentsjs/pull/84)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qfcwe0/really_bad_etiquette_from_langchain_maintainers/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o042d9m",
          "author": "vtrivedy-lc",
          "text": "Hey OP, one of the maintainers from LangChain‚Äôs deepagents here, really sorry to hear that and our apologies.  Closing PRs and opening new ones that could‚Äôve been built on existing community PRs is never our intention.\n\nWe do get A LOT of PRs so it‚Äôs very possible we‚Äôve made this mistake and would love to make it right with proper attribution.  If you want to DM or link it, I‚Äôll take a look at how to give attribution.\n\nWe‚Äôre incredibly grateful for the awesome LangChain community, great contributions and ideas that push our libraries to be better for users.  Will try to make this right, sorry about the bad experience!",
          "score": 17,
          "created_utc": "2026-01-17 14:48:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04e1o6",
              "author": "Total-Chef-420",
              "text": "PR for deepagentsjs: [https://github.com/langchain-ai/deepagentsjs/pull/84](https://github.com/langchain-ai/deepagentsjs/pull/84)\n\n  \nPR for deepagents : [https://github.com/langchain-ai/deepagents/pull/713](https://github.com/langchain-ai/deepagents/pull/713)",
              "score": 13,
              "created_utc": "2026-01-17 15:47:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o04fq6q",
                  "author": "vtrivedy-lc",
                  "text": "Thanks for linking!  See the PR across both, the deepagents one was a duplicate of a previous open PR that the team built on and added testing.  But totally see your frustration, we‚Äôll add attribution for the work!  \n\nAnd we will definitely be better in the future.  Would love feedback on the current PR process.  We‚Äôre exploring using discussions more to make sure that external PRs are linked directly to plans for implementing.  We love the PRs but understand it takes a lot of time and is then frustrating if the work isn‚Äôt used.  This way we can confirm the direction of the work.\n\nNothing decided but wanted to get feedback if anyone here has thoughts.",
                  "score": 5,
                  "created_utc": "2026-01-17 15:55:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o04eks7",
                  "author": "TalosStalioux",
                  "text": "Upvoting for you",
                  "score": 1,
                  "created_utc": "2026-01-17 15:50:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o03ngca",
          "author": "pokemonplayer2001",
          "text": "Do you have links to the PRs in question?",
          "score": 2,
          "created_utc": "2026-01-17 13:25:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04e708",
              "author": "Total-Chef-420",
              "text": "Yes, I have updated in the above comment",
              "score": -1,
              "created_utc": "2026-01-17 15:48:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o060khe",
          "author": "No_Inspection4415",
          "text": "Your PRs are pretty useless, to be honest.",
          "score": 0,
          "created_utc": "2026-01-17 20:24:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o061as9",
              "author": "Total-Chef-420",
              "text": "Simple yes, but useless no.\n\nThey fixed an issue which was open since multiple days.",
              "score": 1,
              "created_utc": "2026-01-17 20:27:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o06574y",
                  "author": "No_Inspection4415",
                  "text": "Isn't it [https://github.com/langchain-ai/deepagents/pull/612/files](https://github.com/langchain-ai/deepagents/pull/612/files) a different implementation? Did you run the validation they asked you to? Honestly, unless this fix is very difficult to find, having to work with \"your\" solution is not a net positive for them...\n\nWhatever, I do not like the framework anyway and I get your point, you did the research which is not complicated to do, but you did it. You do deserve credit, of course (not that being a part of this framework is a huge honor, it is not like you closed a PR for the K8s ;P).\n\nEdit: sorry, this [https://github.com/langchain-ai/deepagents/pull/803](https://github.com/langchain-ai/deepagents/pull/803) \\- they work really weirdly.",
                  "score": 0,
                  "created_utc": "2026-01-17 20:48:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qd4q52",
      "title": "Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/0hoy86u9sedg1",
      "author": "DeathShot7777",
      "created_utc": "2026-01-15 00:36:33",
      "score": 28,
      "num_comments": 28,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qd4q52/building_opensource_client_sided_code/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzpsvl3",
          "author": "ConcertTechnical25",
          "text": "The next frontier of dev tools isn't \"AI in the Cloud,\" but \"Intelligence in the Browser.\" Shipping the entire vector backend and graph query layer via WASM is a game-changer for IP protection. When the code never leaves the client's machine, the barrier for enterprise adoption disappears. The combination of LanceDB for local vector storage and a structural graph layer for dependency mapping is exactly the kind of \"Neural Core\" we need for the next generation of autonomous coding agents.",
          "score": 11,
          "created_utc": "2026-01-15 12:23:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpuz7s",
              "author": "DeathShot7777",
              "text": "Yes. Thats Y i am also trying to make it work through ollama as a provider. Its tough making cyfer generation accurate with SLMs though",
              "score": 0,
              "created_utc": "2026-01-15 12:38:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzpx4a1",
              "author": "DeathShot7777",
              "text": "Ip protection especially with the shit show going on with anthropic nowdays üòÆ‚Äçüí®",
              "score": 0,
              "created_utc": "2026-01-15 12:52:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzn9woy",
          "author": "o5mfiHTNsH748KVq",
          "text": "Hold my beer, I'm gonna throw Unreal Engine at it.",
          "score": 3,
          "created_utc": "2026-01-15 00:56:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzna5yp",
              "author": "DeathShot7777",
              "text": "STOP, the DB engine runs inbrowser and it supports js ts and python right now. It will explode ;-;\n\nShould have mentioned that in the post ;-;",
              "score": 2,
              "created_utc": "2026-01-15 00:58:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzncpaw",
                  "author": "o5mfiHTNsH748KVq",
                  "text": "lol yes that's important information :D\n\nI starred it. This seems like a great project.",
                  "score": 3,
                  "created_utc": "2026-01-15 01:12:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nznapf9",
              "author": "DeathShot7777",
              "text": "Didnt know Unreal engine was opensource, wow!",
              "score": 2,
              "created_utc": "2026-01-15 01:01:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nztzp0h",
          "author": "nineblog",
          "text": "What library is used for this front-end view? It looks very silky",
          "score": 2,
          "created_utc": "2026-01-16 00:40:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzu06pl",
              "author": "DeathShot7777",
              "text": "The graph is made using sigma js combining with force atlas 2 ( physics simulation engine ) and lots of hit and trial with the repulsion so that they dont clump up. \n\nRest of the UI is courtesy of opus 4.5üôè",
              "score": 1,
              "created_utc": "2026-01-16 00:43:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o08dl14",
          "author": "Dizzy_Beat_2422",
          "text": "Whaat the helllll? What is this???",
          "score": 2,
          "created_utc": "2026-01-18 03:51:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09ejc7",
              "author": "DeathShot7777",
              "text": "üòÖ That virus looking thing is a knowledge graph",
              "score": 1,
              "created_utc": "2026-01-18 08:40:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o09tgfd",
                  "author": "Dizzy_Beat_2422",
                  "text": "Wow.. can i use it? ;)",
                  "score": 2,
                  "created_utc": "2026-01-18 10:58:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nznxuuw",
          "author": "DaRandomStoner",
          "text": "This is amazing work... seriously thank you so much. Testing it out now but looking at the repo has me excited!\n\nEdit: Again can't thank you enough. Put together a PR for you with what I found while integrating it. This is such a great idea. Works like a charm in case anyone is wondering.",
          "score": 1,
          "created_utc": "2026-01-15 03:15:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzohoz8",
              "author": "DeathShot7777",
              "text": "Really appreciated it. Feel free to DM if u have some cool idea or need help integrating",
              "score": 2,
              "created_utc": "2026-01-15 05:29:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0bn404",
                  "author": "DaRandomStoner",
                  "text": "Integrated nicely. Now my llm updates this knowledge graph at the start of the session and uses it for more dynamic search options of my code base via custom tool calls. Working great thanks again.",
                  "score": 1,
                  "created_utc": "2026-01-18 17:28:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzp3f7k",
          "author": "hopes_alive123",
          "text": "mainframe support similar to python js ?",
          "score": 1,
          "created_utc": "2026-01-15 08:37:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp5341",
              "author": "DeathShot7777",
              "text": "Hmm good idea. Might be useful for people working on mainframe migration/audits etc",
              "score": 2,
              "created_utc": "2026-01-15 08:53:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzv9wgd",
          "author": "Analytics-Maken",
          "text": "Cool work on the MCP integration. For the cypher generation issue with SLMs, adding clear examples in your prompts helps a lot. Show the model what good queries look like, then add a check layer to catch bad outputs before they run. \n\nHave you considered expanding to data analysis? It could combine with business metrics via ETL tools like Windsor ai or MCP servers and find root causes, what drives performance, etc.",
          "score": 1,
          "created_utc": "2026-01-16 05:10:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvalef",
              "author": "DeathShot7777",
              "text": "Yeah, firstly working on the MCP and next I m experimenting on what if the agent can place certain information in the graph itself for its own reference maybe like a graph based todo and notes etc. This can be useful for long running task like codebase audits for compliances like SOC2 etc. Also adding in business logic into the graph along with code relations will make the agent lot more aware, great for onboarding, identifying gaps and bottlenecks.\n\nLot of work there üòÆ‚Äçüí® but interesting",
              "score": 1,
              "created_utc": "2026-01-16 05:15:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qgdgq5",
      "title": "LangGraph/workflows vs agents: I made a 2-page decision sheet. What would you change?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/gallery/1qgdgq5",
      "author": "OnlyProggingForFun",
      "created_utc": "2026-01-18 16:45:28",
      "score": 27,
      "num_comments": 1,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qgdgq5/langgraphworkflows_vs_agents_i_made_a_2page/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0be227",
          "author": "OnlyProggingForFun",
          "text": "If anyone wants the PDF version, I can share it directly too :)",
          "score": 1,
          "created_utc": "2026-01-18 16:45:40",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdufx4",
      "title": "Honest question: What is currently the \"Gold Standard\" framework for building General Agents?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qdufx4/honest_question_what_is_currently_the_gold/",
      "author": "Strong_Cherry6762",
      "created_utc": "2026-01-15 20:14:21",
      "score": 23,
      "num_comments": 32,
      "upvote_ratio": 0.87,
      "text": "Hi everyone,\n\nI'm a beginner developer diving into AI agents. My goal is to build a solid General Agent, but I want to make sure I start with the right tools.\n\nI keep hearing about LangGraph, but before I commit to learning it, I really want to know what the community considers the actual \"best\" framework right now.\n\nHere is what I‚Äôm hoping to learn from your experience:\n\n1. The #1 Recommendation: If you were starting a new project today, which framework would you choose and why? Is there a clear winner?\n2. LangGraph Reality Check: Is LangGraph truly the best option for a general-purpose agent, or is it overkill/too complex for a starter? What are its main pros and cons?\n3. General Best Practices: Regardless of the framework, what are the most important principles for building a stable agent?\n\nI‚Äôm looking for a solution that balances power with ease of use. Thanks for pointing me in the right direction!",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qdufx4/honest_question_what_is_currently_the_gold/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nzt57if",
          "author": "caprica71",
          "text": "Honestly the framework just disappears from thought most of the time once you get arms deep into a problem.   Most of my life is in evals and integrations.  Just pick a really popular one and get going.   If the agent frame work gets in the way swap it.",
          "score": 18,
          "created_utc": "2026-01-15 22:01:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztqakt",
              "author": "scar0x00",
              "text": "What do you use for evals?",
              "score": 4,
              "created_utc": "2026-01-15 23:49:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o017tve",
                  "author": "caprica71",
                  "text": "Langfuse for the some parts.   I have some custom python for other evals.",
                  "score": 1,
                  "created_utc": "2026-01-17 01:52:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0anbcn",
                  "author": "sunglasses-guy",
                  "text": "We use Confident AI for production monitoring + tracing",
                  "score": 0,
                  "created_utc": "2026-01-18 14:33:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o00uses",
              "author": "AdditionalWeb107",
              "text": "how do you easily swap it when you have gone down the rabbit hotel of its concepts? Like CrewAI is so hard to unpack and move into LangChain. I would prefer we go stock python for most things and think through clean separation of concerns.",
              "score": 1,
              "created_utc": "2026-01-17 00:31:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o016e6d",
                  "author": "caprica71",
                  "text": "Basically you nailed it : use stock python as much as possible and good separation of concerns.   My langraph application has very light nodes. I use langchain sparingly.",
                  "score": 2,
                  "created_utc": "2026-01-17 01:42:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzt0mn2",
          "author": "cordialgerm",
          "text": "Look into Langchain 1.0. you can build a basic agent harness with  a single function call. Langgraph is more for complex / low level use-cases.\n\nI think it's less about the particular framework and more about the  tools, evals, data, etc that you inject into the framework that makes or breaks your agent.",
          "score": 5,
          "created_utc": "2026-01-15 21:40:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzupspl",
              "author": "brucebay",
              "text": "I suggest langgraph. It is easier and in most cases more flexible.¬†",
              "score": 2,
              "created_utc": "2026-01-16 03:06:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzt11hq",
              "author": "Strong_Cherry6762",
              "text": "Thanks~",
              "score": 1,
              "created_utc": "2026-01-15 21:42:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzuplr1",
          "author": "dash_bro",
          "text": "If you're busy starting out, learn langchain/langgraph. They're okay okay to learn, but hard to deploy at scale in production : so just keep that in mind. \n\n- learn what you want to do first\n- you'll maybe realize it isn't the best fit and want to check out what other library offers these features etc.\n\nFWIW I started with autogen -> crew -> now I just do it custom myself",
          "score": 2,
          "created_utc": "2026-01-16 03:04:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw7xba",
              "author": "Born_Owl7750",
              "text": "Why is it hard to deploy in production? What issues have you faced?\nWe currently are in the Microsoft ecosystem and use semantic kernel or native sdk. I was thinking of switching to langchain and langgraph due to the sheer amount of features and updates they have.",
              "score": 1,
              "created_utc": "2026-01-16 09:58:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzwf0dr",
                  "author": "dash_bro",
                  "text": "Biggest problem is bloat and noQA features. The bloat especially throws random problems when you're hosting multiple embedding models + serving to a few users live.\n\nMemory management (RAM/VRAM) harnesses are a nightmare. Bloat around using external integrations without plugging in langchain specific adapters - again, creates non DRY work. The ecosystem is good for when flexibility isn't required fully.\n\nIt's often much better to experiment with, then build for specced out features yourself.",
                  "score": 1,
                  "created_utc": "2026-01-16 11:01:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvbz8a",
          "author": "Playful_Criticism425",
          "text": "People are sleeping on PydanticAI",
          "score": 2,
          "created_utc": "2026-01-16 05:25:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw5xm8",
              "author": "FlowLab99",
              "text": "Do you mean that Pydantic team is not moving quickly or responsive? Just curious m.",
              "score": 1,
              "created_utc": "2026-01-16 09:40:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0231my",
                  "author": "Playful_Criticism425",
                  "text": "It's actually good. For agentic solutions and it is Pythonic and    more deterministic than n8n",
                  "score": 1,
                  "created_utc": "2026-01-17 05:20:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzuef3v",
          "author": "attn-transformer",
          "text": "Honestly none of them. They‚Äôre all experiments. I‚Äôd start with Agno or LangGraph.",
          "score": 3,
          "created_utc": "2026-01-16 02:02:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvz87k",
              "author": "nbvehrfr",
              "text": "Agno is good.¬†",
              "score": 2,
              "created_utc": "2026-01-16 08:37:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzv23w0",
          "author": "Khade_G",
          "text": "There isn‚Äôt a single best framework right now‚Ä¶ the best choice depends on how complex your agent actually needs to be.\n\nIf I were starting today, I‚Äôd pick the simplest thing that can ship, and only upgrade when I feel pain. For most beginners, that‚Äôs a lightweight agent loop (tools + memory + logging) using whatever SDK you‚Äôre already comfortable with. LangChain can get you moving fast. LangGraph is great when you need control: multi-step workflows, branching, retries, and state you can inspect. But for a ‚Äúgeneral agent‚Äù as a first project, it can feel like overkill because you end up building a workflow engine before you‚Äôve proven the agent needs one.\n\nLangGraph pros: more reliable orchestration, easier debugging of state, safer retries, and it scales better once you have multiple steps/roles. Cons: more structure, more upfront thinking, and it‚Äôs slower to iterate if you‚Äôre still learning what your agent should even do.\n\nBest practices that matter more than framework: keep context small and explicit, log everything (inputs/tool calls/outputs), design for failure (timeouts, retries, fallbacks), and don‚Äôt let the agent ‚Äúfree roam‚Äù‚Ä¶ give it clear goals, constraints, and stopping conditions. If you do those, your agent will be stable in almost any framework.",
          "score": 3,
          "created_utc": "2026-01-16 04:19:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzubzmd",
          "author": "siddharthnibjiya",
          "text": "Claude code is now considered the best agentic structure / style to be followed.\n\nRelated but not my only reason for saying this:\n\nhttps://x.com/amasad/status/2011475533369131424?s=46",
          "score": 3,
          "created_utc": "2026-01-16 01:49:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztdbp2",
          "author": "Challseus",
          "text": "Apologies in advance, this may or may not help your cause, but... I worked on a multi-agent setup with Lang Graph close to a year ago. There was the one **Primary Agent** that would pass off the request to other agents (with their own instructions), depending on some business logic. 8-9 agents total. Image analysis, tools, RAG, everything. It worked. It had many bugs, but that was a skill issue thing, not Lang Graph's fault.\n\nWe used **gpt-4o.**\n\nSame company, CEO now wants this logic back in a new product. It's basically the same thing, except one major thing:\n\nNo Lang Graph, just Langchain. Just a single agent, one prompt, and basic conversation history. It handles everything MUCH BETTER.\n\nWe're using **gpt-4.1-mini.**\n\nLong story short, TL;DR, whatever, just go with **Langchain 1.0.** It has great support for built in RAG pipeline stuff to get you started, as well as conversation history.\n\nAlso checkout [https://langfuse.com/](https://langfuse.com/) or [https://smith.langchain.com/](https://smith.langchain.com/) for observability.",
          "score": 1,
          "created_utc": "2026-01-15 22:41:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzte0y5",
          "author": "Bohdanowicz",
          "text": "It depends what you are building.. lang is great when you need a stategraph, adk is great for a2a/subagents out of the box",
          "score": 1,
          "created_utc": "2026-01-15 22:44:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvplkv",
          "author": "Ambitious-Most4485",
          "text": "We are using in production google adk, but langgraph was an option we did consider.\n\nAdk has some flaws (for example we needed to log chunks and was a nightmare navigating the state).\n\n Observability is a must i recommend Phoenix arize (really easy integration)",
          "score": 1,
          "created_utc": "2026-01-16 07:12:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvwgel",
          "author": "KallistiTMP",
          "text": "It's still the wild west out there. All agentic frameworks suck in several dimensions, and often suck differently with different models and tasks. There is no consensus on what the best overall high level approach is even, it's an active area of research.\n\nDon't overthink it, any choice you make is gonna be wrong and require a major overhaul or full replacement in a couple years anyway.",
          "score": 1,
          "created_utc": "2026-01-16 08:12:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzw1kub",
          "author": "Disneyskidney",
          "text": "I went to hackathon for something like this. Both first place, second place, and third place all used Claude code. It‚Äôs surprising how general coding agents are becoming. \n\nI actually just used cursor for lead generation lol.",
          "score": 1,
          "created_utc": "2026-01-16 08:59:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzw1r29",
          "author": "Disneyskidney",
          "text": "Check out DSPy and RLMs",
          "score": 1,
          "created_utc": "2026-01-16 09:01:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwvtia",
          "author": "No-Fail-7644",
          "text": "Plain, old java. I found vertx with quarkus running on java 21 to be extremely versatile if reactive patterns are implemented properly. Langchain4j for agents.",
          "score": 1,
          "created_utc": "2026-01-16 13:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o008sg8",
          "author": "Strong_Worker4090",
          "text": "No real ‚Äúgold standard‚Äù yet. Agents are still kinda messy, and most people end up with a thin framework + their own glue.\n\nIf I‚Äôm starting today:\n\n1. LangGraph if you expect multi-step flows, branching, retries, human-in-the-loop. I moved a project from a messy ‚Äúagent loop‚Äù to LangGraph and it got way easier to debug. Downside: more setup, can feel overkill early.\n2. If you want simpler: plain LangChain (or even just your own wrapper) is fine for v1.\n3. If your agent is mostly RAG: LlamaIndex or Haystack.\n4. If you‚Äôre doing ‚Äúmultiple agents with roles‚Äù: AutoGen or CrewAI.\n\nBiggest mistake I made early was chasing frameworks instead of basics. What actually made things stable was: fewer tools, hard timeouts, good logs, and input/output checks (guardrails). Framework choice matters way less than that.",
          "score": 1,
          "created_utc": "2026-01-16 22:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o08ety4",
          "author": "spersingerorinda",
          "text": "I‚Äôll just gently point out that neither Anthropic nor OpenAI are promoting any ‚Äúcomputational graph‚Äù type framework. What do they know that LangChain doesn‚Äôt ? Cursor may use langgraph under the covers, but do you really need it if Claude Code doesn‚Äôt need it ?",
          "score": 1,
          "created_utc": "2026-01-18 03:58:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o095b7q",
          "author": "Revolutionary-Bet-58",
          "text": "disclosure, i'm working at inkog.io. \n\nI would recommend that you also look at loops and as another commenter says how the evals are set, but also if you have proper human-in-the-loop for unrealistic agent determiniation usecases. We do that at Inkog, which you can use to scan your agent and get fast feedback.\n\nIts quite a lot things to think of if you want to build a true enterprise grade agent, but it comes down to your usecase/scale",
          "score": 1,
          "created_utc": "2026-01-18 07:16:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0fxyzo",
          "author": "TJWrite",
          "text": "Yo, OP, sit down and pay attention to your big bro right here. Also my bad, I didn‚Äôt know I talk to damn much.\n\r\nFirst of all, there is no Gold bro, also there are no standards (With respect to a few unknown places). Most of us are using the shit that works and keeps it moving. As engineers, we tend to want to build the most complex shit in the planet, then cry about the number of bugs and issues it has, then abandon the project, and talk about it like it was a unicorn. The important lesson hete to note is: ‚ÄòReduce complexity whenever possible‚Äô (Remember this for later).\n\r\nSecond, there are other Agent Frameworks other than LangGraph, but what are the true differences and when to choose what? I will give you a few Pros & Cons about other Agent Frameworks and a few Pros & Cons for using LangGraph as a Framework. Note: This is not an exhaustive list, it‚Äôs meant to give you a better understanding of how to choose what and when.\n\r\nOther Agent Framework Pros: They focus on reducing the amount of work you would put in to build a framework. Some of them have colorful IDEs, cute buttons to do so much shit for you, and a step-by-step guide better than the guide ChatGPT could give you. If these frameworks would speak they would say, ‚ÄúI will take care of the Agent Framework, you just choose what you want, click this button and you his button and whoop, aren‚Äôt you such a developer. Now go handle your other shit, I got this‚Äù and they truly do.\n\r\nOther Agent Framework Cons: They only have a limited selection of Agent frameworks. Literally, they have a few options and ask you to choose one bro. Of you can‚Äôt find what you need, it throws a fit and tells you, we can‚Äôt work together. \r\nTherefore, you must check if your use case can be built using one of these Frameworks, it will save you time and reduce complexity. They even tell you what they are compatible with and many more. It‚Äôs really helpful when you can use them. However, if you need something they don‚Äôt offer, it‚Äôs like speaking Chinese to them, more useless than a box of rocks.\n\r\nLangGraph Pros: This Agent Framework is built to redefine the word ‚ÄòCustomization‚Äô. If we combine the amount of customization from an Android phone and a Linux OS, they won‚Äôt come close to the number of Customization that LangGraph has. Also, the number of tools that it has and can connect to. The most powerful feature its seamless integration with LangChain, and as you know LangChain by itself comes with more weapons than all of Call of Duty. \n\r\nLangGraph Cons: No fancy shit, deployment is a little sus with enterprise, requires actual coding, few files step up correctly to get this shit connected well.\nThere are some documentations, however, if you get stuck using a framework from LangGraph that doesn‚Äôt have enough documentations like me, come have a seat next to me, we can cry together. Note: The education courses on LangChain Academy is a beast in understanding LangGraph in more depth, avoid all others, trust me. Additionally, they are trying to build few templates, that anyone can use straight out of the box and works very well. However, if you are using a framework that doesn‚Äôt have a template like me, the seat next to me is still available bro.\r\n\n\r\nQuick Recap for my beginner bro, checkout the other agent frameworks that exists. Try to see all the Agent Framework features that they offer. All of them, I am serious, it will save you time and headache, trust me. Know that LangGraph is and will always be your backup option, while keeping in mind the massive overhead that you must take care of you on your own. Also, be very very careful about what has been deprecated recently when they released the new version and stick to the new version LangGraph v1. When Chatting with ChatGPT or whatever, ensure that you mention that you are using LangGraph v1, otherwise, this idiot could literally give you deprecated code and send you on a rabbit hole where there is no Wi-Fi down there. \n\r\nGood Luck bro,",
          "score": 1,
          "created_utc": "2026-01-19 08:03:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzulj4e",
          "author": "_eltigre_",
          "text": "Mastra.",
          "score": 1,
          "created_utc": "2026-01-16 02:42:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztu53d",
          "author": "naxmax2019",
          "text": "Don‚Äôt use frameworks. They suck!",
          "score": -1,
          "created_utc": "2026-01-16 00:10:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qb2vle",
      "title": "Learning RAG + LangChain: What should I learn first?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qb2vle/learning_rag_langchain_what_should_i_learn_first/",
      "author": "Cobra_venom12",
      "created_utc": "2026-01-12 18:29:03",
      "score": 19,
      "num_comments": 15,
      "upvote_ratio": 0.92,
      "text": "I'm a dev looking to get into RAG. There's a lot of noise out there‚Äîshould I start by learning:\n‚ÄãVector Databases / Embeddings?\n‚ÄãLangChain Expression Language (LCEL)?\n‚ÄãPrompt Engineering?\n‚ÄãWould love any recommendations for a \"from scratch\" guide that isn't just a 10-minute YouTube video. What's the best \"deep dive\" resource available right now?",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qb2vle/learning_rag_langchain_what_should_i_learn_first/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nz7rnwc",
          "author": "cmndr_spanky",
          "text": "Install a vector DB like ChromaDB (a great and easy vectorDB solution), and with its python library learn how to store documents, the chromaDB instructions explain how to use different embeddings models to do that.\n\nJust with ChromaDB docs learn how to query the database. All very straight forward.\n\nThen \"RAG\" just means write a program that uses an LLM to answer user questions, and the LLM is able to pass queries to the chromaDB to get some extra context (text chunks from whatever source material) that the LLM can use to better answer the user's questions. If you find some sample code online it'll teach you enough about \"prompt engineering\" to get by.\n\nTHAT'S IT.\n\nAll langchain does is provide some python abstractions to wire-up an LLM to an application that chats to users and connects to vector DB collections (and has many other features of course).. but honestly I find Langchain often adds more complexity than takes away complexity. I don't find their abstractions particularly intuitive.. but they work of course. My advice is start without langchain.. learn how to query an LLM directly using a basic library (from hugging face or the plain OpenAI APIs) and then augment the LLM responses using results from your VDB. That's it.\n\nBonus sentiment:\n\nI would just rush to learn how to make a full and basic RAG app from scratch rather than meander around many many courses on \"WHATS AN LLM\" \"HOW TO VECTOR DB\" \"HOW TO MAKE RAG\" blah blah blah.. there's so much noise out there now and so many frameworks and marketing jargon because everyone is trying to earn their little gold nugget from the trends surrounding LLMs right now. Don't get distracted by the bullshit.. just make sure you know how to code basic python and the rest is just wiring together a few concepts. start by using as few libraries and frameworks as possible.\n\nIf you don't understand anything I'm saying in this comment, paste it into chatGPT (thinking mode) and ask it to make you a curriculum based on this comment.",
          "score": 12,
          "created_utc": "2026-01-12 19:22:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz7vfsp",
              "author": "Cobra_venom12",
              "text": "Thanks mate",
              "score": 1,
              "created_utc": "2026-01-12 19:40:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz7wd6k",
                  "author": "cmndr_spanky",
                  "text": "np! also forgot to mention, you don't really need to \"install\" chromaDB, you can use its python library in such a way that it just stores the raw db data locally on disk directly from python. Which is a great way to learn it and doesn't require installing a separate server",
                  "score": 1,
                  "created_utc": "2026-01-12 19:44:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz7w9rd",
              "author": "Cobra_venom12",
              "text": "What about embeddings and transformers etc",
              "score": 1,
              "created_utc": "2026-01-12 19:44:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz80gpk",
                  "author": "cmndr_spanky",
                  "text": "An embeddings model (sometimes called a Sentence transformer) is a small model that converts text chunks into a bunch of vectors that gets stored along with the raw texts chunks into the VectorDB records. Those vectors represent the topics related to that text chunk and those vectors are called \"embeddings\" which is a shitty term but sadly we rely on engineers to make up these terms rather than normal people.\n\nChromaDB will by default use a built-in embeddings model to do this for you, or you can use a different one, again check the chromaDB docs / getting start instructions it'll explain how to do this.\n\n\"Transformer\" is the industry term for the neural net architecture that powers large language models, people often use these terms as synonyms: Transformer model = large language model.\n\nAn example popular embeddings model is: sentence-transformers/all-MiniLM-L12-v2 but there are many more you can find here [https://huggingface.co/sentence-transformers/models](https://huggingface.co/sentence-transformers/models)\n\nMy personal favorite is sentence-transformers/all-mpnet-base-v2\n\nan example regular LLM you'd use to power the whole chat bot is chatGPT or similar ones.\n\nBoth embeddings models and \"regular\" LLMs are technically Transformer models, so don't be confused when you see that term used everywhere.\n\nAlso, the official hugging face library for using LLMs from hugging face is called the Transformers library (sorry..).\n\nfor some extra reading hugging face has a nice little article about embeddings: [https://huggingface.co/blog/getting-started-with-embeddings](https://huggingface.co/blog/getting-started-with-embeddings)",
                  "score": 1,
                  "created_utc": "2026-01-12 20:03:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz7ksf2",
          "author": "Valeria_Xenakis",
          "text": "Whatever you do later first start from a 1 hr coursera free course by andrew ng and harrison chase (founder of langchain) called Langchain Chat with your Data. Will give you a good idea of what it is all about and how to delve into it further.",
          "score": 6,
          "created_utc": "2026-01-12 18:51:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz7vgui",
              "author": "Cobra_venom12",
              "text": "Sure",
              "score": 1,
              "created_utc": "2026-01-12 19:40:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz8gw4c",
                  "author": "_blkout",
                  "text": "literally just clone the langraph sample project and look through the llamaindex integration docs, that‚Äôs basically all you need to know. if you already understand how both of these concepts work they literally just plug and play. it‚Äôs how you orchestrate them that matters",
                  "score": 1,
                  "created_utc": "2026-01-12 21:20:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzba3uv",
          "author": "OnyxProyectoUno",
          "text": "Start with embeddings and chunking before touching LangChain. Seriously. Most RAG failures trace back to how documents get split and embedded, not the orchestration layer. If your chunks are garbage, no amount of LCEL elegance saves you.\n\nThe learning order that actually sticks: first understand what embeddings represent and why chunk boundaries matter. Then mess with a vector store directly, no framework. Pinecone or Qdrant have decent free tiers. Only after you've felt the pain of bad retrieval should you layer in LangChain.\n\nFor deep dives, the Pinecone learning center covers embeddings and similarity search well. LlamaIndex docs explain chunking strategies better than most resources. And honestly, the best teacher is building something small, uploading a few PDFs, and watching what comes back when you query. You'll learn more debugging why your retriever missed an obvious answer than from any course.\n\nPrompt engineering matters less than people think at the RAG stage. Get your retrieval solid first.",
          "score": 2,
          "created_utc": "2026-01-13 07:00:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkh94v",
          "author": "pbalIII",
          "text": "Build a tiny RAG app end to end first, without any framework abstractions. Once you can explain each knob, LangChain and LCEL stop feeling like a maze.\n\n1) Chunk + store text with metadata\n2) Retrieve, then rerank, keep the top chunks\n3) Add an eval set and track answer relevance and faithfulness\n\nAfter that, pick LCEL for simple chains and LangGraph when you need real state.",
          "score": 2,
          "created_utc": "2026-01-14 16:59:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzw41k9",
          "author": "Sheikhfahad67",
          "text": "I think you should start from the basics such as agentic ai. Create agents using LangChain and LangGraph for creating React or Deep agents. After that move towards other things such as RAG and so on..",
          "score": 2,
          "created_utc": "2026-01-16 09:22:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz9zg37",
          "author": "d0r1h",
          "text": "I would recommend if you're getting started, you should start with this \n\nRAG From Scratch by Langchain Engineer itself - [https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1\\_PKyVJiQ23ZztA0x](https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x) \n\n  \nExplanation is pretty good and you'll get solid understanding of ecosystem, how things works in RAG system from basic to optimization. \n\nAlso in the lectures they are using OpenAPI for chat and embedding models, but if you want to use opensource models from HuggingFace, you can follow my codes, I've implemented same things using open models, they cost less compare to openAPI (Their documentation is outdated, won't get any help from there if you want to use other models). \n\n[https://github.com/d0r1h/Learn-AI/tree/main/Agentic\\_AI/RAG/Learning\\_RAG](https://github.com/d0r1h/Learn-AI/tree/main/Agentic_AI/RAG/Learning_RAG) \n\nI'm also following same playlist. Thanks!",
          "score": 1,
          "created_utc": "2026-01-13 02:02:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzb755k",
          "author": "Subject-Complex6934",
          "text": "vector databases and RAG is explained here nicely  \n[https://www.youtube.com/@PavelCermakAI/videos](https://www.youtube.com/@PavelCermakAI/videos)",
          "score": 1,
          "created_utc": "2026-01-13 06:34:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbru7z",
      "title": "Open Source Enterprise Search Engine (Generative AI Powered)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qbru7z/open_source_enterprise_search_engine_generative/",
      "author": "Effective-Ad2060",
      "created_utc": "2026-01-13 13:51:44",
      "score": 18,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hey everyone!\n\nI‚Äôm excited to share something we‚Äôve been building for the past 6 months, a¬†**fully open-source Enterprise Search Platform**¬†designed to bring powerful Enterprise Search to every team, without vendor lock-in. The platform brings all your business data together and makes it searchable. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, Local file uploads and more. You can deploy it and run it with just one docker compose command.\n\nYou can run the full platform locally. Recently, one of our users tried¬†**qwen3-vl:8b (16 FP)**¬†with¬†**Ollama**¬†and got very good results.\n\nThe entire system is built on a¬†**fully event-streaming architecture powered by Kafka**, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data.\n\nAt the core, the system uses an **Agentic Graph RAG approach**, where retrieval is guided by an enterprise knowledge graph and reasoning agents. Instead of treating documents as flat text, agents reason over relationships between users, teams, entities, documents, and permissions, allowing more accurate, explainable, and permission-aware answers.\n\n**Key features**\n\n* Deep understanding of user, organization and teams with enterprise knowledge graph\n* Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama\n* Use any provider that supports OpenAI compatible endpoints\n* Choose from 1,000+ embedding models\n* Visual Citations for every answer\n* Vision-Language Models and OCR for visual or scanned docs\n* Login with Google, Microsoft, OAuth, or SSO\n* Rich REST APIs for developers\n* All major file types support including pdfs with images, diagrams and charts\n* Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more\n* Reasoning Agent that plans before executing tasks\n* 40+ Connectors allowing you to connect to your entire business apps\n\nCheck it out and share your thoughts or feedback. Your feedback is immensely valuable and is much appreciated:  \n[https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)\n\nDemo Video:  \n[https://www.youtube.com/watch?v=xA9m3pwOgz8](https://www.youtube.com/watch?v=xA9m3pwOgz8)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qbru7z/open_source_enterprise_search_engine_generative/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qgekmg",
      "title": "fastapi-fullstack v0.1.15 released ‚Äì now with DeepAgents (LangChain's multi-agent framework) + HITL support!",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qgekmg/fastapifullstack_v0115_released_now_with/",
      "author": "VanillaOk4593",
      "created_utc": "2026-01-18 17:27:05",
      "score": 17,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hey r/LangChain,\n\nQuick recap for new folks: fastapi-fullstack is an open-source CLI generator (pip install fastapi-fullstack) that creates production-ready full-stack AI/LLM apps with FastAPI backend + optional Next.js 15 frontend. It supports PydanticAI, LangChain, LangGraph, CrewAI ‚Äì and now DeepAgents for advanced multi-agent systems.\n\n**v0.1.15 just released with full DeepAgents integration:**\n\n**Added:**\n\n* **DeepAgents as the fifth AI framework option** ‚Äì new --ai-framework deepagents CLI flag\n* Built-in tools for file ops (ls/read/write/edit/glob/grep), code execution (disabled by default for safety), and task management (todos/sub-agents)\n* StateBackend for in-memory file state\n* Skills support via DEEPAGENTS\\_SKILLS\\_PATHS env var\n\n**Human-in-the-Loop (HITL) features:**\n\n* Tool approval workflow: Users can approve/edit/reject tool calls (configurable via DEEPAGENTS\\_INTERRUPT\\_TOOLS)\n* Frontend dialog for reviewing/editing JSON args in real-time\n* WebSocket protocol for interrupts: Backend sends tool\\_approval\\_required, frontend responds with resume decisions\n\n**Fixed & improved:**\n\n* Type annotations across CrewAI handlers (from previous updates)\n* WebSocket disconnect handling during agent processing\n* Frontend timeline connectors and message grouping\n* 100% test coverage (720 statements, 0 missing) with tests for all DeepAgents events, stream edges, and disconnects\n\nThis makes building and deploying DeepAgents-powered apps (with HITL for safe, controlled execution) super straightforward ‚Äì perfect for complex, filesystem-aware agents.\n\nFull changelog: [https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template/blob/main/docs/CHANGELOG.md](https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template/blob/main/docs/CHANGELOG.md)  \nRepo: [https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template](https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template)\n\nLangChain community ‚Äì how does DeepAgents + HITL fit your multi-agent projects? Any features to add? Contributions welcome! üöÄ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qgekmg/fastapifullstack_v0115_released_now_with/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o0d557i",
          "author": "vtrivedy-lc",
          "text": "this is awesome! basically breathe fastapi and deepagents these days so love to see this :)\n\nif there‚Äôs any way we can make deepagents easier to dev on, would love to hear it!",
          "score": 1,
          "created_utc": "2026-01-18 21:52:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qasrg6",
      "title": "I tested my LangChain agent with chaos engineering - 95% failure rate on adversarial inputs. Here's what broke.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qasrg6/i_tested_my_langchain_agent_with_chaos/",
      "author": "No-Common1466",
      "created_utc": "2026-01-12 11:45:01",
      "score": 16,
      "num_comments": 7,
      "upvote_ratio": 0.9,
      "text": "Hi r/LangChain,\n\nI'm Frank, the solo developer behind¬†[Flakestorm](https://github.com/flakestorm/flakestorm). I was recently humbled and thrilled to see it featured in the LangChain community spotlight. That validation prompted me to run a serious stress test on a standard LangChain agent, and the results were‚Ä¶ illuminating.\n\nI used Flakestorm, my open-source chaos engineering tool for AI agents to throw 60+ adversarial mutations at a typical agent. The goal wasn't to break it for fun, but to answer:¬†\"How does this agent behave in the messy real world, not just in happy-path demos?\"\n\n**The Sobering Results**\n\n* **Robustness Score:**¬†**5.2%**¬†(57 out of 60 tests failed)\n* **Critical Failures:**\n   1. **Encoding Attacks:**¬†**0% Pass Rate.**¬†The agent diligently¬†*decoded*¬†malicious Base64/encoded inputs instead of rejecting them. This is a major security blind spot.\n   2. **Prompt Injection:**¬†**0% Pass Rate.**¬†Direct \"ignore previous instructions\" attacks succeeded every time.\n   3. **Severe Latency Spikes:**¬†Average response blew past 10-second thresholds, with some taking nearly¬†**30 seconds**¬†under stress.\n\n**What This Means for Your Agents**  \nThis isn't about one \"bad\" agent. It's about a¬†**pattern**: our default setups are often brittle. They handle perfect inputs but crumble under:\n\n* **Obfuscated attacks**¬†(encoding, noise)\n* **Basic prompt injections**\n* **Performance degradation**¬†under adversarial conditions\n\nThese aren't theoretical flaws. They're the exact things that cause user-facing failures, security issues, and broken production deployments.\n\n**What I Learned & Am Building**  \nThis test directly informed Flakestorm's development. I'm focused on providing a \"crash-test dummy\" for your agents¬†*before*¬†deployment. You can:\n\n* **Test locally**¬†with the open-source tool (`pip install flakestorm`).\n* **Generate adversarial variants**¬†of your prompts (22+ mutation types).\n* **Get a robustness score**¬†and see¬†*exactly*¬†which inputs cause timeouts, injection successes, or schema violations.\n\n**Discussion & Next Steps**  \nI'm sharing this not to fear-monger, but to start a conversation the LangChain community is uniquely equipped to have:\n\n1. How are you testing your agents for real-world resilience**?**¬†Are evals enough?\n2. What strategies work for hardening agents against encoding attacks or injections?\n3. Is chaos engineering a missing layer in the LLM development stack?\n\nIf you're building agents you plan to ship, I'd love for you to try¬†[Flakestorm on your own projects](https://github.com/flakestorm/flakestorm). The goal is to help us all build agents that are not just clever, but truly robust.\n\n**Links:**\n\n* Flakestorm GitHub:¬†[https://github.com/flakestorm/flakestorm](https://github.com/flakestorm/flakestorm)\n* LangChain Community Spotlight: [https://x.com/LangChain/status/2007874673703596182](https://x.com/LangChain/status/2007874673703596182)\n* Example config & report from this test:  \n   * [https://github.com/flakestorm/flakestorm/blob/main/examples/langchain\\_agent/flakestorm.yaml](https://github.com/flakestorm/flakestorm/blob/main/examples/langchain_agent/flakestorm.yaml)\n   * [https://github.com/flakestorm/flakestorm/blob/main/flakestorm-20260102-233336.html](https://github.com/flakestorm/flakestorm/blob/main/flakestorm-20260102-233336.html)\n\nI'm here to answer questions and learn from your experiences.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qasrg6/i_tested_my_langchain_agent_with_chaos/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nz6ppun",
          "author": "Goolitone",
          "text": "whaaaa?",
          "score": 1,
          "created_utc": "2026-01-12 16:30:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00fgu1",
          "author": "Revolutionary-Bet-58",
          "text": "this matches what ive seen building agents across langchain and crewai, the happy path works fine but anything adversarial breaks it.   \n  \nwhat helped us was scanning the agent code before deployment to catch places where user input can flow into system prompts. basically treating it like a security review not just testing. found a ton of injection vectors that way also encoding attacks are brutal, base64 stuff especially. we started stripping/validating anything that looks encoded before it hits the llm   \n  \ncurious if youve tried combining chaos testing with static analysis? feels like they catch different things",
          "score": 1,
          "created_utc": "2026-01-16 23:04:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01gx02",
              "author": "No-Common1466",
              "text": "Yeah. That's exactly the vision. Chaos testing for runtime breaks, static analysis for the code flaws. They're a perfect one-two punch.\n\nWould love to know: any static analysis tools or methods you've found useful for those pre-deployment security reviews?",
              "score": 1,
              "created_utc": "2026-01-17 02:49:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o02oais",
                  "author": "Revolutionary-Bet-58",
                  "text": "actually been building something for this - [https://github.com/inkog-io/inkog](https://github.com/inkog-io/inkog)\n\nfull disclosure im the founder so obviously biased, but it came out of exactly this problem. scans langgraph/crewai/n8n code and looks for cycles without exit conditions, places where user input can reach system prompts, unbounded loops etc\n\nstill early and definitely has rough edges. would be curious what you think if you try it against some of the stuff that broke in your chaos tests",
                  "score": 1,
                  "created_utc": "2026-01-17 08:23:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o01hwhw",
          "author": "cordialgerm",
          "text": "Any reason I shouldn't just by default add middleware that rejects any base64-looking input in a user message unless my agent specifically needs to support it?",
          "score": 1,
          "created_utc": "2026-01-17 02:55:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01o64n",
              "author": "No-Common1466",
              "text": "That's the point. Most AI agents in production don't have these guardrails. If you didn't test these scenarios, you wouldn't even know. That's the importance of Adversarial testing like flakestorm. The next step is to mitigate these security issues upon proper testing.",
              "score": 1,
              "created_utc": "2026-01-17 03:35:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qbxrhu",
      "title": "Open-Source Memory Layer for Long-Running Agents: HMLR (LangGraph Integration Available)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qbxrhu/opensource_memory_layer_for_longrunning_agents/",
      "author": "AnAlpacca",
      "created_utc": "2026-01-13 17:43:39",
      "score": 15,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I launched an open-source project a bit over a month ago called HMLR (Hierarchical Memory Lookup & Routing), basically a \"living memory\" system designed specifically for agentic AI that needs to remember across long sessions without forgetting or hallucinating on old context.\n\nThe core problem it solves: Standard vector RAG or simple conversation buffers fall apart in multi-day/week agents (e.g., personal assistants, research agents, or production tools). HMLR utilizes hierarchical routing and multi-hop reasoning to reliably persist and recall information, and it passes benchmarks such as the \"Hydra of Nine Heads\" on mini LLMs. (A full harness for reproducibility of tests is part of the repository.)\n\nKey features:\n\n* Drop-in LangGraph node (just added recently ‚Äì makes it super easy to plug into existing agents)\n* Pip installable: pip install hmlr\n* Benchmarks showing strong recall without massive context bloat\n* Fully open-source (MIT)\n\nRepo: [https://github.com/Sean-V-Dev/HMLR-Agentic-AI-Memory-System](https://github.com/Sean-V-Dev/HMLR-Agentic-AI-Memory-System)",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1qbxrhu/opensource_memory_layer_for_longrunning_agents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nzob732",
          "author": "pbalIII",
          "text": "H-MEM's four-layer structure (domain, category, trace, episode) is interesting, but what I keep bumping into is the consolidation policy. Most hierarchical approaches nail retrieval but get noisy fast if you're not pruning or summarizing aggressively.\n\nTwo things I'd want to know before adopting something like this:\n\n- How does it handle cross-session deduplication? Mem0 and A-Mem both struggle when the same fact surfaces in slightly different forms.\n- What's the write path latency? Retrieval benchmarks look great, but if every turn blocks on memory updates you're eating that cost in UX.",
          "score": 1,
          "created_utc": "2026-01-15 04:42:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qejo7q",
      "title": "Deploying LangGraph agents to your own AWS with one command",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qejo7q/deploying_langgraph_agents_to_your_own_aws_with/",
      "author": "DefangLabs",
      "created_utc": "2026-01-16 15:54:18",
      "score": 15,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "We keep seeing deployment questions come up here, so wanted to share what we've built.\n\n**The problem:**\n\nLangGraph is great for building agents locally. But when you want to deploy:\n\n* LangSmith/LangServe are solid but your data goes through their infra\n* Self-hosting on AWS means ECS, IAM roles, VPCs, load balancers, secrets management...\n* Most tutorials stop at \"run it locally\"\n\n**What we built:**\n\nDefang lets you deploy any containerized app to your own AWS/GCP with one command. You write a compose.yaml:\n\nyaml\n\n    services:\n      agent:\n        build: .\n        ports:\n          - \"8000:8000\"\n        x-defang-llm: true\n\nRun `defang compose up`. Done. It provisions ECS, networking, SSL, everything.\n\nThe `x-defang-llm: true` part auto-configures IAM permissions for AWS Bedrock (Claude, Llama, Mistral) or GCP Vertex AI. No policy writing.\n\n**Why this matters:**\n\n* Your AWS account, your data, your infrastructure\n* Works with any LangChain/LangGraph setup (just containerize it)\n* Scales properly (ECS Fargate under the hood)\n* Free tier for open source repos (forever, not a trial)\n\n**We're launching V3 next week** with:\n\n* Named Stacks ‚Äî deploy separate instances for dev/staging/prod or per customer from the same codebase\n* Agentic CLI ‚Äî auto-debugs deployment errors, understands English commands\n* Zero-config AWS ‚Äî one click to connect, no IAM policies to write\n\nWe have a LangGraph sample ready to go: [github.com/DefangLabs/samples](http://github.com/DefangLabs/samples)\n\nLaunching on Product Hunt Jan 21. \n\nHappy to answer questions about deploying LangGraph or agents in general.",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qejo7q/deploying_langgraph_agents_to_your_own_aws_with/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qbdpo3",
      "title": "Plano v0.4.2: universal v1/responses + Signals (trace sampling for continuous improvement)",
      "subreddit": "LangChain",
      "url": "https://i.redd.it/hgdjg0gdo0dg1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2026-01-13 01:22:52",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Announcement",
      "permalink": "https://reddit.com/r/LangChain/comments/1qbdpo3/plano_v042_universal_v1responses_signals_trace/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qb1vuh",
      "title": "AI testing resources that actually helped me get started with evals",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qb1vuh/ai_testing_resources_that_actually_helped_me_get/",
      "author": "PurpleWho",
      "created_utc": "2026-01-12 17:53:11",
      "score": 9,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Spent the last few months figuring out how to test AI features properly. Here are the resources that actually helped, plus the lesson none of them taught me.\n\n- [**Anthropic's Prompt Eval Course**](https://github.com/anthropics/courses/blob/master/prompt_evaluations/README.md) - Most practical of the bunch. Hands-on exercises, not just theory.\n\n- **[Hamel's LLM Evals FAQ](https://hamel.dev/blog/posts/evals-faq)** - Covers the common questions everyone has but is afraid to ask.\n\n- **[DeepLearning's Evaluation and Monitoring Courses](https://www.deeplearning.ai/courses/)** - Whole category of free courses. Good for building foundational understanding.\n\n- **Lenny's \"[Beyond Vibe Checks: A PM's Complete Guide to Evals](https://www.lennysnewsletter.com/p/beyond-vibe-checks-a-pms-complete)\"** - Best written explanation of when and why to use evals.\n\n### Paid Resources (if you want to go deeper):\n\n- **Hamel Husain & Shreya Shankar's \"[AI Evals for Engineers & PMs](https://maven.com/parlance-labs/evals)\"** - Comprehensive. Worth it if you're doing this seriously.\n\n- **\"[Go from Zero to Eval](https://forestfriends.tech/)\" by Sridatta & Wil** - Heavy on examples, which is what I needed.\n\n### What every resource skips:\n\nBefore you can run any evaluations, you need test cases. And LLMs are terrible at generating realistic ones for your specific use case.\n\nI tried Claude Console to bootstrap scenarios - they were generic and missed actual edge cases. Asking an LLM \"give me 50 test cases\" just gives you 50 variations on the happy path or just the most obvious edge cases.\n\n**What actually worked:**\n\nBuilding my test dataset manually:\n- Someone uses the feature wrong? Test case.\n- Weird edge case while coding? Test case.\n- Prompt breaks on specific input? Test case.\n\nThe bottleneck isn't running evals - it's capturing these moments as they happen.\n\n**My current setup:**\n\nCSV file with test scenarios + test runner in my code editor. That's it.\n\nTried VS Code's AI Toolkit first (works, but felt pushy about Microsoft's paid services). Switched to an open-source extension called Mind Rig - same functionality, simpler. Basically, they save a fixed batch of test inputs so I can re-run the same data set each time I tweak a prompt.\n\n1. Start with test dataset, not eval infrastructure\n2. Capture edge cases as you build\n3. Test iteratively in normal workflow\n4. Graduate to formal evals at 100+ cases (PromptFoo, PromptLayer, Langfuse, Arize, Braintrust, Langwatch, etc)\n\nThe resources above are great for understanding evals. But start by building your test dataset first, or you'll just spend all your time setting up sophisticated infrastructure for nothing.\n\nAnyone else doing AI testing? What's your workflow?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qb1vuh/ai_testing_resources_that_actually_helped_me_get/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nzkjxmh",
          "author": "pbalIII",
          "text": "Capturing cases while you build is the only part that scales. I keep a tiny gold set plus a nasty set, and I add to it every time something surprises me in prod.\n\n- Store input, expected shape, and a short rubric note\n- Tag the failure mode so you can slice later\n- Re-run on every prompt change and in CI\n\nOnce you hit 100ish cases, the fancy tooling finally has something real to measure.",
          "score": 1,
          "created_utc": "2026-01-14 17:12:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzn3y9m",
              "author": "PurpleWho",
              "text": "Nasty set. I love it.   \n  \nYeah, my gold + nasty are all mixed together.\n\nThe other thing I do, though, is I have a set of things the LLM can't do yet. So a lot of the time, problems are either specification issues or changes that need to be made to the context/architecture. But now and then, I run up against an edge case that the LLM just can't handle. I keep those in a separate set so that when the next fancy model release comes out, I can run this can't-do-yet set and see if the new model unlocks any new capability for us.",
              "score": 2,
              "created_utc": "2026-01-15 00:24:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o00f6i7",
                  "author": "pbalIII",
                  "text": "Great call on the can't-do-yet set. Will start tagging the hardest edge cases to measure model improvements.",
                  "score": 1,
                  "created_utc": "2026-01-16 23:03:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qczpml",
      "title": "How did you land your AI Agent Engineer role?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qczpml/how_did_you_land_your_ai_agent_engineer_role/",
      "author": "reidkimball",
      "created_utc": "2026-01-14 21:16:47",
      "score": 9,
      "num_comments": 11,
      "upvote_ratio": 0.74,
      "text": "Hi, \n\nI'm sorry if this is too off-topic. I assume a lot of AI Agent Engineers use LangChain and LangGraph. I'd love to hear stories of how you landed your Agent Engineering role? I'm curious about:   \n\n* General location (state/country is fine)\n* Industry\n* Do you have a technical degree like Computer Science, or IT?\n* How many years experience with programming/software eng. before landing your role?\n* Did you apply cold or was it through networking?\n* Did having a project portfolio help?\n* What do you think helped most to get the job?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qczpml/how_did_you_land_your_ai_agent_engineer_role/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nzmqnfs",
          "author": "battlepi",
          "text": "Combat to the death with the current one, as usual.",
          "score": 5,
          "created_utc": "2026-01-14 23:12:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmr3gr",
              "author": "reidkimball",
              "text": "username checks out.",
              "score": 2,
              "created_utc": "2026-01-14 23:14:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzoe5kz",
          "author": "Educational_Cup9809",
          "text": "Internally, Part of data engineering group. Did a RAG PoC 2+ years ago. Executives saw potential. Lead building the whole unstructured data sources ingestion and organization‚Äôs RAG platform like an internal RAG as a service, now evolved to have MCP server and skills repo available for everyone to build their own agents. So, basically created a role for myself lol. Don‚Äôt use Langchain or any frameworks. All custom code.",
          "score": 5,
          "created_utc": "2026-01-15 05:03:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzth6fu",
              "author": "Sayv_mait",
              "text": "Can you share more on custom code? \nI thought frameworks are the best for agents",
              "score": 1,
              "created_utc": "2026-01-15 23:00:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzumwrn",
                  "author": "Educational_Cup9809",
                  "text": "Sorry if i wasn‚Äôt clear. \n\nThe whole RAG part (including data source ingestion from sharepoint , confluence , s3 , service now , file uploads, azure blob, metadata tag generation chunking, embedding, access control, and MCP server part is built in house. \n\nThe search and answer generation is also done without any framewok \n\nTo build agents we are exploring claude agent sdk. But once RAG components are exposed via MCP server (tools) teams can use it as they like. Create guided orchestrated workflows, code their own agents, or use prebuilt sdk like claude  agent sdk.",
                  "score": 1,
                  "created_utc": "2026-01-16 02:50:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nznwj5h",
          "author": "cali_organics",
          "text": "I was a Data Engineer and the company needed people to work on some AI initiatives related to DE use-cases.",
          "score": 2,
          "created_utc": "2026-01-15 03:07:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o01ct5l",
          "author": "BeerBatteredHemroids",
          "text": "Im a Lead AI Engineer. My career has been exclusively in banking. Here was my trajectory:\n\n1 year as a risk analyst |\n4 years as a sr risk analyst |\n2 years as a BI engineer |\n3 years as MLOps engineer |\n1 year as Lead AI Engineer \n\nMy degree is in philosophy... so let that sink in... You do not need a degree specifically in computer science or software engineering to get this kind of job but you will have to work your way from the bottom. \n\nMy original major was comp sci, but I found myself extremely bored with the classes and increasingly frustrated with the program and decided to pursue Philosophy with the goal of attending law school.\n\nAfter I graduated I realized I could not afford law school so I went to work in banking as a Jr risk analyst. \n\nI had a lot of experience in SQL and Python (7 years total in data analysis, programming and some data engineering / data wrangling), prior to getting my job as an MLOps engineer.\n\nI am completely self-taught and I got my first gig as an MLOps engineer because I had made connections in my company with individuals in the data science department via partnering with them on joint projects. I still had to pass two technical interviews and a technical project to get the job, and no one bothered asking me for my portfolio.\n\nI think personal portfolios are somewhat helpful if you're fresh of out of college, but from my experience most hiring managers want to see your real-world experience and are going to suss you out during the technical interviews.\n\nI would not recommend taking my path, as my journey into AI is somewhat atypical. It took me 7 years to work my way into it (almost by chance), and if I could go back I would have just stayed in my original major of comp sci.\n\nHowever the universe had other plans for me. So here I am, ironically in a career field I had originally shunned in my college years.\n\nWhat helped most to get the job? Sheer fucking grit and determination. Also, being curious about everything, wanting to learn everything I could and not being afraid to fail and try new things. Also, you need to have people skills. If you can't work as part of a larger team, you will not make it very far.",
          "score": 2,
          "created_utc": "2026-01-17 02:23:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o027ol5",
              "author": "reidkimball",
              "text": "Amazing journey, thank you for sharing all of that.",
              "score": 1,
              "created_utc": "2026-01-17 05:56:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzm542d",
          "author": "Specialist-Rise1622",
          "text": "Is this like LangChain guerrilla marketing via content stuffing?",
          "score": 1,
          "created_utc": "2026-01-14 21:29:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qaubry",
      "title": "Battle of AI Gateways: Rust vs. Python for AI Infrastructure: Bridging a 3,400x Performance Gap",
      "subreddit": "LangChain",
      "url": "https://vidai.uk/blog/rust-python-vidai",
      "author": "Guna1260",
      "created_utc": "2026-01-12 13:04:10",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qaubry/battle_of_ai_gateways_rust_vs_python_for_ai/",
      "domain": "vidai.uk",
      "is_self": false,
      "comments": [
        {
          "id": "nz5mol3",
          "author": "pokemonplayer2001",
          "text": "Python's a toy.",
          "score": 0,
          "created_utc": "2026-01-12 13:11:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5mypm",
              "author": "Guna1260",
              "text": "Agree.. but we like to call it Tooling tier.. especially for the ecosystem.",
              "score": 1,
              "created_utc": "2026-01-12 13:12:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qfa9vd",
      "title": "Web Search APIs Are Becoming Core Infrastructure for AI",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qfa9vd/web_search_apis_are_becoming_core_infrastructure/",
      "author": "codes_astro",
      "created_utc": "2026-01-17 10:52:21",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 0.75,
      "text": "Web search used to be a ‚Äúnice-to-have‚Äù in software. With AI, it‚Äôs quickly becoming a requirement.\n\nLLMs are powerful, but without live data they can‚Äôt handle breaking news, current research, or fast-changing markets. At the same time, the traditional options developers relied on are disappearing, Google still doesn‚Äôt offer a truly open web search API and Bing Search API has now been retired in favor of Azure-tied solutions.\n\nI wrote a deep dive on how this gap is being filled by a new generation of AI-focused web search APIs, and why retrieval quality matters more than the model itself in RAG systems.\n\nThe article covers:\n\n* Why search is now core infrastructure for AI agents\n* Benchmarks like SimpleQA and FreshQA and what they actually tell us\n* How AI-first search APIs compare on accuracy, freshness, and latency\n* A breakdown of tools like Tavily, Exa, Valyu, Perplexity, Parallel and Linkup\n* Why general consumer search underperforms badly in AI workflows\n\nI‚Äôd love to hear from people actually building RAG or agent systems:\n\n* Which search APIs are you using today?\n* What tradeoffs have you run into around freshness vs latency vs cost?\n\nRead full writeup [here](https://mranand.substack.com/p/why-web-search-apis-are-becoming)\n\n",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1qfa9vd/web_search_apis_are_becoming_core_infrastructure/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o0542zt",
          "author": "Born_Owl7750",
          "text": "Currently using grounding with bing. But few limitations like you can only use it with a foundry agent service. Results are fine but too high level.\n\nTavily is something we are trying out as well",
          "score": 2,
          "created_utc": "2026-01-17 17:49:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09gx0m",
              "author": "codes_astro",
              "text": "Yes bing is too limited now, I have used tavily in early 2025 but I didn‚Äôt liked the responses, linkup and exa did better. Have to implement new web APIs to test which is better",
              "score": 2,
              "created_utc": "2026-01-18 09:02:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o09pbu6",
                  "author": "Born_Owl7750",
                  "text": "Nice, will try it out",
                  "score": 1,
                  "created_utc": "2026-01-18 10:20:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o09pcoc",
                  "author": "Born_Owl7750",
                  "text": "Nice, will try it out",
                  "score": 1,
                  "created_utc": "2026-01-18 10:20:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0b4vxx",
                  "author": "TheStanfordSimpLord",
                  "text": "\\+1 for linkup - works the best for me overall",
                  "score": 1,
                  "created_utc": "2026-01-18 16:02:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qd4k1f",
      "title": "Number of LLM calls in agentic systems",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qd4k1f/number_of_llm_calls_in_agentic_systems/",
      "author": "usernotfoundo",
      "created_utc": "2026-01-15 00:29:21",
      "score": 8,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "I don't know if I am phrasing this correctly but I am kind of confused about how proper agentic systems are made but I'll try, hopefully someone understands.\n\nWhenever I see something like Claude Code, Copilot or even ChatGPT and read their \"thinking\" part it seems like they generate something, reason over it, generate something else, again \"reason\", and repeat.\n\nBasically from a developer's(just a student so don't have experience with production grade systems) perspective it seems like if I want to make something like that it would require a lot of continuous call to the llm's api for each reasoning step and this isn't possible with just a single api call. Is that actually what's happening? Are there multiple api calls involved and it's not a fixed number i.e. could be 2 , could end up being 4/5? \n\nAdditional questions:\n\n1. Wouldn't this be very expensive to develop with the llm api call charges stacking?\n\n2. What about getting rate limited, with just one use of the agent requiring multiple api calls and having many users for the application?\n\n3. Wouldn't monitoring and debugging be very difficult in this case where you have multiple api calls and there could end up being an error(rate limit, hallucinaton) at any call? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qd4k1f/number_of_llm_calls_in_agentic_systems/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nznqaqu",
          "author": "jeronimoe",
          "text": "Agentic ai sounds all fancy, but at it‚Äôs core it‚Äôs a big for loop calling llm‚Äôs and tools.\n\nYes it, can get expensive, model choice is important. ¬†Sometimes a reasoning agent calls sub agents using cheaper or oss models, especially for summarization. ¬†But the agentic loop will pass the old conversation (or parts), and although there is input caching, it increases token usage further.\n\nYou can get rate limited, as well as exceed context. ¬†You typically build retry logic into it.\n\nMonitoring and debugging are important, there are several products built for agentic observation, some specifically for Lang graph.",
          "score": 2,
          "created_utc": "2026-01-15 02:31:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzo8rgi",
          "author": "pvatokahu",
          "text": "Capture a trace with monocle2ai from Linux foundation and it will answer that for you - you‚Äôll get the agentic spans and inference spans with relevant metadata like token counts, input/outputs, history, turn info etc",
          "score": 1,
          "created_utc": "2026-01-15 04:25:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpfj4h",
          "author": "SmoothRolla",
          "text": "Its one call to the API even if its a reasoning model, it will reason whilst generating the answer (and that counts towards the total tokens). you dont see the reasoning in the API response (or at least i dont know how to with the models i use)",
          "score": 1,
          "created_utc": "2026-01-15 10:34:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqmw86",
              "author": "usernotfoundo",
              "text": "I feel it's multiple api calls(and even some of the other comments agree) because it seems that if I use just a single api call to do the task the results are always worse compared to if I use multiple api calls where I break down the task into multiple steps and only pass relevant context.\n\nIt could be that I have only used the lower cost models and so there is such a huge difference. Are the reasoning models that much better?",
              "score": 1,
              "created_utc": "2026-01-15 15:11:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzqrvqp",
                  "author": "SmoothRolla",
                  "text": "Apologies I misunderstood yout question. Yeah you may need multiple calls. My bot is broken down I to 3 agents and the middle agent may recurse 5 or more times until it had called all the tools it needed to get the data I needs for the next agent to use to generate an answer¬†",
                  "score": 1,
                  "created_utc": "2026-01-15 15:34:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzug1p3",
          "author": "attn-transformer",
          "text": "You need to make as few llm calls as possible. Use the LLM to find out what to do, not how. \n\nUser request -> what to do (llm) -> workflow (deterministic) -> final response (llm)\n\nThis is a a bit over simplified, but start here and only add complexity as needed and carefully. \n\nDon‚Äôt put everything in a react loop by default.",
          "score": 1,
          "created_utc": "2026-01-16 02:11:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxme7w",
          "author": "adlx",
          "text": "Can't you do the math? I mean, if you didn't make the code (yet) you share can get a rough idea of where there will be an llm call involved with what content...\n\nIt can be several calls, sequential, some in parallel...",
          "score": 1,
          "created_utc": "2026-01-16 15:20:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyhegi",
          "author": "gkarthi280",
          "text": "You touch on a very good point about monitoring/observability for these AI agents. One thing that is becoming very important these days is proper observability for AI agents, especially when they go into production. Because AI agents are non-deterministic, you really have no idea what's going on under the hood. If there is latency, errors, incorrect tool calls, or even tool call loops, its very hard to know that its happening and what's causing these issues without proper observability in your AI stack. \n\nI would highly suggest looking into OpenTelemetry. It's the open source standard for observability and supports wide variety of languages and frameworks. Also, it's used for general application related telemetry as well, so you would be able to monitor your AI usage + your entire application as well, with the same framework. Additionally, because it's open source, OpenTelemetry is vendor neutral, allowing you to choose from a wide variety of OpenTelemetry compatible observability platforms, and making plug and play super straightforward.",
          "score": 1,
          "created_utc": "2026-01-16 17:37:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznu9er",
          "author": "Hot_Substance_9432",
          "text": "Yes you can use LangSmith etc for observability and also RAGAS",
          "score": 0,
          "created_utc": "2026-01-15 02:54:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qc0x89",
      "title": "How are people managing agentic LLM systems in production?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qc0x89/how_are_people_managing_agentic_llm_systems_in/",
      "author": "Silly-Hand-9389",
      "created_utc": "2026-01-13 19:35:47",
      "score": 7,
      "num_comments": 8,
      "upvote_ratio": 0.82,
      "text": "Anyone running agentic LLM systems in production? Curious how you‚Äôre handling things once it‚Äôs more than a single prompt or endpoint.\n\nI keep running into issues around cost and token usage at the agent level, instrumentation feeling hacked on, and very little ability to manage things at runtime (budgets, guardrails, retries, steering) instead of just looking at logs after something breaks. Debugging and comparing runs also feels way harder than it should be.\n\nNot selling anything, just trying to understand what people are actually struggling with, what you‚Äôve built yourselves, and what you‚Äôd never want to maintain in-house.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qc0x89/how_are_people_managing_agentic_llm_systems_in/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nzlt7br",
          "author": "gkarthi280",
          "text": "LLM observability is essentially required these days when it comes to scaling up your AI applications and putting them into production. Because LLMs are non-deterministic, it is very hard to \"see\" whats going on under the hood between the user input and llm output. Especially when you deal with AI agents, many things can go wrong like high latency, incorrect tool calls, agent loops, tool errors, and muhc more. Without observability you're basically flying blind and it's very tough to actually gain visiblity into what's happening and what's causing specific things to degrade. \n\n  \nWith that being said, I think OpenTelemetry is the way to go, as the open source standard for observability. There are many libraries using OpenTelemetry that you can use to instrument many popular AI provider and frameworks these days like OpenInference, Traceloop, Langtrace, and OpenLIT. What makes OpenTelemetry great is it's open source aspect, allowing you to choose any vendor you want. As long as you use a OpenTelemetry compatible backend, the plug and play into your existing AI stack is very straightforward, with little to no code change. \n\n\\-",
          "score": 3,
          "created_utc": "2026-01-14 20:35:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmbpa6",
          "author": "sandman_br",
          "text": "And all answers are bots",
          "score": 3,
          "created_utc": "2026-01-14 21:59:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhqdvo",
          "author": "code_vlogger2003",
          "text": "Hey hi, im managing multi agentic architecture production. We used the langchain. We are storing every intermediate steps, scratch pad , and own infra hosted models token details etc. most importantly we didn't use any guard rails because we are providing services as a button in one of the client cores products and also all the dbs were configured only with the read only access. What's the exact problem you are facing?",
          "score": 1,
          "created_utc": "2026-01-14 05:42:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjyq5j",
              "author": "Silly-Hand-9389",
              "text": "Good question. The main issues are around attribution and control, not logging. Once you have multiple agents and multiple model calls in a single run, it‚Äôs hard to cleanly attribute cost and tokens per agent, compare one run to another, or enforce things like budgets or limits at runtime instead of after the fact. We can store all the steps, but turning that into something actionable or governable is where it breaks down. We are currently looking for some tooling that could help us get to the next level.",
              "score": 1,
              "created_utc": "2026-01-14 15:35:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nznrx6i",
                  "author": "code_vlogger2003",
                  "text": "Hey for example, if we use the create agent of langchain, it is very easy to see everything with detailed structured messages of the agent state. For every agent call you can easily checkout the input, output and cost for the run and most importantly they have the feature called a cumulative addition of the costs. For budgeting it's better to have a rough estimate of how much no input and output tokens roughly used in the entire end to end call. Once we know this we can set some hard capped limits in the llm initialisation. For validation the first step is simply start to validate with ground truth tool call traces vs inference tool call traces. I mean first compare whether for a question how many tools calls it made and what are the names of these tools. But this is one level of idea. If you go deeper, i recommend hamel methodology as follows:- \n\nHamel Husain's approach for agentic evals starts with end-to-end success (e.g., does the agent achieve the goal?), then granular step-level diagnostics like tool selection accuracy and parameter extraction.https://hamel.dev/blog/posts/evals-faq/how-do-i-evaluate-agentic-workflows.html For validation, first compare ground truth tool call traces (number of calls, tool names, params) against inference traces; extend to failure matrices mapping success states to error points.",
                  "score": 1,
                  "created_utc": "2026-01-15 02:40:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzoeuyg",
          "author": "bugtank",
          "text": "Agents for prototypes, code for full prod.",
          "score": 1,
          "created_utc": "2026-01-15 05:08:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzrhhjm",
          "author": "IntermediateSwimmer",
          "text": "I started using AgentCore Observability from AWS and honestly I'm pretty impressed with it. Might use more of the AgentCore platform. Already baked in to AWS which we use, doesn't care what LLM it is, supports CrewAI, Langchain. Made my life easier for sure, but we didn't have a whole lot of observability in the first place.\n\nIt solved my problem of not being able to explain to our CTO why the agent got stuck in a loop, hallucinated, etc that I wasn't getting out of the box with LangGraph",
          "score": 1,
          "created_utc": "2026-01-15 17:29:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkho4z",
          "author": "QuirkyCharity9739",
          "text": "This is why we created SudoDog. Check out the Dashboard (free), let us know what you think: [https://dashboard.sudodog.com/login](https://dashboard.sudodog.com/login)",
          "score": 1,
          "created_utc": "2026-01-14 17:01:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdd2ds",
      "title": "Are you using any SDKs for building AI agents?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qdd2ds/are_you_using_any_sdks_for_building_ai_agents/",
      "author": "finally_i_found_one",
      "created_utc": "2026-01-15 07:21:29",
      "score": 6,
      "num_comments": 13,
      "upvote_ratio": 0.81,
      "text": "We shipped an ai agent without using any of the agent building SDKs (openai, anthropic, google etc). It doesn't require much maintenance but time to time we find cases where it breaks (ex: gemini 3.x models needed the input in a certain fashion).\n\nI am wondering if any of these frameworks make it easy and maintainable.\n\nHere are some of our requirements:  \n\\- Integration with custom tools  \n\\- Integration with a variety of LLMs  \n\\- Fine grain control over context  \n\\- State checkpointing in between turns (or even multiple times a turn)  \n\\- Control over the agent loop (ex: max iterations)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qdd2ds/are_you_using_any_sdks_for_building_ai_agents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nzscrqd",
          "author": "saurabhjain1592",
          "text": "One pattern we‚Äôve seen is that most SDKs are optimized for building the agent loop, not for operating it once it becomes stateful and long lived.\n\nThings like checkpointing, loop control, and provider abstraction are necessary but not sufficient once agents start retrying, branching, or touching real systems. At that point, the hard problems show up around partial execution, rollback, and explaining why a step was allowed to proceed.\n\nFrameworks help you get started faster. The production pain tends to appear later, when you need control and auditability rather than more abstractions.",
          "score": 2,
          "created_utc": "2026-01-15 19:50:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzt42k3",
              "author": "caprica71",
              "text": "So is langgraph helping here or not really?",
              "score": 1,
              "created_utc": "2026-01-15 21:56:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzt926s",
                  "author": "saurabhjain1592",
                  "text": "LangGraph helps with structuring the agent loop and making execution more explicit, which is a real step forward compared to ad-hoc chains.\n\nWhere teams still tend to struggle is once that graph is long-lived and interacting with real systems. Things like enforcing permissions per step, handling partial execution and rollback, or explaining *why* a transition was allowed usually sit outside the framework itself.\n\nIn practice, it reduces complexity at build time, but you still need additional control and observability once the system is running in production.",
                  "score": 1,
                  "created_utc": "2026-01-15 22:20:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzqc7do",
          "author": "mdrxy",
          "text": "LangChain does all of this ;)",
          "score": 3,
          "created_utc": "2026-01-15 14:17:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzp68zo",
          "author": "Dan6erbond2",
          "text": "I find an SDK like Vercel's AI SDK a must. Being able to switch providers on the fly is one reason, handling input types uniformly is another. And in the case of Vercel's it also handles structured output and streaming really well. I don't see why I should have to reinvent the wheel.\n\nEdit: As for features the SDK supports custom tools, stopWhen conditions, context control and message shortening and all kinds of hooks to interrupt/change the flow. I haven't had any issues getting it to do what I need.",
          "score": 2,
          "created_utc": "2026-01-15 09:05:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqez3j",
          "author": "FormalAd7367",
          "text": "yea. my next project. will be using google‚Äôs.",
          "score": 1,
          "created_utc": "2026-01-15 14:31:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqgg6x",
          "author": "Reasonable-Life7326",
          "text": "Yeah, frameworks are a lifesaver for this stuff.",
          "score": 1,
          "created_utc": "2026-01-15 14:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzrcp06",
          "author": "eavanvalkenburg",
          "text": "Agent Framework does all this for both dotnet and python",
          "score": 1,
          "created_utc": "2026-01-15 17:08:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzrlfak",
          "author": "Sunchax",
          "text": "No, build my own per project, have used pydentic for some work.\n\nMainly got burned by langchain back in they day and got allergic since then, but maybe things are better now",
          "score": 1,
          "created_utc": "2026-01-15 17:47:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzt46mg",
              "author": "caprica71",
              "text": "Just pydantic or pydantic ai?",
              "score": 1,
              "created_utc": "2026-01-15 21:56:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzuti9v",
          "author": "abzisse",
          "text": "We tried all agent frameworks (LangChain, n8n, CrewAI and others) and did not find it easy to build agents that actually work. \"Not easy\" because there is a learning curve and they seem to add a lot of boilerplate code and then you still need to code your own tools to make the agent functional, more deterministic etc... So we decided to build [a2abase.ai](http://a2abase.ai/)¬†to make building agents (not just workflow automation) easy with access to all major LLMs, 50+ native tools and 500+ MCP servers all under 1 account.",
          "score": 1,
          "created_utc": "2026-01-16 03:26:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvr2n2",
          "author": "Worldly-Pen-8101",
          "text": "Exploring  dbos . The Lang* packages are too bloated IMO",
          "score": 1,
          "created_utc": "2026-01-16 07:24:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05ct8v",
          "author": "PangolinPossible7674",
          "text": "Most agent-building frameworks satisfy your requirements. Regarding LLMs, I have been using LiteLLM, which provides unified API. So, I only need to swap the model name when required.\n\n\nHowever, crafting a system prompt (for agents, in this case) that works with diverse LLMs can take some effort, like you have noted. I have experienced this while I have been building KodeAgent. If you are looking for a minimal agent engine or just some ideas, you can give it a try:¬†https://github.com/barun-saha/kodeagent",
          "score": 1,
          "created_utc": "2026-01-17 18:29:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qf64ge",
      "title": "Learning multiagents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qf64ge/learning_multiagents/",
      "author": "crionuke",
      "created_utc": "2026-01-17 06:42:42",
      "score": 6,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "I am trying to understand multi-agent systems by reading materials online and by building my own prototypes and experiments.\n\nIn most discussions, the term agent is used very broadly. However, I have noticed that it actually refers to two fundamentally different concepts.\n\n1. Agent as an abstraction over an LLM call\n\nIn this model, an agent is essentially a wrapper around an LLM invocation. It is defined by a unique role and a contract for input and output data.\n\nSuch agents do not have a decision loop. They usually provide simple request‚Äìresponse behavior, similar to an API endpoint.\n\n2. Autonomous code agents\n\nExamples include Claude Code, OpenCode, and similar tools. These agents can not only generate code, but also execute tasks and coordinate complex workflows.\n\nThe key difference is that they have their own decision loop. They can plan, act, observe results, and continue working autonomously until a goal is achieved.\n\n\\---\n\nBuilding a multi-agent system composed of agents of the first type is not particularly interesting to me. It is primarily an integration problem.\n\nWhile it is possible to design non-trivial architectures, such as:\n\n\\- agent graphs with or without loops,\n\n\\- routing or pathfinding logic to select the minimal set of agents required to solve a task,\n\nthe agents themselves remain passive and reactive.\n\nWhat I truly want to understand is how to build systems composed of autonomous agents that operate inside their own decision loops and perform real work independently.\n\nThat is the part of multi-agent systems I am trying to learn. \n\nWelcome any comments on the topics.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qf64ge/learning_multiagents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o02gsnl",
          "author": "mdrxy",
          "text": "nice- you may enjoy these new docs: [https://docs.langchain.com/oss/python/langchain/multi-agent/index](https://docs.langchain.com/oss/python/langchain/multi-agent/index)",
          "score": 1,
          "created_utc": "2026-01-17 07:14:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02kixa",
              "author": "crionuke",
              "text": "I quickly went through the docs. The agent described there is a rather low-level entity that requires you to implement all the tools yourself. What makes this especially difficult is having to build your own context management.\n\nHowever, there are already mature agents such as Claude Code, OpenCode, etc. Any thought about reusing them instead of reengineering solutions that have already been solved?",
              "score": 1,
              "created_utc": "2026-01-17 07:48:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o04n8n2",
          "author": "Khade_G",
          "text": "Most multi-agent examples I‚Äôve seen online are really role-based LLM calls stitched together. They‚Äôre useful, but as you said that‚Äôs mostly an integration/orchestration problem. The agents don‚Äôt decide, they‚Äôre invoked. Autonomous agents are a different beast. The defining feature isn‚Äôt that they call tools, it‚Äôs that they own a control loop: goal ‚Üí plan ‚Üí act ‚Üí observe ‚Üí update state ‚Üí repeat. Once you do that, the hard problems stop being prompts and start being systems problems: state management, stopping conditions, failure recovery, credit assignment, and coordination between loops.\n\nA few patterns that show up in real autonomous multi-agent systems:\n- Explicit internal state: each agent has its own persistent state (goals, assumptions, progress), not just shared context.\n- Bounded autonomy: agents don‚Äôt free-run forever; they operate inside constraints (budgets, max steps, human checkpoints).\n- Communication as messages, not prompts: agents exchange structured messages/events, often asynchronously, rather than sharing giant contexts.\n- Task ownership: one agent owns a goal and decides when to ask others for help, instead of a central router deciding everything.\n- Failure as a first-class outcome: agents are allowed to say ‚ÄúI‚Äôm stuck,‚Äù escalate, or back off‚Ä¶ otherwise loops collapse into thrashing.\n\nFramework-wise, this is why things like LangGraph, AutoGPT-style planners, or workflow engines + agents exist: not to make agents smarter, but to make decision loops survivable. The intelligence comes from how you bound and coordinate those loops, not from adding more agents.\n\nA good exercise is to build one autonomous agent that can run for hours, survive restarts, and explain what it‚Äôs doing. Then add a second agent and make coordination the problem. The learning is in the doing",
          "score": 1,
          "created_utc": "2026-01-17 16:31:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05sn64",
              "author": "crionuke",
              "text": "My current idea is that I do not need autonomous agents to run for hours or longer.\n\nIn most cases, it is enough to run an agent once to accomplish a specific task, commit the changes along with all related information, and then exit.\n\nAny thoughts?",
              "score": 1,
              "created_utc": "2026-01-17 19:44:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o04ulvw",
          "author": "DaRandomStoner",
          "text": "If it's an llm that can make tool calls I call it an agent. I don't understand why people want to make the definition all complicated.",
          "score": 1,
          "created_utc": "2026-01-17 17:05:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05poa0",
              "author": "crionuke",
              "text": "As many people, as many opinions. There is no specification yet.\n\nIn some sources, an AI agent is defined simply as:\nAI agent = LLM + tools\n\nAt the same time, there are coding agents on the market that include their own decision loop.\n\nBesides that, the term sub-agents is also widely used.\n\nOn top of this, people often talk about multi-agent systems while meaning different things above by the word agent, which makes the whole topic confusing.",
              "score": 1,
              "created_utc": "2026-01-17 19:30:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o05qlxx",
                  "author": "DaRandomStoner",
                  "text": "So let's simplify it... agent = llm + tools\n\nSubagent = agent that gets called by another agent\n\nMulti agent system = a system where multiple agents with their own context windows collaborate.\n\nDecision loops are just something agents can do... like how a while loop is something a python script can do.",
                  "score": 1,
                  "created_utc": "2026-01-17 19:34:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}