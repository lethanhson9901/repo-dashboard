{
  "metadata": {
    "last_updated": "2026-02-10 09:20:00",
    "time_filter": "week",
    "subreddit": "LangChain",
    "total_items": 20,
    "total_comments": 47,
    "file_size_bytes": 85798
  },
  "items": [
    {
      "id": "1qwno6v",
      "title": "I built “Vercel for AI agents” — a single click deployment platform for any framework",
      "subreddit": "LangChain",
      "url": "/r/aiagents/comments/1qwnnlq/i_built_vercel_for_ai_agents_a_single_click/",
      "author": "Hisham_El-Halabi",
      "created_utc": "2026-02-05 15:05:46",
      "score": 65,
      "num_comments": 3,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qwno6v/i_built_vercel_for_ai_agents_a_single_click/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o3qp3w1",
          "author": "Existing_Way2258",
          "text": "I hope this is better than LangSmith LOL. Pls add LangChain support soon, I'm curious to try it out",
          "score": 0,
          "created_utc": "2026-02-05 16:32:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qpii9",
              "author": "Hisham_El-Halabi",
              "text": "haha noted, we’re working on it. You can join the waitlist on our website so get notified when we launch with langchain support",
              "score": 1,
              "created_utc": "2026-02-05 16:34:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3s03w7",
              "author": "MathematicianTop1654",
              "text": "check out [crewship.dev](http://crewship.dev), it supports LangGraph",
              "score": 1,
              "created_utc": "2026-02-05 20:10:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qw8s1s",
      "title": "I visualized the LLM workflows of the entire LangChain repo",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/bz7sbdzd6lhg1",
      "author": "Cyanosistaken",
      "created_utc": "2026-02-05 02:23:18",
      "score": 47,
      "num_comments": 8,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qw8s1s/i_visualized_the_llm_workflows_of_the_entire/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3nfyhe",
          "author": "Hungry_Age5375",
          "text": "Try visualizing agent workflows instead. Full repo mapping is like charting every neuron - cool but useless for actual work.",
          "score": 2,
          "created_utc": "2026-02-05 02:55:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oefcg",
              "author": "Relevant-Magic-Card",
              "text": "i still find this pretty useful info",
              "score": 1,
              "created_utc": "2026-02-05 06:59:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3p8m6t",
          "author": "Enough-Blacksmith-80",
          "text": "Very cool, man!",
          "score": 1,
          "created_utc": "2026-02-05 11:40:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qe5po",
              "author": "Cyanosistaken",
              "text": "Thanks! ",
              "score": 1,
              "created_utc": "2026-02-05 15:42:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zvsvt",
                  "author": "_harryj",
                  "text": "How did you manage the performance while visualizing such a large repo? Any tips for optimizing these kinds of workflows?",
                  "score": 1,
                  "created_utc": "2026-02-07 00:08:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4b9a65",
          "author": "varun2411",
          "text": "This is good but why only gemini API? It would be good to use any llm including cloud or local",
          "score": 1,
          "created_utc": "2026-02-08 20:25:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ba3vw",
              "author": "Cyanosistaken",
              "text": "yeah good question! I tested it with claude, gpt, and gemini of various models. gemini 2.5 unironically performs the best on the benchmark, even compared to 3.0. I'm also giga broke and self funded, so it's a good thing that the best model for this is the cheapest ",
              "score": 1,
              "created_utc": "2026-02-08 20:29:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvorim",
      "title": "Scalable RAG with LangChain: Handling 2GB+ datasets using Lazy Loading (Generators) + ChromaDB persistence",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qvorim/scalable_rag_with_langchain_handling_2gb_datasets/",
      "author": "jokiruiz",
      "created_utc": "2026-02-04 13:37:20",
      "score": 21,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nWe all love how easy `DirectoryLoader` is in LangChain, but let's be honest: running `.load()` on a massive dataset (2GB+ of PDFs/Docs) is a guaranteed way to get an OOM (Out of Memory) error on a standard machine, since it tries to materialize the full list of Document objects in RAM.\n\nI spent some time refactoring a RAG pipeline to move from a POC to a production-ready architecture capable of ingesting gigabytes of data.\n\n**The Architecture:** Instead of the standard list comprehension, I implemented a **Python Generator pattern (**`yield`**)** wrapping the LangChain loaders.\n\n* **Ingestion:** Custom loop using `DirectoryLoader` but processing files lazily (one by one).\n* **Splitting:** `RecursiveCharacterTextSplitter` with a 200 char overlap (crucial for maintaining context across chunk boundaries).\n* **Embeddings:** Batch processing (groups of 100 chunks) to avoid API timeouts/rate limits with `GoogleGenerativeAIEmbeddings` (though `OpenAIEmbeddings` works the same way).\n* **Storage:** `Chroma` with `persist_directory` (writing to disk, not memory).\n\nI recorded a deep dive video explaining the code structure and the specific LangChain classes used: [**https://youtu.be/QR-jTaHik8k?si=l9jibVhdQmh04Eaz**](https://youtu.be/QR-jTaHik8k?si=l9jibVhdQmh04Eaz)\n\nI found that for this volume of data, Chroma works well locally. Has anyone pushed Chroma to 10GB+ or do you usually switch to Pinecone/Weaviate managed services at that point?",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qvorim/scalable_rag_with_langchain_handling_2gb_datasets/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3jeuxl",
          "author": "pbalIII",
          "text": "Generator pattern is solid for ingestion, but the real question at 10GB+ isn't Chroma vs managed services... it's whether you actually need all that data indexed.\\n\\nChroma has a documented RAM ceiling (can't exceed system memory without LRU cache tuning) and some users hit stability issues around 300k chunks. But before jumping to Pinecone or Weaviate, worth asking: how much of that 2GB+ corpus actually gets retrieved? In most RAG pipelines I've seen, 80% of queries hit maybe 5% of the index.\\n\\nTwo paths:\\n\\n1. Tiered indexing: keep hot docs in Chroma, cold docs in cheaper blob storage with on-demand embedding\\n2. Aggressive deduplication + summarization upstream to shrink the actual index size\\n\\nManaged services solve scale, but they don't solve the retrieval quality problem of having too much noise in the index. Sometimes the better move is pruning before scaling.",
          "score": 3,
          "created_utc": "2026-02-04 14:54:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gkjgv",
          "author": "Main_Payment_6430",
          "text": "the lazy loading pattern is solid for memory but curious how youre handling failures during the batch embedding process. if one batch hits rate limit or api error do you retry the whole batch or track which chunks already succeeded?\n\ncause if your ingestion loop crashes halfway through 2gb and you dont have per chunk state tracking you might re embed stuff that already worked or skip chunks that failed. especially brutal if youre paying per embedding call.\n\nalso yeah chroma local works fine for 10gb plus if you tune sqlite properly but managed services handle retries and replication better when things break at scale.",
          "score": 1,
          "created_utc": "2026-02-09 16:53:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwpf3u",
      "title": "AG-UI: the protocol layer for LangGraph/LangChain UIs",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qwpf3u/agui_the_protocol_layer_for_langgraphlangchain_uis/",
      "author": "Acrobatic-Pay-279",
      "created_utc": "2026-02-05 16:10:42",
      "score": 20,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Agent UIs in LangChain / LangGraph usually start simple: stream final text, maybe echo some logs. But as soon as the goal is real interactivity: step‑level progress, visible tool calls, shared state, retries - the frontend ends up with a custom event schema tightly coupled to the backend.\n\nI have been digging into the AG‑UI (Agent-User Interaction Protocol) which is trying to standardize that layer. It defines a typed event stream that any agent backend can emit and any UI can consume. Instead of “whatever JSON is on the WebSocket” -- there is a small set of event kinds with clear semantics.\n\nAG-UI is not a UI framework and not a model API -- it’s basically the contract between an agent runtime and the UI layer. It groups all the events into core high-level categories:\n\n* Lifecycle: `RunStarted`, `RunFinished`, `RunError`, plus optional `StepStarted` / `StepFinished` that map nicely onto LangGraph nodes or LangChain tool/chain steps.\n* Text streaming: `TextMessageStart`, `TextMessageContent`, `TextMessageEnd` (and a chunk variant) for incremental LLM output.\n* Tool calls: `ToolCallStart`, `ToolCallArgs`, `ToolCallEnd`, `ToolCallResult` so UIs can render tools as first‑class elements instead of log lines.\n* State management: `StateSnapshot` and `StateDelta` (JSON Patch) for synchronizing shared graph/application state, with `MessagesSnapshot` available to resync after reconnects.\n* Special events: custom events in case an interaction doesn’t fit any of the categories above\n\nEach event has a `type` (such as `TextMessageContent`) plus a payload. There are other properties (like `runId`, `threadId`) that are specific to the event type.   \n  \nBecause the stream is standard and ordered, the frontend can reliably interpret what the backend is doing\n\nThe protocol is **transport‑agnostic**: SSE, WebSockets, or HTTP chunked responses can all carry the same event envelope. If a backend emits an AG‑UI‑compatible event stream (or you add a thin adapter), the frontend wiring can stay largely the same across different agent runtimes.\n\nFor people building agents: curious whether this maps cleanly onto the events you are already logging or streaming today, or if there are gaps.  \n  \n[Events docs](https://docs.ag-ui.com/concepts/events)  \nrepo: [https://github.com/ag-ui-protocol/ag-ui](https://github.com/ag-ui-protocol/ag-ui)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qwpf3u/agui_the_protocol_layer_for_langgraphlangchain_uis/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3r8zc4",
          "author": "Informal_Tangerine51",
          "text": "Protocol standardization is useful but doesn't solve the production debugging problem.\n\nStandard event stream helps UI consistency, but when your agent makes a wrong decision, can you reconstruct what it saw? `ToolCallStart` \\+ `ToolCallResult` show what happened, but not what data the tool returned, how fresh it was, or why the agent chose this tool over others.\n\nThe real production gap: evidence capture, not event standardization. When tool call 47 at 3am returns wrong data, you need content lineage (what was retrieved), policy decisions (was this action authorized), and regression fixtures (how to prevent this after model update).\n\nAG-UI events are ephemeral UI updates. Incident debugging needs durable, verifiable records. `StateSnapshot` helps resume execution but doesn't prove what data informed decisions or help debug why behavior changed between runs.\n\nFor production agents: are you capturing decision context (retrieval results, policy evaluations, input data) separately from UI events? Or assuming event logs are enough for post-incident analysis?\n\nStandard protocols are good for interop. Evidence infrastructure is what makes agents debuggable at scale.",
          "score": 1,
          "created_utc": "2026-02-05 18:05:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rpw94",
              "author": "Acrobatic-Pay-279",
              "text": "fair point and I agree with the distinction you are making, though interop and debuggability are two different concerns.\n\nI don't think AG-UI is trying (or claiming) to solve production debugging, evidence capture or auditability. it's intentionally scoped to the agent-UI boundary, making agent execution observable to the user in a consistent way.\n\nwhere it might help indirectly is by providing consistent run/step IDs and typed lifecycle/text/tool/state events (plus Raw/Custom escape hatches) that other observability systems can hang off. But on its own, that's clearly not sufficient for incident debugging\n\nwe would still need durable traces like LangSmith-style runs, retrieval snapshots, policy decisions. AG-UI feels complementary to that layer rather than a replacement.",
              "score": 2,
              "created_utc": "2026-02-05 19:22:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3rk7ki",
              "author": "Niightstalker",
              "text": "This protocol does not target production debugging, logging though as far as I understand. \n\nRegarding production debugging/monitoring I would turn to tools like LangSmith or LangFuse.  Those target exactly the points you mentioned",
              "score": 1,
              "created_utc": "2026-02-05 18:56:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3r6ouk",
          "author": "Number4extraDip",
          "text": "Lookup a2ui",
          "score": 0,
          "created_utc": "2026-02-05 17:54:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rpwnj",
              "author": "Acrobatic-Pay-279",
              "text": "A2UI is more of a declarative generative UI spec/payload and you can definitely translate A2UI messages into AG‑UI and then stream them + handles sync  \n  \nstill this post is about a different layer..",
              "score": 1,
              "created_utc": "2026-02-05 19:22:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3skzoe",
                  "author": "Number4extraDip",
                  "text": "Sure. Just thought it might be a useful bridge",
                  "score": 1,
                  "created_utc": "2026-02-05 21:51:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qxg9dd",
      "title": "3D-Agent multi agent system with LangChain for Blender AI",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/kprnhlhgavhg1",
      "author": "Large-Explorer-8532",
      "created_utc": "2026-02-06 12:27:15",
      "score": 19,
      "num_comments": 10,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qxg9dd/3dagent_multi_agent_system_with_langchain_for/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3ym0vy",
          "author": "Jorsoi13",
          "text": "Cool! Thanks for so much detail! When you mean „verify the frame“ you are talking about sending a screenshot to the agent or how does verification prevent drift? \n\nI‘m also currently building an agent, however I just started out literally a week ago. I‘m really trying to grasp how to orchestrate everything together. I‘m still lacking best practices and Langchain Docs are also a b*tch when it comes to documentation and integrating it with the frontend like yours (nextjs, etc.) \n\nDid you deploy using LangSmith or did you set up a self hosted version yourself? We‘re currently debating what the best approach is since we don’t want to pay 40€ just for their deployment. I was more hoping for an „n8n self-host approach“ on Digitalocean for like 5$ a month :)",
          "score": 3,
          "created_utc": "2026-02-06 20:07:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ynskn",
              "author": "Large-Explorer-8532",
              "text": "We do not use langsmith, we have a mix of custom frameworks and langchain.\n\nI would recomend self-hosted all the way. No reason to pay $40/month for LangSmith when you're starting out. Your n8n on DigitalOcean instinct is solid. Keep it simple, it is easy to get lost buying endless things.",
              "score": 1,
              "created_utc": "2026-02-06 20:16:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zbhks",
                  "author": "mdrxy",
                  "text": "starting out, LangSmith has a free plan for developers ;)",
                  "score": 2,
                  "created_utc": "2026-02-06 22:15:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3zbc8k",
          "author": "mdrxy",
          "text": "cool can you talk more about the langchain architecture you used?",
          "score": 2,
          "created_utc": "2026-02-06 22:14:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o422hvp",
              "author": "Large-Explorer-8532",
              "text": "[](https://docs.langchain.com/oss/python/deepagents/skills)it gives us ready-to-use primitives for subagent orchestration, isolation, memory, and context management out of the box. Each agent (Gemini, GPT, Claude) runs in its own isolated node with its own context window, and LangGraph handles the routing, state persistence, and handoffs between them. It's what lets us keep each agent focused on what it's best at without them stepping on each other. Happy to go deeper on any specific part if you're curious![](https://docs.langchain.com/oss/python/deepagents/skills)",
              "score": 1,
              "created_utc": "2026-02-07 09:58:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3zd2up",
          "author": "Upstairs-Spell7521",
          "text": "why do you use langchain tho? what advantages it gives to you?",
          "score": 2,
          "created_utc": "2026-02-06 22:23:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o421v7z",
              "author": "Large-Explorer-8532",
              "text": "LangChain/LangGraph has some really useful built-in features for subagent orchestration, memory management, human-in-the-loop workflows, and other ways to coordinate a multi-agent system. We've also experimented with skills from DeepAgent... mixed feelings on that so far  \nThe main benefit is not having to build these primitives from scratch, having them ready to use saves us hours of work",
              "score": 1,
              "created_utc": "2026-02-07 09:52:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xiado",
          "author": "Jorsoi13",
          "text": "Wow! Thats amazing. I mean the visuals of that Eiffel Tower are debateable but the fact that it works is great. Now the next step is probably reducing friction. It seems like there are a lot of steps to actually start using the product at least thats what your dashboard suggests. \n\nOther than that: \n\n\\- The agent runs really long. I thought \"Oh shit that thing must swallow some serious credits\". Am I right? How much money did you spend on creating the eiffel tower? Or how many tokens does an operation like that generate? \n\n\\- What is your agent orchestration like? I would really love to see your graph structure for the sake of learning. Would you mind sharing it ?",
          "score": 1,
          "created_utc": "2026-02-06 16:57:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yiph7",
              "author": "Large-Explorer-8532",
              "text": "Thanks man, really appreciate the thoughtful feedback!\n\nyeah the Eiffel Tower is definitely rough blocking, not a portfolio piece. But the point is the agent built it autonomously from a single prompt, which is the hard part. Quality will keep improving as the models get better.\n\nyou're 100% right and we're actively working on this. The onboarding has too many steps right now. Goal is: install the plugin → connect → start chatting. We're cutting it down.\n\nhonestly it's less than you'd think. The agent is smart about batching operations and only calls the reasoning model when it actually needs to make a decision. Most of the \"thinking\" steps are lightweight. The long runtime is mostly Blender executing the code, not the AI burning tokens. The output are short mostly short.\n\nThe high level is: we use a planning agent that breaks the task into stages, another ones comes to reason and think in \"3D/Spatial Math\" then an execution agent handles each stage in a loop (perceive scene → decide next action → execute code → verify via viewport). The key insight was adding the verification step... without it the agent drifts and you get garbage. There's a router in between that decides when to escalate to the reasoning model vs handle it with a faster/cheaper one. Would love to do a deeper write-up at some point once we've solidified the architecture more.\n\nWhat's your background? You sound like you're building agents yourself ",
              "score": 2,
              "created_utc": "2026-02-06 19:51:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qv0mmr",
      "title": "We monitor 4 metrics in production that catch most LLM quality issues early",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qv0mmr/we_monitor_4_metrics_in_production_that_catch/",
      "author": "dinkinflika0",
      "created_utc": "2026-02-03 18:52:58",
      "score": 15,
      "num_comments": 4,
      "upvote_ratio": 0.94,
      "text": "After running LLMs in production for a while, we've narrowed down monitoring to what actually predicts failures before users complain.\n\nLatency p99: Not average latency - p99 catches when specific prompts trigger pathological token generation. We set alerts at 2x baseline.\n\nQuality sampling at configurable rates: Running evaluators on every request burns budget. We sample a percentage of traffic with automated judges checking hallucination, instruction adherence, and factual accuracy. Catches drift without breaking the bank.\n\nCost per request by feature: Token costs vary significantly between features. We track this to identify runaway context windows or inefficient prompt patterns. Found one feature burning 40% of inference budget while serving 8% of traffic.\n\nError rate by model provider: API failures happen. We monitor provider-specific error rates so when one has issues, we can route to alternatives.\n\nWe log everything with distributed tracing. When something breaks, we see the exact execution path - which docs were retrieved, which tools were called, what the LLM actually received.\n\nSetup details: [https://www.getmaxim.ai/docs/introduction/overview](https://www.getmaxim.ai/docs/introduction/overview)\n\nWhat production metrics are you tracking?",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qv0mmr/we_monitor_4_metrics_in_production_that_catch/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3eivw9",
          "author": "Ecto-1A",
          "text": "It really comes down to what you are doing and if you are doing any RAG. What you outlined all seems pretty standard. We monitor latency, tokens, relevance of response, proper tool calling, turns to resolution, confidence, and error handling on every run. Any that fall below our threshold as well as a 20% sample of all runs get sent to an annotation queue and kick off a full suite of G-Eval evaluators and we are working to build out a new testing suite based on the CheckEval paper published a couple months ago.\n\nAre you running any evaluators at build time? That has definitely helped catch some things that could have otherwise flooded our evaluator queues.",
          "score": 2,
          "created_utc": "2026-02-03 20:12:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f1fyh",
          "author": "Informal_Tangerine51",
          "text": "You're monitoring outputs but not capturing inputs. When quality sampling flags hallucination, can you replay what was retrieved to cause it?\n\nWe track similar metrics. The debugging gap: p99 latency spike happens, we know which prompt triggered it, but not what documents were retrieved or whether context was stale. Error rate shows provider failure, doesn't show if retry used different data.\n\nYour distributed tracing logs execution path. Does it capture the actual retrieved content with timestamps, or just that retrieval happened? When evaluator flags factual error, can you verify the source chunks were current?\n\nMetrics catch problems. Evidence proves why they happened. Cost per request is useful, but when that 40% budget feature produces wrong output, can you prevent recurrence or just know it's expensive?",
          "score": 1,
          "created_utc": "2026-02-03 21:39:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gk8um",
          "author": "Main_Payment_6430",
          "text": "an llm can hit normal latency on each individual call but if it retries the same failed action 50 times your total request time and cost explodes.\n\ncost per request by feature is good but do you track cost per attempt vs cost per successful completion? cause a feature might look expensive but if half the cost is from retries that failed anyway thats a different problem than just inefficient prompts.\n\nalso yeah distributed tracing is critical. without execution history you cant debug why something looped or burned budget.",
          "score": 1,
          "created_utc": "2026-02-09 16:52:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r08s87",
      "title": "What's everyone using to deploy LangChain agents to production?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r08s87/whats_everyone_using_to_deploy_langchain_agents/",
      "author": "MathematicianTop1654",
      "created_utc": "2026-02-09 16:28:35",
      "score": 12,
      "num_comments": 3,
      "upvote_ratio": 0.94,
      "text": "Curious what production setups people are running for their LangChain agents/workflows.\n\nI've been cobbling together FastAPI + Docker + some kind of queue system (currently trying Celery), but honestly it feels like I'm reinventing the wheel. Dealing with timeouts, scaling, versioning, keeping secrets organized - it works but it's a lot of moving parts.\n\nWhat are you all using? Are most people just building custom infra, or are there patterns/tools that make this smoother?\n\nSpecifically interested in:\n\n* How you handle long-running agent workflows (async patterns, webhooks, polling?)\n* Deployment/orchestration setup (k8s, serverless, something else?)\n* Managing different versions when you're iterating quickly\n* Observability - how do you actually debug when an agent does something weird in prod?\n\nWould love to hear what's working well for people, or if there are resources/repos I should check out to level up my setup.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1r08s87/whats_everyone_using_to_deploy_langchain_agents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o4gy661",
          "author": "penguinzb1",
          "text": "the observability piece is harder than most people expect. by the time you're debugging in prod, you've already lost—users saw the weird behavior.\n\nwe ended up leaning heavily on pre-deployment simulation. run the agent through scenarios that mimic prod traffic patterns before it goes live. catches a lot of the \"why did it do that?\" moments early when they're cheap to fix, not after users report them.\n\nfor the stack itself—fastapi works, but if you're hitting timeout issues frequently, might be worth looking at whether those are actually agent behavior problems (going down unproductive paths, getting stuck in loops) vs infrastructure problems. simulating those workflows offline first helps separate the two.",
          "score": 4,
          "created_utc": "2026-02-09 17:58:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i87yr",
          "author": "DullTicket5286",
          "text": "Aegra+langfuse",
          "score": 1,
          "created_utc": "2026-02-09 21:42:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4j1tbb",
              "author": "jlebensold",
              "text": "We wrote an agent that reads your langfuse traces and helps save you money in prod: https://www.jetty.io",
              "score": 1,
              "created_utc": "2026-02-10 00:19:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qunx9g",
      "title": "Why doesn't LangChain support agent skills?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qunx9g/why_doesnt_langchain_support_agent_skills/",
      "author": "Suspicious_Fall6860",
      "created_utc": "2026-02-03 09:55:40",
      "score": 11,
      "num_comments": 11,
      "upvote_ratio": 0.82,
      "text": "Why doesn't LangChain support agent skills? It only allows loading a single [skill.md](http://skill.md) file. How can we support references and scripts?\n\nHere are some materials I found.\n\n[Skills - Docs by LangChain](https://docs.langchain.com/oss/python/langchain/multi-agent/skills)  \n  \n[Build a SQL assistant with on-demand skills - Docs by LangChain](https://docs.langchain.com/oss/python/langchain/multi-agent/skills-sql-assistant)  \n\n\n[deepagents/examples/content-builder-agent/skills/blog-post/SKILL.md at master · langchain-ai/deepagents · GitHub](https://github.com/langchain-ai/deepagents/tree/master/examples)  \n  \n[deepagents/examples/content-builder-agent at master · langchain-ai/deepagents](https://github.com/langchain-ai/deepagents/tree/master/examples/content-builder-agent)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qunx9g/why_doesnt_langchain_support_agent_skills/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3bfsbg",
          "author": "Otherwise_Wave9374",
          "text": "Yeah the single SKILL.md thing feels limiting once you want anything beyond toy examples (versioning, shared snippets, scripts, references, etc). I have seen people treat skills as a mini package, folder per skill with an index plus tests, then load/resolve by name and inject into the agent prompt/runtime. Would be nice if LangChain standardized that pattern. I have a couple writeups saved on agent skills/tooling design here too: https://www.agentixlabs.com/blog/",
          "score": 4,
          "created_utc": "2026-02-03 10:03:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bgsic",
          "author": "Suspicious_Fall6860",
          "text": "Actually, I've found that most current support for Agent Skills is basically in CLI-mode systems. Does anyone know of any frameworks that support explicit skill writing and debugging?",
          "score": 3,
          "created_utc": "2026-02-03 10:12:55",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3bpjrz",
              "author": "Tobi-Random",
              "text": "Official spec project provides tools for validation against spec",
              "score": 1,
              "created_utc": "2026-02-03 11:31:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hojmy",
                  "author": "ohansemmanuel",
                  "text": "I think this is still lacking. At best it validates frontmatter and a few best practices. \n\nIn production systems you'll quickly find out that even with the best Skills, they're (almost) useless if NOT triggered by the AI agent. \n\nCurrent \"eval\" systems don't really work with Skills (at least not in a way I think is optimised). So there's still a need for robust systems to build, test, iterate as we do with prompts today. Something along the lines of trigger evals?\n\nA counter argument would be that Skills are just prompts and you can still get by. True, the difference would be we're bundling a lot more in these \"prompts\"",
                  "score": 1,
                  "created_utc": "2026-02-04 07:08:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3d0ga9",
              "author": "Jords13xx",
              "text": "Check out the Rasa framework for building conversational agents with custom skills. It's pretty robust for skill writing and debugging. Also, you might want to explore Botpress or Dialogflow; they offer good support for skill customization.",
              "score": 1,
              "created_utc": "2026-02-03 16:01:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3chz29",
          "author": "pbalIII",
          "text": "Ran into the same friction building a multi-skill agent last month. The single SKILL.md loader is intentional... LangChain wants skills to be self-contained folders with their own files, scripts, and references bundled together.\n\nThe pattern that worked for me: treat each skill as its own directory under ~/.deepagents/agent/skills/, then let the agent discover and load them by name at runtime. The frontmatter gets indexed for discovery, but the full SKILL.md only loads when the agent actually needs it (saves tokens).\n\nFor debugging, deepagents-CLI has a skills list command that shows what's loaded. Not perfect tooling, but better than dumping everything into one monolithic file.",
          "score": 2,
          "created_utc": "2026-02-03 14:30:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3crow1",
          "author": "cordialgerm",
          "text": "References and scripts are supposed to be loaded on demand by the agent after reading the SKILL.md. so all you need to do is include them in your filesystem and reference them in the SKILL.md and it works great.",
          "score": 1,
          "created_utc": "2026-02-03 15:20:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eofke",
          "author": "Niightstalker",
          "text": "Here they wrote a blogpost about supporting skills within their deep agents cli: https://www.blog.langchain.com/using-skills-with-deep-agents/",
          "score": 1,
          "created_utc": "2026-02-03 20:39:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hmmrj",
          "author": "ohansemmanuel",
          "text": "Technically you could probably get by building a system around this yourself. \n\nYou'd need to connect to a VM / sandboxed machine at runtime, that's capable of running scripts, installin dependencies, leveraging bash for reading additional files etc. You'd then expose tools to interact with the said machine. \n\nYou alluded to the bigger issue in your comment - as an industry it seems we're mostly focused on Skills within CLI (coding agents) atm. But in my opinion, the bigger win comes from the use case you're describing i.e., remote agents running determinsitic workflows / SOPs with references and scripts. \n\nIf you're looking for an off the shelve solution, you may like Bluebag AI (handles all the hard stuff so you can integrate in 2-lines of code) \n\nDisclaimer: I built this and already used in production systems. Would happily walk you through it",
          "score": 1,
          "created_utc": "2026-02-04 06:52:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xs2ng",
          "author": "npentrel",
          "text": "Hi there - just updated the docs a bit to make this clearer: You can use more files, as long as they are referenced in [SKILL.md](http://SKILL.md)  \\- thanks for pointing the needed clarification out to us! [https://docs.langchain.com/oss/python/deepagents/skills](https://docs.langchain.com/oss/python/deepagents/skills)",
          "score": 1,
          "created_utc": "2026-02-06 17:43:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3c5nqq",
          "author": "Upset-Pop1136",
          "text": "langchain’s not trying to be a full agent OS. they optimize for demos and DX, not long-lived agents. ",
          "score": 0,
          "created_utc": "2026-02-03 13:23:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qz6i99",
      "title": "For anyone building agents that need email context: here's what the pipeline actually looks like",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qz6i99/for_anyone_building_agents_that_need_email/",
      "author": "EnoughNinja",
      "created_utc": "2026-02-08 11:22:31",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 0.87,
      "text": "Building an agent that needs to reason over email data and wanted to share what the actual infrastructure requirement looks like, because it was way more than I expected.\n\nThe model/reasoning part is straightforward. The hard part is everything before the prompt:\n\n1. OAuth flows per email provider, per user, with token refresh\n2. Thread reconstruction (nested replies, forwarded messages, quoted text stripping, CC/BCC parsing)\n3. Incremental sync so you're not reprocessing full inboxes\n4. Per-user data isolation if you have multiple users\n5. Cross-thread retrieval, because the answer to most work questions spans multiple conversations\n6. Structured extraction into typed JSON, not prose summaries\n\nOne thing I noticed when running dozens of tests with different models (I used threads with 20+ emails, with 4 or 5 different threads per prompt), is that the thread reconstruction is a completely different problem per provider. \n\nGmail gives you threadId but the message ordering and quoted text handling is inconsistent. Outlook threads differently, and forwarded messages break both. If you're building this yourself, don't assume a universal parser will work.\n\nWe built an API that handles all of this (igpt.ai) because we couldn't find anything that did it well. \n\nOne endpoint, you pass a user ID and a query and get back structured JSON with the context already assembled. \n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qz6i99/for_anyone_building_agents_that_need_email/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o49598a",
          "author": "Jorsoi13",
          "text": "How did you handle incremental syncs?",
          "score": 2,
          "created_utc": "2026-02-08 14:07:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4efqxc",
              "author": "EnoughNinja",
              "text": "Per-provider cursors + webhook deltas. We only fetch changes since last sync, then re-index affected \n\nNo full inbox reprocessing",
              "score": 1,
              "created_utc": "2026-02-09 08:08:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ap3yn",
          "author": "bugtank",
          "text": "Honestly this is amazing. Did you use instructor for the structured data?",
          "score": 2,
          "created_utc": "2026-02-08 18:47:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4efpz4",
              "author": "EnoughNinja",
              "text": "No. We enforce schemas at the pipeline level, before the model sees anything ",
              "score": 1,
              "created_utc": "2026-02-09 08:08:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvlwp4",
      "title": "Whats a good typescript friendly agent framework to build with right now?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qvlwp4/whats_a_good_typescript_friendly_agent_framework/",
      "author": "wainegreatski",
      "created_utc": "2026-02-04 11:18:16",
      "score": 10,
      "num_comments": 12,
      "upvote_ratio": 0.87,
      "text": "I am looking to integrate AI agents into a project and want a solid agent framework for clean development. How is the experience with documentation, customization and moving to production?",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qvlwp4/whats_a_good_typescript_friendly_agent_framework/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3mekef",
          "author": "Tzipi_builds",
          "text": "Great breakdown in the comments. I'm currently building an 'Agent-First' startup with a fleet of agents managing multiple products, and I’ve landed on a hybrid approach that follows what I call the **TOM ABC framework** (Agents, Brain, Context, Tools, Output, Memory).\n\nFor the **Brain** (orchestration), I agree with u/pbalIII if you have branching logic, LangGraph.js is hard to beat despite the bundle size. The state management is what actually allows agents to behave like 'employees' rather than just fancy scripts.\n\nHowever, for the **Output** (UI/Dashboard in Next.js), I keep it lean with **Vercel AI SDK** to avoid edge deployment issues.\n\nHas anyone here tried **Mastra** for production-grade memory management (the 'M' in TOM ABC)? I’m curious if it handles persistent state better than a custom Postgres implementation.",
          "score": 5,
          "created_utc": "2026-02-04 23:25:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pf51s",
              "author": "pbalIII",
              "text": "Thread-scoped and resource-scoped working memory out of the box, with semantic recall backed by vector search. Postgres as a backend means you're not adopting some exotic store nobody's heard of.\n\nThe tradeoff is their scoping model. If your agents need memory that crosses weird boundaries, like sharing partial state between unrelated workflows, you'll fight it. But for per-user persistent context... it beats building yet another conversation-aware Postgres schema that everyone ships and nobody enjoys maintaining.",
              "score": 2,
              "created_utc": "2026-02-05 12:28:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3piu5d",
                  "author": "Tzipi_builds",
                  "text": "Thanks for the value! Your breakdown of Mastra’s memory scoping vs. custom Postgres schemas was incredibly helpful. Definitely tilting towards Mastra now to save that maintenance headache. Cheers!",
                  "score": 1,
                  "created_utc": "2026-02-05 12:53:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3vpegm",
              "author": "plasticbrad",
              "text": "We evaluated rolling our own postgres layer and the tricky part wasnt storage, it was orchestration around state transitions, retries and debugging. Mastra’s memory + workflow primitives handled that cleanly without us reinventing half an agent runtime. For agent behavior over time it felt closer to what you are describing than a raw DB setup.",
              "score": 2,
              "created_utc": "2026-02-06 10:43:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jvzvj",
          "author": "hntrl",
          "text": "We've put a lot of TLC into the TypeScript versions of deepagents, langchain, and langgraph! We've made a lot of big strides in terms of typing improvements in the past few months, and we're doing a lot of cool things with our deepagents package that not a lot of others in the TS ecosystem are doing.",
          "score": 3,
          "created_utc": "2026-02-04 16:15:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3p477u",
          "author": "Effective-Mind8185",
          "text": "If you want maximum control, LangChain / LangGraph in TS is solid. Way better typings than a year ago. But once you go beyond demos, you end up wiring state, retries, HITL, and ops yourself. That’s where time leaks. Fast.\n\nWhat surprised me: writing agents as actual TypeScript code (not configs, not graphs) scales much better mentally. We built  https://calljmp.com/ around that idea. You define logic in TS, but the runtime handles the annoying parts: long-running state, pause/resume, human approvals, logs, traces, costs. No infra yak-shaving.\n\nSanity check from our side: a basic support/copilot agent is ~200–300 LOC. Production-ready in a day or two, not weeks. And costs stay predictable (we’re seeing low single-digit € per 1k real interactions, not prompts).\n\nNot saying frameworks are bad — they’re great for experimenting. But moving to prod is a different game (ask me how I know",
          "score": 2,
          "created_utc": "2026-02-05 11:02:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q9lsu",
          "author": "Seeking_Adrenaline",
          "text": "Baml prompts and temporal orchestration\n\nGuys, you dont need frameworks. Prompts are just API calls. Orchestrate it yourself for full control.",
          "score": 1,
          "created_utc": "2026-02-05 15:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3sqlgm",
          "author": "Atsoc1993",
          "text": "LangChain & LangGraph\n\nEdit: I swear I’m not an agent, just zombie typing while on the train home lol",
          "score": 1,
          "created_utc": "2026-02-05 22:18:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jf0mw",
          "author": "pbalIII",
          "text": "Vercel AI SDK gets the most recommendations but it's really only the right call for basic chat UIs with streaming. For anything with branching workflows, LangGraph.js is where teams actually land... 529K weekly downloads despite low GitHub stars tells you something.\n\nThe contrarian bit: LangChain.js carries 101KB gzipped and blocks edge deployment. For straightforward use cases, you're unwinding abstractions you don't need. Prototype in Vercel AI SDK, then evaluate if the complexity justifies switching.",
          "score": 0,
          "created_utc": "2026-02-04 14:55:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jwhc8",
              "author": "hntrl",
              "text": "\\> LangChain.js carries 101KB gzipped  \nHave you checked out the v1 version of langchain? We scoped down a lot on the core of the package to just be an agent abstraction that we think is important\n\n\\> and blocks edge deployment  \nto be clear we don't like this either, we're waiting for the next opportunity to fix this in the next major version",
              "score": 4,
              "created_utc": "2026-02-04 16:17:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kgc9g",
          "author": "upvotes2doge",
          "text": "Mastra. It's got a nice interface, built in observability, workflow creation: [https://www.youtube.com/watch?v=1qnmnRICX50](https://www.youtube.com/watch?v=1qnmnRICX50)",
          "score": 0,
          "created_utc": "2026-02-04 17:48:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nsmm4",
              "author": "LazyActiveCleverFool",
              "text": "This works really nicely with vercel ai sdk  and Litellm.",
              "score": 1,
              "created_utc": "2026-02-05 04:13:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1quni12",
      "title": "AI projects with Langchain and Langgraph",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1quni12/ai_projects_with_langchain_and_langgraph/",
      "author": "Affectionate_Bid2797",
      "created_utc": "2026-02-03 09:28:03",
      "score": 10,
      "num_comments": 9,
      "upvote_ratio": 0.82,
      "text": "Hello everyone,\n\nI hope you’re doing well. I’m a software engineer who’s really passionate about machine learning and AI, and I’d love to get some advice from engineers already working in the field.\n\nI’ve studied the fundamentals and understand the theory and common frameworks, but I feel I need to build more concrete, real-world projects to gain confidence and practical experience.\n\nI’ve gone through tutorials and done quite a bit of research, but much of the advice feels repetitive, and many project suggestions are the same everywhere. So I wanted to ask directly: what projects would you recommend building that are actually useful and help someone stand out?\n\nI’m not looking for generic or cliché advice, but rather insights from people with hands-on experience in the industry.\n\nThanks a lot for your time.I really appreciate any suggestions.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1quni12/ai_projects_with_langchain_and_langgraph/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3gspuy",
          "author": "hrishikamath",
          "text": "Honesty just do projects you really care, where you care about the output and it’s challenging but it’s also reasonable. You will come across problems and then use whatever you learn to diagnose, you will learn better. I did that for finance and ended up learning a lot without taking tutorials or watching any courses. It’s open source happy to share the link/blogpost if you want.",
          "score": 2,
          "created_utc": "2026-02-04 03:19:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42kt3j",
              "author": "Affectionate_Bid2797",
              "text": "Hey, thanks for sharing!   \nYes I would love to have a look and talk more about it.",
              "score": 1,
              "created_utc": "2026-02-07 12:43:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o448uhh",
                  "author": "hrishikamath",
                  "text": "Here you go: [https://github.com/kamathhrishi/stratalens-ai](https://github.com/kamathhrishi/stratalens-ai)",
                  "score": 1,
                  "created_utc": "2026-02-07 18:05:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3bdrb1",
          "author": "Witty_System7237",
          "text": "What kind of domain are you most interested in-like data analysis, chat assistants, or something else? That could help narrow down useful project ideas.",
          "score": 1,
          "created_utc": "2026-02-03 09:43:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3be0jw",
              "author": "Affectionate_Bid2797",
              "text": "I am interested in building agentic workflows from end-to-end.   \nThank you!",
              "score": 1,
              "created_utc": "2026-02-03 09:46:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3boith",
                  "author": "Apart_Commercial2279",
                  "text": "the main issue if you want to face real world issue and not getting basic advice is to build a usefull agent for you, your friends family or coworker and make them use them (this is the hardest). If they don't use it, or use it the wrong way or if they don't get what they need you will iterate and make it more complex, fix bug, add retry, guardrails ect... And really have an end to end system",
                  "score": 0,
                  "created_utc": "2026-02-03 11:22:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3c3nsh",
          "author": "Upset-Pop1136",
          "text": "Learn from the open source products and great tools. One of them is Dify",
          "score": 1,
          "created_utc": "2026-02-03 13:11:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ifk6p",
          "author": "MathematicianTop1654",
          "text": "this tutorial might be useful: [https://www.crewship.dev/blog/deploy-langgraph-to-production](https://www.crewship.dev/blog/deploy-langgraph-to-production)",
          "score": 1,
          "created_utc": "2026-02-04 11:16:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41roq2",
          "author": "Kosemani2",
          "text": "You can study this project. Uses Langgraph deepAgent to orchestrate agentic workflow. https://github.com/olasunkanmi-SE/codebuddy/tree/main/src/agents",
          "score": 1,
          "created_utc": "2026-02-07 08:12:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvvnzd",
      "title": "Build a self-updating wiki from codebases (open source, Apache 2.0)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qvvnzd/build_a_selfupdating_wiki_from_codebases_open/",
      "author": "Whole-Assignment6240",
      "created_utc": "2026-02-04 17:57:34",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I recently have been working on [a new project](https://github.com/cocoindex-io/cocoindex/tree/v1/examples/multi_codebase_summarization) to build a self-updating wiki from codebases. I wrote a step-by-step tutorial.\n\nYour code is the source of truth, and documentations out of sync is such a common pain especially in larger teams. Someone refactors a module, and the wiki is already wrong. Nobody updates it until a new engineer asks a question about it.\n\nThis open source project scans your codebases, extracts structured information with LLMs, and generates Markdown documentation with Mermaid diagrams — using CocoIndex + Instructor + Pydantic.\n\nWhat's cool about this example:\n\n• 𝐈𝐧𝐜𝐫𝐞𝐦𝐞𝐧𝐭𝐚𝐥 𝐩𝐫𝐨𝐜𝐞𝐬𝐬𝐢𝐧𝐠 — Only changed files get reprocessed. saving 90%+ of LLM cost and compute.\n\n• 𝐒𝐭𝐫𝐮𝐜𝐭𝐮𝐫𝐞𝐝 𝐞𝐱𝐭𝐫𝐚𝐜𝐭𝐢𝐨𝐧 𝐰𝐢𝐭𝐡 𝐋𝐋𝐌𝐬 — LLM returns real typed objects — classes, functions, signatures, relationships.\n\n• 𝐀𝐬𝐲𝐧𝐜 𝐟𝐢𝐥𝐞 𝐩𝐫𝐨𝐜𝐞𝐬𝐬𝐢𝐧𝐠 — All files in a project get extracted concurrently with asyncio.gather().\n\n• 𝐌𝐞𝐫𝐦𝐚𝐢𝐝 𝐝𝐢𝐚𝐠𝐫𝐚𝐦𝐬 — Auto-generated pipeline visualizations showing how your functions connect across the project.\n\nThis pattern hooks naturally into PR flows — run it on every merge and your docs stay current without anyone thinking about it. I think it would be cool next to build a coding agent with Langchain on top of this fresh knowledge. \n\nIf you want to explore the full example (fully open source, with code, APACHE 2.0), it's here:\n\n👉 [https://cocoindex.io/examples-v1/multi-codebase-summarization](https://cocoindex.io/examples-v1/multi-codebase-summarization)\n\nIf you find CocoIndex useful, a star on Github means a lot :)\n\n⭐ [https://github.com/cocoindex-io/cocoindex](https://github.com/cocoindex-io/cocoindex)\n\ni'd love to learn from your feedback, thanks!",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qvvnzd/build_a_selfupdating_wiki_from_codebases_open/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r0dvgp",
      "title": "How do you handle agent-to-agent discovery as you scale past 20+ agents?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1r0dvgp/how_do_you_handle_agenttoagent_discovery_as_you/",
      "author": "Sea-Perception1619",
      "created_utc": "2026-02-09 19:28:42",
      "score": 9,
      "num_comments": 4,
      "upvote_ratio": 0.92,
      "text": "We're running about 30 specialized agents (mix of LangGraph and custom) and the coordination is getting painful. Right now everything goes through a central orchestrator that maintains a registry of who can do what. It works but it's fragile — orchestrator went down last week and everything stopped.\n\nCurious how other teams are handling this:\n\n* How do your agents find each other's capabilities?\n* What breaks first as you add more agents?\n* Anyone running agents across multiple teams/orgs? How do you handle discovery across boundaries?\n* Is anyone using MCP or A2A for this, and how's that going?\n\nNot looking for a specific tool recommendation — more interested in architectural patterns that work at scale.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1r0dvgp/how_do_you_handle_agenttoagent_discovery_as_you/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o4iurm5",
          "author": "ArmOk3290",
          "text": "This feels a lot like service discovery plus workflow orchestration, with LLMs making the edges fuzzier.\n\nWhat has worked for me is separating three things:\n- Capability registry as data, not a single process. Put it in a durable store and replicate it. Treat updates as events.\n- Routing as stateless. The orchestrator can die and come back because it only reads registry plus current task state.\n- Execution as durable jobs. If an agent dies mid task, you can retry or reassign based on idempotent steps.\n\nFor agent to agent calls, I would keep a small, versioned contract for each tool or capability and require each agent to self report health and supported versions. At 30 plus agents, the first thing that breaks is observability, so I would invest early in traces and per agent quotas so one bad loop cannot take the whole system down.",
          "score": 4,
          "created_utc": "2026-02-09 23:40:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4iwmzg",
              "author": "Sea-Perception1619",
              "text": "This is a clean separation. The registry-as-data pattern with event-driven updates is something I keep seeing in mature setups — it avoids the SPOF of a registry process while keeping things consistent.\n\nCurious about the routing layer — when multiple agents can handle the same capability and you need to pick one, what's the selection logic? Round-robin, random, or something quality-aware? And does the routing improve over time based on past outcomes, or is it static once the registry is populated?\n\nThe observability point resonates. Per-agent traces and quotas before anything else at scale.",
              "score": 1,
              "created_utc": "2026-02-09 23:50:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4klqca",
                  "author": "TheExodu5",
                  "text": "I'm speaking out of my depth here as I don't yet have any experience at this scale, but what about using a reranking model to choose the best agent if multiple fit the criteria? You'd need to store agent metadata as embeddings, but that would be cheap.",
                  "score": 1,
                  "created_utc": "2026-02-10 06:19:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r05q9n",
      "title": "Building a chat-with-data agent in LangGraph without LLM SQL generation",
      "subreddit": "LangChain",
      "url": "https://i.redd.it/ofdbrxr85hig1.png",
      "author": "deputystaggz",
      "created_utc": "2026-02-09 14:32:31",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1r05q9n/building_a_chatwithdata_agent_in_langgraph/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qzxia4",
      "title": "Added Ollama support to MCPlexor – now you can run it 100% locally (and free)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qzxia4/added_ollama_support_to_mcplexor_now_you_can_run/",
      "author": "Basic_Tea9680",
      "created_utc": "2026-02-09 07:04:55",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nLast week, I posted here about [how preloading MCP tools was costing me \\~50k tokens per run](https://www.reddit.com/r/LangChain/comments/1qukgay/preloading_mcp_tools_cost_me_50k_tokens_per_run/). The TL;DR was that heavy MCP servers like Linear, GitHub, Figma etc. were eating 25% of my context window before I even asked a question.\n\nI built MCPlexor to solve this – it dynamically routes to the right MCP server instead of dumping 100+ tool definitions into your agent's context.\n\n**What's new: Full Ollama Support**\n\nI kept getting asked: \"Can I run this locally without calling your API?\"\n\nShort answer: yes, now you can.\n\nIf you have Ollama running, MCPlexor can use it for the routing logic instead of our cloud. Zero cost, works offline, your data stays on localhost.\n\non localhost.\n\n    # Install\n    curl -fsSL https://mcplexor.com/install.sh | bash\n\nIn MCPlexor cli you can use your local Ollama instance (llama3, mistral, qwen, whatever you've got) to figure out which MCP server to route to.\n\n**How MCPlexor will eventually make money**\n\nFigured I'd be transparent since I'm indie-hacking this:\n\nFor local/low-volume users → Ollama is free. Use it if you have many mcps on for you agent. Seriously.\n\nFor high-volume / cloud users → We run the routing on cheaper, efficient models (not Opus or Gemini Pro). We take a small cut from the savings we're passing on. Think of it as: you were gonna spend $X on context tokens anyway, we help you spend $X/10, and we take a slice of the difference.\n\nHaven't launched the paid tier yet (still in waitlist mode), but that's the game plan.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qzxia4/added_ollama_support_to_mcplexor_now_you_can_run/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o4ehxsc",
          "author": "EcstaticAd9869",
          "text": "Oh that's interesting",
          "score": 1,
          "created_utc": "2026-02-09 08:30:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4emcyt",
              "author": "Basic_Tea9680",
              "text": "![gif](giphy|LXsmHIeoWM16d3w2Ju)",
              "score": 2,
              "created_utc": "2026-02-09 09:14:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4epkri",
                  "author": "EcstaticAd9869",
                  "text": "![gif](giphy|CbSGut2wzWKZy)",
                  "score": 1,
                  "created_utc": "2026-02-09 09:46:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qxa5ip",
      "title": "Built a Website Crawler + RAG (fixed it last night 😅)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qxa5ip/built_a_website_crawler_rag_fixed_it_last_night/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-06 06:27:22",
      "score": 7,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I’m **new to RAG** and learning by building projects.  \nAlmost **2 months ago** I made a very simple RAG, but the **crawler & ingestion were hallucinating**, so the answers were bad.\n\nYesterday night (after office stuff 💻), I thought:  \nEveryone is feeding PDFs… **why not try something that’s not PDF ingestion?**\n\nSo I focused on fixing the **real problem — crawling quality**.\n\n🔗 GitHub: [https://github.com/AnkitNayak-eth/CrawlAI-RAG](https://github.com/AnkitNayak-eth/CrawlAI-RAG)\n\n**What’s better now:**\n\n* Playwright-based crawler (handles JS websites)\n* Clean content extraction (no navbar/footer noise)\n* Smarter chunking + deduplication\n* RAG over **entire websites**, not just PDFs\n\nBad crawling = bad RAG.\n\nIf you all want, **I can make this live / online** as well 👀  \nFeedback, suggestions, and ⭐s are welcome!",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qxa5ip/built_a_website_crawler_rag_fixed_it_last_night/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3vh34k",
          "author": "Ok_Signature_6030",
          "text": "the \"bad crawling = bad RAG\" insight is spot on and something a lot of people skip over. most tutorials jump straight to chunking strategy or retrieval tuning but if your source data is garbage none of that matters.\n\none thing i noticed looking at the repo... the README mentions BeautifulSoup for scraping but your post says Playwright-based. did you switch between versions? because that distinction actually matters a lot for production use. BS4 is fine for static content but if you're targeting JS-heavy sites (SPAs, dynamic dashboards), Playwright is worth the overhead.\n\nthe ChromaDB + Sentence-Transformers + Groq stack is solid for a learning project. if you do make it live, watch out for near-duplicate pages (like paginated content or URL params) polluting your index... a simple content hash before embedding can save you a lot of headaches there.\n\ncool project for 2 months in.",
          "score": 2,
          "created_utc": "2026-02-06 09:26:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vl9ev",
              "author": "Cod3Conjurer",
              "text": "Good catch - my bad\n\n\nInitially it was BeautifulSoup-only for static pages. When I revisited it last night, I switched to Playwright + BS4 because JS-heavy sites were killing extraction quality. That distinction definitely matters.\nAlso agreed on near-duplicates - I'm doing content normalization + hashing before embedding for now, and improving pagination/ URL param handling next.\n\n\nAnd yeah - not really 2 months of active work More like built a rough version, got lazy, realized the crawling was garbage, fixed it properly last night. \n\n\nAppreciate the detailed feedback 🙏",
              "score": 1,
              "created_utc": "2026-02-06 10:06:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3vooed",
                  "author": "Ok_Signature_6030",
                  "text": "nice, the playwright + bs4 combo is solid for that... content hashing before embedding is smart too, avoids wasting vector space on near-identical chunks. good luck with the pagination stuff, that's usually where the edge cases get annoying",
                  "score": 1,
                  "created_utc": "2026-02-06 10:37:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qzsux5",
      "title": "What are the typical steps to turn an idea into a production service using LangChain?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qzsux5/what_are_the_typical_steps_to_turn_an_idea_into_a/",
      "author": "arbiter_rise",
      "created_utc": "2026-02-09 03:06:11",
      "score": 7,
      "num_comments": 8,
      "upvote_ratio": 0.9,
      "text": "*(English may sound a bit awkward — not a native speaker, sorry in advance!)*  \n  \nIf I want to serve my own idea using LangChain, what are the typical steps people go through to get from a prototype to a production-ready service?\n\nMost tutorials and examples cover things like:  \nprompt design → chain composition → a simple RAG setup.  \nThat part makes sense to me.\n\nBut when it comes to **building something real that users actually use**,  \nI’m not very clear on what comes *after* that.\n\nIn particular, I’m curious about:\n\n* Whether people usually keep the LangChain architecture as-is when traffic grows\n* How monitoring, logging, and error handling are typically handled in production\n* Whether LangChain remains a core part of the system in the long run, or if it tends to get stripped out over time\n\nFor those who have taken a project from  \n**idea → real production service** using LangChain,  \nI’d really appreciate hearing about the common stages you went through, or any practical advice like  \n“this is worth doing early” vs. “this can wait until later.”\n\nThanks in advance for sharing your real-world experience",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qzsux5/what_are_the_typical_steps_to_turn_an_idea_into_a/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o4eobll",
          "author": "Sam_YARINK",
          "text": "The gap between tutorial examples and production systems is real. Here’s what typically happens when taking LangChain from prototype to production:\nThe Core Architecture Question\nMost teams actually do keep LangChain in production, but the architecture often evolves significantly. It usually stays as the orchestration layer, but gets wrapped in more infrastructure. Some teams eventually migrate to lighter alternatives if they find they’re only using basic chain logic, but this tends to happen gradually rather than as a planned replacement.\nTypical Production Steps\nInfrastructure additions:\n- Moving from simple chains to LangGraph for more complex, stateful workflows with better control flow\n- Adding proper API layers (FastAPI/Flask) around your chains\n- Implementing request queuing and rate limiting\n- Setting up proper database connections for conversation history and state management\nObservability stack:\n- LangSmith for tracing and debugging (LangChain’s native tool)\n- Structured logging with correlation IDs across chain steps\n- Custom metrics for latency, token usage, and success rates per chain component\n- Error tracking (Sentry or similar) with LangChain-specific context\nProduction hardening:\n- Implementing retries with exponential backoff for LLM calls\n- Adding circuit breakers for external services\n- Prompt versioning and A/B testing infrastructure\n- Input validation and output sanitization\n- Cost tracking per user/request\nCommon “Do This Early” Advice\n- Set up tracing from day one - You’ll need it to debug chain behavior, and retrofitting is painful\n- Design for prompt iteration - Store prompts in config/database, not hardcoded\n- Plan your state management - Conversation memory gets complex quickly with multiple users\n- Implement proper error boundaries - LangChain errors can be cryptic; wrap components with clear error handling\nCommon “Can Wait” Items\n- Highly optimized caching strategies\n- Custom chain implementations (start with LangChain’s built-ins)\n- Complex multi-agent systems (unless core to your use case)\nThe biggest shift is often moving from LangChain Expression Language (LCEL) chains to LangGraph when you need more complex control flow, error recovery, or human-in-the-loop patterns.",
          "score": 2,
          "created_utc": "2026-02-09 09:33:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ju6ig",
              "author": "arbiter_rise",
              "text": "Thank you for your answer. What was the most difficult part of your development process?",
              "score": 1,
              "created_utc": "2026-02-10 03:04:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4dg38b",
          "author": "Rude_Extension_3788",
          "text": "Im working on applications that use langgraph rn. while none have hit production yet, ive never thought of a reason as to why id ever have to strip it out so im just curious as to why you think that would happen? Am i missing out on something myself? I'm fairly new to lang, only 3-4 months in but it seems pretty scalable out of the box.",
          "score": 1,
          "created_utc": "2026-02-09 03:30:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dgsrv",
              "author": "arbiter_rise",
              "text": "I should clarify that this isn’t based on my own direct experience of stripping out LangGraph in production. It’s more something I’ve heard repeatedly from others who started with a framework and later chose to move away from it for various reasons (complexity, constraints, team familiarity, etc.), so I wanted to bring that perspective into the discussion.\n\nThanks for sharing your experience.",
              "score": 1,
              "created_utc": "2026-02-09 03:34:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4dhiaf",
                  "author": "Rude_Extension_3788",
                  "text": "Ahh well its a good thing I came across this post then, would love to hear about the experiences people have under heavy prod applications to see the shortcomings of the library.",
                  "score": 1,
                  "created_utc": "2026-02-09 03:38:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4feysl",
          "author": "llamacoded",
          "text": "Add observability early. Distributed tracing, evaluation on production traffic, cost tracking. We use [Maxim](https://getmax.im/Max1m) for this. Don't wait until production to add monitoring.",
          "score": 1,
          "created_utc": "2026-02-09 13:15:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qx41gl",
      "title": "Open source trust verification for multi-agent systems",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qx41gl/open_source_trust_verification_for_multiagent/",
      "author": "HolidayCharge1511",
      "created_utc": "2026-02-06 01:28:31",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 0.88,
      "text": "Hey everyone,  \n  \nI've been working on a problem that's been bugging me: as AI agents start talking to each other (Google's A2A protocol, LangChain multi-agent systems, etc.), there's no way to verify if an external agent is trustworthy.  \n  \nSo I built \\*\\*TrustAgents\\*\\* — essentially a firewall for the agentic era.  \n  \n**What it does:**  \n\\- Scans agent interactions for prompt injection, jailbreaks, data exfiltration (65+ threat patterns)  \n\\- Tracks reputation scores per agent over time  \n\\- Lets agents prove legitimacy via email/domain verification  \n\\- Sub-millisecond scan times  \n  \n**Stack:**  \n\\- FastAPI + PostgreSQL (Railway)  \n\\- Next.js landing page (Vercel)  \n\\- Clerk auth + Stripe billing  \n\\- Python SDK on PyPI, TypeScript SDK on npm, LangChain integration  \n  \n  \nWould love feedback from anyone building with AI agents. What security concerns do you run into?  \n  \n[https://trustagents.dev](https://trustagents.dev)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qx41gl/open_source_trust_verification_for_multiagent/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3u0cvs",
          "author": "AdditionalWeb107",
          "text": "you should look at [https://github.com/katanemo/plano](https://github.com/katanemo/plano) \\- similar ideas but designed to be framework-agnostic. Its a substrate to manage and handle all traffic coming in/out of agents in a protocol-native way",
          "score": 3,
          "created_utc": "2026-02-06 02:38:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qurqb7",
      "title": "Code vs. Low-Code for AI Agents: Am I over-engineering my \"Social Listening\" swarm?",
      "subreddit": "LangChain",
      "url": "https://i.redd.it/mqrwi0uq5ahg1.png",
      "author": "Tzipi_builds",
      "created_utc": "2026-02-03 13:20:06",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qurqb7/code_vs_lowcode_for_ai_agents_am_i/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3hqdso",
          "author": "emmy_talks_reddit",
          "text": "What's the issue exactly? Are you worried your solution isn't good enough compared to low-code solutions? \n\nAre you worried about writing code and architecting a solution (even though you're a software architect?) \n\nSorry but the post just doesn't add up. Whats the problem exactly?",
          "score": 1,
          "created_utc": "2026-02-04 07:24:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i3zaj",
              "author": "Tzipi_builds",
              "text": "Fair question! It’s definitely not a doubt about my ability to architect or code the solution.\n\nThe 'issue' is more about Opportunity Cost and Maintenance Debt. As an architect, I’m constantly weighing whether I should spend my time building 'plumbing' (orchestration, memory management, handoffs) or focusing on the core business logic of my agents.\n\nI’m seeing a massive explosion in stable, low-code/specialized agent frameworks and methodologies like ABC TOM (Agents, Brain, Context, Tools, Output, Memory) that promise to handle the infra out-of-the-box.\n\nThe core of my dilemma is:\n\n* If I build it custom in LangGraph/FastAPI now, will I be stuck maintaining a custom-coded infrastructure in 6 months that could have been a 5-minute configuration in a more stable, specialized tool?\n* Is the 'Control' I get from pure code actually a competitive advantage for a Social Listening tool like SidKick, or is it just 'gold-plating' the engine?\n\nI haven't gone 'all-in' on the implementation yet, so I’m trying to gauge if others feel that custom-coded orchestration is becoming a liability rather than an asset as the ecosystem matures.\n\nWould love to hear how you decide where to draw the line between 'Building the Engine' and 'Using the Car'.",
              "score": 1,
              "created_utc": "2026-02-04 09:31:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3iae42",
                  "author": "Wide_Brief3025",
                  "text": "Focusing on business logic over infrastructure is almost always the better use of your time, especially as these specialized frameworks get more robust. Maintenance debt for custom orchestration can spiral fast. For social listening specifically, a tool like ParseStream can handle the real time monitoring and lead discovery side, so you can spend your energy on differentiated features instead of background plumbing.",
                  "score": 2,
                  "created_utc": "2026-02-04 10:31:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qxt3s3",
      "title": "I built a CLI to audit custom LangChain @tool definitions for security flaws.",
      "subreddit": "LangChain",
      "url": "https://github.com/HeadyZhang/agent-audit",
      "author": "absolutelyheady",
      "created_utc": "2026-02-06 20:35:08",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qxt3s3/i_built_a_cli_to_audit_custom_langchain_tool/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    }
  ]
}