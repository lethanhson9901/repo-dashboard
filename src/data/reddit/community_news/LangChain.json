{
  "metadata": {
    "last_updated": "2026-01-17 08:40:58",
    "time_filter": "week",
    "subreddit": "LangChain",
    "total_items": 50,
    "total_comments": 220,
    "file_size_bytes": 306269
  },
  "items": [
    {
      "id": "1pzno6m",
      "title": "Semantic caching cut our LLM costs by almost 50% and I feel stupid for not doing it sooner",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1pzno6m/semantic_caching_cut_our_llm_costs_by_almost_50/",
      "author": "Otherwise_Flan7339",
      "created_utc": "2025-12-30 17:12:23",
      "score": 131,
      "num_comments": 27,
      "upvote_ratio": 0.86,
      "text": "So we've been running this AI app in production for about 6 months now. Nothing crazy, maybe a few hundred daily users, but our OpenAI bill hit $4K last month and I was losing my mind. Boss asked me to figure out why we're burning through so much money.\n\nTurns out we were caching responses, but only with exact string matching. Which sounds smart until you realize users never type the exact same thing twice. \"What's the weather in SF?\" gets cached. \"What's the weather in San Francisco?\" hits the API again. Cache hit rate was like 12%. Basically useless.\n\nThen I learned about semantic caching and honestly it's one of those things that feels obvious in hindsight but I had no idea it existed. We ended up using Bifrost (it's an open source LLM gateway) because it has semantic caching built in and I didn't want to build this myself.\n\nThe way it works is pretty simple. Instead of matching exact strings, it matches the meaning of queries using embeddings. You generate an embedding for every query, store it with the response in a vector database, and when a new query comes in you check if something semantically similar already exists. If the similarity score is high enough, return the cached response instead of hitting the API.\n\nReal example from our logs - these four queries all had similarity scores above 0.90:\n\n* \"How do I reset my password?\"\n* \"Can't remember my password, help\"\n* \"Forgot password what do I do\"\n* \"Password reset instructions\"\n\nWith traditional caching that's 4 API calls. With semantic caching it's 1 API call and 3 instant cache hits.\n\nBifrost uses Weaviate for the vector store by default but you can configure it to use Qdrant or other options. The embedding cost is negligible - like $8/month for us even with decent traffic. GitHub: [https://github.com/maximhq/bifrost](https://github.com/maximhq/bifrost)\n\nAfter running this for 30 days our bill dropped drastically. Cache hit rate went up. And as a bonus, cached responses are way faster - like 180ms vs 2+ seconds for actual API calls.\n\nThe tricky part was picking the similarity threshold. We tried 0.70 at first and got some weird responses where the cache would return something that wasn't quite right. Bumped it to 0.95 and the cache barely hit anything. Settled on 0.85 and it's been working great.\n\nAlso had to think about cache invalidation - we expire responses after 24 hours for time-sensitive stuff and 7 days for general queries.\n\nThe best part is we didn't have to change any of our application code. Just pointed our OpenAI client at Bifrost's gateway instead of OpenAI directly and semantic caching just works. It also handles failover to Claude if OpenAI goes down, which has saved us twice already.\n\nIf you're running LLM stuff in production and not doing semantic caching you're probably leaving money on the table. We're saving almost $2K/month now.",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1pzno6m/semantic_caching_cut_our_llm_costs_by_almost_50/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nwt10pe",
          "author": "hyma",
          "text": "Advertisement?",
          "score": 22,
          "created_utc": "2025-12-30 21:48:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuefsp",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 11,
              "created_utc": "2025-12-31 02:16:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww5ka6",
                  "author": "nihal_ar",
                  "text": "7 months lol, read before commenting duh..",
                  "score": 2,
                  "created_utc": "2025-12-31 10:20:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwvvn72",
                  "author": "Masotsheni",
                  "text": "Not sure how you got that impression, but this sounds like a legit experience. Semantic caching is a real game changer for reducing costs. Maybe check out the GitHub link if you're curious about the tech!",
                  "score": 1,
                  "created_utc": "2025-12-31 08:46:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwuj4r4",
          "author": "Whyme-__-",
          "text": "Just pipe the entire codebase of Roo code into Gemini and ask it to pull the algorithm of semantic caching and distill into simple technical spec sheet.  Then add it to your code. Concepts like these are easier to implement if you already have someone who opensourced the tech.",
          "score": 10,
          "created_utc": "2025-12-31 02:43:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1c1rj",
              "author": "UnionCounty22",
              "text": "I bet it would give you 25% of it and shoo you out the door",
              "score": 1,
              "created_utc": "2026-01-01 05:09:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx31c2r",
                  "author": "Whyme-__-",
                  "text": "Nope if you work it right it gives the whole thing. Plus you can read the readme which has all the features so ask it to double and triple check it",
                  "score": 1,
                  "created_utc": "2026-01-01 14:37:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nws6hgz",
          "author": "Conscious_Nobody9571",
          "text": "Repost",
          "score": 5,
          "created_utc": "2025-12-30 19:22:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrnnl6",
          "author": "Far_Buyer_7281",
          "text": "Seems like something I would warn my users about at least?  \nisn't a query more then its semantic meaning?",
          "score": 3,
          "created_utc": "2025-12-30 17:55:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nws74sx",
              "author": "adiznats",
              "text": "I'm also wondering what is the cosine similarity between \"what's the weather in SF\" and \"whats's the weather in NY\". Also probably longer sentences like those above but with a very small detail changed will be even worse with this kind of caching.",
              "score": 8,
              "created_utc": "2025-12-30 19:26:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwzbml6",
          "author": "tomomcat",
          "text": "Lame advert. This is just pollution.",
          "score": 3,
          "created_utc": "2025-12-31 21:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx215r5",
          "author": "getarbiter",
          "text": "The threshold tuning problem you're describing is fundamental to similarity-based caching. You're essentially guessing where \"same meaning\" ends and \"different meaning\" begins.\n\nWe took a different approach‚Äîcoherence scoring instead of similarity scoring. Rather than asking \"how close are these vectors?\", we ask \"does this cached response actually resolve the query under its constraint field?\"\n\n\"What's the weather in SF\" and \"What's the weather in NY\" have high cosine similarity (~0.95+) but zero coherence as cache matches‚Äîdifferent constraint fields.\n\n\"How do I reset my password\" and \"Forgot my password, help\" have moderate similarity but high coherence‚Äîsame constraint resolution.\n\nThe result: no arbitrary thresholds, deterministic scoring, and the cache knows why something matches, not just how close the vectors are.\n\n26MB engine, runs locally, no API calls for the coherence check itself. Happy to share more if useful.",
          "score": 2,
          "created_utc": "2026-01-01 09:11:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx29si4",
              "author": "louis8799",
              "text": "I'm interested.",
              "score": 2,
              "created_utc": "2026-01-01 10:43:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx495se",
                  "author": "getarbiter",
                  "text": "One concrete example based on what you described:\nIn your password-reset case, similarity caching works until two queries are close in embedding space but differ in constraint (account type, region, auth method, etc.). That‚Äôs where threshold tuning starts leaking correctness.\n\nWe handle that by checking whether the cached response resolves the same constraint set, not just a similar query.\nSo:\n‚ÄúHow do I reset my password?‚Äù\n‚ÄúForgot my password, help‚Äù\n‚Üí same cache hit\n\nBut:\n‚ÄúHow do I reset my password for SSO?‚Äù\n‚ÄúReset password for API key‚Äù\n‚Üí no hit, even if embeddings are close.\n\nSeparately, on cost: we don‚Äôt store 768‚Äì3kD embeddings for cache keys. We compress meaning to a 72-D deterministic representation, which cuts vector storage and ANN overhead by ~10√ó. That‚Äôs where a lot of the hidden infra cost actually is.\n\nThis sits in front of an existing semantic cache ‚Äî not a rip-and-replace.",
                  "score": 1,
                  "created_utc": "2026-01-01 18:34:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwv15or",
          "author": "Practical-Rope-7461",
          "text": "Build that gateway requires 30 minutes vibe coding, with some very basic embedding. Do it yourself. \n\nBtw, this is not a good business idea for offering semantic caching.",
          "score": 1,
          "created_utc": "2025-12-31 04:35:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvd8p1",
          "author": "AftyOfTheUK",
          "text": "How did you measure/quantify the impact on the quality of the responses from your app?",
          "score": 1,
          "created_utc": "2025-12-31 06:02:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwve49n",
          "author": "Dramatic_Strain7370",
          "text": "Great point. I will try out bifrost. Was your application a customer service or IT service agent? where caching was paying dividends?",
          "score": 1,
          "created_utc": "2025-12-31 06:09:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxt3zu",
          "author": "Either_War7733",
          "text": "I keep seeing people saying this is a spam but how can you actually implement it without using the tools being promoted here?",
          "score": 1,
          "created_utc": "2025-12-31 16:44:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1xju1",
          "author": "nf_x",
          "text": "Isn‚Äôt getting embeddings another API call? üòâ",
          "score": 1,
          "created_utc": "2026-01-01 08:32:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4jxdj",
          "author": "elrosegod",
          "text": "Great story man, I'll need to keep this in mind when we start have unstructured querying in our apps.",
          "score": 1,
          "created_utc": "2026-01-01 19:27:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxc431q",
          "author": "baadir_",
          "text": "actually i try to jina ai rerank model . ƒ± pull 10 chunks but second layer jina rerank more relavan 5 chunks. \n\nƒ± think its good idea for relavancy",
          "score": 1,
          "created_utc": "2026-01-02 22:40:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxfyla6",
          "author": "Deep_Spice",
          "text": "This worked well for us too, but we eventually ran into cases where similarity wasn‚Äôt the failure mode,  reuse itself was invalid. Things like tenant boundaries, freshness requirements, or state-dependent tools made some ‚Äúhigh similarity‚Äù hits incorrect. We ended up treating semantic cache hits as conditional, similarity is necessary but not sufficient.",
          "score": 1,
          "created_utc": "2026-01-03 14:28:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxltf9t",
          "author": "adlx",
          "text": "It might be interesting in some cases. But in my case, the exact same question might have a different answer depending on the user, even depending on the day... \n\nFor example, \"when does my password expire\", Or \"when does my team take holidays this month\", \"whats is the state of my incidents\" (real questions our chatbot can answer).\n\nSame questions, same tools, obviously different response per user...",
          "score": 1,
          "created_utc": "2026-01-04 10:46:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt6oc5",
          "author": "qa_anaaq",
          "text": "How does the gateway work? You hit your API which hits your gateway which hits the provider‚Äôs API? Or the gateway becomes your API effectively and it‚Äôs just gateway -> provider?",
          "score": 1,
          "created_utc": "2025-12-30 22:15:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q9squ3",
      "title": "Announcing Kreuzberg v4",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q9squ3/announcing_kreuzberg_v4/",
      "author": "Goldziher",
      "created_utc": "2026-01-11 07:03:28",
      "score": 49,
      "num_comments": 10,
      "upvote_ratio": 0.93,
      "text": "Hi Peeps,\n\nI'm excited to announce [Kreuzberg](https://github.com/kreuzberg-dev/kreuzberg) v4.0.0. \n\n## What is Kreuzberg:\n\nKreuzberg is a document intelligence library that extracts structured data from 56+ formats, including PDFs, Office docs, HTML, emails, images and many more. Built for RAG/LLM pipelines with OCR, semantic chunking, embeddings, and metadata extraction. \n\nThe new v4 is a ground-up rewrite in Rust with a bindings for 9 other languages! \n\n## What changed:\n\n- **Rust core**: Significantly faster extraction and lower memory usage. No more Python GIL bottlenecks.\n- **Pandoc is gone**: Native Rust parsers for all formats. One less system dependency to manage.\n- **10 language bindings**: Python, TypeScript/Node.js, Java, Go, C#, Ruby, PHP, Elixir, Rust, and WASM for browsers. Same API, same behavior, pick your stack.\n- **Plugin system**: Register custom document extractors, swap OCR backends (Tesseract, EasyOCR, PaddleOCR), add post-processors for cleaning/normalization, and hook in validators for content verification.\n- **Production-ready**: REST API, MCP server, Docker images, async-first throughout.\n- **ML pipeline features**: ONNX embeddings on CPU (requires ONNX Runtime 1.22.x), streaming parsers for large docs, batch processing, byte-accurate offsets for chunking.\n\n## Why polyglot matters:\n\nDocument processing shouldn't force your language choice. Your Python ML pipeline, Go microservice, and TypeScript frontend can all use the same extraction engine with identical results. The Rust core is the single source of truth; bindings are thin wrappers that expose idiomatic APIs for each language.\n\n## Why the Rust rewrite:\n\nThe Python implementation hit a ceiling, and it also prevented us from offering the library in other languages. Rust gives us predictable performance, lower memory, and a clean path to multi-language support through FFI.\n\n## Is Kreuzberg Open-Source?:\n\nYes! Kreuzberg is MIT-licensed and will stay that way. \n\n## Links\n\n- [Star us on GitHub](https://github.com/kreuzberg-dev/kreuzberg)\n- [Read the Docs](https://kreuzberg.dev/)\n- [Join our Discord Server](https://discord.gg/38pF6qGpYD)\n",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LangChain/comments/1q9squ3/announcing_kreuzberg_v4/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nyye8ye",
          "author": "pyhannes",
          "text": "Hey, Great job! Greetings from Bavaria ;) Just wanted to tell you that the bright and dark color schemes of the docs are really bad to read, at least on mobile :(\n\nhttps://preview.redd.it/yg9b7ilyepcg1.jpeg?width=1440&format=pjpg&auto=webp&s=7ee57c8771eabfbd0eb469edae5648964a4aa68c",
          "score": 3,
          "created_utc": "2026-01-11 11:17:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyye9t0",
              "author": "pyhannes",
              "text": "https://preview.redd.it/63nx0rzzepcg1.jpeg?width=1440&format=pjpg&auto=webp&s=e9ed0dff7d7a2463d9f33bd76a1166f942af3ead",
              "score": 3,
              "created_utc": "2026-01-11 11:17:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyyf0yp",
                  "author": "Goldziher",
                  "text": "Thank you, being handled now",
                  "score": 3,
                  "created_utc": "2026-01-11 11:24:11",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyynotl",
          "author": "red_src",
          "text": "Any benchmark results?",
          "score": 2,
          "created_utc": "2026-01-11 12:36:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyystux",
          "author": "qa_anaaq",
          "text": "Any special processing done for spreadsheets and tables to provide optimal accuracy?",
          "score": 1,
          "created_utc": "2026-01-11 13:14:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyz2juf",
              "author": "Goldziher",
              "text": "We have a dedicated extractor for spreadsheets. It's precise and fast.",
              "score": 2,
              "created_utc": "2026-01-11 14:14:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyz1xbp",
          "author": "abeecrombie",
          "text": "Awesome. Interested to test on PDFs with lots of tables and graphs.",
          "score": 1,
          "created_utc": "2026-01-11 14:10:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1nzon",
          "author": "Business-Weekend-537",
          "text": "Are there examples/workbooks available of using this in RAG pipelines? \n\nAlso is it possible to switch in an vLLM for OCR like Olm OCR or Qwen rather than using paddle OCR/Tesseract etc?",
          "score": 1,
          "created_utc": "2026-01-11 21:34:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2ierg",
          "author": "pbalIII",
          "text": "Ditching Pandoc and the system dependencies is a huge win. Managing a fragile `apt-get` stack just to parse a DOCX is always a pain.\n\nSwitching to Rust handles the concurrency well too. If you're building a live graph, you can't have the GIL locked up by one massive PDF.\n\nCurious about the byte-accurate offsets... do those survive the cleaning steps? You need those coordinates to be exact for reliable citation highlights.",
          "score": 1,
          "created_utc": "2026-01-12 00:05:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nziogni",
              "author": "Eastern-Surround7763",
              "text": "They‚Äôre byte-accurate for the extracted text at chunking time. If you then mutate the text (cleaning/normalization), the offsets won‚Äôt match anymore unless you keep a mapping, so the safe pattern is to keep offsets tied to the raw text and treat cleaned versions as derived",
              "score": 2,
              "created_utc": "2026-01-14 10:52:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxndjy",
      "title": "Advanced RAG: Token Optimization and Cost Reduction in Production. We Cut Query Costs by 60%",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1pxndjy/advanced_rag_token_optimization_and_cost/",
      "author": "Electrical-Signal858",
      "created_utc": "2025-12-28 09:33:31",
      "score": 47,
      "num_comments": 13,
      "upvote_ratio": 0.9,
      "text": "Following up on my previous RAG post: we've optimized production RAG systems further and discovered cost optimizations that nobody talks about. This is specifically about reducing token spend without sacrificing quality.\n\n# The Problem We Solved\n\nOur RAG system was working well (retrieval was solid, generation was accurate), but the token spend kept climbing:\n\n* Hybrid retrieval (BM25 + vector): \\~2,000 tokens/query\n* Retrieved documents: \\~3,000 tokens\n* LLM processing: \\~500 tokens\n* **Total: \\~5,500 tokens/query**¬†√ó 100k queries/day = expensive\n\nAt $0.03 per 1K input tokens, that's¬†**$16.50/day just for input tokens**.¬†**$495/month**.\n\nWe asked: \"Can we get similar quality with fewer tokens?\"\n\nSpoiler: Yes. We reduced it to¬†**2,200 tokens/query average**¬†(60% reduction) while maintaining 92% accuracy (same as before).\n\n# The Optimizations\n\n# 1. Smart Document Chunking Reduces Retrieved Token Count\n\n**Before:**¬†Fixed 1,000-token chunks\n\n* Simple but wasteful\n* Lots of redundant context\n* Padding with irrelevant info\n\n**After:**¬†Semantic chunks with metadata filtering\n\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    from sentence_transformers import SentenceTransformer\n    import numpy as np\n    \n    class SemanticChunker:\n        def __init__(self, min_chunk_size=200, max_chunk_size=800):\n            self.min_chunk_size = min_chunk_size\n            self.max_chunk_size = max_chunk_size\n            self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        \n        def chunk_semantically(self, text, title=\"\"):\n            \"\"\"Break text into semantic chunks\"\"\"\n            sentences = text.split('. ')\n            \n            embeddings = self.model.encode(sentences)\n            chunks = []\n            current_chunk = []\n            current_embedding = None\n            \n            for i, sentence in enumerate(sentences):\n                current_chunk.append(sentence)\n                \n                if len(' '.join(current_chunk)) >= self.min_chunk_size:\n                    # Check semantic coherence\n                    chunk_embedding = self.model.encode(' '.join(current_chunk))\n                    \n                    if current_embedding is not None:\n                        # Cosine similarity with previous chunk\n                        similarity = np.dot(chunk_embedding, current_embedding) / (\n                            np.linalg.norm(chunk_embedding) * np.linalg.norm(current_embedding)\n                        )\n                        \n                        # If semantic break detected or max size reached\n                        if similarity < 0.6 or len(' '.join(current_chunk)) >= self.max_chunk_size:\n                            chunks.append({\n                                'content': ' '.join(current_chunk),\n                                'title': title,\n                                'tokens': len(' '.join(current_chunk).split())\n                            })\n                            current_chunk = []\n                            current_embedding = None\n                            continue\n                    \n                    current_embedding = chunk_embedding\n            \n            if current_chunk:\n                chunks.append({\n                    'content': ' '.join(current_chunk),\n                    'title': title,\n                    'tokens': len(' '.join(current_chunk).split())\n                })\n            \n            return chunks\n    \n\n**Result:**¬†Average chunk size went from 1,000 tokens ‚Üí 400 tokens (but more relevant). Retrieved fewer chunks but with less padding.\n\n# 2. Retrieval Pre-filtering Reduces What Gets Retrieved\n\n**Before:**¬†\"Get top-5 by relevance, send all to LLM\"\n\n**After:**¬†Multi-stage retrieval pre-filtering\n\n    def filtered_retrieval(query: str, documents: List[str], top_k=5):\n        \"\"\"Retrieve with automatic filtering\"\"\"\n        \n        # Stage 1: Broad retrieval (get more candidates)\n        candidates = vector_store.search(query, top_k=20)\n        \n        # Stage 2: Filter by relevance threshold\n        scored = [(doc, score) for doc, score in candidates]\n        high_confidence = [\n            (doc, score) for doc, score in scored \n            if score > 0.7  # Only confident matches\n        ]\n        \n        if not high_confidence:\n            high_confidence = scored[:5]  # Fallback to top-5\n        \n        # Stage 3: Deduplicate similar content\n        unique = []\n        seen_hashes = set()\n        \n        for doc, score in high_confidence:\n            doc_hash = hash(doc[:200])  # Hash of first 200 chars\n            \n            if doc_hash not in seen_hashes:\n                unique.append((doc, score))\n                seen_hashes.add(doc_hash)\n        \n        # Stage 4: Sort by relevance and return top-k\n        final = sorted(unique, key=lambda x: x[1], reverse=True)[:top_k]\n        \n        return [doc for doc, _ in final]\n    \n\n**Result:**¬†Retrieved fewer documents, but only high-confidence ones. Reduced retrieved token count by 40%.\n\n# 3. Query Simplification Before Retrieval\n\n**Before:**¬†Send raw user query to retriever\n\n    User: \"What are the refund policies for digital products if the customer received \n           a defective item and wants to know about international shipping costs?\"\n    (Complex, confusing retriever)\n    \n\n**After:**¬†Pre-process query to find key concepts\n\n    from langchain.chains import LLMChain\n    from langchain.prompts import PromptTemplate\n    \n    def simplify_query(query: str, llm) -> str:\n        \"\"\"Simplify query for better retrieval\"\"\"\n        \n        prompt = PromptTemplate(\n            input_variables=[\"query\"],\n            template=\"\"\"Extract the main topic from this query. \n            Remove adjectives, clarifications, and side questions.\n            \n            User query: {query}\n            \n            Simplified: \"\"\"\n        )\n        \n        chain = LLMChain(llm=llm, prompt=prompt)\n        \n        # Use cheaper model for this (gpt-3.5-turbo)\n        simplified = chain.run(query=query).strip()\n        \n        return simplified\n    \n    # Usage:\n    simplified = simplify_query(\n        \"What are the refund policies for digital products if the customer received \"\n        \"a defective item and wants to know about international shipping costs?\",\n        llm\n    )\n    # Result: \"refund policy digital products\"\n    \n\n**Result:**¬†Better retrieval queries ‚Üí fewer iterations ‚Üí fewer tokens.\n\n# 4. Response Compression Before Sending to LLM\n\n**Before:**¬†Send all retrieved documents as-is\n\n    Retrieved documents (all 3,000 tokens):\n    [Document 1: 1000 tokens]\n    [Document 2: 1000 tokens]\n    [Document 3: 1000 tokens]\n    \n\n**After:**¬†Compress while preserving information\n\n    def compress_context(documents: List[str], query: str, llm) -> str:\n        \"\"\"Compress documents while preserving relevant info\"\"\"\n        \n        compression_prompt = PromptTemplate(\n            input_variables=[\"documents\", \"query\"],\n            template=\"\"\"Summarize the following documents in as few words as possible \n            while preserving information relevant to the question.\n            \n            Question: {query}\n            \n            Documents:\n            {documents}\n            \n            Compressed summary:\"\"\"\n        )\n        \n        chain = LLMChain(llm=llm, prompt=compression_prompt)\n        \n        documents_text = \"\\n---\\n\".join(documents)\n        \n        compressed = chain.run(\n            documents=documents_text,\n            query=query\n        )\n        \n        return compressed\n    \n    # Usage:\n    context = compress_context(retrieved_docs, user_query, llm)\n    # 3000 tokens ‚Üí 800 tokens (still has all relevant info)\n    \n\n**Result:**¬†60-70% context reduction with minimal quality loss.\n\n# 5. Caching at the Context Level (Not Just Response Level)\n\n**Before:**¬†Cache full responses only\n\n    cache_key = hash(f\"{query}_{user_id}\")\n    cached_response = cache.get(cache_key)  # Only hits if identical query\n    \n\n**After:**¬†Cache compressed context\n\n    def cached_context_retrieval(query: str, user_context: str) -> str:\n        \"\"\"Retrieve and cache at context level\"\"\"\n        \n        # Hash just the query (not user context)\n        context_key = f\"context:{hash(query)}\"\n        \n        # Check if we've retrieved this query before\n        cached_context = cache.get(context_key)\n        \n        if cached_context:\n            return cached_context  # Reuse compressed context\n        \n        # If not cached, retrieve and compress\n        documents = retriever.get_relevant_documents(query)\n        compressed = compress_context(documents, query, llm)\n        \n        # Cache the compressed context\n        cache.set(context_key, compressed, ttl=86400)  # 24 hours\n        \n        return compressed\n    \n    # Usage:\n    context = cached_context_retrieval(query, user_context)\n    \n    # For identical queries from different users:\n    # User A: Retrieves, compresses (3000 tokens), caches\n    # User B: Uses cached context (0 tokens)\n    \n\n**Result:**¬†Context-level caching hits on 35% of queries (many users asking similar things).\n\n# 6. Token Counting Before Sending to LLM\n\n**Before:**¬†Blindly send context to LLM, hope it fits\n\n    response = llm.generate(system_prompt + context + user_query)\n    # Sometimes exceeds context window, sometimes wastes tokens\n    \n\n**After:**¬†Count tokens, optimize if needed\n\n    import tiktoken\n    \n    def smart_context_sending(context: str, query: str, llm, max_tokens=6000):\n        \"\"\"Send context to LLM, optimizing token usage\"\"\"\n        \n        enc = tiktoken.encoding_for_model(\"gpt-4\")\n        \n        # Count tokens in different parts\n        system_tokens = len(enc.encode(SYSTEM_PROMPT))\n        query_tokens = len(enc.encode(query))\n        context_tokens = len(enc.encode(context))\n        \n        total_input = system_tokens + query_tokens + context_tokens\n        \n        # If over budget, compress context further\n        if total_input > max_tokens:\n            compression_ratio = (total_input - max_tokens) / context_tokens\n            \n            # Aggressive compression if needed\n            compressed = aggressive_compress(context, compression_ratio)\n            context_tokens = len(enc.encode(compressed))\n            context = compressed\n        \n        # Now send to LLM\n        response = llm.generate(\n            system_prompt=SYSTEM_PROMPT,\n            context=context,\n            query=query\n        )\n        \n        return response\n    \n\n**Result:**¬†Stayed under token limits, never wasted tokens on too-large contexts.\n\n# The Results\n\n|Optimization|Before|After|Savings|\n|:-|:-|:-|:-|\n|Chunk size|1,000 tokens|400 tokens|Smaller chunks|\n|Retrieved docs|5 docs|3 docs|40% fewer|\n|Context compression|None|60% reduction|2x tokens|\n|Query simplification|None|Applied|Better retrieval|\n|Context caching|0% hit rate|35% hit rate|35% queries free|\n|Token counting|None|Applied|No waste|\n|**Total per query**|**5,500 tokens**|**2,200 tokens**|**60% reduction**|\n\n**Cost Impact:**\n\n* Before: 100k queries √ó 5,500 tokens √ó $0.03/1K =¬†**$16.50/day**¬†($495/month)\n* After: 100k queries √ó 2,200 tokens √ó $0.03/1K =¬†**$6.60/day**¬†($198/month)\n* **Savings: $297/month (60% reduction)**\n\n**Accuracy Impact:**\n\n* Before: 92% accuracy\n* After: 92% accuracy (unchanged)\n\n# Important Caveat\n\nThese optimizations come with tradeoffs:\n\n1. **Query simplification**¬†adds latency (extra LLM call, even if cheap)\n2. **Context compression**¬†could lose edge-case information\n3. **Caching**¬†reduces freshness (stale context for 24 hours)\n4. **Aggressive filtering**¬†might miss relevant documents\n\nWe accepted these tradeoffs. Your situation might differ.\n\n# Implementation Difficulty\n\n* **Easy:**¬†Token counting (1 hour)\n* **Easy:**¬†Retrieval filtering (2 hours)\n* **Medium:**¬†Query simplification (3 hours)\n* **Medium:**¬†Context compression (4 hours)\n* **Medium:**¬†Semantic chunking (4 hours)\n* **Hard:**¬†Context-level caching (5 hours)\n\n**Total:**¬†\\~19 hours of engineering work to save $297/month.\n\nPayback period: \\~1 month.\n\n# Code: Complete Pipeline\n\n    class OptimizedRAGPipeline:\n        def __init__(self, llm, retriever, cache):\n            self.llm = llm\n            self.retriever = retriever\n            self.cache = cache\n            self.encoder = tiktoken.encoding_for_model(\"gpt-4\")\n        \n        def process_query(self, user_query: str) -> str:\n            \"\"\"Complete optimized pipeline\"\"\"\n            \n            # Step 1: Simplify query\n            simplified_query = self.simplify_query(user_query)\n            \n            # Step 2: Retrieve with caching\n            context = self.cached_context_retrieval(simplified_query)\n            \n            # Step 3: Smart token handling\n            response = self.smart_context_sending(\n                context=context,\n                query=user_query\n            )\n            \n            return response\n        \n        def simplify_query(self, query: str) -> str:\n            \"\"\"Extract main topic from query\"\"\"\n            # Implementation from above\n            pass\n        \n        def cached_context_retrieval(self, query: str) -> str:\n            \"\"\"Retrieve and cache at context level\"\"\"\n            # Implementation from above\n            pass\n        \n        def smart_context_sending(self, context: str, query: str) -> str:\n            \"\"\"Send context with token optimization\"\"\"\n            # Implementation from above\n            pass\n    \n\n# Questions for the Community\n\n1. **Are you doing context-level caching?**¬†We found 35% hit rate. What's your experience?\n2. **How much quality loss do you see from compression?**¬†We measured \\~1-2% accuracy drop.\n3. **Query simplification latency trade:**¬†Is it worth the extra LLM call?\n4. **Semantic chunking:**¬†Are you doing it? How much better are results?\n5. **Token optimization:**¬†What's the best bang-for-buck optimization you've found?\n\n# Edit: Responses\n\n**On query simplification latency:**¬†\\~200-300ms added. With caching, only happens once per unique query. Worth it for most systems.\n\n**On context compression quality:**¬†We tested with GPT-3.5-turbo for compression (cheaper). Slightly more loss than GPT-4, but acceptable trade. Saves another $150/month.\n\n**On whether these are general:**¬†Yes, we tested on 3 different domains (legal, technical docs, customer support). Results were similar.\n\n**On LangChain compatibility:**¬†All of this integrates cleanly with LangChain's abstractions. No fighting the framework.\n\nWould love to hear if others have found different optimizations. Token cost is becoming the bottleneck.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1pxndjy/advanced_rag_token_optimization_and_cost/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nwcccn3",
          "author": "KegOfAppleJuice",
          "text": "Even though it's written up by AI, nice overview of things one can do for token optimisation",
          "score": 6,
          "created_utc": "2025-12-28 10:13:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg4nze",
          "author": "OnyxProyectoUno",
          "text": "The semantic chunking approach is solid but you're still doing fixed-size boundaries. The real gains come from content-aware splitting that preserves logical units.\n\nYour compression step is interesting but risky. Compressing 3000 tokens to 800 means you're losing 70% of the information and trusting an LLM to keep what matters. That works until it doesn't. The accuracy might look the same in aggregate but you're probably missing edge cases where the compressed context drops critical details.\n\nThe context-level caching is clever. Most people only cache final responses which misses the opportunity to reuse expensive retrieval work. 35% hit rate is good, though 24-hour TTL might be too aggressive depending on how often your docs change.\n\nOne thing that jumps out is you're optimizing downstream when the real waste might be upstream. If your chunks are poorly formed to begin with, you're just optimizing garbage. Bad chunks mean more retrieval attempts, more irrelevant context, more compression losses. Have you looked at what your semantic chunker is actually producing? Sometimes the chunking strategy matters more than all the downstream optimization combined.\n\nThe token counting is table stakes at this point. Surprised more people aren't doing that by default.\n\nWhat's your chunk quality like after the semantic splitting? Are you seeing clean breaks at logical boundaries or is it still cutting mid-thought sometimes?",
          "score": 4,
          "created_utc": "2025-12-28 23:10:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdi6kh",
          "author": "MathematicianSome289",
          "text": "This is great just don‚Äôt need the code",
          "score": 1,
          "created_utc": "2025-12-28 15:24:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwek7jz",
          "author": "saintskytower",
          "text": "Moderator Comment\n\nA reminder to everyone participating:\n\n‚Ä¢ Personal attacks, profanity, and calls to mass-report users or content violate Rule 2 (‚ÄúBe nice‚Äù) and will be removed.\n‚Ä¢ If you believe a post is spam or low quality, use the report function. Do not harass other users or moderators publicly.\n‚Ä¢ Moderation decisions are made by the moderation team based on the posted rules, not by hostile comments.\n\nThe post itself remains open for technical discussion. Keep replies focused on the substance of the content and within subreddit rules.",
          "score": 1,
          "created_utc": "2025-12-28 18:33:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfzxty",
          "author": "cmndr_spanky",
          "text": "Your solution isn‚Äôt good",
          "score": 1,
          "created_utc": "2025-12-28 22:44:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhlcvc",
              "author": "BothContribution7282",
              "text": "Why isn't the above solution good",
              "score": 1,
              "created_utc": "2025-12-29 04:07:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwhw75d",
                  "author": "cmndr_spanky",
                  "text": "‚ÄúSemantic chunking‚Äù in the implementation is effectively no different than picking a small min sized chunk and relying on the VDB default way of using a similarity score to bring back the right articles‚Ä¶ in fact the implementation is worse because you might bring back an 800 token sized chunk during usage when you only needed 200 tokens out of that chunk.\n\nIt‚Äôs almost 100% the above post is AI generated and the person who generated it didn‚Äôt even read it, nor are they commenting here. I wouldn‚Äôt trust any of it.",
                  "score": 1,
                  "created_utc": "2025-12-29 05:17:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q46bl7",
      "title": "Why RAG is hitting a wall‚Äîand how Apple's \"CLaRa\" architecture fixes it",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q46bl7/why_rag_is_hitting_a_walland_how_apples_clara/",
      "author": "SKD_Sumit",
      "created_utc": "2026-01-05 00:15:51",
      "score": 43,
      "num_comments": 6,
      "upvote_ratio": 0.87,
      "text": "Hey everyone,\n\nI‚Äôve been tracking the shift from \"Vanilla RAG\" to more integrated architectures, and Apple‚Äôs recent¬†**CLaRa**¬†paper is a significant milestone that I haven't seen discussed much here yet.\n\nStandard RAG treats retrieval and generation as a \"hand-off\" process, which often leads to the \"lost in the middle\" phenomenon or high latency in long-context tasks.\n\n**What makes CLaRa different?**\n\n* **Salient Compressor:**¬†It doesn't just retrieve chunks; it compresses relevant information into \"Memory Tokens\" in the latent space.\n* **Differentiable Pipeline:**¬†The retriever and generator are optimized together, meaning the system \"learns\" what is actually salient for the specific reasoning task.\n* **The 16x Speedup:**¬†By avoiding the need to process massive raw text blocks in the prompt, it handles long-context reasoning with significantly lower compute.\n\nI put together a technical breakdown of the¬†**Salient Compressor**¬†and how the¬†**two-stage pre-training**¬†works to align the memory tokens with the reasoning model.\n\n**For those interested in the architecture diagrams and math:**¬†[https://yt.openinapp.co/o942t](https://yt.openinapp.co/o942t)\n\nI'd love to discuss: Does anyone here think latent-space retrieval like this will replace standard vector database lookups in production LangChain apps, or is the complexity too high for most use cases?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q46bl7/why_rag_is_hitting_a_walland_how_apples_clara/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nxqm5u5",
          "author": "qa_anaaq",
          "text": "Pretty strong rebuttal for production cases via \n\nhttps://www.reddit.com/r/Rag/s/KyDWMdlGeE\n\nbut the idea is interesting",
          "score": 5,
          "created_utc": "2026-01-05 01:45:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxv9sog",
          "author": "pbalIII",
          "text": "Most RAG bottlenecks come from treating retrieval and generation as separate steps... CLaRa sidesteps this by compressing documents into continuous memory tokens and optimizing both together in the same latent space. The differentiable top-k lets gradients flow from answer tokens back into the retriever, so relevance aligns with actual answer quality.\n\n16x-128x compression is nice, but the real win is the joint optimization. Traditional RAG systems hope the LLM extracts what it needs from retrieved text. Here the compression itself is trained to preserve what the generator actually uses.",
          "score": 3,
          "created_utc": "2026-01-05 19:12:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nye8sr4",
              "author": "Ok_Sense_3580",
              "text": "EXACTLY",
              "score": 1,
              "created_utc": "2026-01-08 13:35:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxqqh7o",
          "author": "BeerBatteredHemroids",
          "text": "He's \"tracking the shift\" ü´°",
          "score": 2,
          "created_utc": "2026-01-05 02:08:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxr5paz",
              "author": "johnerp",
              "text": "Game changer",
              "score": 3,
              "created_utc": "2026-01-05 03:30:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxs4fpu",
          "author": "Upset-Pop1136",
          "text": "We tried both ‚Äúbetter retrieval‚Äù and ‚Äúsmaller context‚Äù because our OpenAI bill was getting silly. The part that mattered was unit cost per successful answer, not top-1 recall on a benchmark. When we cut prompt tokens by \\~60% using aggressive filtering + short summaries, our cost per resolved ticket dropped and response time improved enough that users stopped refreshing.",
          "score": 2,
          "created_utc": "2026-01-05 07:33:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1qjkz",
      "title": "fastapi-fullstack v0.1.11 released ‚Äì now with LangGraph ReAct agent support + multi-framework AI options!",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q1qjkz/fastapifullstack_v0111_released_now_with/",
      "author": "VanillaOk4593",
      "created_utc": "2026-01-02 05:44:43",
      "score": 38,
      "num_comments": 5,
      "upvote_ratio": 0.98,
      "text": "Hey r/LangChain,\n\nFor those new or catching up: fastapi-fullstack is an open-source CLI generator (pip install fastapi-fullstack) that creates production-ready full-stack AI/LLM apps with FastAPI backend + optional Next.js 15 frontend. It's designed to skip boilerplate, with features like real-time WebSocket streaming, conversation persistence, custom tools, multi-provider support (OpenAI/Anthropic/OpenRouter), and observability via LangSmith.\n\nFull changelog: [https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template/blob/main/docs/CHANGELOG.md](https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template/blob/main/docs/CHANGELOG.md?referrer=grok.com)  \nRepo: [https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template](https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template?referrer=grok.com)\n\n**Full feature set:**\n\n* Backend: Async FastAPI with layered architecture, auth (JWT/OAuth/API keys), databases (PostgreSQL/MongoDB/SQLite with SQLModel/SQLAlchemy options), background tasks (Celery/Taskiq/ARQ), rate limiting, admin panels, webhooks\n* Frontend: React 19, Tailwind, dark mode, i18n, real-time chat UI\n* AI: Now supports **LangChain**, **PydanticAI**, and the new **LangGraph** (more below)\n* 20+ configurable integrations: Redis, Sentry, Prometheus, Docker, CI/CD, Kubernetes\n* Django-style CLI + production Docker with Traefik/Nginx reverse proxy options\n\n**Big news in v0.1.11 (just released):**  \nAdded **LangGraph as a third AI framework option** alongside LangChain and PydanticAI!\n\n* New --ai-framework langgraph CLI flag (or interactive prompt)\n* Implements **ReAct (Reasoning + Acting) agent pattern** with graph-based flow: agent node for LLM decisions, tools node for execution, conditional edges for loops\n* Full memory checkpointing for conversation continuity\n* WebSocket streaming via astream() with modes for token deltas and node updates (tool calls/results)\n* Proper tool result correlation via tool\\_call\\_id\n* Dependencies auto-added: langgraph, langgraph-checkpoint, langchain-core/openai/anthropic\n\nThis makes it even easier to build advanced, stateful agents in your full-stack apps ‚Äì LangGraph's graph architecture shines for complex workflows.\n\nLangChain community ‚Äì how does LangGraph integration fit your projects? Any features to expand (e.g., more graph nodes)? Contributions welcome! üöÄ",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LangChain/comments/1q1qjkz/fastapifullstack_v0111_released_now_with/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nx7ub4r",
          "author": "hopes_alive123",
          "text": "any sample projects built using this?",
          "score": 3,
          "created_utc": "2026-01-02 07:23:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7w4ea",
              "author": "VanillaOk4593",
              "text": "It's our internal tool and we use it in the projects we build for our clients so I cant share real examples but it's really easy to generate a new projects with just only 2 commands [https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template?tab=readme-ov-file#installation](https://github.com/vstorm-co/full-stack-fastapi-nextjs-llm-template?tab=readme-ov-file#installation)",
              "score": 1,
              "created_utc": "2026-01-02 07:40:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7jxoa",
          "author": "mamaBiskothu",
          "text": "How do you ensure exactly once processing with celery?",
          "score": 1,
          "created_utc": "2026-01-02 05:55:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx85ip6",
          "author": "Preconf",
          "text": "Can I use phoenix for observability instead?",
          "score": 1,
          "created_utc": "2026-01-02 09:09:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx85t8b",
              "author": "VanillaOk4593",
              "text": "It's not implemented, so you would need to do it yourself. However, you can do anything with it since it only generates a project template, there are no limitations",
              "score": 2,
              "created_utc": "2026-01-02 09:12:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q8q36d",
      "title": "Scaling RAG from MVP to 15M Legal Docs ‚Äì Cost & Stack Advice",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q8q36d/scaling_rag_from_mvp_to_15m_legal_docs_cost_stack/",
      "author": "Additional-Oven4640",
      "created_utc": "2026-01-10 01:01:11",
      "score": 35,
      "num_comments": 20,
      "upvote_ratio": 0.89,
      "text": "Hi all;\n\nWe are seeking investment for a LegalTech RAG project and need a realistic budget estimation for scaling.\n\n**The Context:**\n\n* **Target Scale:**¬†\\~15 million text files (avg. 120k chars/file). Total \\~1.8 TB raw text.\n* **Requirement:**¬†High precision. Must support¬†**continuous data updates**.\n* **MVP Status:**¬†We achieved successful results on a small scale using¬†`gemini-embedding-001`¬†**+**¬†`ChromaDB`.\n\n**Questions:**\n\n1. Moving from MVP to 15 million docs: What is a realistic OpEx range (Embedding + Storage + Inference) to present to investors?\n2. Is our MVP stack scalable/cost-efficient at this magnitude?\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q8q36d/scaling_rag_from_mvp_to_15m_legal_docs_cost_stack/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nypfop8",
          "author": "caprica71",
          "text": "You will run into precision recall issues with that much data.  Look at building some custom metadata extraction for entities of interest in each document (eg people, companies, dates of events,  etc) and then put that data into a set of tables in a database a mcp tool can search (eg postgresql).  The other option is to look at graphrag for those entities ( neo4j).\n\n\nThe graph option could be compelling for that kind of data if there are lots of entities you are tracking",
          "score": 7,
          "created_utc": "2026-01-10 01:14:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nypu2sd",
              "author": "AdditionalWeb107",
              "text": "Did this for a Fortune500 - additional details to consider is late-fusion (binding context from top of key parts of the document to context most relevant), temporal freshness (which is related to metadata), experiments for context compression for speed and accuracy. And of course prompt iteration.",
              "score": 3,
              "created_utc": "2026-01-10 02:33:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nysn3dl",
                  "author": "Umber_john",
                  "text": "MongoDB is putting forward an interesting solution for late-fusion called context chunking. Might be something to look into.",
                  "score": 1,
                  "created_utc": "2026-01-10 15:02:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nysalmm",
          "author": "iamsyr",
          "text": "Scaling to 15M docs for legal tech requires robust infrastructure. While Chroma is great for MVPs, at that scale you might face latency issues; consider **Milvus** or **Weaviate** for better scalability and multi-tenancy support. Also, for legal precision, pure vector search might struggle‚Äîhybrid search (combining dense vectors with sparse keyword search like BM25) is usually necessary. Ossaix.com has comparisons on vector DBs suited for high-scale enterprise RAG that might help with your investor presentation.",
          "score": 3,
          "created_utc": "2026-01-10 13:51:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nysbyda",
          "author": "iamsyr",
          "text": "Scaling to 15M legal docs (1.8TB) will definitely strain a basic Chroma setup. You'll likely need to move to a distributed vector DB like Qdrant or Milvus, and consider 'sparse' vectors (SPLADE) alongside dense embeddings to handle legal terminology precision. On Ossaix, we have comparisons of vector databases that highlight their scalability features and estimated costs for high-volume datasets.",
          "score": 3,
          "created_utc": "2026-01-10 13:59:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyphq06",
          "author": "papipapi419",
          "text": "Check out semantic collapse study on rag (done by Stanford I think) so you‚Äôll need hybrid approach for sure",
          "score": 5,
          "created_utc": "2026-01-10 01:25:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyrd8eq",
              "author": "-Cubie-",
              "text": "This paper does not exist, and the social media posts on it were fully AI generated (or rather, hallucinated). \n\nIt claimed that retrieval fully falls apart after like 50k docs, but that's not true at all. See for example this demo with 41m texts from Wikipedia using dense embedding retrieval: https://huggingface.co/spaces/sentence-transformers/quantized-retrieval",
              "score": 10,
              "created_utc": "2026-01-10 09:23:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyrdcnc",
                  "author": "papipapi419",
                  "text": "Thanks for the fact check",
                  "score": 3,
                  "created_utc": "2026-01-10 09:24:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nypuc2a",
          "author": "AdditionalWeb107",
          "text": "doc size is less important - what is the chunk size per doc on average, any relations? what's the long-tail queries you think you would want to support. the runtime costs on embedding retrieval can stack up if you context build up has holes in it. We built something for T-Mobile and it is easy a 10k/month running cost if done wrong.\n\nAlso you will need to think about query mutations (like re-writing, context stuffing, and build the agent in a more agentic fashion so that it has access to tools for filtering and condensing content), hybrid retrieval strategies for keyword+semantic match, late fusion depending on the type of context, etc. See Plano as that is something we used to scale the solution and use different models for different steps of the workflow in a clean and scalable way: [https://github.com/katanemo/plano](https://github.com/katanemo/plano) \\- especially [https://docs.planoai.dev/concepts/filter\\_chain.html](https://docs.planoai.dev/concepts/filter_chain.html)",
          "score": 2,
          "created_utc": "2026-01-10 02:34:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyr9kvi",
          "author": "mrpeakyblinder2",
          "text": "Regarding the database I would move to elastic search instead of chroma db",
          "score": 2,
          "created_utc": "2026-01-10 08:48:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrbcla",
          "author": "rafttaar",
          "text": "Check turbopuffer",
          "score": 2,
          "created_utc": "2026-01-10 09:05:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nytgl87",
          "author": "rdewolff",
          "text": "Graphrag",
          "score": 2,
          "created_utc": "2026-01-10 17:26:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyt3her",
          "author": "Weary_Long3409",
          "text": "How many docs are your MVP small scale succeded?",
          "score": 1,
          "created_utc": "2026-01-10 16:24:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyu59fl",
          "author": "iamsyr",
          "text": "Moving from an MVP to 15M docs is a major architectural shift. While ChromaDB is great for starting out, at that scale (1.8TB raw text), you might hit bottlenecks with ingestion throughput and memory usage. \n\nFor production at that scale, you should evaluate vector stores designed for high throughput like **Qdrant** or **Milvus**. You also need a robust ingestion pipeline to handle continuous updates without downtime. We track production-grade RAG stacks on Ossaix.com if you want to see what other enterprise-scale projects are using for high-volume retrieval.",
          "score": 1,
          "created_utc": "2026-01-10 19:22:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyyiyyt",
          "author": "Preconf",
          "text": "For legal tech, precision may look different depending on which area of law (civil(contract, tort, trusts) or criminal (common law, legislation) and what the actual service looks like to the end user (consumer, legal practitioners, compliance officers etc). Reranking models and services (zero entropy) may be worth considering if it isn't already in your stack to improve precision. Costing may not be reasonably feasible (outside of inference and storage costs) until you know what your stack looks like. Going down the containerisation/micro services route and using a combination of kubernetes and terraform/open tofu can afford quite a bit of flexibility (traffic/network management, prevent vendor lockin and make ci/cd more streamlined but also may introduce unnecessary technical overhead depending on what your preferences are). Once you have an idea of what a typical interaction looks like, it's easier to extrapolate from and get some ballpark figures while also allowing you to get a start on capacity planning. Creating a small cluster/container stack and using something like litellm to route your inference traffick through will give you a better idea of inference and embedding costs from its dashboard. Databases at scale aren't trivial, more so with vector databases. Postgres has been sworn by by many a developer and devop, but whether that translates to its cousin pgvector I couldn't say off the top of my head. Quality of embeddings plays a big role in the precision of answers so getting the ingestion engine right is a must. Tabular data is a gotcha that can trip people up if it's not tokenized properly so definitely factor it in if it's relevant to you. I hope this bag of words helps in some way.",
          "score": 1,
          "created_utc": "2026-01-11 11:58:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzb5fam",
          "author": "PerformanceFine1228",
          "text": "ex-YC founding engineer (Klarity.ai Series B)  \ntl;dr - the budget varies a lot anywhere from $50K to $250K to god knows what  \nyou can have high precision OCR, what you can't have A) ensure LLMs understand all the legal clauses well (good-old IR techniques work here) B) ensure you can build a pipeline to avoid the long tail of LLM failures. \n\nlatter is more risky - since if the # of ops tickets increase linearly with more docs, you're gonna have a hard time growing Ops team",
          "score": 1,
          "created_utc": "2026-01-13 06:20:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzc3kwo",
          "author": "agentsinthewild",
          "text": "For 15M docs at \\~1.8TB, your one-time embedding bill will be somewhere between $10-80k depending on which model you pick (OpenAI text-embedding-3-small is the budget option, Google/OpenAI large models cost more). Monthly OpEx is harder to pin down without knowing your query volume - ballpark $5-12k/mo on the low end, $40-100k+ if you're handling heavy traffic. Big thing to note: LLM inference is where most of your money goes, not embeddings or vector storage.\n\nOn your stack question, ChromaDB probably won't hold up at 15M docs. Look at Qdrant, Weaviate, or Milvus for that scale. Since you're in legal, you'll likely want hybrid search (semantic + BM25) for precision, which rules out some options. Pinecone is easy but gets expensive fast; self-hosting saves cash if you have the DevOps bandwidth.",
          "score": 1,
          "created_utc": "2026-01-13 11:33:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrswzu",
          "author": "jj_taylor_05",
          "text": "You will face semantic collapse, retrieve challenge, and token bills surprise at soon as you start to scale up",
          "score": 0,
          "created_utc": "2026-01-10 11:46:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q64nnb",
      "title": "A unified Knowledge Graph router for AI agents (Apache-2.0)",
      "subreddit": "LangChain",
      "url": "https://github.com/neuledge/graph",
      "author": "moshestv",
      "created_utc": "2026-01-07 03:37:46",
      "score": 34,
      "num_comments": 8,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1q64nnb/a_unified_knowledge_graph_router_for_ai_agents/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "ny5j8q5",
          "author": "HOLDL",
          "text": "New to this so not trying to throw shade but what‚Äôs the point? Ex if I search any of the examples in Claude I get the same answers. Is the idea that there is more latency with web search and this makes retrieval more efficient / contextual?",
          "score": 2,
          "created_utc": "2026-01-07 05:58:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5w4l3",
              "author": "Signal_Question9074",
              "text": "ok bud. so the thing is we realized in 2025 that graph db;s are the shit when it comes for memory and retrieval for LLM's. and now its one of the standards and focus points of this year, alongside neo4j, redis agent caching and symantic caching, agent fleet orchestration and skills :)",
              "score": 0,
              "created_utc": "2026-01-07 07:46:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6scjz",
                  "author": "justanemptyvoice",
                  "text": "Why does this comment read like it‚Äôs the OP or a friend of the OP? And why does it read as condescending?\n\n‚Äúok bud. so the thing is we realized in 2025 that graph db;s are the shit when it comes for memory and retrieval for LLM's. and now its one of the standards and focus points of this year, alongside neo4j, redis agent caching and symantic caching, agent fleet orchestration and skills :)‚Äù",
                  "score": 3,
                  "created_utc": "2026-01-07 12:24:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny502fe",
          "author": "macromind",
          "text": "This is really cool, the integration tax is so real (API keys, schema plumbing, then massaging JSON so the model does not derail). Love the <100ms structured lookup angle, that is basically the missing piece for agentic flows that need reliable tools.\n\nCurious, do you have a pattern for caching and tool fallback (like if a provider is down)? I have been collecting notes on agent reliability and tool orchestration here too: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-01-07 03:49:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5vzxx",
          "author": "Signal_Question9074",
          "text": "you are getting a fat star from me and hopefully ill be able to add my contributions to you on the weekend.",
          "score": 1,
          "created_utc": "2026-01-07 07:45:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6gka9",
              "author": "moshestv",
              "text": "Thanks man, appreciate it!",
              "score": 1,
              "created_utc": "2026-01-07 10:53:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0t2qd",
      "title": "GraphQLite - Embedded graph database for building GraphRAG with SQLite",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q0t2qd/graphqlite_embedded_graph_database_for_building/",
      "author": "Fit-Presentation-591",
      "created_utc": "2026-01-01 01:59:35",
      "score": 28,
      "num_comments": 15,
      "upvote_ratio": 0.95,
      "text": "For anyone building GraphRAG systems who doesn't want to run Neo4j just to store a knowledge graph, I've been working on something that might help.\n\n\n\nGraphQLite is an SQLite extension that adds Cypher query support. The idea is that you can store your extracted entities and relationships in a graph structure, then use Cypher to traverse and expand context during retrieval. Combined with sqlite-vec for the vector search component, you get a fully embedded RAG stack in a single database file.\n\n\n\nIt includes graph algorithms like PageRank and community detection, which are useful for identifying important entities or clustering related concepts. There's an example in the repo using the HotpotQA multi-hop reasoning dataset if you want to see how the pieces fit together.\n\n\n\n\\`pip install graphqlite\\`\n\n\n\nGitHub: [https://github.com/colliery-io/graphqlite](https://github.com/colliery-io/graphqlite)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q0t2qd/graphqlite_embedded_graph_database_for_building/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nx0rjch",
          "author": "International_Quail8",
          "text": "Very cool!",
          "score": 2,
          "created_utc": "2026-01-01 02:41:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0qvkb",
          "author": "Don_Ozwald",
          "text": "Can someone explain to me, what‚Äôs useful about GraphRAG?",
          "score": 1,
          "created_utc": "2026-01-01 02:37:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0ut5y",
              "author": "Fit-Presentation-591",
              "text": "There‚Äôs an example in the project at [https://github.com/colliery-io/graphqlite/tree/main/examples/llm-graphrag](https://github.com/colliery-io/graphqlite/tree/main/examples/llm-graphrag)  \n\n\nTraditional RAG uses only vector similarity to find relevant documents. But multi-hop questions require connecting information across documents. By pre processing the documents and building up a set of relationships (COOCCURS in this case is set when two people are in the same sentence), we can use traversals to find information that may be related to the inital query and add it to the context for evaluation. \n\nThe concrete example from the demo is \n\n**Example:**¬†\"Were Scott Derrickson and Ed Wood of the same nationality?\"\n\n* Vector search finds \"Ed Wood\" but may miss \"Scott Derrickson\"\n* Graph traversal via COOCCURS edges discovers the connection\n* Community detection finds topically related articles",
              "score": 5,
              "created_utc": "2026-01-01 03:04:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx101ol",
                  "author": "Don_Ozwald",
                  "text": "I understand that. My point is more regarding, does the LLM understand that? I mean. Isn't GraphRAG just adding an unnecessary layer of abstraction for the LLM to struggle with, that are better dealt with just hybrid search. With the example in mind, it certainly seems so, as hybrid search would only fail on that one with k=1.",
                  "score": 1,
                  "created_utc": "2026-01-01 03:39:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx0uiez",
          "author": "BetaCuck80085",
          "text": "Wow, *exactly* what I‚Äôve been looking for. Use Neo4j on the job, wanted to use something more lightweight for a few personal endeavors. Really appreciate you sharing this.",
          "score": 1,
          "created_utc": "2026-01-01 03:02:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0uwh0",
              "author": "Fit-Presentation-591",
              "text": "Music to my ears !",
              "score": 1,
              "created_utc": "2026-01-01 03:05:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2p68j",
          "author": "eternviking",
          "text": "It's SQLite and you missed the chance to name it GRAPHite.",
          "score": 1,
          "created_utc": "2026-01-01 13:11:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2pqds",
              "author": "Fit-Presentation-591",
              "text": "I mean I can edit text its not like that's immutable.",
              "score": 2,
              "created_utc": "2026-01-01 13:15:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2y1vn",
                  "author": "eternviking",
                  "text": "Do it. Will be worth it.",
                  "score": 1,
                  "created_utc": "2026-01-01 14:15:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxn5ia3",
          "author": "Bjalal",
          "text": "Wÿ§ÿ´ÿ§ÿåÿå¬§„Ää Ÿ©ÿ±",
          "score": 1,
          "created_utc": "2026-01-04 15:58:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qd4q52",
      "title": "Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/0hoy86u9sedg1",
      "author": "DeathShot7777",
      "created_utc": "2026-01-15 00:36:33",
      "score": 26,
      "num_comments": 15,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qd4q52/building_opensource_client_sided_code/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzpsvl3",
          "author": "ConcertTechnical25",
          "text": "The next frontier of dev tools isn't \"AI in the Cloud,\" but \"Intelligence in the Browser.\" Shipping the entire vector backend and graph query layer via WASM is a game-changer for IP protection. When the code never leaves the client's machine, the barrier for enterprise adoption disappears. The combination of LanceDB for local vector storage and a structural graph layer for dependency mapping is exactly the kind of \"Neural Core\" we need for the next generation of autonomous coding agents.",
          "score": 10,
          "created_utc": "2026-01-15 12:23:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpuz7s",
              "author": "DeathShot7777",
              "text": "Yes. Thats Y i am also trying to make it work through ollama as a provider. Its tough making cyfer generation accurate with SLMs though",
              "score": 0,
              "created_utc": "2026-01-15 12:38:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzpx4a1",
              "author": "DeathShot7777",
              "text": "Ip protection especially with the shit show going on with anthropic nowdays üòÆ‚Äçüí®",
              "score": 0,
              "created_utc": "2026-01-15 12:52:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzn9woy",
          "author": "o5mfiHTNsH748KVq",
          "text": "Hold my beer, I'm gonna throw Unreal Engine at it.",
          "score": 3,
          "created_utc": "2026-01-15 00:56:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzna5yp",
              "author": "DeathShot7777",
              "text": "STOP, the DB engine runs inbrowser and it supports js ts and python right now. It will explode ;-;\n\nShould have mentioned that in the post ;-;",
              "score": 2,
              "created_utc": "2026-01-15 00:58:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzncpaw",
                  "author": "o5mfiHTNsH748KVq",
                  "text": "lol yes that's important information :D\n\nI starred it. This seems like a great project.",
                  "score": 3,
                  "created_utc": "2026-01-15 01:12:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nznapf9",
              "author": "DeathShot7777",
              "text": "Didnt know Unreal engine was opensource, wow!",
              "score": 2,
              "created_utc": "2026-01-15 01:01:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nztzp0h",
          "author": "nineblog",
          "text": "What library is used for this front-end view? It looks very silky",
          "score": 2,
          "created_utc": "2026-01-16 00:40:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzu06pl",
              "author": "DeathShot7777",
              "text": "The graph is made using sigma js combining with force atlas 2 ( physics simulation engine ) and lots of hit and trial with the repulsion so that they dont clump up. \n\nRest of the UI is courtesy of opus 4.5üôè",
              "score": 1,
              "created_utc": "2026-01-16 00:43:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nznxuuw",
          "author": "DaRandomStoner",
          "text": "This is amazing work... seriously thank you so much. Testing it out now but looking at the repo has me excited!\n\nEdit: Again can't thank you enough. Put together a PR for you with what I found while integrating it. This is such a great idea. Works like a charm in case anyone is wondering.",
          "score": 1,
          "created_utc": "2026-01-15 03:15:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzohoz8",
              "author": "DeathShot7777",
              "text": "Really appreciated it. Feel free to DM if u have some cool idea or need help integrating",
              "score": 2,
              "created_utc": "2026-01-15 05:29:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzp3f7k",
          "author": "hopes_alive123",
          "text": "mainframe support similar to python js ?",
          "score": 1,
          "created_utc": "2026-01-15 08:37:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp5341",
              "author": "DeathShot7777",
              "text": "Hmm good idea. Might be useful for people working on mainframe migration/audits etc",
              "score": 2,
              "created_utc": "2026-01-15 08:53:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzv9wgd",
          "author": "Analytics-Maken",
          "text": "Cool work on the MCP integration. For the cypher generation issue with SLMs, adding clear examples in your prompts helps a lot. Show the model what good queries look like, then add a check layer to catch bad outputs before they run. \n\nHave you considered expanding to data analysis? It could combine with business metrics via ETL tools like Windsor ai or MCP servers and find root causes, what drives performance, etc.",
          "score": 1,
          "created_utc": "2026-01-16 05:10:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvalef",
              "author": "DeathShot7777",
              "text": "Yeah, firstly working on the MCP and next I m experimenting on what if the agent can place certain information in the graph itself for its own reference maybe like a graph based todo and notes etc. This can be useful for long running task like codebase audits for compliances like SOC2 etc. Also adding in business logic into the graph along with code relations will make the agent lot more aware, great for onboarding, identifying gaps and bottlenecks.\n\nLot of work there üòÆ‚Äçüí® but interesting",
              "score": 1,
              "created_utc": "2026-01-16 05:15:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q5mjzy",
      "title": "ChatEpstein with LangChain",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q5mjzy/chatepstein_with_langchain/",
      "author": "br3nn21",
      "created_utc": "2026-01-06 16:02:51",
      "score": 25,
      "num_comments": 29,
      "upvote_ratio": 0.96,
      "text": "While there‚Äôs been a lot of information about Epstein released, much of it is very unorganized. There have been platforms like¬†jmail.world, but it still contains a wide array of information that is difficult to search through quickly.\n\nTo solve these issues, I created¬†[ChatEpstein](https://chat-epstein.vercel.app/), a chatbot with access to the Epstein files to provide a more targeted search. Right now, it only has a subset of text from the documents, but I was planning on adding more if people were more interested. This would include more advanced data types (audio, object recognition, video) while also including more of the files.\n\nHere‚Äôs the data I‚Äôm using:\n\n**Epstein Files Transparency Act (H.R.4405)**¬†\\-> I extracted all pdf text\n\n**Oversight Committee Releases Epstein Records Provided by the Department of Justice**¬†\\-> I extracted all image text\n\n**Oversight Committee Releases Additional Epstein Estate Documents**¬†\\-> I extracted all image text and text files\n\nOverall, this leads to about 300k documents total.\n\nWith all queries, results will be quoted and a link to the source provided. This will be to prevent the dangers of hallucinations, which can lead to more misinformation that can be very harmful. Additionally, proper nouns are strongly highlighted with searches. This helps to analyze specific information about people and groups. My hope with this is to increase accountability while also minimizing misinformation.\n\nFeel free to let me know if there are any issues or improvements you'd let me see. I‚Äôd love to grow this and get it into the hands of more people to spread more information about the Epstein Files.\n\n[https://chat-epstein.vercel.app/](https://chat-epstein.vercel.app/)  \n\n\n[](https://www.reddit.com/submit/?source_id=t3_1q5lgnw)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q5mjzy/chatepstein_with_langchain/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "ny0z4uq",
          "author": "br3nn21",
          "text": "Here‚Äôs the tech I used:\n\nFor initial storage, I put all the files in an AWS S3 bucket. Then, I used Pinecone as a vector database for the documents. For my chunking strategy, I initially used a character count of 1024 for each chunk, which worked well for long, multipage documents. However, since many of the documents are single-page and have a lot of continuous context, I have been experimenting with a page-based chunking strategy. Additionally, I am using spAcy to find people, places, and geopolitical entities.¬†\n\nDuring the retrieval phase, I am fetching both using traditional methods and using entity-based matching. Doing both of these gives me more accurate but diverse results. I am also having it keep track of the last 2 2 exchanges (4 messages: 2 user + 2 assistant). Overall, this gives me a token usage of 2k-5k. Because I‚Äôm semi-broke, I‚Äôm using Groq‚Äôs cheap llama-3.1-8b-instant API.\n\nOne of the most important parts of this phase is accuracy. Hallucinations from an LLM are an inherent certainty in some instances. As a result, I have ensured that I am not only providing information, but also quotes, sources, and links to every piece of information. I also prompted the LLM to try to avoid making assumptions not directly stated in the text.\n\nWith that being said, I‚Äôm certain that there will be issues, given the non-deterministic nature of AI models and the large amount of data being fed. If anyone finds any issues, please let me know! I‚Äôd love to fix them to make this a more usable tool.",
          "score": 3,
          "created_utc": "2026-01-06 16:02:59",
          "is_submitter": true,
          "replies": [
            {
              "id": "ny42mo2",
              "author": "timmy166",
              "text": "You need to do a summary pass for first line retrieval for a dataset this massive. Going straight to vector DB embeddings will lose a lot of context unless your prompt is absolutely massive",
              "score": 2,
              "created_utc": "2026-01-07 00:46:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny2ntb1",
          "author": "Radiate_Wishbone_540",
          "text": "Sent this: \"Who are the most surprising associates of Epstein?\"\n\nAnd got this: \"There was an error processing your request. Please try again.\"",
          "score": 2,
          "created_utc": "2026-01-06 20:38:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny39p8g",
              "author": "br3nn21",
              "text": "So that message actually indicates an API issue, and I saw a 500 request around the time you posted this. Currently, the rate of these errors is much less than 1%, and I'm assuming it's due to either updates or memory issues (which I should have fixed)",
              "score": 1,
              "created_utc": "2026-01-06 22:19:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny13sr8",
          "author": "mdrxy",
          "text": "Nice, can you share the source repo?",
          "score": 1,
          "created_utc": "2026-01-06 16:24:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1ipv9",
              "author": "br3nn21",
              "text": "So I accidentally posted the .env files in an early commitüòÖ I should be able to reclone it tho and make it public for people to check out",
              "score": 2,
              "created_utc": "2026-01-06 17:32:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny1tz4v",
                  "author": "Still-Ad3045",
                  "text": "Nuke it first lol",
                  "score": 1,
                  "created_utc": "2026-01-06 18:22:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny22f6r",
              "author": "br3nn21",
              "text": "A little messy, but here's the cleaned repo: [https://github.com/BrennenFa/ChatEpstein](https://github.com/BrennenFa/ChatEpstein)",
              "score": 1,
              "created_utc": "2026-01-06 18:59:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny1f1ud",
          "author": "OnyxProyectoUno",
          "text": "300k documents is where you start hitting real preprocessing bottlenecks. The issue isn't just volume, it's that OCR'd text from images and PDFs usually comes out messy and your chunking strategy becomes critical for retrieval quality.\n\nWith that much extracted text, you're probably seeing inconsistent formatting between the PDF extractions and image OCR results. The image text especially tends to have weird spacing, broken words, and missing context that makes chunking unpredictable. I've been building document processing tooling at vectorflow.dev specifically because this preprocessing stage is where most RAG systems break down silently.\n\nA few things to watch out for: OCR'd proper nouns (which you mentioned highlighting) often get mangled during extraction, so \"Jeffrey Epstein\" might become \"Jeffrey Ep stein\" in some chunks but not others. Your retrieval will miss these variations. Also, with 300k docs, you'll want to preserve document-level metadata (source file, date, document type) and make sure it propagates to every chunk, otherwise you lose the citation trail you're trying to maintain.\n\nFor accountability applications like this, consider adding entity extraction during preprocessing to tag people, organizations, and dates consistently across all documents. This gives you much better retrieval precision than just relying on semantic similarity.\n\nWhat does your current chunking strategy look like, and are you seeing consistency issues between the PDF and image-extracted content?",
          "score": 1,
          "created_utc": "2026-01-06 17:15:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1jqca",
              "author": "br3nn21",
              "text": "Thanks for the comment! Lots of good stuff here for me to respond to\n\nRetrieval quality is definetly one of the biggest issues, especially since I‚Äôm trying to remain token efficient. The proper noun system has not been perfect like you said, so I‚Äôve been using it with a mix of traditional retrieval. These entities are also stored as metadata which makes the querying more efficient\n\nI stored metadata, like source, file name, s3 location etc. in the vector db for sourcing like u said. At this scale, it‚Äôs definitely been something I had to mess around with. At one point I even uploaded tens of thousands of documents with the wrong sources and that made citations incorrect.\n\n\nCurrently I‚Äôm doing page based (may files are single page so not to complex). I haven‚Äôt compared the different between character and page based but was planning on doing so and using feedback from users as well",
              "score": 1,
              "created_utc": "2026-01-06 17:36:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny1z6wf",
                  "author": "OnyxProyectoUno",
                  "text": "Page-based chunking makes sense for single page docs, but you'll probably hit issues where related information spans pages or where OCR artifacts create artificial boundaries mid-sentence. The metadata approach is solid though, especially storing S3 locations for the citation trail.\n\nThat upload mistake with wrong sources sounds painful but it's exactly the kind of silent failure that kills trust in these systems. With mixed retrieval combining semantic and entity matching, you're probably getting better recall on mangled proper nouns, but I'd be curious about your precision. Are users getting too many irrelevant results when entity extraction picks up common names across unrelated contexts?\n\nFor the character vs page comparison, user feedback is good but also check retrieval metrics if you can. Page chunks might miss connections between related content that got split arbitrarily, while character chunks might break up important document structure. Depends on how structured your source documents are.",
                  "score": 2,
                  "created_utc": "2026-01-06 18:45:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny1m1oa",
          "author": "forevergeeks",
          "text": "is this thing hallucinating? I asked this question: **top 10 famous people in the files**\n\nand got:\n\nBased on the provided documents, I found the following information about the top 10 famous people mentioned:\n\n1. **Vladimir Putin**: Topped the Russian poll with a whopping 24%¬†\n2. **Lionel Messi**: Had broader support, reaching 15th in the world¬†\n3. **David Beckham**: Squeaked into 10th place in Britain¬†\n4. **Edward Snowden**: 10th most admired in the country that granted him asylum¬†\n5. **Angela Merkel**: Scored 65 times higher at 13% in Germany¬†\n6. **The Pope**: Second only to Angela Merkel in Germany¬†\n7. **Nigel Farage**: In 16th place in Britain¬†\n8. **David Cameron**: Received the \"most admired\" accolade from only two people in a thousand in Britain\n9. **Nicolas Sarkozy**: Third in France¬†\n10. **Goodluck Jonathan**: 8th in Nigeria¬†\n\n**Sources:**\n\n* **HOUSE\\_OVERSIGHT\\_022344, Page 1.0 - Oversight Committee Releases Additional Epstein Estate Documents**¬†\\-¬†View Document",
          "score": 1,
          "created_utc": "2026-01-06 17:47:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1qg1z",
              "author": "br3nn21",
              "text": "That's really strange, thanks for the comment!  I had loosened its strictness with looking at the database to allow it to make general inferences in certain instances, but it looks like it's leading to issues such as this. I just changed the prompt and the fixes should be fixed shortly. Using your same prompt, I got these results:\n\n  \nAnswer: I don't have information about that in the documents.\n\nHowever, I can tell you that the documents mention the following individuals:\n\n1. Prince Andrew (DOJ-OGR-00024825, Page 1.0)\n\n2. Bill Clinton (DOJ-OGR-00024826, Page 2.0)\n\n  \nAnswer: I don't have information about that in the documents.\n\n  \nAdditionally, I'm currently using llama 3.1. Because it's the cheapest modell I'm very certain that, especially for how large a dataset I'm giving it and how much I'm trying to limit extreme token usage (I'm broke), there are instances where it will not collect every instance of information at once",
              "score": 1,
              "created_utc": "2026-01-06 18:06:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny2nvn5",
                  "author": "Radiate_Wishbone_540",
                  "text": "Add a button for people to donate to the project?",
                  "score": 2,
                  "created_utc": "2026-01-06 20:38:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny3cia2",
          "author": "Radiate_Wishbone_540",
          "text": "Hallucination/nonsensical response: \"Prince Andrew was reportedly asked by lawyers, \"Have you ever been made to perform sexually on Prince Andrew?\" and Nadia replied with \"Fifth\" (Document 2, Page 1.0 and Document 4, Page 1.0)\"",
          "score": 1,
          "created_utc": "2026-01-06 22:32:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny3e8md",
              "author": "br3nn21",
              "text": "So in this instance, I believe Nadia is pleading the fifth, which means she's reserving her right to not say anything in a trial. To me, that would make sense. However, it could also be that the chunk got cut off",
              "score": 1,
              "created_utc": "2026-01-06 22:41:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny4inqf",
                  "author": "Radiate_Wishbone_540",
                  "text": "No, the sentence structure is completely absurd (unless I'm going crazy). The sentence suggests that Prince Andrew was asked if he ever had to have sex with... Prince Andrew...",
                  "score": 2,
                  "created_utc": "2026-01-07 02:12:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny1k7p9",
          "author": "forevergeeks",
          "text": "Why are you guys using AI to write comments? It makes the whole interaction soulless. \n\nI used the chatbot, and yes, the two Bills are in the files, Bill Clinton, and Bill Gates! üòú",
          "score": 0,
          "created_utc": "2026-01-06 17:39:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1lr0w",
              "author": "battlepi",
              "text": "So, tell me how you feel about the traitor and serial child rapist, Donald Trump?",
              "score": 5,
              "created_utc": "2026-01-06 17:46:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny1mk9z",
                  "author": "forevergeeks",
                  "text": "I feel great! he just got Nicolas Maduro out of Venezuela, and maybe he is going after the Colombian president next!!",
                  "score": -2,
                  "created_utc": "2026-01-06 17:49:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q38zoo",
      "title": "Built a Lovable with Deepagents",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q38zoo/built_a_lovable_with_deepagents/",
      "author": "Releow",
      "created_utc": "2026-01-03 23:01:18",
      "score": 22,
      "num_comments": 12,
      "upvote_ratio": 0.87,
      "text": "Hi guys, just wanted to share my project done used to deep dive into the deepagents architecture.\n\nIt is a little coding agent to build react app inspired by lovable.\n\n[https://github.com/emanueleielo/deepagents-open-lovable](https://github.com/emanueleielo/deepagents-open-lovable)\n\nAsking for feedback!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q38zoo/built_a_lovable_with_deepagents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nxjh686",
          "author": "hwchase17",
          "text": "This is great!",
          "score": 5,
          "created_utc": "2026-01-04 00:49:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlhfrh",
              "author": "Releow",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-01-04 08:57:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxk046x",
          "author": "vtrivedy-lc",
          "text": "Hey this is awesome!!  Also nice agent eng design with the subagent delegation.  Would love to hear how it went using deepagents and how we can make it better to help you build more cool stuff :). Will DM you!",
          "score": 3,
          "created_utc": "2026-01-04 02:33:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkddnm",
          "author": "Feisty-Promise-78",
          "text": "Great work! Did you come up with the prompt as by yourself or ask AI to generate it or you copy pasted from some else?",
          "score": 2,
          "created_utc": "2026-01-04 03:49:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlhbrz",
              "author": "Releow",
              "text": "I‚Äôve been inspired from claude code frontend design skill",
              "score": 1,
              "created_utc": "2026-01-04 08:56:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxll5qg",
                  "author": "Feisty-Promise-78",
                  "text": "Can you share the link of it? I am now aware of it",
                  "score": 1,
                  "created_utc": "2026-01-04 09:31:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxkz6xb",
          "author": "monkeybrain_",
          "text": "Seems pretty interesting, will come back later to review in detail. Did you build the skills middleware yourself?",
          "score": 2,
          "created_utc": "2026-01-04 06:18:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlhxte",
              "author": "Releow",
              "text": "Yes, inspired by deepagents cli",
              "score": 2,
              "created_utc": "2026-01-04 09:02:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxl59pv",
          "author": "lundrog",
          "text": "Ill check this out",
          "score": 2,
          "created_utc": "2026-01-04 07:08:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlfd3h",
          "author": "ryan1257",
          "text": "I‚Äôm curious. Won‚Äôt your GitHub saved app simply be ingested by AI?",
          "score": 1,
          "created_utc": "2026-01-04 08:38:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pykyuj",
      "title": "Are agent evals the new unit tests?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1pykyuj/are_agent_evals_the_new_unit_tests/",
      "author": "Hot-Guide-4464",
      "created_utc": "2025-12-29 12:23:16",
      "score": 22,
      "num_comments": 14,
      "upvote_ratio": 0.8,
      "text": "I‚Äôve been thinking about this a lot as agent workflows get more complex. Because in software, we‚Äôd never ship anything without unit tests. But right now most people just ‚Äútry a few prompts‚Äù and call it good. That clearly doesn‚Äôt scale once you have agents doing workflow automation or anything that has a real failure cost.\n\nSo I‚Äôm wondering if we‚Äôre moving to a future where CI-style evals become a standard part of building and deploying agents? Or am I overthinking it and we‚Äôre still too early for something this structured? I‚Äôd appreciate any insights on how folks in this community are running evals without drowning in infra.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1pykyuj/are_agent_evals_the_new_unit_tests/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nwjsa3j",
          "author": "Kortopi-98",
          "text": "I think evals have to become the new unit tests because once an agent interacts with real data or systems, \"vibes-based QA\" becomes a liability. So we‚Äôve been moving towards lightweight CI-like evals for our internal agents. Nothing super formal, just a set of representative tasks and expected behaviors. Just so you know, setting up the infra for this sucks unless you build your own harness. We switched to Moyai because they make this a lot less painful. Their eval workflow is basically: define agent, run them across diverse tasks, get diffs or outliers, done.",
          "score": 10,
          "created_utc": "2025-12-29 14:28:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjyb12",
              "author": "Hot-Guide-4464",
              "text": "Are you testing reasoning chains, final outputs or both?",
              "score": 2,
              "created_utc": "2025-12-29 15:00:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwk4dqi",
                  "author": "Kortopi-98",
                  "text": "Both. You can check the final result (e.g. reasoning steps, metrics pulled) but also the intermediate reasoning if you want consistency across steps. We treat it almost like snapshot tests for LLMs.",
                  "score": 2,
                  "created_utc": "2025-12-29 15:31:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwjhors",
          "author": "imnotafanofit",
          "text": "We started doing mini regression suite for agents. They're fast and lightweight but the biggest challenge is infra though. Spinning up evals can get expensive if you run them often.",
          "score": 6,
          "created_utc": "2025-12-29 13:25:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjnm9b",
              "author": "Hot-Guide-4464",
              "text": "Yep, infra costs are kind of an underrated part of this conversation. How are you managing overhead?",
              "score": 3,
              "created_utc": "2025-12-29 14:01:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwj9wzh",
          "author": "charlyAtWork2",
          "text": "Most of the time is only ETL.\n\nA boring step by step transformation with LLM in the middle.",
          "score": 4,
          "created_utc": "2025-12-29 12:31:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwld9em",
          "author": "MathematicianSome289",
          "text": "See em as more integration tests than unit tests. Evals test how the pieces work together. Units test the individual pieces.",
          "score": 4,
          "created_utc": "2025-12-29 19:01:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlo4uu",
          "author": "hidai25",
          "text": "I‚Äôm mostly with you, but I don‚Äôt think it maps 1:1 to unit tests. For agents it feels more like integration/regression tests, because the ‚Äúoutput string‚Äù is the least stable thing in the system. What *is* stable is behavior: did it call the right tools, avoid the wrong ones, return valid structure, and stay within time/$ budgets.\n\nThe only way I‚Äôve seen this not turn into eval-infra hell is keeping a small ‚Äúthis can‚Äôt break‚Äù suite in CI, running the bigger flaky stuff nightly, and turning every real failure into a test case. That‚Äôs when it starts compounding like real testing.\n\nFull disclosure: I‚Äôm building an OSS harness around exactly this idea (EvalView). If it‚Äôs useful, it‚Äôs here: [https://github.com/hidai25/eval-view]()",
          "score": 2,
          "created_utc": "2025-12-29 19:54:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa9f75",
          "author": "No-Common1466",
          "text": "Absolutely spot on ‚Äì agent evals are becoming the new unit tests (or more accurately, the new integration/regression suite) for anything that's going to touch production.\n\nTraditional software has deterministic functions, so unit tests with exact assertions work great. Agents? They're non-deterministic, multi-turn, tool-using beasts that can go off the rails in infinite creative ways. \"Try a few prompts\" catches the obvious hallucinations, but it won't save you when a real user phrashes something weirdly, injects noise, or triggers an edge case in a 20-step workflow.\n\nFrom LangChain's own State of Agent Engineering report (late 2025 survey): observability is basically table stakes now (\\~89% adoption), but offline evals on test sets are only at \\~52%. That gap is closing fast though ‚Äì teams shipping real-stakes agents (workflow automation, customer-facing, etc.) are treating evals as non-negotiable regression gates in CI/CD, just like we do with code.\n\nLangSmith/LangGraph is pushing hard here with multi-turn evals, trajectory evaluators, open evals catalog, and even running evals directly in Studio. Other tools (Braintrust, Promptfoo, etc.) are making it easy to fail builds on dropping robustness scores.\n\nThe missing piece a lot of folks run into: most evals focus on correctness (did it get the right answer?), but in production the bigger killer is robustness (does it still work when the input is sloppy, paraphrased, noisy, or adversarial?). That's where adversarial stress-testing comes in ‚Äì mutate prompts automatically and enforce invariants to quantify how \"flaky\" your agent really is.\n\nWe're still early-ish, but the direction is clear: no serious agent ships without automated evals in the pipeline. Curious ‚Äì what tools/workflows are you all using today to avoid drowning in manual testing?\n\n(Full disclosure: I'm building an open-source tool called Flakestorm exactly for the robustness side ‚Äì local-first adversarial mutations + reports. Early days, would love feedback if anyone wants to kick the tires. LInk here: https://github.com/flakestorm/flakestorm)",
          "score": 2,
          "created_utc": "2026-01-02 17:20:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhh1zj",
              "author": "Born_Owl7750",
              "text": "Interesting idea. Do you plan to support hosted model APIs from Azure or Open AI?",
              "score": 1,
              "created_utc": "2026-01-03 18:49:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxjo8vs",
                  "author": "No-Common1466",
                  "text": "Hi, yes but that would be on the cloud version. Still on the roadmap, to see if there's demand. There's a waitlist page on the website for those who are interested. I'll build it there's enough traction",
                  "score": 1,
                  "created_utc": "2026-01-04 01:28:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwm58wp",
          "author": "piyaviraj",
          "text": "We use evals as agent dev testing tool as a part of what we call agent development life cycle. Since it is part of the dev testing, every changes like prompt changes, tool changes, memory schema changes, etc are covered during the dev testing for the agents and their orchestration. This will ensure changes will not break the logic(reasoning) assumptions and as well as integration assumptions. However, we do not configure eval evaluation in a regular build CI or local build because for large project the token economics will be hard to justify at scale.",
          "score": 1,
          "created_utc": "2025-12-29 21:17:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx06uuq",
          "author": "Severe_Insurance_861",
          "text": "Within my team we call them regression eval. 400 examples covering a variety of scenarios.",
          "score": 1,
          "created_utc": "2026-01-01 00:28:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0znnd",
          "author": "Fit-Presentation-591",
          "text": "I‚Äôd say they‚Äôre more analogous to a mix of CI/CD and production testing TBH. They‚Äôre a bit more complex than your average unit test IME.",
          "score": 1,
          "created_utc": "2026-01-01 03:37:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4rm66",
      "title": "I Finished a Fully Local Agentic RAG Tutorial",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q4rm66/i_finished_a_fully_local_agentic_rag_tutorial/",
      "author": "CapitalShake3085",
      "created_utc": "2026-01-05 17:15:28",
      "score": 21,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hi,\nI‚Äôve just finished a **complete Agentic RAG tutorial + repository** that shows how to build a fully local, end-to-end system.\n\nNo APIs, no cloud, no hidden costs.\n\n---\n\n### üí° What‚Äôs inside\n\nThe tutorial covers the full pipeline, including the parts most examples skip:\n\n- PDF ‚Üí Markdown ingestion  \n- Hierarchical chunking (parent / child)  \n- Hybrid retrieval (dense + sparse)  \n- Vector store with **Qdrant**  \n- Query rewriting + **human-in-the-loop**  \n- Context summarization  \n- **Multi-agent map-reduce** with **LangGraph**  \n- Local inference with **Ollama**  \n- Simple **Gradio** UI\n\n---\n\n### üéØ Who it‚Äôs for\n\nIf you want to **understand Agentic RAG by building it**, not just reading theory, this might help.\n\n---\n\n### üîó Repo\n\n https://github.com/GiovanniPasq/agentic-rag-for-dummies",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1q4rm66/i_finished_a_fully_local_agentic_rag_tutorial/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q7k1qb",
      "title": "I built an agent to triage production alerts",
      "subreddit": "LangChain",
      "url": "https://i.redd.it/99cxiaic76cg1.png",
      "author": "Arindam_200",
      "created_utc": "2026-01-08 18:41:10",
      "score": 20,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1q7k1qb/i_built_an_agent_to_triage_production_alerts/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q23mny",
      "title": "I mutation-tested my LangChain agent and it failed in ways evals didn‚Äôt catch",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q23mny/i_mutationtested_my_langchain_agent_and_it_failed/",
      "author": "No-Common1466",
      "created_utc": "2026-01-02 16:48:05",
      "score": 19,
      "num_comments": 4,
      "upvote_ratio": 0.95,
      "text": "I‚Äôve been working on an agent that passed all its evals and manual tests.\n\nOut of curiosity, I ran it through mutation testing  small changes like:\n\n\\- typos\n\n\\- formatting changes\n\n\\- tone shifts\n\n\\- mild prompt injection attempts\n\n\n\nIt broke. Repeatedly.\n\n\n\nSome examples:\n\n\\- Agent ignored tool constraints under minor wording changes\n\n\\- Safety logic failed when context order changed\n\n\\- Agent hallucinated actions it never took before\n\n\n\nI built a small open-source tool to automate this kind of testing (Flakestorm).\n\nIt generates adversarial mutations and runs them against your agent.\n\n\n\nI put together a minimal reproducible example here:\n\n GitHub repo:  [https://github.com/flakestorm/flakestorm](https://github.com/flakestorm/flakestorm)\n\nExample: [https://github.com/flakestorm/flakestorm/tree/main/examples/langchain\\_agent](https://github.com/flakestorm/flakestorm/tree/main/examples/langchain_agent)\n\n\n\nYou can reproduce the failure locally in \\~10 minutes:\n\n\\- pip install\n\n\\- run one command\n\n\\- see the report\n\n\n\nThis is very early and rough - I‚Äôm mostly looking for:\n\n\\- feedback on whether this is useful\n\n\\- what kinds of failures you‚Äôve seen but couldn‚Äôt test for\n\n\\- whether mutation testing belongs in agent workflows at all\n\n\n\nNot selling anything. Genuinely curious if others hit the same issues.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q23mny/i_mutationtested_my_langchain_agent_and_it_failed/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nxat6a4",
          "author": "Reasonable-Life7326",
          "text": "Of, that's rough. Glad you found it though!",
          "score": 1,
          "created_utc": "2026-01-02 18:51:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxcs9hd",
              "author": "No-Common1466",
              "text": "Yeah",
              "score": 1,
              "created_utc": "2026-01-03 00:51:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxd5g1d",
          "author": "erikg1337",
          "text": "Flakestorm looks very interesting..",
          "score": 1,
          "created_utc": "2026-01-03 02:07:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxd5tbp",
              "author": "No-Common1466",
              "text": "Thanks. it started as a way to catch failures in my own agents that evals missed.\nStill early, but I‚Äôm curious what others are using (or missing) today",
              "score": 1,
              "created_utc": "2026-01-03 02:09:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qb2vle",
      "title": "Learning RAG + LangChain: What should I learn first?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qb2vle/learning_rag_langchain_what_should_i_learn_first/",
      "author": "Cobra_venom12",
      "created_utc": "2026-01-12 18:29:03",
      "score": 19,
      "num_comments": 15,
      "upvote_ratio": 0.95,
      "text": "I'm a dev looking to get into RAG. There's a lot of noise out there‚Äîshould I start by learning:\n‚ÄãVector Databases / Embeddings?\n‚ÄãLangChain Expression Language (LCEL)?\n‚ÄãPrompt Engineering?\n‚ÄãWould love any recommendations for a \"from scratch\" guide that isn't just a 10-minute YouTube video. What's the best \"deep dive\" resource available right now?",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qb2vle/learning_rag_langchain_what_should_i_learn_first/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nz7rnwc",
          "author": "cmndr_spanky",
          "text": "Install a vector DB like ChromaDB (a great and easy vectorDB solution), and with its python library learn how to store documents, the chromaDB instructions explain how to use different embeddings models to do that.\n\nJust with ChromaDB docs learn how to query the database. All very straight forward.\n\nThen \"RAG\" just means write a program that uses an LLM to answer user questions, and the LLM is able to pass queries to the chromaDB to get some extra context (text chunks from whatever source material) that the LLM can use to better answer the user's questions. If you find some sample code online it'll teach you enough about \"prompt engineering\" to get by.\n\nTHAT'S IT.\n\nAll langchain does is provide some python abstractions to wire-up an LLM to an application that chats to users and connects to vector DB collections (and has many other features of course).. but honestly I find Langchain often adds more complexity than takes away complexity. I don't find their abstractions particularly intuitive.. but they work of course. My advice is start without langchain.. learn how to query an LLM directly using a basic library (from hugging face or the plain OpenAI APIs) and then augment the LLM responses using results from your VDB. That's it.\n\nBonus sentiment:\n\nI would just rush to learn how to make a full and basic RAG app from scratch rather than meander around many many courses on \"WHATS AN LLM\" \"HOW TO VECTOR DB\" \"HOW TO MAKE RAG\" blah blah blah.. there's so much noise out there now and so many frameworks and marketing jargon because everyone is trying to earn their little gold nugget from the trends surrounding LLMs right now. Don't get distracted by the bullshit.. just make sure you know how to code basic python and the rest is just wiring together a few concepts. start by using as few libraries and frameworks as possible.\n\nIf you don't understand anything I'm saying in this comment, paste it into chatGPT (thinking mode) and ask it to make you a curriculum based on this comment.",
          "score": 11,
          "created_utc": "2026-01-12 19:22:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz7vfsp",
              "author": "Cobra_venom12",
              "text": "Thanks mate",
              "score": 1,
              "created_utc": "2026-01-12 19:40:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz7wd6k",
                  "author": "cmndr_spanky",
                  "text": "np! also forgot to mention, you don't really need to \"install\" chromaDB, you can use its python library in such a way that it just stores the raw db data locally on disk directly from python. Which is a great way to learn it and doesn't require installing a separate server",
                  "score": 1,
                  "created_utc": "2026-01-12 19:44:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz7w9rd",
              "author": "Cobra_venom12",
              "text": "What about embeddings and transformers etc",
              "score": 1,
              "created_utc": "2026-01-12 19:44:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz80gpk",
                  "author": "cmndr_spanky",
                  "text": "An embeddings model (sometimes called a Sentence transformer) is a small model that converts text chunks into a bunch of vectors that gets stored along with the raw texts chunks into the VectorDB records. Those vectors represent the topics related to that text chunk and those vectors are called \"embeddings\" which is a shitty term but sadly we rely on engineers to make up these terms rather than normal people.\n\nChromaDB will by default use a built-in embeddings model to do this for you, or you can use a different one, again check the chromaDB docs / getting start instructions it'll explain how to do this.\n\n\"Transformer\" is the industry term for the neural net architecture that powers large language models, people often use these terms as synonyms: Transformer model = large language model.\n\nAn example popular embeddings model is: sentence-transformers/all-MiniLM-L12-v2 but there are many more you can find here [https://huggingface.co/sentence-transformers/models](https://huggingface.co/sentence-transformers/models)\n\nMy personal favorite is sentence-transformers/all-mpnet-base-v2\n\nan example regular LLM you'd use to power the whole chat bot is chatGPT or similar ones.\n\nBoth embeddings models and \"regular\" LLMs are technically Transformer models, so don't be confused when you see that term used everywhere.\n\nAlso, the official hugging face library for using LLMs from hugging face is called the Transformers library (sorry..).\n\nfor some extra reading hugging face has a nice little article about embeddings: [https://huggingface.co/blog/getting-started-with-embeddings](https://huggingface.co/blog/getting-started-with-embeddings)",
                  "score": 1,
                  "created_utc": "2026-01-12 20:03:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz7ksf2",
          "author": "Valeria_Xenakis",
          "text": "Whatever you do later first start from a 1 hr coursera free course by andrew ng and harrison chase (founder of langchain) called Langchain Chat with your Data. Will give you a good idea of what it is all about and how to delve into it further.",
          "score": 5,
          "created_utc": "2026-01-12 18:51:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz7vgui",
              "author": "Cobra_venom12",
              "text": "Sure",
              "score": 1,
              "created_utc": "2026-01-12 19:40:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz8gw4c",
                  "author": "_blkout",
                  "text": "literally just clone the langraph sample project and look through the llamaindex integration docs, that‚Äôs basically all you need to know. if you already understand how both of these concepts work they literally just plug and play. it‚Äôs how you orchestrate them that matters",
                  "score": 1,
                  "created_utc": "2026-01-12 21:20:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzba3uv",
          "author": "OnyxProyectoUno",
          "text": "Start with embeddings and chunking before touching LangChain. Seriously. Most RAG failures trace back to how documents get split and embedded, not the orchestration layer. If your chunks are garbage, no amount of LCEL elegance saves you.\n\nThe learning order that actually sticks: first understand what embeddings represent and why chunk boundaries matter. Then mess with a vector store directly, no framework. Pinecone or Qdrant have decent free tiers. Only after you've felt the pain of bad retrieval should you layer in LangChain.\n\nFor deep dives, the Pinecone learning center covers embeddings and similarity search well. LlamaIndex docs explain chunking strategies better than most resources. And honestly, the best teacher is building something small, uploading a few PDFs, and watching what comes back when you query. You'll learn more debugging why your retriever missed an obvious answer than from any course.\n\nPrompt engineering matters less than people think at the RAG stage. Get your retrieval solid first.",
          "score": 2,
          "created_utc": "2026-01-13 07:00:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkh94v",
          "author": "pbalIII",
          "text": "Build a tiny RAG app end to end first, without any framework abstractions. Once you can explain each knob, LangChain and LCEL stop feeling like a maze.\n\n1) Chunk + store text with metadata\n2) Retrieve, then rerank, keep the top chunks\n3) Add an eval set and track answer relevance and faithfulness\n\nAfter that, pick LCEL for simple chains and LangGraph when you need real state.",
          "score": 2,
          "created_utc": "2026-01-14 16:59:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzw41k9",
          "author": "Sheikhfahad67",
          "text": "I think you should start from the basics such as agentic ai. Create agents using LangChain and LangGraph for creating React or Deep agents. After that move towards other things such as RAG and so on..",
          "score": 2,
          "created_utc": "2026-01-16 09:22:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz9zg37",
          "author": "d0r1h",
          "text": "I would recommend if you're getting started, you should start with this \n\nRAG From Scratch by Langchain Engineer itself - [https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1\\_PKyVJiQ23ZztA0x](https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x) \n\n  \nExplanation is pretty good and you'll get solid understanding of ecosystem, how things works in RAG system from basic to optimization. \n\nAlso in the lectures they are using OpenAPI for chat and embedding models, but if you want to use opensource models from HuggingFace, you can follow my codes, I've implemented same things using open models, they cost less compare to openAPI (Their documentation is outdated, won't get any help from there if you want to use other models). \n\n[https://github.com/d0r1h/Learn-AI/tree/main/Agentic\\_AI/RAG/Learning\\_RAG](https://github.com/d0r1h/Learn-AI/tree/main/Agentic_AI/RAG/Learning_RAG) \n\nI'm also following same playlist. Thanks!",
          "score": 1,
          "created_utc": "2026-01-13 02:02:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzb755k",
          "author": "Subject-Complex6934",
          "text": "vector databases and RAG is explained here nicely  \n[https://www.youtube.com/@PavelCermakAI/videos](https://www.youtube.com/@PavelCermakAI/videos)",
          "score": 1,
          "created_utc": "2026-01-13 06:34:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5gfsx",
      "title": "So I've been losing my mind over document extraction in insurance for the past few years and I finally figured out what the right approach is.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q5gfsx/so_ive_been_losing_my_mind_over_document/",
      "author": "GloomyEquipment2120",
      "created_utc": "2026-01-06 11:46:32",
      "score": 17,
      "num_comments": 5,
      "upvote_ratio": 0.84,
      "text": "I've been doing document extraction for insurance for a while now and honestly I almost gave up on it completely last year. Spent months fighting with accuracy issues that made no sense until I figured out what I was doing wrong.\n\neveryone's using llms or tools like LlamaParse for extraction and they work fine but then you put them in an actual production env and accuracy just falls off a cliff after a few weeks. I kept thinking I picked the wrong tools or tried to brute force my way through (Like any distinguished engineer would do XD) but it turned out to be way simpler and way more annoying.\n\nSo if you ever worked in an information extraction project you already know that most documents have literally zero consistency. I don't mean like \"oh the formatting is slightly different\" , I mean every single document is structured completely differently than all the others.\n\nFor example in my case : a workers comp FROI from California puts the injury date in a specific box at the top. Texas puts it in a table halfway down. New York embeds it in a paragraph. Then you get medical bills where one provider uses line items, another uses narrative format, another has this weird hybrid table thing. And that's before you even get to the faxed-sideways handwritten nightmares that somehow still exist in 2026???\n\nSadly llms  have no concept of document structure. So when you ask about details in a doc  it might pull from the right field, or from some random sentence, or just make something up. \n\nAfter a lot of headaches and honestly almost giving up completely, I came across a process that might save you some pain, so I thought I'd share it:\n\n1. Stop throwing documents at your extraction model blind. Build a classifier that figures out document type first (FROI vs medical bill vs correspondence vs whatever). Then route to type specific extraction. This alone fixed like 60% of my accuracy problems. (Really This is the golden tip ... a lot of people under estimate classification)\n\n2.  Don't just extract and hope. Get confidence scores for each field. \"I'm 96% sure this is the injury date, 58% sure on this wage calc\" Auto-process anything above 90%, flag the rest. This is how you actually scale without hiring people to validate everything AI does.\n\n3. Layout matters more than you think. Vision-language models that actually see the document structure perform way better than text only approaches. I switched to Qwen2.5-VL and it was night and day.\n\n4. Fine-tune on your actual documents. Generic models choke on industry-specific stuff. Fine-tuning with LoRA takes like 3 hours now and accuracy jumps 15-20%. Worth it every time.\n\n5. When a human corrects an extraction, feed that back into training. Your model should get better over time. (This will save you the struggle of having to recreate your process from scratch each time)\n\nWrote a little blog with more details about this implementation if anyone wants it \"I know... Shameless self promotion). ( link in comments)  \n  \nAnyway this is all the stuff I wish someone had told me when I was starting. Happy to share or just answer questions if you're stuck on this problem. Took me way too long to figure this out.\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q5gfsx/so_ive_been_losing_my_mind_over_document/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nxzq4wy",
          "author": "GloomyEquipment2120",
          "text": "Link to write up : [https://kudra.ai/how-agentic-document-intelligence-transformed-workers-compensation-claims-processing-for-insurance-companies/](https://kudra.ai/how-agentic-document-intelligence-transformed-workers-compensation-claims-processing-for-insurance-companies/)",
          "score": 3,
          "created_utc": "2026-01-06 11:49:20",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "ny01ng0",
          "author": "Mammoth-Nectarine513",
          "text": "Thanks for sharing! \n\nDo you mind sharing details about fine tuning like how did you do? Where did you learn from?",
          "score": 1,
          "created_utc": "2026-01-06 13:09:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny02ixe",
              "author": "GloomyEquipment2120",
              "text": "i studied ai engineering but honestly finetuning is getting way easier with existing tools now. [kudra.ai](http://kudra.ai) and [ubiai.tools](http://ubiai.tools) are what i use in my personal tech stack.\n\nthey both have tutorials on youtube and integration is pretty straightforward.\n\nif you want more details on finetuning i'd recommend getting the theory down first. start with this guide [https://ubiai.gitbook.io/llm-guide](https://ubiai.gitbook.io/llm-guide) then maybe grab some coursera courses if you want to go deeper.\n\nbut honestly for most practical stuff the tools handle a lot of the complexity now. you just need to understand the basics",
              "score": 1,
              "created_utc": "2026-01-06 13:14:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny04m9m",
                  "author": "Mammoth-Nectarine513",
                  "text": "I will go through this for sure. Moreover, Do I need to learn deep learning as well?",
                  "score": 1,
                  "created_utc": "2026-01-06 13:27:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qdufx4",
      "title": "Honest question: What is currently the \"Gold Standard\" framework for building General Agents?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qdufx4/honest_question_what_is_currently_the_gold/",
      "author": "Strong_Cherry6762",
      "created_utc": "2026-01-15 20:14:21",
      "score": 17,
      "num_comments": 28,
      "upvote_ratio": 0.84,
      "text": "Hi everyone,\n\nI'm a beginner developer diving into AI agents. My goal is to build a solid General Agent, but I want to make sure I start with the right tools.\n\nI keep hearing about LangGraph, but before I commit to learning it, I really want to know what the community considers the actual \"best\" framework right now.\n\nHere is what I‚Äôm hoping to learn from your experience:\n\n1. The #1 Recommendation: If you were starting a new project today, which framework would you choose and why? Is there a clear winner?\n2. LangGraph Reality Check: Is LangGraph truly the best option for a general-purpose agent, or is it overkill/too complex for a starter? What are its main pros and cons?\n3. General Best Practices: Regardless of the framework, what are the most important principles for building a stable agent?\n\nI‚Äôm looking for a solution that balances power with ease of use. Thanks for pointing me in the right direction!",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qdufx4/honest_question_what_is_currently_the_gold/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nzt57if",
          "author": "caprica71",
          "text": "Honestly the framework just disappears from thought most of the time once you get arms deep into a problem.   Most of my life is in evals and integrations.  Just pick a really popular one and get going.   If the agent frame work gets in the way swap it.",
          "score": 18,
          "created_utc": "2026-01-15 22:01:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztqakt",
              "author": "scar0x00",
              "text": "What do you use for evals?",
              "score": 4,
              "created_utc": "2026-01-15 23:49:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o017tve",
                  "author": "caprica71",
                  "text": "Langfuse for the some parts.   I have some custom python for other evals.",
                  "score": 1,
                  "created_utc": "2026-01-17 01:52:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o00uses",
              "author": "AdditionalWeb107",
              "text": "how do you easily swap it when you have gone down the rabbit hotel of its concepts? Like CrewAI is so hard to unpack and move into LangChain. I would prefer we go stock python for most things and think through clean separation of concerns.",
              "score": 1,
              "created_utc": "2026-01-17 00:31:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o016e6d",
                  "author": "caprica71",
                  "text": "Basically you nailed it : use stock python as much as possible and good separation of concerns.   My langraph application has very light nodes. I use langchain sparingly.",
                  "score": 1,
                  "created_utc": "2026-01-17 01:42:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzt0mn2",
          "author": "cordialgerm",
          "text": "Look into Langchain 1.0. you can build a basic agent harness with  a single function call. Langgraph is more for complex / low level use-cases.\n\nI think it's less about the particular framework and more about the  tools, evals, data, etc that you inject into the framework that makes or breaks your agent.",
          "score": 6,
          "created_utc": "2026-01-15 21:40:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzupspl",
              "author": "brucebay",
              "text": "I suggest langgraph. It is easier and in most cases more flexible.¬†",
              "score": 2,
              "created_utc": "2026-01-16 03:06:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzt11hq",
              "author": "Strong_Cherry6762",
              "text": "Thanks~",
              "score": 1,
              "created_utc": "2026-01-15 21:42:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzuplr1",
          "author": "dash_bro",
          "text": "If you're busy starting out, learn langchain/langgraph. They're okay okay to learn, but hard to deploy at scale in production : so just keep that in mind. \n\n- learn what you want to do first\n- you'll maybe realize it isn't the best fit and want to check out what other library offers these features etc.\n\nFWIW I started with autogen -> crew -> now I just do it custom myself",
          "score": 2,
          "created_utc": "2026-01-16 03:04:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw7xba",
              "author": "Born_Owl7750",
              "text": "Why is it hard to deploy in production? What issues have you faced?\nWe currently are in the Microsoft ecosystem and use semantic kernel or native sdk. I was thinking of switching to langchain and langgraph due to the sheer amount of features and updates they have.",
              "score": 1,
              "created_utc": "2026-01-16 09:58:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzwf0dr",
                  "author": "dash_bro",
                  "text": "Biggest problem is bloat and noQA features. The bloat especially throws random problems when you're hosting multiple embedding models + serving to a few users live.\n\nMemory management (RAM/VRAM) harnesses are a nightmare. Bloat around using external integrations without plugging in langchain specific adapters - again, creates non DRY work. The ecosystem is good for when flexibility isn't required fully.\n\nIt's often much better to experiment with, then build for specced out features yourself.",
                  "score": 1,
                  "created_utc": "2026-01-16 11:01:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvbz8a",
          "author": "Playful_Criticism425",
          "text": "People are sleeping on PydanticAI",
          "score": 2,
          "created_utc": "2026-01-16 05:25:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw5xm8",
              "author": "FlowLab99",
              "text": "Do you mean that Pydantic team is not moving quickly or responsive? Just curious m.",
              "score": 1,
              "created_utc": "2026-01-16 09:40:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0231my",
                  "author": "Playful_Criticism425",
                  "text": "It's actually good. For agentic solutions and it is Pythonic and    more deterministic than n8n",
                  "score": 1,
                  "created_utc": "2026-01-17 05:20:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzuef3v",
          "author": "attn-transformer",
          "text": "Honestly none of them. They‚Äôre all experiments. I‚Äôd start with Agno or LangGraph.",
          "score": 4,
          "created_utc": "2026-01-16 02:02:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvz87k",
              "author": "nbvehrfr",
              "text": "Agno is good.¬†",
              "score": 2,
              "created_utc": "2026-01-16 08:37:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzv23w0",
          "author": "Khade_G",
          "text": "There isn‚Äôt a single best framework right now‚Ä¶ the best choice depends on how complex your agent actually needs to be.\n\nIf I were starting today, I‚Äôd pick the simplest thing that can ship, and only upgrade when I feel pain. For most beginners, that‚Äôs a lightweight agent loop (tools + memory + logging) using whatever SDK you‚Äôre already comfortable with. LangChain can get you moving fast. LangGraph is great when you need control: multi-step workflows, branching, retries, and state you can inspect. But for a ‚Äúgeneral agent‚Äù as a first project, it can feel like overkill because you end up building a workflow engine before you‚Äôve proven the agent needs one.\n\nLangGraph pros: more reliable orchestration, easier debugging of state, safer retries, and it scales better once you have multiple steps/roles. Cons: more structure, more upfront thinking, and it‚Äôs slower to iterate if you‚Äôre still learning what your agent should even do.\n\nBest practices that matter more than framework: keep context small and explicit, log everything (inputs/tool calls/outputs), design for failure (timeouts, retries, fallbacks), and don‚Äôt let the agent ‚Äúfree roam‚Äù‚Ä¶ give it clear goals, constraints, and stopping conditions. If you do those, your agent will be stable in almost any framework.",
          "score": 3,
          "created_utc": "2026-01-16 04:19:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzubzmd",
          "author": "siddharthnibjiya",
          "text": "Claude code is now considered the best agentic structure / style to be followed.\n\nRelated but not my only reason for saying this:\n\nhttps://x.com/amasad/status/2011475533369131424?s=46",
          "score": 2,
          "created_utc": "2026-01-16 01:49:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztdbp2",
          "author": "Challseus",
          "text": "Apologies in advance, this may or may not help your cause, but... I worked on a multi-agent setup with Lang Graph close to a year ago. There was the one **Primary Agent** that would pass off the request to other agents (with their own instructions), depending on some business logic. 8-9 agents total. Image analysis, tools, RAG, everything. It worked. It had many bugs, but that was a skill issue thing, not Lang Graph's fault.\n\nWe used **gpt-4o.**\n\nSame company, CEO now wants this logic back in a new product. It's basically the same thing, except one major thing:\n\nNo Lang Graph, just Langchain. Just a single agent, one prompt, and basic conversation history. It handles everything MUCH BETTER.\n\nWe're using **gpt-4.1-mini.**\n\nLong story short, TL;DR, whatever, just go with **Langchain 1.0.** It has great support for built in RAG pipeline stuff to get you started, as well as conversation history.\n\nAlso checkout [https://langfuse.com/](https://langfuse.com/) or [https://smith.langchain.com/](https://smith.langchain.com/) for observability.",
          "score": 1,
          "created_utc": "2026-01-15 22:41:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzte0y5",
          "author": "Bohdanowicz",
          "text": "It depends what you are building.. lang is great when you need a stategraph, adk is great for a2a/subagents out of the box",
          "score": 1,
          "created_utc": "2026-01-15 22:44:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvplkv",
          "author": "Ambitious-Most4485",
          "text": "We are using in production google adk, but langgraph was an option we did consider.\n\nAdk has some flaws (for example we needed to log chunks and was a nightmare navigating the state).\n\n Observability is a must i recommend Phoenix arize (really easy integration)",
          "score": 1,
          "created_utc": "2026-01-16 07:12:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvwgel",
          "author": "KallistiTMP",
          "text": "It's still the wild west out there. All agentic frameworks suck in several dimensions, and often suck differently with different models and tasks. There is no consensus on what the best overall high level approach is even, it's an active area of research.\n\nDon't overthink it, any choice you make is gonna be wrong and require a major overhaul or full replacement in a couple years anyway.",
          "score": 1,
          "created_utc": "2026-01-16 08:12:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzw1kub",
          "author": "Disneyskidney",
          "text": "I went to hackathon for something like this. Both first place, second place, and third place all used Claude code. It‚Äôs surprising how general coding agents are becoming. \n\nI actually just used cursor for lead generation lol.",
          "score": 1,
          "created_utc": "2026-01-16 08:59:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzw1r29",
          "author": "Disneyskidney",
          "text": "Check out DSPy and RLMs",
          "score": 1,
          "created_utc": "2026-01-16 09:01:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwvtia",
          "author": "No-Fail-7644",
          "text": "Plain, old java. I found vertx with quarkus running on java 21 to be extremely versatile if reactive patterns are implemented properly. Langchain4j for agents.",
          "score": 1,
          "created_utc": "2026-01-16 13:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o008sg8",
          "author": "Strong_Worker4090",
          "text": "No real ‚Äúgold standard‚Äù yet. Agents are still kinda messy, and most people end up with a thin framework + their own glue.\n\nIf I‚Äôm starting today:\n\n1. LangGraph if you expect multi-step flows, branching, retries, human-in-the-loop. I moved a project from a messy ‚Äúagent loop‚Äù to LangGraph and it got way easier to debug. Downside: more setup, can feel overkill early.\n2. If you want simpler: plain LangChain (or even just your own wrapper) is fine for v1.\n3. If your agent is mostly RAG: LlamaIndex or Haystack.\n4. If you‚Äôre doing ‚Äúmultiple agents with roles‚Äù: AutoGen or CrewAI.\n\nBiggest mistake I made early was chasing frameworks instead of basics. What actually made things stable was: fewer tools, hard timeouts, good logs, and input/output checks (guardrails). Framework choice matters way less than that.",
          "score": 1,
          "created_utc": "2026-01-16 22:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzulj4e",
          "author": "_eltigre_",
          "text": "Mastra.",
          "score": 1,
          "created_utc": "2026-01-16 02:42:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztu53d",
          "author": "naxmax2019",
          "text": "Don‚Äôt use frameworks. They suck!",
          "score": -1,
          "created_utc": "2026-01-16 00:10:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbru7z",
      "title": "Open Source Enterprise Search Engine (Generative AI Powered)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qbru7z/open_source_enterprise_search_engine_generative/",
      "author": "Effective-Ad2060",
      "created_utc": "2026-01-13 13:51:44",
      "score": 16,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hey everyone!\n\nI‚Äôm excited to share something we‚Äôve been building for the past 6 months, a¬†**fully open-source Enterprise Search Platform**¬†designed to bring powerful Enterprise Search to every team, without vendor lock-in. The platform brings all your business data together and makes it searchable. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, Local file uploads and more. You can deploy it and run it with just one docker compose command.\n\nYou can run the full platform locally. Recently, one of our users tried¬†**qwen3-vl:8b (16 FP)**¬†with¬†**Ollama**¬†and got very good results.\n\nThe entire system is built on a¬†**fully event-streaming architecture powered by Kafka**, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data.\n\nAt the core, the system uses an **Agentic Graph RAG approach**, where retrieval is guided by an enterprise knowledge graph and reasoning agents. Instead of treating documents as flat text, agents reason over relationships between users, teams, entities, documents, and permissions, allowing more accurate, explainable, and permission-aware answers.\n\n**Key features**\n\n* Deep understanding of user, organization and teams with enterprise knowledge graph\n* Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama\n* Use any provider that supports OpenAI compatible endpoints\n* Choose from 1,000+ embedding models\n* Visual Citations for every answer\n* Vision-Language Models and OCR for visual or scanned docs\n* Login with Google, Microsoft, OAuth, or SSO\n* Rich REST APIs for developers\n* All major file types support including pdfs with images, diagrams and charts\n* Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more\n* Reasoning Agent that plans before executing tasks\n* 40+ Connectors allowing you to connect to your entire business apps\n\nCheck it out and share your thoughts or feedback. Your feedback is immensely valuable and is much appreciated:  \n[https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)\n\nDemo Video:  \n[https://www.youtube.com/watch?v=xA9m3pwOgz8](https://www.youtube.com/watch?v=xA9m3pwOgz8)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qbru7z/open_source_enterprise_search_engine_generative/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q4qamq",
      "title": "Anyone monitoring their LangChain/LangGraph workflows in production?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q4qamq/anyone_monitoring_their_langchainlanggraph/",
      "author": "gkarthi280",
      "created_utc": "2026-01-05 16:28:37",
      "score": 16,
      "num_comments": 15,
      "upvote_ratio": 0.95,
      "text": "I‚Äôve been building a few apps using LangChain, and once things moved beyond simple chains, I ran into a familiar issue: very little visibility into what‚Äôs actually happening during execution.\n\nAs workflows get more complex (multi-step chains, agents, tool calls, retries), it gets hard to answer questions like:\n\n* Where is latency coming from?\n* How many tokens are we using per chain or user?\n* Which tools, chains, or agents are invoked most?\n* Where do errors, retries, or partial failures happen?\n\nTo get better insight, I instrumented a LangChain-based app with OpenTelemetry, exporting traces, logs, and metrics to an OTEL-compatible backend (SigNoz in my case).\n\nhttps://preview.redd.it/c6iwa9vu4kbg1.png?width=2886&format=png&auto=webp&s=c8d260a9b50eac20b714d7e68d7028a4ce228dfc\n\nYou can use the traces, logs, and metrics to create useful dashboards as well which tracks things like:\n\n* Tool call distribution\n* Errors over time\n* Token usage & cost\n\nCurious how others here think about observability for LangChain apps:\n\n* What metrics or signals are you tracking?\n* How do you evaluate chain or agent output quality over time?\n* Are you monitoring failures or degraded runs?\n\nIf anyone‚Äôs interested, I followed the LangChain + OpenTelemetry setup here:  \n[https://signoz.io/docs/langchain-observability/](https://signoz.io/docs/langchain-observability/)\n\nWould love to hear how others are monitoring and debugging LangChain workflows in production.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q4qamq/anyone_monitoring_their_langchainlanggraph/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nxvagb7",
          "author": "pbalIII",
          "text": "LangSmith is the obvious choice if you're already in the LangChain ecosystem... one env var and you get full trace visibility with zero latency overhead. The async collector runs out of band so it doesn't slow your agent down.\n\nLangfuse is solid if you want something OSS or need to self-host. Works with LangGraph out of the box and gives you the same trace-level debugging.\n\nThe tricky part is figuring out what to actually monitor. Token costs and latency are easy. Catching when your agent loops or picks the wrong tool is harder. I've found step-level tracing plus a few custom evals on production traffic catches most of the weird stuff.",
          "score": 6,
          "created_utc": "2026-01-05 19:15:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxxr43",
              "author": "sweetlemon69",
              "text": "This!",
              "score": 2,
              "created_utc": "2026-01-06 03:13:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny1buxx",
              "author": "gkarthi280",
              "text": "I agree. Agent loops/incorrect tool calls are super important to avoid unknown extra token usage and bills, yet super hard to monitor since these agents are so blackboxed. Interestingly, SigNoz has alerts, which you can use to actually detect these agent loops.   \n  \nJust as a simple example, you can set an alert to trigger every time an agent makes an identical tool call three times in a row or more. In this case you could be notified on any channel whenever an agent is doing these unnecessary repeated tool calls, giving you insight into when these agents are looping and how and where to optimize them and effectively lower spend.",
              "score": 1,
              "created_utc": "2026-01-06 17:00:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxv7oah",
          "author": "mdrxy",
          "text": "[https://www.langchain.com/langsmith/observability](https://www.langchain.com/langsmith/observability)",
          "score": 1,
          "created_utc": "2026-01-05 19:02:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwszqy",
          "author": "OnyxProyectoUno",
          "text": "Solid setup with OTEL, that's the right foundation. The token and latency tracking will save you a lot of headaches.\n\nOne thing I'd add: most of the \"where did this go wrong\" debugging I've done traces back upstream of the chain execution itself. Like, the retrieval returned garbage because the chunks were bad, or the tool got invoked with wrong context because metadata didn't propagate correctly. By the time you're looking at traces, you're seeing symptoms not causes.\n\nFor output quality over time, I've found it useful to log the actual retrieved chunks alongside the final response. When quality degrades, you can usually spot it in what got retrieved vs what should have. Evals on final output alone miss a lot.\n\nWhat's your retrieval setup look like? That's usually where the interesting failure modes hide.",
          "score": 1,
          "created_utc": "2026-01-05 23:35:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxjgod",
          "author": "saurabhjain1592",
          "text": "OTEL + LangSmith or Langfuse work well once you are inside LangChain execution.\n\nOne thing we kept running into in production is that many of the worst failures do not show up as errors in traces. They show up as valid executions that should not have happened, like retries with side effects, tools invoked with stale permissions, or chains continuing after the business outcome was already decided.\n\nTracing tells you what happened. You still need some notion of runtime control to decide whether it should have happened and to stop or intervene mid-run.\n\nCurious if others have hit this once workflows became long-running or stateful.",
          "score": 1,
          "created_utc": "2026-01-06 01:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxy8mnj",
          "author": "dinkinflika0",
          "text": "Your OTel setup handles infra metrics well but how do you track output quality?\n\nWe had the same stack and it showed us when things broke, but not why outputs degraded. Like retrieval working fine (low latency, no errors) but the agent ignoring context.\n\nAdded Maxim on top for LLM-specific metrics - hallucination rates, context usage, tool accuracy. Works with OTel but adds quality evaluation. [https://www.getmaxim.ai/products/agent-observability](https://www.getmaxim.ai/products/agent-observability)",
          "score": 1,
          "created_utc": "2026-01-06 04:18:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyjji8",
          "author": "Tough-Permission-804",
          "text": "just do replit or something similar.  the days of buulding your own workflow nightmare are over",
          "score": 1,
          "created_utc": "2026-01-06 05:33:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyph7m",
          "author": "mrpeakyblinder2",
          "text": "Try sentry",
          "score": 1,
          "created_utc": "2026-01-06 06:19:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyt7yz",
          "author": "HoldZealousideal1966",
          "text": "Mlflow",
          "score": 1,
          "created_utc": "2026-01-06 06:51:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz4506",
          "author": "Ok_Constant_9886",
          "text": "confident ai",
          "score": 1,
          "created_utc": "2026-01-06 08:30:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzn75e",
          "author": "jj_taylor_05",
          "text": "Have a look to phoenix, nevertheless all of this tools are so reactive, yo should look to a dashboard or set up alerts .:. We need something else",
          "score": 1,
          "created_utc": "2026-01-06 11:26:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1d9kc",
              "author": "gkarthi280",
              "text": "agreed! Just exporting traces is one step, but the real power of observability is enhanced when you are able to make relevant dashboards combined with alerts. SigNoz does include dashboard and alert features on their platform which ive found super helpful. I think the main challenge as a dev is to use these tools in a creative and efficient way to be able to detect these problems in prod and solve them effectively.",
              "score": 2,
              "created_utc": "2026-01-06 17:07:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny44vof",
          "author": "BeerBatteredHemroids",
          "text": "MLFlow",
          "score": 1,
          "created_utc": "2026-01-07 00:57:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qasrg6",
      "title": "I tested my LangChain agent with chaos engineering - 95% failure rate on adversarial inputs. Here's what broke.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qasrg6/i_tested_my_langchain_agent_with_chaos/",
      "author": "No-Common1466",
      "created_utc": "2026-01-12 11:45:01",
      "score": 16,
      "num_comments": 7,
      "upvote_ratio": 0.9,
      "text": "Hi r/LangChain,\n\nI'm Frank, the solo developer behind¬†[Flakestorm](https://github.com/flakestorm/flakestorm). I was recently humbled and thrilled to see it featured in the LangChain community spotlight. That validation prompted me to run a serious stress test on a standard LangChain agent, and the results were‚Ä¶ illuminating.\n\nI used Flakestorm, my open-source chaos engineering tool for AI agents to throw 60+ adversarial mutations at a typical agent. The goal wasn't to break it for fun, but to answer:¬†\"How does this agent behave in the messy real world, not just in happy-path demos?\"\n\n**The Sobering Results**\n\n* **Robustness Score:**¬†**5.2%**¬†(57 out of 60 tests failed)\n* **Critical Failures:**\n   1. **Encoding Attacks:**¬†**0% Pass Rate.**¬†The agent diligently¬†*decoded*¬†malicious Base64/encoded inputs instead of rejecting them. This is a major security blind spot.\n   2. **Prompt Injection:**¬†**0% Pass Rate.**¬†Direct \"ignore previous instructions\" attacks succeeded every time.\n   3. **Severe Latency Spikes:**¬†Average response blew past 10-second thresholds, with some taking nearly¬†**30 seconds**¬†under stress.\n\n**What This Means for Your Agents**  \nThis isn't about one \"bad\" agent. It's about a¬†**pattern**: our default setups are often brittle. They handle perfect inputs but crumble under:\n\n* **Obfuscated attacks**¬†(encoding, noise)\n* **Basic prompt injections**\n* **Performance degradation**¬†under adversarial conditions\n\nThese aren't theoretical flaws. They're the exact things that cause user-facing failures, security issues, and broken production deployments.\n\n**What I Learned & Am Building**  \nThis test directly informed Flakestorm's development. I'm focused on providing a \"crash-test dummy\" for your agents¬†*before*¬†deployment. You can:\n\n* **Test locally**¬†with the open-source tool (`pip install flakestorm`).\n* **Generate adversarial variants**¬†of your prompts (22+ mutation types).\n* **Get a robustness score**¬†and see¬†*exactly*¬†which inputs cause timeouts, injection successes, or schema violations.\n\n**Discussion & Next Steps**  \nI'm sharing this not to fear-monger, but to start a conversation the LangChain community is uniquely equipped to have:\n\n1. How are you testing your agents for real-world resilience**?**¬†Are evals enough?\n2. What strategies work for hardening agents against encoding attacks or injections?\n3. Is chaos engineering a missing layer in the LLM development stack?\n\nIf you're building agents you plan to ship, I'd love for you to try¬†[Flakestorm on your own projects](https://github.com/flakestorm/flakestorm). The goal is to help us all build agents that are not just clever, but truly robust.\n\n**Links:**\n\n* Flakestorm GitHub:¬†[https://github.com/flakestorm/flakestorm](https://github.com/flakestorm/flakestorm)\n* LangChain Community Spotlight: [https://x.com/LangChain/status/2007874673703596182](https://x.com/LangChain/status/2007874673703596182)\n* Example config & report from this test:  \n   * [https://github.com/flakestorm/flakestorm/blob/main/examples/langchain\\_agent/flakestorm.yaml](https://github.com/flakestorm/flakestorm/blob/main/examples/langchain_agent/flakestorm.yaml)\n   * [https://github.com/flakestorm/flakestorm/blob/main/flakestorm-20260102-233336.html](https://github.com/flakestorm/flakestorm/blob/main/flakestorm-20260102-233336.html)\n\nI'm here to answer questions and learn from your experiences.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qasrg6/i_tested_my_langchain_agent_with_chaos/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nz6ppun",
          "author": "Goolitone",
          "text": "whaaaa?",
          "score": 1,
          "created_utc": "2026-01-12 16:30:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00fgu1",
          "author": "Revolutionary-Bet-58",
          "text": "this matches what ive seen building agents across langchain and crewai, the happy path works fine but anything adversarial breaks it.   \n  \nwhat helped us was scanning the agent code before deployment to catch places where user input can flow into system prompts. basically treating it like a security review not just testing. found a ton of injection vectors that way also encoding attacks are brutal, base64 stuff especially. we started stripping/validating anything that looks encoded before it hits the llm   \n  \ncurious if youve tried combining chaos testing with static analysis? feels like they catch different things",
          "score": 1,
          "created_utc": "2026-01-16 23:04:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01gx02",
              "author": "No-Common1466",
              "text": "Yeah. That's exactly the vision. Chaos testing for runtime breaks, static analysis for the code flaws. They're a perfect one-two punch.\n\nWould love to know: any static analysis tools or methods you've found useful for those pre-deployment security reviews?",
              "score": 1,
              "created_utc": "2026-01-17 02:49:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o02oais",
                  "author": "Revolutionary-Bet-58",
                  "text": "actually been building something for this - [https://github.com/inkog-io/inkog](https://github.com/inkog-io/inkog)\n\nfull disclosure im the founder so obviously biased, but it came out of exactly this problem. scans langgraph/crewai/n8n code and looks for cycles without exit conditions, places where user input can reach system prompts, unbounded loops etc\n\nstill early and definitely has rough edges. would be curious what you think if you try it against some of the stuff that broke in your chaos tests",
                  "score": 1,
                  "created_utc": "2026-01-17 08:23:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o01hwhw",
          "author": "cordialgerm",
          "text": "Any reason I shouldn't just by default add middleware that rejects any base64-looking input in a user message unless my agent specifically needs to support it?",
          "score": 1,
          "created_utc": "2026-01-17 02:55:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01o64n",
              "author": "No-Common1466",
              "text": "That's the point. Most AI agents in production don't have these guardrails. If you didn't test these scenarios, you wouldn't even know. That's the importance of Adversarial testing like flakestorm. The next step is to mitigate these security issues upon proper testing.",
              "score": 1,
              "created_utc": "2026-01-17 03:35:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qbxrhu",
      "title": "Open-Source Memory Layer for Long-Running Agents: HMLR (LangGraph Integration Available)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qbxrhu/opensource_memory_layer_for_longrunning_agents/",
      "author": "AnAlpacca",
      "created_utc": "2026-01-13 17:43:39",
      "score": 15,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I launched an open-source project a bit over a month ago called HMLR (Hierarchical Memory Lookup & Routing), basically a \"living memory\" system designed specifically for agentic AI that needs to remember across long sessions without forgetting or hallucinating on old context.\n\nThe core problem it solves: Standard vector RAG or simple conversation buffers fall apart in multi-day/week agents (e.g., personal assistants, research agents, or production tools). HMLR utilizes hierarchical routing and multi-hop reasoning to reliably persist and recall information, and it passes benchmarks such as the \"Hydra of Nine Heads\" on mini LLMs. (A full harness for reproducibility of tests is part of the repository.)\n\nKey features:\n\n* Drop-in LangGraph node (just added recently ‚Äì makes it super easy to plug into existing agents)\n* Pip installable: pip install hmlr\n* Benchmarks showing strong recall without massive context bloat\n* Fully open-source (MIT)\n\nRepo: [https://github.com/Sean-V-Dev/HMLR-Agentic-AI-Memory-System](https://github.com/Sean-V-Dev/HMLR-Agentic-AI-Memory-System)",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1qbxrhu/opensource_memory_layer_for_longrunning_agents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nzob732",
          "author": "pbalIII",
          "text": "H-MEM's four-layer structure (domain, category, trace, episode) is interesting, but what I keep bumping into is the consolidation policy. Most hierarchical approaches nail retrieval but get noisy fast if you're not pruning or summarizing aggressively.\n\nTwo things I'd want to know before adopting something like this:\n\n- How does it handle cross-session deduplication? Mem0 and A-Mem both struggle when the same fact surfaces in slightly different forms.\n- What's the write path latency? Retrieval benchmarks look great, but if every turn blocks on memory updates you're eating that cost in UX.",
          "score": 1,
          "created_utc": "2026-01-15 04:42:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5rtr8",
      "title": "I applied \"Systemic Design\" principles from Game Dev (BioShock/Star Wars) to AI Agents. Here is why it works better than hard-coding.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q5rtr8/i_applied_systemic_design_principles_from_game/",
      "author": "reidkimball",
      "created_utc": "2026-01-06 19:11:00",
      "score": 13,
      "num_comments": 3,
      "upvote_ratio": 0.85,
      "text": "I spent 10+ years as a game designer (LucasArts) before moving into AI and App development. In games, we rely heavily on \"Systemic Design\" where we create systems (physics, AI rules, environmental hazards) that interact to create emergent gameplay instead of scripting every single moment.\n\nI‚Äôve been applying this same philosophy to building AI Agents, and I think it solves the \"brittleness\" problem a lot of us are facing with LLMs.\n\n**The Problem: Deterministic vs. Systemic**  \nWhen I started building my current health app (Meadow Mentor), my instinct was to hard-code logic for safety.\n\n* The Old Way: Write endless if/else statements. If user.isDairyFree AND item == 'milk', then suggest\\_alternative().\n* The Issue: This doesn't scale. You spend weeks mapping out edge cases.\n\n**The Solution: Systemic Agent Design**  \nInstead of scripting the path, I set up a system consisting of three parts:\n\n1. **Dynamic Data:** The user's live state (e.g., \"Dairy-Free,\" \"High Stress\").\n2. **Systemic Tools:** Functions like addToShoppingList or updateStressLevel.\n3. **Reasoning:** An LLM with a system prompt to strictly adhere to health safety.\n\n**The Result (Emergent Behavior)**  \nI tested this by asking my agent to \"add milk, eggs, and bananas\" to my list while my profile was set to Dairy-Free.\n\nI hadn't written a specific script to handle this conflict. However, the agent paused, analyzed the input against the **Dynamic Data**, and refused the request. It autonomously suggested swapping for Oat or Almond milk. Once I confirmed, it called the tool with the safe ingredients.\n\nWhat would have been a 2-week sprint of mapping out diet vs. ingredient conflicts took about an hour to set up as a system.\n\n**The Takeaway**  \nIf you are building agents, stop trying to predict every user path. Focus on defining the \"physics\" of your app (the tools) and the \"environment\" (the data) and let the model handle the navigation.\n\nI wrote a longer breakdown of the logic and the \"Recipe Search\" implementation on my site if anyone wants to see the specific setup:\n\n[https://reidkimball.com/journal/systemic-agent-design/](https://reidkimball.com/journal/systemic-agent-design/)\n\nAre you building Systemic Agents? Feel free to reach out, would love to share notes and help each other grow in this new discipline.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q5rtr8/i_applied_systemic_design_principles_from_game/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "ny2ezdm",
          "author": "Objective-Fig-4250",
          "text": "You can go even more meta than this. By your analogy, choose \"physics\" (tools) as - application types you want - assistant / researcher / consultant - \"environment\" (data) as - sectors like HealthCare / education / environment & let LLM think of ways to jot down & connect all these low-level components, to form something cohesive & workable at a high level - like your \"Meadows Mentor\" application.\n\nThe excerpt above is just to state that this paradigm of agentic AI development was designed with EXACTLY this methodology or workflow in mind, that you have eventually have come across in your own explorations - of defining the tools + data & letting the LLM come up with reasoning of what control flow the program should opt for. Without needing to chart your own decision tree of all possible scenarios & edge cases! \n\nIf I am wrong, please correct me, but I think you've just rehashed what agentic development expects of us, based on your own deep understanding of game development. I can't seem to find that great of a distinction here. Sorry, if I misread what you're trying to convey. Like, for example, how will \"your way\" reduce confabulations of a diet plan that adheres to constraints by suggesting something that's not edible ?",
          "score": 5,
          "created_utc": "2026-01-06 19:57:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny32v1b",
              "author": "reidkimball",
              "text": "Hey thank you for the thoughtful reply. You may be right that I'm already describing what is expected of AI Agent Engineers. I think that's great! I'd like to connect with other AI Agent Engineers. \n\nGood question about preventing confabulations. As I'm sure you know, that often happens when an AI relies on its training data or lack of data. My systemic approach grounds everything in a user's current reality, like the physics engine in a game world.   \n  \nThe AI doesn't rely on its training data, but the data of the user, the rules of the therapeutic diet, and data about food in my custom food database. It's like if an AI enemy tries to run through a wall in a video game, it can't because the physics system won't allow it to.   \n  \nIn my health app, the AI can't produce wrong answers because my system won't allow it to. If the AI produces a recipe JSON with ingredients not allowed, the system prompts it to retry again until the recipe is approved. Because I first ground the AI in rules of the diet, and user data, it usually nails it 9/10 times. When it doesn't, it takes one revision to fix it.",
              "score": 2,
              "created_utc": "2026-01-06 21:47:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny52cvr",
                  "author": "Objective-Fig-4250",
                  "text": "So kind of like RLHF hyperscaled by self-supervision - where you design the template (\"physics\") as a system that holds the last authority to approve the diet plan before the user sees it. See, exactly this is how crude, pre-trained models like GPT are polished into a serviceable products like ChatGPT, inferences of which are increasingly more \"aligned\" to human-like responses with time. I personally haven't encountered the hot paths that trigger its confabulations until something that I ask about requires multi-step, complex planning or/and is esoteric.\n\nI think these extrinsic, cosmetic checks are still duct-tape fixes to the more innate architectural choices of transformer network & the current way of how computation is being utilised towards simulating intelligent human behavior. These are well & good, until the day they are not. RAG, vector stores, etc. all are meant for grounding the responses, AFTER they had been 'conjured' up by the network.\n\nHave you stress-tested your application under various use-cases ? Given your LLM sees the diet plan, health history, how do you ensure that LLM puts  the optimal set of food items in the cart, of all the allowed permutations & combinations ?",
                  "score": 2,
                  "created_utc": "2026-01-07 04:03:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q6bl1n",
      "title": "LangChain or LangGraph? for building multi agent system",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q6bl1n/langchain_or_langgraph_for_building_multi_agent/",
      "author": "Major_Ad7865",
      "created_utc": "2026-01-07 10:08:38",
      "score": 13,
      "num_comments": 32,
      "upvote_ratio": 0.93,
      "text": "I‚Äôve just started learning LangChain and LangGraph, and I want to build a multi-agent application. I‚Äôm a bit confused about which one I should use. Should I go with LangChain or LangGraph? Also, is LangChain built on top of LangGraph, or are they separate? which one to learn first?",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1q6bl1n/langchain_or_langgraph_for_building_multi_agent/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "ny6i6oo",
          "author": "honorable_uncle",
          "text": "Skip langchain and go with langgraph",
          "score": 7,
          "created_utc": "2026-01-07 11:07:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz46slj",
              "author": "mdrxy",
              "text": "what for? langchain's agent loop is built on top of langgraph",
              "score": 1,
              "created_utc": "2026-01-12 05:47:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny7ckij",
          "author": "ScaleBrave6626",
          "text": "Langchain is just for the core components and wrappers. It‚Äôs useful for apis, vector stores and utilities in general. For building agentic applications go for langgraph that‚Äôconceived for this purpose. They provide built in function for building off the shelves react agent but also the logic to customize your agent",
          "score": 3,
          "created_utc": "2026-01-07 14:23:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyxr0yy",
              "author": "Major_Ad7865",
              "text": "sure ;)",
              "score": 1,
              "created_utc": "2026-01-11 07:43:01",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nz466it",
              "author": "mdrxy",
              "text": "A lot of this was moved to `langchain` in `1.0.0`, see [docs](https://docs.langchain.com/oss/python/releases/langchain-v1)",
              "score": 1,
              "created_utc": "2026-01-12 05:43:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nybnp48",
          "author": "Khade_G",
          "text": "They‚Äôre related, but they solve different problems. LangChain is the easier starting point‚Ä¶ it‚Äôs a toolbox for prompts, tools, retrievers, and simple agents. Great for getting something working quickly.\n\nLangGraph is for when things get more complex: multiple steps, multiple agents, branching logic, retries, and state. It gives you more control over how an agent actually moves through a process.\n\nThey‚Äôre not built on top of each other, but they work well together. A good path is to learn LangChain basics first, then move to LangGraph once you start thinking in workflows instead of single chats.",
          "score": 3,
          "created_utc": "2026-01-08 02:25:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyxqvgw",
              "author": "Major_Ad7865",
              "text": ">",
              "score": 1,
              "created_utc": "2026-01-11 07:41:39",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nz469ns",
              "author": "mdrxy",
              "text": ">They're not built on top of each other\n\nnot entirely true anymore! see [here](https://docs.langchain.com/oss/python/releases/langchain-v1)",
              "score": 1,
              "created_utc": "2026-01-12 05:43:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6cgue",
          "author": "Hot_Substance_9432",
          "text": "LangGraph is built on LangChain and uses several of its core models, you can build MAS using langGraph for sure here is some guidance\n\n[https://medium.com/@sushmita2310/building-multi-agent-systems-with-langgraph-a-step-by-step-guide-d14088e90f72](https://medium.com/@sushmita2310/building-multi-agent-systems-with-langgraph-a-step-by-step-guide-d14088e90f72)",
          "score": 2,
          "created_utc": "2026-01-07 10:17:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8t28m",
          "author": "USToffee",
          "text": "Check out Langgraph Deep Agents",
          "score": 2,
          "created_utc": "2026-01-07 18:25:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2haz7",
          "author": "eliaweiss",
          "text": "none of them",
          "score": 2,
          "created_utc": "2026-01-12 00:00:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyanv0q",
          "author": "theboldestgaze",
          "text": "Langgraph is unnecessarily complex.",
          "score": 4,
          "created_utc": "2026-01-07 23:21:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyats4n",
              "author": "cay7man",
              "text": "What‚Äôs the alternative?",
              "score": 3,
              "created_utc": "2026-01-07 23:51:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyduk58",
                  "author": "theboldestgaze",
                  "text": "Llama workflows is way more thought-through in my book. Any event driven approach would be easier.\n\n\nLanggraph has zero tech advantages. Pydantic gives great ecosystem and foundation, llama gives RAG and events. There are others which I am not familiar with.\n\n\nLanggraph is market acquisition with fancy buzzwords. I mean, seriously.",
                  "score": 1,
                  "created_utc": "2026-01-08 12:07:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyxrdje",
              "author": "Major_Ad7865",
              "text": "I don‚Äôt think so. Replit and Lovable are using this framework to build their agents. To some extent, yes, maybe but in the beginning, LangGraph feels overwhelming. Still, it‚Äôs worth a shot.",
              "score": 1,
              "created_utc": "2026-01-11 07:46:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyy7bbf",
                  "author": "theboldestgaze",
                  "text": "They started a while ago. Better alternatives might have not been available back then.\n\n\nLangGraph invested in first mover advantage as opposed to engineering advantage. First movers are hardly ever the best engineering.",
                  "score": 1,
                  "created_utc": "2026-01-11 10:13:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz46huc",
              "author": "mdrxy",
              "text": "can you be specific?",
              "score": 1,
              "created_utc": "2026-01-12 05:45:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz62t9y",
                  "author": "theboldestgaze",
                  "text": "Sure. Reducers, nodes, edges, separate build graph step, different edge types, checkpoints... They even mention Google Pregel as an inspiration, which is bonkers.\n\nIn Llamaindex workflows you have events and a \\`Workflow\\` base class and it serves the same purpose. I am not a llamaindex fan, I just happen to know it.\n\nIt is not a matter of presonal preference. LangChain and LangGraph have been famous for unnecessary and overly complex abstractions. They design their software as if their engineering was a group of bored to death guys frustrated they didn't make it to Google.",
                  "score": 1,
                  "created_utc": "2026-01-12 14:40:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny7e93o",
          "author": "kk_red",
          "text": "Single AI agent, Langgraph, \nMultiple AI agents to work together Langgchain. \n\nNow as we know we have movee beyound a single agent. Plus they are bringing something called deepagents.",
          "score": 1,
          "created_utc": "2026-01-07 14:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7j9i9",
          "author": "Born_Owl7750",
          "text": "Langgraph if strict graph like workflows.\nTake a look at crewai and autogen too for group chat, mutli agent collaboration kind of flows.",
          "score": 1,
          "created_utc": "2026-01-07 14:57:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyb9ez7",
          "author": "bzImage",
          "text": "Started with langchain.. needed more control.. Langgraph.. .. now.. go straight to \"deep agents\" with langgraph..",
          "score": 1,
          "created_utc": "2026-01-08 01:10:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycki53",
          "author": "Dramatic_Strain7370",
          "text": "i‚Äôve seen some slick agents being developer on low code platforms like n8n.com. has anyone done comparison between langgraph and n8n in terms of velocity of development?",
          "score": 1,
          "created_utc": "2026-01-08 05:39:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nye1v1k",
          "author": "PuzzleheadedPear6672",
          "text": "Use crew.ai for multi agent. Way better then langchain",
          "score": 1,
          "created_utc": "2026-01-08 12:55:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz46ox7",
              "author": "mdrxy",
              "text": "a substantial amount of work has been done to improve multi-agent in langchain/langgraph, would encourage checking out the [new section in the docs](https://docs.langchain.com/oss/python/langchain/multi-agent/index)",
              "score": 1,
              "created_utc": "2026-01-12 05:47:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nysbylr",
          "author": "iamsyr",
          "text": "For a multi-agent system, LangGraph is definitely the way to go. LangChain is great for linear pipelines (DAGs), but LangGraph introduces the cyclic capability and state management required for agents to loop, retry, and hand off tasks. Ossaix has a breakdown of agent frameworks if you want to see how LangGraph compares to alternatives like CrewAI or AutoGen.",
          "score": 1,
          "created_utc": "2026-01-10 13:59:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyz6kux",
          "author": "trendspotman",
          "text": "Start with Langchain. Graduate to langgraoh if and only if required",
          "score": 1,
          "created_utc": "2026-01-11 14:36:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyz6r0z",
          "author": "trendspotman",
          "text": "Start with langchain. Graduate to langhraph if and only if required",
          "score": 1,
          "created_utc": "2026-01-11 14:37:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1kbv2",
          "author": "jbindc20001",
          "text": "I've used LangChain, LangGraph, Crew.ai and most recently Google ADK. Google ADK documentation unfortunately is not in AIs base dataset to help you develop it without breaking unless you update it's memory on areas you need help with but I find it the best from a feature set perspective. And seems every month Google is adding more and more features to it.¬†",
          "score": 1,
          "created_utc": "2026-01-11 21:17:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz46qpx",
              "author": "mdrxy",
              "text": "what do you like from ADK?",
              "score": 1,
              "created_utc": "2026-01-12 05:47:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny97qzj",
          "author": "AdditionalWeb107",
          "text": "Plano - obviously ;-) [https://github.com/katanemo/plano](https://github.com/katanemo/plano)",
          "score": 0,
          "created_utc": "2026-01-07 19:29:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0dpty",
      "title": "mem0, Zep, Letta, Supermemory etc: why do memory layers keep remembering the wrong things?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q0dpty/mem0_zep_letta_supermemory_etc_why_do_memory/",
      "author": "nicolo_memorymodel",
      "created_utc": "2025-12-31 14:02:45",
      "score": 11,
      "num_comments": 7,
      "upvote_ratio": 0.87,
      "text": "Hi everyone,\nthis question is for people building AI agents that go a bit beyond basic demos.\nI keep running into the same limitation: many memory layers (mem0, Zep, Letta, Supermemory, etc.) decide for you what should be remembered.\n\nConcrete example: contracts that evolve over time\n‚Äì initial agreement\n‚Äì addenda / amendments\n‚Äì clauses that get modified or replaced\n\nWhat I see in practice:\nRAG: good at retrieving text, but it doesn‚Äôt understand versions, temporal priority, or clause replacement.\nVector DBs: they flatten everything, mixing old and new clauses together.\n\nMemory layers: they store generic or conversational ‚Äúmemories‚Äù, but not the information that actually matters, such as:\n\n-clause IDs or fingerprints\n-effective dates\n-active vs superseded clauses\n-relationships between different versions of the same contract\n\nThe problem isn‚Äôt how much is remembered, but what gets chosen as memory.\n\nSo my questions are:\nhow do you handle cases where you need structured, deterministic, temporal memory?\n\ndo you build custom schemas, graphs, or event logs on top of the LLM?\n\nor do these use cases inevitably require a fully custom memory layer?",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1q0dpty/mem0_zep_letta_supermemory_etc_why_do_memory/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nwxs397",
          "author": "southern_gio",
          "text": "Have you tried EverMemOS?",
          "score": 1,
          "created_utc": "2025-12-31 16:39:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2gvr1",
              "author": "nicolo_memorymodel",
              "text": "Honestly no, do you think it could be right for me? \nI am very interested in deciding what kind of memories to save mainly",
              "score": 1,
              "created_utc": "2026-01-01 11:57:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwz0z2s",
          "author": "rkpandey20",
          "text": "All of these memory layers just solve the problem of compaction of context. Compaction can be done on context from many different ways and depending on the use case, some may work better than others.¬†\nI am not sure if general purpose memory layer can solve all use cases. You may have to plug-in your code to extract important bits from the context and preserve it.¬†",
          "score": 1,
          "created_utc": "2025-12-31 20:27:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0qbi4",
          "author": "BeerBatteredHemroids",
          "text": "Idk wtf you're talking about. You can store anything you want in your database as memory. Why are you pretending like you have no choice in the matter? Im not even sure you know what you're talking about.",
          "score": 0,
          "created_utc": "2026-01-01 02:33:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2h7db",
              "author": "nicolo_memorymodel",
              "text": "Wow, quiet haha\n\nSaving them in my personal db means managing vdb, building ingestion and retrieval middleware, avoiding duplicates, and structuring the vdb so that it scales over time without creating hallucinations in memory retrieval. \n\nThere are great (cloud-managed) systems that do this, but I struggle to find one that fits very vertical use cases, they are mainly made for personal assistants",
              "score": 1,
              "created_utc": "2026-01-01 12:00:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qejo7q",
      "title": "Deploying LangGraph agents to your own AWS with one command",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qejo7q/deploying_langgraph_agents_to_your_own_aws_with/",
      "author": "DefangLabs",
      "created_utc": "2026-01-16 15:54:18",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 0.83,
      "text": "We keep seeing deployment questions come up here, so wanted to share what we've built.\n\n**The problem:**\n\nLangGraph is great for building agents locally. But when you want to deploy:\n\n* LangSmith/LangServe are solid but your data goes through their infra\n* Self-hosting on AWS means ECS, IAM roles, VPCs, load balancers, secrets management...\n* Most tutorials stop at \"run it locally\"\n\n**What we built:**\n\nDefang lets you deploy any containerized app to your own AWS/GCP with one command. You write a compose.yaml:\n\nyaml\n\n    services:\n      agent:\n        build: .\n        ports:\n          - \"8000:8000\"\n        x-defang-llm: true\n\nRun `defang compose up`. Done. It provisions ECS, networking, SSL, everything.\n\nThe `x-defang-llm: true` part auto-configures IAM permissions for AWS Bedrock (Claude, Llama, Mistral) or GCP Vertex AI. No policy writing.\n\n**Why this matters:**\n\n* Your AWS account, your data, your infrastructure\n* Works with any LangChain/LangGraph setup (just containerize it)\n* Scales properly (ECS Fargate under the hood)\n* Free tier for open source repos (forever, not a trial)\n\n**We're launching V3 next week** with:\n\n* Named Stacks ‚Äî deploy separate instances for dev/staging/prod or per customer from the same codebase\n* Agentic CLI ‚Äî auto-debugs deployment errors, understands English commands\n* Zero-config AWS ‚Äî one click to connect, no IAM policies to write\n\nWe have a LangGraph sample ready to go: [github.com/DefangLabs/samples](http://github.com/DefangLabs/samples)\n\nLaunching on Product Hunt Jan 21. \n\nHappy to answer questions about deploying LangGraph or agents in general.",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qejo7q/deploying_langgraph_agents_to_your_own_aws_with/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qahw7y",
      "title": "Moving from n8n to production code. Struggling with LangGraph and integrations. Need guidance",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qahw7y/moving_from_n8n_to_production_code_struggling/",
      "author": "Abdullah_Said",
      "created_utc": "2026-01-12 01:48:34",
      "score": 9,
      "num_comments": 8,
      "upvote_ratio": 0.92,
      "text": "Hi everyone\n\nI need some guidance on moving from a No Code prototype to a full code production environment\n\nBackground\nI am an ML NLP Engineer comfortable with DL CV Python I am currently the AI lead for a SaaS startup We are building an Automated Social Media Content Generator User inputs info and We generate full posts images reels etc\n\nCurrent Situation\nI built a working prototype using n8n It was amazing for quick prototyping and the integrations were like magic But now we need to build the real deal for production and I am facing some decision paralysis\n\nWhat I have looked at\nI explored OpenAI SDK CrewAI AutoGen Agno and LangChain I am leaning towards LangGraph because it seems robust for complex flows but I have a few blockers\n\nFramework and Integrations\nIn n8n connecting tools is effortless In code LangGraph LangChain it feels much harder to handle authentication and API definitions from scratch Is LangGraph the right choice for a complex SaaS app like this Are there libraries or community nodes where I can find pre written tool integrations like n8n nodes but for code Or do I have to write every API wrapper manually\n\nLearning and Resources\nI struggle with just reading raw documentation Are there any real world open source projects or repos I can study Where do you find reusable agents or templates\n\nDeployment and Ops\nI have never deployed an Agentic system at scale How do you guys handle deployment Docker Kubernetes specific platforms Any resources on monitoring agents in production\n\nPrompt Engineering\nI feel lost structuring my prompts System vs User vs Context Can anyone share a good guide or cheat sheet for advanced prompt engineering structures\n\nInfrastructure\nFor a startup MVP Should I stick to APIs OpenAI Claude or try self hosting models on AWS GCP Is self hosting worth the headache early on\n\nSorry if these are newbie questions I am just trying to bridge the gap between ML Research and Agent Engineering\n\nAny links repos or advice would be super helpful Thanks\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qahw7y/moving_from_n8n_to_production_code_struggling/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nz4beta",
          "author": "Asif_ibrahim_",
          "text": "You‚Äôre not stuck because you lack tools; you‚Äôre stuck because you crossed the boundary from *automation* to software engineering.  \nn8n gave you integrations, state, retries, auth, and observability ‚Äúfor free.‚Äù When you move to LangGraph you‚Äôre taking ownership of all of that, which is why it suddenly feels harder than ML.  \nThe pattern that actually works is to keep n8n (or something similar) at the edges for auth, webhooks, and SaaS integrations, and use LangGraph only for the agent logic. Trying to recreate n8n inside Python is where most teams burn months.  \nTreat agents like a service with a strict input/output contract, not like a workflow you rebuild per integration.",
          "score": 7,
          "created_utc": "2026-01-12 06:25:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgv97r",
              "author": "Abdullah_Said",
              "text": "This perspective on Automation vs SWE hits home The hybrid approach sounds like the perfect middle ground to start Regarding your last point about treating agents like a service with a strict contract vs a workflow could you elaborate on that maybe with a simple example or pseudo code",
              "score": 1,
              "created_utc": "2026-01-14 02:22:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzgqls8",
          "author": "Bubbly_Gap6378",
          "text": "For the auth/integrations problem specifically, I built something for this.\n\nThe gap you're describing (\"in n8n connecting tools is effortless, in code it feels much harder\") is real. The other commenter is right that n8n handles auth \"for free.\"\n\nOne option: use Romek to bring that same auth simplicity into your Python code. It grabs your existing Chrome sessions and lets your agents use them:\n\nromek grab [linkedin.com](http://linkedin.com)\n\nThen in LangGraph:\n\nfrom romek import Vault\n\nvault = Vault()\n\ncookies = vault.get\\_session(\"linkedin.com\")\n\nNo OAuth setup, no writing auth wrappers from scratch. If you're logged into a site in Chrome, your agent can use that session.\n\nFor your other questions:\n\n\\- LangGraph is a good choice for complex flows\n\n\\- Start with Docker on a simple VPS, skip K8s until you actually need it\n\n\\- For prompts: [https://docs.anthropic.com/claude/docs/prompt-engineering](https://docs.anthropic.com/claude/docs/prompt-engineering) is solid\n\n\\- Stick with APIs (OpenAI/Claude) for MVP, self-hosting is a distraction early on\n\nGitHub: [https://github.com/jacobgadek/romek](https://github.com/jacobgadek/romek)",
          "score": 1,
          "created_utc": "2026-01-14 01:56:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgwuzg",
              "author": "Abdullah_Said",
              "text": "Romek looks like a clever hack to bypass the OAuth pain specifically for local dev\n\nQuick question though How would this work in a production environment on a headless VPS Since there is no active Chrome session to grab would I need to handle cookie export manually\n\nThanks for the Anthropic link strictly following their guide seems to be the consensus",
              "score": 1,
              "created_utc": "2026-01-14 02:31:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzgzg90",
                  "author": "Bubbly_Gap6378",
                  "text": "Good question. You're right, Romek is designed for local dev and personal automation, not headless production servers.\n\nFor production on a VPS, you'd either:\n\n1. Run Romek locally, export the cookies, then upload them to your VPS (manual but works)\n\n2. Use a headed browser on the VPS (some people run Chrome with a display via Xvfb)\n\n3. For true production at scale, you'd still need proper OAuth/API integrations\n\nRomek shines when:\n\n\\- You're building/testing locally\n\n\\- You're running personal automations on your own machine\n\n\\- You need to quickly prototype something that requires auth\n\nIt's not meant to replace OAuth for multi-user production apps,  it's meant to skip the OAuth headache when you just need YOUR session for YOUR agent.\n\nDoes that match your use case?",
                  "score": 2,
                  "created_utc": "2026-01-14 02:46:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q4qgr7",
      "title": "What are you using instead of LangSmith?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q4qgr7/what_are_you_using_instead_of_langsmith/",
      "author": "clickittech",
      "created_utc": "2026-01-05 16:34:44",
      "score": 9,
      "num_comments": 31,
      "upvote_ratio": 0.7,
      "text": "I‚Äôve been reading some negative opinions about LangSmith lately, not that it‚Äôs bad, just that it doesn‚Äôt always fit once things get real.\n\nStuff like, gets expensive fast or hard to fit into existing observability stacks\n\nI‚Äôve some alternatives for langsimth like\n\n* Arize Phoenix\n* OpenTelemetry setups\n* Datadog/ELK\n* ZenML\n* Mirascope\n* HoneyHive\n* Helicone\n\nwhat are you guys using instead?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1q4qgr7/what_are_you_using_instead_of_langsmith/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nxutkom",
          "author": "caprica71",
          "text": " Langfuse",
          "score": 16,
          "created_utc": "2026-01-05 17:59:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nym52ul",
              "author": "C4ptainK1ng",
              "text": "This is not only the alternative but also no 1 software when it comes to LLMops. Fully open source, awesome features and nice community. Using it in many productions setups. It's the most mandatory software in our stack for sure",
              "score": 1,
              "created_utc": "2026-01-09 15:52:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxujyqi",
          "author": "Ecto-1A",
          "text": "We‚Äôre in the process of getting Confident AI set up. We tested most of these and it seemed to be the best fit without much code rewriting. Langsmith has been fine for the past year and a half but as our infrastructure grows, it becomes more obvious where langsmith is lacking.",
          "score": 4,
          "created_utc": "2026-01-05 17:15:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxusmik",
              "author": "Business-Hyena-6173",
              "text": "I work on LangSmith and would love to understand where we‚Äôre falling short. Feel free to DM me!",
              "score": 3,
              "created_utc": "2026-01-05 17:55:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxumv5n",
          "author": "necati-ozmen",
          "text": "We have users migration from langsmith to Voltagent which has built-in observability & automation layer.  \n[https://github.com/VoltAgent/voltagent](https://github.com/VoltAgent/voltagent)",
          "score": 2,
          "created_utc": "2026-01-05 17:29:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvh232",
              "author": "nineelevglen",
              "text": "\\+1 on Volt. I've found it really good. We use langfuse for logging / benchmarking still, but Volt has plugins for that",
              "score": 2,
              "created_utc": "2026-01-05 19:45:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxv57x7",
          "author": "Ancient-Direction231",
          "text": "https://www.nfrax.com/ai-infra\n\nCombines langsmith/langchain, pydantic-ai, llmlite and more capabilities into one",
          "score": 2,
          "created_utc": "2026-01-05 18:51:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxue701",
          "author": "ninadpathak",
          "text": "The fact that this question even exists shows observability tooling for AI wasn't built with developers in mind. In 5 years these tools will be baked into the LLM frameworks themselves. Right now people cobble together open-source solutions. Keep an eye on who's building observability-first AI agents because that market category is about to explode.",
          "score": 2,
          "created_utc": "2026-01-05 16:48:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxv0kb9",
              "author": "OneTurnover3432",
              "text": "100% agree - check what I'm building : [thinkhive.ai](http://thinkhive.ai) \n\nWe're platform agnostic and focused on making the management of AI agents as easy as possible",
              "score": -6,
              "created_utc": "2026-01-05 18:30:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxuevfq",
          "author": "thelolzmaster",
          "text": "Throw https://tryspyglass.com into the mix",
          "score": 1,
          "created_utc": "2026-01-05 16:51:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxuwgnd",
          "author": "ChipsAhoy21",
          "text": "MLFlow 3.0, it‚Äôs open source and great at what it does. My org is databricks forward so that helps",
          "score": 1,
          "created_utc": "2026-01-05 18:12:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvlnlw",
          "author": "pbalIII",
          "text": "Depends on whether you're locked into LangChain or not.\n\nIf you are, LangSmith is still the path of least resistance. If you want out, Langfuse is the open-source default now... 19k+ GitHub stars, MIT license, self-host for free. Native SDKs for Python/JS, plus it plugs into LangChain, LlamaIndex, and 50+ other frameworks.\n\nFor vanilla API calls, gateway tools like Helicone or Portkey work well. Just a URL change to start logging. Portkey also handles fallbacks and load balancing across providers.\n\nIf you're already in Datadog's ecosystem, their LLM Observability module auto-instruments OpenAI, Anthropic, and Bedrock without code changes. Pricey at scale though.",
          "score": 1,
          "created_utc": "2026-01-05 20:06:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyph6n",
          "author": "FluffyFill64",
          "text": "Confident AI",
          "score": 1,
          "created_utc": "2026-01-06 06:19:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyqjlp",
          "author": "xelnet",
          "text": "AI is so custom that the value needed for troubleshooting forced me to build my own. Happy to consult if need any help.",
          "score": 1,
          "created_utc": "2026-01-06 06:28:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz2ftf",
          "author": "Ok_Constant_9886",
          "text": "Helicone was a solid choice but eventually we needed really solid evals on top of observability, and ended up choosing Confident AI",
          "score": 1,
          "created_utc": "2026-01-06 08:14:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzs73k",
          "author": "jaisanant",
          "text": "Mlflow",
          "score": 1,
          "created_utc": "2026-01-06 12:04:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny61g1d",
              "author": "Masotsheni",
              "text": "Mlflow's a solid choice! It really shines for tracking experiments and managing models. How have you found its integration with your existing tools?",
              "score": 1,
              "created_utc": "2026-01-07 08:34:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny648ur",
                  "author": "jaisanant",
                  "text": "I am using langgraph for multi agent architecture and using mlflow autolog for langgraph/langchain.\nUsed set experiment for multiple sessions and tracking each run in that session \nYou can set custom tag too for each agent run under different sessions\nIt is very easy. Look in their doc.",
                  "score": 1,
                  "created_utc": "2026-01-07 09:00:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny0c45s",
          "author": "bzImage",
          "text": "I ask the llm to provide a reasoning and explanation for their actions.. i catch that output and save it on opensearch.. that is my observability. What can i gain with langsmith/langfuse/x ?",
          "score": 1,
          "created_utc": "2026-01-06 14:08:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny552n3",
          "author": "HoldZealousideal1966",
          "text": "Mlflow - Open Source, self managed, and can also be used for experiment tracking",
          "score": 1,
          "created_utc": "2026-01-07 04:20:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5lwxm",
          "author": "MisterIndemni",
          "text": "langsmith is good, not perfect, but you can do most anything on it - and if your using langchain its easy and makes sense. I use a few in that list, but I always use confident ai with it because of their red teaming functionality you can't really get with any of the alternatives. Considering to exclusively use their platform going forward if they keep up with great updates this year.",
          "score": 1,
          "created_utc": "2026-01-07 06:19:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6kzg3",
          "author": "Preconf",
          "text": "Phoenix is what I'm using while putting together the project I'm working on. It plays well with langchain and langgraph",
          "score": 1,
          "created_utc": "2026-01-07 11:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8va23",
          "author": "Bright-Aks",
          "text": "Use langfuse",
          "score": 1,
          "created_utc": "2026-01-07 18:35:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvkrvi",
          "author": "gkarthi280",
          "text": "I‚Äôve been using SigNoz and it‚Äôs been a really solid alternative for me.\n\nIt‚Äôs open source and natively compatible with OpenTelemetry, which is a big plus. You get traces, metrics, and logs all in one place, with strong correlation between them, so debugging feels much more straightforward than jumping across tools.\n\nAnother big advantage is that since it‚Äôs Otel based, you‚Äôre not limited to just LLM calls. You can instrument and monitor your entire application like API latency, background jobs, infra metrics, etc. alongside your LLM traces. That broader context has been super useful as things get more ‚Äúreal‚Äù in production.\n\nCheck out the SigNoz LangChain Observability docs it's pretty helpful to get you started: [https://signoz.io/docs/langchain-observability/](https://signoz.io/docs/langchain-observability/)",
          "score": 0,
          "created_utc": "2026-01-05 20:02:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxy889a",
          "author": "Otherwise_Flan7339",
          "text": "We moved from LangSmith to Maxim a few months ago for exactly the cost reason you mentioned - per-trace pricing gets wild at scale.\n\nThe bigger issue for us was that LangSmith treats everything as pass/fail on the whole system. When something breaks, you know *that* it broke but not *why*. With agents, you need component-level testing - is retrieval broken? Is the LLM ignoring context? Is tool selection wrong?\n\nMaxim does workspace-based pricing instead of per-trace and has way better component isolation for debugging. You can test each part of your agent separately which makes it actually possible to fix issues instead of just knowing they exist.\n\n[https://www.getmaxim.ai](https://www.getmaxim.ai)\n\nNot saying LangSmith is bad, just didn't scale well for our use case. If you're deep in LangChain it probably still makes sense though.",
          "score": 0,
          "created_utc": "2026-01-06 04:15:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxubdk1",
          "author": "clickittech",
          "text": "if you guys want to know more about these alternative here is breakdown of LangSmith alternatives and differences  \n[https://www.clickittech.com/ai/langsmith-alternatives/](https://www.clickittech.com/ai/langsmith-alternatives/)",
          "score": -3,
          "created_utc": "2026-01-05 16:35:40",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nxuzkgz",
          "author": "OneTurnover3432",
          "text": "I‚Äôve seen the same pattern, and I agree with most of what‚Äôs being said here.\n\nIn my experience, **LangSmith** works well early on, but once agents are in real production, teams start hitting the same walls: cost scaling with traces, lots of raw data, and still no clear answer to *what‚Äôs actually hurting or improving outcomes*.\n\nMost teams I‚Äôve worked with end up stitching together:\n\n* LangSmith or something similar for dev/debug\n* And then a manual analysis when it comes to explaining behavior ‚Üí impact ‚Üí ROI\n\nThat gap is exactly why I‚Äôm building **ThinkHive**.\n\nThinkHive sits on top of traces and logs (including OTel-based setups) and focuses on:\n\n* Summarizing logs and traces into clear issue patterns instead of raw data\n* Highlighting which agent behaviors actually move business metrics (cost, deflection, resolution, quality)\n\n It‚Äôs meant to answer the question those tools don‚Äôt: *what should I fix first to improve ROI?*\n\nI‚Äôm opening a small, free beta right now for teams:\n\n* Building AI agents internally for enterprises, or\n* Deploying agents for clients as consultants or agencies\n\nIf anyone here wants early access or to sanity-check whether this fits their setup, feel free to DM me. Happy to share and get feedback from people actually in the trenches.",
          "score": -3,
          "created_utc": "2026-01-05 18:26:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxx0qq6",
              "author": "ctwillie77",
              "text": "This looks really good and promising! Seems the year is static at the bottom of your site: ¬© 2025 ThinkHive. All rights reserved.",
              "score": 0,
              "created_utc": "2026-01-06 00:16:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzierz",
      "title": "I built a lightweight, durable full stack AI orchestration framework",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1pzierz/i_built_a_lightweight_durable_full_stack_ai/",
      "author": "Worried_Market4466",
      "created_utc": "2025-12-30 13:41:19",
      "score": 9,
      "num_comments": 5,
      "upvote_ratio": 0.91,
      "text": "Hello everyone,\n\nI've been building agentic webapps for around a year and a half now. Started with loops, then moved onto langgraph + Assistant UI. I've been using the lang ecosystem since their launch and have seen their evolution.\n\nIt's great and easy to build agents, but things got really frustrating once I needed more fine grained control, especially has a hard time building interesting user experiences. I loved the idea of building agents as DAGs, but I really wanted to model UIs in my flow as nodes too. \n\nDeployment was another nightmare. I am kinda cheap and the per node executed tax seemed ... Well, not great. But hey, the devs gotta eat.\n\n\nAround six months back, I snapped and started working on an idea i had been throwing around for a while. It's called Cascaide.\n\nCascaide is a lightweight low level AI orchestration framework written in typescript designed to run anywhere JS/TS can. It is primarily built for web applications. However, you can create headless AI agents and workflows with it in Node.js.\n\nHere are the reasons why you should try it out. We are in the process of opensourcing it(probably Jan first week).\n\nDeveloper Experience and UX\n\nüç± Learn Fast ‚Äì Simple, powerful abstractions you can learn over lunch\n\nüé® Build UI First ‚Äì UI and human-in-the-loop support is natural, not an add-on\n\nüèéÔ∏è Build Fast ‚Äì Single codebase (if you choose), no context switching\n\n‚è≥ Debug Easily ‚Äì Debugging and time-travel out of the box\n\nüåç Deploy Anywhere ‚Äì Deploy like any other application, no caveats\n\nü™∂ Stay Light ‚Äì Tiny bundle size, small enough to actually understand\n\nüîÆ UX Possibilities ‚Äì Enables novel UX patterns beyond chatbots: smart components, AI workflow visualization, and dynamic portalling\n\nüîå Extensibility ‚Äì Easily extend for custom capabilities via middleware patterns\n\nüßë‚ÄçüíªStack Agnostic ‚Äì Use with your favorite stack\n\nCosts\n\nZero orchestration costs in production \n\nLow TCO - far less moving parts to maintain\n\nTalent pool: enable any web dev to easily transition to AI engineering.\n\nObservability and reliability \n\n\nDurability: enterprise grade durability with no new overhead. Resume workflows post server/client crashes easily, or pick up weeks or months later.\n\nObservability and control: full observability out of the box with easy timetravel rollback and forking\n\n\nI have two production apps running on it and it's working great for us. It's very easy to use with serverless as well.\n\nI would love to talk to devs and get some feedback. We can do an early sneek peek!\n\nCheers!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1pzierz/i_built_a_lightweight_durable_full_stack_ai/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nwqjlwz",
          "author": "hyma",
          "text": "is this a competitor to langgraph?",
          "score": 2,
          "created_utc": "2025-12-30 14:43:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqla0m",
              "author": "Worried_Market4466",
              "text": "Yes, it's an alternative. Especially if you've been struggling with UI integrations, deployment/pricing and prefer to work with TS. A great use case would be AI SaaS.",
              "score": 1,
              "created_utc": "2025-12-30 14:52:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwt1dzz",
                  "author": "Plaszz",
                  "text": "Interesting!",
                  "score": 1,
                  "created_utc": "2025-12-30 21:50:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwu75h4",
          "author": "Live-Guitar-8661",
          "text": "Sounds interesting, we are building something similar. Would love to chat if you are up for it.",
          "score": 1,
          "created_utc": "2025-12-31 01:34:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww9ud3",
          "author": "Worried_Market4466",
          "text": "Opensourcing on 3rd January!",
          "score": 1,
          "created_utc": "2025-12-31 11:00:18",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbdpo3",
      "title": "Plano v0.4.2: universal v1/responses + Signals (trace sampling for continuous improvement)",
      "subreddit": "LangChain",
      "url": "https://i.redd.it/hgdjg0gdo0dg1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2026-01-13 01:22:52",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Announcement",
      "permalink": "https://reddit.com/r/LangChain/comments/1qbdpo3/plano_v042_universal_v1responses_signals_trace/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q3t915",
      "title": "How can I develop an agent skill system on top of LangChain 1.0",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q3t915/how_can_i_develop_an_agent_skill_system_on_top_of/",
      "author": "GardenOwn1917",
      "created_utc": "2026-01-04 15:50:20",
      "score": 8,
      "num_comments": 11,
      "upvote_ratio": 0.79,
      "text": "How can I develop an agent skill system on top of LangChain 1.0 toolset to replace tools, and enable the agent to automatically unload and load these tools? How should I design the prompts for this? Can anyone share their approach?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q3t915/how_can_i_develop_an_agent_skill_system_on_top_of/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nxnepq3",
          "author": "vtrivedy-lc",
          "text": "Hey op, our deepagents repo is built on the 1.0 create_agent() and we have an integrated SkillsMiddleware in the deepagents-cli that supports adding skills [here](https://github.com/langchain-ai/deepagents/blob/master/libs/deepagents-cli/deepagents_cli/skills/middleware.py)\n\nHope that‚Äôs a good example to get started with and understand how skill building/loading works.  It uses the latest AgentSkills spec.\n\nWe also have this [blog](https://blog.langchain.com/using-skills-with-deep-agents/) on Using Skills with DeepAgents that walks through it.  Hope this helps, lmk if there‚Äôs any way we can help as you build!",
          "score": 7,
          "created_utc": "2026-01-04 16:41:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxnta4l",
              "author": "cmndr_spanky",
              "text": "Is this basically the same thing as ‚Äútool selector‚Äù tool for the agent and a ‚Äútool executor‚Äù ? The first can return a short list / descriptions of available tools / skills, the second is how it executes one.  Other than semantics, if implemented this way, is there really any difference between a tool and a skill? A tool‚Äôs docstring is effectively merged into the prompt, and the function itself can contain whatever code you want the LLM to be able to execute.",
              "score": 1,
              "created_utc": "2026-01-04 17:48:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxnwriz",
                  "author": "vtrivedy-lc",
                  "text": "it's a bit different.  Skills are bundles of capabilities (md files, scripts to run, anything really).  The Middleware adds the frontmatter to the prompt (kind of like a tool\\_description), but the agent access all of the additional instructions and scripts in the skill folder, these are primarily executed via the agent using bash.  Less bloat than bundling all tools into the context.  So if you design your the function of your tools as scripts in the Skill, the agent can still discover and run them when needed.\n\nWhat's your use case?  Trying to understand the exact flow so can make a good suggestion.",
                  "score": 1,
                  "created_utc": "2026-01-04 18:04:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxp6qo1",
          "author": "pbalIII",
          "text": "The key distinction u/cmndr_spanky raised is worth clarifying... skills are higher-order than tools. A tool is a single function with a docstring. A skill bundles prompts, scripts, and context that the agent can discover and execute on demand.\n\nFor the mode-switching you're describing, LLMToolSelectorMiddleware is probably what you want. It runs before the main model call and filters down to relevant tools based on query context. Combine that with domain-specific prompt templates in a custom before_model hook and you get something close to skill switching without the multi-agent overhead.\n\nOne gotcha: dynamic tool addition after agent creation isn't supported yet. All tools need to be registered upfront, middleware just filters the active set per turn.",
          "score": 2,
          "created_utc": "2026-01-04 21:31:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpkb1s",
              "author": "cmndr_spanky",
              "text": "Makes sense, and I‚Äôm not trying to be argumentative here, but if a skill is a ‚Äúbundle of prompts, scripts, and context‚Äù how is that different from an agent conditionally accessing a sub-agent? This would seem (to me) like a preferred architecture if prompts and non-deterministic decisioning is a necessary part of that ‚Äúbundle‚Äù.. also you can use a cheaper (or conversely more powerful) LLM with that sub-agent depending on the complexity of what it should do and how much context it‚Äôs going to manage.",
              "score": 1,
              "created_utc": "2026-01-04 22:34:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxq73h8",
                  "author": "pbalIII",
                  "text": "You're right that the line gets blurry. The main difference is overhead and state management. With sub-agents, each invocation spins up a new context window, manages its own conversation state, and has full autonomy to make tool calls. That's powerful but expensive... you pay for the orchestration layer, the handoff logic, and the latency of multiple LLM roundtrips. Skills sit lower in the stack. They're more like parameterized recipes the parent agent executes directly. No separate context window, no independent decision-making. The parent stays in control and just runs the bundled instructions. The tradeoff: sub-agents give you isolation and flexibility (different models, independent error handling). Skills give you speed and predictability. If your bundle needs to make multiple LLM calls with its own reasoning, sub-agent makes sense.",
                  "score": 1,
                  "created_utc": "2026-01-05 00:26:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxn57ld",
          "author": "adlx",
          "text": "Do you mean a first step does select some most  relevant tools from a listbof (possibly many) available tools and then offers them to the next llm call as tools?\n\nI'd say first is a chain with structured output to return list of selected tools. \n\nThen just bind those selected tools for the llm to use in next calls/graph",
          "score": 1,
          "created_utc": "2026-01-04 15:57:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxn6d7c",
              "author": "GardenOwn1917",
              "text": "Because the tools on my side can be divided into collections from different domains, the current architecture is a multi-agent architecture. I would prefer a single-agent implementation that can load prompt templates from different domains without needing retrieval‚Äîinstead, something similar to mode switching.",
              "score": 1,
              "created_utc": "2026-01-04 16:02:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxn8eq9",
                  "author": "nate_f",
                  "text": "You might consider langchain.agents.middleware.dynamic_prompt and the tool selector middleware. https://docs.langchain.com/oss/python/langchain/middleware/built-in#llm-tool-selector",
                  "score": 1,
                  "created_utc": "2026-01-04 16:12:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q1y63d",
      "title": "I wrote a beginner-friendly explanation of how Large Language Models work",
      "subreddit": "LangChain",
      "url": "https://blog.lokes.dev/how-large-language-models-work",
      "author": "Feisty-Promise-78",
      "created_utc": "2026-01-02 13:07:59",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1q1y63d/i_wrote_a_beginnerfriendly_explanation_of_how/",
      "domain": "blog.lokes.dev",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pzpltc",
      "title": "Building AI agents that actually learn from you, instead of just reacting",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1pzpltc/building_ai_agents_that_actually_learn_from_you/",
      "author": "Nir777",
      "created_utc": "2025-12-30 18:25:02",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Just added a brand new tutorial about Mem0 to my \"Agents Towards Production\" repo. It addresses the \"amnesia\" problem in AI, which is the limitation where agents lose valuable context the moment a session ends.\n\nWhile many developers use standard chat history or basic RAG, Mem0 offers a specific approach by creating a self-improving memory layer. It extracts insights, resolves conflicting information, and evolves as you interact with it.\n\nThe tutorial walks through building a Personal AI Research Assistant with a two-phase architecture:\n\n* Vector Memory Foundation: Focusing on storing semantic facts. It covers how the system handles knowledge extraction and conflict resolution, such as updating your preferences when they change.\n* Graph Enhancement: Mapping explicit relationships. This allows the agent to understand lineage, like how one research paper influenced another, rather than just finding similar text.\n\nA significant benefit of this approach is efficiency. Instead of stuffing the entire chat history into a context window, the system retrieves only the specific memories relevant to the current query. This helps maintain accuracy and manages token usage effectively.\n\nThis foundation helps transform a generic chatbot into a personalized assistant that remembers your interests, research notes, and specific domain connections over time.\n\nPart of the collection of practical guides for building production-ready AI systems.\n\nCheck out the full repo with 30+ tutorials and give it a ‚≠ê if you find it useful:[https://github.com/NirDiamant/agents-towards-production](https://github.com/NirDiamant/agents-towards-production)\n\nDirect link to the tutorial:[https://github.com/NirDiamant/agents-towards-production/blob/main/tutorials/agent-memory-with-mem0/mem0\\_tutorial.ipynb](https://github.com/NirDiamant/agents-towards-production/blob/main/tutorials/agent-memory-with-mem0/mem0_tutorial.ipynb)\n\nHow are you handling long-term context? Are you relying on raw history, or are you implementing structured memory layers?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1pzpltc/building_ai_agents_that_actually_learn_from_you/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pys6wv",
      "title": "Implementing Production-Grade Human-in-the-Loop (HITL) with LangGraph for Sensitive Workflows",
      "subreddit": "LangChain",
      "url": "https://rampakanayev.com/blog/langgraph-human-in-the-loop",
      "author": "No-Conversation-8984",
      "created_utc": "2025-12-29 17:23:45",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1pys6wv/implementing_productiongrade_humanintheloop_hitl/",
      "domain": "rampakanayev.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qb1vuh",
      "title": "AI testing resources that actually helped me get started with evals",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qb1vuh/ai_testing_resources_that_actually_helped_me_get/",
      "author": "PurpleWho",
      "created_utc": "2026-01-12 17:53:11",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 0.91,
      "text": "Spent the last few months figuring out how to test AI features properly. Here are the resources that actually helped, plus the lesson none of them taught me.\n\n- [**Anthropic's Prompt Eval Course**](https://github.com/anthropics/courses/blob/master/prompt_evaluations/README.md) - Most practical of the bunch. Hands-on exercises, not just theory.\n\n- **[Hamel's LLM Evals FAQ](https://hamel.dev/blog/posts/evals-faq)** - Covers the common questions everyone has but is afraid to ask.\n\n- **[DeepLearning's Evaluation and Monitoring Courses](https://www.deeplearning.ai/courses/)** - Whole category of free courses. Good for building foundational understanding.\n\n- **Lenny's \"[Beyond Vibe Checks: A PM's Complete Guide to Evals](https://www.lennysnewsletter.com/p/beyond-vibe-checks-a-pms-complete)\"** - Best written explanation of when and why to use evals.\n\n### Paid Resources (if you want to go deeper):\n\n- **Hamel Husain & Shreya Shankar's \"[AI Evals for Engineers & PMs](https://maven.com/parlance-labs/evals)\"** - Comprehensive. Worth it if you're doing this seriously.\n\n- **\"[Go from Zero to Eval](https://forestfriends.tech/)\" by Sridatta & Wil** - Heavy on examples, which is what I needed.\n\n### What every resource skips:\n\nBefore you can run any evaluations, you need test cases. And LLMs are terrible at generating realistic ones for your specific use case.\n\nI tried Claude Console to bootstrap scenarios - they were generic and missed actual edge cases. Asking an LLM \"give me 50 test cases\" just gives you 50 variations on the happy path or just the most obvious edge cases.\n\n**What actually worked:**\n\nBuilding my test dataset manually:\n- Someone uses the feature wrong? Test case.\n- Weird edge case while coding? Test case.\n- Prompt breaks on specific input? Test case.\n\nThe bottleneck isn't running evals - it's capturing these moments as they happen.\n\n**My current setup:**\n\nCSV file with test scenarios + test runner in my code editor. That's it.\n\nTried VS Code's AI Toolkit first (works, but felt pushy about Microsoft's paid services). Switched to an open-source extension called Mind Rig - same functionality, simpler. Basically, they save a fixed batch of test inputs so I can re-run the same data set each time I tweak a prompt.\n\n1. Start with test dataset, not eval infrastructure\n2. Capture edge cases as you build\n3. Test iteratively in normal workflow\n4. Graduate to formal evals at 100+ cases (PromptFoo, PromptLayer, Langfuse, Arize, Braintrust, Langwatch, etc)\n\nThe resources above are great for understanding evals. But start by building your test dataset first, or you'll just spend all your time setting up sophisticated infrastructure for nothing.\n\nAnyone else doing AI testing? What's your workflow?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qb1vuh/ai_testing_resources_that_actually_helped_me_get/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nzkjxmh",
          "author": "pbalIII",
          "text": "Capturing cases while you build is the only part that scales. I keep a tiny gold set plus a nasty set, and I add to it every time something surprises me in prod.\n\n- Store input, expected shape, and a short rubric note\n- Tag the failure mode so you can slice later\n- Re-run on every prompt change and in CI\n\nOnce you hit 100ish cases, the fancy tooling finally has something real to measure.",
          "score": 1,
          "created_utc": "2026-01-14 17:12:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzn3y9m",
              "author": "PurpleWho",
              "text": "Nasty set. I love it.   \n  \nYeah, my gold + nasty are all mixed together.\n\nThe other thing I do, though, is I have a set of things the LLM can't do yet. So a lot of the time, problems are either specification issues or changes that need to be made to the context/architecture. But now and then, I run up against an edge case that the LLM just can't handle. I keep those in a separate set so that when the next fancy model release comes out, I can run this can't-do-yet set and see if the new model unlocks any new capability for us.",
              "score": 2,
              "created_utc": "2026-01-15 00:24:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o00f6i7",
                  "author": "pbalIII",
                  "text": "Great call on the can't-do-yet set. Will start tagging the hardest edge cases to measure model improvements.",
                  "score": 1,
                  "created_utc": "2026-01-16 23:03:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qd4k1f",
      "title": "Number of LLM calls in agentic systems",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qd4k1f/number_of_llm_calls_in_agentic_systems/",
      "author": "usernotfoundo",
      "created_utc": "2026-01-15 00:29:21",
      "score": 8,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "I don't know if I am phrasing this correctly but I am kind of confused about how proper agentic systems are made but I'll try, hopefully someone understands.\n\nWhenever I see something like Claude Code, Copilot or even ChatGPT and read their \"thinking\" part it seems like they generate something, reason over it, generate something else, again \"reason\", and repeat.\n\nBasically from a developer's(just a student so don't have experience with production grade systems) perspective it seems like if I want to make something like that it would require a lot of continuous call to the llm's api for each reasoning step and this isn't possible with just a single api call. Is that actually what's happening? Are there multiple api calls involved and it's not a fixed number i.e. could be 2 , could end up being 4/5? \n\nAdditional questions:\n\n1. Wouldn't this be very expensive to develop with the llm api call charges stacking?\n\n2. What about getting rate limited, with just one use of the agent requiring multiple api calls and having many users for the application?\n\n3. Wouldn't monitoring and debugging be very difficult in this case where you have multiple api calls and there could end up being an error(rate limit, hallucinaton) at any call? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qd4k1f/number_of_llm_calls_in_agentic_systems/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nznqaqu",
          "author": "jeronimoe",
          "text": "Agentic ai sounds all fancy, but at it‚Äôs core it‚Äôs a big for loop calling llm‚Äôs and tools.\n\nYes it, can get expensive, model choice is important. ¬†Sometimes a reasoning agent calls sub agents using cheaper or oss models, especially for summarization. ¬†But the agentic loop will pass the old conversation (or parts), and although there is input caching, it increases token usage further.\n\nYou can get rate limited, as well as exceed context. ¬†You typically build retry logic into it.\n\nMonitoring and debugging are important, there are several products built for agentic observation, some specifically for Lang graph.",
          "score": 2,
          "created_utc": "2026-01-15 02:31:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzo8rgi",
          "author": "pvatokahu",
          "text": "Capture a trace with monocle2ai from Linux foundation and it will answer that for you - you‚Äôll get the agentic spans and inference spans with relevant metadata like token counts, input/outputs, history, turn info etc",
          "score": 1,
          "created_utc": "2026-01-15 04:25:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpfj4h",
          "author": "SmoothRolla",
          "text": "Its one call to the API even if its a reasoning model, it will reason whilst generating the answer (and that counts towards the total tokens). you dont see the reasoning in the API response (or at least i dont know how to with the models i use)",
          "score": 1,
          "created_utc": "2026-01-15 10:34:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqmw86",
              "author": "usernotfoundo",
              "text": "I feel it's multiple api calls(and even some of the other comments agree) because it seems that if I use just a single api call to do the task the results are always worse compared to if I use multiple api calls where I break down the task into multiple steps and only pass relevant context.\n\nIt could be that I have only used the lower cost models and so there is such a huge difference. Are the reasoning models that much better?",
              "score": 1,
              "created_utc": "2026-01-15 15:11:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzqrvqp",
                  "author": "SmoothRolla",
                  "text": "Apologies I misunderstood yout question. Yeah you may need multiple calls. My bot is broken down I to 3 agents and the middle agent may recurse 5 or more times until it had called all the tools it needed to get the data I needs for the next agent to use to generate an answer¬†",
                  "score": 1,
                  "created_utc": "2026-01-15 15:34:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzug1p3",
          "author": "attn-transformer",
          "text": "You need to make as few llm calls as possible. Use the LLM to find out what to do, not how. \n\nUser request -> what to do (llm) -> workflow (deterministic) -> final response (llm)\n\nThis is a a bit over simplified, but start here and only add complexity as needed and carefully. \n\nDon‚Äôt put everything in a react loop by default.",
          "score": 1,
          "created_utc": "2026-01-16 02:11:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxme7w",
          "author": "adlx",
          "text": "Can't you do the math? I mean, if you didn't make the code (yet) you share can get a rough idea of where there will be an llm call involved with what content...\n\nIt can be several calls, sequential, some in parallel...",
          "score": 1,
          "created_utc": "2026-01-16 15:20:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyhegi",
          "author": "gkarthi280",
          "text": "You touch on a very good point about monitoring/observability for these AI agents. One thing that is becoming very important these days is proper observability for AI agents, especially when they go into production. Because AI agents are non-deterministic, you really have no idea what's going on under the hood. If there is latency, errors, incorrect tool calls, or even tool call loops, its very hard to know that its happening and what's causing these issues without proper observability in your AI stack. \n\nI would highly suggest looking into OpenTelemetry. It's the open source standard for observability and supports wide variety of languages and frameworks. Also, it's used for general application related telemetry as well, so you would be able to monitor your AI usage + your entire application as well, with the same framework. Additionally, because it's open source, OpenTelemetry is vendor neutral, allowing you to choose from a wide variety of OpenTelemetry compatible observability platforms, and making plug and play super straightforward.",
          "score": 1,
          "created_utc": "2026-01-16 17:37:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznu9er",
          "author": "Hot_Substance_9432",
          "text": "Yes you can use LangSmith etc for observability and also RAGAS",
          "score": 0,
          "created_utc": "2026-01-15 02:54:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qaubry",
      "title": "Battle of AI Gateways: Rust vs. Python for AI Infrastructure: Bridging a 3,400x Performance Gap",
      "subreddit": "LangChain",
      "url": "https://vidai.uk/blog/rust-python-vidai",
      "author": "Guna1260",
      "created_utc": "2026-01-12 13:04:10",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qaubry/battle_of_ai_gateways_rust_vs_python_for_ai/",
      "domain": "vidai.uk",
      "is_self": false,
      "comments": [
        {
          "id": "nz5mol3",
          "author": "pokemonplayer2001",
          "text": "Python's a toy.",
          "score": 0,
          "created_utc": "2026-01-12 13:11:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5mypm",
              "author": "Guna1260",
              "text": "Agree.. but we like to call it Tooling tier.. especially for the ecosystem.",
              "score": 1,
              "created_utc": "2026-01-12 13:12:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qadngl",
      "title": "STELLA - Simple Terminal Agent for Ubunt using local AI. Built with LangChain / Ollama",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/gallery/1qadngl",
      "author": "petyussz",
      "created_utc": "2026-01-11 22:47:50",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Announcement",
      "permalink": "https://reddit.com/r/LangChain/comments/1qadngl/stella_simple_terminal_agent_for_ubunt_using/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q8os1p",
      "title": "I built an open-source SDK for AI Agent authentication (no more hardcoded cookies)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q8os1p/i_built_an_opensource_sdk_for_ai_agent/",
      "author": "Bubbly_Gap6378",
      "created_utc": "2026-01-10 00:04:57",
      "score": 7,
      "num_comments": 2,
      "upvote_ratio": 0.82,
      "text": "I kept running into the same problem: my agents need to log into websites (LinkedIn, Gmail, internal tools), and I was hardcoding cookies like everyone else.\n\n\n\nIt's insecure, breaks constantly, and there's no way to track what agents are doing.\n\n\n\nSo I built AgentAuth - an open-source SDK that:\n\n\\- Stores sessions in an encrypted vault (not in your code)\n\n\\- Gives each agent a cryptographic identity\n\n\\- Scopes access (agent X can only access linkedin.com)\n\n\\- Logs every access for audit trails\n\n\n\nBasic usage:\n\n\\`\\`\\`python\n\nfrom agent\\_auth import Agent, AgentAuthClient\n\n\n\nagent = Agent.load(\"sales-bot\")\n\nclient = AgentAuthClient(agent)\n\nsession = client.get\\_session(\"linkedin.com\")\n\n\\`\\`\\`\n\n\n\nIt's early but it works. Looking for feedback from people building agents.\n\n\n\nGitHub: [https://github.com/jacobgadek/agent-auth](https://github.com/jacobgadek/agent-auth)\n\n\n\nWhat auth problems are you running into with your agents?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q8os1p/i_built_an_opensource_sdk_for_ai_agent/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nytweyk",
          "author": "Bubbly_Gap6378",
          "text": "OP here ‚Äî I built this in 24 hours because I was tired of my own bots breaking.\n\n**If you think this is useful or plan to use it, a Star on GitHub ‚≠êÔ∏è would really help me out.** It lets me know people actually want this maintained so I don't abandon it.",
          "score": 1,
          "created_utc": "2026-01-10 18:40:49",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nyxo87z",
          "author": "CarpMadMan",
          "text": "Looks good, will check it out, thanks.",
          "score": 1,
          "created_utc": "2026-01-11 07:17:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q615eb",
      "title": "Experiences from building enterprise agents with DSPy and GEPA",
      "subreddit": "LangChain",
      "url": "https://slavozard.bearblog.dev/experiences-from-building-enterprise-agents-with-dspy-and-gepa/",
      "author": "FreakedoutNeurotic98",
      "created_utc": "2026-01-07 01:04:25",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q615eb/experiences_from_building_enterprise_agents_with/",
      "domain": "slavozard.bearblog.dev",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q32p4h",
      "title": "Do you prefer to make Human-in-the-loop approvals on your phone or PC",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q32p4h/do_you_prefer_to_make_humanintheloop_approvals_on/",
      "author": "Antique_Try7765",
      "created_utc": "2026-01-03 18:52:36",
      "score": 7,
      "num_comments": 16,
      "upvote_ratio": 0.82,
      "text": "I am currently building an HITL system that initiates with systems but I want to understand how people prefer to make human input into their workflows or agents. >> [hitl.sh](http://hitl.sh)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q32p4h/do_you_prefer_to_make_humanintheloop_approvals_on/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nxiq316",
          "author": "Signal_Question9074",
          "text": "ok, let me save you from some hair pulling.\n\nGreat question furst if all becuase I've been building HITL into my own agent system, so I have some thoughts on this.\n\n**Short answer: PC for complex decisions, phone for quick approvals.**\n\nBut the real insight I learned: **the UX design matters more than the device.**\n\n**What I built:**\n\nMy system uses a request/response pattern (similar to how Claude Code handles it). When the agent needs human input, it doesn't truly \"pause\" mid-execution it completes its turn, returns a question marker, and the backend emits a socket event to the frontend:\n\n    // Agent returns this when it needs input\n    {\n      __askUser: true,\n      question: \"Which database should we use?\",\n      options: [\n        { label: \"PostgreSQL\", description: \"Relational, ACID compliant\" },\n        { label: \"MongoDB\", description: \"Document store, flexible schema\" }\n      ],\n      multiSelect: false\n    }",
          "score": 4,
          "created_utc": "2026-01-03 22:28:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxiq3ye",
              "author": "Signal_Question9074",
              "text": "Frontend catches this and renders clickable options. User picks one, answer goes back as a new message, agent continues with full context preserved.\n\n**Why this matters for phone vs PC:**\n\n1. **Single-select options** ‚Äî Work great on phone. Tap and done.\n2. **Multi-select or free-text** ‚Äî Better on PC. Typing on phone sucks.\n3. **Context-heavy decisions** ‚Äî PC. You need to see the conversation history.\n4. **Quick \"approve/reject\"** ‚Äî Phone. Perfect for notifications.\n\n**The two main patterns:**\n\n| Pattern | How it works | Best for |\n\n|---------|--------------|----------|\n\n| **Request/Response** | Agent completes turn, returns question, waits for answer as new message | Simpler to build, works well with existing chat UIs |\n\n| **True Interrupt** (LangGraph style) | Agent pauses mid-execution via `interrupt()`, state checkpointed, resumes with `Command(resume=...)` | More complex, but cleaner for multi-step approvals |\n\nLangGraph's `interrupt()` is cleaner architecturally execution actually pauses and resumes from the exact point. My approach re-executes with full context, which works but burns more tokens on long conversations.\n\n**What I'd recommend for your build:**\n\n1. **Start with simple options UI:** 2-4 clickable buttons covers 80% of cases\n2. **Add \"Other\" free-text fallback:** Users hate being boxed in\n3. **Persist pending questions:** If user refreshes page or switches devices, the question should still be there (I use Redis for this)\n4. **Handle timeouts gracefully:** What happens if user takes 10 minutes? Don't let the agent hang.\n\n**Honest take on phone:**\n\nI rarely approve from phone unless it's truly urgent. Context switching kills focus. But for async workflows where you're not actively watching the agent and phone notifications for \"needs approval\" ‚Üí tap to approve ‚Üí done. that flow is solid.\n\nWhat kind of decisions are you handling? Simple approve/reject, or complex multi-option stuff?",
              "score": 5,
              "created_utc": "2026-01-03 22:28:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxjkpg3",
                  "author": "Antique_Try7765",
                  "text": "Is there any out of the box solution I can use for this ?",
                  "score": 2,
                  "created_utc": "2026-01-04 01:08:32",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nxoyxs6",
                  "author": "vansterdam_city",
                  "text": "Add voice to text support for mobile and your text box issues are mitigated quite a bit.",
                  "score": 1,
                  "created_utc": "2026-01-04 20:54:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxj37vi",
          "author": "amilo111",
          "text": "Who are you building this for?  The answer to your question depends heavily on the target persona and use case.\n\nAlso what hitl interactions are you solving for?  I‚Äôve worked on applications that had hitl and we had to solve for multiple different interaction models depending on what the customer was trying to do and how we fit into their processes.",
          "score": 3,
          "created_utc": "2026-01-03 23:35:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxjkgsh",
              "author": "Antique_Try7765",
              "text": "The client is into content management and want to approve before content is automatically posted to their social media platform.",
              "score": 1,
              "created_utc": "2026-01-04 01:07:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxjrx5k",
                  "author": "pebblebypebble",
                  "text": "I‚Äôd definitely want to show them any related news stories that could shift meaning before pushing live if you want 1 click from mobile",
                  "score": 2,
                  "created_utc": "2026-01-04 01:48:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxk8ium",
          "author": "Reasonable-Life7326",
          "text": "PC all the way",
          "score": 2,
          "created_utc": "2026-01-04 03:21:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjrn6k",
          "author": "pebblebypebble",
          "text": "Phone if it is quick but mostly on my laptop‚Ä¶ either way I prefer to do any of it with audio playback at the same time. Some of it can be complicated and the audio playback as I read really helps.",
          "score": 1,
          "created_utc": "2026-01-04 01:46:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxjs8i8",
              "author": "Antique_Try7765",
              "text": "Are you available to try it and give me an honest opinion ?? >> [hitl.sh](http://hitl.sh)",
              "score": 1,
              "created_utc": "2026-01-04 01:49:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxjulg3",
                  "author": "pebblebypebble",
                  "text": "In the morning if you help me with answering some user research questions I have about professional teams working and communicating via AI. You can even send me a user onboarding test and I‚Äôll do a 30m greenfield voice of customer for that kind of a trade. My 30m for your 30m.",
                  "score": 1,
                  "created_utc": "2026-01-04 02:03:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q3mpy1",
      "title": "How can I use use-stream-react / CopilotKit without LangSmith Cloud / AgentServer (self-hosted LangGraph)?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q3mpy1/how_can_i_use_usestreamreact_copilotkit_without/",
      "author": "Other_Past_2880",
      "created_utc": "2026-01-04 10:30:10",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hey all,  \nI‚Äôm building a web app with **LangGraph** and I‚Äôm running my own backend/server.\n\nI‚Äôd like to use **LangSmith** `use-stream-react` (and possibly **CopilotKit**) to stream agent/graph updates to the React client, but the docs seem to assume **LangSmith Cloud + AgentServer**.\n\n**Question:**  \nCan [use-stream-react](https://docs.langchain.com/langsmith/use-stream-react) / CopilotKit work with a self-hosted server (no AgentServer / no LangSmith Cloud)?  \nIf yes, what does my server need to expose (SSE? specific event schema?) so the client hooks/components work?\n\nIf not, what‚Äôs the recommended way to stream LangGraph events to React in a similar experience?\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1q3mpy1/how_can_i_use_usestreamreact_copilotkit_without/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q427ua",
      "title": "Added a validation layer between my SQL agent and the database - sharing in case useful",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q427ua/added_a_validation_layer_between_my_sql_agent_and/",
      "author": "Educational_Poet_862",
      "created_utc": "2026-01-04 21:29:18",
      "score": 7,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Been building a LangChain agent that queries a Postgres database. Model is smart enough not to do anything malicious, but I wanted:\n\n1. **Explicit scope control** - define exactly which tables the agent can touch\n2. **Observability** - log when the agent tries something outside its lane\n3. **Another layer** - defense in depth alongside read-only DB creds\n\nBuilt a small validation layer:\n\n    from langchain_community.utilities import SQLDatabase\n    from proxql import Validator\n\n    db = SQLDatabase.from_uri(\"postgresql://readonly@localhost/mydb\")\n\n    validator = Validator(\n        mode=\"read_only\",\n        allowed_tables=[\"products\", \"orders\", \"categories\"]\n    )\n\n    def run_query(query: str) -> str:\n        check = validator.validate(query)\n        if not check.is_safe:\n            logger.warning(f\"Out of scope: {query} - {check.reason}\")\n            return f\"Query not allowed: {check.reason}\"\n        return db.run(query)\n\n**What it does:**\n\n- Table allowlist - hard boundary on which tables are accessible (catches subqueries, CTEs, JOINs)\n- Statement filtering - read_only mode only allows SELECT\n- Dialect-aware - uses sqlglot for Postgres/MySQL/Snowflake support\n\n**What it doesn't do:**\n\n- Replace proper DB permissions (still use a read-only user)\n- Prevent expensive queries\n- Protect against a determined attacker - it's a guardrail for mistakes, not security\n\nMostly useful for observability. When a query gets blocked, I review what the agent was trying to do - usually means my prompts need tuning.\n\n---\n\npip install proxql\n\nGitHub: https://github.com/zeredbaron/proxql\n\n---\n\nCurious what others are doing for agent scope control. Are you just trusting the model + DB permissions, or adding validation layers?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q427ua/added_a_validation_layer_between_my_sql_agent_and/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nxsby05",
          "author": "rkpandey20",
          "text": "I see a lot of people building protection layer to process action described by the agent. Like in this case, sql generated by agent.¬†\nJust wondering if age old way of creating indirection where you create a grammar equivalent to sql that you want to support, is out of fashion these days.¬†",
          "score": 1,
          "created_utc": "2026-01-05 08:43:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxy8b5b",
          "author": "pbalIII",
          "text": "Most SQL agents ship the query straight to the DB and pray. The validation layer is where you catch the expensive mistakes... malformed joins, accidental mutations, queries that'd time out in prod.\n\nLangChain has a built-in sql_db_query_checker tool but layering your own checks (schema validation, read-only enforcement, cost estimation) gives you way more control. Curious what checks you're running in yours.",
          "score": 1,
          "created_utc": "2026-01-06 04:16:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qc0x89",
      "title": "How are people managing agentic LLM systems in production?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qc0x89/how_are_people_managing_agentic_llm_systems_in/",
      "author": "Silly-Hand-9389",
      "created_utc": "2026-01-13 19:35:47",
      "score": 7,
      "num_comments": 8,
      "upvote_ratio": 0.82,
      "text": "Anyone running agentic LLM systems in production? Curious how you‚Äôre handling things once it‚Äôs more than a single prompt or endpoint.\n\nI keep running into issues around cost and token usage at the agent level, instrumentation feeling hacked on, and very little ability to manage things at runtime (budgets, guardrails, retries, steering) instead of just looking at logs after something breaks. Debugging and comparing runs also feels way harder than it should be.\n\nNot selling anything, just trying to understand what people are actually struggling with, what you‚Äôve built yourselves, and what you‚Äôd never want to maintain in-house.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qc0x89/how_are_people_managing_agentic_llm_systems_in/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nzlt7br",
          "author": "gkarthi280",
          "text": "LLM observability is essentially required these days when it comes to scaling up your AI applications and putting them into production. Because LLMs are non-deterministic, it is very hard to \"see\" whats going on under the hood between the user input and llm output. Especially when you deal with AI agents, many things can go wrong like high latency, incorrect tool calls, agent loops, tool errors, and muhc more. Without observability you're basically flying blind and it's very tough to actually gain visiblity into what's happening and what's causing specific things to degrade. \n\n  \nWith that being said, I think OpenTelemetry is the way to go, as the open source standard for observability. There are many libraries using OpenTelemetry that you can use to instrument many popular AI provider and frameworks these days like OpenInference, Traceloop, Langtrace, and OpenLIT. What makes OpenTelemetry great is it's open source aspect, allowing you to choose any vendor you want. As long as you use a OpenTelemetry compatible backend, the plug and play into your existing AI stack is very straightforward, with little to no code change. \n\n\\-",
          "score": 3,
          "created_utc": "2026-01-14 20:35:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmbpa6",
          "author": "sandman_br",
          "text": "And all answers are bots",
          "score": 3,
          "created_utc": "2026-01-14 21:59:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhqdvo",
          "author": "code_vlogger2003",
          "text": "Hey hi, im managing multi agentic architecture production. We used the langchain. We are storing every intermediate steps, scratch pad , and own infra hosted models token details etc. most importantly we didn't use any guard rails because we are providing services as a button in one of the client cores products and also all the dbs were configured only with the read only access. What's the exact problem you are facing?",
          "score": 1,
          "created_utc": "2026-01-14 05:42:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjyq5j",
              "author": "Silly-Hand-9389",
              "text": "Good question. The main issues are around attribution and control, not logging. Once you have multiple agents and multiple model calls in a single run, it‚Äôs hard to cleanly attribute cost and tokens per agent, compare one run to another, or enforce things like budgets or limits at runtime instead of after the fact. We can store all the steps, but turning that into something actionable or governable is where it breaks down. We are currently looking for some tooling that could help us get to the next level.",
              "score": 1,
              "created_utc": "2026-01-14 15:35:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nznrx6i",
                  "author": "code_vlogger2003",
                  "text": "Hey for example, if we use the create agent of langchain, it is very easy to see everything with detailed structured messages of the agent state. For every agent call you can easily checkout the input, output and cost for the run and most importantly they have the feature called a cumulative addition of the costs. For budgeting it's better to have a rough estimate of how much no input and output tokens roughly used in the entire end to end call. Once we know this we can set some hard capped limits in the llm initialisation. For validation the first step is simply start to validate with ground truth tool call traces vs inference tool call traces. I mean first compare whether for a question how many tools calls it made and what are the names of these tools. But this is one level of idea. If you go deeper, i recommend hamel methodology as follows:- \n\nHamel Husain's approach for agentic evals starts with end-to-end success (e.g., does the agent achieve the goal?), then granular step-level diagnostics like tool selection accuracy and parameter extraction.https://hamel.dev/blog/posts/evals-faq/how-do-i-evaluate-agentic-workflows.html For validation, first compare ground truth tool call traces (number of calls, tool names, params) against inference traces; extend to failure matrices mapping success states to error points.",
                  "score": 1,
                  "created_utc": "2026-01-15 02:40:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzoeuyg",
          "author": "bugtank",
          "text": "Agents for prototypes, code for full prod.",
          "score": 1,
          "created_utc": "2026-01-15 05:08:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzrhhjm",
          "author": "IntermediateSwimmer",
          "text": "I started using AgentCore Observability from AWS and honestly I'm pretty impressed with it. Might use more of the AgentCore platform. Already baked in to AWS which we use, doesn't care what LLM it is, supports CrewAI, Langchain. Made my life easier for sure, but we didn't have a whole lot of observability in the first place.\n\nIt solved my problem of not being able to explain to our CTO why the agent got stuck in a loop, hallucinated, etc that I wasn't getting out of the box with LangGraph",
          "score": 1,
          "created_utc": "2026-01-15 17:29:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkho4z",
          "author": "QuirkyCharity9739",
          "text": "This is why we created SudoDog. Check out the Dashboard (free), let us know what you think: [https://dashboard.sudodog.com/login](https://dashboard.sudodog.com/login)",
          "score": 1,
          "created_utc": "2026-01-14 17:01:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdd2ds",
      "title": "Are you using any SDKs for building AI agents?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qdd2ds/are_you_using_any_sdks_for_building_ai_agents/",
      "author": "finally_i_found_one",
      "created_utc": "2026-01-15 07:21:29",
      "score": 7,
      "num_comments": 12,
      "upvote_ratio": 0.89,
      "text": "We shipped an ai agent without using any of the agent building SDKs (openai, anthropic, google etc). It doesn't require much maintenance but time to time we find cases where it breaks (ex: gemini 3.x models needed the input in a certain fashion).\n\nI am wondering if any of these frameworks make it easy and maintainable.\n\nHere are some of our requirements:  \n\\- Integration with custom tools  \n\\- Integration with a variety of LLMs  \n\\- Fine grain control over context  \n\\- State checkpointing in between turns (or even multiple times a turn)  \n\\- Control over the agent loop (ex: max iterations)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qdd2ds/are_you_using_any_sdks_for_building_ai_agents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nzscrqd",
          "author": "saurabhjain1592",
          "text": "One pattern we‚Äôve seen is that most SDKs are optimized for building the agent loop, not for operating it once it becomes stateful and long lived.\n\nThings like checkpointing, loop control, and provider abstraction are necessary but not sufficient once agents start retrying, branching, or touching real systems. At that point, the hard problems show up around partial execution, rollback, and explaining why a step was allowed to proceed.\n\nFrameworks help you get started faster. The production pain tends to appear later, when you need control and auditability rather than more abstractions.",
          "score": 2,
          "created_utc": "2026-01-15 19:50:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzt42k3",
              "author": "caprica71",
              "text": "So is langgraph helping here or not really?",
              "score": 1,
              "created_utc": "2026-01-15 21:56:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzt926s",
                  "author": "saurabhjain1592",
                  "text": "LangGraph helps with structuring the agent loop and making execution more explicit, which is a real step forward compared to ad-hoc chains.\n\nWhere teams still tend to struggle is once that graph is long-lived and interacting with real systems. Things like enforcing permissions per step, handling partial execution and rollback, or explaining *why* a transition was allowed usually sit outside the framework itself.\n\nIn practice, it reduces complexity at build time, but you still need additional control and observability once the system is running in production.",
                  "score": 1,
                  "created_utc": "2026-01-15 22:20:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzqc7do",
          "author": "mdrxy",
          "text": "LangChain does all of this ;)",
          "score": 4,
          "created_utc": "2026-01-15 14:17:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzp68zo",
          "author": "Dan6erbond2",
          "text": "I find an SDK like Vercel's AI SDK a must. Being able to switch providers on the fly is one reason, handling input types uniformly is another. And in the case of Vercel's it also handles structured output and streaming really well. I don't see why I should have to reinvent the wheel.\n\nEdit: As for features the SDK supports custom tools, stopWhen conditions, context control and message shortening and all kinds of hooks to interrupt/change the flow. I haven't had any issues getting it to do what I need.",
          "score": 2,
          "created_utc": "2026-01-15 09:05:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqez3j",
          "author": "FormalAd7367",
          "text": "yea. my next project. will be using google‚Äôs.",
          "score": 1,
          "created_utc": "2026-01-15 14:31:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqgg6x",
          "author": "Reasonable-Life7326",
          "text": "Yeah, frameworks are a lifesaver for this stuff.",
          "score": 1,
          "created_utc": "2026-01-15 14:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzrcp06",
          "author": "eavanvalkenburg",
          "text": "Agent Framework does all this for both dotnet and python",
          "score": 1,
          "created_utc": "2026-01-15 17:08:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzrlfak",
          "author": "Sunchax",
          "text": "No, build my own per project, have used pydentic for some work.\n\nMainly got burned by langchain back in they day and got allergic since then, but maybe things are better now",
          "score": 1,
          "created_utc": "2026-01-15 17:47:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzt46mg",
              "author": "caprica71",
              "text": "Just pydantic or pydantic ai?",
              "score": 1,
              "created_utc": "2026-01-15 21:56:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzuti9v",
          "author": "abzisse",
          "text": "We tried all agent frameworks (LangChain, n8n, CrewAI and others) and did not find it easy to build agents that actually work. \"Not easy\" because there is a learning curve and they seem to add a lot of boilerplate code and then you still need to code your own tools to make the agent functional, more deterministic etc... So we decided to build [a2abase.ai](http://a2abase.ai/)¬†to make building agents (not just workflow automation) easy with access to all major LLMs, 50+ native tools and 500+ MCP servers all under 1 account.",
          "score": 1,
          "created_utc": "2026-01-16 03:26:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvr2n2",
          "author": "Worldly-Pen-8101",
          "text": "Exploring  dbos . The Lang* packages are too bloated IMO",
          "score": 1,
          "created_utc": "2026-01-16 07:24:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7uqcj",
      "title": "Research Vault ‚Äì open-source agentic research assistant with structured pattern extraction (not chunked RAG)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q7uqcj/research_vault_opensource_agentic_research/",
      "author": "prod_first",
      "created_utc": "2026-01-09 01:33:35",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 0.88,
      "text": "I built an agentic research assistant for my own workflow.  \nI was drowning in PDFs and couldn‚Äôt reliably query *across* papers without hallucinations or brittle chunking.\n\n**What it does (quickly):**  \nInstead of chunking text, it extracts structured patterns from papers.\n\nUpload paper ‚Üí extract **Claim / Evidence / Context** ‚Üí store in hybrid DB ‚Üí query in natural language ‚Üí get synthesized answers *with citations*.\n\n**Key idea**  \nStructured extraction instead of raw text chunks. Not a new concept, but I focused on production rigor and verification. Orchestrated with LangGraph because I needed explicit state + retries.\n\n**Pipeline (3 passes):**\n\n* Pass 1 (Haiku): evidence inventory\n* Pass 2 (Sonnet): pattern extraction with `[E#]` citations\n* Pass 3 (Haiku): citation verification Patterns can cite *multiple* evidence items (not 1:1).\n\n**Architecture highlights**\n\n* Hybrid storage: SQLite (metadata + relationships) + Qdrant (semantic search)\n* LangGraph for async orchestration + error handling\n* Local-first (runs on your machine)\n* Heavy testing: \\~640 backend tests, docs-first approach\n\n**Things that surprised me**\n\n* Integration tests caught \\~90% of real bugs\n* LLMs *constantly* lie about JSON ‚Üí defensive parsing is mandatory\n* Error handling is easily 10‚Äì20% of the code in real systems\n\n**Repo**  \n[https://github.com/aakashsharan/research-vault](https://github.com/aakashsharan/research-vault)\n\n**Status**  \nBeta, but the core workflow (upload ‚Üí extract ‚Üí query) is stable.  \nMostly looking for feedback on architecture and RAG tradeoffs.\n\n**Curious about**\n\n* How do you manage research papers today?\n* Has structured extraction helped you vs chunked RAG?\n* How are you handling unreliable JSON from LLMs?",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1q7uqcj/research_vault_opensource_agentic_research/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nyiqu30",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-09 02:14:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyiv2hi",
              "author": "prod_first",
              "text": "Thanks for the feedback, appreciate it.\n\n\"Might be worth logging what the raw parsed output looks like for papers where extraction seems off.\" that's a good point. Will try out with a few samples of different quality to understand any degradation due to parsing. cheers.",
              "score": 1,
              "created_utc": "2026-01-09 02:36:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q4j1vc",
      "title": "How to use postgres for checkpointer, when using langchain dev",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q4j1vc/how_to_use_postgres_for_checkpointer_when_using/",
      "author": "SearchTricky7875",
      "created_utc": "2026-01-05 11:14:38",
      "score": 6,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "I m using free version of langsmith n hosting my deep agent with agent server [https://docs.langchain.com/langsmith/agent-server#parts-of-a-deployment](https://docs.langchain.com/langsmith/agent-server#parts-of-a-deployment) everything works but my agent using inmemory for storing checkpoint store, if I configure postgres, created checkpoint tables calling with PostgresSaver.from\\_conn\\_string(DATABASE\\_URL) as checkpointer:\n\ncheckpointer.setup()\n\nbut when I pass the checkpointer to agent it says, on dev mode langchain doesnt support custom checkpointer. Do I need to buy license for it, which version of license should I buy if I need to.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1q4j1vc/how_to_use_postgres_for_checkpointer_when_using/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nxyxxur",
          "author": "Heavymetal_17",
          "text": "I have been attempting it too,\nBut found out we just have to set environment variables \nhttps://github.com/langchain-ai/langgraph/issues/5360 https://github.com/langchain-ai/langgraph/issues/6559 https://docs.langchain.com/langsmith/env-var#postgres-uri-custom https://forum.langchain.com/t/postgres-uri-custom-is-ignored-by-langgraph-runtime-runtime-always-fa‚Ä¶\n\nBut as you can see in the issues even that is not working. So I am not at all sure how to proceed.",
          "score": 1,
          "created_utc": "2026-01-06 07:33:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzm43k",
              "author": "SearchTricky7875",
              "text": "only way seems to be to use 'langgraph serve' which requires license, does langgraph also need license, if I use langgraph and dont use agent server? I am thinking to redevelop my agent using langgraph.",
              "score": 1,
              "created_utc": "2026-01-06 11:16:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxznkdk",
                  "author": "Heavymetal_17",
                  "text": "You can attempt that, but it seems that would be much of an effort, I am attempting to deploy deepagents directly through fastapi on same api signatures endpoints.",
                  "score": 1,
                  "created_utc": "2026-01-06 11:29:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxyy3pl",
          "author": "Heavymetal_17",
          "text": "One might bypass langgraph dev completely and create fastapis directly and then attempt  from there.",
          "score": 1,
          "created_utc": "2026-01-06 07:34:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py0skm",
      "title": "Relay: a proposal for framework-agnostic agent orchestration",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1py0skm/relay_a_proposal_for_frameworkagnostic_agent/",
      "author": "bumswagger",
      "created_utc": "2025-12-28 19:57:35",
      "score": 6,
      "num_comments": 7,
      "upvote_ratio": 0.88,
      "text": "You have LangGraph agents, teammate has CrewAI, another team uses custom agents. Getting them to work together sucks.\n\nProposal: agents coordinate through \"relay repos\"\n\n* Shared versioned state store\n* Agents commit outputs, read inputs from previous commits\n* Branch for parallel experimentation\n* Policies define triggers (when agent A commits, run agent B)\n* MCP for agent interface - framework agnostic\n\nIt's like git for agent collaboration instead of code collaboration.\n\nWould this actually help? What's wrong with this model?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1py0skm/relay_a_proposal_for_frameworkagnostic_agent/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nwftvl5",
          "author": "rkpandey20",
          "text": "Just wondering if you can use A2A protocol to communicate.¬†",
          "score": 3,
          "created_utc": "2025-12-28 22:13:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwh9936",
              "author": "AdditionalWeb107",
              "text": "You have to implement all the low-level logic yourself in that case.",
              "score": 1,
              "created_utc": "2025-12-29 02:54:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwhlexy",
                  "author": "rkpandey20",
                  "text": "You are right. But it is not that much. It is just Agent card besides the agent endpoint. ¬†",
                  "score": 1,
                  "created_utc": "2025-12-29 04:07:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwg7ztz",
          "author": "Khade_G",
          "text": "Interesting idea. I think the ‚Äúgit for agents‚Äù framing is directionally right, but the pain usually isn‚Äôt state storage‚Ä¶ it‚Äôs semantics + contracts.\n\nWhat would help:\n- Framework-agnostic interfaces (MCP-style)\n- Versioned artifacts so runs are reproducible\n- Branching for experimentation\n\nPotential pitfalls id see:\n- ‚ÄúShared state‚Äù becomes a junk drawer unless outputs are strongly typed\n- Triggers quickly turn into a hidden workflow engine (Airflow/Temporal vibes)\n- Merge conflicts aren‚Äôt like code‚Ä¶ agents need domain-specific conflict rules\n- Latency + debugging get ugly if everything is commit/poll/trigger\n\nSo this works only if it‚Äôs really a typed artifact registry + eventing + policies, with git-like versioning as UX‚Ä¶ not literally ‚Äúgit as the runtime.‚Äù\n\nOtherwise it risks being a clever abstraction that mostly re-implements orchestration, but harder to debug.",
          "score": 2,
          "created_utc": "2025-12-28 23:28:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwh96o3",
          "author": "AdditionalWeb107",
          "text": "Checkout: [https://github.com/katanemo/plano](https://github.com/katanemo/plano) \\- Plano is delivery infrastructure for agents, and promises to be framework-agnostic and offers support for \"agents as tools\" via MCP. It doesn't have policy-defined orchestration, but has user-trigger orchestration worfklows built in.",
          "score": 1,
          "created_utc": "2025-12-29 02:54:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwikr0w",
          "author": "Ok-Priority35",
          "text": " were actively working on this at [slashmcp.com](http://slashmcp.com)",
          "score": 1,
          "created_utc": "2025-12-29 08:48:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0uf3q",
      "title": "Is it one big agent, or sub-agents?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1q0uf3q/is_it_one_big_agent_or_subagents/",
      "author": "AdditionalWeb107",
      "created_utc": "2026-01-01 03:16:38",
      "score": 6,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "If you are building agents, are you resorting to send traffic to one agent that is responsible for all sub-tasks (via its instructions) and packaging tools intelligently - or are you using a lightweight router to define/test/update sub-agents that can handle user specific tasks.\n\nThe former is a simple architecture, but I feel its a large bloated piece of software that's harder to debug. The latter is cleaner and simpler to build (especially packaging tools) but requires a great/robust orchestration/router.\n\nHow are you all thinking about this? Would love framework-agnostic approaches because these frameworks add very little value and become an operational nightmare as you push agents to production.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1q0uf3q/is_it_one_big_agent_or_subagents/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "nx1xzxr",
          "author": "Seeking_Adrenaline",
          "text": "Start with s single prompt and tools. Grow and split as needed. Use evals to measure this and detect when to change and if you are making progress. Every additional prompt layer is a slower end result, take this into consideration.",
          "score": 3,
          "created_utc": "2026-01-01 08:37:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx44q1p",
              "author": "AdditionalWeb107",
              "text": "Managing the split feels very painful - as I am now in the business of routing and middleware.",
              "score": 0,
              "created_utc": "2026-01-01 18:12:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx58xdh",
          "author": "attn-transformer",
          "text": "Once you include more than ~8 tools then the llm may not select the right tool. This is the limitation of the llm that should drive the decision between single or multi agent.",
          "score": 2,
          "created_utc": "2026-01-01 21:36:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx593uq",
              "author": "AdditionalWeb107",
              "text": "interesting way to think about - is this practical/empirical testing or a fact that more than 8 tools and the models hallucinate?",
              "score": 0,
              "created_utc": "2026-01-01 21:37:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx5dsu0",
                  "author": "attn-transformer",
                  "text": "It‚Äôs just fact. A tool call is a decision that the llm needs to make. The more options the more confusion. Humans have this exact limitation.\n\nEmpirically after 8-10 tool choices the llm starts to make mistakes.",
                  "score": 1,
                  "created_utc": "2026-01-01 22:00:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx663tu",
          "author": "Zealousideal_Emu7912",
          "text": "Depends on how long a single agent's context gets. I find that agents handle upto 100k context length quite well these days, so I tend to keep to simple and have everything in one prompt. I only use sub-agents when there's an isolated context-intensive task with little to no follow ups for the sub-agent.",
          "score": 1,
          "created_utc": "2026-01-02 00:36:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q3a09f",
      "title": "Autonomous Manim Coder with Deepagents",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/1krmmk8t08bg1",
      "author": "Eastwindy123",
      "created_utc": "2026-01-03 23:43:28",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1q3a09f/autonomous_manim_coder_with_deepagents/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxjz7go",
          "author": "vtrivedy-lc",
          "text": "Hey this is awesome thanks for sharing!!  Would love to feature it and hear about how using DeepAgents went (and how we can make it better).  Will DM you!\n\nWas a math major in undergrad, Manim was always magic :)",
          "score": 1,
          "created_utc": "2026-01-04 02:28:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}