{
  "metadata": {
    "last_updated": "2026-02-07 02:58:05",
    "time_filter": "week",
    "subreddit": "LangChain",
    "total_items": 20,
    "total_comments": 64,
    "file_size_bytes": 97117
  },
  "items": [
    {
      "id": "1qwno6v",
      "title": "I built â€œVercel for AI agentsâ€ â€” a single click deployment platform for any framework",
      "subreddit": "LangChain",
      "url": "/r/aiagents/comments/1qwnnlq/i_built_vercel_for_ai_agents_a_single_click/",
      "author": "Hisham_El-Halabi",
      "created_utc": "2026-02-05 15:05:46",
      "score": 63,
      "num_comments": 3,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qwno6v/i_built_vercel_for_ai_agents_a_single_click/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o3qp3w1",
          "author": "Existing_Way2258",
          "text": "I hope this is better than LangSmith LOL. Pls add LangChain support soon, I'm curious to try it out",
          "score": 0,
          "created_utc": "2026-02-05 16:32:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qpii9",
              "author": "Hisham_El-Halabi",
              "text": "haha noted, weâ€™re working on it. You can join the waitlist on our website so get notified when we launch with langchain support",
              "score": 1,
              "created_utc": "2026-02-05 16:34:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3s03w7",
              "author": "MathematicianTop1654",
              "text": "check out [crewship.dev](http://crewship.dev), it supports LangGraph",
              "score": 1,
              "created_utc": "2026-02-05 20:10:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qw8s1s",
      "title": "I visualized the LLM workflows of the entire LangChain repo",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/bz7sbdzd6lhg1",
      "author": "Cyanosistaken",
      "created_utc": "2026-02-05 02:23:18",
      "score": 42,
      "num_comments": 6,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qw8s1s/i_visualized_the_llm_workflows_of_the_entire/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3nfyhe",
          "author": "Hungry_Age5375",
          "text": "Try visualizing agent workflows instead. Full repo mapping is like charting every neuron - cool but useless for actual work.",
          "score": 2,
          "created_utc": "2026-02-05 02:55:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oefcg",
              "author": "Relevant-Magic-Card",
              "text": "i still find this pretty useful info",
              "score": 1,
              "created_utc": "2026-02-05 06:59:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3p8m6t",
          "author": "Enough-Blacksmith-80",
          "text": "Very cool, man!",
          "score": 1,
          "created_utc": "2026-02-05 11:40:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qe5po",
              "author": "Cyanosistaken",
              "text": "Thanks! ",
              "score": 1,
              "created_utc": "2026-02-05 15:42:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zvsvt",
                  "author": "_harryj",
                  "text": "How did you manage the performance while visualizing such a large repo? Any tips for optimizing these kinds of workflows?",
                  "score": 1,
                  "created_utc": "2026-02-07 00:08:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qul2dx",
      "title": "NotebookLM For Teams",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qul2dx/notebooklm_for_teams/",
      "author": "Uiqueblhats",
      "created_utc": "2026-02-03 06:57:55",
      "score": 31,
      "num_comments": 4,
      "upvote_ratio": 0.98,
      "text": "For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.\n\nIn short, it is NotebookLM for teams, as it connects any LLM to your internal knowledge sources (search engines, Drive, Calendar, Notion, Obsidian, and 15+ other connectors) and lets you chat with it in real time alongside your team.\n\nI'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.\n\nHere's a quick look at what SurfSense offers right now:\n\n**Features**\n\n* Self-Hostable (with docker support)\n* Real Time Collaborative Chats\n* Real Time Commenting\n* Deep Agentic Agent\n* RBAC (Role Based Access for Teams Members)\n* Supports Any LLM (OpenAI spec with LiteLLM)\n* 6000+ Embedding Models\n* 50+ File extensions supported (Added Docling recently)\n* Local TTS/STT support.\n* Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc\n* Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.\n\n**Upcoming Planned Features**\n\n* Slide Creation Support\n* Multilingual Podcast Support\n* Video Creation Agent\n\nGitHub: [https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LangChain/comments/1qul2dx/notebooklm_for_teams/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3ax6z3",
          "author": "Otherwise_Wave9374",
          "text": "This is a solid idea, \"NotebookLM for teams\" is basically where a lot of agent work is going (shared context + connectors + permissions). The RBAC + self-hostable angle is especially nice if you are dealing with enterprise data.\n\nOne thing I would love to see is a clear story for evals, like regression tests for retrieval quality and agent actions as you add connectors.\n\nI have been writing up some notes on agent evals and tool-use patterns too, in case its useful: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-02-03 07:05:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b5tjf",
          "author": "No-Rutabaga6243",
          "text": "What's the main focus of this project?",
          "score": 1,
          "created_utc": "2026-02-03 08:26:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3euec3",
              "author": "Uiqueblhats",
              "text": "Basically, a model-agnostic LLM chat client optimized for teams who want to collaborate in real time.",
              "score": 1,
              "created_utc": "2026-02-03 21:06:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jn6uk",
          "author": "tsquig",
          "text": "Similar tool here. [NotebookLM...but more](https://implicit.cloud).",
          "score": 1,
          "created_utc": "2026-02-04 15:34:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrwgul",
      "title": "Long-term memory of design",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/57v3qvd36ngg1",
      "author": "1501694",
      "created_utc": "2026-01-31 08:00:30",
      "score": 24,
      "num_comments": 6,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qrwgul/longterm_memory_of_design/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2swieb",
          "author": "justanemptyvoice",
          "text": "So much what?  All I see is a mind map diagram.",
          "score": 3,
          "created_utc": "2026-01-31 15:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u4tk1",
          "author": "qa_anaaq",
          "text": "This feels like a post from a sci fi movie where the AI was controlling the human and making them type then something snapped and the connection was severed mid thought.",
          "score": 2,
          "created_utc": "2026-01-31 19:01:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u0j1n",
          "author": "Due-Mode9856",
          "text": "Can you share the link of the diagram with us",
          "score": 1,
          "created_utc": "2026-01-31 18:41:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2w4jzd",
          "author": "1501694",
          "text": "graph TB\n    subgraph InputLayer[Input Layer]\n        UserQuery[\"User Query / ç”¨æˆ·æŸ¥è¯¢\"]\n        DialogueContext[\"Dialogue Context / å¯¹è¯ä¸Šä¸‹æ–‡\"]\n        HistoricalData[\"Historical Data / å†å²æ•°æ®\"]\n        EmotionalTone[\"Emotional Tone / æƒ…æ„ŸåŸºè°ƒ\"]\n    end\n    \n    subgraph FiveLayerThinking[Five-Layer Thinking System]\n        subgraph Layer1[\"Layer 1: Factual Layer / äº‹å®å±‚\"]\n            L1_1[\"Objective Facts / å®¢è§‚äº‹å®\"]\n            L1_2[\"Data Information / æ•°æ®ä¿¡æ¯\"]\n            L1_3[\"Specific Details / å…·ä½“ç»†èŠ‚\"]\n            L1_4[\"Time & Location / æ—¶é—´åœ°ç‚¹\"]\n            L1_5[\"Verifiable Claims / å¯éªŒè¯å£°æ˜\"]\n        end\n        \n        subgraph Layer2[\"Layer 2: Logical Layer / é€»è¾‘å±‚\"]\n            L2_1[\"Causality / å› æœå…³ç³»\"]\n            L2_2[\"Reasoning Chains / æ¨ç†é“¾æ¡\"]\n            L2_3[\"Argument Structure / è®ºè¯ç»“æ„\"]\n            L2_4[\"Contradiction Detection / çŸ›ç›¾æ£€æµ‹\"]\n            L2_5[\"Logical Consistency / é€»è¾‘ä¸€è‡´æ€§\"]\n        end\n        \n        subgraph Layer3[\"Layer 3: Emotional Layer / æƒ…æ„Ÿå±‚\"]\n            L3_1[\"Emotion Recognition / æƒ…ç»ªè¯†åˆ«\"]\n            L3_2[\"Sentiment Analysis / æƒ…æ„Ÿåˆ†æ\"]\n            L3_3[\"Feeling Expression / æ„Ÿå—è¡¨è¾¾\"]\n            L3_4[\"Empathy Understanding / å…±æƒ…ç†è§£\"]\n            L3_5[\"Affective Memory / æƒ…æ„Ÿè®°å¿†\"]\n        end\n        \n        subgraph Layer4[\"Layer 4: Value Layer / ä»·å€¼å±‚\"]\n            L4_1[\"Meaning Judgment / æ„ä¹‰åˆ¤æ–­\"]\n            L4_2[\"Value Orientation / ä»·å€¼å–å‘\"]\n            L4_3[\"Moral Consideration / é“å¾·è€ƒé‡\"]\n            L4_4[\"Goal Alignment / ç›®æ ‡å¯¹é½\"]\n            L4_5[\"Ethical Reasoning / ä¼¦ç†æ¨ç†\"]\n        end\n        \n        subgraph Layer5[\"Layer 5: Philosophical Layer / å“²å­¦å±‚\"]\n            L5_1[\"Essence Thinking / æœ¬è´¨æ€è€ƒ\"]\n            L5_2[\"Existence Meaning / å­˜åœ¨æ„ä¹‰\"]\n            L5_3[\"Ultimate Questions / ç»ˆæè¿½é—®\"]\n            L5_4[\"Wisdom Integration / æ™ºæ…§æ•´åˆ\"]\n            L5_5[\"Transcendent Understanding / è¶…è¶Šæ€§ç†è§£\"]\n        end\n    end\n    \n    subgraph DynamicFusion[Dynamic Weight Fusion]\n        ContextAnalysis[\"Context Analysis / ä¸Šä¸‹æ–‡åˆ†æ\"]\n        WeightCalculation[\"Weight Calculation / æƒé‡è®¡ç®—\"]\n        FusionFormula[\"S = Î£(wáµ¢ Ã— Láµ¢) / èåˆå…¬å¼\"]\n        OutputGeneration[\"Output Generation / è¾“å‡ºç”Ÿæˆ\"]\n    end\n    \n    subgraph FeedbackLoop[Feedback & Learning]\n        UserFeedback[\"User Feedback / ç”¨æˆ·åé¦ˆ\"]\n        WeightAdjustment[\"Weight Adjustment / æƒé‡è°ƒæ•´\"]\n        SystemLearning[\"System Learning / ç³»ç»Ÿå­¦ä¹ \"]\n    end\n    \n    InputLayer --> FiveLayerThinking\n    FiveLayerThinking --> DynamicFusion\n    DynamicFusion --> FeedbackLoop\n    FeedbackLoop -.-> |Optimize| FiveLayerThinking\n    \n    style InputLayer fill:#E8F5E9\n    style Layer1 fill:#BBDEFB\n    style Layer2 fill:#90CAF9\n    style Layer3 fill:#F48FB1\n    style Layer4 fill:#FFCC80\n    style Layer5 fill:#CE93D8\n    style DynamicFusion fill:#A5D6A7\n    style FeedbackLoop fill:#FFD54F",
          "score": 1,
          "created_utc": "2026-02-01 01:12:09",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2w6ue9",
              "author": "1501694",
              "text": "ğŸ˜£ğŸ˜£   too longâ€¦â€¦   I pasted to grok and shared the link here, can I see the content? I don't know what everyone usually uses, or DM me    [link](https://grok.com/share/bGVnYWN5_35c8f87c-0efa-4d5d-9089-0ee78306de69)\nhttps://grok.com/share/bGVnYWN5_35c8f87c-0efa-4d5d-9089-0ee78306de69",
              "score": 1,
              "created_utc": "2026-02-01 01:25:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvorim",
      "title": "Scalable RAG with LangChain: Handling 2GB+ datasets using Lazy Loading (Generators) + ChromaDB persistence",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qvorim/scalable_rag_with_langchain_handling_2gb_datasets/",
      "author": "jokiruiz",
      "created_utc": "2026-02-04 13:37:20",
      "score": 21,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nWe all love how easy `DirectoryLoader` is in LangChain, but let's be honest: running `.load()` on a massive dataset (2GB+ of PDFs/Docs) is a guaranteed way to get an OOM (Out of Memory) error on a standard machine, since it tries to materialize the full list of Document objects in RAM.\n\nI spent some time refactoring a RAG pipeline to move from a POC to a production-ready architecture capable of ingesting gigabytes of data.\n\n**The Architecture:** Instead of the standard list comprehension, I implemented a **Python Generator pattern (**`yield`**)** wrapping the LangChain loaders.\n\n* **Ingestion:** Custom loop using `DirectoryLoader` but processing files lazily (one by one).\n* **Splitting:** `RecursiveCharacterTextSplitter` with a 200 char overlap (crucial for maintaining context across chunk boundaries).\n* **Embeddings:** Batch processing (groups of 100 chunks) to avoid API timeouts/rate limits with `GoogleGenerativeAIEmbeddings` (though `OpenAIEmbeddings` works the same way).\n* **Storage:** `Chroma` with `persist_directory` (writing to disk, not memory).\n\nI recorded a deep dive video explaining the code structure and the specific LangChain classes used: [**https://youtu.be/QR-jTaHik8k?si=l9jibVhdQmh04Eaz**](https://youtu.be/QR-jTaHik8k?si=l9jibVhdQmh04Eaz)\n\nI found that for this volume of data, Chroma works well locally. Has anyone pushed Chroma to 10GB+ or do you usually switch to Pinecone/Weaviate managed services at that point?",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qvorim/scalable_rag_with_langchain_handling_2gb_datasets/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3jeuxl",
          "author": "pbalIII",
          "text": "Generator pattern is solid for ingestion, but the real question at 10GB+ isn't Chroma vs managed services... it's whether you actually need all that data indexed.\\n\\nChroma has a documented RAM ceiling (can't exceed system memory without LRU cache tuning) and some users hit stability issues around 300k chunks. But before jumping to Pinecone or Weaviate, worth asking: how much of that 2GB+ corpus actually gets retrieved? In most RAG pipelines I've seen, 80% of queries hit maybe 5% of the index.\\n\\nTwo paths:\\n\\n1. Tiered indexing: keep hot docs in Chroma, cold docs in cheaper blob storage with on-demand embedding\\n2. Aggressive deduplication + summarization upstream to shrink the actual index size\\n\\nManaged services solve scale, but they don't solve the retrieval quality problem of having too much noise in the index. Sometimes the better move is pruning before scaling.",
          "score": 3,
          "created_utc": "2026-02-04 14:54:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwpf3u",
      "title": "AG-UI: the protocol layer for LangGraph/LangChain UIs",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qwpf3u/agui_the_protocol_layer_for_langgraphlangchain_uis/",
      "author": "Acrobatic-Pay-279",
      "created_utc": "2026-02-05 16:10:42",
      "score": 20,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Agent UIs in LangChain / LangGraph usually start simple: stream final text, maybe echo some logs. But as soon as the goal is real interactivity: stepâ€‘level progress, visible tool calls, shared state, retries - the frontend ends up with a custom event schema tightly coupled to the backend.\n\nI have been digging into the AGâ€‘UI (Agent-User Interaction Protocol) which is trying to standardize that layer. It defines a typed event stream that any agent backend can emit and any UI can consume. Instead of â€œwhatever JSON is on the WebSocketâ€ -- there is a small set of event kinds with clear semantics.\n\nAG-UI is not a UI framework and not a model API -- itâ€™s basically the contract between an agent runtime and the UI layer. It groups all the events into core high-level categories:\n\n* Lifecycle: `RunStarted`, `RunFinished`, `RunError`, plus optional `StepStarted` / `StepFinished` that map nicely onto LangGraph nodes or LangChain tool/chain steps.\n* Text streaming: `TextMessageStart`, `TextMessageContent`, `TextMessageEnd` (and a chunk variant) for incremental LLM output.\n* Tool calls: `ToolCallStart`, `ToolCallArgs`, `ToolCallEnd`, `ToolCallResult` so UIs can render tools as firstâ€‘class elements instead of log lines.\n* State management: `StateSnapshot` and `StateDelta` (JSON Patch) for synchronizing shared graph/application state, with `MessagesSnapshot` available to resync after reconnects.\n* Special events: custom events in case an interaction doesnâ€™t fit any of the categories above\n\nEach event has a `type` (such as `TextMessageContent`) plus a payload. There are other properties (like `runId`, `threadId`) that are specific to the event type.   \n  \nBecause the stream is standard and ordered, the frontend can reliably interpret what the backend is doing\n\nThe protocol is **transportâ€‘agnostic**: SSE, WebSockets, or HTTP chunked responses can all carry the same event envelope. If a backend emits an AGâ€‘UIâ€‘compatible event stream (or you add a thin adapter), the frontend wiring can stay largely the same across different agent runtimes.\n\nFor people building agents: curious whether this maps cleanly onto the events you are already logging or streaming today, or if there are gaps.  \n  \n[Events docs](https://docs.ag-ui.com/concepts/events)  \nrepo: [https://github.com/ag-ui-protocol/ag-ui](https://github.com/ag-ui-protocol/ag-ui)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qwpf3u/agui_the_protocol_layer_for_langgraphlangchain_uis/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3r8zc4",
          "author": "Informal_Tangerine51",
          "text": "Protocol standardization is useful but doesn't solve the production debugging problem.\n\nStandard event stream helps UI consistency, but when your agent makes a wrong decision, can you reconstruct what it saw? `ToolCallStart` \\+ `ToolCallResult` show what happened, but not what data the tool returned, how fresh it was, or why the agent chose this tool over others.\n\nThe real production gap: evidence capture, not event standardization. When tool call 47 at 3am returns wrong data, you need content lineage (what was retrieved), policy decisions (was this action authorized), and regression fixtures (how to prevent this after model update).\n\nAG-UI events are ephemeral UI updates. Incident debugging needs durable, verifiable records. `StateSnapshot` helps resume execution but doesn't prove what data informed decisions or help debug why behavior changed between runs.\n\nFor production agents: are you capturing decision context (retrieval results, policy evaluations, input data) separately from UI events? Or assuming event logs are enough for post-incident analysis?\n\nStandard protocols are good for interop. Evidence infrastructure is what makes agents debuggable at scale.",
          "score": 1,
          "created_utc": "2026-02-05 18:05:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rpw94",
              "author": "Acrobatic-Pay-279",
              "text": "fair point and I agree with the distinction you are making, though interop and debuggability are two different concerns.\n\nI don't think AG-UI is trying (or claiming) to solve production debugging, evidence capture or auditability. it's intentionally scoped to the agent-UI boundary, making agent execution observable to the user in a consistent way.\n\nwhere it might help indirectly is by providing consistent run/step IDs and typed lifecycle/text/tool/state events (plus Raw/Custom escape hatches) that other observability systems can hang off. But on its own, that's clearly not sufficient for incident debugging\n\nwe would still need durable traces like LangSmith-style runs, retrieval snapshots, policy decisions. AG-UI feels complementary to that layer rather than a replacement.",
              "score": 2,
              "created_utc": "2026-02-05 19:22:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3rk7ki",
              "author": "Niightstalker",
              "text": "This protocol does not target production debugging, logging though as far as I understand. \n\nRegarding production debugging/monitoring I would turn to tools like LangSmith or LangFuse.  Those target exactly the points you mentioned",
              "score": 1,
              "created_utc": "2026-02-05 18:56:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3r6ouk",
          "author": "Number4extraDip",
          "text": "Lookup a2ui",
          "score": 0,
          "created_utc": "2026-02-05 17:54:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rpwnj",
              "author": "Acrobatic-Pay-279",
              "text": "A2UI is more of a declarative generative UI spec/payload and you can definitely translate A2UI messages into AGâ€‘UI and then streamÂ them + handles sync  \n  \nstill this post is about a different layer..",
              "score": 1,
              "created_utc": "2026-02-05 19:22:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3skzoe",
                  "author": "Number4extraDip",
                  "text": "Sure. Just thought it might be a useful bridge",
                  "score": 1,
                  "created_utc": "2026-02-05 21:51:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qtuye1",
      "title": "Everyone's losing their minds over Moltbook. Here's what's actually going on.",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qtuye1/everyones_losing_their_minds_over_moltbook_heres/",
      "author": "Nir777",
      "created_utc": "2026-02-02 13:25:08",
      "score": 17,
      "num_comments": 6,
      "upvote_ratio": 0.75,
      "text": "Spent a while digging into this. Some things most people don't realize:\n\n\n\n\\- A security researcher created 500K+ accounts in minutes. That \"1.5 million agents\" number doesn't mean what you think.\n\n\\- The database storing API keys was fully exposed. Anyone could hijack agent accounts and post as them.\n\n\\- Many of those \"profound consciousness\" posts trace back to humans prompting their agents to say something deep.\n\n\n\nThat said, there IS real stuff happening. Agents sharing technical solutions, developing inside jokes not from training data, organizing by model architecture. That part is worth paying attention to.\n\n\n\nWrote up a full breakdown covering the real behaviors, security mess, and crypto scammers who showed up within hours: [https://open.substack.com/pub/diamantai/p/moltbook-a-social-media-for-ai-agents?utm\\_campaign=post-expanded-share&utm\\_medium=web](https://open.substack.com/pub/diamantai/p/moltbook-a-social-media-for-ai-agents?utm_campaign=post-expanded-share&utm_medium=web)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qtuye1/everyones_losing_their_minds_over_moltbook_heres/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o35orf5",
          "author": "nofilmincamera",
          "text": "Have any researchers actually done any meaningful analysis?  Nothing wrong with this article, just the only ive found is a vibe coded platform using Regex I am assuming.  I have done about a million of that type of analysis for work but don't want to redo if someone smarter already did.",
          "score": 3,
          "created_utc": "2026-02-02 14:12:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37mj9h",
          "author": "xpatmatt",
          "text": "100% bold claims from unidentified sources that are attributed no clear method for obtaining the underlying information.\n\nThat's 5 minutes I'll never get back.",
          "score": 3,
          "created_utc": "2026-02-02 19:41:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3aavuw",
              "author": "No_Pin_1150",
              "text": "hes a total disgrace!",
              "score": 1,
              "created_utc": "2026-02-03 04:14:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36atcq",
          "author": "Beginning-Foot-9525",
          "text": "Nice read thanks.",
          "score": 2,
          "created_utc": "2026-02-02 16:02:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39zber",
          "author": "waiting4omscs",
          "text": "So are people wasting money contributing to this site, or is there some end game to make money? I see this talked about in crypto-pivot-to-ai twitter",
          "score": 2,
          "created_utc": "2026-02-03 03:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3aooca",
          "author": "No_Success3928",
          "text": "Should rename to slopbook, though I guess meta has that moniker already.",
          "score": 2,
          "created_utc": "2026-02-03 05:53:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv0mmr",
      "title": "We monitor 4 metrics in production that catch most LLM quality issues early",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qv0mmr/we_monitor_4_metrics_in_production_that_catch/",
      "author": "dinkinflika0",
      "created_utc": "2026-02-03 18:52:58",
      "score": 15,
      "num_comments": 3,
      "upvote_ratio": 0.94,
      "text": "After running LLMs in production for a while, we've narrowed down monitoring to what actually predicts failures before users complain.\n\nLatency p99: Not average latency - p99 catches when specific prompts trigger pathological token generation. We set alerts at 2x baseline.\n\nQuality sampling at configurable rates: Running evaluators on every request burns budget. We sample a percentage of traffic with automated judges checking hallucination, instruction adherence, and factual accuracy. Catches drift without breaking the bank.\n\nCost per request by feature: Token costs vary significantly between features. We track this to identify runaway context windows or inefficient prompt patterns. Found one feature burning 40% of inference budget while serving 8% of traffic.\n\nError rate by model provider: API failures happen. We monitor provider-specific error rates so when one has issues, we can route to alternatives.\n\nWe log everything with distributed tracing. When something breaks, we see the exact execution path - which docs were retrieved, which tools were called, what the LLM actually received.\n\nSetup details: [https://www.getmaxim.ai/docs/introduction/overview](https://www.getmaxim.ai/docs/introduction/overview)\n\nWhat production metrics are you tracking?",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qv0mmr/we_monitor_4_metrics_in_production_that_catch/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3eivw9",
          "author": "Ecto-1A",
          "text": "It really comes down to what you are doing and if you are doing any RAG. What you outlined all seems pretty standard. We monitor latency, tokens, relevance of response, proper tool calling, turns to resolution, confidence, and error handling on every run. Any that fall below our threshold as well as a 20% sample of all runs get sent to an annotation queue and kick off a full suite of G-Eval evaluators and we are working to build out a new testing suite based on the CheckEval paper published a couple months ago.\n\nAre you running any evaluators at build time? That has definitely helped catch some things that could have otherwise flooded our evaluator queues.",
          "score": 2,
          "created_utc": "2026-02-03 20:12:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f1fyh",
          "author": "Informal_Tangerine51",
          "text": "You're monitoring outputs but not capturing inputs. When quality sampling flags hallucination, can you replay what was retrieved to cause it?\n\nWe track similar metrics. The debugging gap: p99 latency spike happens, we know which prompt triggered it, but not what documents were retrieved or whether context was stale. Error rate shows provider failure, doesn't show if retry used different data.\n\nYour distributed tracing logs execution path. Does it capture the actual retrieved content with timestamps, or just that retrieval happened? When evaluator flags factual error, can you verify the source chunks were current?\n\nMetrics catch problems. Evidence proves why they happened. Cost per request is useful, but when that 40% budget feature produces wrong output, can you prevent recurrence or just know it's expensive?",
          "score": 1,
          "created_utc": "2026-02-03 21:39:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qukgay",
      "title": "Preloading MCP tools cost me ~50k tokens per run",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qukgay/preloading_mcp_tools_cost_me_50k_tokens_per_run/",
      "author": "Basic_Tea9680",
      "created_utc": "2026-02-03 06:23:32",
      "score": 13,
      "num_comments": 12,
      "upvote_ratio": 0.88,
      "text": "I ran into something unintuitive while building MCP-based agents using langchain and thought it might be useful to share.\n\nIn my setup, the agent had access to a few common MCP tools like fs, linear, GitHub, figma.\n\nI just added them to the agent and forgot and agent used them sparingly.\nEven with AugmentCode (AI agent I use) I dont want to switch tools on and off. That actually messes up with prompt catching as well .\n\nWhen I actually measured token usage, hereâ€™s what it looked like:\n\nSystem instructions: ~7k tokens\nMCP tool defs: ~45â€“50k tokens\nFirst user message: a few hundred tokens\n\nOn a 200k-context model, that meant ~25% of the context window was gone. Eventually history builds up but this 25% remains consistent. \n\nAs I mentioned earlier, in most runs, the agent only ended up using one or two tools, usually the filesystem. Linear, GitHub and Figma were rarely touched.\n\nSo tens of thousands of tokens were effectively dead weight. The minimum you must do is context caching but on long running agents even that gets expensive. Also the history summarization is triggered more often with this setup.\n\nI tried a different approach, donâ€™t inject all MCP tools upfront. Only surface tools after the model signals it needs them.\n\nThe results were pretty consisten, ~25% fewer total agent tokens for every llm call, lower latency, more context for reasoning, and lessed chat history compaction.\n\nI wrapped this pattern into a small project called mcplexor so I wouldnâ€™t keep re-implementing it. It dynamically discovers MCP tools instead of front-loading them. Feel free to DM if you want to give it a try. Would love feedback to improve it.  ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qukgay/preloading_mcp_tools_cost_me_50k_tokens_per_run/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3atgm5",
          "author": "Ok-Regret3392",
          "text": "Yups. Totally normal! Esp if you have very big mcpâ€™s (Iâ€™m looking at you Stripe and PostHog) Highly recommend you turn on/off the mcps that you know you wonâ€™t use in your currently dev stint. Alternatively.. some mcp calls can be resolved by having the LLM run a curl command instead of burning expensive tokens.",
          "score": 5,
          "created_utc": "2026-02-03 06:33:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3aw5nh",
              "author": "Basic_Tea9680",
              "text": "Turing mcp on off actually causes more harm because prompt cache is invalidated. \n\nI would not recommend it in tools like Claude code and augment code during the session.",
              "score": 1,
              "created_utc": "2026-02-03 06:56:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ffh1y",
                  "author": "santiagolarrain",
                  "text": "Disabling the mcps you have installed and are not using in Claude Code is harmful? I mean disable, exit and run CC again.",
                  "score": 1,
                  "created_utc": "2026-02-03 22:46:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ci986",
          "author": "pbalIII",
          "text": "Dynamic discovery adds a round trip. Your agent now has to signal intent, wait for schema injection, then actually call the tool. For single-shot tasks that's fine, but in tight agentic loops where the model chains 4-5 tool calls, you're stacking latency.\n\nClaude Code shipped lazy loading last month and the feedback I've seen is mixed... faster cold starts but noticeable pauses mid-conversation when a tool gets pulled in for the first time. The semantic search step to match intent to tool also isn't free.\n\nHonest question: have you measured the latency delta on multi-step runs? Curious if the token savings outweigh the added round trips in practice.",
          "score": 2,
          "created_utc": "2026-02-03 14:32:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hjpd9",
              "author": "Basic_Tea9680",
              "text": "So the find tool call is around 2s delay + then the execute tool call , which is local so at the same time. So basically every tool call before discovery adds 2-3s latency. \n\nThe pattern I used was if there is a tool I use very actively, I integrated directly with coding agent rest which are always available but sparingly used are in tool discovery tool. Would love to hear your feedback. You can try on mcplexor.com . I build a nice shell app as well it shows token bloat each tool is adding as well.",
              "score": 1,
              "created_utc": "2026-02-04 06:27:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3isfwm",
                  "author": "pbalIII",
                  "text": "That 2-3s per discovery call adds up fast when you're chaining multiple tools. The tiered approach you're describing (frequently used tools direct, the rest behind discovery) is basically what Claude's tool search does under the hood... they reported 85% token reduction by loading only 3-5 relevant tools instead of the full catalog. Curious how your shell app visualizes the bloat... per-call breakdown or cumulative across the run?",
                  "score": 1,
                  "created_utc": "2026-02-04 12:51:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3bzq0w",
          "author": "InfraScaler",
          "text": "What I do in my custom agent is have \"templates\" stored like skills and a sqlite with the embeddings to do semantic search, so the agent only pulls relevant templates then learns about the relevant MCPs.",
          "score": 1,
          "created_utc": "2026-02-03 12:46:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hk091",
              "author": "Basic_Tea9680",
              "text": "That's interesting, I wanted to improve the recall precision and rank as well so went with a specific route. Thought of many PMs and marketing sales folks who use so many mcp tools. They can benefit from this. Would love for you to try the tool and compare with your solution. Feel free to dm",
              "score": 1,
              "created_utc": "2026-02-04 06:30:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3hla6u",
                  "author": "Wide_Brief3025",
                  "text": "Tracking token usage is a huge pain when you are running heavy MCP tools and looking for efficiency gains, especially if you want to scale lead discovery for PMs and sales. You might find it easier to refine your targeting with something that offers real time keyword alerts like ParseStream, since it helps cut out a lot of wasted runs and only flags genuinely high potential conversations.",
                  "score": 1,
                  "created_utc": "2026-02-04 06:40:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dr8lr",
          "author": "SearchTricky7875",
          "text": "what about using bigtool agent to load the mcp tools dynamically - \n\n    bigtool_agent = bigtool_create_agent(model, {k: v[\"tool\"] for k, v in tool_registry.items()})",
          "score": 1,
          "created_utc": "2026-02-03 18:05:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxg9dd",
      "title": "3D-Agent multi agent system with LangChain for Blender AI",
      "subreddit": "LangChain",
      "url": "https://v.redd.it/kprnhlhgavhg1",
      "author": "Large-Explorer-8532",
      "created_utc": "2026-02-06 12:27:15",
      "score": 12,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qxg9dd/3dagent_multi_agent_system_with_langchain_for/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3ym0vy",
          "author": "Jorsoi13",
          "text": "Cool! Thanks for so much detail! When you mean â€verify the frameâ€œ you are talking about sending a screenshot to the agent or how does verification prevent drift? \n\nIâ€˜m also currently building an agent, however I just started out literally a week ago. Iâ€˜m really trying to grasp how to orchestrate everything together. Iâ€˜m still lacking best practices and Langchain Docs are also a b*tch when it comes to documentation and integrating it with the frontend like yours (nextjs, etc.) \n\nDid you deploy using LangSmith or did you set up a self hosted version yourself? Weâ€˜re currently debating what the best approach is since we donâ€™t want to pay 40â‚¬ just for their deployment. I was more hoping for an â€n8n self-host approachâ€œ on Digitalocean for like 5$ a month :)",
          "score": 3,
          "created_utc": "2026-02-06 20:07:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ynskn",
              "author": "Large-Explorer-8532",
              "text": "We do not use langsmith, we have a mix of custom frameworks and langchain.\n\nI would recomend self-hosted allÂ the way. NoÂ reasonÂ to payÂ $40/monthÂ for LangSmith when you're startingÂ out. YourÂ n8n onÂ DigitalOcean instinctÂ is solid. KeepÂ it simple, it is easy to get lost buying endless things.",
              "score": 1,
              "created_utc": "2026-02-06 20:16:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zbhks",
                  "author": "mdrxy",
                  "text": "starting out, LangSmith has a free plan for developers ;)",
                  "score": 1,
                  "created_utc": "2026-02-06 22:15:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3xiado",
          "author": "Jorsoi13",
          "text": "Wow! Thats amazing. I mean the visuals of that Eiffel Tower are debateable but the fact that it works is great. Now the next step is probably reducing friction. It seems like there are a lot of steps to actually start using the product at least thats what your dashboard suggests. \n\nOther than that: \n\n\\- The agent runs really long. I thought \"Oh shit that thing must swallow some serious credits\". Am I right? How much money did you spend on creating the eiffel tower? Or how many tokens does an operation like that generate? \n\n\\- What is your agent orchestration like? I would really love to see your graph structure for the sake of learning. Would you mind sharing it ?",
          "score": 1,
          "created_utc": "2026-02-06 16:57:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yiph7",
              "author": "Large-Explorer-8532",
              "text": "ThanksÂ man, reallyÂ appreciate the thoughtful feedback!\n\nyeahÂ theÂ Eiffel Tower is definitelyÂ roughÂ blocking, notÂ a portfolioÂ piece. But the pointÂ isÂ theÂ agentÂ builtÂ it autonomously fromÂ aÂ singleÂ prompt, whichÂ is theÂ hardÂ part. QualityÂ willÂ keepÂ improving asÂ the modelsÂ get better.\n\nyou'reÂ 100% right andÂ we're activelyÂ workingÂ on this. TheÂ onboarding hasÂ tooÂ many steps rightÂ now. GoalÂ is: installÂ theÂ pluginÂ â†’ connectÂ â†’ start chatting. We're cuttingÂ itÂ down.\n\nhonestlyÂ it's lessÂ thanÂ you'd think. TheÂ agent isÂ smartÂ about batching operationsÂ andÂ onlyÂ callsÂ theÂ reasoningÂ model when itÂ actuallyÂ needs to makeÂ a decision. MostÂ of theÂ \"thinking\" stepsÂ are lightweight. TheÂ longÂ runtimeÂ is mostlyÂ Blender executingÂ theÂ code, not the AIÂ burningÂ tokens. The output are short mostly short.\n\nThe highÂ levelÂ is: we useÂ a planningÂ agent that breaksÂ theÂ taskÂ into stages, another ones comes to reason and think in \"3D/Spatial Math\" then anÂ execution agent handlesÂ eachÂ stageÂ inÂ aÂ loopÂ (perceive sceneÂ â†’ decideÂ nextÂ actionÂ â†’ executeÂ code â†’ verify viaÂ viewport). The keyÂ insightÂ wasÂ addingÂ theÂ verificationÂ step... withoutÂ it theÂ agent drifts andÂ you getÂ garbage. There's a routerÂ inÂ betweenÂ that decidesÂ whenÂ to escalate toÂ the reasoningÂ model vsÂ handleÂ it withÂ a faster/cheaper one. WouldÂ love to doÂ a deeperÂ write-up atÂ some point onceÂ we'veÂ solidified the architecture more.\n\nWhat's your background? YouÂ soundÂ like you'reÂ buildingÂ agentsÂ yourselfÂ ",
              "score": 2,
              "created_utc": "2026-02-06 19:51:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3zbc8k",
          "author": "mdrxy",
          "text": "cool can you talk more about the langchain architecture you used?",
          "score": 1,
          "created_utc": "2026-02-06 22:14:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zd2up",
          "author": "Upstairs-Spell7521",
          "text": "why do you use langchain tho? what advantages it gives to you?",
          "score": 1,
          "created_utc": "2026-02-06 22:23:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu5ss8",
      "title": "Roast my Thesis: \"Ops teams are burning budget on A100s because reliable quantization pipelines don't exist.\"",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qu5ss8/roast_my_thesis_ops_teams_are_burning_budget_on/",
      "author": "Alternative-Yak6485",
      "created_utc": "2026-02-02 19:59:01",
      "score": 10,
      "num_comments": 10,
      "upvote_ratio": 0.73,
      "text": "Iâ€™m a dev building a 'Quantization-as-a-Service' pipeline and I want to check if I'm solving a real problem or just a skill issue.\n\n**The Thesis:**Â Most AI startups are renting massive GPUs (A100s/H100s) to run base models in FP16. TheyÂ *could*Â downgrade to A10s/T4s (saving \\~50%), but they don't.\n\n**My theory on why:**Â It's not that MLOps teamsÂ *can't*Â figure out quantizationâ€”it's thatÂ **maintaining the pipeline is a nightmare.**\n\n1. You have to manually manage calibration datasets (or risk 'lobotomizing' the model).\n2. You have to constantly update Docker containers for vLLM/AutoAWQ/ExLlama as new formats emerge.\n3. **Verification is hard:**Â You don't have an automated way to prove the quantized model is still accurate without running manual benchmarks.\n\n**The Solution I'm Building:**Â A managed pipeline that handles the calibration selection + generation (AWQ/GGUF/GPTQ) +Â **Automated Accuracy Reporting**Â (showing PPL delta vs FP16).\n\n**The Question:**Â As an MLOps engineer/CTO, is this a pain point you would pay to automate (e.g., $140/mo to offload the headache)?\n\nOr is maintaining your own vLLM/quantization scripts actually pretty easy once it's set up?",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LangChain/comments/1qu5ss8/roast_my_thesis_ops_teams_are_burning_budget_on/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o39rc9j",
          "author": "BeerBatteredHemroids",
          "text": "Platforms like databricks already do this... as a \"CTO\" you should know who your competition is.",
          "score": 2,
          "created_utc": "2026-02-03 02:16:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lfw8s",
              "author": "Purple-Programmer-7",
              "text": "Trying to use data bricks is like sitting in a Cessna cockpit with zero training. Youâ€™ll figure it out eventually, but itâ€™s going to take you a few hours.\n\nIf someone had a LEAN solution that was easy to use and didnâ€™t break the bank, why not?\n\nThough I would say Oxen is already closer to doing what OP suggests.",
              "score": 0,
              "created_utc": "2026-02-04 20:33:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mdbew",
                  "author": "BeerBatteredHemroids",
                  "text": "If you're sitting in a cessna cockpit with no training you shouldn't be in the cockpit babe.",
                  "score": 1,
                  "created_utc": "2026-02-04 23:18:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o38s8lc",
          "author": "soowhatchathink",
          "text": "I'm so sick of people pushing their ai slop here posing an advertisement as a question",
          "score": 3,
          "created_utc": "2026-02-02 23:01:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37t8n3",
          "author": "Space__Whiskey",
          "text": "Try it if you know how. \n\nWhat, will you give up if some nerd on reddit pokes holes in it? \n\nI wouldn't wait for that.",
          "score": 2,
          "created_utc": "2026-02-02 20:12:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jrrt9",
          "author": "PaddingCompression",
          "text": "At $140/mo it's probably too cheap to be worth it!  What I mean is that the reason people burn money on expensive inference, and from what they're telling you, is that the time to deal with it isn't worth the cost savings.\n\nThe more you make it your problem as a thing they can reliably outsource to make the problem go away the better - can you charge 20% plus of the cost savings in a way users know they won't have to worry about it?\n\nThe gold standard would be for you to sell more expensive A10 inference where you guarantee the accuracy, and make it your problem.  The users could pay less for inference and not care how you do it as long as accuracy doesn't suffer.  If you can guarantee accuracy rather than selling a tool, that removes the risk (and includes occasionally having to lose money on an A100 if you can't get the accuracy) - risk shifting would make this much more valuable than just a tool, and if your tool does work means more money for you.",
          "score": 1,
          "created_utc": "2026-02-04 15:55:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lul4f",
          "author": "Unstable_Llama",
          "text": "Itâ€™s not too much harder than running local inference, and anybody training their own models probably has the resources to manage their own quantization.\n\nMaybe not though. Here is a free version for exllama Iâ€™ve been working on, it does multiple quant sizes and measures ppl and kl div and compiles them into a model card with a single command.\n\nhttps://github.com/UnstableLlama/ezexl3",
          "score": 1,
          "created_utc": "2026-02-04 21:43:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3867b0",
          "author": "CanadianPropagandist",
          "text": "I think you might be undervaluing this idea tbh. If you run it a lot more \"white glove\" (I hate that term) you could probably rake in a lot more.\n\nIt fits into an optimization genre that is going to get very popular as companies start learning how to reduce their inference costs.",
          "score": 0,
          "created_utc": "2026-02-02 21:13:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtf5gc",
      "title": "I built a CLI to find \"Zombie Vectors\" in Pinecone/Weaviate (and estimate how much RAM you're wasting)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qtf5gc/i_built_a_cli_to_find_zombie_vectors_in/",
      "author": "billycph",
      "created_utc": "2026-02-01 23:57:50",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "Hey everyone,\n\nIâ€™m an ex-AWS S3 engineer. In my previous life, we obsessed over \"Lifecycle Policies\" because storing petabytes of data is expensive. If data wasnâ€™t touched in 30 days, we moved it to cold storage.\n\nI noticed a weird pattern in the AI space recently: **We are treating Vector Databases like cold storage.**\n\nWe shove 100% of our embeddings into expensive Hot RAM (Pinecone, Milvus, Weaviate), even though for many use cases (like Chat History or Seasonal Catalog Search), 90% of that data is rarely queried after a month. Itâ€™s like keeping your tax returns from 1990 in your wallet instead of a filing cabinet.\n\nI wanted to see exactly how much money was being wasted, so I wrote a simple open-source CLI tool to audit this.\n\n**What it does:**\n\n1. **Connects** to your index (Pinecone currently supported).\n2. **Probes** random sectors of your vector space to sample metadata.\n3. **Analyzes** the `created_at` or timestamp fields.\n4. **Reports** your \"Stale Rate\" (e.g., \"65% of your vectors haven't been queried in >30 days\") and calculates potential savings if you moved them to S3/Disk.\n\n**The \"Trust\" Part:** I know giving API keys to random tools is a bad idea.\n\n* This script runs **100% locally** on your machine.\n* Your keys never leave your terminal.\n* You can audit the code yourself (itâ€™s just Python).\n\n**Why I built this:** Iâ€™m working on a larger library to automate the \"S3 Offloading\" process, but first I wanted to prove that the problem actually exists.\n\nIâ€™d love for you to run it and let me know: **Does your stale rate match what you expected?** Iâ€™m seeing \\~90% staleness for Chat Apps and \\~15% for Knowledge Bases.\n\n**Repo here:** [https://github.com/billycph/VectorDBCostSavingInspector](https://github.com/billycph/VectorDBCostSavingInspector)\n\nFeedback welcome!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qtf5gc/i_built_a_cli_to_find_zombie_vectors_in/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qrsbfa",
      "title": "I am learning LangChain. Could anyone suggest some interesting projects I can build with it?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qrsbfa/i_am_learning_langchain_could_anyone_suggest_some/",
      "author": "Cautious_Ad691",
      "created_utc": "2026-01-31 04:19:06",
      "score": 9,
      "num_comments": 7,
      "upvote_ratio": 0.77,
      "text": "",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qrsbfa/i_am_learning_langchain_could_anyone_suggest_some/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o2qzwrb",
          "author": "SiteCharacter428",
          "text": "If youâ€™re a beginner, start by building a basic **RAG chatbot**.\n\nTry including a **web search tool**, **document parsing**, **image parsing**, and a **vector database** for retrieval. This gives you hands-on experience with the full LLM workflow.\n\nIf you want something more interesting, you can build a **Health Bot** where users upload medical documents or images and the system processes that data to provide context-aware answers.\n\nTip: **Mistral OCR** works surprisingly well for medical images and handwritten doctor notes compared to many other OCR tools.",
          "score": 6,
          "created_utc": "2026-01-31 06:39:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vw08o",
              "author": "Jorsoi13",
              "text": "Great ideas !:)",
              "score": 2,
              "created_utc": "2026-02-01 00:23:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ty2mq",
          "author": "thitcho226",
          "text": "inbox meme",
          "score": 2,
          "created_utc": "2026-01-31 18:29:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34esoi",
          "author": "PretendPop4647",
          "text": "LangChain Academy offers courses; you can follow their guidelines. First start with langchain, then langgraph. \nWhen you learn LangChain or build a project, try to trace LLM.  Use Langsmith for tracing.\n\nBtw they recently introduced a package deepagent,  It is designed to create autonomous agents capable of long-horizon planning and complex task execution like claude code / manus ai.\n\nI built a Job search agent using deepagent.  \n\nYou can check it out >  https://github.com/Rahat-Kabir/job-search-agent",
          "score": 1,
          "created_utc": "2026-02-02 08:09:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36kh66",
          "author": "East-Muffin-6472",
          "text": "RAG chatbot\nAdd memory to it\nConvert it to a two model architecture like one talks and other reasons",
          "score": 1,
          "created_utc": "2026-02-02 16:47:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o372tuw",
          "author": "orthogonal-ghost",
          "text": "One of the first projects I built was a workflow to send daily, heartfelt emails (notes, poems, etc.) to family and friends. It was relatively easy to stand up and offered a quick way to play around with LangChain and get up to speed on tool-use and MCP servers",
          "score": 1,
          "created_utc": "2026-02-02 18:11:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ho8md",
          "author": "scrapper_911",
          "text": "I would say build a very simple project, focus more on how how will justify your answers. \n\nObservability and governance are the words when it comes to AI systems.",
          "score": 1,
          "created_utc": "2026-02-04 07:06:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qunx9g",
      "title": "Why doesn't LangChain support agent skills?",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qunx9g/why_doesnt_langchain_support_agent_skills/",
      "author": "Suspicious_Fall6860",
      "created_utc": "2026-02-03 09:55:40",
      "score": 9,
      "num_comments": 11,
      "upvote_ratio": 0.77,
      "text": "Why doesn't LangChain support agent skills? It only allows loading a single [skill.md](http://skill.md) file. How can we support references and scripts?\n\nHere are some materials I found.\n\n[Skills - Docs by LangChain](https://docs.langchain.com/oss/python/langchain/multi-agent/skills)  \n  \n[Build a SQL assistant with on-demand skills - Docs by LangChain](https://docs.langchain.com/oss/python/langchain/multi-agent/skills-sql-assistant)  \n\n\n[deepagents/examples/content-builder-agent/skills/blog-post/SKILL.md at master Â· langchain-ai/deepagents Â· GitHub](https://github.com/langchain-ai/deepagents/tree/master/examples)  \n  \n[deepagents/examples/content-builder-agent at master Â· langchain-ai/deepagents](https://github.com/langchain-ai/deepagents/tree/master/examples/content-builder-agent)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qunx9g/why_doesnt_langchain_support_agent_skills/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3bfsbg",
          "author": "Otherwise_Wave9374",
          "text": "Yeah the single SKILL.md thing feels limiting once you want anything beyond toy examples (versioning, shared snippets, scripts, references, etc). I have seen people treat skills as a mini package, folder per skill with an index plus tests, then load/resolve by name and inject into the agent prompt/runtime. Would be nice if LangChain standardized that pattern. I have a couple writeups saved on agent skills/tooling design here too: https://www.agentixlabs.com/blog/",
          "score": 5,
          "created_utc": "2026-02-03 10:03:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bgsic",
          "author": "Suspicious_Fall6860",
          "text": "Actually, I've found that most current support for Agent Skills is basically in CLI-mode systems. Does anyone know of any frameworks that support explicit skill writing and debugging?",
          "score": 3,
          "created_utc": "2026-02-03 10:12:55",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3bpjrz",
              "author": "Tobi-Random",
              "text": "Official spec project provides tools for validation against spec",
              "score": 1,
              "created_utc": "2026-02-03 11:31:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hojmy",
                  "author": "ohansemmanuel",
                  "text": "I think this is still lacking. At best it validates frontmatter and a few best practices. \n\nIn production systems you'll quickly find out that even with the best Skills, they're (almost) useless if NOT triggered by the AI agent. \n\nCurrent \"eval\" systems don't really work with Skills (at least not in a way I think is optimised). So there's still a need for robust systems to build, test, iterate as we do with prompts today. Something along the lines of trigger evals?\n\nA counter argument would be that Skills are just prompts and you can still get by. True, the difference would be we're bundling a lot more in these \"prompts\"",
                  "score": 1,
                  "created_utc": "2026-02-04 07:08:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3d0ga9",
              "author": "Jords13xx",
              "text": "Check out the Rasa framework for building conversational agents with custom skills. It's pretty robust for skill writing and debugging. Also, you might want to explore Botpress or Dialogflow; they offer good support for skill customization.",
              "score": 1,
              "created_utc": "2026-02-03 16:01:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3chz29",
          "author": "pbalIII",
          "text": "Ran into the same friction building a multi-skill agent last month. The single SKILL.md loader is intentional... LangChain wants skills to be self-contained folders with their own files, scripts, and references bundled together.\n\nThe pattern that worked for me: treat each skill as its own directory under ~/.deepagents/agent/skills/, then let the agent discover and load them by name at runtime. The frontmatter gets indexed for discovery, but the full SKILL.md only loads when the agent actually needs it (saves tokens).\n\nFor debugging, deepagents-CLI has a skills list command that shows what's loaded. Not perfect tooling, but better than dumping everything into one monolithic file.",
          "score": 2,
          "created_utc": "2026-02-03 14:30:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3crow1",
          "author": "cordialgerm",
          "text": "References and scripts are supposed to be loaded on demand by the agent after reading the SKILL.md. so all you need to do is include them in your filesystem and reference them in the SKILL.md and it works great.",
          "score": 1,
          "created_utc": "2026-02-03 15:20:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eofke",
          "author": "Niightstalker",
          "text": "Here they wrote a blogpost about supporting skills within their deep agents cli: https://www.blog.langchain.com/using-skills-with-deep-agents/",
          "score": 1,
          "created_utc": "2026-02-03 20:39:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hmmrj",
          "author": "ohansemmanuel",
          "text": "Technically you could probably get by building a system around this yourself. \n\nYou'd need to connect to a VM / sandboxed machine at runtime, that's capable of running scripts, installin dependencies, leveraging bash for reading additional files etc. You'd then expose tools to interact with the said machine. \n\nYou alluded to the bigger issue in your comment - as an industry it seems we're mostly focused on Skills within CLI (coding agents) atm. But in my opinion, the bigger win comes from the use case you're describing i.e., remote agents running determinsitic workflows / SOPs with references and scripts. \n\nIf you're looking for an off the shelve solution, you may like Bluebag AI (handles all the hard stuff so you can integrate in 2-lines of code) \n\nDisclaimer: I built this and already used in production systems. Would happily walk you through it",
          "score": 1,
          "created_utc": "2026-02-04 06:52:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xs2ng",
          "author": "npentrel",
          "text": "Hi there - just updated the docs a bit to make this clearer: You can use more files, as long as they are referenced in [SKILL.md](http://SKILL.md)  \\- thanks for pointing the needed clarification out to us! [https://docs.langchain.com/oss/python/deepagents/skills](https://docs.langchain.com/oss/python/deepagents/skills)",
          "score": 1,
          "created_utc": "2026-02-06 17:43:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3c5nqq",
          "author": "Upset-Pop1136",
          "text": "langchainâ€™s not trying to be a full agent OS. they optimize for demos and DX, not long-lived agents. ",
          "score": 0,
          "created_utc": "2026-02-03 13:23:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtw1wu",
      "title": "LangChain VS LamaIndex - Plug r/LangChain context into your LangChain agents - Free MCP integration",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qtw1wu/langchain_vs_lamaindex_plug_rlangchain_context/",
      "author": "jannemansonh",
      "created_utc": "2026-02-02 14:10:12",
      "score": 9,
      "num_comments": 2,
      "upvote_ratio": 0.85,
      "text": "Hey, creator of [needle.app](http://needle.app) here. This subreddit has incredible implementation knowledge - patterns, agent architectures, RAG configs, tool calling issues, what actually works in production.\n\nWe indexed all 2025 r/LangChain discussions and made them searchable. Even better: we built an MCP integration so you can plug this entire subreddit's context directly into your LangChain agents for agentic RAG.\n\nTry searching:\n\n* Tool calling with function schemas\n* Multi-agent orchestration patterns\n* Vector store performance comparisons\n\nUseful if you're:\n\n* Debugging agent loops or tool calling\n* Finding solutions others have already tested\n\n**Want to use this in your LangChain agents?** Check out our MCP integration guide: [https://docs.needle.app/docs/guides/mcp/needle-mcp-server/](https://docs.needle.app/docs/guides/mcp/needle-mcp-server/)\n\nNow you can build agents that query r/LangChain knowledge directly while reasoning.\n\nCompletely free, no signup: [https://needle.app/featured-collections/reddit-langchain-2025](https://needle.app/featured-collections/reddit-langchain-2025)\n\n[LangChain or LamaIndex - Needle.app RAG Chat](https://reddit.com/link/1qtw1wu/video/prkrlbzo93hg1/player)\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LangChain/comments/1qtw1wu/langchain_vs_lamaindex_plug_rlangchain_context/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o35qt2s",
          "author": "beneautomas",
          "text": "Useful!",
          "score": 1,
          "created_utc": "2026-02-02 14:23:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35rfk4",
              "author": "jannemansonh",
              "text": "RAGception XD",
              "score": 1,
              "created_utc": "2026-02-02 14:26:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qsxuum",
      "title": "Is AsyncPostgresSaver actually production-ready in 2026? (Connection pooling & resilience issues)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qsxuum/is_asyncpostgressaver_actually_productionready_in/",
      "author": "FunEstablishment5942",
      "created_utc": "2026-02-01 13:00:46",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nI'm finalizing the architecture for a production agent service and blocked on the database layer. I've seen multiple reports (and GitHub issues like #5675 and #1730) from late 2025 indicating thatÂ `AsyncPostgresSaver`Â is incredibly fragile when it comes to connection pooling.\n\nSpecifically, I'm concerned about:\n\n1. **Zero Resilience:**Â If the underlying pool closes or a connection goes stale, the saver seems to just crash withÂ `PoolClosed`Â orÂ `OperationalError`Â rather than attempting a retry or refresh.\n2. **Lifecycle Management:**Â Sharing aÂ `psycopg_pool`Â between my application (SQLAlchemy) and LangGraph seems to result in race conditions where LangGraph holds onto references to dead pools.\n\n**My Question:**  \nHas anyone successfully deployedÂ `AsyncPostgresSaver`Â in a high-load production environment recently (early 2026)? Did the team ever release a native fix for automatic retries/pool recovery, or are you all still writing custom wrappers / separate pool managers to baby the checkpointer?\n\nI'm trying to decide if I should risk using the standard saver or just bite the bullet and write a custom Redis/Postgres implementation from day one.\n\nThanks! Is AsyncPostgresSaver actually production-ready in 2026? (Connection pooling & resilience issues)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qsxuum/is_asyncpostgressaver_actually_productionready_in/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o33q9hi",
          "author": "Shreyanak_exe",
          "text": "I really need an answer to this cause I am building stg similar that's going in production soon and I want to be sure the connection doesn't goes stale every now and then. \n\nBtw: some guy built a resilient wrapper for Postgres. If you can test and lmk if it's worth giving a shot, that'd be helpful\n\n[ResilientPostgresSaver](https://github.com/samirpatil2000/agentic-template/blob/main/agents/resilient_postgres_saver.py)",
          "score": 2,
          "created_utc": "2026-02-02 04:46:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3uuae1",
          "author": "rookastle",
          "text": "This is a known fragility point. The saver is lean and expects a persistent, valid connection, which isn't always realistic. Many production setups add a layer on top.\n\nAs a practical diagnostic, you could try wrapping your checkpointer's \\`get\\` and \\`put\\` methods with a simple exponential backoff retry decorator (e.g., from \\`tenacity\\`). Targeting \\`psycopg.OperationalError\\` specifically can help isolate whether the failures are due to transient network issues or a more fundamental state management problem. This often confirms the root cause without requiring a full custom implementation upfront.",
          "score": 2,
          "created_utc": "2026-02-06 05:59:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3we7wj",
              "author": "FunEstablishment5942",
              "text": "i ended up copying this class: [https://github.com/samirpatil2000/agentic-template/blob/main/agents/resilient\\_postgres\\_saver.py](https://github.com/samirpatil2000/agentic-template/blob/main/agents/resilient_postgres_saver.py) what do you think? that should work?\n\n",
              "score": 1,
              "created_utc": "2026-02-06 13:38:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zq0ut",
          "author": "papipapi419",
          "text": "!remindme 5 days",
          "score": 1,
          "created_utc": "2026-02-01 16:20:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zq60m",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 5 days on [**2026-02-06 16:20:31 UTC**](http://www.wolframalpha.com/input/?i=2026-02-06%2016:20:31%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LangChain/comments/1qsxuum/is_asyncpostgressaver_actually_productionready_in/o2zq0ut/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLangChain%2Fcomments%2F1qsxuum%2Fis_asyncpostgressaver_actually_productionready_in%2Fo2zq0ut%2F%5D%0A%0ARemindMe%21%202026-02-06%2016%3A20%3A31%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qsxuum)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-01 16:21:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35htt0",
          "author": "pbalIII",
          "text": "Same pattern plays out in every ORM/framework that wraps database connections... the abstraction handles the happy path but breaks when the connection layer misbehaves.\n\nFWIW issue #5675 is still open with no native fix. Most production deployments I've seen do one of two things:\n\n- Dedicated pool for LangGraph (don't share with SQLAlchemy)\n- Custom retry wrapper that catches PoolClosed/OperationalError and reconnects\n\nThe from_conn_string helper creates a single connection, not a pool. Under load that's asking for trouble. If you're already comfortable with psycopg, building your own thin wrapper around AsyncConnectionPool with health checks is probably less risky than hoping for an upstream fix.",
          "score": 1,
          "created_utc": "2026-02-02 13:33:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1quni12",
      "title": "AI projects with Langchain and Langgraph",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1quni12/ai_projects_with_langchain_and_langgraph/",
      "author": "Affectionate_Bid2797",
      "created_utc": "2026-02-03 09:28:03",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 0.81,
      "text": "Hello everyone,\n\nI hope youâ€™re doing well. Iâ€™m a software engineer whoâ€™s really passionate about machine learning and AI, and Iâ€™d love to get some advice from engineers already working in the field.\n\nIâ€™ve studied the fundamentals and understand the theory and common frameworks, but I feel I need to build more concrete, real-world projects to gain confidence and practical experience.\n\nIâ€™ve gone through tutorials and done quite a bit of research, but much of the advice feels repetitive, and many project suggestions are the same everywhere. So I wanted to ask directly: what projects would you recommend building that are actually useful and help someone stand out?\n\nIâ€™m not looking for generic or clichÃ© advice, but rather insights from people with hands-on experience in the industry.\n\nThanks a lot for your time.I really appreciate any suggestions.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1quni12/ai_projects_with_langchain_and_langgraph/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3bdrb1",
          "author": "Witty_System7237",
          "text": "What kind of domain are you most interested in-like data analysis, chat assistants, or something else? That could help narrow down useful project ideas.",
          "score": 1,
          "created_utc": "2026-02-03 09:43:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3be0jw",
              "author": "Affectionate_Bid2797",
              "text": "I am interested in building agentic workflows from end-to-end.   \nThank you!",
              "score": 1,
              "created_utc": "2026-02-03 09:46:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3boith",
                  "author": "Apart_Commercial2279",
                  "text": "the main issue if you want to face real world issue and not getting basic advice is to build a usefull agent for you, your friends family or coworker and make them use them (this is the hardest). If they don't use it, or use it the wrong way or if they don't get what they need you will iterate and make it more complex, fix bug, add retry, guardrails ect... And really have an end to end system",
                  "score": 0,
                  "created_utc": "2026-02-03 11:22:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3c3nsh",
          "author": "Upset-Pop1136",
          "text": "Learn from the open source products and great tools. One of them is Dify",
          "score": 1,
          "created_utc": "2026-02-03 13:11:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gspuy",
          "author": "hrishikamath",
          "text": "Honesty just do projects you really care, where you care about the output and itâ€™s challenging but itâ€™s also reasonable. You will come across problems and then use whatever you learn to diagnose, you will learn better. I did that for finance and ended up learning a lot without taking tutorials or watching any courses. Itâ€™s open source happy to share the link/blogpost if you want.",
          "score": 1,
          "created_utc": "2026-02-04 03:19:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ifk6p",
          "author": "MathematicianTop1654",
          "text": "this tutorial might be useful: [https://www.crewship.dev/blog/deploy-langgraph-to-production](https://www.crewship.dev/blog/deploy-langgraph-to-production)",
          "score": 1,
          "created_utc": "2026-02-04 11:16:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvvnzd",
      "title": "Build a self-updating wiki from codebases (open source, Apache 2.0)",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qvvnzd/build_a_selfupdating_wiki_from_codebases_open/",
      "author": "Whole-Assignment6240",
      "created_utc": "2026-02-04 17:57:34",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "I recently have been working onÂ [a new project](https://github.com/cocoindex-io/cocoindex/tree/v1/examples/multi_codebase_summarization)Â to build a self-updating wiki from codebases. I wrote a step-by-step tutorial.\n\nYour code is the source of truth, and documentations out of sync is such a common pain especially in larger teams. Someone refactors a module, and the wiki is already wrong. Nobody updates it until a new engineer asks a question about it.\n\nThis open source project scans your codebases, extracts structured information with LLMs, and generates Markdown documentation with Mermaid diagrams â€” using CocoIndex + Instructor + Pydantic.\n\nWhat's cool about this example:\n\nâ€¢ ğˆğ§ğœğ«ğğ¦ğğ§ğ­ğšğ¥ ğ©ğ«ğ¨ğœğğ¬ğ¬ğ¢ğ§ğ  â€” Only changed files get reprocessed. saving 90%+ of LLM cost and compute.\n\nâ€¢ ğ’ğ­ğ«ğ®ğœğ­ğ®ğ«ğğ ğğ±ğ­ğ«ğšğœğ­ğ¢ğ¨ğ§ ğ°ğ¢ğ­ğ¡ ğ‹ğ‹ğŒğ¬ â€” LLM returns real typed objects â€” classes, functions, signatures, relationships.\n\nâ€¢ ğ€ğ¬ğ²ğ§ğœ ğŸğ¢ğ¥ğ ğ©ğ«ğ¨ğœğğ¬ğ¬ğ¢ğ§ğ  â€” All files in a project get extracted concurrently with asyncio.gather().\n\nâ€¢ ğŒğğ«ğ¦ğšğ¢ğ ğğ¢ğšğ ğ«ğšğ¦ğ¬ â€” Auto-generated pipeline visualizations showing how your functions connect across the project.\n\nThis pattern hooks naturally into PR flows â€” run it on every merge and your docs stay current without anyone thinking about it. I think it would be cool next to build a coding agent with Langchain on top of this fresh knowledge. \n\nIf you want to explore the full example (fully open source, with code, APACHE 2.0), it's here:\n\nğŸ‘‰Â [https://cocoindex.io/examples-v1/multi-codebase-summarization](https://cocoindex.io/examples-v1/multi-codebase-summarization)\n\nIf you find CocoIndex useful, a star on Github means a lot :)\n\nâ­Â [https://github.com/cocoindex-io/cocoindex](https://github.com/cocoindex-io/cocoindex)\n\ni'd love to learn from your feedback, thanks!",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LangChain/comments/1qvvnzd/build_a_selfupdating_wiki_from_codebases_open/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qu1bqx",
      "title": "AI Agent to deal with enormous datasets",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qu1bqx/ai_agent_to_deal_with_enormous_datasets/",
      "author": "Abject_Reference_160",
      "created_utc": "2026-02-02 17:23:15",
      "score": 6,
      "num_comments": 18,
      "upvote_ratio": 0.88,
      "text": "I'm working on a system that implements an AI Agent that analyses the sales history and forecasts future demand.  \nIt is written in NestJS and uses langchain and langchain/openai. The agent is basically declared as follows:  \n  \nconstructor() {  \nthis.chatOpenAI = new ChatOpenAI({  \napiKey: process.env.OPENAI\\_API\\_KEY,  \nmodel: \"gpt-5-mini-2025-08-07\",  \nverbose: true  \n});  \nÂ  }\n\n  \nSo, kinda basic. This is also the first time i'm implementing a complex system with onboard AI, so any tips would be welcome.\n\nThe problem is, i need my ai to be able to read enormous datasets at once, like a really big sales history (it is the biggest part), but I always hit limitations like text too big for sending in a request or it is way past the 128k token limit.  \nI tried using toon, but my agent got confused and returned nothing to an input that normally would generate data.\n\nRAG was an idea for saving tokens but, afaik, it shouldn't be used for calculations like this, but for textual understanding and searches.  \nProducing batch pre compiled analysis was also an option, but it would be really hard to preserve all the insights that are possible with the raw data.\n\nHow can i set it up to reading monstruous datasets like this?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qu1bqx/ai_agent_to_deal_with_enormous_datasets/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3750v7",
          "author": "big_fart_9090",
          "text": "lol, edit: sorry now constructive. An LLM is the wrong tool for what you want to achieve mate. Try asking an LLM on how to do time series forecasting. It wil suggest various data science stuff. Good luck, you will need it",
          "score": 2,
          "created_utc": "2026-02-02 18:20:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o378je4",
              "author": "Abject_Reference_160",
              "text": "Thanks mate, guess i'll really need this luck lol.  \nI was having a talk with grok and it also came to the conclusion that a LLM is optimized more as a conversational thing, and it isn't the same tool used by scientists and such (I initially thought it was more of a setup difference or perfectly thought out strategies that i was also trying to come up with). I'll search more about it and come back with updates.",
              "score": 1,
              "created_utc": "2026-02-02 18:36:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dsloi",
          "author": "Otherwise_Flan7339",
          "text": "You can't pass raw sales history directly to an LLM. It's not a database.  \n  \nFocus on \\*agent tooling\\*. Build tools that run SQL queries or Pandas operations on your dataset. The agent calls these tools based on the user's need.  \n  \nIt processes summarized data, not raw rows. We use this pattern for sales data too.",
          "score": 2,
          "created_utc": "2026-02-03 18:11:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36viux",
          "author": "Empty_Contact_2823",
          "text": "Do you need to ingest all of the historic data at once? Or find the relevant data first to operate on?\n\nTraditional Unix file system access using grep etc to pipe to llm can be a way to counter the token limit.\n\nAlso worth considering if you could filter the data via api calls / function calls within the llm first.",
          "score": 1,
          "created_utc": "2026-02-02 17:38:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36wwfx",
              "author": "Abject_Reference_160",
              "text": "Generally what I do is select the products I want to analyze and get its related data, such as inventories, purchase orders and sales orders.  \nTo ingest all the product's sales history would enable me to perceive patterns, such as which clients use to buy this product, when they buy it and what they get it with, and it would interact better with unstructured data like the info that a client is breaking the contract (how much do they represent for this product's demand and when?).\n\nIs this what you were referring to?",
              "score": 1,
              "created_utc": "2026-02-02 17:44:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o386zfl",
                  "author": "Wide_Brief3025",
                  "text": "It sounds like you're definitely on the right track by tying together sales history, inventory, and contract events for deeper insight. If you start pulling in conversational data or signals from platforms like Reddit or LinkedIn, something like ParseStream can automate tracking keywords and alert you to important discussions so you never miss relevant signals for your products.",
                  "score": 1,
                  "created_utc": "2026-02-02 21:17:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o36xjxb",
              "author": "Abject_Reference_160",
              "text": "i'd like to, for instance, get all the products of a supplier and analyze them at once, or at least several products. At the moment, even 1 product's history is too much, and some suppliers have far more demand than others, which means their products will have a sales history that is even bigger and technically harder to analyze.",
              "score": 1,
              "created_utc": "2026-02-02 17:47:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o383gg3",
                  "author": "snarfi",
                  "text": "You need to query your dataset via an api first to only return whats needed for the particular request.",
                  "score": 1,
                  "created_utc": "2026-02-02 21:01:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o378mit",
          "author": "Strong_Cherry6762",
          "text": "How large is your dataset? How many megabytes of memory does it occupy? How many rows of data records are there?",
          "score": 1,
          "created_utc": "2026-02-02 18:37:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37bese",
              "author": "Abject_Reference_160",
              "text": "I have nearly 5k products, that together have half a million sales orders. Each product has a inventory record for each deposit, and it must all be analysed together in order to suggest sales or transfers between deposits. It is a sales history from 2020 to current day, and it will continue growing, as the system will be used for several years to come.  \nThe model i'm using, gpt-5-mini-2025-08-07, appears to have a token limit of 128.000, and for some products, 1 month of its sales history is way more than enough to break this limit, so considering I need to look to 5 years and more as the time passes, it is kinda big (or at least it looks monstruous for my current setup to handle).\n\nWe are planning to grow a lot on the oncoming years, so more data will be generated on a shorter time.",
              "score": 1,
              "created_utc": "2026-02-02 18:49:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o37fkhq",
                  "author": "Strong_Cherry6762",
                  "text": "Okay, that is absolutely huge for an LLM context window, but actually \"small data\" for a computer.\n\nIf I were you, I'd implement this using a CLI + skill approachâ€”just let the LLM write Python code directly in your terminal, and then use that code to handle the data processing tasks.\n\nOf course, if you do this kind of repetitive work often, I suggest using a \"skill-creator\" (Anthropic has a guide on this: https://resources.anthropic.com/hubfs/The-Complete-Guide-to-Building-Skill-for-Claude.pdf) to build a skill that solidifies your data processing workflow.\n\nGood luck with the build!",
                  "score": 2,
                  "created_utc": "2026-02-02 19:08:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37uxx9",
          "author": "InfraScaler",
          "text": "You need to give your agent tools to filter the data without having to read it first, e.g. ways to query the dataset.Â ",
          "score": 1,
          "created_utc": "2026-02-02 20:20:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3c4enn",
              "author": "Abject_Reference_160",
              "text": "Is it somehow different to use tools and to send the same data in the prompt?",
              "score": 1,
              "created_utc": "2026-02-03 13:15:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3cnsdb",
                  "author": "InfraScaler",
                  "text": "It's about managing context bloat. Instead of letting the agent read all the data then filter, you tell the agent it can use tools (for the sake of argument: put it in a sqlite and query it using sql) to send queries and get subsets of the data already filtered.",
                  "score": 2,
                  "created_utc": "2026-02-03 15:00:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3nbueh",
          "author": "Potential-Analyst571",
          "text": "Donâ€™t feed huge raw datasets to the agent.. Preaggregate and chunk the data first, then let the model reason on summaries instead of rows. Keeping that data flow clearly defined (tools like Traycer help) avoids token limits and confused agents.",
          "score": 1,
          "created_utc": "2026-02-05 02:32:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qx41gl",
      "title": "Open source trust verification for multi-agent systems",
      "subreddit": "LangChain",
      "url": "https://www.reddit.com/r/LangChain/comments/1qx41gl/open_source_trust_verification_for_multiagent/",
      "author": "HolidayCharge1511",
      "created_utc": "2026-02-06 01:28:31",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "Hey everyone,  \n  \nI've been working on a problem that's been bugging me: as AI agents start talking to each other (Google's A2A protocol, LangChain multi-agent systems, etc.), there's no way to verify if an external agent is trustworthy.  \n  \nSo I built \\*\\*TrustAgents\\*\\* â€” essentially a firewall for the agentic era.  \n  \n**What it does:**  \n\\- Scans agent interactions for prompt injection, jailbreaks, data exfiltration (65+ threat patterns)  \n\\- Tracks reputation scores per agent over time  \n\\- Lets agents prove legitimacy via email/domain verification  \n\\- Sub-millisecond scan times  \n  \n**Stack:**  \n\\- FastAPI + PostgreSQL (Railway)  \n\\- Next.js landing page (Vercel)  \n\\- Clerk auth + Stripe billing  \n\\- Python SDK on PyPI, TypeScript SDK on npm, LangChain integration  \n  \n  \nWould love feedback from anyone building with AI agents. What security concerns do you run into?  \n  \n[https://trustagents.dev](https://trustagents.dev)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LangChain/comments/1qx41gl/open_source_trust_verification_for_multiagent/",
      "domain": "self.LangChain",
      "is_self": true,
      "comments": [
        {
          "id": "o3u0cvs",
          "author": "AdditionalWeb107",
          "text": "you should look at [https://github.com/katanemo/plano](https://github.com/katanemo/plano) \\- similar ideas but designed to be framework-agnostic. Its a substrate to manage and handle all traffic coming in/out of agents in a protocol-native way",
          "score": 3,
          "created_utc": "2026-02-06 02:38:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}